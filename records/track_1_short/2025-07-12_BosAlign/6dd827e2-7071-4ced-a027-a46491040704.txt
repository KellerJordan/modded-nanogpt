import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 01:02:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    5856MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1517MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   39C    P0            124W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   33C    P0            117W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   40C    P0            121W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   32C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           69683      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           69684      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           69685      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           69686      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           69687      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           69688      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           69689      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           69690      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           69684      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           69685      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           69686      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           69687      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           69688      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           69689      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           69690      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:150ms step_avg:150.02ms
step:2/1750 train_time:176ms step_avg:87.95ms
step:3/1750 train_time:248ms step_avg:82.78ms
step:4/1750 train_time:340ms step_avg:84.91ms
step:5/1750 train_time:432ms step_avg:86.40ms
step:6/1750 train_time:524ms step_avg:87.36ms
step:7/1750 train_time:616ms step_avg:88.04ms
step:8/1750 train_time:709ms step_avg:88.60ms
step:9/1750 train_time:801ms step_avg:89.02ms
step:10/1750 train_time:894ms step_avg:89.38ms
step:11/1750 train_time:986ms step_avg:89.66ms
step:12/1750 train_time:1082ms step_avg:90.14ms
step:13/1750 train_time:1177ms step_avg:90.53ms
step:14/1750 train_time:1273ms step_avg:90.92ms
step:15/1750 train_time:1365ms step_avg:91.02ms
step:16/1750 train_time:1459ms step_avg:91.17ms
step:17/1750 train_time:1552ms step_avg:91.28ms
step:18/1750 train_time:1644ms step_avg:91.36ms
step:19/1750 train_time:1737ms step_avg:91.42ms
step:20/1750 train_time:1829ms step_avg:91.47ms
step:21/1750 train_time:1922ms step_avg:91.51ms
step:22/1750 train_time:2015ms step_avg:91.60ms
step:23/1750 train_time:2109ms step_avg:91.68ms
step:24/1750 train_time:2204ms step_avg:91.83ms
step:25/1750 train_time:2298ms step_avg:91.91ms
step:26/1750 train_time:2391ms step_avg:91.98ms
step:27/1750 train_time:2485ms step_avg:92.04ms
step:28/1750 train_time:2578ms step_avg:92.09ms
step:29/1750 train_time:2671ms step_avg:92.10ms
step:30/1750 train_time:2763ms step_avg:92.10ms
step:31/1750 train_time:2856ms step_avg:92.12ms
step:32/1750 train_time:2949ms step_avg:92.14ms
step:33/1750 train_time:3042ms step_avg:92.19ms
step:34/1750 train_time:3136ms step_avg:92.23ms
step:35/1750 train_time:3229ms step_avg:92.26ms
step:36/1750 train_time:3323ms step_avg:92.29ms
step:37/1750 train_time:3417ms step_avg:92.35ms
step:38/1750 train_time:3511ms step_avg:92.39ms
step:39/1750 train_time:3604ms step_avg:92.40ms
step:40/1750 train_time:3697ms step_avg:92.42ms
step:41/1750 train_time:3790ms step_avg:92.45ms
step:42/1750 train_time:3884ms step_avg:92.47ms
step:43/1750 train_time:3976ms step_avg:92.47ms
step:44/1750 train_time:4069ms step_avg:92.48ms
step:45/1750 train_time:4163ms step_avg:92.51ms
step:46/1750 train_time:4256ms step_avg:92.53ms
step:47/1750 train_time:4349ms step_avg:92.54ms
step:48/1750 train_time:4443ms step_avg:92.57ms
step:49/1750 train_time:4537ms step_avg:92.59ms
step:50/1750 train_time:4630ms step_avg:92.59ms
step:51/1750 train_time:4723ms step_avg:92.60ms
step:52/1750 train_time:4816ms step_avg:92.62ms
step:53/1750 train_time:4910ms step_avg:92.64ms
step:54/1750 train_time:5003ms step_avg:92.65ms
step:55/1750 train_time:5096ms step_avg:92.65ms
step:56/1750 train_time:5188ms step_avg:92.65ms
step:57/1750 train_time:5282ms step_avg:92.67ms
step:58/1750 train_time:5376ms step_avg:92.69ms
step:59/1750 train_time:5470ms step_avg:92.71ms
step:60/1750 train_time:5564ms step_avg:92.73ms
step:61/1750 train_time:5657ms step_avg:92.73ms
step:62/1750 train_time:5750ms step_avg:92.74ms
step:63/1750 train_time:5843ms step_avg:92.75ms
step:64/1750 train_time:5937ms step_avg:92.76ms
step:65/1750 train_time:6030ms step_avg:92.78ms
step:66/1750 train_time:6123ms step_avg:92.78ms
step:67/1750 train_time:6216ms step_avg:92.78ms
step:68/1750 train_time:6311ms step_avg:92.81ms
step:69/1750 train_time:6405ms step_avg:92.83ms
step:70/1750 train_time:6499ms step_avg:92.84ms
step:71/1750 train_time:6592ms step_avg:92.85ms
step:72/1750 train_time:6686ms step_avg:92.85ms
step:73/1750 train_time:6779ms step_avg:92.86ms
step:74/1750 train_time:6871ms step_avg:92.85ms
step:75/1750 train_time:6964ms step_avg:92.85ms
step:76/1750 train_time:7057ms step_avg:92.86ms
step:77/1750 train_time:7150ms step_avg:92.86ms
step:78/1750 train_time:7243ms step_avg:92.86ms
step:79/1750 train_time:7337ms step_avg:92.87ms
step:80/1750 train_time:7430ms step_avg:92.87ms
step:81/1750 train_time:7524ms step_avg:92.89ms
step:82/1750 train_time:7617ms step_avg:92.89ms
step:83/1750 train_time:7710ms step_avg:92.90ms
step:84/1750 train_time:7804ms step_avg:92.90ms
step:85/1750 train_time:7897ms step_avg:92.91ms
step:86/1750 train_time:7990ms step_avg:92.91ms
step:87/1750 train_time:8083ms step_avg:92.91ms
step:88/1750 train_time:8177ms step_avg:92.92ms
step:89/1750 train_time:8270ms step_avg:92.92ms
step:90/1750 train_time:8363ms step_avg:92.92ms
step:91/1750 train_time:8456ms step_avg:92.93ms
step:92/1750 train_time:8550ms step_avg:92.93ms
step:93/1750 train_time:8643ms step_avg:92.94ms
step:94/1750 train_time:8737ms step_avg:92.94ms
step:95/1750 train_time:8830ms step_avg:92.94ms
step:96/1750 train_time:8923ms step_avg:92.95ms
step:97/1750 train_time:9017ms step_avg:92.96ms
step:98/1750 train_time:9110ms step_avg:92.96ms
step:99/1750 train_time:9204ms step_avg:92.97ms
step:100/1750 train_time:9297ms step_avg:92.97ms
step:101/1750 train_time:9389ms step_avg:92.96ms
step:102/1750 train_time:9482ms step_avg:92.96ms
step:103/1750 train_time:9577ms step_avg:92.98ms
step:104/1750 train_time:9670ms step_avg:92.98ms
step:105/1750 train_time:9763ms step_avg:92.98ms
step:106/1750 train_time:9856ms step_avg:92.98ms
step:107/1750 train_time:9949ms step_avg:92.98ms
step:108/1750 train_time:10042ms step_avg:92.99ms
step:109/1750 train_time:10136ms step_avg:92.99ms
step:110/1750 train_time:10229ms step_avg:92.99ms
step:111/1750 train_time:10323ms step_avg:93.00ms
step:112/1750 train_time:10415ms step_avg:92.99ms
step:113/1750 train_time:10508ms step_avg:92.99ms
step:114/1750 train_time:10602ms step_avg:93.00ms
step:115/1750 train_time:10696ms step_avg:93.01ms
step:116/1750 train_time:10789ms step_avg:93.01ms
step:117/1750 train_time:10882ms step_avg:93.01ms
step:118/1750 train_time:10976ms step_avg:93.01ms
step:119/1750 train_time:11069ms step_avg:93.02ms
step:120/1750 train_time:11163ms step_avg:93.02ms
step:121/1750 train_time:11255ms step_avg:93.02ms
step:122/1750 train_time:11349ms step_avg:93.02ms
step:123/1750 train_time:11442ms step_avg:93.02ms
step:124/1750 train_time:11535ms step_avg:93.02ms
step:125/1750 train_time:11627ms step_avg:93.02ms
step:125/1750 val_loss:4.6320 train_time:11715ms step_avg:93.72ms
step:126/1750 train_time:11746ms step_avg:93.22ms
step:127/1750 train_time:11821ms step_avg:93.08ms
step:128/1750 train_time:11920ms step_avg:93.13ms
step:129/1750 train_time:12015ms step_avg:93.14ms
step:130/1750 train_time:12109ms step_avg:93.15ms
step:131/1750 train_time:12202ms step_avg:93.14ms
step:132/1750 train_time:12294ms step_avg:93.14ms
step:133/1750 train_time:12387ms step_avg:93.14ms
step:134/1750 train_time:12480ms step_avg:93.14ms
step:135/1750 train_time:12573ms step_avg:93.14ms
step:136/1750 train_time:12666ms step_avg:93.13ms
step:137/1750 train_time:12759ms step_avg:93.13ms
step:138/1750 train_time:12856ms step_avg:93.16ms
step:139/1750 train_time:12952ms step_avg:93.18ms
step:140/1750 train_time:13047ms step_avg:93.19ms
step:141/1750 train_time:13141ms step_avg:93.20ms
step:142/1750 train_time:13235ms step_avg:93.20ms
step:143/1750 train_time:13328ms step_avg:93.20ms
step:144/1750 train_time:13421ms step_avg:93.20ms
step:145/1750 train_time:13514ms step_avg:93.20ms
step:146/1750 train_time:13608ms step_avg:93.21ms
step:147/1750 train_time:13702ms step_avg:93.21ms
step:148/1750 train_time:13796ms step_avg:93.22ms
step:149/1750 train_time:13890ms step_avg:93.22ms
step:150/1750 train_time:13985ms step_avg:93.23ms
step:151/1750 train_time:14079ms step_avg:93.24ms
step:152/1750 train_time:14174ms step_avg:93.25ms
step:153/1750 train_time:14267ms step_avg:93.25ms
step:154/1750 train_time:14361ms step_avg:93.25ms
step:155/1750 train_time:14454ms step_avg:93.25ms
step:156/1750 train_time:14548ms step_avg:93.25ms
step:157/1750 train_time:14642ms step_avg:93.26ms
step:158/1750 train_time:14735ms step_avg:93.26ms
step:159/1750 train_time:14829ms step_avg:93.26ms
step:160/1750 train_time:14923ms step_avg:93.27ms
step:161/1750 train_time:15018ms step_avg:93.28ms
step:162/1750 train_time:15112ms step_avg:93.28ms
step:163/1750 train_time:15206ms step_avg:93.29ms
step:164/1750 train_time:15299ms step_avg:93.29ms
step:165/1750 train_time:15393ms step_avg:93.29ms
step:166/1750 train_time:15486ms step_avg:93.29ms
step:167/1750 train_time:15580ms step_avg:93.29ms
step:168/1750 train_time:15674ms step_avg:93.29ms
step:169/1750 train_time:15767ms step_avg:93.30ms
step:170/1750 train_time:15861ms step_avg:93.30ms
step:171/1750 train_time:15955ms step_avg:93.31ms
step:172/1750 train_time:16049ms step_avg:93.31ms
step:173/1750 train_time:16143ms step_avg:93.31ms
step:174/1750 train_time:16237ms step_avg:93.32ms
step:175/1750 train_time:16331ms step_avg:93.32ms
step:176/1750 train_time:16425ms step_avg:93.32ms
step:177/1750 train_time:16518ms step_avg:93.32ms
step:178/1750 train_time:16612ms step_avg:93.33ms
step:179/1750 train_time:16705ms step_avg:93.32ms
step:180/1750 train_time:16799ms step_avg:93.33ms
step:181/1750 train_time:16893ms step_avg:93.33ms
step:182/1750 train_time:16987ms step_avg:93.34ms
step:183/1750 train_time:17081ms step_avg:93.34ms
step:184/1750 train_time:17175ms step_avg:93.34ms
step:185/1750 train_time:17269ms step_avg:93.34ms
step:186/1750 train_time:17363ms step_avg:93.35ms
step:187/1750 train_time:17456ms step_avg:93.35ms
step:188/1750 train_time:17550ms step_avg:93.35ms
step:189/1750 train_time:17644ms step_avg:93.35ms
step:190/1750 train_time:17738ms step_avg:93.36ms
step:191/1750 train_time:17832ms step_avg:93.36ms
step:192/1750 train_time:17926ms step_avg:93.36ms
step:193/1750 train_time:18020ms step_avg:93.37ms
step:194/1750 train_time:18113ms step_avg:93.37ms
step:195/1750 train_time:18206ms step_avg:93.37ms
step:196/1750 train_time:18300ms step_avg:93.37ms
step:197/1750 train_time:18394ms step_avg:93.37ms
step:198/1750 train_time:18488ms step_avg:93.38ms
step:199/1750 train_time:18582ms step_avg:93.38ms
step:200/1750 train_time:18676ms step_avg:93.38ms
step:201/1750 train_time:18770ms step_avg:93.38ms
step:202/1750 train_time:18864ms step_avg:93.38ms
step:203/1750 train_time:18958ms step_avg:93.39ms
step:204/1750 train_time:19053ms step_avg:93.40ms
step:205/1750 train_time:19146ms step_avg:93.40ms
step:206/1750 train_time:19241ms step_avg:93.40ms
step:207/1750 train_time:19334ms step_avg:93.40ms
step:208/1750 train_time:19429ms step_avg:93.41ms
step:209/1750 train_time:19522ms step_avg:93.41ms
step:210/1750 train_time:19616ms step_avg:93.41ms
step:211/1750 train_time:19710ms step_avg:93.41ms
step:212/1750 train_time:19804ms step_avg:93.41ms
step:213/1750 train_time:19898ms step_avg:93.42ms
step:214/1750 train_time:19991ms step_avg:93.42ms
step:215/1750 train_time:20085ms step_avg:93.42ms
step:216/1750 train_time:20179ms step_avg:93.42ms
step:217/1750 train_time:20272ms step_avg:93.42ms
step:218/1750 train_time:20366ms step_avg:93.42ms
step:219/1750 train_time:20460ms step_avg:93.42ms
step:220/1750 train_time:20554ms step_avg:93.43ms
step:221/1750 train_time:20648ms step_avg:93.43ms
step:222/1750 train_time:20742ms step_avg:93.43ms
step:223/1750 train_time:20836ms step_avg:93.44ms
step:224/1750 train_time:20930ms step_avg:93.44ms
step:225/1750 train_time:21025ms step_avg:93.44ms
step:226/1750 train_time:21119ms step_avg:93.45ms
step:227/1750 train_time:21214ms step_avg:93.45ms
step:228/1750 train_time:21308ms step_avg:93.46ms
step:229/1750 train_time:21401ms step_avg:93.46ms
step:230/1750 train_time:21494ms step_avg:93.45ms
step:231/1750 train_time:21588ms step_avg:93.45ms
step:232/1750 train_time:21682ms step_avg:93.46ms
step:233/1750 train_time:21776ms step_avg:93.46ms
step:234/1750 train_time:21870ms step_avg:93.46ms
step:235/1750 train_time:21964ms step_avg:93.46ms
step:236/1750 train_time:22057ms step_avg:93.46ms
step:237/1750 train_time:22152ms step_avg:93.47ms
step:238/1750 train_time:22246ms step_avg:93.47ms
step:239/1750 train_time:22340ms step_avg:93.47ms
step:240/1750 train_time:22433ms step_avg:93.47ms
step:241/1750 train_time:22528ms step_avg:93.48ms
step:242/1750 train_time:22621ms step_avg:93.47ms
step:243/1750 train_time:22715ms step_avg:93.48ms
step:244/1750 train_time:22809ms step_avg:93.48ms
step:245/1750 train_time:22903ms step_avg:93.48ms
step:246/1750 train_time:22997ms step_avg:93.48ms
step:247/1750 train_time:23091ms step_avg:93.48ms
step:248/1750 train_time:23185ms step_avg:93.49ms
step:249/1750 train_time:23279ms step_avg:93.49ms
step:250/1750 train_time:23372ms step_avg:93.49ms
step:250/1750 val_loss:4.0988 train_time:23461ms step_avg:93.85ms
step:251/1750 train_time:23491ms step_avg:93.59ms
step:252/1750 train_time:23569ms step_avg:93.53ms
step:253/1750 train_time:23669ms step_avg:93.55ms
step:254/1750 train_time:23763ms step_avg:93.56ms
step:255/1750 train_time:23857ms step_avg:93.56ms
step:256/1750 train_time:23950ms step_avg:93.55ms
step:257/1750 train_time:24043ms step_avg:93.55ms
step:258/1750 train_time:24136ms step_avg:93.55ms
step:259/1750 train_time:24229ms step_avg:93.55ms
step:260/1750 train_time:24322ms step_avg:93.55ms
step:261/1750 train_time:24416ms step_avg:93.55ms
step:262/1750 train_time:24510ms step_avg:93.55ms
step:263/1750 train_time:24606ms step_avg:93.56ms
step:264/1750 train_time:24701ms step_avg:93.57ms
step:265/1750 train_time:24796ms step_avg:93.57ms
step:266/1750 train_time:24891ms step_avg:93.58ms
step:267/1750 train_time:24985ms step_avg:93.58ms
step:268/1750 train_time:25080ms step_avg:93.58ms
step:269/1750 train_time:25174ms step_avg:93.58ms
step:270/1750 train_time:25268ms step_avg:93.59ms
step:271/1750 train_time:25362ms step_avg:93.59ms
step:272/1750 train_time:25457ms step_avg:93.59ms
step:273/1750 train_time:25552ms step_avg:93.60ms
step:274/1750 train_time:25647ms step_avg:93.60ms
step:275/1750 train_time:25741ms step_avg:93.61ms
step:276/1750 train_time:25837ms step_avg:93.61ms
step:277/1750 train_time:25931ms step_avg:93.61ms
step:278/1750 train_time:26025ms step_avg:93.62ms
step:279/1750 train_time:26119ms step_avg:93.62ms
step:280/1750 train_time:26213ms step_avg:93.62ms
step:281/1750 train_time:26308ms step_avg:93.62ms
step:282/1750 train_time:26402ms step_avg:93.62ms
step:283/1750 train_time:26496ms step_avg:93.63ms
step:284/1750 train_time:26592ms step_avg:93.63ms
step:285/1750 train_time:26686ms step_avg:93.64ms
step:286/1750 train_time:26781ms step_avg:93.64ms
step:287/1750 train_time:26876ms step_avg:93.64ms
step:288/1750 train_time:26971ms step_avg:93.65ms
step:289/1750 train_time:27066ms step_avg:93.65ms
step:290/1750 train_time:27160ms step_avg:93.65ms
step:291/1750 train_time:27254ms step_avg:93.66ms
step:292/1750 train_time:27349ms step_avg:93.66ms
step:293/1750 train_time:27443ms step_avg:93.66ms
step:294/1750 train_time:27537ms step_avg:93.66ms
step:295/1750 train_time:27632ms step_avg:93.67ms
step:296/1750 train_time:27726ms step_avg:93.67ms
step:297/1750 train_time:27820ms step_avg:93.67ms
step:298/1750 train_time:27915ms step_avg:93.68ms
step:299/1750 train_time:28010ms step_avg:93.68ms
step:300/1750 train_time:28104ms step_avg:93.68ms
step:301/1750 train_time:28198ms step_avg:93.68ms
step:302/1750 train_time:28292ms step_avg:93.68ms
step:303/1750 train_time:28387ms step_avg:93.69ms
step:304/1750 train_time:28481ms step_avg:93.69ms
step:305/1750 train_time:28576ms step_avg:93.69ms
step:306/1750 train_time:28670ms step_avg:93.69ms
step:307/1750 train_time:28765ms step_avg:93.70ms
step:308/1750 train_time:28860ms step_avg:93.70ms
step:309/1750 train_time:28954ms step_avg:93.70ms
step:310/1750 train_time:29049ms step_avg:93.71ms
step:311/1750 train_time:29144ms step_avg:93.71ms
step:312/1750 train_time:29238ms step_avg:93.71ms
step:313/1750 train_time:29332ms step_avg:93.71ms
step:314/1750 train_time:29427ms step_avg:93.72ms
step:315/1750 train_time:29522ms step_avg:93.72ms
step:316/1750 train_time:29616ms step_avg:93.72ms
step:317/1750 train_time:29711ms step_avg:93.72ms
step:318/1750 train_time:29805ms step_avg:93.73ms
step:319/1750 train_time:29899ms step_avg:93.73ms
step:320/1750 train_time:29993ms step_avg:93.73ms
step:321/1750 train_time:30088ms step_avg:93.73ms
step:322/1750 train_time:30182ms step_avg:93.73ms
step:323/1750 train_time:30278ms step_avg:93.74ms
step:324/1750 train_time:30372ms step_avg:93.74ms
step:325/1750 train_time:30467ms step_avg:93.74ms
step:326/1750 train_time:30561ms step_avg:93.75ms
step:327/1750 train_time:30656ms step_avg:93.75ms
step:328/1750 train_time:30751ms step_avg:93.75ms
step:329/1750 train_time:30845ms step_avg:93.76ms
step:330/1750 train_time:30940ms step_avg:93.76ms
step:331/1750 train_time:31034ms step_avg:93.76ms
step:332/1750 train_time:31129ms step_avg:93.76ms
step:333/1750 train_time:31223ms step_avg:93.76ms
step:334/1750 train_time:31317ms step_avg:93.76ms
step:335/1750 train_time:31412ms step_avg:93.77ms
step:336/1750 train_time:31506ms step_avg:93.77ms
step:337/1750 train_time:31600ms step_avg:93.77ms
step:338/1750 train_time:31694ms step_avg:93.77ms
step:339/1750 train_time:31789ms step_avg:93.77ms
step:340/1750 train_time:31883ms step_avg:93.77ms
step:341/1750 train_time:31978ms step_avg:93.78ms
step:342/1750 train_time:32072ms step_avg:93.78ms
step:343/1750 train_time:32168ms step_avg:93.78ms
step:344/1750 train_time:32263ms step_avg:93.79ms
step:345/1750 train_time:32356ms step_avg:93.79ms
step:346/1750 train_time:32450ms step_avg:93.79ms
step:347/1750 train_time:32545ms step_avg:93.79ms
step:348/1750 train_time:32639ms step_avg:93.79ms
step:349/1750 train_time:32733ms step_avg:93.79ms
step:350/1750 train_time:32829ms step_avg:93.80ms
step:351/1750 train_time:32923ms step_avg:93.80ms
step:352/1750 train_time:33017ms step_avg:93.80ms
step:353/1750 train_time:33111ms step_avg:93.80ms
step:354/1750 train_time:33205ms step_avg:93.80ms
step:355/1750 train_time:33299ms step_avg:93.80ms
step:356/1750 train_time:33394ms step_avg:93.80ms
step:357/1750 train_time:33489ms step_avg:93.81ms
step:358/1750 train_time:33583ms step_avg:93.81ms
step:359/1750 train_time:33677ms step_avg:93.81ms
step:360/1750 train_time:33771ms step_avg:93.81ms
step:361/1750 train_time:33866ms step_avg:93.81ms
step:362/1750 train_time:33961ms step_avg:93.81ms
step:363/1750 train_time:34055ms step_avg:93.81ms
step:364/1750 train_time:34150ms step_avg:93.82ms
step:365/1750 train_time:34244ms step_avg:93.82ms
step:366/1750 train_time:34338ms step_avg:93.82ms
step:367/1750 train_time:34433ms step_avg:93.82ms
step:368/1750 train_time:34527ms step_avg:93.82ms
step:369/1750 train_time:34622ms step_avg:93.83ms
step:370/1750 train_time:34716ms step_avg:93.83ms
step:371/1750 train_time:34811ms step_avg:93.83ms
step:372/1750 train_time:34905ms step_avg:93.83ms
step:373/1750 train_time:34999ms step_avg:93.83ms
step:374/1750 train_time:35094ms step_avg:93.83ms
step:375/1750 train_time:35189ms step_avg:93.84ms
step:375/1750 val_loss:3.8947 train_time:35278ms step_avg:94.07ms
step:376/1750 train_time:35305ms step_avg:93.90ms
step:377/1750 train_time:35384ms step_avg:93.86ms
step:378/1750 train_time:35482ms step_avg:93.87ms
step:379/1750 train_time:35579ms step_avg:93.88ms
step:380/1750 train_time:35673ms step_avg:93.88ms
step:381/1750 train_time:35767ms step_avg:93.88ms
step:382/1750 train_time:35861ms step_avg:93.88ms
step:383/1750 train_time:35954ms step_avg:93.88ms
step:384/1750 train_time:36048ms step_avg:93.88ms
step:385/1750 train_time:36142ms step_avg:93.88ms
step:386/1750 train_time:36236ms step_avg:93.88ms
step:387/1750 train_time:36331ms step_avg:93.88ms
step:388/1750 train_time:36427ms step_avg:93.88ms
step:389/1750 train_time:36524ms step_avg:93.89ms
step:390/1750 train_time:36619ms step_avg:93.89ms
step:391/1750 train_time:36715ms step_avg:93.90ms
step:392/1750 train_time:36812ms step_avg:93.91ms
step:393/1750 train_time:36908ms step_avg:93.91ms
step:394/1750 train_time:37004ms step_avg:93.92ms
step:395/1750 train_time:37100ms step_avg:93.92ms
step:396/1750 train_time:37196ms step_avg:93.93ms
step:397/1750 train_time:37292ms step_avg:93.93ms
step:398/1750 train_time:37388ms step_avg:93.94ms
step:399/1750 train_time:37485ms step_avg:93.95ms
step:400/1750 train_time:37582ms step_avg:93.95ms
step:401/1750 train_time:37678ms step_avg:93.96ms
step:402/1750 train_time:37774ms step_avg:93.96ms
step:403/1750 train_time:37870ms step_avg:93.97ms
step:404/1750 train_time:37966ms step_avg:93.98ms
step:405/1750 train_time:38062ms step_avg:93.98ms
step:406/1750 train_time:38159ms step_avg:93.99ms
step:407/1750 train_time:38255ms step_avg:93.99ms
step:408/1750 train_time:38352ms step_avg:94.00ms
step:409/1750 train_time:38449ms step_avg:94.01ms
step:410/1750 train_time:38544ms step_avg:94.01ms
step:411/1750 train_time:38641ms step_avg:94.02ms
step:412/1750 train_time:38737ms step_avg:94.02ms
step:413/1750 train_time:38833ms step_avg:94.03ms
step:414/1750 train_time:38930ms step_avg:94.03ms
step:415/1750 train_time:39027ms step_avg:94.04ms
step:416/1750 train_time:39124ms step_avg:94.05ms
step:417/1750 train_time:39221ms step_avg:94.06ms
step:418/1750 train_time:39318ms step_avg:94.06ms
step:419/1750 train_time:39414ms step_avg:94.07ms
step:420/1750 train_time:39511ms step_avg:94.07ms
step:421/1750 train_time:39608ms step_avg:94.08ms
step:422/1750 train_time:39705ms step_avg:94.09ms
step:423/1750 train_time:39800ms step_avg:94.09ms
step:424/1750 train_time:39897ms step_avg:94.10ms
step:425/1750 train_time:39993ms step_avg:94.10ms
step:426/1750 train_time:40090ms step_avg:94.11ms
step:427/1750 train_time:40187ms step_avg:94.12ms
step:428/1750 train_time:40284ms step_avg:94.12ms
step:429/1750 train_time:40381ms step_avg:94.13ms
step:430/1750 train_time:40477ms step_avg:94.13ms
step:431/1750 train_time:40574ms step_avg:94.14ms
step:432/1750 train_time:40670ms step_avg:94.14ms
step:433/1750 train_time:40767ms step_avg:94.15ms
step:434/1750 train_time:40864ms step_avg:94.16ms
step:435/1750 train_time:40959ms step_avg:94.16ms
step:436/1750 train_time:41056ms step_avg:94.17ms
step:437/1750 train_time:41153ms step_avg:94.17ms
step:438/1750 train_time:41250ms step_avg:94.18ms
step:439/1750 train_time:41347ms step_avg:94.18ms
step:440/1750 train_time:41444ms step_avg:94.19ms
step:441/1750 train_time:41540ms step_avg:94.19ms
step:442/1750 train_time:41636ms step_avg:94.20ms
step:443/1750 train_time:41732ms step_avg:94.20ms
step:444/1750 train_time:41829ms step_avg:94.21ms
step:445/1750 train_time:41925ms step_avg:94.21ms
step:446/1750 train_time:42022ms step_avg:94.22ms
step:447/1750 train_time:42118ms step_avg:94.22ms
step:448/1750 train_time:42215ms step_avg:94.23ms
step:449/1750 train_time:42312ms step_avg:94.24ms
step:450/1750 train_time:42409ms step_avg:94.24ms
step:451/1750 train_time:42505ms step_avg:94.25ms
step:452/1750 train_time:42602ms step_avg:94.25ms
step:453/1750 train_time:42698ms step_avg:94.26ms
step:454/1750 train_time:42795ms step_avg:94.26ms
step:455/1750 train_time:42890ms step_avg:94.26ms
step:456/1750 train_time:42988ms step_avg:94.27ms
step:457/1750 train_time:43084ms step_avg:94.28ms
step:458/1750 train_time:43180ms step_avg:94.28ms
step:459/1750 train_time:43277ms step_avg:94.29ms
step:460/1750 train_time:43373ms step_avg:94.29ms
step:461/1750 train_time:43470ms step_avg:94.30ms
step:462/1750 train_time:43567ms step_avg:94.30ms
step:463/1750 train_time:43663ms step_avg:94.31ms
step:464/1750 train_time:43760ms step_avg:94.31ms
step:465/1750 train_time:43856ms step_avg:94.31ms
step:466/1750 train_time:43953ms step_avg:94.32ms
step:467/1750 train_time:44049ms step_avg:94.32ms
step:468/1750 train_time:44146ms step_avg:94.33ms
step:469/1750 train_time:44243ms step_avg:94.33ms
step:470/1750 train_time:44339ms step_avg:94.34ms
step:471/1750 train_time:44436ms step_avg:94.34ms
step:472/1750 train_time:44533ms step_avg:94.35ms
step:473/1750 train_time:44630ms step_avg:94.36ms
step:474/1750 train_time:44726ms step_avg:94.36ms
step:475/1750 train_time:44823ms step_avg:94.36ms
step:476/1750 train_time:44919ms step_avg:94.37ms
step:477/1750 train_time:45015ms step_avg:94.37ms
step:478/1750 train_time:45112ms step_avg:94.38ms
step:479/1750 train_time:45208ms step_avg:94.38ms
step:480/1750 train_time:45305ms step_avg:94.39ms
step:481/1750 train_time:45402ms step_avg:94.39ms
step:482/1750 train_time:45499ms step_avg:94.40ms
step:483/1750 train_time:45597ms step_avg:94.40ms
step:484/1750 train_time:45694ms step_avg:94.41ms
step:485/1750 train_time:45790ms step_avg:94.41ms
step:486/1750 train_time:45888ms step_avg:94.42ms
step:487/1750 train_time:45985ms step_avg:94.42ms
step:488/1750 train_time:46081ms step_avg:94.43ms
step:489/1750 train_time:46178ms step_avg:94.43ms
step:490/1750 train_time:46275ms step_avg:94.44ms
step:491/1750 train_time:46371ms step_avg:94.44ms
step:492/1750 train_time:46468ms step_avg:94.45ms
step:493/1750 train_time:46564ms step_avg:94.45ms
step:494/1750 train_time:46660ms step_avg:94.45ms
step:495/1750 train_time:46757ms step_avg:94.46ms
step:496/1750 train_time:46854ms step_avg:94.46ms
step:497/1750 train_time:46950ms step_avg:94.47ms
step:498/1750 train_time:47047ms step_avg:94.47ms
step:499/1750 train_time:47143ms step_avg:94.47ms
step:500/1750 train_time:47239ms step_avg:94.48ms
step:500/1750 val_loss:3.7455 train_time:47331ms step_avg:94.66ms
step:501/1750 train_time:47358ms step_avg:94.53ms
step:502/1750 train_time:47439ms step_avg:94.50ms
step:503/1750 train_time:47540ms step_avg:94.51ms
step:504/1750 train_time:47638ms step_avg:94.52ms
step:505/1750 train_time:47735ms step_avg:94.52ms
step:506/1750 train_time:47831ms step_avg:94.53ms
step:507/1750 train_time:47927ms step_avg:94.53ms
step:508/1750 train_time:48022ms step_avg:94.53ms
step:509/1750 train_time:48118ms step_avg:94.53ms
step:510/1750 train_time:48213ms step_avg:94.54ms
step:511/1750 train_time:48309ms step_avg:94.54ms
step:512/1750 train_time:48406ms step_avg:94.54ms
step:513/1750 train_time:48503ms step_avg:94.55ms
step:514/1750 train_time:48601ms step_avg:94.55ms
step:515/1750 train_time:48698ms step_avg:94.56ms
step:516/1750 train_time:48795ms step_avg:94.56ms
step:517/1750 train_time:48891ms step_avg:94.57ms
step:518/1750 train_time:48987ms step_avg:94.57ms
step:519/1750 train_time:49083ms step_avg:94.57ms
step:520/1750 train_time:49179ms step_avg:94.57ms
step:521/1750 train_time:49275ms step_avg:94.58ms
step:522/1750 train_time:49372ms step_avg:94.58ms
step:523/1750 train_time:49469ms step_avg:94.59ms
step:524/1750 train_time:49567ms step_avg:94.59ms
step:525/1750 train_time:49664ms step_avg:94.60ms
step:526/1750 train_time:49762ms step_avg:94.60ms
step:527/1750 train_time:49859ms step_avg:94.61ms
step:528/1750 train_time:49955ms step_avg:94.61ms
step:529/1750 train_time:50052ms step_avg:94.62ms
step:530/1750 train_time:50148ms step_avg:94.62ms
step:531/1750 train_time:50244ms step_avg:94.62ms
step:532/1750 train_time:50340ms step_avg:94.62ms
step:533/1750 train_time:50437ms step_avg:94.63ms
step:534/1750 train_time:50534ms step_avg:94.63ms
step:535/1750 train_time:50631ms step_avg:94.64ms
step:536/1750 train_time:50729ms step_avg:94.64ms
step:537/1750 train_time:50826ms step_avg:94.65ms
step:538/1750 train_time:50922ms step_avg:94.65ms
step:539/1750 train_time:51019ms step_avg:94.66ms
step:540/1750 train_time:51115ms step_avg:94.66ms
step:541/1750 train_time:51212ms step_avg:94.66ms
step:542/1750 train_time:51309ms step_avg:94.67ms
step:543/1750 train_time:51405ms step_avg:94.67ms
step:544/1750 train_time:51502ms step_avg:94.67ms
step:545/1750 train_time:51600ms step_avg:94.68ms
step:546/1750 train_time:51697ms step_avg:94.68ms
step:547/1750 train_time:51794ms step_avg:94.69ms
step:548/1750 train_time:51892ms step_avg:94.69ms
step:549/1750 train_time:51989ms step_avg:94.70ms
step:550/1750 train_time:52086ms step_avg:94.70ms
step:551/1750 train_time:52183ms step_avg:94.71ms
step:552/1750 train_time:52280ms step_avg:94.71ms
step:553/1750 train_time:52376ms step_avg:94.71ms
step:554/1750 train_time:52473ms step_avg:94.72ms
step:555/1750 train_time:52569ms step_avg:94.72ms
step:556/1750 train_time:52667ms step_avg:94.72ms
step:557/1750 train_time:52764ms step_avg:94.73ms
step:558/1750 train_time:52861ms step_avg:94.73ms
step:559/1750 train_time:52959ms step_avg:94.74ms
step:560/1750 train_time:53055ms step_avg:94.74ms
step:561/1750 train_time:53152ms step_avg:94.75ms
step:562/1750 train_time:53249ms step_avg:94.75ms
step:563/1750 train_time:53346ms step_avg:94.75ms
step:564/1750 train_time:53442ms step_avg:94.76ms
step:565/1750 train_time:53539ms step_avg:94.76ms
step:566/1750 train_time:53635ms step_avg:94.76ms
step:567/1750 train_time:53732ms step_avg:94.77ms
step:568/1750 train_time:53829ms step_avg:94.77ms
step:569/1750 train_time:53926ms step_avg:94.77ms
step:570/1750 train_time:54023ms step_avg:94.78ms
step:571/1750 train_time:54119ms step_avg:94.78ms
step:572/1750 train_time:54217ms step_avg:94.78ms
step:573/1750 train_time:54313ms step_avg:94.79ms
step:574/1750 train_time:54409ms step_avg:94.79ms
step:575/1750 train_time:54506ms step_avg:94.79ms
step:576/1750 train_time:54602ms step_avg:94.80ms
step:577/1750 train_time:54699ms step_avg:94.80ms
step:578/1750 train_time:54797ms step_avg:94.80ms
step:579/1750 train_time:54894ms step_avg:94.81ms
step:580/1750 train_time:54991ms step_avg:94.81ms
step:581/1750 train_time:55089ms step_avg:94.82ms
step:582/1750 train_time:55186ms step_avg:94.82ms
step:583/1750 train_time:55283ms step_avg:94.82ms
step:584/1750 train_time:55380ms step_avg:94.83ms
step:585/1750 train_time:55476ms step_avg:94.83ms
step:586/1750 train_time:55573ms step_avg:94.83ms
step:587/1750 train_time:55669ms step_avg:94.84ms
step:588/1750 train_time:55765ms step_avg:94.84ms
step:589/1750 train_time:55863ms step_avg:94.84ms
step:590/1750 train_time:55960ms step_avg:94.85ms
step:591/1750 train_time:56058ms step_avg:94.85ms
step:592/1750 train_time:56155ms step_avg:94.86ms
step:593/1750 train_time:56252ms step_avg:94.86ms
step:594/1750 train_time:56349ms step_avg:94.86ms
step:595/1750 train_time:56446ms step_avg:94.87ms
step:596/1750 train_time:56543ms step_avg:94.87ms
step:597/1750 train_time:56640ms step_avg:94.87ms
step:598/1750 train_time:56737ms step_avg:94.88ms
step:599/1750 train_time:56833ms step_avg:94.88ms
step:600/1750 train_time:56930ms step_avg:94.88ms
step:601/1750 train_time:57027ms step_avg:94.89ms
step:602/1750 train_time:57123ms step_avg:94.89ms
step:603/1750 train_time:57219ms step_avg:94.89ms
step:604/1750 train_time:57317ms step_avg:94.89ms
step:605/1750 train_time:57413ms step_avg:94.90ms
step:606/1750 train_time:57511ms step_avg:94.90ms
step:607/1750 train_time:57608ms step_avg:94.91ms
step:608/1750 train_time:57704ms step_avg:94.91ms
step:609/1750 train_time:57801ms step_avg:94.91ms
step:610/1750 train_time:57897ms step_avg:94.91ms
step:611/1750 train_time:57994ms step_avg:94.92ms
step:612/1750 train_time:58092ms step_avg:94.92ms
step:613/1750 train_time:58189ms step_avg:94.92ms
step:614/1750 train_time:58285ms step_avg:94.93ms
step:615/1750 train_time:58383ms step_avg:94.93ms
step:616/1750 train_time:58479ms step_avg:94.93ms
step:617/1750 train_time:58576ms step_avg:94.94ms
step:618/1750 train_time:58673ms step_avg:94.94ms
step:619/1750 train_time:58771ms step_avg:94.94ms
step:620/1750 train_time:58867ms step_avg:94.95ms
step:621/1750 train_time:58964ms step_avg:94.95ms
step:622/1750 train_time:59061ms step_avg:94.95ms
step:623/1750 train_time:59158ms step_avg:94.96ms
step:624/1750 train_time:59255ms step_avg:94.96ms
step:625/1750 train_time:59352ms step_avg:94.96ms
step:625/1750 val_loss:3.6583 train_time:59443ms step_avg:95.11ms
step:626/1750 train_time:59472ms step_avg:95.00ms
step:627/1750 train_time:59556ms step_avg:94.98ms
step:628/1750 train_time:59655ms step_avg:94.99ms
step:629/1750 train_time:59752ms step_avg:95.00ms
step:630/1750 train_time:59848ms step_avg:95.00ms
step:631/1750 train_time:59945ms step_avg:95.00ms
step:632/1750 train_time:60041ms step_avg:95.00ms
step:633/1750 train_time:60136ms step_avg:95.00ms
step:634/1750 train_time:60232ms step_avg:95.00ms
step:635/1750 train_time:60328ms step_avg:95.00ms
step:636/1750 train_time:60425ms step_avg:95.01ms
step:637/1750 train_time:60522ms step_avg:95.01ms
step:638/1750 train_time:60621ms step_avg:95.02ms
step:639/1750 train_time:60718ms step_avg:95.02ms
step:640/1750 train_time:60815ms step_avg:95.02ms
step:641/1750 train_time:60913ms step_avg:95.03ms
step:642/1750 train_time:61010ms step_avg:95.03ms
step:643/1750 train_time:61106ms step_avg:95.03ms
step:644/1750 train_time:61204ms step_avg:95.04ms
step:645/1750 train_time:61300ms step_avg:95.04ms
step:646/1750 train_time:61397ms step_avg:95.04ms
step:647/1750 train_time:61494ms step_avg:95.04ms
step:648/1750 train_time:61591ms step_avg:95.05ms
step:649/1750 train_time:61689ms step_avg:95.05ms
step:650/1750 train_time:61785ms step_avg:95.05ms
step:651/1750 train_time:61884ms step_avg:95.06ms
step:652/1750 train_time:61982ms step_avg:95.06ms
step:653/1750 train_time:62080ms step_avg:95.07ms
step:654/1750 train_time:62179ms step_avg:95.07ms
step:655/1750 train_time:62277ms step_avg:95.08ms
step:656/1750 train_time:62376ms step_avg:95.08ms
step:657/1750 train_time:62475ms step_avg:95.09ms
step:658/1750 train_time:62573ms step_avg:95.10ms
step:659/1750 train_time:62672ms step_avg:95.10ms
step:660/1750 train_time:62771ms step_avg:95.11ms
step:661/1750 train_time:62870ms step_avg:95.11ms
step:662/1750 train_time:62970ms step_avg:95.12ms
step:663/1750 train_time:63068ms step_avg:95.13ms
step:664/1750 train_time:63167ms step_avg:95.13ms
step:665/1750 train_time:63265ms step_avg:95.14ms
step:666/1750 train_time:63363ms step_avg:95.14ms
step:667/1750 train_time:63462ms step_avg:95.15ms
step:668/1750 train_time:63562ms step_avg:95.15ms
step:669/1750 train_time:63660ms step_avg:95.16ms
step:670/1750 train_time:63759ms step_avg:95.16ms
step:671/1750 train_time:63857ms step_avg:95.17ms
step:672/1750 train_time:63957ms step_avg:95.17ms
step:673/1750 train_time:64057ms step_avg:95.18ms
step:674/1750 train_time:64157ms step_avg:95.19ms
step:675/1750 train_time:64255ms step_avg:95.19ms
step:676/1750 train_time:64354ms step_avg:95.20ms
step:677/1750 train_time:64454ms step_avg:95.21ms
step:678/1750 train_time:64553ms step_avg:95.21ms
step:679/1750 train_time:64652ms step_avg:95.22ms
step:680/1750 train_time:64750ms step_avg:95.22ms
step:681/1750 train_time:64848ms step_avg:95.22ms
step:682/1750 train_time:64946ms step_avg:95.23ms
step:683/1750 train_time:65045ms step_avg:95.23ms
step:684/1750 train_time:65145ms step_avg:95.24ms
step:685/1750 train_time:65244ms step_avg:95.25ms
step:686/1750 train_time:65343ms step_avg:95.25ms
step:687/1750 train_time:65442ms step_avg:95.26ms
step:688/1750 train_time:65540ms step_avg:95.26ms
step:689/1750 train_time:65640ms step_avg:95.27ms
step:690/1750 train_time:65738ms step_avg:95.27ms
step:691/1750 train_time:65837ms step_avg:95.28ms
step:692/1750 train_time:65935ms step_avg:95.28ms
step:693/1750 train_time:66036ms step_avg:95.29ms
step:694/1750 train_time:66135ms step_avg:95.30ms
step:695/1750 train_time:66234ms step_avg:95.30ms
step:696/1750 train_time:66334ms step_avg:95.31ms
step:697/1750 train_time:66433ms step_avg:95.31ms
step:698/1750 train_time:66531ms step_avg:95.32ms
step:699/1750 train_time:66630ms step_avg:95.32ms
step:700/1750 train_time:66728ms step_avg:95.33ms
step:701/1750 train_time:66826ms step_avg:95.33ms
step:702/1750 train_time:66925ms step_avg:95.33ms
step:703/1750 train_time:67024ms step_avg:95.34ms
step:704/1750 train_time:67123ms step_avg:95.34ms
step:705/1750 train_time:67222ms step_avg:95.35ms
step:706/1750 train_time:67321ms step_avg:95.36ms
step:707/1750 train_time:67420ms step_avg:95.36ms
step:708/1750 train_time:67519ms step_avg:95.37ms
step:709/1750 train_time:67618ms step_avg:95.37ms
step:710/1750 train_time:67717ms step_avg:95.38ms
step:711/1750 train_time:67817ms step_avg:95.38ms
step:712/1750 train_time:67916ms step_avg:95.39ms
step:713/1750 train_time:68015ms step_avg:95.39ms
step:714/1750 train_time:68115ms step_avg:95.40ms
step:715/1750 train_time:68214ms step_avg:95.40ms
step:716/1750 train_time:68312ms step_avg:95.41ms
step:717/1750 train_time:68410ms step_avg:95.41ms
step:718/1750 train_time:68509ms step_avg:95.42ms
step:719/1750 train_time:68607ms step_avg:95.42ms
step:720/1750 train_time:68705ms step_avg:95.42ms
step:721/1750 train_time:68804ms step_avg:95.43ms
step:722/1750 train_time:68903ms step_avg:95.43ms
step:723/1750 train_time:69002ms step_avg:95.44ms
step:724/1750 train_time:69100ms step_avg:95.44ms
step:725/1750 train_time:69199ms step_avg:95.45ms
step:726/1750 train_time:69297ms step_avg:95.45ms
step:727/1750 train_time:69396ms step_avg:95.46ms
step:728/1750 train_time:69495ms step_avg:95.46ms
step:729/1750 train_time:69594ms step_avg:95.46ms
step:730/1750 train_time:69692ms step_avg:95.47ms
step:731/1750 train_time:69790ms step_avg:95.47ms
step:732/1750 train_time:69889ms step_avg:95.48ms
step:733/1750 train_time:69987ms step_avg:95.48ms
step:734/1750 train_time:70086ms step_avg:95.48ms
step:735/1750 train_time:70184ms step_avg:95.49ms
step:736/1750 train_time:70283ms step_avg:95.49ms
step:737/1750 train_time:70382ms step_avg:95.50ms
step:738/1750 train_time:70482ms step_avg:95.50ms
step:739/1750 train_time:70581ms step_avg:95.51ms
step:740/1750 train_time:70681ms step_avg:95.51ms
step:741/1750 train_time:70780ms step_avg:95.52ms
step:742/1750 train_time:70878ms step_avg:95.52ms
step:743/1750 train_time:70977ms step_avg:95.53ms
step:744/1750 train_time:71076ms step_avg:95.53ms
step:745/1750 train_time:71176ms step_avg:95.54ms
step:746/1750 train_time:71275ms step_avg:95.54ms
step:747/1750 train_time:71375ms step_avg:95.55ms
step:748/1750 train_time:71474ms step_avg:95.55ms
step:749/1750 train_time:71573ms step_avg:95.56ms
step:750/1750 train_time:71672ms step_avg:95.56ms
step:750/1750 val_loss:3.5950 train_time:71765ms step_avg:95.69ms
step:751/1750 train_time:71792ms step_avg:95.60ms
step:752/1750 train_time:71878ms step_avg:95.58ms
step:753/1750 train_time:71977ms step_avg:95.59ms
step:754/1750 train_time:72076ms step_avg:95.59ms
step:755/1750 train_time:72174ms step_avg:95.60ms
step:756/1750 train_time:72272ms step_avg:95.60ms
step:757/1750 train_time:72370ms step_avg:95.60ms
step:758/1750 train_time:72468ms step_avg:95.60ms
step:759/1750 train_time:72566ms step_avg:95.61ms
step:760/1750 train_time:72663ms step_avg:95.61ms
step:761/1750 train_time:72762ms step_avg:95.61ms
step:762/1750 train_time:72862ms step_avg:95.62ms
step:763/1750 train_time:72961ms step_avg:95.62ms
step:764/1750 train_time:73060ms step_avg:95.63ms
step:765/1750 train_time:73159ms step_avg:95.63ms
step:766/1750 train_time:73257ms step_avg:95.64ms
step:767/1750 train_time:73355ms step_avg:95.64ms
step:768/1750 train_time:73453ms step_avg:95.64ms
step:769/1750 train_time:73551ms step_avg:95.64ms
step:770/1750 train_time:73649ms step_avg:95.65ms
step:771/1750 train_time:73748ms step_avg:95.65ms
step:772/1750 train_time:73847ms step_avg:95.66ms
step:773/1750 train_time:73945ms step_avg:95.66ms
step:774/1750 train_time:74044ms step_avg:95.66ms
step:775/1750 train_time:74144ms step_avg:95.67ms
step:776/1750 train_time:74243ms step_avg:95.67ms
step:777/1750 train_time:74342ms step_avg:95.68ms
step:778/1750 train_time:74440ms step_avg:95.68ms
step:779/1750 train_time:74537ms step_avg:95.68ms
step:780/1750 train_time:74636ms step_avg:95.69ms
step:781/1750 train_time:74734ms step_avg:95.69ms
step:782/1750 train_time:74833ms step_avg:95.69ms
step:783/1750 train_time:74932ms step_avg:95.70ms
step:784/1750 train_time:75032ms step_avg:95.70ms
step:785/1750 train_time:75131ms step_avg:95.71ms
step:786/1750 train_time:75231ms step_avg:95.71ms
step:787/1750 train_time:75330ms step_avg:95.72ms
step:788/1750 train_time:75429ms step_avg:95.72ms
step:789/1750 train_time:75529ms step_avg:95.73ms
step:790/1750 train_time:75628ms step_avg:95.73ms
step:791/1750 train_time:75727ms step_avg:95.74ms
step:792/1750 train_time:75827ms step_avg:95.74ms
step:793/1750 train_time:75926ms step_avg:95.75ms
step:794/1750 train_time:76025ms step_avg:95.75ms
step:795/1750 train_time:76124ms step_avg:95.75ms
step:796/1750 train_time:76223ms step_avg:95.76ms
step:797/1750 train_time:76322ms step_avg:95.76ms
step:798/1750 train_time:76420ms step_avg:95.76ms
step:799/1750 train_time:76519ms step_avg:95.77ms
step:800/1750 train_time:76618ms step_avg:95.77ms
step:801/1750 train_time:76717ms step_avg:95.78ms
step:802/1750 train_time:76816ms step_avg:95.78ms
step:803/1750 train_time:76916ms step_avg:95.79ms
step:804/1750 train_time:77016ms step_avg:95.79ms
step:805/1750 train_time:77116ms step_avg:95.80ms
step:806/1750 train_time:77215ms step_avg:95.80ms
step:807/1750 train_time:77313ms step_avg:95.80ms
step:808/1750 train_time:77412ms step_avg:95.81ms
step:809/1750 train_time:77510ms step_avg:95.81ms
step:810/1750 train_time:77610ms step_avg:95.81ms
step:811/1750 train_time:77709ms step_avg:95.82ms
step:812/1750 train_time:77808ms step_avg:95.82ms
step:813/1750 train_time:77907ms step_avg:95.83ms
step:814/1750 train_time:78007ms step_avg:95.83ms
step:815/1750 train_time:78108ms step_avg:95.84ms
step:816/1750 train_time:78207ms step_avg:95.84ms
step:817/1750 train_time:78306ms step_avg:95.85ms
step:818/1750 train_time:78406ms step_avg:95.85ms
step:819/1750 train_time:78504ms step_avg:95.85ms
step:820/1750 train_time:78602ms step_avg:95.86ms
step:821/1750 train_time:78700ms step_avg:95.86ms
step:822/1750 train_time:78799ms step_avg:95.86ms
step:823/1750 train_time:78898ms step_avg:95.87ms
step:824/1750 train_time:78997ms step_avg:95.87ms
step:825/1750 train_time:79098ms step_avg:95.88ms
step:826/1750 train_time:79197ms step_avg:95.88ms
step:827/1750 train_time:79297ms step_avg:95.88ms
step:828/1750 train_time:79396ms step_avg:95.89ms
step:829/1750 train_time:79495ms step_avg:95.89ms
step:830/1750 train_time:79594ms step_avg:95.90ms
step:831/1750 train_time:79693ms step_avg:95.90ms
step:832/1750 train_time:79791ms step_avg:95.90ms
step:833/1750 train_time:79890ms step_avg:95.91ms
step:834/1750 train_time:79989ms step_avg:95.91ms
step:835/1750 train_time:80088ms step_avg:95.91ms
step:836/1750 train_time:80188ms step_avg:95.92ms
step:837/1750 train_time:80287ms step_avg:95.92ms
step:838/1750 train_time:80386ms step_avg:95.93ms
step:839/1750 train_time:80486ms step_avg:95.93ms
step:840/1750 train_time:80586ms step_avg:95.94ms
step:841/1750 train_time:80685ms step_avg:95.94ms
step:842/1750 train_time:80783ms step_avg:95.94ms
step:843/1750 train_time:80881ms step_avg:95.94ms
step:844/1750 train_time:80980ms step_avg:95.95ms
step:845/1750 train_time:81078ms step_avg:95.95ms
step:846/1750 train_time:81177ms step_avg:95.95ms
step:847/1750 train_time:81276ms step_avg:95.96ms
step:848/1750 train_time:81376ms step_avg:95.96ms
step:849/1750 train_time:81475ms step_avg:95.97ms
step:850/1750 train_time:81574ms step_avg:95.97ms
step:851/1750 train_time:81673ms step_avg:95.97ms
step:852/1750 train_time:81772ms step_avg:95.98ms
step:853/1750 train_time:81870ms step_avg:95.98ms
step:854/1750 train_time:81969ms step_avg:95.98ms
step:855/1750 train_time:82067ms step_avg:95.99ms
step:856/1750 train_time:82167ms step_avg:95.99ms
step:857/1750 train_time:82266ms step_avg:95.99ms
step:858/1750 train_time:82365ms step_avg:96.00ms
step:859/1750 train_time:82465ms step_avg:96.00ms
step:860/1750 train_time:82564ms step_avg:96.00ms
step:861/1750 train_time:82662ms step_avg:96.01ms
step:862/1750 train_time:82760ms step_avg:96.01ms
step:863/1750 train_time:82859ms step_avg:96.01ms
step:864/1750 train_time:82957ms step_avg:96.02ms
step:865/1750 train_time:83055ms step_avg:96.02ms
step:866/1750 train_time:83155ms step_avg:96.02ms
step:867/1750 train_time:83254ms step_avg:96.03ms
step:868/1750 train_time:83353ms step_avg:96.03ms
step:869/1750 train_time:83453ms step_avg:96.03ms
step:870/1750 train_time:83552ms step_avg:96.04ms
step:871/1750 train_time:83651ms step_avg:96.04ms
step:872/1750 train_time:83751ms step_avg:96.04ms
step:873/1750 train_time:83849ms step_avg:96.05ms
step:874/1750 train_time:83949ms step_avg:96.05ms
step:875/1750 train_time:84048ms step_avg:96.05ms
step:875/1750 val_loss:3.5459 train_time:84142ms step_avg:96.16ms
step:876/1750 train_time:84168ms step_avg:96.08ms
step:877/1750 train_time:84255ms step_avg:96.07ms
step:878/1750 train_time:84356ms step_avg:96.08ms
step:879/1750 train_time:84455ms step_avg:96.08ms
step:880/1750 train_time:84554ms step_avg:96.08ms
step:881/1750 train_time:84652ms step_avg:96.09ms
step:882/1750 train_time:84751ms step_avg:96.09ms
step:883/1750 train_time:84850ms step_avg:96.09ms
step:884/1750 train_time:84949ms step_avg:96.10ms
step:885/1750 train_time:85047ms step_avg:96.10ms
step:886/1750 train_time:85146ms step_avg:96.10ms
step:887/1750 train_time:85246ms step_avg:96.11ms
step:888/1750 train_time:85346ms step_avg:96.11ms
step:889/1750 train_time:85445ms step_avg:96.11ms
step:890/1750 train_time:85543ms step_avg:96.12ms
step:891/1750 train_time:85642ms step_avg:96.12ms
step:892/1750 train_time:85741ms step_avg:96.12ms
step:893/1750 train_time:85841ms step_avg:96.13ms
step:894/1750 train_time:85940ms step_avg:96.13ms
step:895/1750 train_time:86039ms step_avg:96.13ms
step:896/1750 train_time:86138ms step_avg:96.14ms
step:897/1750 train_time:86237ms step_avg:96.14ms
step:898/1750 train_time:86336ms step_avg:96.14ms
step:899/1750 train_time:86434ms step_avg:96.15ms
step:900/1750 train_time:86534ms step_avg:96.15ms
step:901/1750 train_time:86634ms step_avg:96.15ms
step:902/1750 train_time:86733ms step_avg:96.16ms
step:903/1750 train_time:86832ms step_avg:96.16ms
step:904/1750 train_time:86931ms step_avg:96.16ms
step:905/1750 train_time:87029ms step_avg:96.17ms
step:906/1750 train_time:87128ms step_avg:96.17ms
step:907/1750 train_time:87226ms step_avg:96.17ms
step:908/1750 train_time:87325ms step_avg:96.17ms
step:909/1750 train_time:87423ms step_avg:96.17ms
step:910/1750 train_time:87524ms step_avg:96.18ms
step:911/1750 train_time:87624ms step_avg:96.18ms
step:912/1750 train_time:87724ms step_avg:96.19ms
step:913/1750 train_time:87825ms step_avg:96.19ms
step:914/1750 train_time:87925ms step_avg:96.20ms
step:915/1750 train_time:88025ms step_avg:96.20ms
step:916/1750 train_time:88126ms step_avg:96.21ms
step:917/1750 train_time:88227ms step_avg:96.21ms
step:918/1750 train_time:88327ms step_avg:96.22ms
step:919/1750 train_time:88427ms step_avg:96.22ms
step:920/1750 train_time:88527ms step_avg:96.22ms
step:921/1750 train_time:88627ms step_avg:96.23ms
step:922/1750 train_time:88727ms step_avg:96.23ms
step:923/1750 train_time:88827ms step_avg:96.24ms
step:924/1750 train_time:88926ms step_avg:96.24ms
step:925/1750 train_time:89026ms step_avg:96.24ms
step:926/1750 train_time:89126ms step_avg:96.25ms
step:927/1750 train_time:89226ms step_avg:96.25ms
step:928/1750 train_time:89327ms step_avg:96.26ms
step:929/1750 train_time:89427ms step_avg:96.26ms
step:930/1750 train_time:89527ms step_avg:96.27ms
step:931/1750 train_time:89628ms step_avg:96.27ms
step:932/1750 train_time:89728ms step_avg:96.27ms
step:933/1750 train_time:89828ms step_avg:96.28ms
step:934/1750 train_time:89928ms step_avg:96.28ms
step:935/1750 train_time:90028ms step_avg:96.29ms
step:936/1750 train_time:90128ms step_avg:96.29ms
step:937/1750 train_time:90227ms step_avg:96.29ms
step:938/1750 train_time:90327ms step_avg:96.30ms
step:939/1750 train_time:90428ms step_avg:96.30ms
step:940/1750 train_time:90528ms step_avg:96.31ms
step:941/1750 train_time:90629ms step_avg:96.31ms
step:942/1750 train_time:90729ms step_avg:96.31ms
step:943/1750 train_time:90829ms step_avg:96.32ms
step:944/1750 train_time:90929ms step_avg:96.32ms
step:945/1750 train_time:91031ms step_avg:96.33ms
step:946/1750 train_time:91131ms step_avg:96.33ms
step:947/1750 train_time:91232ms step_avg:96.34ms
step:948/1750 train_time:91332ms step_avg:96.34ms
step:949/1750 train_time:91432ms step_avg:96.35ms
step:950/1750 train_time:91533ms step_avg:96.35ms
step:951/1750 train_time:91634ms step_avg:96.35ms
step:952/1750 train_time:91735ms step_avg:96.36ms
step:953/1750 train_time:91835ms step_avg:96.36ms
step:954/1750 train_time:91936ms step_avg:96.37ms
step:955/1750 train_time:92036ms step_avg:96.37ms
step:956/1750 train_time:92136ms step_avg:96.38ms
step:957/1750 train_time:92237ms step_avg:96.38ms
step:958/1750 train_time:92337ms step_avg:96.38ms
step:959/1750 train_time:92437ms step_avg:96.39ms
step:960/1750 train_time:92538ms step_avg:96.39ms
step:961/1750 train_time:92638ms step_avg:96.40ms
step:962/1750 train_time:92738ms step_avg:96.40ms
step:963/1750 train_time:92838ms step_avg:96.41ms
step:964/1750 train_time:92938ms step_avg:96.41ms
step:965/1750 train_time:93038ms step_avg:96.41ms
step:966/1750 train_time:93138ms step_avg:96.42ms
step:967/1750 train_time:93239ms step_avg:96.42ms
step:968/1750 train_time:93340ms step_avg:96.43ms
step:969/1750 train_time:93442ms step_avg:96.43ms
step:970/1750 train_time:93542ms step_avg:96.44ms
step:971/1750 train_time:93643ms step_avg:96.44ms
step:972/1750 train_time:93744ms step_avg:96.44ms
step:973/1750 train_time:93845ms step_avg:96.45ms
step:974/1750 train_time:93946ms step_avg:96.45ms
step:975/1750 train_time:94046ms step_avg:96.46ms
step:976/1750 train_time:94146ms step_avg:96.46ms
step:977/1750 train_time:94247ms step_avg:96.47ms
step:978/1750 train_time:94347ms step_avg:96.47ms
step:979/1750 train_time:94447ms step_avg:96.47ms
step:980/1750 train_time:94548ms step_avg:96.48ms
step:981/1750 train_time:94648ms step_avg:96.48ms
step:982/1750 train_time:94748ms step_avg:96.49ms
step:983/1750 train_time:94849ms step_avg:96.49ms
step:984/1750 train_time:94949ms step_avg:96.49ms
step:985/1750 train_time:95048ms step_avg:96.50ms
step:986/1750 train_time:95149ms step_avg:96.50ms
step:987/1750 train_time:95249ms step_avg:96.50ms
step:988/1750 train_time:95350ms step_avg:96.51ms
step:989/1750 train_time:95450ms step_avg:96.51ms
step:990/1750 train_time:95551ms step_avg:96.52ms
step:991/1750 train_time:95652ms step_avg:96.52ms
step:992/1750 train_time:95753ms step_avg:96.52ms
step:993/1750 train_time:95854ms step_avg:96.53ms
step:994/1750 train_time:95954ms step_avg:96.53ms
step:995/1750 train_time:96055ms step_avg:96.54ms
step:996/1750 train_time:96155ms step_avg:96.54ms
step:997/1750 train_time:96255ms step_avg:96.54ms
step:998/1750 train_time:96354ms step_avg:96.55ms
step:999/1750 train_time:96454ms step_avg:96.55ms
step:1000/1750 train_time:96555ms step_avg:96.55ms
step:1000/1750 val_loss:3.5047 train_time:96650ms step_avg:96.65ms
step:1001/1750 train_time:96677ms step_avg:96.58ms
step:1002/1750 train_time:96764ms step_avg:96.57ms
step:1003/1750 train_time:96864ms step_avg:96.57ms
step:1004/1750 train_time:96964ms step_avg:96.58ms
step:1005/1750 train_time:97064ms step_avg:96.58ms
step:1006/1750 train_time:97163ms step_avg:96.58ms
step:1007/1750 train_time:97262ms step_avg:96.59ms
step:1008/1750 train_time:97361ms step_avg:96.59ms
step:1009/1750 train_time:97461ms step_avg:96.59ms
step:1010/1750 train_time:97560ms step_avg:96.59ms
step:1011/1750 train_time:97662ms step_avg:96.60ms
step:1012/1750 train_time:97763ms step_avg:96.60ms
step:1013/1750 train_time:97864ms step_avg:96.61ms
step:1014/1750 train_time:97964ms step_avg:96.61ms
step:1015/1750 train_time:98064ms step_avg:96.61ms
step:1016/1750 train_time:98164ms step_avg:96.62ms
step:1017/1750 train_time:98264ms step_avg:96.62ms
step:1018/1750 train_time:98363ms step_avg:96.62ms
step:1019/1750 train_time:98462ms step_avg:96.63ms
step:1020/1750 train_time:98562ms step_avg:96.63ms
step:1021/1750 train_time:98662ms step_avg:96.63ms
step:1022/1750 train_time:98763ms step_avg:96.64ms
step:1023/1750 train_time:98864ms step_avg:96.64ms
step:1024/1750 train_time:98965ms step_avg:96.65ms
step:1025/1750 train_time:99066ms step_avg:96.65ms
step:1026/1750 train_time:99166ms step_avg:96.65ms
step:1027/1750 train_time:99266ms step_avg:96.66ms
step:1028/1750 train_time:99366ms step_avg:96.66ms
step:1029/1750 train_time:99468ms step_avg:96.66ms
step:1030/1750 train_time:99569ms step_avg:96.67ms
step:1031/1750 train_time:99670ms step_avg:96.67ms
step:1032/1750 train_time:99771ms step_avg:96.68ms
step:1033/1750 train_time:99871ms step_avg:96.68ms
step:1034/1750 train_time:99971ms step_avg:96.68ms
step:1035/1750 train_time:100073ms step_avg:96.69ms
step:1036/1750 train_time:100173ms step_avg:96.69ms
step:1037/1750 train_time:100274ms step_avg:96.70ms
step:1038/1750 train_time:100375ms step_avg:96.70ms
step:1039/1750 train_time:100475ms step_avg:96.70ms
step:1040/1750 train_time:100577ms step_avg:96.71ms
step:1041/1750 train_time:100678ms step_avg:96.71ms
step:1042/1750 train_time:100778ms step_avg:96.72ms
step:1043/1750 train_time:100880ms step_avg:96.72ms
step:1044/1750 train_time:100980ms step_avg:96.72ms
step:1045/1750 train_time:101081ms step_avg:96.73ms
step:1046/1750 train_time:101181ms step_avg:96.73ms
step:1047/1750 train_time:101282ms step_avg:96.74ms
step:1048/1750 train_time:101383ms step_avg:96.74ms
step:1049/1750 train_time:101483ms step_avg:96.74ms
step:1050/1750 train_time:101583ms step_avg:96.75ms
step:1051/1750 train_time:101684ms step_avg:96.75ms
step:1052/1750 train_time:101784ms step_avg:96.75ms
step:1053/1750 train_time:101884ms step_avg:96.76ms
step:1054/1750 train_time:101985ms step_avg:96.76ms
step:1055/1750 train_time:102085ms step_avg:96.76ms
step:1056/1750 train_time:102186ms step_avg:96.77ms
step:1057/1750 train_time:102286ms step_avg:96.77ms
step:1058/1750 train_time:102386ms step_avg:96.77ms
step:1059/1750 train_time:102486ms step_avg:96.78ms
step:1060/1750 train_time:102586ms step_avg:96.78ms
step:1061/1750 train_time:102686ms step_avg:96.78ms
step:1062/1750 train_time:102786ms step_avg:96.79ms
step:1063/1750 train_time:102887ms step_avg:96.79ms
step:1064/1750 train_time:102988ms step_avg:96.79ms
step:1065/1750 train_time:103090ms step_avg:96.80ms
step:1066/1750 train_time:103191ms step_avg:96.80ms
step:1067/1750 train_time:103292ms step_avg:96.81ms
step:1068/1750 train_time:103393ms step_avg:96.81ms
step:1069/1750 train_time:103493ms step_avg:96.81ms
step:1070/1750 train_time:103595ms step_avg:96.82ms
step:1071/1750 train_time:103695ms step_avg:96.82ms
step:1072/1750 train_time:103796ms step_avg:96.82ms
step:1073/1750 train_time:103897ms step_avg:96.83ms
step:1074/1750 train_time:103998ms step_avg:96.83ms
step:1075/1750 train_time:104100ms step_avg:96.84ms
step:1076/1750 train_time:104201ms step_avg:96.84ms
step:1077/1750 train_time:104301ms step_avg:96.84ms
step:1078/1750 train_time:104401ms step_avg:96.85ms
step:1079/1750 train_time:104502ms step_avg:96.85ms
step:1080/1750 train_time:104601ms step_avg:96.85ms
step:1081/1750 train_time:104702ms step_avg:96.86ms
step:1082/1750 train_time:104802ms step_avg:96.86ms
step:1083/1750 train_time:104902ms step_avg:96.86ms
step:1084/1750 train_time:105003ms step_avg:96.87ms
step:1085/1750 train_time:105104ms step_avg:96.87ms
step:1086/1750 train_time:105205ms step_avg:96.87ms
step:1087/1750 train_time:105306ms step_avg:96.88ms
step:1088/1750 train_time:105407ms step_avg:96.88ms
step:1089/1750 train_time:105507ms step_avg:96.88ms
step:1090/1750 train_time:105610ms step_avg:96.89ms
step:1091/1750 train_time:105711ms step_avg:96.89ms
step:1092/1750 train_time:105811ms step_avg:96.90ms
step:1093/1750 train_time:105911ms step_avg:96.90ms
step:1094/1750 train_time:106012ms step_avg:96.90ms
step:1095/1750 train_time:106113ms step_avg:96.91ms
step:1096/1750 train_time:106214ms step_avg:96.91ms
step:1097/1750 train_time:106314ms step_avg:96.91ms
step:1098/1750 train_time:106417ms step_avg:96.92ms
step:1099/1750 train_time:106518ms step_avg:96.92ms
step:1100/1750 train_time:106619ms step_avg:96.93ms
step:1101/1750 train_time:106719ms step_avg:96.93ms
step:1102/1750 train_time:106820ms step_avg:96.93ms
step:1103/1750 train_time:106921ms step_avg:96.94ms
step:1104/1750 train_time:107022ms step_avg:96.94ms
step:1105/1750 train_time:107122ms step_avg:96.94ms
step:1106/1750 train_time:107223ms step_avg:96.95ms
step:1107/1750 train_time:107324ms step_avg:96.95ms
step:1108/1750 train_time:107424ms step_avg:96.95ms
step:1109/1750 train_time:107524ms step_avg:96.96ms
step:1110/1750 train_time:107625ms step_avg:96.96ms
step:1111/1750 train_time:107727ms step_avg:96.96ms
step:1112/1750 train_time:107828ms step_avg:96.97ms
step:1113/1750 train_time:107929ms step_avg:96.97ms
step:1114/1750 train_time:108031ms step_avg:96.98ms
step:1115/1750 train_time:108133ms step_avg:96.98ms
step:1116/1750 train_time:108233ms step_avg:96.98ms
step:1117/1750 train_time:108334ms step_avg:96.99ms
step:1118/1750 train_time:108433ms step_avg:96.99ms
step:1119/1750 train_time:108534ms step_avg:96.99ms
step:1120/1750 train_time:108635ms step_avg:97.00ms
step:1121/1750 train_time:108736ms step_avg:97.00ms
step:1122/1750 train_time:108837ms step_avg:97.00ms
step:1123/1750 train_time:108938ms step_avg:97.01ms
step:1124/1750 train_time:109040ms step_avg:97.01ms
step:1125/1750 train_time:109140ms step_avg:97.01ms
step:1125/1750 val_loss:3.4515 train_time:109235ms step_avg:97.10ms
step:1126/1750 train_time:109262ms step_avg:97.04ms
step:1127/1750 train_time:109353ms step_avg:97.03ms
step:1128/1750 train_time:109455ms step_avg:97.03ms
step:1129/1750 train_time:109556ms step_avg:97.04ms
step:1130/1750 train_time:109656ms step_avg:97.04ms
step:1131/1750 train_time:109756ms step_avg:97.04ms
step:1132/1750 train_time:109857ms step_avg:97.05ms
step:1133/1750 train_time:109957ms step_avg:97.05ms
step:1134/1750 train_time:110057ms step_avg:97.05ms
step:1135/1750 train_time:110157ms step_avg:97.05ms
step:1136/1750 train_time:110261ms step_avg:97.06ms
step:1137/1750 train_time:110365ms step_avg:97.07ms
step:1138/1750 train_time:110465ms step_avg:97.07ms
step:1139/1750 train_time:110565ms step_avg:97.07ms
step:1140/1750 train_time:110664ms step_avg:97.07ms
step:1141/1750 train_time:110764ms step_avg:97.08ms
step:1142/1750 train_time:110864ms step_avg:97.08ms
step:1143/1750 train_time:110964ms step_avg:97.08ms
step:1144/1750 train_time:111063ms step_avg:97.08ms
step:1145/1750 train_time:111164ms step_avg:97.09ms
step:1146/1750 train_time:111265ms step_avg:97.09ms
step:1147/1750 train_time:111366ms step_avg:97.09ms
step:1148/1750 train_time:111466ms step_avg:97.10ms
step:1149/1750 train_time:111567ms step_avg:97.10ms
step:1150/1750 train_time:111666ms step_avg:97.10ms
step:1151/1750 train_time:111767ms step_avg:97.10ms
step:1152/1750 train_time:111867ms step_avg:97.11ms
step:1153/1750 train_time:111968ms step_avg:97.11ms
step:1154/1750 train_time:112068ms step_avg:97.11ms
step:1155/1750 train_time:112170ms step_avg:97.12ms
step:1156/1750 train_time:112272ms step_avg:97.12ms
step:1157/1750 train_time:112374ms step_avg:97.13ms
step:1158/1750 train_time:112475ms step_avg:97.13ms
step:1159/1750 train_time:112575ms step_avg:97.13ms
step:1160/1750 train_time:112676ms step_avg:97.13ms
step:1161/1750 train_time:112776ms step_avg:97.14ms
step:1162/1750 train_time:112876ms step_avg:97.14ms
step:1163/1750 train_time:112977ms step_avg:97.14ms
step:1164/1750 train_time:113078ms step_avg:97.15ms
step:1165/1750 train_time:113180ms step_avg:97.15ms
step:1166/1750 train_time:113281ms step_avg:97.15ms
step:1167/1750 train_time:113382ms step_avg:97.16ms
step:1168/1750 train_time:113482ms step_avg:97.16ms
step:1169/1750 train_time:113585ms step_avg:97.16ms
step:1170/1750 train_time:113685ms step_avg:97.17ms
step:1171/1750 train_time:113787ms step_avg:97.17ms
step:1172/1750 train_time:113889ms step_avg:97.17ms
step:1173/1750 train_time:113992ms step_avg:97.18ms
step:1174/1750 train_time:114095ms step_avg:97.18ms
step:1175/1750 train_time:114196ms step_avg:97.19ms
step:1176/1750 train_time:114299ms step_avg:97.19ms
step:1177/1750 train_time:114401ms step_avg:97.20ms
step:1178/1750 train_time:114503ms step_avg:97.20ms
step:1179/1750 train_time:114606ms step_avg:97.21ms
step:1180/1750 train_time:114707ms step_avg:97.21ms
step:1181/1750 train_time:114808ms step_avg:97.21ms
step:1182/1750 train_time:114911ms step_avg:97.22ms
step:1183/1750 train_time:115013ms step_avg:97.22ms
step:1184/1750 train_time:115116ms step_avg:97.23ms
step:1185/1750 train_time:115219ms step_avg:97.23ms
step:1186/1750 train_time:115321ms step_avg:97.24ms
step:1187/1750 train_time:115423ms step_avg:97.24ms
step:1188/1750 train_time:115525ms step_avg:97.24ms
step:1189/1750 train_time:115626ms step_avg:97.25ms
step:1190/1750 train_time:115727ms step_avg:97.25ms
step:1191/1750 train_time:115827ms step_avg:97.25ms
step:1192/1750 train_time:115929ms step_avg:97.26ms
step:1193/1750 train_time:116032ms step_avg:97.26ms
step:1194/1750 train_time:116135ms step_avg:97.27ms
step:1195/1750 train_time:116237ms step_avg:97.27ms
step:1196/1750 train_time:116339ms step_avg:97.27ms
step:1197/1750 train_time:116440ms step_avg:97.28ms
step:1198/1750 train_time:116542ms step_avg:97.28ms
step:1199/1750 train_time:116644ms step_avg:97.28ms
step:1200/1750 train_time:116745ms step_avg:97.29ms
step:1201/1750 train_time:116846ms step_avg:97.29ms
step:1202/1750 train_time:116948ms step_avg:97.29ms
step:1203/1750 train_time:117051ms step_avg:97.30ms
step:1204/1750 train_time:117154ms step_avg:97.30ms
step:1205/1750 train_time:117257ms step_avg:97.31ms
step:1206/1750 train_time:117359ms step_avg:97.31ms
step:1207/1750 train_time:117460ms step_avg:97.32ms
step:1208/1750 train_time:117563ms step_avg:97.32ms
step:1209/1750 train_time:117665ms step_avg:97.32ms
step:1210/1750 train_time:117767ms step_avg:97.33ms
step:1211/1750 train_time:117868ms step_avg:97.33ms
step:1212/1750 train_time:117969ms step_avg:97.33ms
step:1213/1750 train_time:118071ms step_avg:97.34ms
step:1214/1750 train_time:118173ms step_avg:97.34ms
step:1215/1750 train_time:118277ms step_avg:97.35ms
step:1216/1750 train_time:118379ms step_avg:97.35ms
step:1217/1750 train_time:118481ms step_avg:97.35ms
step:1218/1750 train_time:118584ms step_avg:97.36ms
step:1219/1750 train_time:118686ms step_avg:97.36ms
step:1220/1750 train_time:118787ms step_avg:97.37ms
step:1221/1750 train_time:118888ms step_avg:97.37ms
step:1222/1750 train_time:118990ms step_avg:97.37ms
step:1223/1750 train_time:119092ms step_avg:97.38ms
step:1224/1750 train_time:119195ms step_avg:97.38ms
step:1225/1750 train_time:119297ms step_avg:97.39ms
step:1226/1750 train_time:119399ms step_avg:97.39ms
step:1227/1750 train_time:119501ms step_avg:97.39ms
step:1228/1750 train_time:119603ms step_avg:97.40ms
step:1229/1750 train_time:119705ms step_avg:97.40ms
step:1230/1750 train_time:119806ms step_avg:97.40ms
step:1231/1750 train_time:119906ms step_avg:97.41ms
step:1232/1750 train_time:120008ms step_avg:97.41ms
step:1233/1750 train_time:120109ms step_avg:97.41ms
step:1234/1750 train_time:120214ms step_avg:97.42ms
step:1235/1750 train_time:120316ms step_avg:97.42ms
step:1236/1750 train_time:120418ms step_avg:97.43ms
step:1237/1750 train_time:120520ms step_avg:97.43ms
step:1238/1750 train_time:120622ms step_avg:97.43ms
step:1239/1750 train_time:120724ms step_avg:97.44ms
step:1240/1750 train_time:120826ms step_avg:97.44ms
step:1241/1750 train_time:120928ms step_avg:97.44ms
step:1242/1750 train_time:121029ms step_avg:97.45ms
step:1243/1750 train_time:121130ms step_avg:97.45ms
step:1244/1750 train_time:121233ms step_avg:97.45ms
step:1245/1750 train_time:121335ms step_avg:97.46ms
step:1246/1750 train_time:121437ms step_avg:97.46ms
step:1247/1750 train_time:121539ms step_avg:97.47ms
step:1248/1750 train_time:121642ms step_avg:97.47ms
step:1249/1750 train_time:121743ms step_avg:97.47ms
step:1250/1750 train_time:121844ms step_avg:97.48ms
step:1250/1750 val_loss:3.4051 train_time:121940ms step_avg:97.55ms
step:1251/1750 train_time:121967ms step_avg:97.50ms
step:1252/1750 train_time:122058ms step_avg:97.49ms
step:1253/1750 train_time:122161ms step_avg:97.49ms
step:1254/1750 train_time:122263ms step_avg:97.50ms
step:1255/1750 train_time:122364ms step_avg:97.50ms
step:1256/1750 train_time:122465ms step_avg:97.50ms
step:1257/1750 train_time:122566ms step_avg:97.51ms
step:1258/1750 train_time:122667ms step_avg:97.51ms
step:1259/1750 train_time:122767ms step_avg:97.51ms
step:1260/1750 train_time:122868ms step_avg:97.51ms
step:1261/1750 train_time:122970ms step_avg:97.52ms
step:1262/1750 train_time:123074ms step_avg:97.52ms
step:1263/1750 train_time:123177ms step_avg:97.53ms
step:1264/1750 train_time:123279ms step_avg:97.53ms
step:1265/1750 train_time:123379ms step_avg:97.53ms
step:1266/1750 train_time:123480ms step_avg:97.54ms
step:1267/1750 train_time:123582ms step_avg:97.54ms
step:1268/1750 train_time:123683ms step_avg:97.54ms
step:1269/1750 train_time:123785ms step_avg:97.55ms
step:1270/1750 train_time:123887ms step_avg:97.55ms
step:1271/1750 train_time:123991ms step_avg:97.55ms
step:1272/1750 train_time:124093ms step_avg:97.56ms
step:1273/1750 train_time:124195ms step_avg:97.56ms
step:1274/1750 train_time:124296ms step_avg:97.56ms
step:1275/1750 train_time:124398ms step_avg:97.57ms
step:1276/1750 train_time:124500ms step_avg:97.57ms
step:1277/1750 train_time:124601ms step_avg:97.57ms
step:1278/1750 train_time:124702ms step_avg:97.58ms
step:1279/1750 train_time:124804ms step_avg:97.58ms
step:1280/1750 train_time:124906ms step_avg:97.58ms
step:1281/1750 train_time:125008ms step_avg:97.59ms
step:1282/1750 train_time:125110ms step_avg:97.59ms
step:1283/1750 train_time:125210ms step_avg:97.59ms
step:1284/1750 train_time:125311ms step_avg:97.59ms
step:1285/1750 train_time:125413ms step_avg:97.60ms
step:1286/1750 train_time:125515ms step_avg:97.60ms
step:1287/1750 train_time:125618ms step_avg:97.61ms
step:1288/1750 train_time:125719ms step_avg:97.61ms
step:1289/1750 train_time:125821ms step_avg:97.61ms
step:1290/1750 train_time:125924ms step_avg:97.62ms
step:1291/1750 train_time:126025ms step_avg:97.62ms
step:1292/1750 train_time:126128ms step_avg:97.62ms
step:1293/1750 train_time:126229ms step_avg:97.62ms
step:1294/1750 train_time:126330ms step_avg:97.63ms
step:1295/1750 train_time:126433ms step_avg:97.63ms
step:1296/1750 train_time:126535ms step_avg:97.63ms
step:1297/1750 train_time:126638ms step_avg:97.64ms
step:1298/1750 train_time:126740ms step_avg:97.64ms
step:1299/1750 train_time:126842ms step_avg:97.65ms
step:1300/1750 train_time:126944ms step_avg:97.65ms
step:1301/1750 train_time:127046ms step_avg:97.65ms
step:1302/1750 train_time:127148ms step_avg:97.66ms
step:1303/1750 train_time:127250ms step_avg:97.66ms
step:1304/1750 train_time:127351ms step_avg:97.66ms
step:1305/1750 train_time:127454ms step_avg:97.67ms
step:1306/1750 train_time:127556ms step_avg:97.67ms
step:1307/1750 train_time:127659ms step_avg:97.67ms
step:1308/1750 train_time:127761ms step_avg:97.68ms
step:1309/1750 train_time:127863ms step_avg:97.68ms
step:1310/1750 train_time:127965ms step_avg:97.68ms
step:1311/1750 train_time:128067ms step_avg:97.69ms
step:1312/1750 train_time:128169ms step_avg:97.69ms
step:1313/1750 train_time:128272ms step_avg:97.69ms
step:1314/1750 train_time:128374ms step_avg:97.70ms
step:1315/1750 train_time:128476ms step_avg:97.70ms
step:1316/1750 train_time:128577ms step_avg:97.70ms
step:1317/1750 train_time:128679ms step_avg:97.71ms
step:1318/1750 train_time:128780ms step_avg:97.71ms
step:1319/1750 train_time:128883ms step_avg:97.71ms
step:1320/1750 train_time:128986ms step_avg:97.72ms
step:1321/1750 train_time:129087ms step_avg:97.72ms
step:1322/1750 train_time:129189ms step_avg:97.72ms
step:1323/1750 train_time:129290ms step_avg:97.72ms
step:1324/1750 train_time:129391ms step_avg:97.73ms
step:1325/1750 train_time:129494ms step_avg:97.73ms
step:1326/1750 train_time:129597ms step_avg:97.74ms
step:1327/1750 train_time:129700ms step_avg:97.74ms
step:1328/1750 train_time:129801ms step_avg:97.74ms
step:1329/1750 train_time:129903ms step_avg:97.74ms
step:1330/1750 train_time:130005ms step_avg:97.75ms
step:1331/1750 train_time:130108ms step_avg:97.75ms
step:1332/1750 train_time:130209ms step_avg:97.75ms
step:1333/1750 train_time:130311ms step_avg:97.76ms
step:1334/1750 train_time:130412ms step_avg:97.76ms
step:1335/1750 train_time:130514ms step_avg:97.76ms
step:1336/1750 train_time:130616ms step_avg:97.77ms
step:1337/1750 train_time:130719ms step_avg:97.77ms
step:1338/1750 train_time:130820ms step_avg:97.77ms
step:1339/1750 train_time:130923ms step_avg:97.78ms
step:1340/1750 train_time:131025ms step_avg:97.78ms
step:1341/1750 train_time:131127ms step_avg:97.78ms
step:1342/1750 train_time:131229ms step_avg:97.79ms
step:1343/1750 train_time:131331ms step_avg:97.79ms
step:1344/1750 train_time:131432ms step_avg:97.79ms
step:1345/1750 train_time:131534ms step_avg:97.79ms
step:1346/1750 train_time:131638ms step_avg:97.80ms
step:1347/1750 train_time:131739ms step_avg:97.80ms
step:1348/1750 train_time:131841ms step_avg:97.80ms
step:1349/1750 train_time:131942ms step_avg:97.81ms
step:1350/1750 train_time:132044ms step_avg:97.81ms
step:1351/1750 train_time:132147ms step_avg:97.81ms
step:1352/1750 train_time:132248ms step_avg:97.82ms
step:1353/1750 train_time:132350ms step_avg:97.82ms
step:1354/1750 train_time:132451ms step_avg:97.82ms
step:1355/1750 train_time:132552ms step_avg:97.82ms
step:1356/1750 train_time:132654ms step_avg:97.83ms
step:1357/1750 train_time:132758ms step_avg:97.83ms
step:1358/1750 train_time:132860ms step_avg:97.84ms
step:1359/1750 train_time:132963ms step_avg:97.84ms
step:1360/1750 train_time:133064ms step_avg:97.84ms
step:1361/1750 train_time:133165ms step_avg:97.84ms
step:1362/1750 train_time:133267ms step_avg:97.85ms
step:1363/1750 train_time:133370ms step_avg:97.85ms
step:1364/1750 train_time:133472ms step_avg:97.85ms
step:1365/1750 train_time:133574ms step_avg:97.86ms
step:1366/1750 train_time:133676ms step_avg:97.86ms
step:1367/1750 train_time:133778ms step_avg:97.86ms
step:1368/1750 train_time:133882ms step_avg:97.87ms
step:1369/1750 train_time:133983ms step_avg:97.87ms
step:1370/1750 train_time:134084ms step_avg:97.87ms
step:1371/1750 train_time:134186ms step_avg:97.87ms
step:1372/1750 train_time:134288ms step_avg:97.88ms
step:1373/1750 train_time:134389ms step_avg:97.88ms
step:1374/1750 train_time:134491ms step_avg:97.88ms
step:1375/1750 train_time:134594ms step_avg:97.89ms
step:1375/1750 val_loss:3.3644 train_time:134691ms step_avg:97.96ms
step:1376/1750 train_time:134718ms step_avg:97.91ms
step:1377/1750 train_time:134806ms step_avg:97.90ms
step:1378/1750 train_time:134908ms step_avg:97.90ms
step:1379/1750 train_time:135009ms step_avg:97.90ms
step:1380/1750 train_time:135111ms step_avg:97.91ms
step:1381/1750 train_time:135213ms step_avg:97.91ms
step:1382/1750 train_time:135314ms step_avg:97.91ms
step:1383/1750 train_time:135415ms step_avg:97.91ms
step:1384/1750 train_time:135516ms step_avg:97.92ms
step:1385/1750 train_time:135618ms step_avg:97.92ms
step:1386/1750 train_time:135723ms step_avg:97.92ms
step:1387/1750 train_time:135827ms step_avg:97.93ms
step:1388/1750 train_time:135927ms step_avg:97.93ms
step:1389/1750 train_time:136029ms step_avg:97.93ms
step:1390/1750 train_time:136130ms step_avg:97.94ms
step:1391/1750 train_time:136231ms step_avg:97.94ms
step:1392/1750 train_time:136332ms step_avg:97.94ms
step:1393/1750 train_time:136434ms step_avg:97.94ms
step:1394/1750 train_time:136537ms step_avg:97.95ms
step:1395/1750 train_time:136640ms step_avg:97.95ms
step:1396/1750 train_time:136743ms step_avg:97.95ms
step:1397/1750 train_time:136845ms step_avg:97.96ms
step:1398/1750 train_time:136946ms step_avg:97.96ms
step:1399/1750 train_time:137049ms step_avg:97.96ms
step:1400/1750 train_time:137150ms step_avg:97.96ms
step:1401/1750 train_time:137252ms step_avg:97.97ms
step:1402/1750 train_time:137353ms step_avg:97.97ms
step:1403/1750 train_time:137455ms step_avg:97.97ms
step:1404/1750 train_time:137557ms step_avg:97.97ms
step:1405/1750 train_time:137659ms step_avg:97.98ms
step:1406/1750 train_time:137763ms step_avg:97.98ms
step:1407/1750 train_time:137865ms step_avg:97.98ms
step:1408/1750 train_time:137966ms step_avg:97.99ms
step:1409/1750 train_time:138069ms step_avg:97.99ms
step:1410/1750 train_time:138170ms step_avg:97.99ms
step:1411/1750 train_time:138271ms step_avg:97.99ms
step:1412/1750 train_time:138373ms step_avg:98.00ms
step:1413/1750 train_time:138474ms step_avg:98.00ms
step:1414/1750 train_time:138577ms step_avg:98.00ms
step:1415/1750 train_time:138680ms step_avg:98.01ms
step:1416/1750 train_time:138783ms step_avg:98.01ms
step:1417/1750 train_time:138885ms step_avg:98.01ms
step:1418/1750 train_time:138987ms step_avg:98.02ms
step:1419/1750 train_time:139090ms step_avg:98.02ms
step:1420/1750 train_time:139192ms step_avg:98.02ms
step:1421/1750 train_time:139295ms step_avg:98.03ms
step:1422/1750 train_time:139395ms step_avg:98.03ms
step:1423/1750 train_time:139496ms step_avg:98.03ms
step:1424/1750 train_time:139598ms step_avg:98.03ms
step:1425/1750 train_time:139701ms step_avg:98.04ms
step:1426/1750 train_time:139803ms step_avg:98.04ms
step:1427/1750 train_time:139905ms step_avg:98.04ms
step:1428/1750 train_time:140009ms step_avg:98.05ms
step:1429/1750 train_time:140112ms step_avg:98.05ms
step:1430/1750 train_time:140214ms step_avg:98.05ms
step:1431/1750 train_time:140318ms step_avg:98.06ms
step:1432/1750 train_time:140420ms step_avg:98.06ms
step:1433/1750 train_time:140523ms step_avg:98.06ms
step:1434/1750 train_time:140626ms step_avg:98.07ms
step:1435/1750 train_time:140730ms step_avg:98.07ms
step:1436/1750 train_time:140834ms step_avg:98.07ms
step:1437/1750 train_time:140939ms step_avg:98.08ms
step:1438/1750 train_time:141043ms step_avg:98.08ms
step:1439/1750 train_time:141146ms step_avg:98.09ms
step:1440/1750 train_time:141251ms step_avg:98.09ms
step:1441/1750 train_time:141355ms step_avg:98.10ms
step:1442/1750 train_time:141457ms step_avg:98.10ms
step:1443/1750 train_time:141559ms step_avg:98.10ms
step:1444/1750 train_time:141662ms step_avg:98.10ms
step:1445/1750 train_time:141765ms step_avg:98.11ms
step:1446/1750 train_time:141868ms step_avg:98.11ms
step:1447/1750 train_time:141970ms step_avg:98.11ms
step:1448/1750 train_time:142073ms step_avg:98.12ms
step:1449/1750 train_time:142175ms step_avg:98.12ms
step:1450/1750 train_time:142279ms step_avg:98.12ms
step:1451/1750 train_time:142381ms step_avg:98.13ms
step:1452/1750 train_time:142485ms step_avg:98.13ms
step:1453/1750 train_time:142589ms step_avg:98.13ms
step:1454/1750 train_time:142693ms step_avg:98.14ms
step:1455/1750 train_time:142795ms step_avg:98.14ms
step:1456/1750 train_time:142898ms step_avg:98.14ms
step:1457/1750 train_time:143002ms step_avg:98.15ms
step:1458/1750 train_time:143105ms step_avg:98.15ms
step:1459/1750 train_time:143208ms step_avg:98.15ms
step:1460/1750 train_time:143310ms step_avg:98.16ms
step:1461/1750 train_time:143413ms step_avg:98.16ms
step:1462/1750 train_time:143516ms step_avg:98.16ms
step:1463/1750 train_time:143619ms step_avg:98.17ms
step:1464/1750 train_time:143722ms step_avg:98.17ms
step:1465/1750 train_time:143825ms step_avg:98.17ms
step:1466/1750 train_time:143927ms step_avg:98.18ms
step:1467/1750 train_time:144029ms step_avg:98.18ms
step:1468/1750 train_time:144133ms step_avg:98.18ms
step:1469/1750 train_time:144237ms step_avg:98.19ms
step:1470/1750 train_time:144341ms step_avg:98.19ms
step:1471/1750 train_time:144443ms step_avg:98.19ms
step:1472/1750 train_time:144545ms step_avg:98.20ms
step:1473/1750 train_time:144649ms step_avg:98.20ms
step:1474/1750 train_time:144752ms step_avg:98.20ms
step:1475/1750 train_time:144854ms step_avg:98.21ms
step:1476/1750 train_time:144958ms step_avg:98.21ms
step:1477/1750 train_time:145062ms step_avg:98.21ms
step:1478/1750 train_time:145166ms step_avg:98.22ms
step:1479/1750 train_time:145268ms step_avg:98.22ms
step:1480/1750 train_time:145371ms step_avg:98.22ms
step:1481/1750 train_time:145474ms step_avg:98.23ms
step:1482/1750 train_time:145578ms step_avg:98.23ms
step:1483/1750 train_time:145681ms step_avg:98.23ms
step:1484/1750 train_time:145784ms step_avg:98.24ms
step:1485/1750 train_time:145889ms step_avg:98.24ms
step:1486/1750 train_time:145991ms step_avg:98.24ms
step:1487/1750 train_time:146094ms step_avg:98.25ms
step:1488/1750 train_time:146199ms step_avg:98.25ms
step:1489/1750 train_time:146303ms step_avg:98.26ms
step:1490/1750 train_time:146406ms step_avg:98.26ms
step:1491/1750 train_time:146509ms step_avg:98.26ms
step:1492/1750 train_time:146611ms step_avg:98.26ms
step:1493/1750 train_time:146713ms step_avg:98.27ms
step:1494/1750 train_time:146816ms step_avg:98.27ms
step:1495/1750 train_time:146919ms step_avg:98.27ms
step:1496/1750 train_time:147022ms step_avg:98.28ms
step:1497/1750 train_time:147125ms step_avg:98.28ms
step:1498/1750 train_time:147227ms step_avg:98.28ms
step:1499/1750 train_time:147330ms step_avg:98.29ms
step:1500/1750 train_time:147434ms step_avg:98.29ms
step:1500/1750 val_loss:3.3290 train_time:147531ms step_avg:98.35ms
step:1501/1750 train_time:147558ms step_avg:98.31ms
step:1502/1750 train_time:147649ms step_avg:98.30ms
step:1503/1750 train_time:147752ms step_avg:98.30ms
step:1504/1750 train_time:147854ms step_avg:98.31ms
step:1505/1750 train_time:147957ms step_avg:98.31ms
step:1506/1750 train_time:148059ms step_avg:98.31ms
step:1507/1750 train_time:148162ms step_avg:98.32ms
step:1508/1750 train_time:148263ms step_avg:98.32ms
step:1509/1750 train_time:148366ms step_avg:98.32ms
step:1510/1750 train_time:148468ms step_avg:98.32ms
step:1511/1750 train_time:148574ms step_avg:98.33ms
step:1512/1750 train_time:148678ms step_avg:98.33ms
step:1513/1750 train_time:148782ms step_avg:98.34ms
step:1514/1750 train_time:148887ms step_avg:98.34ms
step:1515/1750 train_time:148992ms step_avg:98.34ms
step:1516/1750 train_time:149095ms step_avg:98.35ms
step:1517/1750 train_time:149197ms step_avg:98.35ms
step:1518/1750 train_time:149300ms step_avg:98.35ms
step:1519/1750 train_time:149405ms step_avg:98.36ms
step:1520/1750 train_time:149507ms step_avg:98.36ms
step:1521/1750 train_time:149609ms step_avg:98.36ms
step:1522/1750 train_time:149712ms step_avg:98.37ms
step:1523/1750 train_time:149816ms step_avg:98.37ms
step:1524/1750 train_time:149921ms step_avg:98.37ms
step:1525/1750 train_time:150025ms step_avg:98.38ms
step:1526/1750 train_time:150127ms step_avg:98.38ms
step:1527/1750 train_time:150231ms step_avg:98.38ms
step:1528/1750 train_time:150336ms step_avg:98.39ms
step:1529/1750 train_time:150438ms step_avg:98.39ms
step:1530/1750 train_time:150541ms step_avg:98.39ms
step:1531/1750 train_time:150643ms step_avg:98.40ms
step:1532/1750 train_time:150746ms step_avg:98.40ms
step:1533/1750 train_time:150848ms step_avg:98.40ms
step:1534/1750 train_time:150952ms step_avg:98.40ms
step:1535/1750 train_time:151054ms step_avg:98.41ms
step:1536/1750 train_time:151156ms step_avg:98.41ms
step:1537/1750 train_time:151259ms step_avg:98.41ms
step:1538/1750 train_time:151362ms step_avg:98.41ms
step:1539/1750 train_time:151465ms step_avg:98.42ms
step:1540/1750 train_time:151567ms step_avg:98.42ms
step:1541/1750 train_time:151673ms step_avg:98.43ms
step:1542/1750 train_time:151777ms step_avg:98.43ms
step:1543/1750 train_time:151880ms step_avg:98.43ms
step:1544/1750 train_time:151984ms step_avg:98.44ms
step:1545/1750 train_time:152087ms step_avg:98.44ms
step:1546/1750 train_time:152190ms step_avg:98.44ms
step:1547/1750 train_time:152294ms step_avg:98.44ms
step:1548/1750 train_time:152397ms step_avg:98.45ms
step:1549/1750 train_time:152500ms step_avg:98.45ms
step:1550/1750 train_time:152604ms step_avg:98.45ms
step:1551/1750 train_time:152707ms step_avg:98.46ms
step:1552/1750 train_time:152809ms step_avg:98.46ms
step:1553/1750 train_time:152912ms step_avg:98.46ms
step:1554/1750 train_time:153015ms step_avg:98.47ms
step:1555/1750 train_time:153118ms step_avg:98.47ms
step:1556/1750 train_time:153221ms step_avg:98.47ms
step:1557/1750 train_time:153326ms step_avg:98.48ms
step:1558/1750 train_time:153430ms step_avg:98.48ms
step:1559/1750 train_time:153534ms step_avg:98.48ms
step:1560/1750 train_time:153637ms step_avg:98.49ms
step:1561/1750 train_time:153741ms step_avg:98.49ms
step:1562/1750 train_time:153844ms step_avg:98.49ms
step:1563/1750 train_time:153950ms step_avg:98.50ms
step:1564/1750 train_time:154052ms step_avg:98.50ms
step:1565/1750 train_time:154155ms step_avg:98.50ms
step:1566/1750 train_time:154259ms step_avg:98.51ms
step:1567/1750 train_time:154362ms step_avg:98.51ms
step:1568/1750 train_time:154465ms step_avg:98.51ms
step:1569/1750 train_time:154567ms step_avg:98.51ms
step:1570/1750 train_time:154673ms step_avg:98.52ms
step:1571/1750 train_time:154776ms step_avg:98.52ms
step:1572/1750 train_time:154880ms step_avg:98.52ms
step:1573/1750 train_time:154983ms step_avg:98.53ms
step:1574/1750 train_time:155086ms step_avg:98.53ms
step:1575/1750 train_time:155188ms step_avg:98.53ms
step:1576/1750 train_time:155292ms step_avg:98.54ms
step:1577/1750 train_time:155397ms step_avg:98.54ms
step:1578/1750 train_time:155500ms step_avg:98.54ms
step:1579/1750 train_time:155604ms step_avg:98.55ms
step:1580/1750 train_time:155707ms step_avg:98.55ms
step:1581/1750 train_time:155810ms step_avg:98.55ms
step:1582/1750 train_time:155912ms step_avg:98.55ms
step:1583/1750 train_time:156016ms step_avg:98.56ms
step:1584/1750 train_time:156121ms step_avg:98.56ms
step:1585/1750 train_time:156225ms step_avg:98.56ms
step:1586/1750 train_time:156330ms step_avg:98.57ms
step:1587/1750 train_time:156433ms step_avg:98.57ms
step:1588/1750 train_time:156536ms step_avg:98.57ms
step:1589/1750 train_time:156639ms step_avg:98.58ms
step:1590/1750 train_time:156743ms step_avg:98.58ms
step:1591/1750 train_time:156846ms step_avg:98.58ms
step:1592/1750 train_time:156949ms step_avg:98.59ms
step:1593/1750 train_time:157052ms step_avg:98.59ms
step:1594/1750 train_time:157158ms step_avg:98.59ms
step:1595/1750 train_time:157261ms step_avg:98.60ms
step:1596/1750 train_time:157364ms step_avg:98.60ms
step:1597/1750 train_time:157468ms step_avg:98.60ms
step:1598/1750 train_time:157572ms step_avg:98.61ms
step:1599/1750 train_time:157675ms step_avg:98.61ms
step:1600/1750 train_time:157779ms step_avg:98.61ms
step:1601/1750 train_time:157883ms step_avg:98.62ms
step:1602/1750 train_time:157985ms step_avg:98.62ms
step:1603/1750 train_time:158088ms step_avg:98.62ms
step:1604/1750 train_time:158191ms step_avg:98.62ms
step:1605/1750 train_time:158296ms step_avg:98.63ms
step:1606/1750 train_time:158399ms step_avg:98.63ms
step:1607/1750 train_time:158502ms step_avg:98.63ms
step:1608/1750 train_time:158605ms step_avg:98.63ms
step:1609/1750 train_time:158707ms step_avg:98.64ms
step:1610/1750 train_time:158812ms step_avg:98.64ms
step:1611/1750 train_time:158917ms step_avg:98.65ms
step:1612/1750 train_time:159021ms step_avg:98.65ms
step:1613/1750 train_time:159123ms step_avg:98.65ms
step:1614/1750 train_time:159227ms step_avg:98.65ms
step:1615/1750 train_time:159329ms step_avg:98.66ms
step:1616/1750 train_time:159432ms step_avg:98.66ms
step:1617/1750 train_time:159536ms step_avg:98.66ms
step:1618/1750 train_time:159638ms step_avg:98.66ms
step:1619/1750 train_time:159742ms step_avg:98.67ms
step:1620/1750 train_time:159845ms step_avg:98.67ms
step:1621/1750 train_time:159947ms step_avg:98.67ms
step:1622/1750 train_time:160049ms step_avg:98.67ms
step:1623/1750 train_time:160153ms step_avg:98.68ms
step:1624/1750 train_time:160258ms step_avg:98.68ms
step:1625/1750 train_time:160362ms step_avg:98.68ms
step:1625/1750 val_loss:3.2985 train_time:160460ms step_avg:98.74ms
step:1626/1750 train_time:160486ms step_avg:98.70ms
step:1627/1750 train_time:160577ms step_avg:98.70ms
step:1628/1750 train_time:160679ms step_avg:98.70ms
step:1629/1750 train_time:160782ms step_avg:98.70ms
step:1630/1750 train_time:160885ms step_avg:98.70ms
step:1631/1750 train_time:160989ms step_avg:98.71ms
step:1632/1750 train_time:161092ms step_avg:98.71ms
step:1633/1750 train_time:161194ms step_avg:98.71ms
step:1634/1750 train_time:161298ms step_avg:98.71ms
step:1635/1750 train_time:161400ms step_avg:98.72ms
step:1636/1750 train_time:161507ms step_avg:98.72ms
step:1637/1750 train_time:161611ms step_avg:98.72ms
step:1638/1750 train_time:161714ms step_avg:98.73ms
step:1639/1750 train_time:161817ms step_avg:98.73ms
step:1640/1750 train_time:161921ms step_avg:98.73ms
step:1641/1750 train_time:162024ms step_avg:98.73ms
step:1642/1750 train_time:162126ms step_avg:98.74ms
step:1643/1750 train_time:162229ms step_avg:98.74ms
step:1644/1750 train_time:162332ms step_avg:98.74ms
step:1645/1750 train_time:162434ms step_avg:98.74ms
step:1646/1750 train_time:162537ms step_avg:98.75ms
step:1647/1750 train_time:162642ms step_avg:98.75ms
step:1648/1750 train_time:162746ms step_avg:98.75ms
step:1649/1750 train_time:162849ms step_avg:98.76ms
step:1650/1750 train_time:162953ms step_avg:98.76ms
step:1651/1750 train_time:163056ms step_avg:98.76ms
step:1652/1750 train_time:163160ms step_avg:98.77ms
step:1653/1750 train_time:163263ms step_avg:98.77ms
step:1654/1750 train_time:163365ms step_avg:98.77ms
step:1655/1750 train_time:163468ms step_avg:98.77ms
step:1656/1750 train_time:163572ms step_avg:98.78ms
step:1657/1750 train_time:163673ms step_avg:98.78ms
step:1658/1750 train_time:163776ms step_avg:98.78ms
step:1659/1750 train_time:163885ms step_avg:98.79ms
step:1660/1750 train_time:163987ms step_avg:98.79ms
step:1661/1750 train_time:164092ms step_avg:98.79ms
step:1662/1750 train_time:164196ms step_avg:98.79ms
step:1663/1750 train_time:164299ms step_avg:98.80ms
step:1664/1750 train_time:164402ms step_avg:98.80ms
step:1665/1750 train_time:164506ms step_avg:98.80ms
step:1666/1750 train_time:164609ms step_avg:98.80ms
step:1667/1750 train_time:164712ms step_avg:98.81ms
step:1668/1750 train_time:164816ms step_avg:98.81ms
step:1669/1750 train_time:164922ms step_avg:98.82ms
step:1670/1750 train_time:165025ms step_avg:98.82ms
step:1671/1750 train_time:165128ms step_avg:98.82ms
step:1672/1750 train_time:165232ms step_avg:98.82ms
step:1673/1750 train_time:165335ms step_avg:98.83ms
step:1674/1750 train_time:165439ms step_avg:98.83ms
step:1675/1750 train_time:165542ms step_avg:98.83ms
step:1676/1750 train_time:165645ms step_avg:98.83ms
step:1677/1750 train_time:165748ms step_avg:98.84ms
step:1678/1750 train_time:165852ms step_avg:98.84ms
step:1679/1750 train_time:165956ms step_avg:98.84ms
step:1680/1750 train_time:166059ms step_avg:98.84ms
step:1681/1750 train_time:166162ms step_avg:98.85ms
step:1682/1750 train_time:166268ms step_avg:98.85ms
step:1683/1750 train_time:166372ms step_avg:98.85ms
step:1684/1750 train_time:166476ms step_avg:98.86ms
step:1685/1750 train_time:166580ms step_avg:98.86ms
step:1686/1750 train_time:166682ms step_avg:98.86ms
step:1687/1750 train_time:166785ms step_avg:98.86ms
step:1688/1750 train_time:166889ms step_avg:98.87ms
step:1689/1750 train_time:166994ms step_avg:98.87ms
step:1690/1750 train_time:167098ms step_avg:98.87ms
step:1691/1750 train_time:167202ms step_avg:98.88ms
step:1692/1750 train_time:167305ms step_avg:98.88ms
step:1693/1750 train_time:167411ms step_avg:98.88ms
step:1694/1750 train_time:167514ms step_avg:98.89ms
step:1695/1750 train_time:167619ms step_avg:98.89ms
step:1696/1750 train_time:167722ms step_avg:98.89ms
step:1697/1750 train_time:167829ms step_avg:98.90ms
step:1698/1750 train_time:167934ms step_avg:98.90ms
step:1699/1750 train_time:168037ms step_avg:98.90ms
step:1700/1750 train_time:168142ms step_avg:98.91ms
step:1701/1750 train_time:168244ms step_avg:98.91ms
step:1702/1750 train_time:168351ms step_avg:98.91ms
step:1703/1750 train_time:168455ms step_avg:98.92ms
step:1704/1750 train_time:168558ms step_avg:98.92ms
step:1705/1750 train_time:168662ms step_avg:98.92ms
step:1706/1750 train_time:168766ms step_avg:98.93ms
step:1707/1750 train_time:168872ms step_avg:98.93ms
step:1708/1750 train_time:168977ms step_avg:98.93ms
step:1709/1750 train_time:169080ms step_avg:98.94ms
step:1710/1750 train_time:169185ms step_avg:98.94ms
step:1711/1750 train_time:169291ms step_avg:98.94ms
step:1712/1750 train_time:169394ms step_avg:98.95ms
step:1713/1750 train_time:169500ms step_avg:98.95ms
step:1714/1750 train_time:169603ms step_avg:98.95ms
step:1715/1750 train_time:169709ms step_avg:98.96ms
step:1716/1750 train_time:169814ms step_avg:98.96ms
step:1717/1750 train_time:169918ms step_avg:98.96ms
step:1718/1750 train_time:170022ms step_avg:98.97ms
step:1719/1750 train_time:170129ms step_avg:98.97ms
step:1720/1750 train_time:170232ms step_avg:98.97ms
step:1721/1750 train_time:170337ms step_avg:98.98ms
step:1722/1750 train_time:170441ms step_avg:98.98ms
step:1723/1750 train_time:170545ms step_avg:98.98ms
step:1724/1750 train_time:170649ms step_avg:98.98ms
step:1725/1750 train_time:170753ms step_avg:98.99ms
step:1726/1750 train_time:170857ms step_avg:98.99ms
step:1727/1750 train_time:170961ms step_avg:98.99ms
step:1728/1750 train_time:171066ms step_avg:99.00ms
step:1729/1750 train_time:171171ms step_avg:99.00ms
step:1730/1750 train_time:171275ms step_avg:99.00ms
step:1731/1750 train_time:171379ms step_avg:99.01ms
step:1732/1750 train_time:171483ms step_avg:99.01ms
step:1733/1750 train_time:171586ms step_avg:99.01ms
step:1734/1750 train_time:171692ms step_avg:99.01ms
step:1735/1750 train_time:171795ms step_avg:99.02ms
step:1736/1750 train_time:171899ms step_avg:99.02ms
step:1737/1750 train_time:172004ms step_avg:99.02ms
step:1738/1750 train_time:172107ms step_avg:99.03ms
step:1739/1750 train_time:172211ms step_avg:99.03ms
step:1740/1750 train_time:172315ms step_avg:99.03ms
step:1741/1750 train_time:172424ms step_avg:99.04ms
step:1742/1750 train_time:172528ms step_avg:99.04ms
step:1743/1750 train_time:172634ms step_avg:99.04ms
step:1744/1750 train_time:172737ms step_avg:99.05ms
step:1745/1750 train_time:172843ms step_avg:99.05ms
step:1746/1750 train_time:172946ms step_avg:99.05ms
step:1747/1750 train_time:173050ms step_avg:99.06ms
step:1748/1750 train_time:173154ms step_avg:99.06ms
step:1749/1750 train_time:173258ms step_avg:99.06ms
step:1750/1750 train_time:173363ms step_avg:99.06ms
step:1750/1750 val_loss:3.2778 train_time:173462ms step_avg:99.12ms
peak memory allocated: 33277 MiB reserved: 48512 MiB

import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 01:45:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    5856MiB /  81559MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           91688      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           91689      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           91690      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           91691      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           91692      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           91693      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           91694      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           91695      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           91689      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           91690      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           91691      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           91692      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           91693      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           91694      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           91695      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:151ms step_avg:151.14ms
step:2/1750 train_time:175ms step_avg:87.54ms
step:3/1750 train_time:249ms step_avg:83.11ms
step:4/1750 train_time:341ms step_avg:85.14ms
step:5/1750 train_time:433ms step_avg:86.61ms
step:6/1750 train_time:525ms step_avg:87.56ms
step:7/1750 train_time:618ms step_avg:88.29ms
step:8/1750 train_time:711ms step_avg:88.86ms
step:9/1750 train_time:804ms step_avg:89.32ms
step:10/1750 train_time:896ms step_avg:89.63ms
step:11/1750 train_time:989ms step_avg:89.92ms
step:12/1750 train_time:1083ms step_avg:90.24ms
step:13/1750 train_time:1177ms step_avg:90.56ms
step:14/1750 train_time:1272ms step_avg:90.88ms
step:15/1750 train_time:1366ms step_avg:91.04ms
step:16/1750 train_time:1459ms step_avg:91.16ms
step:17/1750 train_time:1552ms step_avg:91.29ms
step:18/1750 train_time:1644ms step_avg:91.36ms
step:19/1750 train_time:1739ms step_avg:91.50ms
step:20/1750 train_time:1830ms step_avg:91.52ms
step:21/1750 train_time:1923ms step_avg:91.58ms
step:22/1750 train_time:2016ms step_avg:91.64ms
step:23/1750 train_time:2110ms step_avg:91.76ms
step:24/1750 train_time:2204ms step_avg:91.84ms
step:25/1750 train_time:2298ms step_avg:91.91ms
step:26/1750 train_time:2391ms step_avg:91.98ms
step:27/1750 train_time:2485ms step_avg:92.04ms
step:28/1750 train_time:2578ms step_avg:92.06ms
step:29/1750 train_time:2671ms step_avg:92.09ms
step:30/1750 train_time:2763ms step_avg:92.10ms
step:31/1750 train_time:2856ms step_avg:92.12ms
step:32/1750 train_time:2950ms step_avg:92.18ms
step:33/1750 train_time:3043ms step_avg:92.22ms
step:34/1750 train_time:3136ms step_avg:92.24ms
step:35/1750 train_time:3230ms step_avg:92.29ms
step:36/1750 train_time:3324ms step_avg:92.34ms
step:37/1750 train_time:3418ms step_avg:92.37ms
step:38/1750 train_time:3511ms step_avg:92.39ms
step:39/1750 train_time:3604ms step_avg:92.40ms
step:40/1750 train_time:3696ms step_avg:92.41ms
step:41/1750 train_time:3790ms step_avg:92.43ms
step:42/1750 train_time:3883ms step_avg:92.45ms
step:43/1750 train_time:3976ms step_avg:92.46ms
step:44/1750 train_time:4069ms step_avg:92.48ms
step:45/1750 train_time:4163ms step_avg:92.50ms
step:46/1750 train_time:4256ms step_avg:92.53ms
step:47/1750 train_time:4351ms step_avg:92.56ms
step:48/1750 train_time:4444ms step_avg:92.58ms
step:49/1750 train_time:4537ms step_avg:92.60ms
step:50/1750 train_time:4631ms step_avg:92.62ms
step:51/1750 train_time:4724ms step_avg:92.63ms
step:52/1750 train_time:4817ms step_avg:92.63ms
step:53/1750 train_time:4911ms step_avg:92.65ms
step:54/1750 train_time:5005ms step_avg:92.68ms
step:55/1750 train_time:5098ms step_avg:92.68ms
step:56/1750 train_time:5191ms step_avg:92.70ms
step:57/1750 train_time:5285ms step_avg:92.72ms
step:58/1750 train_time:5379ms step_avg:92.74ms
step:59/1750 train_time:5471ms step_avg:92.74ms
step:60/1750 train_time:5564ms step_avg:92.74ms
step:61/1750 train_time:5658ms step_avg:92.75ms
step:62/1750 train_time:5750ms step_avg:92.75ms
step:63/1750 train_time:5843ms step_avg:92.75ms
step:64/1750 train_time:5935ms step_avg:92.74ms
step:65/1750 train_time:6029ms step_avg:92.75ms
step:66/1750 train_time:6122ms step_avg:92.76ms
step:67/1750 train_time:6215ms step_avg:92.77ms
step:68/1750 train_time:6309ms step_avg:92.78ms
step:69/1750 train_time:6402ms step_avg:92.78ms
step:70/1750 train_time:6495ms step_avg:92.78ms
step:71/1750 train_time:6588ms step_avg:92.79ms
step:72/1750 train_time:6681ms step_avg:92.80ms
step:73/1750 train_time:6774ms step_avg:92.79ms
step:74/1750 train_time:6868ms step_avg:92.81ms
step:75/1750 train_time:6959ms step_avg:92.79ms
step:76/1750 train_time:7052ms step_avg:92.79ms
step:77/1750 train_time:7146ms step_avg:92.80ms
step:78/1750 train_time:7238ms step_avg:92.80ms
step:79/1750 train_time:7332ms step_avg:92.81ms
step:80/1750 train_time:7425ms step_avg:92.82ms
step:81/1750 train_time:7519ms step_avg:92.82ms
step:82/1750 train_time:7612ms step_avg:92.82ms
step:83/1750 train_time:7705ms step_avg:92.83ms
step:84/1750 train_time:7798ms step_avg:92.83ms
step:85/1750 train_time:7891ms step_avg:92.83ms
step:86/1750 train_time:7984ms step_avg:92.83ms
step:87/1750 train_time:8077ms step_avg:92.84ms
step:88/1750 train_time:8170ms step_avg:92.84ms
step:89/1750 train_time:8264ms step_avg:92.86ms
step:90/1750 train_time:8358ms step_avg:92.86ms
step:91/1750 train_time:8452ms step_avg:92.88ms
step:92/1750 train_time:8545ms step_avg:92.88ms
step:93/1750 train_time:8638ms step_avg:92.89ms
step:94/1750 train_time:8732ms step_avg:92.89ms
step:95/1750 train_time:8826ms step_avg:92.90ms
step:96/1750 train_time:8919ms step_avg:92.90ms
step:97/1750 train_time:9012ms step_avg:92.90ms
step:98/1750 train_time:9104ms step_avg:92.90ms
step:99/1750 train_time:9197ms step_avg:92.90ms
step:100/1750 train_time:9290ms step_avg:92.90ms
step:101/1750 train_time:9384ms step_avg:92.91ms
step:102/1750 train_time:9477ms step_avg:92.91ms
step:103/1750 train_time:9570ms step_avg:92.92ms
step:104/1750 train_time:9664ms step_avg:92.93ms
step:105/1750 train_time:9757ms step_avg:92.92ms
step:106/1750 train_time:9851ms step_avg:92.93ms
step:107/1750 train_time:9945ms step_avg:92.94ms
step:108/1750 train_time:10037ms step_avg:92.94ms
step:109/1750 train_time:10130ms step_avg:92.94ms
step:110/1750 train_time:10224ms step_avg:92.94ms
step:111/1750 train_time:10317ms step_avg:92.95ms
step:112/1750 train_time:10411ms step_avg:92.95ms
step:113/1750 train_time:10504ms step_avg:92.95ms
step:114/1750 train_time:10596ms step_avg:92.95ms
step:115/1750 train_time:10689ms step_avg:92.95ms
step:116/1750 train_time:10782ms step_avg:92.95ms
step:117/1750 train_time:10876ms step_avg:92.95ms
step:118/1750 train_time:10970ms step_avg:92.96ms
step:119/1750 train_time:11064ms step_avg:92.97ms
step:120/1750 train_time:11157ms step_avg:92.98ms
step:121/1750 train_time:11251ms step_avg:92.98ms
step:122/1750 train_time:11344ms step_avg:92.99ms
step:123/1750 train_time:11438ms step_avg:92.99ms
step:124/1750 train_time:11530ms step_avg:92.99ms
step:125/1750 train_time:11623ms step_avg:92.99ms
step:125/1750 val_loss:4.6529 train_time:11712ms step_avg:93.69ms
step:126/1750 train_time:11737ms step_avg:93.15ms
step:127/1750 train_time:11815ms step_avg:93.03ms
step:128/1750 train_time:11914ms step_avg:93.08ms
step:129/1750 train_time:12010ms step_avg:93.10ms
step:130/1750 train_time:12102ms step_avg:93.09ms
step:131/1750 train_time:12195ms step_avg:93.09ms
step:132/1750 train_time:12287ms step_avg:93.09ms
step:133/1750 train_time:12380ms step_avg:93.08ms
step:134/1750 train_time:12472ms step_avg:93.08ms
step:135/1750 train_time:12565ms step_avg:93.07ms
step:136/1750 train_time:12658ms step_avg:93.07ms
step:137/1750 train_time:12752ms step_avg:93.08ms
step:138/1750 train_time:12847ms step_avg:93.09ms
step:139/1750 train_time:12942ms step_avg:93.10ms
step:140/1750 train_time:13036ms step_avg:93.12ms
step:141/1750 train_time:13130ms step_avg:93.12ms
step:142/1750 train_time:13223ms step_avg:93.12ms
step:143/1750 train_time:13316ms step_avg:93.12ms
step:144/1750 train_time:13409ms step_avg:93.12ms
step:145/1750 train_time:13502ms step_avg:93.12ms
step:146/1750 train_time:13595ms step_avg:93.12ms
step:147/1750 train_time:13689ms step_avg:93.12ms
step:148/1750 train_time:13783ms step_avg:93.13ms
step:149/1750 train_time:13877ms step_avg:93.13ms
step:150/1750 train_time:13972ms step_avg:93.15ms
step:151/1750 train_time:14066ms step_avg:93.15ms
step:152/1750 train_time:14159ms step_avg:93.15ms
step:153/1750 train_time:14252ms step_avg:93.15ms
step:154/1750 train_time:14346ms step_avg:93.15ms
step:155/1750 train_time:14439ms step_avg:93.16ms
step:156/1750 train_time:14532ms step_avg:93.15ms
step:157/1750 train_time:14625ms step_avg:93.15ms
step:158/1750 train_time:14719ms step_avg:93.16ms
step:159/1750 train_time:14814ms step_avg:93.17ms
step:160/1750 train_time:14908ms step_avg:93.17ms
step:161/1750 train_time:15001ms step_avg:93.18ms
step:162/1750 train_time:15095ms step_avg:93.18ms
step:163/1750 train_time:15190ms step_avg:93.19ms
step:164/1750 train_time:15283ms step_avg:93.19ms
step:165/1750 train_time:15377ms step_avg:93.19ms
step:166/1750 train_time:15470ms step_avg:93.19ms
step:167/1750 train_time:15563ms step_avg:93.19ms
step:168/1750 train_time:15657ms step_avg:93.19ms
step:169/1750 train_time:15750ms step_avg:93.19ms
step:170/1750 train_time:15843ms step_avg:93.20ms
step:171/1750 train_time:15938ms step_avg:93.20ms
step:172/1750 train_time:16032ms step_avg:93.21ms
step:173/1750 train_time:16126ms step_avg:93.21ms
step:174/1750 train_time:16220ms step_avg:93.22ms
step:175/1750 train_time:16313ms step_avg:93.22ms
step:176/1750 train_time:16407ms step_avg:93.22ms
step:177/1750 train_time:16501ms step_avg:93.22ms
step:178/1750 train_time:16594ms step_avg:93.22ms
step:179/1750 train_time:16687ms step_avg:93.22ms
step:180/1750 train_time:16781ms step_avg:93.23ms
step:181/1750 train_time:16875ms step_avg:93.23ms
step:182/1750 train_time:16968ms step_avg:93.23ms
step:183/1750 train_time:17062ms step_avg:93.24ms
step:184/1750 train_time:17156ms step_avg:93.24ms
step:185/1750 train_time:17250ms step_avg:93.24ms
step:186/1750 train_time:17343ms step_avg:93.24ms
step:187/1750 train_time:17436ms step_avg:93.24ms
step:188/1750 train_time:17530ms step_avg:93.24ms
step:189/1750 train_time:17623ms step_avg:93.24ms
step:190/1750 train_time:17717ms step_avg:93.25ms
step:191/1750 train_time:17811ms step_avg:93.25ms
step:192/1750 train_time:17904ms step_avg:93.25ms
step:193/1750 train_time:17998ms step_avg:93.25ms
step:194/1750 train_time:18092ms step_avg:93.26ms
step:195/1750 train_time:18185ms step_avg:93.26ms
step:196/1750 train_time:18279ms step_avg:93.26ms
step:197/1750 train_time:18374ms step_avg:93.27ms
step:198/1750 train_time:18468ms step_avg:93.27ms
step:199/1750 train_time:18561ms step_avg:93.27ms
step:200/1750 train_time:18656ms step_avg:93.28ms
step:201/1750 train_time:18749ms step_avg:93.28ms
step:202/1750 train_time:18843ms step_avg:93.28ms
step:203/1750 train_time:18936ms step_avg:93.28ms
step:204/1750 train_time:19029ms step_avg:93.28ms
step:205/1750 train_time:19123ms step_avg:93.28ms
step:206/1750 train_time:19217ms step_avg:93.29ms
step:207/1750 train_time:19311ms step_avg:93.29ms
step:208/1750 train_time:19404ms step_avg:93.29ms
step:209/1750 train_time:19498ms step_avg:93.29ms
step:210/1750 train_time:19592ms step_avg:93.29ms
step:211/1750 train_time:19686ms step_avg:93.30ms
step:212/1750 train_time:19779ms step_avg:93.30ms
step:213/1750 train_time:19877ms step_avg:93.32ms
step:214/1750 train_time:19967ms step_avg:93.31ms
step:215/1750 train_time:20061ms step_avg:93.31ms
step:216/1750 train_time:20154ms step_avg:93.31ms
step:217/1750 train_time:20248ms step_avg:93.31ms
step:218/1750 train_time:20342ms step_avg:93.31ms
step:219/1750 train_time:20436ms step_avg:93.32ms
step:220/1750 train_time:20530ms step_avg:93.32ms
step:221/1750 train_time:20623ms step_avg:93.32ms
step:222/1750 train_time:20718ms step_avg:93.32ms
step:223/1750 train_time:20811ms step_avg:93.32ms
step:224/1750 train_time:20904ms step_avg:93.32ms
step:225/1750 train_time:20998ms step_avg:93.33ms
step:226/1750 train_time:21092ms step_avg:93.33ms
step:227/1750 train_time:21184ms step_avg:93.32ms
step:228/1750 train_time:21279ms step_avg:93.33ms
step:229/1750 train_time:21372ms step_avg:93.33ms
step:230/1750 train_time:21466ms step_avg:93.33ms
step:231/1750 train_time:21560ms step_avg:93.33ms
step:232/1750 train_time:21653ms step_avg:93.33ms
step:233/1750 train_time:21747ms step_avg:93.33ms
step:234/1750 train_time:21841ms step_avg:93.34ms
step:235/1750 train_time:21935ms step_avg:93.34ms
step:236/1750 train_time:22028ms step_avg:93.34ms
step:237/1750 train_time:22121ms step_avg:93.34ms
step:238/1750 train_time:22215ms step_avg:93.34ms
step:239/1750 train_time:22308ms step_avg:93.34ms
step:240/1750 train_time:22402ms step_avg:93.34ms
step:241/1750 train_time:22496ms step_avg:93.34ms
step:242/1750 train_time:22590ms step_avg:93.35ms
step:243/1750 train_time:22683ms step_avg:93.35ms
step:244/1750 train_time:22777ms step_avg:93.35ms
step:245/1750 train_time:22871ms step_avg:93.35ms
step:246/1750 train_time:22964ms step_avg:93.35ms
step:247/1750 train_time:23058ms step_avg:93.35ms
step:248/1750 train_time:23151ms step_avg:93.35ms
step:249/1750 train_time:23245ms step_avg:93.35ms
step:250/1750 train_time:23339ms step_avg:93.36ms
step:250/1750 val_loss:4.1043 train_time:23428ms step_avg:93.71ms
step:251/1750 train_time:23453ms step_avg:93.44ms
step:252/1750 train_time:23535ms step_avg:93.39ms
step:253/1750 train_time:23636ms step_avg:93.42ms
step:254/1750 train_time:23731ms step_avg:93.43ms
step:255/1750 train_time:23824ms step_avg:93.43ms
step:256/1750 train_time:23917ms step_avg:93.43ms
step:257/1750 train_time:24010ms step_avg:93.42ms
step:258/1750 train_time:24103ms step_avg:93.42ms
step:259/1750 train_time:24196ms step_avg:93.42ms
step:260/1750 train_time:24289ms step_avg:93.42ms
step:261/1750 train_time:24382ms step_avg:93.42ms
step:262/1750 train_time:24476ms step_avg:93.42ms
step:263/1750 train_time:24573ms step_avg:93.43ms
step:264/1750 train_time:24670ms step_avg:93.45ms
step:265/1750 train_time:24764ms step_avg:93.45ms
step:266/1750 train_time:24857ms step_avg:93.45ms
step:267/1750 train_time:24951ms step_avg:93.45ms
step:268/1750 train_time:25045ms step_avg:93.45ms
step:269/1750 train_time:25139ms step_avg:93.45ms
step:270/1750 train_time:25232ms step_avg:93.45ms
step:271/1750 train_time:25326ms step_avg:93.45ms
step:272/1750 train_time:25420ms step_avg:93.46ms
step:273/1750 train_time:25515ms step_avg:93.46ms
step:274/1750 train_time:25611ms step_avg:93.47ms
step:275/1750 train_time:25705ms step_avg:93.47ms
step:276/1750 train_time:25801ms step_avg:93.48ms
step:277/1750 train_time:25895ms step_avg:93.49ms
step:278/1750 train_time:25990ms step_avg:93.49ms
step:279/1750 train_time:26084ms step_avg:93.49ms
step:280/1750 train_time:26178ms step_avg:93.49ms
step:281/1750 train_time:26271ms step_avg:93.49ms
step:282/1750 train_time:26365ms step_avg:93.49ms
step:283/1750 train_time:26459ms step_avg:93.49ms
step:284/1750 train_time:26553ms step_avg:93.50ms
step:285/1750 train_time:26647ms step_avg:93.50ms
step:286/1750 train_time:26742ms step_avg:93.50ms
step:287/1750 train_time:26836ms step_avg:93.51ms
step:288/1750 train_time:26930ms step_avg:93.51ms
step:289/1750 train_time:27024ms step_avg:93.51ms
step:290/1750 train_time:27118ms step_avg:93.51ms
step:291/1750 train_time:27213ms step_avg:93.51ms
step:292/1750 train_time:27306ms step_avg:93.51ms
step:293/1750 train_time:27400ms step_avg:93.51ms
step:294/1750 train_time:27494ms step_avg:93.52ms
step:295/1750 train_time:27588ms step_avg:93.52ms
step:296/1750 train_time:27682ms step_avg:93.52ms
step:297/1750 train_time:27777ms step_avg:93.53ms
step:298/1750 train_time:27871ms step_avg:93.53ms
step:299/1750 train_time:27965ms step_avg:93.53ms
step:300/1750 train_time:28075ms step_avg:93.58ms
step:301/1750 train_time:28153ms step_avg:93.53ms
step:302/1750 train_time:28246ms step_avg:93.53ms
step:303/1750 train_time:28341ms step_avg:93.53ms
step:304/1750 train_time:28435ms step_avg:93.54ms
step:305/1750 train_time:28529ms step_avg:93.54ms
step:306/1750 train_time:28623ms step_avg:93.54ms
step:307/1750 train_time:28718ms step_avg:93.54ms
step:308/1750 train_time:28812ms step_avg:93.54ms
step:309/1750 train_time:28905ms step_avg:93.54ms
step:310/1750 train_time:29000ms step_avg:93.55ms
step:311/1750 train_time:29094ms step_avg:93.55ms
step:312/1750 train_time:29188ms step_avg:93.55ms
step:313/1750 train_time:29282ms step_avg:93.55ms
step:314/1750 train_time:29376ms step_avg:93.55ms
step:315/1750 train_time:29470ms step_avg:93.56ms
step:316/1750 train_time:29564ms step_avg:93.56ms
step:317/1750 train_time:29658ms step_avg:93.56ms
step:318/1750 train_time:29752ms step_avg:93.56ms
step:319/1750 train_time:29846ms step_avg:93.56ms
step:320/1750 train_time:29940ms step_avg:93.56ms
step:321/1750 train_time:30034ms step_avg:93.56ms
step:322/1750 train_time:30129ms step_avg:93.57ms
step:323/1750 train_time:30223ms step_avg:93.57ms
step:324/1750 train_time:30317ms step_avg:93.57ms
step:325/1750 train_time:30411ms step_avg:93.57ms
step:326/1750 train_time:30505ms step_avg:93.57ms
step:327/1750 train_time:30600ms step_avg:93.58ms
step:328/1750 train_time:30694ms step_avg:93.58ms
step:329/1750 train_time:30788ms step_avg:93.58ms
step:330/1750 train_time:30883ms step_avg:93.58ms
step:331/1750 train_time:30976ms step_avg:93.58ms
step:332/1750 train_time:31070ms step_avg:93.59ms
step:333/1750 train_time:31164ms step_avg:93.59ms
step:334/1750 train_time:31258ms step_avg:93.59ms
step:335/1750 train_time:31351ms step_avg:93.59ms
step:336/1750 train_time:31445ms step_avg:93.59ms
step:337/1750 train_time:31539ms step_avg:93.59ms
step:338/1750 train_time:31633ms step_avg:93.59ms
step:339/1750 train_time:31727ms step_avg:93.59ms
step:340/1750 train_time:31822ms step_avg:93.59ms
step:341/1750 train_time:31917ms step_avg:93.60ms
step:342/1750 train_time:32010ms step_avg:93.60ms
step:343/1750 train_time:32104ms step_avg:93.60ms
step:344/1750 train_time:32198ms step_avg:93.60ms
step:345/1750 train_time:32293ms step_avg:93.60ms
step:346/1750 train_time:32387ms step_avg:93.60ms
step:347/1750 train_time:32480ms step_avg:93.60ms
step:348/1750 train_time:32574ms step_avg:93.60ms
step:349/1750 train_time:32668ms step_avg:93.60ms
step:350/1750 train_time:32762ms step_avg:93.60ms
step:351/1750 train_time:32856ms step_avg:93.61ms
step:352/1750 train_time:32951ms step_avg:93.61ms
step:353/1750 train_time:33044ms step_avg:93.61ms
step:354/1750 train_time:33138ms step_avg:93.61ms
step:355/1750 train_time:33232ms step_avg:93.61ms
step:356/1750 train_time:33327ms step_avg:93.61ms
step:357/1750 train_time:33420ms step_avg:93.61ms
step:358/1750 train_time:33514ms step_avg:93.61ms
step:359/1750 train_time:33608ms step_avg:93.61ms
step:360/1750 train_time:33702ms step_avg:93.62ms
step:361/1750 train_time:33796ms step_avg:93.62ms
step:362/1750 train_time:33890ms step_avg:93.62ms
step:363/1750 train_time:33984ms step_avg:93.62ms
step:364/1750 train_time:34078ms step_avg:93.62ms
step:365/1750 train_time:34172ms step_avg:93.62ms
step:366/1750 train_time:34266ms step_avg:93.62ms
step:367/1750 train_time:34361ms step_avg:93.63ms
step:368/1750 train_time:34455ms step_avg:93.63ms
step:369/1750 train_time:34549ms step_avg:93.63ms
step:370/1750 train_time:34643ms step_avg:93.63ms
step:371/1750 train_time:34737ms step_avg:93.63ms
step:372/1750 train_time:34831ms step_avg:93.63ms
step:373/1750 train_time:34925ms step_avg:93.63ms
step:374/1750 train_time:35019ms step_avg:93.63ms
step:375/1750 train_time:35113ms step_avg:93.64ms
step:375/1750 val_loss:3.8988 train_time:35202ms step_avg:93.87ms
step:376/1750 train_time:35227ms step_avg:93.69ms
step:377/1750 train_time:35308ms step_avg:93.65ms
step:378/1750 train_time:35405ms step_avg:93.67ms
step:379/1750 train_time:35500ms step_avg:93.67ms
step:380/1750 train_time:35594ms step_avg:93.67ms
step:381/1750 train_time:35687ms step_avg:93.67ms
step:382/1750 train_time:35781ms step_avg:93.67ms
step:383/1750 train_time:35875ms step_avg:93.67ms
step:384/1750 train_time:35968ms step_avg:93.67ms
step:385/1750 train_time:36061ms step_avg:93.66ms
step:386/1750 train_time:36155ms step_avg:93.67ms
step:387/1750 train_time:36250ms step_avg:93.67ms
step:388/1750 train_time:36345ms step_avg:93.67ms
step:389/1750 train_time:36441ms step_avg:93.68ms
step:390/1750 train_time:36536ms step_avg:93.68ms
step:391/1750 train_time:36633ms step_avg:93.69ms
step:392/1750 train_time:36729ms step_avg:93.70ms
step:393/1750 train_time:36824ms step_avg:93.70ms
step:394/1750 train_time:36920ms step_avg:93.71ms
step:395/1750 train_time:37016ms step_avg:93.71ms
step:396/1750 train_time:37112ms step_avg:93.72ms
step:397/1750 train_time:37208ms step_avg:93.72ms
step:398/1750 train_time:37304ms step_avg:93.73ms
step:399/1750 train_time:37402ms step_avg:93.74ms
step:400/1750 train_time:37499ms step_avg:93.75ms
step:401/1750 train_time:37595ms step_avg:93.75ms
step:402/1750 train_time:37692ms step_avg:93.76ms
step:403/1750 train_time:37788ms step_avg:93.77ms
step:404/1750 train_time:37884ms step_avg:93.77ms
step:405/1750 train_time:37980ms step_avg:93.78ms
step:406/1750 train_time:38077ms step_avg:93.78ms
step:407/1750 train_time:38172ms step_avg:93.79ms
step:408/1750 train_time:38269ms step_avg:93.80ms
step:409/1750 train_time:38365ms step_avg:93.80ms
step:410/1750 train_time:38462ms step_avg:93.81ms
step:411/1750 train_time:38558ms step_avg:93.81ms
step:412/1750 train_time:38654ms step_avg:93.82ms
step:413/1750 train_time:38751ms step_avg:93.83ms
step:414/1750 train_time:38846ms step_avg:93.83ms
step:415/1750 train_time:38942ms step_avg:93.84ms
step:416/1750 train_time:39039ms step_avg:93.84ms
step:417/1750 train_time:39135ms step_avg:93.85ms
step:418/1750 train_time:39231ms step_avg:93.85ms
step:419/1750 train_time:39328ms step_avg:93.86ms
step:420/1750 train_time:39424ms step_avg:93.87ms
step:421/1750 train_time:39522ms step_avg:93.88ms
step:422/1750 train_time:39619ms step_avg:93.88ms
step:423/1750 train_time:39715ms step_avg:93.89ms
step:424/1750 train_time:39811ms step_avg:93.89ms
step:425/1750 train_time:39907ms step_avg:93.90ms
step:426/1750 train_time:40004ms step_avg:93.91ms
step:427/1750 train_time:40099ms step_avg:93.91ms
step:428/1750 train_time:40196ms step_avg:93.92ms
step:429/1750 train_time:40292ms step_avg:93.92ms
step:430/1750 train_time:40389ms step_avg:93.93ms
step:431/1750 train_time:40485ms step_avg:93.93ms
step:432/1750 train_time:40582ms step_avg:93.94ms
step:433/1750 train_time:40679ms step_avg:93.95ms
step:434/1750 train_time:40775ms step_avg:93.95ms
step:435/1750 train_time:40871ms step_avg:93.96ms
step:436/1750 train_time:40967ms step_avg:93.96ms
step:437/1750 train_time:41063ms step_avg:93.97ms
step:438/1750 train_time:41159ms step_avg:93.97ms
step:439/1750 train_time:41255ms step_avg:93.98ms
step:440/1750 train_time:41351ms step_avg:93.98ms
step:441/1750 train_time:41449ms step_avg:93.99ms
step:442/1750 train_time:41544ms step_avg:93.99ms
step:443/1750 train_time:41641ms step_avg:94.00ms
step:444/1750 train_time:41738ms step_avg:94.00ms
step:445/1750 train_time:41835ms step_avg:94.01ms
step:446/1750 train_time:41932ms step_avg:94.02ms
step:447/1750 train_time:42028ms step_avg:94.02ms
step:448/1750 train_time:42124ms step_avg:94.03ms
step:449/1750 train_time:42220ms step_avg:94.03ms
step:450/1750 train_time:42317ms step_avg:94.04ms
step:451/1750 train_time:42413ms step_avg:94.04ms
step:452/1750 train_time:42509ms step_avg:94.05ms
step:453/1750 train_time:42605ms step_avg:94.05ms
step:454/1750 train_time:42702ms step_avg:94.06ms
step:455/1750 train_time:42799ms step_avg:94.06ms
step:456/1750 train_time:42895ms step_avg:94.07ms
step:457/1750 train_time:42992ms step_avg:94.07ms
step:458/1750 train_time:43088ms step_avg:94.08ms
step:459/1750 train_time:43184ms step_avg:94.08ms
step:460/1750 train_time:43281ms step_avg:94.09ms
step:461/1750 train_time:43378ms step_avg:94.10ms
step:462/1750 train_time:43474ms step_avg:94.10ms
step:463/1750 train_time:43571ms step_avg:94.11ms
step:464/1750 train_time:43667ms step_avg:94.11ms
step:465/1750 train_time:43762ms step_avg:94.11ms
step:466/1750 train_time:43859ms step_avg:94.12ms
step:467/1750 train_time:43956ms step_avg:94.12ms
step:468/1750 train_time:44052ms step_avg:94.13ms
step:469/1750 train_time:44149ms step_avg:94.13ms
step:470/1750 train_time:44245ms step_avg:94.14ms
step:471/1750 train_time:44342ms step_avg:94.14ms
step:472/1750 train_time:44439ms step_avg:94.15ms
step:473/1750 train_time:44536ms step_avg:94.16ms
step:474/1750 train_time:44632ms step_avg:94.16ms
step:475/1750 train_time:44728ms step_avg:94.17ms
step:476/1750 train_time:44824ms step_avg:94.17ms
step:477/1750 train_time:44921ms step_avg:94.17ms
step:478/1750 train_time:45018ms step_avg:94.18ms
step:479/1750 train_time:45116ms step_avg:94.19ms
step:480/1750 train_time:45212ms step_avg:94.19ms
step:481/1750 train_time:45308ms step_avg:94.20ms
step:482/1750 train_time:45405ms step_avg:94.20ms
step:483/1750 train_time:45501ms step_avg:94.21ms
step:484/1750 train_time:45598ms step_avg:94.21ms
step:485/1750 train_time:45694ms step_avg:94.22ms
step:486/1750 train_time:45790ms step_avg:94.22ms
step:487/1750 train_time:45886ms step_avg:94.22ms
step:488/1750 train_time:45982ms step_avg:94.23ms
step:489/1750 train_time:46079ms step_avg:94.23ms
step:490/1750 train_time:46175ms step_avg:94.23ms
step:491/1750 train_time:46271ms step_avg:94.24ms
step:492/1750 train_time:46366ms step_avg:94.24ms
step:493/1750 train_time:46463ms step_avg:94.24ms
step:494/1750 train_time:46560ms step_avg:94.25ms
step:495/1750 train_time:46656ms step_avg:94.26ms
step:496/1750 train_time:46753ms step_avg:94.26ms
step:497/1750 train_time:46850ms step_avg:94.26ms
step:498/1750 train_time:46945ms step_avg:94.27ms
step:499/1750 train_time:47042ms step_avg:94.27ms
step:500/1750 train_time:47139ms step_avg:94.28ms
step:500/1750 val_loss:3.7496 train_time:47230ms step_avg:94.46ms
step:501/1750 train_time:47256ms step_avg:94.32ms
step:502/1750 train_time:47344ms step_avg:94.31ms
step:503/1750 train_time:47443ms step_avg:94.32ms
step:504/1750 train_time:47541ms step_avg:94.33ms
step:505/1750 train_time:47637ms step_avg:94.33ms
step:506/1750 train_time:47733ms step_avg:94.33ms
step:507/1750 train_time:47828ms step_avg:94.34ms
step:508/1750 train_time:47924ms step_avg:94.34ms
step:509/1750 train_time:48020ms step_avg:94.34ms
step:510/1750 train_time:48116ms step_avg:94.34ms
step:511/1750 train_time:48211ms step_avg:94.35ms
step:512/1750 train_time:48309ms step_avg:94.35ms
step:513/1750 train_time:48408ms step_avg:94.36ms
step:514/1750 train_time:48505ms step_avg:94.37ms
step:515/1750 train_time:48602ms step_avg:94.37ms
step:516/1750 train_time:48698ms step_avg:94.38ms
step:517/1750 train_time:48794ms step_avg:94.38ms
step:518/1750 train_time:48890ms step_avg:94.38ms
step:519/1750 train_time:48986ms step_avg:94.39ms
step:520/1750 train_time:49082ms step_avg:94.39ms
step:521/1750 train_time:49178ms step_avg:94.39ms
step:522/1750 train_time:49275ms step_avg:94.40ms
step:523/1750 train_time:49372ms step_avg:94.40ms
step:524/1750 train_time:49468ms step_avg:94.41ms
step:525/1750 train_time:49565ms step_avg:94.41ms
step:526/1750 train_time:49663ms step_avg:94.42ms
step:527/1750 train_time:49760ms step_avg:94.42ms
step:528/1750 train_time:49858ms step_avg:94.43ms
step:529/1750 train_time:49954ms step_avg:94.43ms
step:530/1750 train_time:50050ms step_avg:94.43ms
step:531/1750 train_time:50147ms step_avg:94.44ms
step:532/1750 train_time:50244ms step_avg:94.44ms
step:533/1750 train_time:50342ms step_avg:94.45ms
step:534/1750 train_time:50439ms step_avg:94.46ms
step:535/1750 train_time:50535ms step_avg:94.46ms
step:536/1750 train_time:50632ms step_avg:94.46ms
step:537/1750 train_time:50728ms step_avg:94.47ms
step:538/1750 train_time:50825ms step_avg:94.47ms
step:539/1750 train_time:50922ms step_avg:94.48ms
step:540/1750 train_time:51019ms step_avg:94.48ms
step:541/1750 train_time:51116ms step_avg:94.48ms
step:542/1750 train_time:51212ms step_avg:94.49ms
step:543/1750 train_time:51309ms step_avg:94.49ms
step:544/1750 train_time:51406ms step_avg:94.50ms
step:545/1750 train_time:51503ms step_avg:94.50ms
step:546/1750 train_time:51600ms step_avg:94.50ms
step:547/1750 train_time:51696ms step_avg:94.51ms
step:548/1750 train_time:51793ms step_avg:94.51ms
step:549/1750 train_time:51889ms step_avg:94.52ms
step:550/1750 train_time:51986ms step_avg:94.52ms
step:551/1750 train_time:52082ms step_avg:94.52ms
step:552/1750 train_time:52179ms step_avg:94.53ms
step:553/1750 train_time:52277ms step_avg:94.53ms
step:554/1750 train_time:52374ms step_avg:94.54ms
step:555/1750 train_time:52470ms step_avg:94.54ms
step:556/1750 train_time:52567ms step_avg:94.54ms
step:557/1750 train_time:52663ms step_avg:94.55ms
step:558/1750 train_time:52760ms step_avg:94.55ms
step:559/1750 train_time:52857ms step_avg:94.56ms
step:560/1750 train_time:52954ms step_avg:94.56ms
step:561/1750 train_time:53050ms step_avg:94.56ms
step:562/1750 train_time:53148ms step_avg:94.57ms
step:563/1750 train_time:53245ms step_avg:94.57ms
step:564/1750 train_time:53342ms step_avg:94.58ms
step:565/1750 train_time:53438ms step_avg:94.58ms
step:566/1750 train_time:53535ms step_avg:94.58ms
step:567/1750 train_time:53633ms step_avg:94.59ms
step:568/1750 train_time:53729ms step_avg:94.59ms
step:569/1750 train_time:53827ms step_avg:94.60ms
step:570/1750 train_time:53925ms step_avg:94.60ms
step:571/1750 train_time:54021ms step_avg:94.61ms
step:572/1750 train_time:54118ms step_avg:94.61ms
step:573/1750 train_time:54214ms step_avg:94.61ms
step:574/1750 train_time:54310ms step_avg:94.62ms
step:575/1750 train_time:54407ms step_avg:94.62ms
step:576/1750 train_time:54503ms step_avg:94.62ms
step:577/1750 train_time:54601ms step_avg:94.63ms
step:578/1750 train_time:54698ms step_avg:94.63ms
step:579/1750 train_time:54796ms step_avg:94.64ms
step:580/1750 train_time:54893ms step_avg:94.64ms
step:581/1750 train_time:54990ms step_avg:94.65ms
step:582/1750 train_time:55087ms step_avg:94.65ms
step:583/1750 train_time:55184ms step_avg:94.66ms
step:584/1750 train_time:55281ms step_avg:94.66ms
step:585/1750 train_time:55377ms step_avg:94.66ms
step:586/1750 train_time:55474ms step_avg:94.67ms
step:587/1750 train_time:55570ms step_avg:94.67ms
step:588/1750 train_time:55667ms step_avg:94.67ms
step:589/1750 train_time:55764ms step_avg:94.68ms
step:590/1750 train_time:55862ms step_avg:94.68ms
step:591/1750 train_time:55960ms step_avg:94.69ms
step:592/1750 train_time:56057ms step_avg:94.69ms
step:593/1750 train_time:56153ms step_avg:94.69ms
step:594/1750 train_time:56250ms step_avg:94.70ms
step:595/1750 train_time:56346ms step_avg:94.70ms
step:596/1750 train_time:56443ms step_avg:94.70ms
step:597/1750 train_time:56539ms step_avg:94.70ms
step:598/1750 train_time:56635ms step_avg:94.71ms
step:599/1750 train_time:56732ms step_avg:94.71ms
step:600/1750 train_time:56829ms step_avg:94.71ms
step:601/1750 train_time:56925ms step_avg:94.72ms
step:602/1750 train_time:57023ms step_avg:94.72ms
step:603/1750 train_time:57120ms step_avg:94.73ms
step:604/1750 train_time:57218ms step_avg:94.73ms
step:605/1750 train_time:57314ms step_avg:94.73ms
step:606/1750 train_time:57410ms step_avg:94.74ms
step:607/1750 train_time:57507ms step_avg:94.74ms
step:608/1750 train_time:57604ms step_avg:94.74ms
step:609/1750 train_time:57700ms step_avg:94.75ms
step:610/1750 train_time:57797ms step_avg:94.75ms
step:611/1750 train_time:57894ms step_avg:94.75ms
step:612/1750 train_time:57991ms step_avg:94.76ms
step:613/1750 train_time:58088ms step_avg:94.76ms
step:614/1750 train_time:58185ms step_avg:94.76ms
step:615/1750 train_time:58282ms step_avg:94.77ms
step:616/1750 train_time:58379ms step_avg:94.77ms
step:617/1750 train_time:58475ms step_avg:94.77ms
step:618/1750 train_time:58572ms step_avg:94.78ms
step:619/1750 train_time:58668ms step_avg:94.78ms
step:620/1750 train_time:58765ms step_avg:94.78ms
step:621/1750 train_time:58863ms step_avg:94.79ms
step:622/1750 train_time:58960ms step_avg:94.79ms
step:623/1750 train_time:59057ms step_avg:94.79ms
step:624/1750 train_time:59155ms step_avg:94.80ms
step:625/1750 train_time:59251ms step_avg:94.80ms
step:625/1750 val_loss:3.6601 train_time:59343ms step_avg:94.95ms
step:626/1750 train_time:59368ms step_avg:94.84ms
step:627/1750 train_time:59451ms step_avg:94.82ms
step:628/1750 train_time:59551ms step_avg:94.83ms
step:629/1750 train_time:59648ms step_avg:94.83ms
step:630/1750 train_time:59745ms step_avg:94.83ms
step:631/1750 train_time:59842ms step_avg:94.84ms
step:632/1750 train_time:59938ms step_avg:94.84ms
step:633/1750 train_time:60033ms step_avg:94.84ms
step:634/1750 train_time:60129ms step_avg:94.84ms
step:635/1750 train_time:60226ms step_avg:94.84ms
step:636/1750 train_time:60322ms step_avg:94.85ms
step:637/1750 train_time:60420ms step_avg:94.85ms
step:638/1750 train_time:60518ms step_avg:94.86ms
step:639/1750 train_time:60615ms step_avg:94.86ms
step:640/1750 train_time:60712ms step_avg:94.86ms
step:641/1750 train_time:60809ms step_avg:94.87ms
step:642/1750 train_time:60905ms step_avg:94.87ms
step:643/1750 train_time:61002ms step_avg:94.87ms
step:644/1750 train_time:61098ms step_avg:94.87ms
step:645/1750 train_time:61194ms step_avg:94.87ms
step:646/1750 train_time:61290ms step_avg:94.88ms
step:647/1750 train_time:61386ms step_avg:94.88ms
step:648/1750 train_time:61484ms step_avg:94.88ms
step:649/1750 train_time:61582ms step_avg:94.89ms
step:650/1750 train_time:61679ms step_avg:94.89ms
step:651/1750 train_time:61778ms step_avg:94.90ms
step:652/1750 train_time:61877ms step_avg:94.90ms
step:653/1750 train_time:61975ms step_avg:94.91ms
step:654/1750 train_time:62073ms step_avg:94.91ms
step:655/1750 train_time:62171ms step_avg:94.92ms
step:656/1750 train_time:62269ms step_avg:94.92ms
step:657/1750 train_time:62368ms step_avg:94.93ms
step:658/1750 train_time:62467ms step_avg:94.93ms
step:659/1750 train_time:62566ms step_avg:94.94ms
step:660/1750 train_time:62665ms step_avg:94.95ms
step:661/1750 train_time:62765ms step_avg:94.95ms
step:662/1750 train_time:62864ms step_avg:94.96ms
step:663/1750 train_time:62963ms step_avg:94.97ms
step:664/1750 train_time:63062ms step_avg:94.97ms
step:665/1750 train_time:63161ms step_avg:94.98ms
step:666/1750 train_time:63260ms step_avg:94.98ms
step:667/1750 train_time:63358ms step_avg:94.99ms
step:668/1750 train_time:63456ms step_avg:94.99ms
step:669/1750 train_time:63555ms step_avg:95.00ms
step:670/1750 train_time:63652ms step_avg:95.00ms
step:671/1750 train_time:63750ms step_avg:95.01ms
step:672/1750 train_time:63848ms step_avg:95.01ms
step:673/1750 train_time:63947ms step_avg:95.02ms
step:674/1750 train_time:64046ms step_avg:95.02ms
step:675/1750 train_time:64145ms step_avg:95.03ms
step:676/1750 train_time:64244ms step_avg:95.04ms
step:677/1750 train_time:64343ms step_avg:95.04ms
step:678/1750 train_time:64442ms step_avg:95.05ms
step:679/1750 train_time:64540ms step_avg:95.05ms
step:680/1750 train_time:64638ms step_avg:95.06ms
step:681/1750 train_time:64736ms step_avg:95.06ms
step:682/1750 train_time:64834ms step_avg:95.07ms
step:683/1750 train_time:64932ms step_avg:95.07ms
step:684/1750 train_time:65030ms step_avg:95.07ms
step:685/1750 train_time:65128ms step_avg:95.08ms
step:686/1750 train_time:65228ms step_avg:95.08ms
step:687/1750 train_time:65327ms step_avg:95.09ms
step:688/1750 train_time:65426ms step_avg:95.10ms
step:689/1750 train_time:65524ms step_avg:95.10ms
step:690/1750 train_time:65623ms step_avg:95.11ms
step:691/1750 train_time:65722ms step_avg:95.11ms
step:692/1750 train_time:65821ms step_avg:95.12ms
step:693/1750 train_time:65919ms step_avg:95.12ms
step:694/1750 train_time:66018ms step_avg:95.13ms
step:695/1750 train_time:66116ms step_avg:95.13ms
step:696/1750 train_time:66214ms step_avg:95.14ms
step:697/1750 train_time:66312ms step_avg:95.14ms
step:698/1750 train_time:66410ms step_avg:95.14ms
step:699/1750 train_time:66508ms step_avg:95.15ms
step:700/1750 train_time:66606ms step_avg:95.15ms
step:701/1750 train_time:66705ms step_avg:95.16ms
step:702/1750 train_time:66804ms step_avg:95.16ms
step:703/1750 train_time:66903ms step_avg:95.17ms
step:704/1750 train_time:67002ms step_avg:95.17ms
step:705/1750 train_time:67101ms step_avg:95.18ms
step:706/1750 train_time:67200ms step_avg:95.18ms
step:707/1750 train_time:67298ms step_avg:95.19ms
step:708/1750 train_time:67396ms step_avg:95.19ms
step:709/1750 train_time:67495ms step_avg:95.20ms
step:710/1750 train_time:67592ms step_avg:95.20ms
step:711/1750 train_time:67690ms step_avg:95.20ms
step:712/1750 train_time:67789ms step_avg:95.21ms
step:713/1750 train_time:67887ms step_avg:95.21ms
step:714/1750 train_time:67987ms step_avg:95.22ms
step:715/1750 train_time:68086ms step_avg:95.22ms
step:716/1750 train_time:68185ms step_avg:95.23ms
step:717/1750 train_time:68285ms step_avg:95.24ms
step:718/1750 train_time:68384ms step_avg:95.24ms
step:719/1750 train_time:68483ms step_avg:95.25ms
step:720/1750 train_time:68583ms step_avg:95.25ms
step:721/1750 train_time:68683ms step_avg:95.26ms
step:722/1750 train_time:68782ms step_avg:95.27ms
step:723/1750 train_time:68881ms step_avg:95.27ms
step:724/1750 train_time:68979ms step_avg:95.28ms
step:725/1750 train_time:69079ms step_avg:95.28ms
step:726/1750 train_time:69177ms step_avg:95.29ms
step:727/1750 train_time:69276ms step_avg:95.29ms
step:728/1750 train_time:69374ms step_avg:95.29ms
step:729/1750 train_time:69473ms step_avg:95.30ms
step:730/1750 train_time:69571ms step_avg:95.30ms
step:731/1750 train_time:69669ms step_avg:95.31ms
step:732/1750 train_time:69767ms step_avg:95.31ms
step:733/1750 train_time:69865ms step_avg:95.31ms
step:734/1750 train_time:69963ms step_avg:95.32ms
step:735/1750 train_time:70062ms step_avg:95.32ms
step:736/1750 train_time:70160ms step_avg:95.33ms
step:737/1750 train_time:70259ms step_avg:95.33ms
step:738/1750 train_time:70358ms step_avg:95.34ms
step:739/1750 train_time:70455ms step_avg:95.34ms
step:740/1750 train_time:70553ms step_avg:95.34ms
step:741/1750 train_time:70651ms step_avg:95.35ms
step:742/1750 train_time:70750ms step_avg:95.35ms
step:743/1750 train_time:70848ms step_avg:95.35ms
step:744/1750 train_time:70946ms step_avg:95.36ms
step:745/1750 train_time:71044ms step_avg:95.36ms
step:746/1750 train_time:71143ms step_avg:95.37ms
step:747/1750 train_time:71242ms step_avg:95.37ms
step:748/1750 train_time:71341ms step_avg:95.38ms
step:749/1750 train_time:71439ms step_avg:95.38ms
step:750/1750 train_time:71538ms step_avg:95.38ms
step:750/1750 val_loss:3.5959 train_time:71631ms step_avg:95.51ms
step:751/1750 train_time:71656ms step_avg:95.41ms
step:752/1750 train_time:71743ms step_avg:95.40ms
step:753/1750 train_time:71843ms step_avg:95.41ms
step:754/1750 train_time:71941ms step_avg:95.41ms
step:755/1750 train_time:72039ms step_avg:95.42ms
step:756/1750 train_time:72137ms step_avg:95.42ms
step:757/1750 train_time:72236ms step_avg:95.42ms
step:758/1750 train_time:72334ms step_avg:95.43ms
step:759/1750 train_time:72433ms step_avg:95.43ms
step:760/1750 train_time:72531ms step_avg:95.44ms
step:761/1750 train_time:72630ms step_avg:95.44ms
step:762/1750 train_time:72729ms step_avg:95.44ms
step:763/1750 train_time:72828ms step_avg:95.45ms
step:764/1750 train_time:72926ms step_avg:95.45ms
step:765/1750 train_time:73025ms step_avg:95.46ms
step:766/1750 train_time:73123ms step_avg:95.46ms
step:767/1750 train_time:73221ms step_avg:95.46ms
step:768/1750 train_time:73319ms step_avg:95.47ms
step:769/1750 train_time:73418ms step_avg:95.47ms
step:770/1750 train_time:73516ms step_avg:95.48ms
step:771/1750 train_time:73615ms step_avg:95.48ms
step:772/1750 train_time:73713ms step_avg:95.48ms
step:773/1750 train_time:73812ms step_avg:95.49ms
step:774/1750 train_time:73912ms step_avg:95.49ms
step:775/1750 train_time:74010ms step_avg:95.50ms
step:776/1750 train_time:74109ms step_avg:95.50ms
step:777/1750 train_time:74207ms step_avg:95.50ms
step:778/1750 train_time:74305ms step_avg:95.51ms
step:779/1750 train_time:74404ms step_avg:95.51ms
step:780/1750 train_time:74502ms step_avg:95.52ms
step:781/1750 train_time:74600ms step_avg:95.52ms
step:782/1750 train_time:74699ms step_avg:95.52ms
step:783/1750 train_time:74798ms step_avg:95.53ms
step:784/1750 train_time:74897ms step_avg:95.53ms
step:785/1750 train_time:74997ms step_avg:95.54ms
step:786/1750 train_time:75096ms step_avg:95.54ms
step:787/1750 train_time:75195ms step_avg:95.55ms
step:788/1750 train_time:75294ms step_avg:95.55ms
step:789/1750 train_time:75394ms step_avg:95.56ms
step:790/1750 train_time:75493ms step_avg:95.56ms
step:791/1750 train_time:75593ms step_avg:95.57ms
step:792/1750 train_time:75692ms step_avg:95.57ms
step:793/1750 train_time:75791ms step_avg:95.57ms
step:794/1750 train_time:75889ms step_avg:95.58ms
step:795/1750 train_time:75988ms step_avg:95.58ms
step:796/1750 train_time:76087ms step_avg:95.59ms
step:797/1750 train_time:76185ms step_avg:95.59ms
step:798/1750 train_time:76284ms step_avg:95.59ms
step:799/1750 train_time:76382ms step_avg:95.60ms
step:800/1750 train_time:76480ms step_avg:95.60ms
step:801/1750 train_time:76579ms step_avg:95.60ms
step:802/1750 train_time:76678ms step_avg:95.61ms
step:803/1750 train_time:76776ms step_avg:95.61ms
step:804/1750 train_time:76876ms step_avg:95.62ms
step:805/1750 train_time:76976ms step_avg:95.62ms
step:806/1750 train_time:77076ms step_avg:95.63ms
step:807/1750 train_time:77175ms step_avg:95.63ms
step:808/1750 train_time:77275ms step_avg:95.64ms
step:809/1750 train_time:77375ms step_avg:95.64ms
step:810/1750 train_time:77475ms step_avg:95.65ms
step:811/1750 train_time:77575ms step_avg:95.65ms
step:812/1750 train_time:77674ms step_avg:95.66ms
step:813/1750 train_time:77773ms step_avg:95.66ms
step:814/1750 train_time:77873ms step_avg:95.67ms
step:815/1750 train_time:77973ms step_avg:95.67ms
step:816/1750 train_time:78071ms step_avg:95.68ms
step:817/1750 train_time:78170ms step_avg:95.68ms
step:818/1750 train_time:78269ms step_avg:95.68ms
step:819/1750 train_time:78368ms step_avg:95.69ms
step:820/1750 train_time:78467ms step_avg:95.69ms
step:821/1750 train_time:78567ms step_avg:95.70ms
step:822/1750 train_time:78665ms step_avg:95.70ms
step:823/1750 train_time:78765ms step_avg:95.70ms
step:824/1750 train_time:78863ms step_avg:95.71ms
step:825/1750 train_time:78962ms step_avg:95.71ms
step:826/1750 train_time:79060ms step_avg:95.71ms
step:827/1750 train_time:79158ms step_avg:95.72ms
step:828/1750 train_time:79256ms step_avg:95.72ms
step:829/1750 train_time:79356ms step_avg:95.72ms
step:830/1750 train_time:79456ms step_avg:95.73ms
step:831/1750 train_time:79555ms step_avg:95.73ms
step:832/1750 train_time:79655ms step_avg:95.74ms
step:833/1750 train_time:79755ms step_avg:95.74ms
step:834/1750 train_time:79854ms step_avg:95.75ms
step:835/1750 train_time:79954ms step_avg:95.75ms
step:836/1750 train_time:80053ms step_avg:95.76ms
step:837/1750 train_time:80152ms step_avg:95.76ms
step:838/1750 train_time:80250ms step_avg:95.76ms
step:839/1750 train_time:80349ms step_avg:95.77ms
step:840/1750 train_time:80448ms step_avg:95.77ms
step:841/1750 train_time:80547ms step_avg:95.78ms
step:842/1750 train_time:80646ms step_avg:95.78ms
step:843/1750 train_time:80745ms step_avg:95.78ms
step:844/1750 train_time:80844ms step_avg:95.79ms
step:845/1750 train_time:80943ms step_avg:95.79ms
step:846/1750 train_time:81041ms step_avg:95.79ms
step:847/1750 train_time:81139ms step_avg:95.80ms
step:848/1750 train_time:81238ms step_avg:95.80ms
step:849/1750 train_time:81336ms step_avg:95.80ms
step:850/1750 train_time:81435ms step_avg:95.81ms
step:851/1750 train_time:81534ms step_avg:95.81ms
step:852/1750 train_time:81633ms step_avg:95.81ms
step:853/1750 train_time:81733ms step_avg:95.82ms
step:854/1750 train_time:81831ms step_avg:95.82ms
step:855/1750 train_time:81931ms step_avg:95.83ms
step:856/1750 train_time:82029ms step_avg:95.83ms
step:857/1750 train_time:82128ms step_avg:95.83ms
step:858/1750 train_time:82227ms step_avg:95.84ms
step:859/1750 train_time:82325ms step_avg:95.84ms
step:860/1750 train_time:82424ms step_avg:95.84ms
step:861/1750 train_time:82522ms step_avg:95.84ms
step:862/1750 train_time:82620ms step_avg:95.85ms
step:863/1750 train_time:82719ms step_avg:95.85ms
step:864/1750 train_time:82818ms step_avg:95.85ms
step:865/1750 train_time:82917ms step_avg:95.86ms
step:866/1750 train_time:83017ms step_avg:95.86ms
step:867/1750 train_time:83116ms step_avg:95.87ms
step:868/1750 train_time:83214ms step_avg:95.87ms
step:869/1750 train_time:83314ms step_avg:95.87ms
step:870/1750 train_time:83414ms step_avg:95.88ms
step:871/1750 train_time:83513ms step_avg:95.88ms
step:872/1750 train_time:83612ms step_avg:95.89ms
step:873/1750 train_time:83711ms step_avg:95.89ms
step:874/1750 train_time:83810ms step_avg:95.89ms
step:875/1750 train_time:83909ms step_avg:95.90ms
step:875/1750 val_loss:3.5454 train_time:84003ms step_avg:96.00ms
step:876/1750 train_time:84028ms step_avg:95.92ms
step:877/1750 train_time:84116ms step_avg:95.91ms
step:878/1750 train_time:84217ms step_avg:95.92ms
step:879/1750 train_time:84315ms step_avg:95.92ms
step:880/1750 train_time:84413ms step_avg:95.92ms
step:881/1750 train_time:84511ms step_avg:95.93ms
step:882/1750 train_time:84609ms step_avg:95.93ms
step:883/1750 train_time:84707ms step_avg:95.93ms
step:884/1750 train_time:84806ms step_avg:95.93ms
step:885/1750 train_time:84905ms step_avg:95.94ms
step:886/1750 train_time:85006ms step_avg:95.94ms
step:887/1750 train_time:85106ms step_avg:95.95ms
step:888/1750 train_time:85206ms step_avg:95.95ms
step:889/1750 train_time:85306ms step_avg:95.96ms
step:890/1750 train_time:85405ms step_avg:95.96ms
step:891/1750 train_time:85505ms step_avg:95.96ms
step:892/1750 train_time:85603ms step_avg:95.97ms
step:893/1750 train_time:85703ms step_avg:95.97ms
step:894/1750 train_time:85802ms step_avg:95.98ms
step:895/1750 train_time:85901ms step_avg:95.98ms
step:896/1750 train_time:86001ms step_avg:95.98ms
step:897/1750 train_time:86101ms step_avg:95.99ms
step:898/1750 train_time:86201ms step_avg:95.99ms
step:899/1750 train_time:86300ms step_avg:96.00ms
step:900/1750 train_time:86399ms step_avg:96.00ms
step:901/1750 train_time:86499ms step_avg:96.00ms
step:902/1750 train_time:86598ms step_avg:96.01ms
step:903/1750 train_time:86696ms step_avg:96.01ms
step:904/1750 train_time:86796ms step_avg:96.01ms
step:905/1750 train_time:86895ms step_avg:96.02ms
step:906/1750 train_time:86993ms step_avg:96.02ms
step:907/1750 train_time:87091ms step_avg:96.02ms
step:908/1750 train_time:87189ms step_avg:96.02ms
step:909/1750 train_time:87288ms step_avg:96.03ms
step:910/1750 train_time:87389ms step_avg:96.03ms
step:911/1750 train_time:87489ms step_avg:96.04ms
step:912/1750 train_time:87589ms step_avg:96.04ms
step:913/1750 train_time:87689ms step_avg:96.04ms
step:914/1750 train_time:87789ms step_avg:96.05ms
step:915/1750 train_time:87890ms step_avg:96.05ms
step:916/1750 train_time:87990ms step_avg:96.06ms
step:917/1750 train_time:88090ms step_avg:96.06ms
step:918/1750 train_time:88190ms step_avg:96.07ms
step:919/1750 train_time:88291ms step_avg:96.07ms
step:920/1750 train_time:88391ms step_avg:96.08ms
step:921/1750 train_time:88492ms step_avg:96.08ms
step:922/1750 train_time:88591ms step_avg:96.09ms
step:923/1750 train_time:88692ms step_avg:96.09ms
step:924/1750 train_time:88792ms step_avg:96.09ms
step:925/1750 train_time:88891ms step_avg:96.10ms
step:926/1750 train_time:88991ms step_avg:96.10ms
step:927/1750 train_time:89090ms step_avg:96.11ms
step:928/1750 train_time:89190ms step_avg:96.11ms
step:929/1750 train_time:89290ms step_avg:96.11ms
step:930/1750 train_time:89391ms step_avg:96.12ms
step:931/1750 train_time:89491ms step_avg:96.12ms
step:932/1750 train_time:89591ms step_avg:96.13ms
step:933/1750 train_time:89691ms step_avg:96.13ms
step:934/1750 train_time:89791ms step_avg:96.14ms
step:935/1750 train_time:89891ms step_avg:96.14ms
step:936/1750 train_time:89992ms step_avg:96.14ms
step:937/1750 train_time:90092ms step_avg:96.15ms
step:938/1750 train_time:90192ms step_avg:96.15ms
step:939/1750 train_time:90292ms step_avg:96.16ms
step:940/1750 train_time:90392ms step_avg:96.16ms
step:941/1750 train_time:90492ms step_avg:96.17ms
step:942/1750 train_time:90592ms step_avg:96.17ms
step:943/1750 train_time:90691ms step_avg:96.17ms
step:944/1750 train_time:90791ms step_avg:96.18ms
step:945/1750 train_time:90891ms step_avg:96.18ms
step:946/1750 train_time:90991ms step_avg:96.18ms
step:947/1750 train_time:91090ms step_avg:96.19ms
step:948/1750 train_time:91190ms step_avg:96.19ms
step:949/1750 train_time:91290ms step_avg:96.20ms
step:950/1750 train_time:91390ms step_avg:96.20ms
step:951/1750 train_time:91490ms step_avg:96.20ms
step:952/1750 train_time:91591ms step_avg:96.21ms
step:953/1750 train_time:91692ms step_avg:96.21ms
step:954/1750 train_time:91791ms step_avg:96.22ms
step:955/1750 train_time:91891ms step_avg:96.22ms
step:956/1750 train_time:91990ms step_avg:96.22ms
step:957/1750 train_time:92090ms step_avg:96.23ms
step:958/1750 train_time:92190ms step_avg:96.23ms
step:959/1750 train_time:92290ms step_avg:96.24ms
step:960/1750 train_time:92390ms step_avg:96.24ms
step:961/1750 train_time:92490ms step_avg:96.24ms
step:962/1750 train_time:92591ms step_avg:96.25ms
step:963/1750 train_time:92692ms step_avg:96.25ms
step:964/1750 train_time:92792ms step_avg:96.26ms
step:965/1750 train_time:92892ms step_avg:96.26ms
step:966/1750 train_time:92992ms step_avg:96.26ms
step:967/1750 train_time:93092ms step_avg:96.27ms
step:968/1750 train_time:93191ms step_avg:96.27ms
step:969/1750 train_time:93291ms step_avg:96.28ms
step:970/1750 train_time:93392ms step_avg:96.28ms
step:971/1750 train_time:93492ms step_avg:96.28ms
step:972/1750 train_time:93592ms step_avg:96.29ms
step:973/1750 train_time:93691ms step_avg:96.29ms
step:974/1750 train_time:93791ms step_avg:96.29ms
step:975/1750 train_time:93891ms step_avg:96.30ms
step:976/1750 train_time:93991ms step_avg:96.30ms
step:977/1750 train_time:94091ms step_avg:96.31ms
step:978/1750 train_time:94191ms step_avg:96.31ms
step:979/1750 train_time:94291ms step_avg:96.31ms
step:980/1750 train_time:94391ms step_avg:96.32ms
step:981/1750 train_time:94491ms step_avg:96.32ms
step:982/1750 train_time:94591ms step_avg:96.32ms
step:983/1750 train_time:94692ms step_avg:96.33ms
step:984/1750 train_time:94791ms step_avg:96.33ms
step:985/1750 train_time:94891ms step_avg:96.34ms
step:986/1750 train_time:94990ms step_avg:96.34ms
step:987/1750 train_time:95091ms step_avg:96.34ms
step:988/1750 train_time:95190ms step_avg:96.35ms
step:989/1750 train_time:95290ms step_avg:96.35ms
step:990/1750 train_time:95390ms step_avg:96.35ms
step:991/1750 train_time:95490ms step_avg:96.36ms
step:992/1750 train_time:95590ms step_avg:96.36ms
step:993/1750 train_time:95691ms step_avg:96.37ms
step:994/1750 train_time:95790ms step_avg:96.37ms
step:995/1750 train_time:95891ms step_avg:96.37ms
step:996/1750 train_time:95991ms step_avg:96.38ms
step:997/1750 train_time:96091ms step_avg:96.38ms
step:998/1750 train_time:96191ms step_avg:96.38ms
step:999/1750 train_time:96290ms step_avg:96.39ms
step:1000/1750 train_time:96390ms step_avg:96.39ms
step:1000/1750 val_loss:3.5049 train_time:96485ms step_avg:96.48ms
step:1001/1750 train_time:96509ms step_avg:96.41ms
step:1002/1750 train_time:96598ms step_avg:96.41ms
step:1003/1750 train_time:96701ms step_avg:96.41ms
step:1004/1750 train_time:96803ms step_avg:96.42ms
step:1005/1750 train_time:96904ms step_avg:96.42ms
step:1006/1750 train_time:97004ms step_avg:96.43ms
step:1007/1750 train_time:97104ms step_avg:96.43ms
step:1008/1750 train_time:97204ms step_avg:96.43ms
step:1009/1750 train_time:97304ms step_avg:96.44ms
step:1010/1750 train_time:97404ms step_avg:96.44ms
step:1011/1750 train_time:97506ms step_avg:96.44ms
step:1012/1750 train_time:97607ms step_avg:96.45ms
step:1013/1750 train_time:97708ms step_avg:96.45ms
step:1014/1750 train_time:97809ms step_avg:96.46ms
step:1015/1750 train_time:97909ms step_avg:96.46ms
step:1016/1750 train_time:98008ms step_avg:96.46ms
step:1017/1750 train_time:98108ms step_avg:96.47ms
step:1018/1750 train_time:98207ms step_avg:96.47ms
step:1019/1750 train_time:98307ms step_avg:96.47ms
step:1020/1750 train_time:98407ms step_avg:96.48ms
step:1021/1750 train_time:98507ms step_avg:96.48ms
step:1022/1750 train_time:98608ms step_avg:96.48ms
step:1023/1750 train_time:98708ms step_avg:96.49ms
step:1024/1750 train_time:98809ms step_avg:96.49ms
step:1025/1750 train_time:98909ms step_avg:96.50ms
step:1026/1750 train_time:99009ms step_avg:96.50ms
step:1027/1750 train_time:99109ms step_avg:96.50ms
step:1028/1750 train_time:99209ms step_avg:96.51ms
step:1029/1750 train_time:99309ms step_avg:96.51ms
step:1030/1750 train_time:99408ms step_avg:96.51ms
step:1031/1750 train_time:99508ms step_avg:96.52ms
step:1032/1750 train_time:99609ms step_avg:96.52ms
step:1033/1750 train_time:99709ms step_avg:96.52ms
step:1034/1750 train_time:99808ms step_avg:96.53ms
step:1035/1750 train_time:99908ms step_avg:96.53ms
step:1036/1750 train_time:100007ms step_avg:96.53ms
step:1037/1750 train_time:100109ms step_avg:96.54ms
step:1038/1750 train_time:100208ms step_avg:96.54ms
step:1039/1750 train_time:100308ms step_avg:96.54ms
step:1040/1750 train_time:100407ms step_avg:96.55ms
step:1041/1750 train_time:100507ms step_avg:96.55ms
step:1042/1750 train_time:100608ms step_avg:96.55ms
step:1043/1750 train_time:100708ms step_avg:96.56ms
step:1044/1750 train_time:100807ms step_avg:96.56ms
step:1045/1750 train_time:100907ms step_avg:96.56ms
step:1046/1750 train_time:101008ms step_avg:96.57ms
step:1047/1750 train_time:101108ms step_avg:96.57ms
step:1048/1750 train_time:101208ms step_avg:96.57ms
step:1049/1750 train_time:101307ms step_avg:96.58ms
step:1050/1750 train_time:101408ms step_avg:96.58ms
step:1051/1750 train_time:101508ms step_avg:96.58ms
step:1052/1750 train_time:101609ms step_avg:96.59ms
step:1053/1750 train_time:101709ms step_avg:96.59ms
step:1054/1750 train_time:101808ms step_avg:96.59ms
step:1055/1750 train_time:101909ms step_avg:96.60ms
step:1056/1750 train_time:102009ms step_avg:96.60ms
step:1057/1750 train_time:102108ms step_avg:96.60ms
step:1058/1750 train_time:102208ms step_avg:96.61ms
step:1059/1750 train_time:102308ms step_avg:96.61ms
step:1060/1750 train_time:102408ms step_avg:96.61ms
step:1061/1750 train_time:102508ms step_avg:96.61ms
step:1062/1750 train_time:102607ms step_avg:96.62ms
step:1063/1750 train_time:102708ms step_avg:96.62ms
step:1064/1750 train_time:102809ms step_avg:96.62ms
step:1065/1750 train_time:102909ms step_avg:96.63ms
step:1066/1750 train_time:103008ms step_avg:96.63ms
step:1067/1750 train_time:103109ms step_avg:96.63ms
step:1068/1750 train_time:103209ms step_avg:96.64ms
step:1069/1750 train_time:103309ms step_avg:96.64ms
step:1070/1750 train_time:103409ms step_avg:96.64ms
step:1071/1750 train_time:103509ms step_avg:96.65ms
step:1072/1750 train_time:103609ms step_avg:96.65ms
step:1073/1750 train_time:103708ms step_avg:96.65ms
step:1074/1750 train_time:103808ms step_avg:96.66ms
step:1075/1750 train_time:103909ms step_avg:96.66ms
step:1076/1750 train_time:104008ms step_avg:96.66ms
step:1077/1750 train_time:104109ms step_avg:96.67ms
step:1078/1750 train_time:104209ms step_avg:96.67ms
step:1079/1750 train_time:104308ms step_avg:96.67ms
step:1080/1750 train_time:104408ms step_avg:96.67ms
step:1081/1750 train_time:104508ms step_avg:96.68ms
step:1082/1750 train_time:104608ms step_avg:96.68ms
step:1083/1750 train_time:104707ms step_avg:96.68ms
step:1084/1750 train_time:104807ms step_avg:96.69ms
step:1085/1750 train_time:104907ms step_avg:96.69ms
step:1086/1750 train_time:105007ms step_avg:96.69ms
step:1087/1750 train_time:105107ms step_avg:96.69ms
step:1088/1750 train_time:105207ms step_avg:96.70ms
step:1089/1750 train_time:105307ms step_avg:96.70ms
step:1090/1750 train_time:105408ms step_avg:96.70ms
step:1091/1750 train_time:105508ms step_avg:96.71ms
step:1092/1750 train_time:105609ms step_avg:96.71ms
step:1093/1750 train_time:105709ms step_avg:96.71ms
step:1094/1750 train_time:105809ms step_avg:96.72ms
step:1095/1750 train_time:105909ms step_avg:96.72ms
step:1096/1750 train_time:106009ms step_avg:96.72ms
step:1097/1750 train_time:106109ms step_avg:96.73ms
step:1098/1750 train_time:106209ms step_avg:96.73ms
step:1099/1750 train_time:106309ms step_avg:96.73ms
step:1100/1750 train_time:106408ms step_avg:96.73ms
step:1101/1750 train_time:106509ms step_avg:96.74ms
step:1102/1750 train_time:106608ms step_avg:96.74ms
step:1103/1750 train_time:106708ms step_avg:96.74ms
step:1104/1750 train_time:106809ms step_avg:96.75ms
step:1105/1750 train_time:106909ms step_avg:96.75ms
step:1106/1750 train_time:107010ms step_avg:96.75ms
step:1107/1750 train_time:107110ms step_avg:96.76ms
step:1108/1750 train_time:107210ms step_avg:96.76ms
step:1109/1750 train_time:107310ms step_avg:96.76ms
step:1110/1750 train_time:107410ms step_avg:96.77ms
step:1111/1750 train_time:107511ms step_avg:96.77ms
step:1112/1750 train_time:107611ms step_avg:96.77ms
step:1113/1750 train_time:107711ms step_avg:96.78ms
step:1114/1750 train_time:107811ms step_avg:96.78ms
step:1115/1750 train_time:107912ms step_avg:96.78ms
step:1116/1750 train_time:108013ms step_avg:96.79ms
step:1117/1750 train_time:108114ms step_avg:96.79ms
step:1118/1750 train_time:108214ms step_avg:96.79ms
step:1119/1750 train_time:108314ms step_avg:96.80ms
step:1120/1750 train_time:108415ms step_avg:96.80ms
step:1121/1750 train_time:108516ms step_avg:96.80ms
step:1122/1750 train_time:108616ms step_avg:96.81ms
step:1123/1750 train_time:108717ms step_avg:96.81ms
step:1124/1750 train_time:108818ms step_avg:96.81ms
step:1125/1750 train_time:108920ms step_avg:96.82ms
step:1125/1750 val_loss:3.4530 train_time:109016ms step_avg:96.90ms
step:1126/1750 train_time:109041ms step_avg:96.84ms
step:1127/1750 train_time:109132ms step_avg:96.83ms
step:1128/1750 train_time:109234ms step_avg:96.84ms
step:1129/1750 train_time:109335ms step_avg:96.84ms
step:1130/1750 train_time:109435ms step_avg:96.85ms
step:1131/1750 train_time:109535ms step_avg:96.85ms
step:1132/1750 train_time:109635ms step_avg:96.85ms
step:1133/1750 train_time:109735ms step_avg:96.85ms
step:1134/1750 train_time:109835ms step_avg:96.86ms
step:1135/1750 train_time:109934ms step_avg:96.86ms
step:1136/1750 train_time:110036ms step_avg:96.86ms
step:1137/1750 train_time:110138ms step_avg:96.87ms
step:1138/1750 train_time:110238ms step_avg:96.87ms
step:1139/1750 train_time:110338ms step_avg:96.87ms
step:1140/1750 train_time:110438ms step_avg:96.88ms
step:1141/1750 train_time:110538ms step_avg:96.88ms
step:1142/1750 train_time:110638ms step_avg:96.88ms
step:1143/1750 train_time:110737ms step_avg:96.88ms
step:1144/1750 train_time:110836ms step_avg:96.88ms
step:1145/1750 train_time:110936ms step_avg:96.89ms
step:1146/1750 train_time:111036ms step_avg:96.89ms
step:1147/1750 train_time:111136ms step_avg:96.89ms
step:1148/1750 train_time:111237ms step_avg:96.90ms
step:1149/1750 train_time:111338ms step_avg:96.90ms
step:1150/1750 train_time:111438ms step_avg:96.90ms
step:1151/1750 train_time:111538ms step_avg:96.91ms
step:1152/1750 train_time:111638ms step_avg:96.91ms
step:1153/1750 train_time:111738ms step_avg:96.91ms
step:1154/1750 train_time:111837ms step_avg:96.91ms
step:1155/1750 train_time:111936ms step_avg:96.91ms
step:1156/1750 train_time:112036ms step_avg:96.92ms
step:1157/1750 train_time:112137ms step_avg:96.92ms
step:1158/1750 train_time:112237ms step_avg:96.92ms
step:1159/1750 train_time:112337ms step_avg:96.93ms
step:1160/1750 train_time:112437ms step_avg:96.93ms
step:1161/1750 train_time:112538ms step_avg:96.93ms
step:1162/1750 train_time:112638ms step_avg:96.93ms
step:1163/1750 train_time:112739ms step_avg:96.94ms
step:1164/1750 train_time:112839ms step_avg:96.94ms
step:1165/1750 train_time:112939ms step_avg:96.94ms
step:1166/1750 train_time:113040ms step_avg:96.95ms
step:1167/1750 train_time:113140ms step_avg:96.95ms
step:1168/1750 train_time:113240ms step_avg:96.95ms
step:1169/1750 train_time:113342ms step_avg:96.96ms
step:1170/1750 train_time:113444ms step_avg:96.96ms
step:1171/1750 train_time:113546ms step_avg:96.97ms
step:1172/1750 train_time:113649ms step_avg:96.97ms
step:1173/1750 train_time:113751ms step_avg:96.97ms
step:1174/1750 train_time:113852ms step_avg:96.98ms
step:1175/1750 train_time:113954ms step_avg:96.98ms
step:1176/1750 train_time:114056ms step_avg:96.99ms
step:1177/1750 train_time:114157ms step_avg:96.99ms
step:1178/1750 train_time:114259ms step_avg:96.99ms
step:1179/1750 train_time:114362ms step_avg:97.00ms
step:1180/1750 train_time:114465ms step_avg:97.00ms
step:1181/1750 train_time:114566ms step_avg:97.01ms
step:1182/1750 train_time:114669ms step_avg:97.01ms
step:1183/1750 train_time:114771ms step_avg:97.02ms
step:1184/1750 train_time:114873ms step_avg:97.02ms
step:1185/1750 train_time:114977ms step_avg:97.03ms
step:1186/1750 train_time:115079ms step_avg:97.03ms
step:1187/1750 train_time:115181ms step_avg:97.04ms
step:1188/1750 train_time:115283ms step_avg:97.04ms
step:1189/1750 train_time:115384ms step_avg:97.04ms
step:1190/1750 train_time:115484ms step_avg:97.05ms
step:1191/1750 train_time:115586ms step_avg:97.05ms
step:1192/1750 train_time:115688ms step_avg:97.05ms
step:1193/1750 train_time:115790ms step_avg:97.06ms
step:1194/1750 train_time:115893ms step_avg:97.06ms
step:1195/1750 train_time:115996ms step_avg:97.07ms
step:1196/1750 train_time:116097ms step_avg:97.07ms
step:1197/1750 train_time:116199ms step_avg:97.08ms
step:1198/1750 train_time:116300ms step_avg:97.08ms
step:1199/1750 train_time:116401ms step_avg:97.08ms
step:1200/1750 train_time:116502ms step_avg:97.09ms
step:1201/1750 train_time:116604ms step_avg:97.09ms
step:1202/1750 train_time:116709ms step_avg:97.10ms
step:1203/1750 train_time:116810ms step_avg:97.10ms
step:1204/1750 train_time:116911ms step_avg:97.10ms
step:1205/1750 train_time:117015ms step_avg:97.11ms
step:1206/1750 train_time:117117ms step_avg:97.11ms
step:1207/1750 train_time:117220ms step_avg:97.12ms
step:1208/1750 train_time:117321ms step_avg:97.12ms
step:1209/1750 train_time:117423ms step_avg:97.12ms
step:1210/1750 train_time:117524ms step_avg:97.13ms
step:1211/1750 train_time:117625ms step_avg:97.13ms
step:1212/1750 train_time:117727ms step_avg:97.13ms
step:1213/1750 train_time:117829ms step_avg:97.14ms
step:1214/1750 train_time:117932ms step_avg:97.14ms
step:1215/1750 train_time:118036ms step_avg:97.15ms
step:1216/1750 train_time:118138ms step_avg:97.15ms
step:1217/1750 train_time:118240ms step_avg:97.16ms
step:1218/1750 train_time:118343ms step_avg:97.16ms
step:1219/1750 train_time:118445ms step_avg:97.17ms
step:1220/1750 train_time:118546ms step_avg:97.17ms
step:1221/1750 train_time:118648ms step_avg:97.17ms
step:1222/1750 train_time:118751ms step_avg:97.18ms
step:1223/1750 train_time:118853ms step_avg:97.18ms
step:1224/1750 train_time:118956ms step_avg:97.19ms
step:1225/1750 train_time:119057ms step_avg:97.19ms
step:1226/1750 train_time:119158ms step_avg:97.19ms
step:1227/1750 train_time:119260ms step_avg:97.20ms
step:1228/1750 train_time:119361ms step_avg:97.20ms
step:1229/1750 train_time:119463ms step_avg:97.20ms
step:1230/1750 train_time:119565ms step_avg:97.21ms
step:1231/1750 train_time:119667ms step_avg:97.21ms
step:1232/1750 train_time:119769ms step_avg:97.22ms
step:1233/1750 train_time:119872ms step_avg:97.22ms
step:1234/1750 train_time:119974ms step_avg:97.22ms
step:1235/1750 train_time:120076ms step_avg:97.23ms
step:1236/1750 train_time:120179ms step_avg:97.23ms
step:1237/1750 train_time:120280ms step_avg:97.23ms
step:1238/1750 train_time:120381ms step_avg:97.24ms
step:1239/1750 train_time:120482ms step_avg:97.24ms
step:1240/1750 train_time:120584ms step_avg:97.25ms
step:1241/1750 train_time:120686ms step_avg:97.25ms
step:1242/1750 train_time:120788ms step_avg:97.25ms
step:1243/1750 train_time:120890ms step_avg:97.26ms
step:1244/1750 train_time:120993ms step_avg:97.26ms
step:1245/1750 train_time:121095ms step_avg:97.27ms
step:1246/1750 train_time:121198ms step_avg:97.27ms
step:1247/1750 train_time:121298ms step_avg:97.27ms
step:1248/1750 train_time:121400ms step_avg:97.28ms
step:1249/1750 train_time:121501ms step_avg:97.28ms
step:1250/1750 train_time:121602ms step_avg:97.28ms
step:1250/1750 val_loss:3.4074 train_time:121699ms step_avg:97.36ms
step:1251/1750 train_time:121724ms step_avg:97.30ms
step:1252/1750 train_time:121813ms step_avg:97.29ms
step:1253/1750 train_time:121915ms step_avg:97.30ms
step:1254/1750 train_time:122018ms step_avg:97.30ms
step:1255/1750 train_time:122118ms step_avg:97.31ms
step:1256/1750 train_time:122219ms step_avg:97.31ms
step:1257/1750 train_time:122320ms step_avg:97.31ms
step:1258/1750 train_time:122421ms step_avg:97.31ms
step:1259/1750 train_time:122521ms step_avg:97.32ms
step:1260/1750 train_time:122622ms step_avg:97.32ms
step:1261/1750 train_time:122726ms step_avg:97.32ms
step:1262/1750 train_time:122830ms step_avg:97.33ms
step:1263/1750 train_time:122933ms step_avg:97.33ms
step:1264/1750 train_time:123034ms step_avg:97.34ms
step:1265/1750 train_time:123135ms step_avg:97.34ms
step:1266/1750 train_time:123236ms step_avg:97.34ms
step:1267/1750 train_time:123337ms step_avg:97.35ms
step:1268/1750 train_time:123439ms step_avg:97.35ms
step:1269/1750 train_time:123539ms step_avg:97.35ms
step:1270/1750 train_time:123641ms step_avg:97.36ms
step:1271/1750 train_time:123745ms step_avg:97.36ms
step:1272/1750 train_time:123848ms step_avg:97.36ms
step:1273/1750 train_time:123950ms step_avg:97.37ms
step:1274/1750 train_time:124052ms step_avg:97.37ms
step:1275/1750 train_time:124154ms step_avg:97.38ms
step:1276/1750 train_time:124256ms step_avg:97.38ms
step:1277/1750 train_time:124357ms step_avg:97.38ms
step:1278/1750 train_time:124457ms step_avg:97.38ms
step:1279/1750 train_time:124559ms step_avg:97.39ms
step:1280/1750 train_time:124660ms step_avg:97.39ms
step:1281/1750 train_time:124762ms step_avg:97.39ms
step:1282/1750 train_time:124864ms step_avg:97.40ms
step:1283/1750 train_time:124966ms step_avg:97.40ms
step:1284/1750 train_time:125069ms step_avg:97.41ms
step:1285/1750 train_time:125172ms step_avg:97.41ms
step:1286/1750 train_time:125275ms step_avg:97.41ms
step:1287/1750 train_time:125376ms step_avg:97.42ms
step:1288/1750 train_time:125477ms step_avg:97.42ms
step:1289/1750 train_time:125579ms step_avg:97.42ms
step:1290/1750 train_time:125681ms step_avg:97.43ms
step:1291/1750 train_time:125782ms step_avg:97.43ms
step:1292/1750 train_time:125883ms step_avg:97.43ms
step:1293/1750 train_time:125985ms step_avg:97.44ms
step:1294/1750 train_time:126089ms step_avg:97.44ms
step:1295/1750 train_time:126191ms step_avg:97.44ms
step:1296/1750 train_time:126294ms step_avg:97.45ms
step:1297/1750 train_time:126397ms step_avg:97.45ms
step:1298/1750 train_time:126497ms step_avg:97.46ms
step:1299/1750 train_time:126599ms step_avg:97.46ms
step:1300/1750 train_time:126700ms step_avg:97.46ms
step:1301/1750 train_time:126802ms step_avg:97.46ms
step:1302/1750 train_time:126904ms step_avg:97.47ms
step:1303/1750 train_time:127006ms step_avg:97.47ms
step:1304/1750 train_time:127108ms step_avg:97.48ms
step:1305/1750 train_time:127211ms step_avg:97.48ms
step:1306/1750 train_time:127314ms step_avg:97.48ms
step:1307/1750 train_time:127416ms step_avg:97.49ms
step:1308/1750 train_time:127518ms step_avg:97.49ms
step:1309/1750 train_time:127619ms step_avg:97.49ms
step:1310/1750 train_time:127721ms step_avg:97.50ms
step:1311/1750 train_time:127822ms step_avg:97.50ms
step:1312/1750 train_time:127925ms step_avg:97.50ms
step:1313/1750 train_time:128027ms step_avg:97.51ms
step:1314/1750 train_time:128130ms step_avg:97.51ms
step:1315/1750 train_time:128231ms step_avg:97.51ms
step:1316/1750 train_time:128333ms step_avg:97.52ms
step:1317/1750 train_time:128436ms step_avg:97.52ms
step:1318/1750 train_time:128537ms step_avg:97.52ms
step:1319/1750 train_time:128638ms step_avg:97.53ms
step:1320/1750 train_time:128741ms step_avg:97.53ms
step:1321/1750 train_time:128842ms step_avg:97.53ms
step:1322/1750 train_time:128943ms step_avg:97.54ms
step:1323/1750 train_time:129045ms step_avg:97.54ms
step:1324/1750 train_time:129148ms step_avg:97.54ms
step:1325/1750 train_time:129250ms step_avg:97.55ms
step:1326/1750 train_time:129353ms step_avg:97.55ms
step:1327/1750 train_time:129455ms step_avg:97.55ms
step:1328/1750 train_time:129557ms step_avg:97.56ms
step:1329/1750 train_time:129658ms step_avg:97.56ms
step:1330/1750 train_time:129761ms step_avg:97.56ms
step:1331/1750 train_time:129862ms step_avg:97.57ms
step:1332/1750 train_time:129964ms step_avg:97.57ms
step:1333/1750 train_time:130066ms step_avg:97.57ms
step:1334/1750 train_time:130168ms step_avg:97.58ms
step:1335/1750 train_time:130271ms step_avg:97.58ms
step:1336/1750 train_time:130374ms step_avg:97.59ms
step:1337/1750 train_time:130477ms step_avg:97.59ms
step:1338/1750 train_time:130578ms step_avg:97.59ms
step:1339/1750 train_time:130680ms step_avg:97.59ms
step:1340/1750 train_time:130781ms step_avg:97.60ms
step:1341/1750 train_time:130882ms step_avg:97.60ms
step:1342/1750 train_time:130984ms step_avg:97.60ms
step:1343/1750 train_time:131085ms step_avg:97.61ms
step:1344/1750 train_time:131187ms step_avg:97.61ms
step:1345/1750 train_time:131290ms step_avg:97.61ms
step:1346/1750 train_time:131394ms step_avg:97.62ms
step:1347/1750 train_time:131496ms step_avg:97.62ms
step:1348/1750 train_time:131599ms step_avg:97.63ms
step:1349/1750 train_time:131701ms step_avg:97.63ms
step:1350/1750 train_time:131803ms step_avg:97.63ms
step:1351/1750 train_time:131904ms step_avg:97.63ms
step:1352/1750 train_time:132006ms step_avg:97.64ms
step:1353/1750 train_time:132108ms step_avg:97.64ms
step:1354/1750 train_time:132209ms step_avg:97.64ms
step:1355/1750 train_time:132311ms step_avg:97.65ms
step:1356/1750 train_time:132415ms step_avg:97.65ms
step:1357/1750 train_time:132517ms step_avg:97.65ms
step:1358/1750 train_time:132619ms step_avg:97.66ms
step:1359/1750 train_time:132720ms step_avg:97.66ms
step:1360/1750 train_time:132822ms step_avg:97.66ms
step:1361/1750 train_time:132922ms step_avg:97.67ms
step:1362/1750 train_time:133025ms step_avg:97.67ms
step:1363/1750 train_time:133128ms step_avg:97.67ms
step:1364/1750 train_time:133230ms step_avg:97.68ms
step:1365/1750 train_time:133332ms step_avg:97.68ms
step:1366/1750 train_time:133434ms step_avg:97.68ms
step:1367/1750 train_time:133536ms step_avg:97.69ms
step:1368/1750 train_time:133638ms step_avg:97.69ms
step:1369/1750 train_time:133739ms step_avg:97.69ms
step:1370/1750 train_time:133841ms step_avg:97.69ms
step:1371/1750 train_time:133943ms step_avg:97.70ms
step:1372/1750 train_time:134046ms step_avg:97.70ms
step:1373/1750 train_time:134148ms step_avg:97.70ms
step:1374/1750 train_time:134250ms step_avg:97.71ms
step:1375/1750 train_time:134353ms step_avg:97.71ms
step:1375/1750 val_loss:3.3662 train_time:134450ms step_avg:97.78ms
step:1376/1750 train_time:134475ms step_avg:97.73ms
step:1377/1750 train_time:134571ms step_avg:97.73ms
step:1378/1750 train_time:134672ms step_avg:97.73ms
step:1379/1750 train_time:134774ms step_avg:97.73ms
step:1380/1750 train_time:134877ms step_avg:97.74ms
step:1381/1750 train_time:134978ms step_avg:97.74ms
step:1382/1750 train_time:135079ms step_avg:97.74ms
step:1383/1750 train_time:135179ms step_avg:97.74ms
step:1384/1750 train_time:135279ms step_avg:97.75ms
step:1385/1750 train_time:135380ms step_avg:97.75ms
step:1386/1750 train_time:135483ms step_avg:97.75ms
step:1387/1750 train_time:135587ms step_avg:97.76ms
step:1388/1750 train_time:135690ms step_avg:97.76ms
step:1389/1750 train_time:135792ms step_avg:97.76ms
step:1390/1750 train_time:135894ms step_avg:97.77ms
step:1391/1750 train_time:135996ms step_avg:97.77ms
step:1392/1750 train_time:136098ms step_avg:97.77ms
step:1393/1750 train_time:136199ms step_avg:97.77ms
step:1394/1750 train_time:136301ms step_avg:97.78ms
step:1395/1750 train_time:136403ms step_avg:97.78ms
step:1396/1750 train_time:136504ms step_avg:97.78ms
step:1397/1750 train_time:136606ms step_avg:97.79ms
step:1398/1750 train_time:136708ms step_avg:97.79ms
step:1399/1750 train_time:136811ms step_avg:97.79ms
step:1400/1750 train_time:136913ms step_avg:97.79ms
step:1401/1750 train_time:137016ms step_avg:97.80ms
step:1402/1750 train_time:137118ms step_avg:97.80ms
step:1403/1750 train_time:137221ms step_avg:97.81ms
step:1404/1750 train_time:137322ms step_avg:97.81ms
step:1405/1750 train_time:137423ms step_avg:97.81ms
step:1406/1750 train_time:137524ms step_avg:97.81ms
step:1407/1750 train_time:137628ms step_avg:97.82ms
step:1408/1750 train_time:137731ms step_avg:97.82ms
step:1409/1750 train_time:137834ms step_avg:97.82ms
step:1410/1750 train_time:137936ms step_avg:97.83ms
step:1411/1750 train_time:138037ms step_avg:97.83ms
step:1412/1750 train_time:138139ms step_avg:97.83ms
step:1413/1750 train_time:138240ms step_avg:97.83ms
step:1414/1750 train_time:138342ms step_avg:97.84ms
step:1415/1750 train_time:138445ms step_avg:97.84ms
step:1416/1750 train_time:138546ms step_avg:97.84ms
step:1417/1750 train_time:138648ms step_avg:97.85ms
step:1418/1750 train_time:138750ms step_avg:97.85ms
step:1419/1750 train_time:138853ms step_avg:97.85ms
step:1420/1750 train_time:138956ms step_avg:97.86ms
step:1421/1750 train_time:139059ms step_avg:97.86ms
step:1422/1750 train_time:139160ms step_avg:97.86ms
step:1423/1750 train_time:139261ms step_avg:97.86ms
step:1424/1750 train_time:139363ms step_avg:97.87ms
step:1425/1750 train_time:139465ms step_avg:97.87ms
step:1426/1750 train_time:139566ms step_avg:97.87ms
step:1427/1750 train_time:139669ms step_avg:97.88ms
step:1428/1750 train_time:139773ms step_avg:97.88ms
step:1429/1750 train_time:139876ms step_avg:97.88ms
step:1430/1750 train_time:139979ms step_avg:97.89ms
step:1431/1750 train_time:140083ms step_avg:97.89ms
step:1432/1750 train_time:140185ms step_avg:97.89ms
step:1433/1750 train_time:140287ms step_avg:97.90ms
step:1434/1750 train_time:140389ms step_avg:97.90ms
step:1435/1750 train_time:140494ms step_avg:97.90ms
step:1436/1750 train_time:140597ms step_avg:97.91ms
step:1437/1750 train_time:140701ms step_avg:97.91ms
step:1438/1750 train_time:140804ms step_avg:97.92ms
step:1439/1750 train_time:140907ms step_avg:97.92ms
step:1440/1750 train_time:141011ms step_avg:97.92ms
step:1441/1750 train_time:141117ms step_avg:97.93ms
step:1442/1750 train_time:141219ms step_avg:97.93ms
step:1443/1750 train_time:141322ms step_avg:97.94ms
step:1444/1750 train_time:141425ms step_avg:97.94ms
step:1445/1750 train_time:141527ms step_avg:97.94ms
step:1446/1750 train_time:141629ms step_avg:97.95ms
step:1447/1750 train_time:141733ms step_avg:97.95ms
step:1448/1750 train_time:141836ms step_avg:97.95ms
step:1449/1750 train_time:141938ms step_avg:97.96ms
step:1450/1750 train_time:142040ms step_avg:97.96ms
step:1451/1750 train_time:142143ms step_avg:97.96ms
step:1452/1750 train_time:142246ms step_avg:97.97ms
step:1453/1750 train_time:142349ms step_avg:97.97ms
step:1454/1750 train_time:142454ms step_avg:97.97ms
step:1455/1750 train_time:142556ms step_avg:97.98ms
step:1456/1750 train_time:142658ms step_avg:97.98ms
step:1457/1750 train_time:142762ms step_avg:97.98ms
step:1458/1750 train_time:142865ms step_avg:97.99ms
step:1459/1750 train_time:142967ms step_avg:97.99ms
step:1460/1750 train_time:143070ms step_avg:97.99ms
step:1461/1750 train_time:143174ms step_avg:98.00ms
step:1462/1750 train_time:143277ms step_avg:98.00ms
step:1463/1750 train_time:143380ms step_avg:98.00ms
step:1464/1750 train_time:143483ms step_avg:98.01ms
step:1465/1750 train_time:143585ms step_avg:98.01ms
step:1466/1750 train_time:143688ms step_avg:98.01ms
step:1467/1750 train_time:143791ms step_avg:98.02ms
step:1468/1750 train_time:143895ms step_avg:98.02ms
step:1469/1750 train_time:143998ms step_avg:98.02ms
step:1470/1750 train_time:144101ms step_avg:98.03ms
step:1471/1750 train_time:144204ms step_avg:98.03ms
step:1472/1750 train_time:144307ms step_avg:98.03ms
step:1473/1750 train_time:144409ms step_avg:98.04ms
step:1474/1750 train_time:144513ms step_avg:98.04ms
step:1475/1750 train_time:144616ms step_avg:98.04ms
step:1476/1750 train_time:144719ms step_avg:98.05ms
step:1477/1750 train_time:144821ms step_avg:98.05ms
step:1478/1750 train_time:144925ms step_avg:98.05ms
step:1479/1750 train_time:145026ms step_avg:98.06ms
step:1480/1750 train_time:145130ms step_avg:98.06ms
step:1481/1750 train_time:145234ms step_avg:98.06ms
step:1482/1750 train_time:145337ms step_avg:98.07ms
step:1483/1750 train_time:145440ms step_avg:98.07ms
step:1484/1750 train_time:145543ms step_avg:98.07ms
step:1485/1750 train_time:145647ms step_avg:98.08ms
step:1486/1750 train_time:145750ms step_avg:98.08ms
step:1487/1750 train_time:145854ms step_avg:98.09ms
step:1488/1750 train_time:145958ms step_avg:98.09ms
step:1489/1750 train_time:146061ms step_avg:98.09ms
step:1490/1750 train_time:146163ms step_avg:98.10ms
step:1491/1750 train_time:146266ms step_avg:98.10ms
step:1492/1750 train_time:146368ms step_avg:98.10ms
step:1493/1750 train_time:146471ms step_avg:98.11ms
step:1494/1750 train_time:146575ms step_avg:98.11ms
step:1495/1750 train_time:146678ms step_avg:98.11ms
step:1496/1750 train_time:146780ms step_avg:98.11ms
step:1497/1750 train_time:146881ms step_avg:98.12ms
step:1498/1750 train_time:146984ms step_avg:98.12ms
step:1499/1750 train_time:147085ms step_avg:98.12ms
step:1500/1750 train_time:147189ms step_avg:98.13ms
step:1500/1750 val_loss:3.3291 train_time:147287ms step_avg:98.19ms
step:1501/1750 train_time:147312ms step_avg:98.14ms
step:1502/1750 train_time:147408ms step_avg:98.14ms
step:1503/1750 train_time:147510ms step_avg:98.14ms
step:1504/1750 train_time:147613ms step_avg:98.15ms
step:1505/1750 train_time:147715ms step_avg:98.15ms
step:1506/1750 train_time:147817ms step_avg:98.15ms
step:1507/1750 train_time:147919ms step_avg:98.15ms
step:1508/1750 train_time:148021ms step_avg:98.16ms
step:1509/1750 train_time:148124ms step_avg:98.16ms
step:1510/1750 train_time:148227ms step_avg:98.16ms
step:1511/1750 train_time:148333ms step_avg:98.17ms
step:1512/1750 train_time:148436ms step_avg:98.17ms
step:1513/1750 train_time:148540ms step_avg:98.18ms
step:1514/1750 train_time:148644ms step_avg:98.18ms
step:1515/1750 train_time:148751ms step_avg:98.19ms
step:1516/1750 train_time:148854ms step_avg:98.19ms
step:1517/1750 train_time:148956ms step_avg:98.19ms
step:1518/1750 train_time:149059ms step_avg:98.19ms
step:1519/1750 train_time:149163ms step_avg:98.20ms
step:1520/1750 train_time:149265ms step_avg:98.20ms
step:1521/1750 train_time:149368ms step_avg:98.20ms
step:1522/1750 train_time:149471ms step_avg:98.21ms
step:1523/1750 train_time:149574ms step_avg:98.21ms
step:1524/1750 train_time:149679ms step_avg:98.21ms
step:1525/1750 train_time:149783ms step_avg:98.22ms
step:1526/1750 train_time:149886ms step_avg:98.22ms
step:1527/1750 train_time:149990ms step_avg:98.23ms
step:1528/1750 train_time:150095ms step_avg:98.23ms
step:1529/1750 train_time:150197ms step_avg:98.23ms
step:1530/1750 train_time:150301ms step_avg:98.24ms
step:1531/1750 train_time:150403ms step_avg:98.24ms
step:1532/1750 train_time:150506ms step_avg:98.24ms
step:1533/1750 train_time:150608ms step_avg:98.24ms
step:1534/1750 train_time:150712ms step_avg:98.25ms
step:1535/1750 train_time:150816ms step_avg:98.25ms
step:1536/1750 train_time:150917ms step_avg:98.25ms
step:1537/1750 train_time:151020ms step_avg:98.26ms
step:1538/1750 train_time:151124ms step_avg:98.26ms
step:1539/1750 train_time:151226ms step_avg:98.26ms
step:1540/1750 train_time:151330ms step_avg:98.27ms
step:1541/1750 train_time:151432ms step_avg:98.27ms
step:1542/1750 train_time:151536ms step_avg:98.27ms
step:1543/1750 train_time:151639ms step_avg:98.28ms
step:1544/1750 train_time:151742ms step_avg:98.28ms
step:1545/1750 train_time:151846ms step_avg:98.28ms
step:1546/1750 train_time:151950ms step_avg:98.29ms
step:1547/1750 train_time:152053ms step_avg:98.29ms
step:1548/1750 train_time:152158ms step_avg:98.29ms
step:1549/1750 train_time:152262ms step_avg:98.30ms
step:1550/1750 train_time:152366ms step_avg:98.30ms
step:1551/1750 train_time:152470ms step_avg:98.30ms
step:1552/1750 train_time:152573ms step_avg:98.31ms
step:1553/1750 train_time:152675ms step_avg:98.31ms
step:1554/1750 train_time:152778ms step_avg:98.31ms
step:1555/1750 train_time:152880ms step_avg:98.31ms
step:1556/1750 train_time:152983ms step_avg:98.32ms
step:1557/1750 train_time:153086ms step_avg:98.32ms
step:1558/1750 train_time:153191ms step_avg:98.33ms
step:1559/1750 train_time:153295ms step_avg:98.33ms
step:1560/1750 train_time:153398ms step_avg:98.33ms
step:1561/1750 train_time:153500ms step_avg:98.33ms
step:1562/1750 train_time:153604ms step_avg:98.34ms
step:1563/1750 train_time:153710ms step_avg:98.34ms
step:1564/1750 train_time:153813ms step_avg:98.35ms
step:1565/1750 train_time:153915ms step_avg:98.35ms
step:1566/1750 train_time:154017ms step_avg:98.35ms
step:1567/1750 train_time:154119ms step_avg:98.35ms
step:1568/1750 train_time:154222ms step_avg:98.36ms
step:1569/1750 train_time:154324ms step_avg:98.36ms
step:1570/1750 train_time:154430ms step_avg:98.36ms
step:1571/1750 train_time:154533ms step_avg:98.37ms
step:1572/1750 train_time:154635ms step_avg:98.37ms
step:1573/1750 train_time:154738ms step_avg:98.37ms
step:1574/1750 train_time:154842ms step_avg:98.37ms
step:1575/1750 train_time:154944ms step_avg:98.38ms
step:1576/1750 train_time:155046ms step_avg:98.38ms
step:1577/1750 train_time:155150ms step_avg:98.38ms
step:1578/1750 train_time:155253ms step_avg:98.39ms
step:1579/1750 train_time:155356ms step_avg:98.39ms
step:1580/1750 train_time:155459ms step_avg:98.39ms
step:1581/1750 train_time:155562ms step_avg:98.39ms
step:1582/1750 train_time:155665ms step_avg:98.40ms
step:1583/1750 train_time:155771ms step_avg:98.40ms
step:1584/1750 train_time:155877ms step_avg:98.41ms
step:1585/1750 train_time:155980ms step_avg:98.41ms
step:1586/1750 train_time:156083ms step_avg:98.41ms
step:1587/1750 train_time:156186ms step_avg:98.42ms
step:1588/1750 train_time:156290ms step_avg:98.42ms
step:1589/1750 train_time:156394ms step_avg:98.42ms
step:1590/1750 train_time:156497ms step_avg:98.43ms
step:1591/1750 train_time:156599ms step_avg:98.43ms
step:1592/1750 train_time:156703ms step_avg:98.43ms
step:1593/1750 train_time:156806ms step_avg:98.43ms
step:1594/1750 train_time:156912ms step_avg:98.44ms
step:1595/1750 train_time:157014ms step_avg:98.44ms
step:1596/1750 train_time:157116ms step_avg:98.44ms
step:1597/1750 train_time:157219ms step_avg:98.45ms
step:1598/1750 train_time:157323ms step_avg:98.45ms
step:1599/1750 train_time:157426ms step_avg:98.45ms
step:1600/1750 train_time:157531ms step_avg:98.46ms
step:1601/1750 train_time:157635ms step_avg:98.46ms
step:1602/1750 train_time:157738ms step_avg:98.46ms
step:1603/1750 train_time:157840ms step_avg:98.47ms
step:1604/1750 train_time:157944ms step_avg:98.47ms
step:1605/1750 train_time:158049ms step_avg:98.47ms
step:1606/1750 train_time:158152ms step_avg:98.48ms
step:1607/1750 train_time:158255ms step_avg:98.48ms
step:1608/1750 train_time:158357ms step_avg:98.48ms
step:1609/1750 train_time:158460ms step_avg:98.48ms
step:1610/1750 train_time:158565ms step_avg:98.49ms
step:1611/1750 train_time:158669ms step_avg:98.49ms
step:1612/1750 train_time:158772ms step_avg:98.49ms
step:1613/1750 train_time:158875ms step_avg:98.50ms
step:1614/1750 train_time:158977ms step_avg:98.50ms
step:1615/1750 train_time:159079ms step_avg:98.50ms
step:1616/1750 train_time:159181ms step_avg:98.50ms
step:1617/1750 train_time:159284ms step_avg:98.51ms
step:1618/1750 train_time:159388ms step_avg:98.51ms
step:1619/1750 train_time:159491ms step_avg:98.51ms
step:1620/1750 train_time:159595ms step_avg:98.52ms
step:1621/1750 train_time:159697ms step_avg:98.52ms
step:1622/1750 train_time:159799ms step_avg:98.52ms
step:1623/1750 train_time:159902ms step_avg:98.52ms
step:1624/1750 train_time:160008ms step_avg:98.53ms
step:1625/1750 train_time:160112ms step_avg:98.53ms
step:1625/1750 val_loss:3.2995 train_time:160210ms step_avg:98.59ms
step:1626/1750 train_time:160235ms step_avg:98.55ms
step:1627/1750 train_time:160324ms step_avg:98.54ms
step:1628/1750 train_time:160426ms step_avg:98.54ms
step:1629/1750 train_time:160528ms step_avg:98.54ms
step:1630/1750 train_time:160632ms step_avg:98.55ms
step:1631/1750 train_time:160733ms step_avg:98.55ms
step:1632/1750 train_time:160836ms step_avg:98.55ms
step:1633/1750 train_time:160938ms step_avg:98.55ms
step:1634/1750 train_time:161043ms step_avg:98.56ms
step:1635/1750 train_time:161145ms step_avg:98.56ms
step:1636/1750 train_time:161250ms step_avg:98.56ms
step:1637/1750 train_time:161356ms step_avg:98.57ms
step:1638/1750 train_time:161459ms step_avg:98.57ms
step:1639/1750 train_time:161563ms step_avg:98.57ms
step:1640/1750 train_time:161666ms step_avg:98.58ms
step:1641/1750 train_time:161768ms step_avg:98.58ms
step:1642/1750 train_time:161870ms step_avg:98.58ms
step:1643/1750 train_time:161973ms step_avg:98.58ms
step:1644/1750 train_time:162076ms step_avg:98.59ms
step:1645/1750 train_time:162179ms step_avg:98.59ms
step:1646/1750 train_time:162283ms step_avg:98.59ms
step:1647/1750 train_time:162388ms step_avg:98.60ms
step:1648/1750 train_time:162492ms step_avg:98.60ms
step:1649/1750 train_time:162596ms step_avg:98.60ms
step:1650/1750 train_time:162700ms step_avg:98.61ms
step:1651/1750 train_time:162802ms step_avg:98.61ms
step:1652/1750 train_time:162905ms step_avg:98.61ms
step:1653/1750 train_time:163008ms step_avg:98.61ms
step:1654/1750 train_time:163111ms step_avg:98.62ms
step:1655/1750 train_time:163216ms step_avg:98.62ms
step:1656/1750 train_time:163320ms step_avg:98.62ms
step:1657/1750 train_time:163424ms step_avg:98.63ms
step:1658/1750 train_time:163526ms step_avg:98.63ms
step:1659/1750 train_time:163634ms step_avg:98.63ms
step:1660/1750 train_time:163737ms step_avg:98.64ms
step:1661/1750 train_time:163844ms step_avg:98.64ms
step:1662/1750 train_time:163947ms step_avg:98.64ms
step:1663/1750 train_time:164050ms step_avg:98.65ms
step:1664/1750 train_time:164153ms step_avg:98.65ms
step:1665/1750 train_time:164257ms step_avg:98.65ms
step:1666/1750 train_time:164361ms step_avg:98.66ms
step:1667/1750 train_time:164464ms step_avg:98.66ms
step:1668/1750 train_time:164569ms step_avg:98.66ms
step:1669/1750 train_time:164673ms step_avg:98.67ms
step:1670/1750 train_time:164776ms step_avg:98.67ms
step:1671/1750 train_time:164880ms step_avg:98.67ms
step:1672/1750 train_time:164986ms step_avg:98.68ms
step:1673/1750 train_time:165087ms step_avg:98.68ms
step:1674/1750 train_time:165190ms step_avg:98.68ms
step:1675/1750 train_time:165293ms step_avg:98.68ms
step:1676/1750 train_time:165396ms step_avg:98.69ms
step:1677/1750 train_time:165502ms step_avg:98.69ms
step:1678/1750 train_time:165606ms step_avg:98.69ms
step:1679/1750 train_time:165709ms step_avg:98.69ms
step:1680/1750 train_time:165812ms step_avg:98.70ms
step:1681/1750 train_time:165917ms step_avg:98.70ms
step:1682/1750 train_time:166021ms step_avg:98.70ms
step:1683/1750 train_time:166124ms step_avg:98.71ms
step:1684/1750 train_time:166227ms step_avg:98.71ms
step:1685/1750 train_time:166330ms step_avg:98.71ms
step:1686/1750 train_time:166433ms step_avg:98.71ms
step:1687/1750 train_time:166536ms step_avg:98.72ms
step:1688/1750 train_time:166641ms step_avg:98.72ms
step:1689/1750 train_time:166746ms step_avg:98.72ms
step:1690/1750 train_time:166849ms step_avg:98.73ms
step:1691/1750 train_time:166952ms step_avg:98.73ms
step:1692/1750 train_time:167055ms step_avg:98.73ms
step:1693/1750 train_time:167160ms step_avg:98.74ms
step:1694/1750 train_time:167264ms step_avg:98.74ms
step:1695/1750 train_time:167369ms step_avg:98.74ms
step:1696/1750 train_time:167472ms step_avg:98.74ms
step:1697/1750 train_time:167578ms step_avg:98.75ms
step:1698/1750 train_time:167681ms step_avg:98.75ms
step:1699/1750 train_time:167784ms step_avg:98.75ms
step:1700/1750 train_time:167888ms step_avg:98.76ms
step:1701/1750 train_time:167991ms step_avg:98.76ms
step:1702/1750 train_time:168098ms step_avg:98.76ms
step:1703/1750 train_time:168202ms step_avg:98.77ms
step:1704/1750 train_time:168306ms step_avg:98.77ms
step:1705/1750 train_time:168409ms step_avg:98.77ms
step:1706/1750 train_time:168514ms step_avg:98.78ms
step:1707/1750 train_time:168618ms step_avg:98.78ms
step:1708/1750 train_time:168723ms step_avg:98.78ms
step:1709/1750 train_time:168827ms step_avg:98.79ms
step:1710/1750 train_time:168931ms step_avg:98.79ms
step:1711/1750 train_time:169036ms step_avg:98.79ms
step:1712/1750 train_time:169139ms step_avg:98.80ms
step:1713/1750 train_time:169245ms step_avg:98.80ms
step:1714/1750 train_time:169348ms step_avg:98.80ms
step:1715/1750 train_time:169453ms step_avg:98.81ms
step:1716/1750 train_time:169557ms step_avg:98.81ms
step:1717/1750 train_time:169660ms step_avg:98.81ms
step:1718/1750 train_time:169765ms step_avg:98.82ms
step:1719/1750 train_time:169871ms step_avg:98.82ms
step:1720/1750 train_time:169975ms step_avg:98.82ms
step:1721/1750 train_time:170080ms step_avg:98.83ms
step:1722/1750 train_time:170184ms step_avg:98.83ms
step:1723/1750 train_time:170287ms step_avg:98.83ms
step:1724/1750 train_time:170392ms step_avg:98.84ms
step:1725/1750 train_time:170496ms step_avg:98.84ms
step:1726/1750 train_time:170601ms step_avg:98.84ms
step:1727/1750 train_time:170705ms step_avg:98.84ms
step:1728/1750 train_time:170810ms step_avg:98.85ms
step:1729/1750 train_time:170914ms step_avg:98.85ms
step:1730/1750 train_time:171016ms step_avg:98.85ms
step:1731/1750 train_time:171121ms step_avg:98.86ms
step:1732/1750 train_time:171225ms step_avg:98.86ms
step:1733/1750 train_time:171330ms step_avg:98.86ms
step:1734/1750 train_time:171436ms step_avg:98.87ms
step:1735/1750 train_time:171539ms step_avg:98.87ms
step:1736/1750 train_time:171644ms step_avg:98.87ms
step:1737/1750 train_time:171749ms step_avg:98.88ms
step:1738/1750 train_time:171852ms step_avg:98.88ms
step:1739/1750 train_time:171956ms step_avg:98.88ms
step:1740/1750 train_time:172061ms step_avg:98.89ms
step:1741/1750 train_time:172170ms step_avg:98.89ms
step:1742/1750 train_time:172274ms step_avg:98.89ms
step:1743/1750 train_time:172379ms step_avg:98.90ms
step:1744/1750 train_time:172484ms step_avg:98.90ms
step:1745/1750 train_time:172588ms step_avg:98.90ms
step:1746/1750 train_time:172692ms step_avg:98.91ms
step:1747/1750 train_time:172797ms step_avg:98.91ms
step:1748/1750 train_time:172903ms step_avg:98.91ms
step:1749/1750 train_time:173006ms step_avg:98.92ms
step:1750/1750 train_time:173111ms step_avg:98.92ms
step:1750/1750 val_loss:3.2784 train_time:173209ms step_avg:98.98ms
peak memory allocated: 33644 MiB reserved: 48552 MiB

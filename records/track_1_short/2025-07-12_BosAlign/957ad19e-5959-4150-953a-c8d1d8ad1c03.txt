import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 00:43:37 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            116W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            118W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           59740      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           59741      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           59742      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           59743      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           59744      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           59745      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           59746      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           59747      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           59741      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           59742      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           59743      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           59744      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           59745      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           59746      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           59747      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.01ms
step:1/1750 train_time:148ms step_avg:148.36ms
step:2/1750 train_time:172ms step_avg:86.16ms
step:3/1750 train_time:246ms step_avg:82.09ms
step:4/1750 train_time:338ms step_avg:84.52ms
step:5/1750 train_time:430ms step_avg:86.09ms
step:6/1750 train_time:523ms step_avg:87.20ms
step:7/1750 train_time:616ms step_avg:87.97ms
step:8/1750 train_time:708ms step_avg:88.51ms
step:9/1750 train_time:801ms step_avg:89.00ms
step:10/1750 train_time:893ms step_avg:89.34ms
step:11/1750 train_time:986ms step_avg:89.63ms
step:12/1750 train_time:1080ms step_avg:89.96ms
step:13/1750 train_time:1175ms step_avg:90.38ms
step:14/1750 train_time:1269ms step_avg:90.68ms
step:15/1750 train_time:1363ms step_avg:90.88ms
step:16/1750 train_time:1457ms step_avg:91.05ms
step:17/1750 train_time:1549ms step_avg:91.12ms
step:18/1750 train_time:1642ms step_avg:91.21ms
step:19/1750 train_time:1735ms step_avg:91.29ms
step:20/1750 train_time:1827ms step_avg:91.37ms
step:21/1750 train_time:1921ms step_avg:91.45ms
step:22/1750 train_time:2015ms step_avg:91.57ms
step:23/1750 train_time:2108ms step_avg:91.66ms
step:24/1750 train_time:2202ms step_avg:91.76ms
step:25/1750 train_time:2296ms step_avg:91.83ms
step:26/1750 train_time:2389ms step_avg:91.90ms
step:27/1750 train_time:2483ms step_avg:91.96ms
step:28/1750 train_time:2577ms step_avg:92.03ms
step:29/1750 train_time:2670ms step_avg:92.06ms
step:30/1750 train_time:2763ms step_avg:92.10ms
step:31/1750 train_time:2856ms step_avg:92.13ms
step:32/1750 train_time:2949ms step_avg:92.16ms
step:33/1750 train_time:3043ms step_avg:92.20ms
step:34/1750 train_time:3138ms step_avg:92.28ms
step:35/1750 train_time:3231ms step_avg:92.31ms
step:36/1750 train_time:3325ms step_avg:92.35ms
step:37/1750 train_time:3418ms step_avg:92.37ms
step:38/1750 train_time:3510ms step_avg:92.37ms
step:39/1750 train_time:3603ms step_avg:92.39ms
step:40/1750 train_time:3697ms step_avg:92.41ms
step:41/1750 train_time:3790ms step_avg:92.44ms
step:42/1750 train_time:3883ms step_avg:92.45ms
step:43/1750 train_time:3976ms step_avg:92.47ms
step:44/1750 train_time:4071ms step_avg:92.53ms
step:45/1750 train_time:4163ms step_avg:92.52ms
step:46/1750 train_time:4257ms step_avg:92.54ms
step:47/1750 train_time:4350ms step_avg:92.55ms
step:48/1750 train_time:4443ms step_avg:92.57ms
step:49/1750 train_time:4536ms step_avg:92.58ms
step:50/1750 train_time:4630ms step_avg:92.59ms
step:51/1750 train_time:4723ms step_avg:92.60ms
step:52/1750 train_time:4816ms step_avg:92.62ms
step:53/1750 train_time:4909ms step_avg:92.63ms
step:54/1750 train_time:5003ms step_avg:92.64ms
step:55/1750 train_time:5096ms step_avg:92.65ms
step:56/1750 train_time:5189ms step_avg:92.67ms
step:57/1750 train_time:5283ms step_avg:92.69ms
step:58/1750 train_time:5377ms step_avg:92.71ms
step:59/1750 train_time:5470ms step_avg:92.71ms
step:60/1750 train_time:5563ms step_avg:92.72ms
step:61/1750 train_time:5657ms step_avg:92.74ms
step:62/1750 train_time:5750ms step_avg:92.74ms
step:63/1750 train_time:5843ms step_avg:92.75ms
step:64/1750 train_time:5937ms step_avg:92.76ms
step:65/1750 train_time:6030ms step_avg:92.77ms
step:66/1750 train_time:6122ms step_avg:92.76ms
step:67/1750 train_time:6216ms step_avg:92.77ms
step:68/1750 train_time:6308ms step_avg:92.77ms
step:69/1750 train_time:6402ms step_avg:92.78ms
step:70/1750 train_time:6495ms step_avg:92.79ms
step:71/1750 train_time:6588ms step_avg:92.79ms
step:72/1750 train_time:6682ms step_avg:92.80ms
step:73/1750 train_time:6775ms step_avg:92.81ms
step:74/1750 train_time:6868ms step_avg:92.81ms
step:75/1750 train_time:6962ms step_avg:92.83ms
step:76/1750 train_time:7055ms step_avg:92.83ms
step:77/1750 train_time:7148ms step_avg:92.83ms
step:78/1750 train_time:7241ms step_avg:92.83ms
step:79/1750 train_time:7334ms step_avg:92.83ms
step:80/1750 train_time:7427ms step_avg:92.84ms
step:81/1750 train_time:7521ms step_avg:92.85ms
step:82/1750 train_time:7614ms step_avg:92.85ms
step:83/1750 train_time:7707ms step_avg:92.85ms
step:84/1750 train_time:7801ms step_avg:92.87ms
step:85/1750 train_time:7893ms step_avg:92.86ms
step:86/1750 train_time:7986ms step_avg:92.86ms
step:87/1750 train_time:8079ms step_avg:92.86ms
step:88/1750 train_time:8171ms step_avg:92.85ms
step:89/1750 train_time:8264ms step_avg:92.85ms
step:90/1750 train_time:8357ms step_avg:92.86ms
step:91/1750 train_time:8450ms step_avg:92.86ms
step:92/1750 train_time:8544ms step_avg:92.87ms
step:93/1750 train_time:8638ms step_avg:92.89ms
step:94/1750 train_time:8731ms step_avg:92.89ms
step:95/1750 train_time:8825ms step_avg:92.89ms
step:96/1750 train_time:8918ms step_avg:92.90ms
step:97/1750 train_time:9011ms step_avg:92.90ms
step:98/1750 train_time:9104ms step_avg:92.90ms
step:99/1750 train_time:9198ms step_avg:92.91ms
step:100/1750 train_time:9291ms step_avg:92.91ms
step:101/1750 train_time:9383ms step_avg:92.90ms
step:102/1750 train_time:9477ms step_avg:92.91ms
step:103/1750 train_time:9570ms step_avg:92.91ms
step:104/1750 train_time:9663ms step_avg:92.91ms
step:105/1750 train_time:9756ms step_avg:92.91ms
step:106/1750 train_time:9850ms step_avg:92.93ms
step:107/1750 train_time:9944ms step_avg:92.93ms
step:108/1750 train_time:10037ms step_avg:92.94ms
step:109/1750 train_time:10130ms step_avg:92.94ms
step:110/1750 train_time:10224ms step_avg:92.94ms
step:111/1750 train_time:10317ms step_avg:92.94ms
step:112/1750 train_time:10410ms step_avg:92.94ms
step:113/1750 train_time:10504ms step_avg:92.96ms
step:114/1750 train_time:10597ms step_avg:92.96ms
step:115/1750 train_time:10690ms step_avg:92.95ms
step:116/1750 train_time:10782ms step_avg:92.95ms
step:117/1750 train_time:10875ms step_avg:92.95ms
step:118/1750 train_time:10969ms step_avg:92.96ms
step:119/1750 train_time:11063ms step_avg:92.97ms
step:120/1750 train_time:11156ms step_avg:92.97ms
step:121/1750 train_time:11249ms step_avg:92.97ms
step:122/1750 train_time:11343ms step_avg:92.97ms
step:123/1750 train_time:11436ms step_avg:92.97ms
step:124/1750 train_time:11529ms step_avg:92.98ms
step:125/1750 train_time:11623ms step_avg:92.98ms
step:125/1750 val_loss:4.6456 train_time:11712ms step_avg:93.69ms
step:126/1750 train_time:11737ms step_avg:93.15ms
step:127/1750 train_time:11816ms step_avg:93.04ms
step:128/1750 train_time:11917ms step_avg:93.10ms
step:129/1750 train_time:12011ms step_avg:93.11ms
step:130/1750 train_time:12105ms step_avg:93.11ms
step:131/1750 train_time:12198ms step_avg:93.12ms
step:132/1750 train_time:12291ms step_avg:93.12ms
step:133/1750 train_time:12384ms step_avg:93.11ms
step:134/1750 train_time:12477ms step_avg:93.11ms
step:135/1750 train_time:12570ms step_avg:93.11ms
step:136/1750 train_time:12663ms step_avg:93.11ms
step:137/1750 train_time:12757ms step_avg:93.12ms
step:138/1750 train_time:12852ms step_avg:93.13ms
step:139/1750 train_time:12947ms step_avg:93.15ms
step:140/1750 train_time:13043ms step_avg:93.16ms
step:141/1750 train_time:13136ms step_avg:93.16ms
step:142/1750 train_time:13230ms step_avg:93.17ms
step:143/1750 train_time:13323ms step_avg:93.17ms
step:144/1750 train_time:13416ms step_avg:93.16ms
step:145/1750 train_time:13509ms step_avg:93.16ms
step:146/1750 train_time:13602ms step_avg:93.17ms
step:147/1750 train_time:13695ms step_avg:93.16ms
step:148/1750 train_time:13789ms step_avg:93.17ms
step:149/1750 train_time:13884ms step_avg:93.18ms
step:150/1750 train_time:13978ms step_avg:93.19ms
step:151/1750 train_time:14072ms step_avg:93.19ms
step:152/1750 train_time:14167ms step_avg:93.20ms
step:153/1750 train_time:14261ms step_avg:93.21ms
step:154/1750 train_time:14354ms step_avg:93.21ms
step:155/1750 train_time:14447ms step_avg:93.21ms
step:156/1750 train_time:14541ms step_avg:93.21ms
step:157/1750 train_time:14634ms step_avg:93.21ms
step:158/1750 train_time:14728ms step_avg:93.21ms
step:159/1750 train_time:14822ms step_avg:93.22ms
step:160/1750 train_time:14916ms step_avg:93.23ms
step:161/1750 train_time:15010ms step_avg:93.23ms
step:162/1750 train_time:15104ms step_avg:93.24ms
step:163/1750 train_time:15198ms step_avg:93.24ms
step:164/1750 train_time:15291ms step_avg:93.24ms
step:165/1750 train_time:15385ms step_avg:93.24ms
step:166/1750 train_time:15478ms step_avg:93.24ms
step:167/1750 train_time:15571ms step_avg:93.24ms
step:168/1750 train_time:15665ms step_avg:93.24ms
step:169/1750 train_time:15758ms step_avg:93.24ms
step:170/1750 train_time:15852ms step_avg:93.25ms
step:171/1750 train_time:15946ms step_avg:93.25ms
step:172/1750 train_time:16040ms step_avg:93.26ms
step:173/1750 train_time:16134ms step_avg:93.26ms
step:174/1750 train_time:16228ms step_avg:93.27ms
step:175/1750 train_time:16322ms step_avg:93.27ms
step:176/1750 train_time:16415ms step_avg:93.27ms
step:177/1750 train_time:16509ms step_avg:93.27ms
step:178/1750 train_time:16602ms step_avg:93.27ms
step:179/1750 train_time:16695ms step_avg:93.27ms
step:180/1750 train_time:16789ms step_avg:93.27ms
step:181/1750 train_time:16884ms step_avg:93.28ms
step:182/1750 train_time:16977ms step_avg:93.28ms
step:183/1750 train_time:17071ms step_avg:93.28ms
step:184/1750 train_time:17165ms step_avg:93.29ms
step:185/1750 train_time:17259ms step_avg:93.29ms
step:186/1750 train_time:17352ms step_avg:93.29ms
step:187/1750 train_time:17446ms step_avg:93.29ms
step:188/1750 train_time:17540ms step_avg:93.30ms
step:189/1750 train_time:17633ms step_avg:93.30ms
step:190/1750 train_time:17727ms step_avg:93.30ms
step:191/1750 train_time:17821ms step_avg:93.30ms
step:192/1750 train_time:17914ms step_avg:93.30ms
step:193/1750 train_time:18008ms step_avg:93.31ms
step:194/1750 train_time:18103ms step_avg:93.31ms
step:195/1750 train_time:18196ms step_avg:93.31ms
step:196/1750 train_time:18290ms step_avg:93.31ms
step:197/1750 train_time:18384ms step_avg:93.32ms
step:198/1750 train_time:18478ms step_avg:93.32ms
step:199/1750 train_time:18571ms step_avg:93.32ms
step:200/1750 train_time:18665ms step_avg:93.32ms
step:201/1750 train_time:18759ms step_avg:93.33ms
step:202/1750 train_time:18852ms step_avg:93.33ms
step:203/1750 train_time:18946ms step_avg:93.33ms
step:204/1750 train_time:19040ms step_avg:93.33ms
step:205/1750 train_time:19134ms step_avg:93.34ms
step:206/1750 train_time:19228ms step_avg:93.34ms
step:207/1750 train_time:19323ms step_avg:93.35ms
step:208/1750 train_time:19416ms step_avg:93.35ms
step:209/1750 train_time:19510ms step_avg:93.35ms
step:210/1750 train_time:19604ms step_avg:93.35ms
step:211/1750 train_time:19697ms step_avg:93.35ms
step:212/1750 train_time:19791ms step_avg:93.35ms
step:213/1750 train_time:19885ms step_avg:93.35ms
step:214/1750 train_time:19979ms step_avg:93.36ms
step:215/1750 train_time:20072ms step_avg:93.36ms
step:216/1750 train_time:20166ms step_avg:93.36ms
step:217/1750 train_time:20260ms step_avg:93.37ms
step:218/1750 train_time:20355ms step_avg:93.37ms
step:219/1750 train_time:20448ms step_avg:93.37ms
step:220/1750 train_time:20542ms step_avg:93.37ms
step:221/1750 train_time:20636ms step_avg:93.38ms
step:222/1750 train_time:20730ms step_avg:93.38ms
step:223/1750 train_time:20823ms step_avg:93.38ms
step:224/1750 train_time:20916ms step_avg:93.37ms
step:225/1750 train_time:21009ms step_avg:93.37ms
step:226/1750 train_time:21103ms step_avg:93.38ms
step:227/1750 train_time:21197ms step_avg:93.38ms
step:228/1750 train_time:21291ms step_avg:93.38ms
step:229/1750 train_time:21385ms step_avg:93.38ms
step:230/1750 train_time:21479ms step_avg:93.39ms
step:231/1750 train_time:21573ms step_avg:93.39ms
step:232/1750 train_time:21667ms step_avg:93.39ms
step:233/1750 train_time:21761ms step_avg:93.39ms
step:234/1750 train_time:21854ms step_avg:93.39ms
step:235/1750 train_time:21948ms step_avg:93.40ms
step:236/1750 train_time:22042ms step_avg:93.40ms
step:237/1750 train_time:22135ms step_avg:93.40ms
step:238/1750 train_time:22230ms step_avg:93.40ms
step:239/1750 train_time:22324ms step_avg:93.40ms
step:240/1750 train_time:22416ms step_avg:93.40ms
step:241/1750 train_time:22510ms step_avg:93.40ms
step:242/1750 train_time:22604ms step_avg:93.40ms
step:243/1750 train_time:22698ms step_avg:93.41ms
step:244/1750 train_time:22792ms step_avg:93.41ms
step:245/1750 train_time:22886ms step_avg:93.41ms
step:246/1750 train_time:22980ms step_avg:93.41ms
step:247/1750 train_time:23074ms step_avg:93.42ms
step:248/1750 train_time:23168ms step_avg:93.42ms
step:249/1750 train_time:23262ms step_avg:93.42ms
step:250/1750 train_time:23356ms step_avg:93.42ms
step:250/1750 val_loss:4.1000 train_time:23445ms step_avg:93.78ms
step:251/1750 train_time:23469ms step_avg:93.50ms
step:252/1750 train_time:23551ms step_avg:93.46ms
step:253/1750 train_time:23650ms step_avg:93.48ms
step:254/1750 train_time:23746ms step_avg:93.49ms
step:255/1750 train_time:23839ms step_avg:93.49ms
step:256/1750 train_time:23932ms step_avg:93.48ms
step:257/1750 train_time:24025ms step_avg:93.48ms
step:258/1750 train_time:24118ms step_avg:93.48ms
step:259/1750 train_time:24211ms step_avg:93.48ms
step:260/1750 train_time:24304ms step_avg:93.48ms
step:261/1750 train_time:24397ms step_avg:93.48ms
step:262/1750 train_time:24492ms step_avg:93.48ms
step:263/1750 train_time:24588ms step_avg:93.49ms
step:264/1750 train_time:24684ms step_avg:93.50ms
step:265/1750 train_time:24779ms step_avg:93.50ms
step:266/1750 train_time:24873ms step_avg:93.51ms
step:267/1750 train_time:24967ms step_avg:93.51ms
step:268/1750 train_time:25061ms step_avg:93.51ms
step:269/1750 train_time:25154ms step_avg:93.51ms
step:270/1750 train_time:25248ms step_avg:93.51ms
step:271/1750 train_time:25342ms step_avg:93.51ms
step:272/1750 train_time:25436ms step_avg:93.52ms
step:273/1750 train_time:25531ms step_avg:93.52ms
step:274/1750 train_time:25626ms step_avg:93.53ms
step:275/1750 train_time:25721ms step_avg:93.53ms
step:276/1750 train_time:25815ms step_avg:93.53ms
step:277/1750 train_time:25910ms step_avg:93.54ms
step:278/1750 train_time:26005ms step_avg:93.54ms
step:279/1750 train_time:26099ms step_avg:93.54ms
step:280/1750 train_time:26193ms step_avg:93.54ms
step:281/1750 train_time:26287ms step_avg:93.55ms
step:282/1750 train_time:26382ms step_avg:93.55ms
step:283/1750 train_time:26475ms step_avg:93.55ms
step:284/1750 train_time:26570ms step_avg:93.56ms
step:285/1750 train_time:26664ms step_avg:93.56ms
step:286/1750 train_time:26759ms step_avg:93.56ms
step:287/1750 train_time:26853ms step_avg:93.56ms
step:288/1750 train_time:26948ms step_avg:93.57ms
step:289/1750 train_time:27042ms step_avg:93.57ms
step:290/1750 train_time:27137ms step_avg:93.58ms
step:291/1750 train_time:27231ms step_avg:93.58ms
step:292/1750 train_time:27325ms step_avg:93.58ms
step:293/1750 train_time:27419ms step_avg:93.58ms
step:294/1750 train_time:27513ms step_avg:93.58ms
step:295/1750 train_time:27608ms step_avg:93.59ms
step:296/1750 train_time:27702ms step_avg:93.59ms
step:297/1750 train_time:27797ms step_avg:93.59ms
step:298/1750 train_time:27892ms step_avg:93.60ms
step:299/1750 train_time:27986ms step_avg:93.60ms
step:300/1750 train_time:28080ms step_avg:93.60ms
step:301/1750 train_time:28174ms step_avg:93.60ms
step:302/1750 train_time:28269ms step_avg:93.61ms
step:303/1750 train_time:28363ms step_avg:93.61ms
step:304/1750 train_time:28457ms step_avg:93.61ms
step:305/1750 train_time:28552ms step_avg:93.61ms
step:306/1750 train_time:28646ms step_avg:93.61ms
step:307/1750 train_time:28740ms step_avg:93.62ms
step:308/1750 train_time:28834ms step_avg:93.62ms
step:309/1750 train_time:28928ms step_avg:93.62ms
step:310/1750 train_time:29022ms step_avg:93.62ms
step:311/1750 train_time:29116ms step_avg:93.62ms
step:312/1750 train_time:29210ms step_avg:93.62ms
step:313/1750 train_time:29304ms step_avg:93.62ms
step:314/1750 train_time:29398ms step_avg:93.62ms
step:315/1750 train_time:29493ms step_avg:93.63ms
step:316/1750 train_time:29588ms step_avg:93.63ms
step:317/1750 train_time:29683ms step_avg:93.64ms
step:318/1750 train_time:29776ms step_avg:93.64ms
step:319/1750 train_time:29871ms step_avg:93.64ms
step:320/1750 train_time:29965ms step_avg:93.64ms
step:321/1750 train_time:30059ms step_avg:93.64ms
step:322/1750 train_time:30154ms step_avg:93.64ms
step:323/1750 train_time:30249ms step_avg:93.65ms
step:324/1750 train_time:30343ms step_avg:93.65ms
step:325/1750 train_time:30438ms step_avg:93.65ms
step:326/1750 train_time:30532ms step_avg:93.66ms
step:327/1750 train_time:30626ms step_avg:93.66ms
step:328/1750 train_time:30720ms step_avg:93.66ms
step:329/1750 train_time:30814ms step_avg:93.66ms
step:330/1750 train_time:30909ms step_avg:93.66ms
step:331/1750 train_time:31003ms step_avg:93.66ms
step:332/1750 train_time:31097ms step_avg:93.67ms
step:333/1750 train_time:31191ms step_avg:93.67ms
step:334/1750 train_time:31286ms step_avg:93.67ms
step:335/1750 train_time:31380ms step_avg:93.67ms
step:336/1750 train_time:31474ms step_avg:93.67ms
step:337/1750 train_time:31569ms step_avg:93.68ms
step:338/1750 train_time:31663ms step_avg:93.68ms
step:339/1750 train_time:31757ms step_avg:93.68ms
step:340/1750 train_time:31851ms step_avg:93.68ms
step:341/1750 train_time:31946ms step_avg:93.68ms
step:342/1750 train_time:32039ms step_avg:93.68ms
step:343/1750 train_time:32133ms step_avg:93.68ms
step:344/1750 train_time:32227ms step_avg:93.68ms
step:345/1750 train_time:32321ms step_avg:93.68ms
step:346/1750 train_time:32415ms step_avg:93.68ms
step:347/1750 train_time:32509ms step_avg:93.69ms
step:348/1750 train_time:32604ms step_avg:93.69ms
step:349/1750 train_time:32698ms step_avg:93.69ms
step:350/1750 train_time:32793ms step_avg:93.69ms
step:351/1750 train_time:32887ms step_avg:93.70ms
step:352/1750 train_time:32982ms step_avg:93.70ms
step:353/1750 train_time:33076ms step_avg:93.70ms
step:354/1750 train_time:33171ms step_avg:93.70ms
step:355/1750 train_time:33265ms step_avg:93.70ms
step:356/1750 train_time:33359ms step_avg:93.70ms
step:357/1750 train_time:33453ms step_avg:93.71ms
step:358/1750 train_time:33547ms step_avg:93.71ms
step:359/1750 train_time:33641ms step_avg:93.71ms
step:360/1750 train_time:33735ms step_avg:93.71ms
step:361/1750 train_time:33829ms step_avg:93.71ms
step:362/1750 train_time:33924ms step_avg:93.71ms
step:363/1750 train_time:34018ms step_avg:93.71ms
step:364/1750 train_time:34112ms step_avg:93.71ms
step:365/1750 train_time:34206ms step_avg:93.72ms
step:366/1750 train_time:34300ms step_avg:93.72ms
step:367/1750 train_time:34395ms step_avg:93.72ms
step:368/1750 train_time:34490ms step_avg:93.72ms
step:369/1750 train_time:34585ms step_avg:93.73ms
step:370/1750 train_time:34679ms step_avg:93.73ms
step:371/1750 train_time:34773ms step_avg:93.73ms
step:372/1750 train_time:34867ms step_avg:93.73ms
step:373/1750 train_time:34961ms step_avg:93.73ms
step:374/1750 train_time:35055ms step_avg:93.73ms
step:375/1750 train_time:35151ms step_avg:93.74ms
step:375/1750 val_loss:3.8910 train_time:35240ms step_avg:93.97ms
step:376/1750 train_time:35265ms step_avg:93.79ms
step:377/1750 train_time:35346ms step_avg:93.75ms
step:378/1750 train_time:35442ms step_avg:93.76ms
step:379/1750 train_time:35537ms step_avg:93.77ms
step:380/1750 train_time:35630ms step_avg:93.76ms
step:381/1750 train_time:35724ms step_avg:93.76ms
step:382/1750 train_time:35818ms step_avg:93.76ms
step:383/1750 train_time:35911ms step_avg:93.76ms
step:384/1750 train_time:36005ms step_avg:93.76ms
step:385/1750 train_time:36098ms step_avg:93.76ms
step:386/1750 train_time:36192ms step_avg:93.76ms
step:387/1750 train_time:36288ms step_avg:93.77ms
step:388/1750 train_time:36384ms step_avg:93.77ms
step:389/1750 train_time:36480ms step_avg:93.78ms
step:390/1750 train_time:36574ms step_avg:93.78ms
step:391/1750 train_time:36671ms step_avg:93.79ms
step:392/1750 train_time:36768ms step_avg:93.80ms
step:393/1750 train_time:36864ms step_avg:93.80ms
step:394/1750 train_time:36960ms step_avg:93.81ms
step:395/1750 train_time:37056ms step_avg:93.81ms
step:396/1750 train_time:37152ms step_avg:93.82ms
step:397/1750 train_time:37248ms step_avg:93.82ms
step:398/1750 train_time:37345ms step_avg:93.83ms
step:399/1750 train_time:37442ms step_avg:93.84ms
step:400/1750 train_time:37539ms step_avg:93.85ms
step:401/1750 train_time:37636ms step_avg:93.86ms
step:402/1750 train_time:37733ms step_avg:93.86ms
step:403/1750 train_time:37829ms step_avg:93.87ms
step:404/1750 train_time:37925ms step_avg:93.87ms
step:405/1750 train_time:38022ms step_avg:93.88ms
step:406/1750 train_time:38118ms step_avg:93.89ms
step:407/1750 train_time:38214ms step_avg:93.89ms
step:408/1750 train_time:38311ms step_avg:93.90ms
step:409/1750 train_time:38409ms step_avg:93.91ms
step:410/1750 train_time:38507ms step_avg:93.92ms
step:411/1750 train_time:38604ms step_avg:93.93ms
step:412/1750 train_time:38701ms step_avg:93.93ms
step:413/1750 train_time:38797ms step_avg:93.94ms
step:414/1750 train_time:38892ms step_avg:93.94ms
step:415/1750 train_time:38989ms step_avg:93.95ms
step:416/1750 train_time:39085ms step_avg:93.95ms
step:417/1750 train_time:39181ms step_avg:93.96ms
step:418/1750 train_time:39278ms step_avg:93.97ms
step:419/1750 train_time:39374ms step_avg:93.97ms
step:420/1750 train_time:39472ms step_avg:93.98ms
step:421/1750 train_time:39569ms step_avg:93.99ms
step:422/1750 train_time:39665ms step_avg:93.99ms
step:423/1750 train_time:39761ms step_avg:94.00ms
step:424/1750 train_time:39857ms step_avg:94.00ms
step:425/1750 train_time:39953ms step_avg:94.01ms
step:426/1750 train_time:40049ms step_avg:94.01ms
step:427/1750 train_time:40146ms step_avg:94.02ms
step:428/1750 train_time:40243ms step_avg:94.02ms
step:429/1750 train_time:40339ms step_avg:94.03ms
step:430/1750 train_time:40435ms step_avg:94.04ms
step:431/1750 train_time:40532ms step_avg:94.04ms
step:432/1750 train_time:40629ms step_avg:94.05ms
step:433/1750 train_time:40725ms step_avg:94.05ms
step:434/1750 train_time:40822ms step_avg:94.06ms
step:435/1750 train_time:40919ms step_avg:94.07ms
step:436/1750 train_time:41015ms step_avg:94.07ms
step:437/1750 train_time:41112ms step_avg:94.08ms
step:438/1750 train_time:41209ms step_avg:94.09ms
step:439/1750 train_time:41306ms step_avg:94.09ms
step:440/1750 train_time:41403ms step_avg:94.10ms
step:441/1750 train_time:41500ms step_avg:94.10ms
step:442/1750 train_time:41597ms step_avg:94.11ms
step:443/1750 train_time:41693ms step_avg:94.11ms
step:444/1750 train_time:41790ms step_avg:94.12ms
step:445/1750 train_time:41886ms step_avg:94.13ms
step:446/1750 train_time:41983ms step_avg:94.13ms
step:447/1750 train_time:42079ms step_avg:94.14ms
step:448/1750 train_time:42176ms step_avg:94.14ms
step:449/1750 train_time:42272ms step_avg:94.15ms
step:450/1750 train_time:42369ms step_avg:94.15ms
step:451/1750 train_time:42466ms step_avg:94.16ms
step:452/1750 train_time:42563ms step_avg:94.17ms
step:453/1750 train_time:42660ms step_avg:94.17ms
step:454/1750 train_time:42756ms step_avg:94.18ms
step:455/1750 train_time:42852ms step_avg:94.18ms
step:456/1750 train_time:42949ms step_avg:94.19ms
step:457/1750 train_time:43046ms step_avg:94.19ms
step:458/1750 train_time:43143ms step_avg:94.20ms
step:459/1750 train_time:43240ms step_avg:94.20ms
step:460/1750 train_time:43336ms step_avg:94.21ms
step:461/1750 train_time:43433ms step_avg:94.21ms
step:462/1750 train_time:43530ms step_avg:94.22ms
step:463/1750 train_time:43627ms step_avg:94.23ms
step:464/1750 train_time:43723ms step_avg:94.23ms
step:465/1750 train_time:43819ms step_avg:94.23ms
step:466/1750 train_time:43915ms step_avg:94.24ms
step:467/1750 train_time:44012ms step_avg:94.24ms
step:468/1750 train_time:44108ms step_avg:94.25ms
step:469/1750 train_time:44205ms step_avg:94.25ms
step:470/1750 train_time:44301ms step_avg:94.26ms
step:471/1750 train_time:44397ms step_avg:94.26ms
step:472/1750 train_time:44494ms step_avg:94.27ms
step:473/1750 train_time:44590ms step_avg:94.27ms
step:474/1750 train_time:44686ms step_avg:94.28ms
step:475/1750 train_time:44784ms step_avg:94.28ms
step:476/1750 train_time:44880ms step_avg:94.29ms
step:477/1750 train_time:44976ms step_avg:94.29ms
step:478/1750 train_time:45072ms step_avg:94.29ms
step:479/1750 train_time:45169ms step_avg:94.30ms
step:480/1750 train_time:45267ms step_avg:94.31ms
step:481/1750 train_time:45363ms step_avg:94.31ms
step:482/1750 train_time:45459ms step_avg:94.31ms
step:483/1750 train_time:45556ms step_avg:94.32ms
step:484/1750 train_time:45652ms step_avg:94.32ms
step:485/1750 train_time:45749ms step_avg:94.33ms
step:486/1750 train_time:45846ms step_avg:94.33ms
step:487/1750 train_time:45942ms step_avg:94.34ms
step:488/1750 train_time:46038ms step_avg:94.34ms
step:489/1750 train_time:46134ms step_avg:94.34ms
step:490/1750 train_time:46231ms step_avg:94.35ms
step:491/1750 train_time:46328ms step_avg:94.35ms
step:492/1750 train_time:46424ms step_avg:94.36ms
step:493/1750 train_time:46521ms step_avg:94.36ms
step:494/1750 train_time:46618ms step_avg:94.37ms
step:495/1750 train_time:46714ms step_avg:94.37ms
step:496/1750 train_time:46811ms step_avg:94.38ms
step:497/1750 train_time:46908ms step_avg:94.38ms
step:498/1750 train_time:47004ms step_avg:94.39ms
step:499/1750 train_time:47101ms step_avg:94.39ms
step:500/1750 train_time:47197ms step_avg:94.39ms
step:500/1750 val_loss:3.7415 train_time:47288ms step_avg:94.58ms
step:501/1750 train_time:47313ms step_avg:94.44ms
step:502/1750 train_time:47400ms step_avg:94.42ms
step:503/1750 train_time:47499ms step_avg:94.43ms
step:504/1750 train_time:47595ms step_avg:94.43ms
step:505/1750 train_time:47690ms step_avg:94.44ms
step:506/1750 train_time:47786ms step_avg:94.44ms
step:507/1750 train_time:47882ms step_avg:94.44ms
step:508/1750 train_time:47978ms step_avg:94.44ms
step:509/1750 train_time:48074ms step_avg:94.45ms
step:510/1750 train_time:48170ms step_avg:94.45ms
step:511/1750 train_time:48267ms step_avg:94.46ms
step:512/1750 train_time:48365ms step_avg:94.46ms
step:513/1750 train_time:48464ms step_avg:94.47ms
step:514/1750 train_time:48561ms step_avg:94.48ms
step:515/1750 train_time:48657ms step_avg:94.48ms
step:516/1750 train_time:48753ms step_avg:94.48ms
step:517/1750 train_time:48849ms step_avg:94.48ms
step:518/1750 train_time:48945ms step_avg:94.49ms
step:519/1750 train_time:49041ms step_avg:94.49ms
step:520/1750 train_time:49137ms step_avg:94.49ms
step:521/1750 train_time:49233ms step_avg:94.50ms
step:522/1750 train_time:49330ms step_avg:94.50ms
step:523/1750 train_time:49429ms step_avg:94.51ms
step:524/1750 train_time:49526ms step_avg:94.51ms
step:525/1750 train_time:49623ms step_avg:94.52ms
step:526/1750 train_time:49720ms step_avg:94.52ms
step:527/1750 train_time:49816ms step_avg:94.53ms
step:528/1750 train_time:49913ms step_avg:94.53ms
step:529/1750 train_time:50009ms step_avg:94.54ms
step:530/1750 train_time:50105ms step_avg:94.54ms
step:531/1750 train_time:50202ms step_avg:94.54ms
step:532/1750 train_time:50299ms step_avg:94.55ms
step:533/1750 train_time:50396ms step_avg:94.55ms
step:534/1750 train_time:50494ms step_avg:94.56ms
step:535/1750 train_time:50591ms step_avg:94.56ms
step:536/1750 train_time:50687ms step_avg:94.57ms
step:537/1750 train_time:50785ms step_avg:94.57ms
step:538/1750 train_time:50882ms step_avg:94.58ms
step:539/1750 train_time:50978ms step_avg:94.58ms
step:540/1750 train_time:51075ms step_avg:94.58ms
step:541/1750 train_time:51172ms step_avg:94.59ms
step:542/1750 train_time:51269ms step_avg:94.59ms
step:543/1750 train_time:51366ms step_avg:94.60ms
step:544/1750 train_time:51462ms step_avg:94.60ms
step:545/1750 train_time:51559ms step_avg:94.60ms
step:546/1750 train_time:51656ms step_avg:94.61ms
step:547/1750 train_time:51752ms step_avg:94.61ms
step:548/1750 train_time:51849ms step_avg:94.62ms
step:549/1750 train_time:51946ms step_avg:94.62ms
step:550/1750 train_time:52043ms step_avg:94.62ms
step:551/1750 train_time:52140ms step_avg:94.63ms
step:552/1750 train_time:52236ms step_avg:94.63ms
step:553/1750 train_time:52334ms step_avg:94.64ms
step:554/1750 train_time:52431ms step_avg:94.64ms
step:555/1750 train_time:52528ms step_avg:94.64ms
step:556/1750 train_time:52625ms step_avg:94.65ms
step:557/1750 train_time:52722ms step_avg:94.65ms
step:558/1750 train_time:52818ms step_avg:94.66ms
step:559/1750 train_time:52915ms step_avg:94.66ms
step:560/1750 train_time:53012ms step_avg:94.66ms
step:561/1750 train_time:53108ms step_avg:94.67ms
step:562/1750 train_time:53206ms step_avg:94.67ms
step:563/1750 train_time:53303ms step_avg:94.68ms
step:564/1750 train_time:53399ms step_avg:94.68ms
step:565/1750 train_time:53497ms step_avg:94.68ms
step:566/1750 train_time:53593ms step_avg:94.69ms
step:567/1750 train_time:53690ms step_avg:94.69ms
step:568/1750 train_time:53787ms step_avg:94.70ms
step:569/1750 train_time:53885ms step_avg:94.70ms
step:570/1750 train_time:53981ms step_avg:94.70ms
step:571/1750 train_time:54078ms step_avg:94.71ms
step:572/1750 train_time:54174ms step_avg:94.71ms
step:573/1750 train_time:54272ms step_avg:94.72ms
step:574/1750 train_time:54368ms step_avg:94.72ms
step:575/1750 train_time:54466ms step_avg:94.72ms
step:576/1750 train_time:54563ms step_avg:94.73ms
step:577/1750 train_time:54660ms step_avg:94.73ms
step:578/1750 train_time:54757ms step_avg:94.74ms
step:579/1750 train_time:54854ms step_avg:94.74ms
step:580/1750 train_time:54950ms step_avg:94.74ms
step:581/1750 train_time:55047ms step_avg:94.75ms
step:582/1750 train_time:55143ms step_avg:94.75ms
step:583/1750 train_time:55240ms step_avg:94.75ms
step:584/1750 train_time:55337ms step_avg:94.75ms
step:585/1750 train_time:55434ms step_avg:94.76ms
step:586/1750 train_time:55531ms step_avg:94.76ms
step:587/1750 train_time:55628ms step_avg:94.77ms
step:588/1750 train_time:55725ms step_avg:94.77ms
step:589/1750 train_time:55821ms step_avg:94.77ms
step:590/1750 train_time:55918ms step_avg:94.78ms
step:591/1750 train_time:56014ms step_avg:94.78ms
step:592/1750 train_time:56111ms step_avg:94.78ms
step:593/1750 train_time:56208ms step_avg:94.79ms
step:594/1750 train_time:56305ms step_avg:94.79ms
step:595/1750 train_time:56402ms step_avg:94.79ms
step:596/1750 train_time:56499ms step_avg:94.80ms
step:597/1750 train_time:56596ms step_avg:94.80ms
step:598/1750 train_time:56693ms step_avg:94.80ms
step:599/1750 train_time:56790ms step_avg:94.81ms
step:600/1750 train_time:56887ms step_avg:94.81ms
step:601/1750 train_time:56984ms step_avg:94.81ms
step:602/1750 train_time:57080ms step_avg:94.82ms
step:603/1750 train_time:57177ms step_avg:94.82ms
step:604/1750 train_time:57274ms step_avg:94.82ms
step:605/1750 train_time:57371ms step_avg:94.83ms
step:606/1750 train_time:57469ms step_avg:94.83ms
step:607/1750 train_time:57566ms step_avg:94.84ms
step:608/1750 train_time:57662ms step_avg:94.84ms
step:609/1750 train_time:57758ms step_avg:94.84ms
step:610/1750 train_time:57855ms step_avg:94.84ms
step:611/1750 train_time:57952ms step_avg:94.85ms
step:612/1750 train_time:58049ms step_avg:94.85ms
step:613/1750 train_time:58146ms step_avg:94.86ms
step:614/1750 train_time:58243ms step_avg:94.86ms
step:615/1750 train_time:58338ms step_avg:94.86ms
step:616/1750 train_time:58435ms step_avg:94.86ms
step:617/1750 train_time:58533ms step_avg:94.87ms
step:618/1750 train_time:58630ms step_avg:94.87ms
step:619/1750 train_time:58728ms step_avg:94.88ms
step:620/1750 train_time:58826ms step_avg:94.88ms
step:621/1750 train_time:58923ms step_avg:94.88ms
step:622/1750 train_time:59019ms step_avg:94.89ms
step:623/1750 train_time:59116ms step_avg:94.89ms
step:624/1750 train_time:59213ms step_avg:94.89ms
step:625/1750 train_time:59310ms step_avg:94.90ms
step:625/1750 val_loss:3.6543 train_time:59402ms step_avg:95.04ms
step:626/1750 train_time:59428ms step_avg:94.93ms
step:627/1750 train_time:59513ms step_avg:94.92ms
step:628/1750 train_time:59612ms step_avg:94.92ms
step:629/1750 train_time:59709ms step_avg:94.93ms
step:630/1750 train_time:59805ms step_avg:94.93ms
step:631/1750 train_time:59902ms step_avg:94.93ms
step:632/1750 train_time:59997ms step_avg:94.93ms
step:633/1750 train_time:60094ms step_avg:94.93ms
step:634/1750 train_time:60190ms step_avg:94.94ms
step:635/1750 train_time:60285ms step_avg:94.94ms
step:636/1750 train_time:60381ms step_avg:94.94ms
step:637/1750 train_time:60480ms step_avg:94.94ms
step:638/1750 train_time:60578ms step_avg:94.95ms
step:639/1750 train_time:60675ms step_avg:94.95ms
step:640/1750 train_time:60772ms step_avg:94.96ms
step:641/1750 train_time:60868ms step_avg:94.96ms
step:642/1750 train_time:60965ms step_avg:94.96ms
step:643/1750 train_time:61061ms step_avg:94.96ms
step:644/1750 train_time:61157ms step_avg:94.96ms
step:645/1750 train_time:61253ms step_avg:94.97ms
step:646/1750 train_time:61349ms step_avg:94.97ms
step:647/1750 train_time:61447ms step_avg:94.97ms
step:648/1750 train_time:61543ms step_avg:94.97ms
step:649/1750 train_time:61640ms step_avg:94.98ms
step:650/1750 train_time:61737ms step_avg:94.98ms
step:651/1750 train_time:61835ms step_avg:94.99ms
step:652/1750 train_time:61935ms step_avg:94.99ms
step:653/1750 train_time:62034ms step_avg:95.00ms
step:654/1750 train_time:62132ms step_avg:95.00ms
step:655/1750 train_time:62231ms step_avg:95.01ms
step:656/1750 train_time:62329ms step_avg:95.01ms
step:657/1750 train_time:62427ms step_avg:95.02ms
step:658/1750 train_time:62525ms step_avg:95.02ms
step:659/1750 train_time:62624ms step_avg:95.03ms
step:660/1750 train_time:62723ms step_avg:95.03ms
step:661/1750 train_time:62822ms step_avg:95.04ms
step:662/1750 train_time:62920ms step_avg:95.05ms
step:663/1750 train_time:63019ms step_avg:95.05ms
step:664/1750 train_time:63119ms step_avg:95.06ms
step:665/1750 train_time:63218ms step_avg:95.06ms
step:666/1750 train_time:63318ms step_avg:95.07ms
step:667/1750 train_time:63417ms step_avg:95.08ms
step:668/1750 train_time:63515ms step_avg:95.08ms
step:669/1750 train_time:63613ms step_avg:95.09ms
step:670/1750 train_time:63712ms step_avg:95.09ms
step:671/1750 train_time:63811ms step_avg:95.10ms
step:672/1750 train_time:63911ms step_avg:95.11ms
step:673/1750 train_time:64012ms step_avg:95.11ms
step:674/1750 train_time:64111ms step_avg:95.12ms
step:675/1750 train_time:64211ms step_avg:95.13ms
step:676/1750 train_time:64310ms step_avg:95.13ms
step:677/1750 train_time:64408ms step_avg:95.14ms
step:678/1750 train_time:64506ms step_avg:95.14ms
step:679/1750 train_time:64605ms step_avg:95.15ms
step:680/1750 train_time:64702ms step_avg:95.15ms
step:681/1750 train_time:64801ms step_avg:95.16ms
step:682/1750 train_time:64899ms step_avg:95.16ms
step:683/1750 train_time:64999ms step_avg:95.17ms
step:684/1750 train_time:65099ms step_avg:95.17ms
step:685/1750 train_time:65199ms step_avg:95.18ms
step:686/1750 train_time:65299ms step_avg:95.19ms
step:687/1750 train_time:65398ms step_avg:95.19ms
step:688/1750 train_time:65496ms step_avg:95.20ms
step:689/1750 train_time:65595ms step_avg:95.20ms
step:690/1750 train_time:65693ms step_avg:95.21ms
step:691/1750 train_time:65792ms step_avg:95.21ms
step:692/1750 train_time:65890ms step_avg:95.22ms
step:693/1750 train_time:65990ms step_avg:95.22ms
step:694/1750 train_time:66089ms step_avg:95.23ms
step:695/1750 train_time:66188ms step_avg:95.23ms
step:696/1750 train_time:66287ms step_avg:95.24ms
step:697/1750 train_time:66386ms step_avg:95.25ms
step:698/1750 train_time:66485ms step_avg:95.25ms
step:699/1750 train_time:66583ms step_avg:95.25ms
step:700/1750 train_time:66681ms step_avg:95.26ms
step:701/1750 train_time:66779ms step_avg:95.26ms
step:702/1750 train_time:66877ms step_avg:95.27ms
step:703/1750 train_time:66975ms step_avg:95.27ms
step:704/1750 train_time:67073ms step_avg:95.27ms
step:705/1750 train_time:67172ms step_avg:95.28ms
step:706/1750 train_time:67271ms step_avg:95.28ms
step:707/1750 train_time:67370ms step_avg:95.29ms
step:708/1750 train_time:67470ms step_avg:95.30ms
step:709/1750 train_time:67570ms step_avg:95.30ms
step:710/1750 train_time:67669ms step_avg:95.31ms
step:711/1750 train_time:67768ms step_avg:95.31ms
step:712/1750 train_time:67867ms step_avg:95.32ms
step:713/1750 train_time:67965ms step_avg:95.32ms
step:714/1750 train_time:68062ms step_avg:95.33ms
step:715/1750 train_time:68161ms step_avg:95.33ms
step:716/1750 train_time:68259ms step_avg:95.33ms
step:717/1750 train_time:68358ms step_avg:95.34ms
step:718/1750 train_time:68457ms step_avg:95.34ms
step:719/1750 train_time:68556ms step_avg:95.35ms
step:720/1750 train_time:68656ms step_avg:95.36ms
step:721/1750 train_time:68755ms step_avg:95.36ms
step:722/1750 train_time:68854ms step_avg:95.37ms
step:723/1750 train_time:68952ms step_avg:95.37ms
step:724/1750 train_time:69051ms step_avg:95.37ms
step:725/1750 train_time:69151ms step_avg:95.38ms
step:726/1750 train_time:69250ms step_avg:95.39ms
step:727/1750 train_time:69348ms step_avg:95.39ms
step:728/1750 train_time:69447ms step_avg:95.39ms
step:729/1750 train_time:69545ms step_avg:95.40ms
step:730/1750 train_time:69643ms step_avg:95.40ms
step:731/1750 train_time:69741ms step_avg:95.40ms
step:732/1750 train_time:69838ms step_avg:95.41ms
step:733/1750 train_time:69937ms step_avg:95.41ms
step:734/1750 train_time:70035ms step_avg:95.42ms
step:735/1750 train_time:70134ms step_avg:95.42ms
step:736/1750 train_time:70233ms step_avg:95.43ms
step:737/1750 train_time:70332ms step_avg:95.43ms
step:738/1750 train_time:70431ms step_avg:95.43ms
step:739/1750 train_time:70529ms step_avg:95.44ms
step:740/1750 train_time:70628ms step_avg:95.44ms
step:741/1750 train_time:70727ms step_avg:95.45ms
step:742/1750 train_time:70825ms step_avg:95.45ms
step:743/1750 train_time:70923ms step_avg:95.45ms
step:744/1750 train_time:71021ms step_avg:95.46ms
step:745/1750 train_time:71119ms step_avg:95.46ms
step:746/1750 train_time:71218ms step_avg:95.47ms
step:747/1750 train_time:71317ms step_avg:95.47ms
step:748/1750 train_time:71417ms step_avg:95.48ms
step:749/1750 train_time:71516ms step_avg:95.48ms
step:750/1750 train_time:71616ms step_avg:95.49ms
step:750/1750 val_loss:3.5931 train_time:71710ms step_avg:95.61ms
step:751/1750 train_time:71734ms step_avg:95.52ms
step:752/1750 train_time:71821ms step_avg:95.51ms
step:753/1750 train_time:71925ms step_avg:95.52ms
step:754/1750 train_time:72024ms step_avg:95.52ms
step:755/1750 train_time:72122ms step_avg:95.53ms
step:756/1750 train_time:72221ms step_avg:95.53ms
step:757/1750 train_time:72319ms step_avg:95.53ms
step:758/1750 train_time:72416ms step_avg:95.54ms
step:759/1750 train_time:72514ms step_avg:95.54ms
step:760/1750 train_time:72612ms step_avg:95.54ms
step:761/1750 train_time:72710ms step_avg:95.55ms
step:762/1750 train_time:72810ms step_avg:95.55ms
step:763/1750 train_time:72910ms step_avg:95.56ms
step:764/1750 train_time:73009ms step_avg:95.56ms
step:765/1750 train_time:73107ms step_avg:95.57ms
step:766/1750 train_time:73206ms step_avg:95.57ms
step:767/1750 train_time:73305ms step_avg:95.57ms
step:768/1750 train_time:73402ms step_avg:95.58ms
step:769/1750 train_time:73501ms step_avg:95.58ms
step:770/1750 train_time:73599ms step_avg:95.58ms
step:771/1750 train_time:73697ms step_avg:95.59ms
step:772/1750 train_time:73796ms step_avg:95.59ms
step:773/1750 train_time:73896ms step_avg:95.60ms
step:774/1750 train_time:73995ms step_avg:95.60ms
step:775/1750 train_time:74094ms step_avg:95.61ms
step:776/1750 train_time:74193ms step_avg:95.61ms
step:777/1750 train_time:74293ms step_avg:95.62ms
step:778/1750 train_time:74391ms step_avg:95.62ms
step:779/1750 train_time:74490ms step_avg:95.62ms
step:780/1750 train_time:74589ms step_avg:95.63ms
step:781/1750 train_time:74688ms step_avg:95.63ms
step:782/1750 train_time:74786ms step_avg:95.63ms
step:783/1750 train_time:74886ms step_avg:95.64ms
step:784/1750 train_time:74985ms step_avg:95.64ms
step:785/1750 train_time:75084ms step_avg:95.65ms
step:786/1750 train_time:75182ms step_avg:95.65ms
step:787/1750 train_time:75282ms step_avg:95.66ms
step:788/1750 train_time:75381ms step_avg:95.66ms
step:789/1750 train_time:75481ms step_avg:95.67ms
step:790/1750 train_time:75580ms step_avg:95.67ms
step:791/1750 train_time:75680ms step_avg:95.68ms
step:792/1750 train_time:75778ms step_avg:95.68ms
step:793/1750 train_time:75877ms step_avg:95.68ms
step:794/1750 train_time:75975ms step_avg:95.69ms
step:795/1750 train_time:76073ms step_avg:95.69ms
step:796/1750 train_time:76172ms step_avg:95.69ms
step:797/1750 train_time:76270ms step_avg:95.70ms
step:798/1750 train_time:76369ms step_avg:95.70ms
step:799/1750 train_time:76468ms step_avg:95.70ms
step:800/1750 train_time:76566ms step_avg:95.71ms
step:801/1750 train_time:76665ms step_avg:95.71ms
step:802/1750 train_time:76764ms step_avg:95.72ms
step:803/1750 train_time:76863ms step_avg:95.72ms
step:804/1750 train_time:76962ms step_avg:95.72ms
step:805/1750 train_time:77062ms step_avg:95.73ms
step:806/1750 train_time:77161ms step_avg:95.73ms
step:807/1750 train_time:77261ms step_avg:95.74ms
step:808/1750 train_time:77361ms step_avg:95.74ms
step:809/1750 train_time:77460ms step_avg:95.75ms
step:810/1750 train_time:77559ms step_avg:95.75ms
step:811/1750 train_time:77657ms step_avg:95.76ms
step:812/1750 train_time:77756ms step_avg:95.76ms
step:813/1750 train_time:77855ms step_avg:95.76ms
step:814/1750 train_time:77953ms step_avg:95.77ms
step:815/1750 train_time:78051ms step_avg:95.77ms
step:816/1750 train_time:78150ms step_avg:95.77ms
step:817/1750 train_time:78249ms step_avg:95.78ms
step:818/1750 train_time:78348ms step_avg:95.78ms
step:819/1750 train_time:78448ms step_avg:95.78ms
step:820/1750 train_time:78548ms step_avg:95.79ms
step:821/1750 train_time:78647ms step_avg:95.79ms
step:822/1750 train_time:78746ms step_avg:95.80ms
step:823/1750 train_time:78845ms step_avg:95.80ms
step:824/1750 train_time:78945ms step_avg:95.81ms
step:825/1750 train_time:79045ms step_avg:95.81ms
step:826/1750 train_time:79145ms step_avg:95.82ms
step:827/1750 train_time:79244ms step_avg:95.82ms
step:828/1750 train_time:79343ms step_avg:95.82ms
step:829/1750 train_time:79442ms step_avg:95.83ms
step:830/1750 train_time:79541ms step_avg:95.83ms
step:831/1750 train_time:79640ms step_avg:95.84ms
step:832/1750 train_time:79739ms step_avg:95.84ms
step:833/1750 train_time:79837ms step_avg:95.84ms
step:834/1750 train_time:79935ms step_avg:95.85ms
step:835/1750 train_time:80034ms step_avg:95.85ms
step:836/1750 train_time:80132ms step_avg:95.85ms
step:837/1750 train_time:80231ms step_avg:95.86ms
step:838/1750 train_time:80331ms step_avg:95.86ms
step:839/1750 train_time:80430ms step_avg:95.86ms
step:840/1750 train_time:80529ms step_avg:95.87ms
step:841/1750 train_time:80629ms step_avg:95.87ms
step:842/1750 train_time:80729ms step_avg:95.88ms
step:843/1750 train_time:80828ms step_avg:95.88ms
step:844/1750 train_time:80927ms step_avg:95.88ms
step:845/1750 train_time:81025ms step_avg:95.89ms
step:846/1750 train_time:81124ms step_avg:95.89ms
step:847/1750 train_time:81223ms step_avg:95.90ms
step:848/1750 train_time:81322ms step_avg:95.90ms
step:849/1750 train_time:81421ms step_avg:95.90ms
step:850/1750 train_time:81521ms step_avg:95.91ms
step:851/1750 train_time:81620ms step_avg:95.91ms
step:852/1750 train_time:81720ms step_avg:95.92ms
step:853/1750 train_time:81819ms step_avg:95.92ms
step:854/1750 train_time:81919ms step_avg:95.92ms
step:855/1750 train_time:82016ms step_avg:95.93ms
step:856/1750 train_time:82114ms step_avg:95.93ms
step:857/1750 train_time:82212ms step_avg:95.93ms
step:858/1750 train_time:82311ms step_avg:95.93ms
step:859/1750 train_time:82410ms step_avg:95.94ms
step:860/1750 train_time:82508ms step_avg:95.94ms
step:861/1750 train_time:82608ms step_avg:95.94ms
step:862/1750 train_time:82707ms step_avg:95.95ms
step:863/1750 train_time:82806ms step_avg:95.95ms
step:864/1750 train_time:82906ms step_avg:95.96ms
step:865/1750 train_time:83005ms step_avg:95.96ms
step:866/1750 train_time:83104ms step_avg:95.96ms
step:867/1750 train_time:83204ms step_avg:95.97ms
step:868/1750 train_time:83303ms step_avg:95.97ms
step:869/1750 train_time:83401ms step_avg:95.97ms
step:870/1750 train_time:83500ms step_avg:95.98ms
step:871/1750 train_time:83599ms step_avg:95.98ms
step:872/1750 train_time:83698ms step_avg:95.98ms
step:873/1750 train_time:83797ms step_avg:95.99ms
step:874/1750 train_time:83896ms step_avg:95.99ms
step:875/1750 train_time:83994ms step_avg:95.99ms
step:875/1750 val_loss:3.5426 train_time:84088ms step_avg:96.10ms
step:876/1750 train_time:84113ms step_avg:96.02ms
step:877/1750 train_time:84201ms step_avg:96.01ms
step:878/1750 train_time:84301ms step_avg:96.02ms
step:879/1750 train_time:84401ms step_avg:96.02ms
step:880/1750 train_time:84500ms step_avg:96.02ms
step:881/1750 train_time:84597ms step_avg:96.02ms
step:882/1750 train_time:84695ms step_avg:96.03ms
step:883/1750 train_time:84793ms step_avg:96.03ms
step:884/1750 train_time:84891ms step_avg:96.03ms
step:885/1750 train_time:84989ms step_avg:96.03ms
step:886/1750 train_time:85087ms step_avg:96.04ms
step:887/1750 train_time:85187ms step_avg:96.04ms
step:888/1750 train_time:85286ms step_avg:96.04ms
step:889/1750 train_time:85385ms step_avg:96.05ms
step:890/1750 train_time:85484ms step_avg:96.05ms
step:891/1750 train_time:85583ms step_avg:96.05ms
step:892/1750 train_time:85682ms step_avg:96.06ms
step:893/1750 train_time:85780ms step_avg:96.06ms
step:894/1750 train_time:85878ms step_avg:96.06ms
step:895/1750 train_time:85976ms step_avg:96.06ms
step:896/1750 train_time:86075ms step_avg:96.07ms
step:897/1750 train_time:86174ms step_avg:96.07ms
step:898/1750 train_time:86273ms step_avg:96.07ms
step:899/1750 train_time:86372ms step_avg:96.08ms
step:900/1750 train_time:86472ms step_avg:96.08ms
step:901/1750 train_time:86571ms step_avg:96.08ms
step:902/1750 train_time:86671ms step_avg:96.09ms
step:903/1750 train_time:86770ms step_avg:96.09ms
step:904/1750 train_time:86869ms step_avg:96.09ms
step:905/1750 train_time:86968ms step_avg:96.10ms
step:906/1750 train_time:87066ms step_avg:96.10ms
step:907/1750 train_time:87164ms step_avg:96.10ms
step:908/1750 train_time:87262ms step_avg:96.10ms
step:909/1750 train_time:87361ms step_avg:96.11ms
step:910/1750 train_time:87461ms step_avg:96.11ms
step:911/1750 train_time:87562ms step_avg:96.12ms
step:912/1750 train_time:87663ms step_avg:96.12ms
step:913/1750 train_time:87765ms step_avg:96.13ms
step:914/1750 train_time:87866ms step_avg:96.13ms
step:915/1750 train_time:87966ms step_avg:96.14ms
step:916/1750 train_time:88066ms step_avg:96.14ms
step:917/1750 train_time:88166ms step_avg:96.15ms
step:918/1750 train_time:88266ms step_avg:96.15ms
step:919/1750 train_time:88367ms step_avg:96.16ms
step:920/1750 train_time:88467ms step_avg:96.16ms
step:921/1750 train_time:88567ms step_avg:96.16ms
step:922/1750 train_time:88667ms step_avg:96.17ms
step:923/1750 train_time:88767ms step_avg:96.17ms
step:924/1750 train_time:88867ms step_avg:96.18ms
step:925/1750 train_time:88966ms step_avg:96.18ms
step:926/1750 train_time:89066ms step_avg:96.18ms
step:927/1750 train_time:89166ms step_avg:96.19ms
step:928/1750 train_time:89266ms step_avg:96.19ms
step:929/1750 train_time:89366ms step_avg:96.20ms
step:930/1750 train_time:89466ms step_avg:96.20ms
step:931/1750 train_time:89566ms step_avg:96.20ms
step:932/1750 train_time:89667ms step_avg:96.21ms
step:933/1750 train_time:89767ms step_avg:96.21ms
step:934/1750 train_time:89867ms step_avg:96.22ms
step:935/1750 train_time:89967ms step_avg:96.22ms
step:936/1750 train_time:90067ms step_avg:96.23ms
step:937/1750 train_time:90167ms step_avg:96.23ms
step:938/1750 train_time:90267ms step_avg:96.23ms
step:939/1750 train_time:90367ms step_avg:96.24ms
step:940/1750 train_time:90466ms step_avg:96.24ms
step:941/1750 train_time:90566ms step_avg:96.24ms
step:942/1750 train_time:90666ms step_avg:96.25ms
step:943/1750 train_time:90767ms step_avg:96.25ms
step:944/1750 train_time:90866ms step_avg:96.26ms
step:945/1750 train_time:90967ms step_avg:96.26ms
step:946/1750 train_time:91067ms step_avg:96.27ms
step:947/1750 train_time:91166ms step_avg:96.27ms
step:948/1750 train_time:91266ms step_avg:96.27ms
step:949/1750 train_time:91366ms step_avg:96.28ms
step:950/1750 train_time:91465ms step_avg:96.28ms
step:951/1750 train_time:91566ms step_avg:96.28ms
step:952/1750 train_time:91666ms step_avg:96.29ms
step:953/1750 train_time:91766ms step_avg:96.29ms
step:954/1750 train_time:91866ms step_avg:96.30ms
step:955/1750 train_time:91966ms step_avg:96.30ms
step:956/1750 train_time:92066ms step_avg:96.30ms
step:957/1750 train_time:92167ms step_avg:96.31ms
step:958/1750 train_time:92266ms step_avg:96.31ms
step:959/1750 train_time:92366ms step_avg:96.31ms
step:960/1750 train_time:92465ms step_avg:96.32ms
step:961/1750 train_time:92566ms step_avg:96.32ms
step:962/1750 train_time:92666ms step_avg:96.33ms
step:963/1750 train_time:92766ms step_avg:96.33ms
step:964/1750 train_time:92867ms step_avg:96.33ms
step:965/1750 train_time:92966ms step_avg:96.34ms
step:966/1750 train_time:93067ms step_avg:96.34ms
step:967/1750 train_time:93166ms step_avg:96.35ms
step:968/1750 train_time:93266ms step_avg:96.35ms
step:969/1750 train_time:93366ms step_avg:96.35ms
step:970/1750 train_time:93466ms step_avg:96.36ms
step:971/1750 train_time:93566ms step_avg:96.36ms
step:972/1750 train_time:93667ms step_avg:96.36ms
step:973/1750 train_time:93766ms step_avg:96.37ms
step:974/1750 train_time:93866ms step_avg:96.37ms
step:975/1750 train_time:93966ms step_avg:96.38ms
step:976/1750 train_time:94066ms step_avg:96.38ms
step:977/1750 train_time:94166ms step_avg:96.38ms
step:978/1750 train_time:94266ms step_avg:96.39ms
step:979/1750 train_time:94366ms step_avg:96.39ms
step:980/1750 train_time:94466ms step_avg:96.39ms
step:981/1750 train_time:94566ms step_avg:96.40ms
step:982/1750 train_time:94666ms step_avg:96.40ms
step:983/1750 train_time:94767ms step_avg:96.41ms
step:984/1750 train_time:94867ms step_avg:96.41ms
step:985/1750 train_time:94967ms step_avg:96.41ms
step:986/1750 train_time:95067ms step_avg:96.42ms
step:987/1750 train_time:95167ms step_avg:96.42ms
step:988/1750 train_time:95267ms step_avg:96.42ms
step:989/1750 train_time:95367ms step_avg:96.43ms
step:990/1750 train_time:95467ms step_avg:96.43ms
step:991/1750 train_time:95567ms step_avg:96.43ms
step:992/1750 train_time:95666ms step_avg:96.44ms
step:993/1750 train_time:95766ms step_avg:96.44ms
step:994/1750 train_time:95865ms step_avg:96.44ms
step:995/1750 train_time:95965ms step_avg:96.45ms
step:996/1750 train_time:96065ms step_avg:96.45ms
step:997/1750 train_time:96165ms step_avg:96.45ms
step:998/1750 train_time:96266ms step_avg:96.46ms
step:999/1750 train_time:96366ms step_avg:96.46ms
step:1000/1750 train_time:96466ms step_avg:96.47ms
step:1000/1750 val_loss:3.5017 train_time:96561ms step_avg:96.56ms
step:1001/1750 train_time:96586ms step_avg:96.49ms
step:1002/1750 train_time:96676ms step_avg:96.48ms
step:1003/1750 train_time:96779ms step_avg:96.49ms
step:1004/1750 train_time:96879ms step_avg:96.49ms
step:1005/1750 train_time:96979ms step_avg:96.50ms
step:1006/1750 train_time:97078ms step_avg:96.50ms
step:1007/1750 train_time:97178ms step_avg:96.50ms
step:1008/1750 train_time:97278ms step_avg:96.51ms
step:1009/1750 train_time:97378ms step_avg:96.51ms
step:1010/1750 train_time:97479ms step_avg:96.51ms
step:1011/1750 train_time:97583ms step_avg:96.52ms
step:1012/1750 train_time:97687ms step_avg:96.53ms
step:1013/1750 train_time:97788ms step_avg:96.53ms
step:1014/1750 train_time:97888ms step_avg:96.54ms
step:1015/1750 train_time:97988ms step_avg:96.54ms
step:1016/1750 train_time:98087ms step_avg:96.54ms
step:1017/1750 train_time:98188ms step_avg:96.55ms
step:1018/1750 train_time:98288ms step_avg:96.55ms
step:1019/1750 train_time:98387ms step_avg:96.55ms
step:1020/1750 train_time:98487ms step_avg:96.56ms
step:1021/1750 train_time:98588ms step_avg:96.56ms
step:1022/1750 train_time:98688ms step_avg:96.56ms
step:1023/1750 train_time:98788ms step_avg:96.57ms
step:1024/1750 train_time:98889ms step_avg:96.57ms
step:1025/1750 train_time:98988ms step_avg:96.57ms
step:1026/1750 train_time:99089ms step_avg:96.58ms
step:1027/1750 train_time:99189ms step_avg:96.58ms
step:1028/1750 train_time:99288ms step_avg:96.58ms
step:1029/1750 train_time:99389ms step_avg:96.59ms
step:1030/1750 train_time:99489ms step_avg:96.59ms
step:1031/1750 train_time:99589ms step_avg:96.59ms
step:1032/1750 train_time:99689ms step_avg:96.60ms
step:1033/1750 train_time:99788ms step_avg:96.60ms
step:1034/1750 train_time:99889ms step_avg:96.60ms
step:1035/1750 train_time:99989ms step_avg:96.61ms
step:1036/1750 train_time:100089ms step_avg:96.61ms
step:1037/1750 train_time:100191ms step_avg:96.62ms
step:1038/1750 train_time:100292ms step_avg:96.62ms
step:1039/1750 train_time:100392ms step_avg:96.62ms
step:1040/1750 train_time:100492ms step_avg:96.63ms
step:1041/1750 train_time:100593ms step_avg:96.63ms
step:1042/1750 train_time:100693ms step_avg:96.63ms
step:1043/1750 train_time:100795ms step_avg:96.64ms
step:1044/1750 train_time:100896ms step_avg:96.64ms
step:1045/1750 train_time:100997ms step_avg:96.65ms
step:1046/1750 train_time:101098ms step_avg:96.65ms
step:1047/1750 train_time:101199ms step_avg:96.66ms
step:1048/1750 train_time:101299ms step_avg:96.66ms
step:1049/1750 train_time:101399ms step_avg:96.66ms
step:1050/1750 train_time:101500ms step_avg:96.67ms
step:1051/1750 train_time:101602ms step_avg:96.67ms
step:1052/1750 train_time:101703ms step_avg:96.68ms
step:1053/1750 train_time:101804ms step_avg:96.68ms
step:1054/1750 train_time:101905ms step_avg:96.68ms
step:1055/1750 train_time:102006ms step_avg:96.69ms
step:1056/1750 train_time:102106ms step_avg:96.69ms
step:1057/1750 train_time:102206ms step_avg:96.69ms
step:1058/1750 train_time:102307ms step_avg:96.70ms
step:1059/1750 train_time:102407ms step_avg:96.70ms
step:1060/1750 train_time:102508ms step_avg:96.71ms
step:1061/1750 train_time:102608ms step_avg:96.71ms
step:1062/1750 train_time:102708ms step_avg:96.71ms
step:1063/1750 train_time:102808ms step_avg:96.72ms
step:1064/1750 train_time:102909ms step_avg:96.72ms
step:1065/1750 train_time:103009ms step_avg:96.72ms
step:1066/1750 train_time:103108ms step_avg:96.72ms
step:1067/1750 train_time:103209ms step_avg:96.73ms
step:1068/1750 train_time:103309ms step_avg:96.73ms
step:1069/1750 train_time:103409ms step_avg:96.73ms
step:1070/1750 train_time:103509ms step_avg:96.74ms
step:1071/1750 train_time:103609ms step_avg:96.74ms
step:1072/1750 train_time:103709ms step_avg:96.74ms
step:1073/1750 train_time:103809ms step_avg:96.75ms
step:1074/1750 train_time:103908ms step_avg:96.75ms
step:1075/1750 train_time:104009ms step_avg:96.75ms
step:1076/1750 train_time:104109ms step_avg:96.76ms
step:1077/1750 train_time:104209ms step_avg:96.76ms
step:1078/1750 train_time:104309ms step_avg:96.76ms
step:1079/1750 train_time:104409ms step_avg:96.76ms
step:1080/1750 train_time:104509ms step_avg:96.77ms
step:1081/1750 train_time:104609ms step_avg:96.77ms
step:1082/1750 train_time:104710ms step_avg:96.77ms
step:1083/1750 train_time:104810ms step_avg:96.78ms
step:1084/1750 train_time:104910ms step_avg:96.78ms
step:1085/1750 train_time:105011ms step_avg:96.78ms
step:1086/1750 train_time:105112ms step_avg:96.79ms
step:1087/1750 train_time:105212ms step_avg:96.79ms
step:1088/1750 train_time:105313ms step_avg:96.79ms
step:1089/1750 train_time:105413ms step_avg:96.80ms
step:1090/1750 train_time:105515ms step_avg:96.80ms
step:1091/1750 train_time:105617ms step_avg:96.81ms
step:1092/1750 train_time:105717ms step_avg:96.81ms
step:1093/1750 train_time:105818ms step_avg:96.81ms
step:1094/1750 train_time:105919ms step_avg:96.82ms
step:1095/1750 train_time:106019ms step_avg:96.82ms
step:1096/1750 train_time:106120ms step_avg:96.82ms
step:1097/1750 train_time:106220ms step_avg:96.83ms
step:1098/1750 train_time:106321ms step_avg:96.83ms
step:1099/1750 train_time:106421ms step_avg:96.83ms
step:1100/1750 train_time:106522ms step_avg:96.84ms
step:1101/1750 train_time:106622ms step_avg:96.84ms
step:1102/1750 train_time:106723ms step_avg:96.85ms
step:1103/1750 train_time:106824ms step_avg:96.85ms
step:1104/1750 train_time:106924ms step_avg:96.85ms
step:1105/1750 train_time:107024ms step_avg:96.85ms
step:1106/1750 train_time:107125ms step_avg:96.86ms
step:1107/1750 train_time:107225ms step_avg:96.86ms
step:1108/1750 train_time:107326ms step_avg:96.86ms
step:1109/1750 train_time:107426ms step_avg:96.87ms
step:1110/1750 train_time:107527ms step_avg:96.87ms
step:1111/1750 train_time:107627ms step_avg:96.87ms
step:1112/1750 train_time:107728ms step_avg:96.88ms
step:1113/1750 train_time:107829ms step_avg:96.88ms
step:1114/1750 train_time:107929ms step_avg:96.88ms
step:1115/1750 train_time:108029ms step_avg:96.89ms
step:1116/1750 train_time:108129ms step_avg:96.89ms
step:1117/1750 train_time:108229ms step_avg:96.89ms
step:1118/1750 train_time:108329ms step_avg:96.90ms
step:1119/1750 train_time:108429ms step_avg:96.90ms
step:1120/1750 train_time:108529ms step_avg:96.90ms
step:1121/1750 train_time:108629ms step_avg:96.90ms
step:1122/1750 train_time:108729ms step_avg:96.91ms
step:1123/1750 train_time:108829ms step_avg:96.91ms
step:1124/1750 train_time:108929ms step_avg:96.91ms
step:1125/1750 train_time:109029ms step_avg:96.91ms
step:1125/1750 val_loss:3.4504 train_time:109123ms step_avg:97.00ms
step:1126/1750 train_time:109148ms step_avg:96.93ms
step:1127/1750 train_time:109237ms step_avg:96.93ms
step:1128/1750 train_time:109338ms step_avg:96.93ms
step:1129/1750 train_time:109439ms step_avg:96.93ms
step:1130/1750 train_time:109540ms step_avg:96.94ms
step:1131/1750 train_time:109639ms step_avg:96.94ms
step:1132/1750 train_time:109740ms step_avg:96.94ms
step:1133/1750 train_time:109840ms step_avg:96.95ms
step:1134/1750 train_time:109939ms step_avg:96.95ms
step:1135/1750 train_time:110039ms step_avg:96.95ms
step:1136/1750 train_time:110140ms step_avg:96.95ms
step:1137/1750 train_time:110241ms step_avg:96.96ms
step:1138/1750 train_time:110341ms step_avg:96.96ms
step:1139/1750 train_time:110442ms step_avg:96.96ms
step:1140/1750 train_time:110542ms step_avg:96.97ms
step:1141/1750 train_time:110642ms step_avg:96.97ms
step:1142/1750 train_time:110743ms step_avg:96.97ms
step:1143/1750 train_time:110842ms step_avg:96.97ms
step:1144/1750 train_time:110942ms step_avg:96.98ms
step:1145/1750 train_time:111043ms step_avg:96.98ms
step:1146/1750 train_time:111144ms step_avg:96.98ms
step:1147/1750 train_time:111245ms step_avg:96.99ms
step:1148/1750 train_time:111346ms step_avg:96.99ms
step:1149/1750 train_time:111448ms step_avg:97.00ms
step:1150/1750 train_time:111549ms step_avg:97.00ms
step:1151/1750 train_time:111649ms step_avg:97.00ms
step:1152/1750 train_time:111750ms step_avg:97.00ms
step:1153/1750 train_time:111851ms step_avg:97.01ms
step:1154/1750 train_time:111952ms step_avg:97.01ms
step:1155/1750 train_time:112054ms step_avg:97.02ms
step:1156/1750 train_time:112155ms step_avg:97.02ms
step:1157/1750 train_time:112258ms step_avg:97.03ms
step:1158/1750 train_time:112359ms step_avg:97.03ms
step:1159/1750 train_time:112460ms step_avg:97.03ms
step:1160/1750 train_time:112561ms step_avg:97.04ms
step:1161/1750 train_time:112661ms step_avg:97.04ms
step:1162/1750 train_time:112761ms step_avg:97.04ms
step:1163/1750 train_time:112863ms step_avg:97.04ms
step:1164/1750 train_time:112963ms step_avg:97.05ms
step:1165/1750 train_time:113063ms step_avg:97.05ms
step:1166/1750 train_time:113163ms step_avg:97.05ms
step:1167/1750 train_time:113265ms step_avg:97.06ms
step:1168/1750 train_time:113367ms step_avg:97.06ms
step:1169/1750 train_time:113470ms step_avg:97.07ms
step:1170/1750 train_time:113571ms step_avg:97.07ms
step:1171/1750 train_time:113673ms step_avg:97.07ms
step:1172/1750 train_time:113776ms step_avg:97.08ms
step:1173/1750 train_time:113878ms step_avg:97.08ms
step:1174/1750 train_time:113980ms step_avg:97.09ms
step:1175/1750 train_time:114080ms step_avg:97.09ms
step:1176/1750 train_time:114182ms step_avg:97.09ms
step:1177/1750 train_time:114283ms step_avg:97.10ms
step:1178/1750 train_time:114384ms step_avg:97.10ms
step:1179/1750 train_time:114488ms step_avg:97.11ms
step:1180/1750 train_time:114591ms step_avg:97.11ms
step:1181/1750 train_time:114694ms step_avg:97.12ms
step:1182/1750 train_time:114796ms step_avg:97.12ms
step:1183/1750 train_time:114897ms step_avg:97.12ms
step:1184/1750 train_time:114999ms step_avg:97.13ms
step:1185/1750 train_time:115101ms step_avg:97.13ms
step:1186/1750 train_time:115202ms step_avg:97.14ms
step:1187/1750 train_time:115303ms step_avg:97.14ms
step:1188/1750 train_time:115404ms step_avg:97.14ms
step:1189/1750 train_time:115506ms step_avg:97.15ms
step:1190/1750 train_time:115608ms step_avg:97.15ms
step:1191/1750 train_time:115710ms step_avg:97.15ms
step:1192/1750 train_time:115813ms step_avg:97.16ms
step:1193/1750 train_time:115915ms step_avg:97.16ms
step:1194/1750 train_time:116018ms step_avg:97.17ms
step:1195/1750 train_time:116119ms step_avg:97.17ms
step:1196/1750 train_time:116221ms step_avg:97.17ms
step:1197/1750 train_time:116323ms step_avg:97.18ms
step:1198/1750 train_time:116425ms step_avg:97.18ms
step:1199/1750 train_time:116526ms step_avg:97.19ms
step:1200/1750 train_time:116628ms step_avg:97.19ms
step:1201/1750 train_time:116729ms step_avg:97.19ms
step:1202/1750 train_time:116833ms step_avg:97.20ms
step:1203/1750 train_time:116934ms step_avg:97.20ms
step:1204/1750 train_time:117036ms step_avg:97.21ms
step:1205/1750 train_time:117137ms step_avg:97.21ms
step:1206/1750 train_time:117238ms step_avg:97.21ms
step:1207/1750 train_time:117340ms step_avg:97.22ms
step:1208/1750 train_time:117442ms step_avg:97.22ms
step:1209/1750 train_time:117543ms step_avg:97.22ms
step:1210/1750 train_time:117645ms step_avg:97.23ms
step:1211/1750 train_time:117746ms step_avg:97.23ms
step:1212/1750 train_time:117849ms step_avg:97.24ms
step:1213/1750 train_time:117952ms step_avg:97.24ms
step:1214/1750 train_time:118054ms step_avg:97.24ms
step:1215/1750 train_time:118155ms step_avg:97.25ms
step:1216/1750 train_time:118258ms step_avg:97.25ms
step:1217/1750 train_time:118360ms step_avg:97.26ms
step:1218/1750 train_time:118462ms step_avg:97.26ms
step:1219/1750 train_time:118564ms step_avg:97.26ms
step:1220/1750 train_time:118665ms step_avg:97.27ms
step:1221/1750 train_time:118766ms step_avg:97.27ms
step:1222/1750 train_time:118869ms step_avg:97.27ms
step:1223/1750 train_time:118973ms step_avg:97.28ms
step:1224/1750 train_time:119074ms step_avg:97.28ms
step:1225/1750 train_time:119176ms step_avg:97.29ms
step:1226/1750 train_time:119278ms step_avg:97.29ms
step:1227/1750 train_time:119380ms step_avg:97.29ms
step:1228/1750 train_time:119481ms step_avg:97.30ms
step:1229/1750 train_time:119582ms step_avg:97.30ms
step:1230/1750 train_time:119684ms step_avg:97.30ms
step:1231/1750 train_time:119786ms step_avg:97.31ms
step:1232/1750 train_time:119890ms step_avg:97.31ms
step:1233/1750 train_time:119992ms step_avg:97.32ms
step:1234/1750 train_time:120094ms step_avg:97.32ms
step:1235/1750 train_time:120195ms step_avg:97.32ms
step:1236/1750 train_time:120297ms step_avg:97.33ms
step:1237/1750 train_time:120399ms step_avg:97.33ms
step:1238/1750 train_time:120501ms step_avg:97.33ms
step:1239/1750 train_time:120603ms step_avg:97.34ms
step:1240/1750 train_time:120703ms step_avg:97.34ms
step:1241/1750 train_time:120805ms step_avg:97.35ms
step:1242/1750 train_time:120907ms step_avg:97.35ms
step:1243/1750 train_time:121009ms step_avg:97.35ms
step:1244/1750 train_time:121110ms step_avg:97.36ms
step:1245/1750 train_time:121212ms step_avg:97.36ms
step:1246/1750 train_time:121314ms step_avg:97.36ms
step:1247/1750 train_time:121416ms step_avg:97.37ms
step:1248/1750 train_time:121519ms step_avg:97.37ms
step:1249/1750 train_time:121621ms step_avg:97.37ms
step:1250/1750 train_time:121723ms step_avg:97.38ms
step:1250/1750 val_loss:3.4047 train_time:121819ms step_avg:97.46ms
step:1251/1750 train_time:121844ms step_avg:97.40ms
step:1252/1750 train_time:121935ms step_avg:97.39ms
step:1253/1750 train_time:122038ms step_avg:97.40ms
step:1254/1750 train_time:122140ms step_avg:97.40ms
step:1255/1750 train_time:122241ms step_avg:97.40ms
step:1256/1750 train_time:122342ms step_avg:97.41ms
step:1257/1750 train_time:122442ms step_avg:97.41ms
step:1258/1750 train_time:122543ms step_avg:97.41ms
step:1259/1750 train_time:122644ms step_avg:97.41ms
step:1260/1750 train_time:122745ms step_avg:97.42ms
step:1261/1750 train_time:122848ms step_avg:97.42ms
step:1262/1750 train_time:122951ms step_avg:97.43ms
step:1263/1750 train_time:123053ms step_avg:97.43ms
step:1264/1750 train_time:123155ms step_avg:97.43ms
step:1265/1750 train_time:123257ms step_avg:97.44ms
step:1266/1750 train_time:123358ms step_avg:97.44ms
step:1267/1750 train_time:123459ms step_avg:97.44ms
step:1268/1750 train_time:123560ms step_avg:97.45ms
step:1269/1750 train_time:123662ms step_avg:97.45ms
step:1270/1750 train_time:123764ms step_avg:97.45ms
step:1271/1750 train_time:123867ms step_avg:97.46ms
step:1272/1750 train_time:123969ms step_avg:97.46ms
step:1273/1750 train_time:124070ms step_avg:97.46ms
step:1274/1750 train_time:124172ms step_avg:97.47ms
step:1275/1750 train_time:124274ms step_avg:97.47ms
step:1276/1750 train_time:124377ms step_avg:97.47ms
step:1277/1750 train_time:124478ms step_avg:97.48ms
step:1278/1750 train_time:124580ms step_avg:97.48ms
step:1279/1750 train_time:124682ms step_avg:97.48ms
step:1280/1750 train_time:124784ms step_avg:97.49ms
step:1281/1750 train_time:124887ms step_avg:97.49ms
step:1282/1750 train_time:124988ms step_avg:97.49ms
step:1283/1750 train_time:125089ms step_avg:97.50ms
step:1284/1750 train_time:125189ms step_avg:97.50ms
step:1285/1750 train_time:125292ms step_avg:97.50ms
step:1286/1750 train_time:125394ms step_avg:97.51ms
step:1287/1750 train_time:125496ms step_avg:97.51ms
step:1288/1750 train_time:125597ms step_avg:97.51ms
step:1289/1750 train_time:125699ms step_avg:97.52ms
step:1290/1750 train_time:125802ms step_avg:97.52ms
step:1291/1750 train_time:125904ms step_avg:97.52ms
step:1292/1750 train_time:126006ms step_avg:97.53ms
step:1293/1750 train_time:126108ms step_avg:97.53ms
step:1294/1750 train_time:126210ms step_avg:97.53ms
step:1295/1750 train_time:126311ms step_avg:97.54ms
step:1296/1750 train_time:126414ms step_avg:97.54ms
step:1297/1750 train_time:126516ms step_avg:97.54ms
step:1298/1750 train_time:126617ms step_avg:97.55ms
step:1299/1750 train_time:126719ms step_avg:97.55ms
step:1300/1750 train_time:126822ms step_avg:97.56ms
step:1301/1750 train_time:126925ms step_avg:97.56ms
step:1302/1750 train_time:127026ms step_avg:97.56ms
step:1303/1750 train_time:127128ms step_avg:97.57ms
step:1304/1750 train_time:127229ms step_avg:97.57ms
step:1305/1750 train_time:127331ms step_avg:97.57ms
step:1306/1750 train_time:127433ms step_avg:97.58ms
step:1307/1750 train_time:127536ms step_avg:97.58ms
step:1308/1750 train_time:127638ms step_avg:97.58ms
step:1309/1750 train_time:127739ms step_avg:97.59ms
step:1310/1750 train_time:127842ms step_avg:97.59ms
step:1311/1750 train_time:127944ms step_avg:97.59ms
step:1312/1750 train_time:128046ms step_avg:97.60ms
step:1313/1750 train_time:128148ms step_avg:97.60ms
step:1314/1750 train_time:128249ms step_avg:97.60ms
step:1315/1750 train_time:128352ms step_avg:97.61ms
step:1316/1750 train_time:128453ms step_avg:97.61ms
step:1317/1750 train_time:128555ms step_avg:97.61ms
step:1318/1750 train_time:128657ms step_avg:97.62ms
step:1319/1750 train_time:128758ms step_avg:97.62ms
step:1320/1750 train_time:128861ms step_avg:97.62ms
step:1321/1750 train_time:128963ms step_avg:97.63ms
step:1322/1750 train_time:129065ms step_avg:97.63ms
step:1323/1750 train_time:129166ms step_avg:97.63ms
step:1324/1750 train_time:129269ms step_avg:97.64ms
step:1325/1750 train_time:129370ms step_avg:97.64ms
step:1326/1750 train_time:129473ms step_avg:97.64ms
step:1327/1750 train_time:129575ms step_avg:97.64ms
step:1328/1750 train_time:129677ms step_avg:97.65ms
step:1329/1750 train_time:129778ms step_avg:97.65ms
step:1330/1750 train_time:129880ms step_avg:97.65ms
step:1331/1750 train_time:129983ms step_avg:97.66ms
step:1332/1750 train_time:130085ms step_avg:97.66ms
step:1333/1750 train_time:130187ms step_avg:97.66ms
step:1334/1750 train_time:130288ms step_avg:97.67ms
step:1335/1750 train_time:130389ms step_avg:97.67ms
step:1336/1750 train_time:130491ms step_avg:97.67ms
step:1337/1750 train_time:130594ms step_avg:97.68ms
step:1338/1750 train_time:130696ms step_avg:97.68ms
step:1339/1750 train_time:130798ms step_avg:97.68ms
step:1340/1750 train_time:130900ms step_avg:97.69ms
step:1341/1750 train_time:131002ms step_avg:97.69ms
step:1342/1750 train_time:131103ms step_avg:97.69ms
step:1343/1750 train_time:131205ms step_avg:97.70ms
step:1344/1750 train_time:131307ms step_avg:97.70ms
step:1345/1750 train_time:131409ms step_avg:97.70ms
step:1346/1750 train_time:131511ms step_avg:97.71ms
step:1347/1750 train_time:131614ms step_avg:97.71ms
step:1348/1750 train_time:131716ms step_avg:97.71ms
step:1349/1750 train_time:131818ms step_avg:97.72ms
step:1350/1750 train_time:131920ms step_avg:97.72ms
step:1351/1750 train_time:132022ms step_avg:97.72ms
step:1352/1750 train_time:132124ms step_avg:97.73ms
step:1353/1750 train_time:132226ms step_avg:97.73ms
step:1354/1750 train_time:132327ms step_avg:97.73ms
step:1355/1750 train_time:132429ms step_avg:97.73ms
step:1356/1750 train_time:132531ms step_avg:97.74ms
step:1357/1750 train_time:132633ms step_avg:97.74ms
step:1358/1750 train_time:132736ms step_avg:97.74ms
step:1359/1750 train_time:132838ms step_avg:97.75ms
step:1360/1750 train_time:132940ms step_avg:97.75ms
step:1361/1750 train_time:133041ms step_avg:97.75ms
step:1362/1750 train_time:133143ms step_avg:97.76ms
step:1363/1750 train_time:133246ms step_avg:97.76ms
step:1364/1750 train_time:133349ms step_avg:97.76ms
step:1365/1750 train_time:133450ms step_avg:97.77ms
step:1366/1750 train_time:133552ms step_avg:97.77ms
step:1367/1750 train_time:133653ms step_avg:97.77ms
step:1368/1750 train_time:133757ms step_avg:97.78ms
step:1369/1750 train_time:133859ms step_avg:97.78ms
step:1370/1750 train_time:133961ms step_avg:97.78ms
step:1371/1750 train_time:134063ms step_avg:97.78ms
step:1372/1750 train_time:134164ms step_avg:97.79ms
step:1373/1750 train_time:134267ms step_avg:97.79ms
step:1374/1750 train_time:134369ms step_avg:97.79ms
step:1375/1750 train_time:134471ms step_avg:97.80ms
step:1375/1750 val_loss:3.3638 train_time:134566ms step_avg:97.87ms
step:1376/1750 train_time:134591ms step_avg:97.81ms
step:1377/1750 train_time:134683ms step_avg:97.81ms
step:1378/1750 train_time:134786ms step_avg:97.81ms
step:1379/1750 train_time:134887ms step_avg:97.82ms
step:1380/1750 train_time:134990ms step_avg:97.82ms
step:1381/1750 train_time:135092ms step_avg:97.82ms
step:1382/1750 train_time:135193ms step_avg:97.82ms
step:1383/1750 train_time:135294ms step_avg:97.83ms
step:1384/1750 train_time:135396ms step_avg:97.83ms
step:1385/1750 train_time:135497ms step_avg:97.83ms
step:1386/1750 train_time:135599ms step_avg:97.83ms
step:1387/1750 train_time:135702ms step_avg:97.84ms
step:1388/1750 train_time:135804ms step_avg:97.84ms
step:1389/1750 train_time:135907ms step_avg:97.85ms
step:1390/1750 train_time:136010ms step_avg:97.85ms
step:1391/1750 train_time:136112ms step_avg:97.85ms
step:1392/1750 train_time:136215ms step_avg:97.86ms
step:1393/1750 train_time:136316ms step_avg:97.86ms
step:1394/1750 train_time:136418ms step_avg:97.86ms
step:1395/1750 train_time:136519ms step_avg:97.86ms
step:1396/1750 train_time:136621ms step_avg:97.87ms
step:1397/1750 train_time:136723ms step_avg:97.87ms
step:1398/1750 train_time:136825ms step_avg:97.87ms
step:1399/1750 train_time:136927ms step_avg:97.88ms
step:1400/1750 train_time:137030ms step_avg:97.88ms
step:1401/1750 train_time:137132ms step_avg:97.88ms
step:1402/1750 train_time:137234ms step_avg:97.88ms
step:1403/1750 train_time:137335ms step_avg:97.89ms
step:1404/1750 train_time:137437ms step_avg:97.89ms
step:1405/1750 train_time:137539ms step_avg:97.89ms
step:1406/1750 train_time:137641ms step_avg:97.90ms
step:1407/1750 train_time:137744ms step_avg:97.90ms
step:1408/1750 train_time:137847ms step_avg:97.90ms
step:1409/1750 train_time:137951ms step_avg:97.91ms
step:1410/1750 train_time:138052ms step_avg:97.91ms
step:1411/1750 train_time:138154ms step_avg:97.91ms
step:1412/1750 train_time:138256ms step_avg:97.91ms
step:1413/1750 train_time:138357ms step_avg:97.92ms
step:1414/1750 train_time:138459ms step_avg:97.92ms
step:1415/1750 train_time:138562ms step_avg:97.92ms
step:1416/1750 train_time:138663ms step_avg:97.93ms
step:1417/1750 train_time:138765ms step_avg:97.93ms
step:1418/1750 train_time:138867ms step_avg:97.93ms
step:1419/1750 train_time:138971ms step_avg:97.94ms
step:1420/1750 train_time:139072ms step_avg:97.94ms
step:1421/1750 train_time:139174ms step_avg:97.94ms
step:1422/1750 train_time:139275ms step_avg:97.94ms
step:1423/1750 train_time:139377ms step_avg:97.95ms
step:1424/1750 train_time:139479ms step_avg:97.95ms
step:1425/1750 train_time:139582ms step_avg:97.95ms
step:1426/1750 train_time:139683ms step_avg:97.95ms
step:1427/1750 train_time:139785ms step_avg:97.96ms
step:1428/1750 train_time:139889ms step_avg:97.96ms
step:1429/1750 train_time:139992ms step_avg:97.97ms
step:1430/1750 train_time:140095ms step_avg:97.97ms
step:1431/1750 train_time:140198ms step_avg:97.97ms
step:1432/1750 train_time:140301ms step_avg:97.98ms
step:1433/1750 train_time:140403ms step_avg:97.98ms
step:1434/1750 train_time:140506ms step_avg:97.98ms
step:1435/1750 train_time:140610ms step_avg:97.99ms
step:1436/1750 train_time:140712ms step_avg:97.99ms
step:1437/1750 train_time:140816ms step_avg:97.99ms
step:1438/1750 train_time:140919ms step_avg:98.00ms
step:1439/1750 train_time:141023ms step_avg:98.00ms
step:1440/1750 train_time:141127ms step_avg:98.00ms
step:1441/1750 train_time:141231ms step_avg:98.01ms
step:1442/1750 train_time:141332ms step_avg:98.01ms
step:1443/1750 train_time:141436ms step_avg:98.02ms
step:1444/1750 train_time:141541ms step_avg:98.02ms
step:1445/1750 train_time:141643ms step_avg:98.02ms
step:1446/1750 train_time:141746ms step_avg:98.03ms
step:1447/1750 train_time:141848ms step_avg:98.03ms
step:1448/1750 train_time:141953ms step_avg:98.03ms
step:1449/1750 train_time:142054ms step_avg:98.04ms
step:1450/1750 train_time:142157ms step_avg:98.04ms
step:1451/1750 train_time:142260ms step_avg:98.04ms
step:1452/1750 train_time:142363ms step_avg:98.05ms
step:1453/1750 train_time:142467ms step_avg:98.05ms
step:1454/1750 train_time:142571ms step_avg:98.05ms
step:1455/1750 train_time:142674ms step_avg:98.06ms
step:1456/1750 train_time:142776ms step_avg:98.06ms
step:1457/1750 train_time:142881ms step_avg:98.07ms
step:1458/1750 train_time:142984ms step_avg:98.07ms
step:1459/1750 train_time:143087ms step_avg:98.07ms
step:1460/1750 train_time:143189ms step_avg:98.07ms
step:1461/1750 train_time:143293ms step_avg:98.08ms
step:1462/1750 train_time:143396ms step_avg:98.08ms
step:1463/1750 train_time:143499ms step_avg:98.09ms
step:1464/1750 train_time:143603ms step_avg:98.09ms
step:1465/1750 train_time:143706ms step_avg:98.09ms
step:1466/1750 train_time:143811ms step_avg:98.10ms
step:1467/1750 train_time:143912ms step_avg:98.10ms
step:1468/1750 train_time:144016ms step_avg:98.10ms
step:1469/1750 train_time:144119ms step_avg:98.11ms
step:1470/1750 train_time:144223ms step_avg:98.11ms
step:1471/1750 train_time:144326ms step_avg:98.11ms
step:1472/1750 train_time:144429ms step_avg:98.12ms
step:1473/1750 train_time:144532ms step_avg:98.12ms
step:1474/1750 train_time:144636ms step_avg:98.12ms
step:1475/1750 train_time:144739ms step_avg:98.13ms
step:1476/1750 train_time:144842ms step_avg:98.13ms
step:1477/1750 train_time:144945ms step_avg:98.13ms
step:1478/1750 train_time:145048ms step_avg:98.14ms
step:1479/1750 train_time:145150ms step_avg:98.14ms
step:1480/1750 train_time:145252ms step_avg:98.14ms
step:1481/1750 train_time:145356ms step_avg:98.15ms
step:1482/1750 train_time:145460ms step_avg:98.15ms
step:1483/1750 train_time:145563ms step_avg:98.15ms
step:1484/1750 train_time:145667ms step_avg:98.16ms
step:1485/1750 train_time:145770ms step_avg:98.16ms
step:1486/1750 train_time:145873ms step_avg:98.16ms
step:1487/1750 train_time:145975ms step_avg:98.17ms
step:1488/1750 train_time:146079ms step_avg:98.17ms
step:1489/1750 train_time:146182ms step_avg:98.17ms
step:1490/1750 train_time:146284ms step_avg:98.18ms
step:1491/1750 train_time:146388ms step_avg:98.18ms
step:1492/1750 train_time:146490ms step_avg:98.18ms
step:1493/1750 train_time:146593ms step_avg:98.19ms
step:1494/1750 train_time:146696ms step_avg:98.19ms
step:1495/1750 train_time:146799ms step_avg:98.19ms
step:1496/1750 train_time:146903ms step_avg:98.20ms
step:1497/1750 train_time:147005ms step_avg:98.20ms
step:1498/1750 train_time:147109ms step_avg:98.20ms
step:1499/1750 train_time:147210ms step_avg:98.21ms
step:1500/1750 train_time:147314ms step_avg:98.21ms
step:1500/1750 val_loss:3.3278 train_time:147411ms step_avg:98.27ms
step:1501/1750 train_time:147436ms step_avg:98.23ms
step:1502/1750 train_time:147529ms step_avg:98.22ms
step:1503/1750 train_time:147631ms step_avg:98.22ms
step:1504/1750 train_time:147734ms step_avg:98.23ms
step:1505/1750 train_time:147837ms step_avg:98.23ms
step:1506/1750 train_time:147938ms step_avg:98.23ms
step:1507/1750 train_time:148041ms step_avg:98.24ms
step:1508/1750 train_time:148144ms step_avg:98.24ms
step:1509/1750 train_time:148247ms step_avg:98.24ms
step:1510/1750 train_time:148349ms step_avg:98.24ms
step:1511/1750 train_time:148454ms step_avg:98.25ms
step:1512/1750 train_time:148558ms step_avg:98.25ms
step:1513/1750 train_time:148661ms step_avg:98.26ms
step:1514/1750 train_time:148765ms step_avg:98.26ms
step:1515/1750 train_time:148870ms step_avg:98.26ms
step:1516/1750 train_time:148974ms step_avg:98.27ms
step:1517/1750 train_time:149076ms step_avg:98.27ms
step:1518/1750 train_time:149178ms step_avg:98.27ms
step:1519/1750 train_time:149282ms step_avg:98.28ms
step:1520/1750 train_time:149385ms step_avg:98.28ms
step:1521/1750 train_time:149487ms step_avg:98.28ms
step:1522/1750 train_time:149591ms step_avg:98.29ms
step:1523/1750 train_time:149695ms step_avg:98.29ms
step:1524/1750 train_time:149798ms step_avg:98.29ms
step:1525/1750 train_time:149903ms step_avg:98.30ms
step:1526/1750 train_time:150005ms step_avg:98.30ms
step:1527/1750 train_time:150107ms step_avg:98.30ms
step:1528/1750 train_time:150213ms step_avg:98.31ms
step:1529/1750 train_time:150315ms step_avg:98.31ms
step:1530/1750 train_time:150419ms step_avg:98.31ms
step:1531/1750 train_time:150522ms step_avg:98.32ms
step:1532/1750 train_time:150625ms step_avg:98.32ms
step:1533/1750 train_time:150728ms step_avg:98.32ms
step:1534/1750 train_time:150831ms step_avg:98.33ms
step:1535/1750 train_time:150935ms step_avg:98.33ms
step:1536/1750 train_time:151037ms step_avg:98.33ms
step:1537/1750 train_time:151140ms step_avg:98.33ms
step:1538/1750 train_time:151243ms step_avg:98.34ms
step:1539/1750 train_time:151344ms step_avg:98.34ms
step:1540/1750 train_time:151447ms step_avg:98.34ms
step:1541/1750 train_time:151551ms step_avg:98.35ms
step:1542/1750 train_time:151654ms step_avg:98.35ms
step:1543/1750 train_time:151758ms step_avg:98.35ms
step:1544/1750 train_time:151862ms step_avg:98.36ms
step:1545/1750 train_time:151965ms step_avg:98.36ms
step:1546/1750 train_time:152067ms step_avg:98.36ms
step:1547/1750 train_time:152171ms step_avg:98.37ms
step:1548/1750 train_time:152275ms step_avg:98.37ms
step:1549/1750 train_time:152379ms step_avg:98.37ms
step:1550/1750 train_time:152481ms step_avg:98.38ms
step:1551/1750 train_time:152587ms step_avg:98.38ms
step:1552/1750 train_time:152689ms step_avg:98.38ms
step:1553/1750 train_time:152792ms step_avg:98.39ms
step:1554/1750 train_time:152895ms step_avg:98.39ms
step:1555/1750 train_time:152998ms step_avg:98.39ms
step:1556/1750 train_time:153103ms step_avg:98.39ms
step:1557/1750 train_time:153206ms step_avg:98.40ms
step:1558/1750 train_time:153309ms step_avg:98.40ms
step:1559/1750 train_time:153413ms step_avg:98.40ms
step:1560/1750 train_time:153517ms step_avg:98.41ms
step:1561/1750 train_time:153620ms step_avg:98.41ms
step:1562/1750 train_time:153724ms step_avg:98.42ms
step:1563/1750 train_time:153830ms step_avg:98.42ms
step:1564/1750 train_time:153932ms step_avg:98.42ms
step:1565/1750 train_time:154036ms step_avg:98.43ms
step:1566/1750 train_time:154140ms step_avg:98.43ms
step:1567/1750 train_time:154244ms step_avg:98.43ms
step:1568/1750 train_time:154347ms step_avg:98.44ms
step:1569/1750 train_time:154448ms step_avg:98.44ms
step:1570/1750 train_time:154552ms step_avg:98.44ms
step:1571/1750 train_time:154656ms step_avg:98.44ms
step:1572/1750 train_time:154760ms step_avg:98.45ms
step:1573/1750 train_time:154864ms step_avg:98.45ms
step:1574/1750 train_time:154967ms step_avg:98.45ms
step:1575/1750 train_time:155069ms step_avg:98.46ms
step:1576/1750 train_time:155173ms step_avg:98.46ms
step:1577/1750 train_time:155278ms step_avg:98.46ms
step:1578/1750 train_time:155380ms step_avg:98.47ms
step:1579/1750 train_time:155483ms step_avg:98.47ms
step:1580/1750 train_time:155587ms step_avg:98.47ms
step:1581/1750 train_time:155690ms step_avg:98.48ms
step:1582/1750 train_time:155794ms step_avg:98.48ms
step:1583/1750 train_time:155897ms step_avg:98.48ms
step:1584/1750 train_time:156003ms step_avg:98.49ms
step:1585/1750 train_time:156107ms step_avg:98.49ms
step:1586/1750 train_time:156211ms step_avg:98.49ms
step:1587/1750 train_time:156315ms step_avg:98.50ms
step:1588/1750 train_time:156418ms step_avg:98.50ms
step:1589/1750 train_time:156521ms step_avg:98.50ms
step:1590/1750 train_time:156625ms step_avg:98.51ms
step:1591/1750 train_time:156728ms step_avg:98.51ms
step:1592/1750 train_time:156831ms step_avg:98.51ms
step:1593/1750 train_time:156934ms step_avg:98.51ms
step:1594/1750 train_time:157040ms step_avg:98.52ms
step:1595/1750 train_time:157143ms step_avg:98.52ms
step:1596/1750 train_time:157245ms step_avg:98.52ms
step:1597/1750 train_time:157348ms step_avg:98.53ms
step:1598/1750 train_time:157452ms step_avg:98.53ms
step:1599/1750 train_time:157555ms step_avg:98.53ms
step:1600/1750 train_time:157659ms step_avg:98.54ms
step:1601/1750 train_time:157763ms step_avg:98.54ms
step:1602/1750 train_time:157866ms step_avg:98.54ms
step:1603/1750 train_time:157969ms step_avg:98.55ms
step:1604/1750 train_time:158072ms step_avg:98.55ms
step:1605/1750 train_time:158176ms step_avg:98.55ms
step:1606/1750 train_time:158280ms step_avg:98.56ms
step:1607/1750 train_time:158382ms step_avg:98.56ms
step:1608/1750 train_time:158485ms step_avg:98.56ms
step:1609/1750 train_time:158587ms step_avg:98.56ms
step:1610/1750 train_time:158691ms step_avg:98.57ms
step:1611/1750 train_time:158796ms step_avg:98.57ms
step:1612/1750 train_time:158899ms step_avg:98.57ms
step:1613/1750 train_time:159002ms step_avg:98.58ms
step:1614/1750 train_time:159104ms step_avg:98.58ms
step:1615/1750 train_time:159206ms step_avg:98.58ms
step:1616/1750 train_time:159309ms step_avg:98.58ms
step:1617/1750 train_time:159413ms step_avg:98.59ms
step:1618/1750 train_time:159517ms step_avg:98.59ms
step:1619/1750 train_time:159620ms step_avg:98.59ms
step:1620/1750 train_time:159724ms step_avg:98.60ms
step:1621/1750 train_time:159826ms step_avg:98.60ms
step:1622/1750 train_time:159929ms step_avg:98.60ms
step:1623/1750 train_time:160032ms step_avg:98.60ms
step:1624/1750 train_time:160136ms step_avg:98.61ms
step:1625/1750 train_time:160240ms step_avg:98.61ms
step:1625/1750 val_loss:3.2979 train_time:160338ms step_avg:98.67ms
step:1626/1750 train_time:160364ms step_avg:98.62ms
step:1627/1750 train_time:160458ms step_avg:98.62ms
step:1628/1750 train_time:160561ms step_avg:98.62ms
step:1629/1750 train_time:160665ms step_avg:98.63ms
step:1630/1750 train_time:160770ms step_avg:98.63ms
step:1631/1750 train_time:160872ms step_avg:98.63ms
step:1632/1750 train_time:160975ms step_avg:98.64ms
step:1633/1750 train_time:161077ms step_avg:98.64ms
step:1634/1750 train_time:161181ms step_avg:98.64ms
step:1635/1750 train_time:161283ms step_avg:98.64ms
step:1636/1750 train_time:161389ms step_avg:98.65ms
step:1637/1750 train_time:161493ms step_avg:98.65ms
step:1638/1750 train_time:161596ms step_avg:98.65ms
step:1639/1750 train_time:161700ms step_avg:98.66ms
step:1640/1750 train_time:161803ms step_avg:98.66ms
step:1641/1750 train_time:161906ms step_avg:98.66ms
step:1642/1750 train_time:162010ms step_avg:98.67ms
step:1643/1750 train_time:162112ms step_avg:98.67ms
step:1644/1750 train_time:162214ms step_avg:98.67ms
step:1645/1750 train_time:162317ms step_avg:98.67ms
step:1646/1750 train_time:162421ms step_avg:98.68ms
step:1647/1750 train_time:162525ms step_avg:98.68ms
step:1648/1750 train_time:162628ms step_avg:98.68ms
step:1649/1750 train_time:162732ms step_avg:98.69ms
step:1650/1750 train_time:162836ms step_avg:98.69ms
step:1651/1750 train_time:162939ms step_avg:98.69ms
step:1652/1750 train_time:163043ms step_avg:98.69ms
step:1653/1750 train_time:163146ms step_avg:98.70ms
step:1654/1750 train_time:163249ms step_avg:98.70ms
step:1655/1750 train_time:163353ms step_avg:98.70ms
step:1656/1750 train_time:163457ms step_avg:98.71ms
step:1657/1750 train_time:163559ms step_avg:98.71ms
step:1658/1750 train_time:163663ms step_avg:98.71ms
step:1659/1750 train_time:163770ms step_avg:98.72ms
step:1660/1750 train_time:163873ms step_avg:98.72ms
step:1661/1750 train_time:163978ms step_avg:98.72ms
step:1662/1750 train_time:164081ms step_avg:98.73ms
step:1663/1750 train_time:164184ms step_avg:98.73ms
step:1664/1750 train_time:164287ms step_avg:98.73ms
step:1665/1750 train_time:164391ms step_avg:98.73ms
step:1666/1750 train_time:164495ms step_avg:98.74ms
step:1667/1750 train_time:164598ms step_avg:98.74ms
step:1668/1750 train_time:164702ms step_avg:98.74ms
step:1669/1750 train_time:164806ms step_avg:98.75ms
step:1670/1750 train_time:164910ms step_avg:98.75ms
step:1671/1750 train_time:165012ms step_avg:98.75ms
step:1672/1750 train_time:165116ms step_avg:98.75ms
step:1673/1750 train_time:165220ms step_avg:98.76ms
step:1674/1750 train_time:165322ms step_avg:98.76ms
step:1675/1750 train_time:165426ms step_avg:98.76ms
step:1676/1750 train_time:165530ms step_avg:98.76ms
step:1677/1750 train_time:165632ms step_avg:98.77ms
step:1678/1750 train_time:165736ms step_avg:98.77ms
step:1679/1750 train_time:165841ms step_avg:98.77ms
step:1680/1750 train_time:165942ms step_avg:98.78ms
step:1681/1750 train_time:166046ms step_avg:98.78ms
step:1682/1750 train_time:166150ms step_avg:98.78ms
step:1683/1750 train_time:166253ms step_avg:98.78ms
step:1684/1750 train_time:166357ms step_avg:98.79ms
step:1685/1750 train_time:166461ms step_avg:98.79ms
step:1686/1750 train_time:166564ms step_avg:98.79ms
step:1687/1750 train_time:166666ms step_avg:98.79ms
step:1688/1750 train_time:166770ms step_avg:98.80ms
step:1689/1750 train_time:166874ms step_avg:98.80ms
step:1690/1750 train_time:166977ms step_avg:98.80ms
step:1691/1750 train_time:167081ms step_avg:98.81ms
step:1692/1750 train_time:167184ms step_avg:98.81ms
step:1693/1750 train_time:167287ms step_avg:98.81ms
step:1694/1750 train_time:167392ms step_avg:98.81ms
step:1695/1750 train_time:167498ms step_avg:98.82ms
step:1696/1750 train_time:167601ms step_avg:98.82ms
step:1697/1750 train_time:167708ms step_avg:98.83ms
step:1698/1750 train_time:167810ms step_avg:98.83ms
step:1699/1750 train_time:167914ms step_avg:98.83ms
step:1700/1750 train_time:168019ms step_avg:98.83ms
step:1701/1750 train_time:168122ms step_avg:98.84ms
step:1702/1750 train_time:168227ms step_avg:98.84ms
step:1703/1750 train_time:168331ms step_avg:98.84ms
step:1704/1750 train_time:168435ms step_avg:98.85ms
step:1705/1750 train_time:168538ms step_avg:98.85ms
step:1706/1750 train_time:168642ms step_avg:98.85ms
step:1707/1750 train_time:168746ms step_avg:98.86ms
step:1708/1750 train_time:168851ms step_avg:98.86ms
step:1709/1750 train_time:168956ms step_avg:98.86ms
step:1710/1750 train_time:169060ms step_avg:98.87ms
step:1711/1750 train_time:169165ms step_avg:98.87ms
step:1712/1750 train_time:169269ms step_avg:98.87ms
step:1713/1750 train_time:169373ms step_avg:98.88ms
step:1714/1750 train_time:169476ms step_avg:98.88ms
step:1715/1750 train_time:169582ms step_avg:98.88ms
step:1716/1750 train_time:169686ms step_avg:98.88ms
step:1717/1750 train_time:169789ms step_avg:98.89ms
step:1718/1750 train_time:169893ms step_avg:98.89ms
step:1719/1750 train_time:169999ms step_avg:98.89ms
step:1720/1750 train_time:170102ms step_avg:98.90ms
step:1721/1750 train_time:170207ms step_avg:98.90ms
step:1722/1750 train_time:170312ms step_avg:98.90ms
step:1723/1750 train_time:170415ms step_avg:98.91ms
step:1724/1750 train_time:170521ms step_avg:98.91ms
step:1725/1750 train_time:170625ms step_avg:98.91ms
step:1726/1750 train_time:170729ms step_avg:98.92ms
step:1727/1750 train_time:170833ms step_avg:98.92ms
step:1728/1750 train_time:170940ms step_avg:98.92ms
step:1729/1750 train_time:171044ms step_avg:98.93ms
step:1730/1750 train_time:171147ms step_avg:98.93ms
step:1731/1750 train_time:171253ms step_avg:98.93ms
step:1732/1750 train_time:171357ms step_avg:98.94ms
step:1733/1750 train_time:171462ms step_avg:98.94ms
step:1734/1750 train_time:171567ms step_avg:98.94ms
step:1735/1750 train_time:171670ms step_avg:98.95ms
step:1736/1750 train_time:171774ms step_avg:98.95ms
step:1737/1750 train_time:171880ms step_avg:98.95ms
step:1738/1750 train_time:171983ms step_avg:98.95ms
step:1739/1750 train_time:172087ms step_avg:98.96ms
step:1740/1750 train_time:172192ms step_avg:98.96ms
step:1741/1750 train_time:172301ms step_avg:98.97ms
step:1742/1750 train_time:172405ms step_avg:98.97ms
step:1743/1750 train_time:172510ms step_avg:98.97ms
step:1744/1750 train_time:172615ms step_avg:98.98ms
step:1745/1750 train_time:172718ms step_avg:98.98ms
step:1746/1750 train_time:172822ms step_avg:98.98ms
step:1747/1750 train_time:172925ms step_avg:98.98ms
step:1748/1750 train_time:173031ms step_avg:98.99ms
step:1749/1750 train_time:173134ms step_avg:98.99ms
step:1750/1750 train_time:173239ms step_avg:98.99ms
step:1750/1750 val_loss:3.2773 train_time:173338ms step_avg:99.05ms
peak memory allocated: 33277 MiB reserved: 48652 MiB

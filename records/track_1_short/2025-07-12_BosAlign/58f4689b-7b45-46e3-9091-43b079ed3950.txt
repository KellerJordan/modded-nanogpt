import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 01:29:58 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            117W /  700W |    5856MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   39C    P0            119W /  700W |    1517MiB /  81559MiB |      2%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           83675      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           83676      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           83677      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           83678      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           83679      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           83680      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           83681      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           83683      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           83676      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           83677      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           83678      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           83679      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           83680      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           83681      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           83683      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:150ms step_avg:150.00ms
step:2/1750 train_time:176ms step_avg:88.00ms
step:3/1750 train_time:248ms step_avg:82.64ms
step:4/1750 train_time:339ms step_avg:84.86ms
step:5/1750 train_time:432ms step_avg:86.45ms
step:6/1750 train_time:526ms step_avg:87.64ms
step:7/1750 train_time:617ms step_avg:88.17ms
step:8/1750 train_time:710ms step_avg:88.74ms
step:9/1750 train_time:802ms step_avg:89.15ms
step:10/1750 train_time:895ms step_avg:89.48ms
step:11/1750 train_time:987ms step_avg:89.75ms
step:12/1750 train_time:1081ms step_avg:90.05ms
step:13/1750 train_time:1176ms step_avg:90.43ms
step:14/1750 train_time:1270ms step_avg:90.71ms
step:15/1750 train_time:1364ms step_avg:90.91ms
step:16/1750 train_time:1457ms step_avg:91.03ms
step:17/1750 train_time:1550ms step_avg:91.18ms
step:18/1750 train_time:1643ms step_avg:91.29ms
step:19/1750 train_time:1737ms step_avg:91.41ms
step:20/1750 train_time:1829ms step_avg:91.46ms
step:21/1750 train_time:1922ms step_avg:91.51ms
step:22/1750 train_time:2015ms step_avg:91.60ms
step:23/1750 train_time:2109ms step_avg:91.70ms
step:24/1750 train_time:2203ms step_avg:91.79ms
step:25/1750 train_time:2297ms step_avg:91.86ms
step:26/1750 train_time:2390ms step_avg:91.91ms
step:27/1750 train_time:2483ms step_avg:91.95ms
step:28/1750 train_time:2577ms step_avg:92.02ms
step:29/1750 train_time:2669ms step_avg:92.03ms
step:30/1750 train_time:2762ms step_avg:92.06ms
step:31/1750 train_time:2854ms step_avg:92.07ms
step:32/1750 train_time:2948ms step_avg:92.12ms
step:33/1750 train_time:3042ms step_avg:92.18ms
step:34/1750 train_time:3136ms step_avg:92.22ms
step:35/1750 train_time:3230ms step_avg:92.27ms
step:36/1750 train_time:3323ms step_avg:92.31ms
step:37/1750 train_time:3416ms step_avg:92.34ms
step:38/1750 train_time:3510ms step_avg:92.36ms
step:39/1750 train_time:3603ms step_avg:92.38ms
step:40/1750 train_time:3696ms step_avg:92.39ms
step:41/1750 train_time:3789ms step_avg:92.41ms
step:42/1750 train_time:3882ms step_avg:92.43ms
step:43/1750 train_time:3975ms step_avg:92.45ms
step:44/1750 train_time:4069ms step_avg:92.48ms
step:45/1750 train_time:4162ms step_avg:92.49ms
step:46/1750 train_time:4256ms step_avg:92.52ms
step:47/1750 train_time:4350ms step_avg:92.55ms
step:48/1750 train_time:4444ms step_avg:92.58ms
step:49/1750 train_time:4537ms step_avg:92.59ms
step:50/1750 train_time:4630ms step_avg:92.59ms
step:51/1750 train_time:4722ms step_avg:92.60ms
step:52/1750 train_time:4816ms step_avg:92.61ms
step:53/1750 train_time:4909ms step_avg:92.62ms
step:54/1750 train_time:5002ms step_avg:92.63ms
step:55/1750 train_time:5095ms step_avg:92.63ms
step:56/1750 train_time:5188ms step_avg:92.65ms
step:57/1750 train_time:5282ms step_avg:92.66ms
step:58/1750 train_time:5376ms step_avg:92.68ms
step:59/1750 train_time:5469ms step_avg:92.69ms
step:60/1750 train_time:5562ms step_avg:92.69ms
step:61/1750 train_time:5655ms step_avg:92.71ms
step:62/1750 train_time:5748ms step_avg:92.71ms
step:63/1750 train_time:5841ms step_avg:92.71ms
step:64/1750 train_time:5934ms step_avg:92.73ms
step:65/1750 train_time:6029ms step_avg:92.75ms
step:66/1750 train_time:6121ms step_avg:92.74ms
step:67/1750 train_time:6214ms step_avg:92.75ms
step:68/1750 train_time:6308ms step_avg:92.76ms
step:69/1750 train_time:6402ms step_avg:92.78ms
step:70/1750 train_time:6495ms step_avg:92.78ms
step:71/1750 train_time:6588ms step_avg:92.78ms
step:72/1750 train_time:6681ms step_avg:92.79ms
step:73/1750 train_time:6773ms step_avg:92.79ms
step:74/1750 train_time:6867ms step_avg:92.80ms
step:75/1750 train_time:6959ms step_avg:92.79ms
step:76/1750 train_time:7054ms step_avg:92.82ms
step:77/1750 train_time:7146ms step_avg:92.81ms
step:78/1750 train_time:7240ms step_avg:92.82ms
step:79/1750 train_time:7333ms step_avg:92.82ms
step:80/1750 train_time:7429ms step_avg:92.86ms
step:81/1750 train_time:7522ms step_avg:92.86ms
step:82/1750 train_time:7614ms step_avg:92.85ms
step:83/1750 train_time:7707ms step_avg:92.86ms
step:84/1750 train_time:7801ms step_avg:92.86ms
step:85/1750 train_time:7894ms step_avg:92.87ms
step:86/1750 train_time:7987ms step_avg:92.87ms
step:87/1750 train_time:8081ms step_avg:92.88ms
step:88/1750 train_time:8174ms step_avg:92.88ms
step:89/1750 train_time:8266ms step_avg:92.88ms
step:90/1750 train_time:8360ms step_avg:92.89ms
step:91/1750 train_time:8453ms step_avg:92.89ms
step:92/1750 train_time:8546ms step_avg:92.89ms
step:93/1750 train_time:8640ms step_avg:92.90ms
step:94/1750 train_time:8733ms step_avg:92.90ms
step:95/1750 train_time:8826ms step_avg:92.91ms
step:96/1750 train_time:8920ms step_avg:92.92ms
step:97/1750 train_time:9013ms step_avg:92.92ms
step:98/1750 train_time:9106ms step_avg:92.92ms
step:99/1750 train_time:9200ms step_avg:92.93ms
step:100/1750 train_time:9293ms step_avg:92.93ms
step:101/1750 train_time:9386ms step_avg:92.93ms
step:102/1750 train_time:9479ms step_avg:92.93ms
step:103/1750 train_time:9573ms step_avg:92.94ms
step:104/1750 train_time:9666ms step_avg:92.95ms
step:105/1750 train_time:9760ms step_avg:92.95ms
step:106/1750 train_time:9853ms step_avg:92.95ms
step:107/1750 train_time:9946ms step_avg:92.95ms
step:108/1750 train_time:10040ms step_avg:92.96ms
step:109/1750 train_time:10133ms step_avg:92.96ms
step:110/1750 train_time:10227ms step_avg:92.97ms
step:111/1750 train_time:10320ms step_avg:92.97ms
step:112/1750 train_time:10413ms step_avg:92.97ms
step:113/1750 train_time:10506ms step_avg:92.97ms
step:114/1750 train_time:10599ms step_avg:92.98ms
step:115/1750 train_time:10692ms step_avg:92.97ms
step:116/1750 train_time:10785ms step_avg:92.97ms
step:117/1750 train_time:10878ms step_avg:92.97ms
step:118/1750 train_time:10971ms step_avg:92.98ms
step:119/1750 train_time:11065ms step_avg:92.98ms
step:120/1750 train_time:11157ms step_avg:92.98ms
step:121/1750 train_time:11250ms step_avg:92.98ms
step:122/1750 train_time:11344ms step_avg:92.99ms
step:123/1750 train_time:11437ms step_avg:92.98ms
step:124/1750 train_time:11531ms step_avg:92.99ms
step:125/1750 train_time:11623ms step_avg:92.99ms
step:125/1750 val_loss:4.6361 train_time:11711ms step_avg:93.69ms
step:126/1750 train_time:11739ms step_avg:93.16ms
step:127/1750 train_time:11815ms step_avg:93.03ms
step:128/1750 train_time:11914ms step_avg:93.08ms
step:129/1750 train_time:12012ms step_avg:93.12ms
step:130/1750 train_time:12105ms step_avg:93.12ms
step:131/1750 train_time:12198ms step_avg:93.11ms
step:132/1750 train_time:12290ms step_avg:93.11ms
step:133/1750 train_time:12384ms step_avg:93.11ms
step:134/1750 train_time:12477ms step_avg:93.11ms
step:135/1750 train_time:12569ms step_avg:93.11ms
step:136/1750 train_time:12662ms step_avg:93.11ms
step:137/1750 train_time:12756ms step_avg:93.11ms
step:138/1750 train_time:12851ms step_avg:93.12ms
step:139/1750 train_time:12947ms step_avg:93.15ms
step:140/1750 train_time:13042ms step_avg:93.16ms
step:141/1750 train_time:13136ms step_avg:93.16ms
step:142/1750 train_time:13229ms step_avg:93.16ms
step:143/1750 train_time:13323ms step_avg:93.17ms
step:144/1750 train_time:13416ms step_avg:93.17ms
step:145/1750 train_time:13510ms step_avg:93.17ms
step:146/1750 train_time:13603ms step_avg:93.17ms
step:147/1750 train_time:13696ms step_avg:93.17ms
step:148/1750 train_time:13790ms step_avg:93.18ms
step:149/1750 train_time:13885ms step_avg:93.19ms
step:150/1750 train_time:13979ms step_avg:93.19ms
step:151/1750 train_time:14073ms step_avg:93.20ms
step:152/1750 train_time:14167ms step_avg:93.20ms
step:153/1750 train_time:14260ms step_avg:93.20ms
step:154/1750 train_time:14354ms step_avg:93.21ms
step:155/1750 train_time:14448ms step_avg:93.21ms
step:156/1750 train_time:14541ms step_avg:93.21ms
step:157/1750 train_time:14634ms step_avg:93.21ms
step:158/1750 train_time:14729ms step_avg:93.22ms
step:159/1750 train_time:14823ms step_avg:93.23ms
step:160/1750 train_time:14917ms step_avg:93.23ms
step:161/1750 train_time:15011ms step_avg:93.23ms
step:162/1750 train_time:15105ms step_avg:93.24ms
step:163/1750 train_time:15198ms step_avg:93.24ms
step:164/1750 train_time:15292ms step_avg:93.24ms
step:165/1750 train_time:15385ms step_avg:93.25ms
step:166/1750 train_time:15479ms step_avg:93.25ms
step:167/1750 train_time:15573ms step_avg:93.25ms
step:168/1750 train_time:15667ms step_avg:93.26ms
step:169/1750 train_time:15761ms step_avg:93.26ms
step:170/1750 train_time:15854ms step_avg:93.26ms
step:171/1750 train_time:15949ms step_avg:93.27ms
step:172/1750 train_time:16044ms step_avg:93.28ms
step:173/1750 train_time:16137ms step_avg:93.28ms
step:174/1750 train_time:16231ms step_avg:93.28ms
step:175/1750 train_time:16325ms step_avg:93.29ms
step:176/1750 train_time:16418ms step_avg:93.29ms
step:177/1750 train_time:16512ms step_avg:93.29ms
step:178/1750 train_time:16605ms step_avg:93.29ms
step:179/1750 train_time:16700ms step_avg:93.30ms
step:180/1750 train_time:16792ms step_avg:93.29ms
step:181/1750 train_time:16886ms step_avg:93.29ms
step:182/1750 train_time:16980ms step_avg:93.30ms
step:183/1750 train_time:17074ms step_avg:93.30ms
step:184/1750 train_time:17168ms step_avg:93.30ms
step:185/1750 train_time:17262ms step_avg:93.31ms
step:186/1750 train_time:17355ms step_avg:93.30ms
step:187/1750 train_time:17449ms step_avg:93.31ms
step:188/1750 train_time:17542ms step_avg:93.31ms
step:189/1750 train_time:17636ms step_avg:93.31ms
step:190/1750 train_time:17729ms step_avg:93.31ms
step:191/1750 train_time:17823ms step_avg:93.32ms
step:192/1750 train_time:17917ms step_avg:93.32ms
step:193/1750 train_time:18010ms step_avg:93.32ms
step:194/1750 train_time:18104ms step_avg:93.32ms
step:195/1750 train_time:18199ms step_avg:93.33ms
step:196/1750 train_time:18292ms step_avg:93.33ms
step:197/1750 train_time:18385ms step_avg:93.33ms
step:198/1750 train_time:18479ms step_avg:93.33ms
step:199/1750 train_time:18572ms step_avg:93.33ms
step:200/1750 train_time:18666ms step_avg:93.33ms
step:201/1750 train_time:18761ms step_avg:93.34ms
step:202/1750 train_time:18854ms step_avg:93.34ms
step:203/1750 train_time:18948ms step_avg:93.34ms
step:204/1750 train_time:19043ms step_avg:93.35ms
step:205/1750 train_time:19137ms step_avg:93.35ms
step:206/1750 train_time:19231ms step_avg:93.36ms
step:207/1750 train_time:19325ms step_avg:93.36ms
step:208/1750 train_time:19419ms step_avg:93.36ms
step:209/1750 train_time:19512ms step_avg:93.36ms
step:210/1750 train_time:19606ms step_avg:93.36ms
step:211/1750 train_time:19699ms step_avg:93.36ms
step:212/1750 train_time:19792ms step_avg:93.36ms
step:213/1750 train_time:19886ms step_avg:93.36ms
step:214/1750 train_time:19979ms step_avg:93.36ms
step:215/1750 train_time:20074ms step_avg:93.37ms
step:216/1750 train_time:20167ms step_avg:93.37ms
step:217/1750 train_time:20261ms step_avg:93.37ms
step:218/1750 train_time:20355ms step_avg:93.37ms
step:219/1750 train_time:20449ms step_avg:93.37ms
step:220/1750 train_time:20543ms step_avg:93.38ms
step:221/1750 train_time:20637ms step_avg:93.38ms
step:222/1750 train_time:20731ms step_avg:93.38ms
step:223/1750 train_time:20824ms step_avg:93.38ms
step:224/1750 train_time:20918ms step_avg:93.38ms
step:225/1750 train_time:21011ms step_avg:93.38ms
step:226/1750 train_time:21105ms step_avg:93.38ms
step:227/1750 train_time:21198ms step_avg:93.38ms
step:228/1750 train_time:21292ms step_avg:93.39ms
step:229/1750 train_time:21386ms step_avg:93.39ms
step:230/1750 train_time:21479ms step_avg:93.39ms
step:231/1750 train_time:21574ms step_avg:93.39ms
step:232/1750 train_time:21667ms step_avg:93.39ms
step:233/1750 train_time:21760ms step_avg:93.39ms
step:234/1750 train_time:21856ms step_avg:93.40ms
step:235/1750 train_time:21948ms step_avg:93.40ms
step:236/1750 train_time:22042ms step_avg:93.40ms
step:237/1750 train_time:22136ms step_avg:93.40ms
step:238/1750 train_time:22229ms step_avg:93.40ms
step:239/1750 train_time:22324ms step_avg:93.41ms
step:240/1750 train_time:22418ms step_avg:93.41ms
step:241/1750 train_time:22511ms step_avg:93.41ms
step:242/1750 train_time:22605ms step_avg:93.41ms
step:243/1750 train_time:22698ms step_avg:93.41ms
step:244/1750 train_time:22792ms step_avg:93.41ms
step:245/1750 train_time:22885ms step_avg:93.41ms
step:246/1750 train_time:22978ms step_avg:93.41ms
step:247/1750 train_time:23073ms step_avg:93.41ms
step:248/1750 train_time:23165ms step_avg:93.41ms
step:249/1750 train_time:23259ms step_avg:93.41ms
step:250/1750 train_time:23352ms step_avg:93.41ms
step:250/1750 val_loss:4.0896 train_time:23441ms step_avg:93.76ms
step:251/1750 train_time:23470ms step_avg:93.51ms
step:252/1750 train_time:23546ms step_avg:93.44ms
step:253/1750 train_time:23642ms step_avg:93.45ms
step:254/1750 train_time:23736ms step_avg:93.45ms
step:255/1750 train_time:23830ms step_avg:93.45ms
step:256/1750 train_time:23922ms step_avg:93.45ms
step:257/1750 train_time:24015ms step_avg:93.44ms
step:258/1750 train_time:24107ms step_avg:93.44ms
step:259/1750 train_time:24201ms step_avg:93.44ms
step:260/1750 train_time:24293ms step_avg:93.43ms
step:261/1750 train_time:24386ms step_avg:93.43ms
step:262/1750 train_time:24481ms step_avg:93.44ms
step:263/1750 train_time:24577ms step_avg:93.45ms
step:264/1750 train_time:24671ms step_avg:93.45ms
step:265/1750 train_time:24766ms step_avg:93.46ms
step:266/1750 train_time:24860ms step_avg:93.46ms
step:267/1750 train_time:24954ms step_avg:93.46ms
step:268/1750 train_time:25048ms step_avg:93.46ms
step:269/1750 train_time:25142ms step_avg:93.47ms
step:270/1750 train_time:25235ms step_avg:93.46ms
step:271/1750 train_time:25330ms step_avg:93.47ms
step:272/1750 train_time:25424ms step_avg:93.47ms
step:273/1750 train_time:25518ms step_avg:93.47ms
step:274/1750 train_time:25614ms step_avg:93.48ms
step:275/1750 train_time:25708ms step_avg:93.48ms
step:276/1750 train_time:25803ms step_avg:93.49ms
step:277/1750 train_time:25897ms step_avg:93.49ms
step:278/1750 train_time:25991ms step_avg:93.49ms
step:279/1750 train_time:26087ms step_avg:93.50ms
step:280/1750 train_time:26180ms step_avg:93.50ms
step:281/1750 train_time:26274ms step_avg:93.50ms
step:282/1750 train_time:26367ms step_avg:93.50ms
step:283/1750 train_time:26461ms step_avg:93.50ms
step:284/1750 train_time:26556ms step_avg:93.51ms
step:285/1750 train_time:26651ms step_avg:93.51ms
step:286/1750 train_time:26747ms step_avg:93.52ms
step:287/1750 train_time:26841ms step_avg:93.52ms
step:288/1750 train_time:26935ms step_avg:93.53ms
step:289/1750 train_time:27030ms step_avg:93.53ms
step:290/1750 train_time:27125ms step_avg:93.53ms
step:291/1750 train_time:27219ms step_avg:93.54ms
step:292/1750 train_time:27313ms step_avg:93.54ms
step:293/1750 train_time:27407ms step_avg:93.54ms
step:294/1750 train_time:27501ms step_avg:93.54ms
step:295/1750 train_time:27595ms step_avg:93.54ms
step:296/1750 train_time:27691ms step_avg:93.55ms
step:297/1750 train_time:27786ms step_avg:93.56ms
step:298/1750 train_time:27881ms step_avg:93.56ms
step:299/1750 train_time:27974ms step_avg:93.56ms
step:300/1750 train_time:28068ms step_avg:93.56ms
step:301/1750 train_time:28163ms step_avg:93.56ms
step:302/1750 train_time:28257ms step_avg:93.57ms
step:303/1750 train_time:28351ms step_avg:93.57ms
step:304/1750 train_time:28445ms step_avg:93.57ms
step:305/1750 train_time:28539ms step_avg:93.57ms
step:306/1750 train_time:28633ms step_avg:93.57ms
step:307/1750 train_time:28727ms step_avg:93.57ms
step:308/1750 train_time:28822ms step_avg:93.58ms
step:309/1750 train_time:28916ms step_avg:93.58ms
step:310/1750 train_time:29010ms step_avg:93.58ms
step:311/1750 train_time:29105ms step_avg:93.58ms
step:312/1750 train_time:29200ms step_avg:93.59ms
step:313/1750 train_time:29293ms step_avg:93.59ms
step:314/1750 train_time:29388ms step_avg:93.59ms
step:315/1750 train_time:29483ms step_avg:93.60ms
step:316/1750 train_time:29577ms step_avg:93.60ms
step:317/1750 train_time:29672ms step_avg:93.60ms
step:318/1750 train_time:29766ms step_avg:93.60ms
step:319/1750 train_time:29860ms step_avg:93.61ms
step:320/1750 train_time:29955ms step_avg:93.61ms
step:321/1750 train_time:30049ms step_avg:93.61ms
step:322/1750 train_time:30143ms step_avg:93.61ms
step:323/1750 train_time:30237ms step_avg:93.61ms
step:324/1750 train_time:30331ms step_avg:93.61ms
step:325/1750 train_time:30426ms step_avg:93.62ms
step:326/1750 train_time:30520ms step_avg:93.62ms
step:327/1750 train_time:30614ms step_avg:93.62ms
step:328/1750 train_time:30709ms step_avg:93.62ms
step:329/1750 train_time:30802ms step_avg:93.62ms
step:330/1750 train_time:30897ms step_avg:93.63ms
step:331/1750 train_time:30991ms step_avg:93.63ms
step:332/1750 train_time:31086ms step_avg:93.63ms
step:333/1750 train_time:31183ms step_avg:93.64ms
step:334/1750 train_time:31276ms step_avg:93.64ms
step:335/1750 train_time:31370ms step_avg:93.64ms
step:336/1750 train_time:31465ms step_avg:93.64ms
step:337/1750 train_time:31559ms step_avg:93.65ms
step:338/1750 train_time:31654ms step_avg:93.65ms
step:339/1750 train_time:31748ms step_avg:93.65ms
step:340/1750 train_time:31842ms step_avg:93.65ms
step:341/1750 train_time:31937ms step_avg:93.66ms
step:342/1750 train_time:32031ms step_avg:93.66ms
step:343/1750 train_time:32125ms step_avg:93.66ms
step:344/1750 train_time:32220ms step_avg:93.66ms
step:345/1750 train_time:32314ms step_avg:93.66ms
step:346/1750 train_time:32408ms step_avg:93.67ms
step:347/1750 train_time:32503ms step_avg:93.67ms
step:348/1750 train_time:32597ms step_avg:93.67ms
step:349/1750 train_time:32691ms step_avg:93.67ms
step:350/1750 train_time:32786ms step_avg:93.67ms
step:351/1750 train_time:32880ms step_avg:93.68ms
step:352/1750 train_time:32975ms step_avg:93.68ms
step:353/1750 train_time:33069ms step_avg:93.68ms
step:354/1750 train_time:33163ms step_avg:93.68ms
step:355/1750 train_time:33257ms step_avg:93.68ms
step:356/1750 train_time:33351ms step_avg:93.68ms
step:357/1750 train_time:33446ms step_avg:93.69ms
step:358/1750 train_time:33540ms step_avg:93.69ms
step:359/1750 train_time:33635ms step_avg:93.69ms
step:360/1750 train_time:33730ms step_avg:93.69ms
step:361/1750 train_time:33824ms step_avg:93.70ms
step:362/1750 train_time:33918ms step_avg:93.70ms
step:363/1750 train_time:34012ms step_avg:93.70ms
step:364/1750 train_time:34108ms step_avg:93.70ms
step:365/1750 train_time:34203ms step_avg:93.71ms
step:366/1750 train_time:34295ms step_avg:93.70ms
step:367/1750 train_time:34390ms step_avg:93.71ms
step:368/1750 train_time:34485ms step_avg:93.71ms
step:369/1750 train_time:34580ms step_avg:93.71ms
step:370/1750 train_time:34674ms step_avg:93.71ms
step:371/1750 train_time:34769ms step_avg:93.72ms
step:372/1750 train_time:34864ms step_avg:93.72ms
step:373/1750 train_time:34958ms step_avg:93.72ms
step:374/1750 train_time:35052ms step_avg:93.72ms
step:375/1750 train_time:35146ms step_avg:93.72ms
step:375/1750 val_loss:3.8919 train_time:35235ms step_avg:93.96ms
step:376/1750 train_time:35263ms step_avg:93.78ms
step:377/1750 train_time:35343ms step_avg:93.75ms
step:378/1750 train_time:35445ms step_avg:93.77ms
step:379/1750 train_time:35537ms step_avg:93.77ms
step:380/1750 train_time:35631ms step_avg:93.77ms
step:381/1750 train_time:35724ms step_avg:93.76ms
step:382/1750 train_time:35818ms step_avg:93.76ms
step:383/1750 train_time:35911ms step_avg:93.76ms
step:384/1750 train_time:36004ms step_avg:93.76ms
step:385/1750 train_time:36098ms step_avg:93.76ms
step:386/1750 train_time:36191ms step_avg:93.76ms
step:387/1750 train_time:36286ms step_avg:93.76ms
step:388/1750 train_time:36382ms step_avg:93.77ms
step:389/1750 train_time:36478ms step_avg:93.77ms
step:390/1750 train_time:36574ms step_avg:93.78ms
step:391/1750 train_time:36670ms step_avg:93.79ms
step:392/1750 train_time:36767ms step_avg:93.79ms
step:393/1750 train_time:36862ms step_avg:93.80ms
step:394/1750 train_time:36958ms step_avg:93.80ms
step:395/1750 train_time:37053ms step_avg:93.81ms
step:396/1750 train_time:37149ms step_avg:93.81ms
step:397/1750 train_time:37245ms step_avg:93.82ms
step:398/1750 train_time:37342ms step_avg:93.82ms
step:399/1750 train_time:37439ms step_avg:93.83ms
step:400/1750 train_time:37536ms step_avg:93.84ms
step:401/1750 train_time:37633ms step_avg:93.85ms
step:402/1750 train_time:37729ms step_avg:93.85ms
step:403/1750 train_time:37825ms step_avg:93.86ms
step:404/1750 train_time:37922ms step_avg:93.87ms
step:405/1750 train_time:38017ms step_avg:93.87ms
step:406/1750 train_time:38114ms step_avg:93.88ms
step:407/1750 train_time:38210ms step_avg:93.88ms
step:408/1750 train_time:38307ms step_avg:93.89ms
step:409/1750 train_time:38405ms step_avg:93.90ms
step:410/1750 train_time:38502ms step_avg:93.91ms
step:411/1750 train_time:38598ms step_avg:93.91ms
step:412/1750 train_time:38695ms step_avg:93.92ms
step:413/1750 train_time:38791ms step_avg:93.93ms
step:414/1750 train_time:38888ms step_avg:93.93ms
step:415/1750 train_time:38984ms step_avg:93.94ms
step:416/1750 train_time:39081ms step_avg:93.94ms
step:417/1750 train_time:39178ms step_avg:93.95ms
step:418/1750 train_time:39274ms step_avg:93.96ms
step:419/1750 train_time:39371ms step_avg:93.96ms
step:420/1750 train_time:39468ms step_avg:93.97ms
step:421/1750 train_time:39565ms step_avg:93.98ms
step:422/1750 train_time:39662ms step_avg:93.99ms
step:423/1750 train_time:39759ms step_avg:93.99ms
step:424/1750 train_time:39856ms step_avg:94.00ms
step:425/1750 train_time:39952ms step_avg:94.01ms
step:426/1750 train_time:40049ms step_avg:94.01ms
step:427/1750 train_time:40145ms step_avg:94.02ms
step:428/1750 train_time:40241ms step_avg:94.02ms
step:429/1750 train_time:40339ms step_avg:94.03ms
step:430/1750 train_time:40434ms step_avg:94.03ms
step:431/1750 train_time:40533ms step_avg:94.04ms
step:432/1750 train_time:40628ms step_avg:94.05ms
step:433/1750 train_time:40723ms step_avg:94.05ms
step:434/1750 train_time:40820ms step_avg:94.05ms
step:435/1750 train_time:40917ms step_avg:94.06ms
step:436/1750 train_time:41014ms step_avg:94.07ms
step:437/1750 train_time:41110ms step_avg:94.07ms
step:438/1750 train_time:41207ms step_avg:94.08ms
step:439/1750 train_time:41304ms step_avg:94.09ms
step:440/1750 train_time:41400ms step_avg:94.09ms
step:441/1750 train_time:41497ms step_avg:94.10ms
step:442/1750 train_time:41593ms step_avg:94.10ms
step:443/1750 train_time:41690ms step_avg:94.11ms
step:444/1750 train_time:41786ms step_avg:94.11ms
step:445/1750 train_time:41883ms step_avg:94.12ms
step:446/1750 train_time:41980ms step_avg:94.13ms
step:447/1750 train_time:42077ms step_avg:94.13ms
step:448/1750 train_time:42174ms step_avg:94.14ms
step:449/1750 train_time:42270ms step_avg:94.14ms
step:450/1750 train_time:42367ms step_avg:94.15ms
step:451/1750 train_time:42463ms step_avg:94.15ms
step:452/1750 train_time:42559ms step_avg:94.16ms
step:453/1750 train_time:42655ms step_avg:94.16ms
step:454/1750 train_time:42752ms step_avg:94.17ms
step:455/1750 train_time:42848ms step_avg:94.17ms
step:456/1750 train_time:42946ms step_avg:94.18ms
step:457/1750 train_time:43042ms step_avg:94.18ms
step:458/1750 train_time:43139ms step_avg:94.19ms
step:459/1750 train_time:43235ms step_avg:94.19ms
step:460/1750 train_time:43332ms step_avg:94.20ms
step:461/1750 train_time:43428ms step_avg:94.20ms
step:462/1750 train_time:43524ms step_avg:94.21ms
step:463/1750 train_time:43621ms step_avg:94.21ms
step:464/1750 train_time:43717ms step_avg:94.22ms
step:465/1750 train_time:43814ms step_avg:94.22ms
step:466/1750 train_time:43911ms step_avg:94.23ms
step:467/1750 train_time:44007ms step_avg:94.23ms
step:468/1750 train_time:44104ms step_avg:94.24ms
step:469/1750 train_time:44200ms step_avg:94.24ms
step:470/1750 train_time:44296ms step_avg:94.25ms
step:471/1750 train_time:44393ms step_avg:94.25ms
step:472/1750 train_time:44489ms step_avg:94.26ms
step:473/1750 train_time:44585ms step_avg:94.26ms
step:474/1750 train_time:44681ms step_avg:94.26ms
step:475/1750 train_time:44777ms step_avg:94.27ms
step:476/1750 train_time:44875ms step_avg:94.28ms
step:477/1750 train_time:44970ms step_avg:94.28ms
step:478/1750 train_time:45067ms step_avg:94.28ms
step:479/1750 train_time:45164ms step_avg:94.29ms
step:480/1750 train_time:45260ms step_avg:94.29ms
step:481/1750 train_time:45357ms step_avg:94.30ms
step:482/1750 train_time:45454ms step_avg:94.30ms
step:483/1750 train_time:45551ms step_avg:94.31ms
step:484/1750 train_time:45648ms step_avg:94.32ms
step:485/1750 train_time:45745ms step_avg:94.32ms
step:486/1750 train_time:45841ms step_avg:94.32ms
step:487/1750 train_time:45937ms step_avg:94.33ms
step:488/1750 train_time:46034ms step_avg:94.33ms
step:489/1750 train_time:46131ms step_avg:94.34ms
step:490/1750 train_time:46226ms step_avg:94.34ms
step:491/1750 train_time:46323ms step_avg:94.34ms
step:492/1750 train_time:46419ms step_avg:94.35ms
step:493/1750 train_time:46516ms step_avg:94.35ms
step:494/1750 train_time:46613ms step_avg:94.36ms
step:495/1750 train_time:46710ms step_avg:94.36ms
step:496/1750 train_time:46807ms step_avg:94.37ms
step:497/1750 train_time:46903ms step_avg:94.37ms
step:498/1750 train_time:46999ms step_avg:94.38ms
step:499/1750 train_time:47096ms step_avg:94.38ms
step:500/1750 train_time:47192ms step_avg:94.38ms
step:500/1750 val_loss:3.7476 train_time:47283ms step_avg:94.57ms
step:501/1750 train_time:47310ms step_avg:94.43ms
step:502/1750 train_time:47395ms step_avg:94.41ms
step:503/1750 train_time:47496ms step_avg:94.42ms
step:504/1750 train_time:47592ms step_avg:94.43ms
step:505/1750 train_time:47688ms step_avg:94.43ms
step:506/1750 train_time:47784ms step_avg:94.44ms
step:507/1750 train_time:47881ms step_avg:94.44ms
step:508/1750 train_time:47977ms step_avg:94.44ms
step:509/1750 train_time:48072ms step_avg:94.44ms
step:510/1750 train_time:48168ms step_avg:94.45ms
step:511/1750 train_time:48264ms step_avg:94.45ms
step:512/1750 train_time:48362ms step_avg:94.46ms
step:513/1750 train_time:48461ms step_avg:94.47ms
step:514/1750 train_time:48558ms step_avg:94.47ms
step:515/1750 train_time:48656ms step_avg:94.48ms
step:516/1750 train_time:48753ms step_avg:94.48ms
step:517/1750 train_time:48849ms step_avg:94.49ms
step:518/1750 train_time:48944ms step_avg:94.49ms
step:519/1750 train_time:49040ms step_avg:94.49ms
step:520/1750 train_time:49136ms step_avg:94.49ms
step:521/1750 train_time:49232ms step_avg:94.50ms
step:522/1750 train_time:49328ms step_avg:94.50ms
step:523/1750 train_time:49426ms step_avg:94.50ms
step:524/1750 train_time:49524ms step_avg:94.51ms
step:525/1750 train_time:49622ms step_avg:94.52ms
step:526/1750 train_time:49720ms step_avg:94.52ms
step:527/1750 train_time:49817ms step_avg:94.53ms
step:528/1750 train_time:49915ms step_avg:94.54ms
step:529/1750 train_time:50009ms step_avg:94.54ms
step:530/1750 train_time:50106ms step_avg:94.54ms
step:531/1750 train_time:50202ms step_avg:94.54ms
step:532/1750 train_time:50299ms step_avg:94.55ms
step:533/1750 train_time:50396ms step_avg:94.55ms
step:534/1750 train_time:50493ms step_avg:94.56ms
step:535/1750 train_time:50591ms step_avg:94.56ms
step:536/1750 train_time:50689ms step_avg:94.57ms
step:537/1750 train_time:50785ms step_avg:94.57ms
step:538/1750 train_time:50882ms step_avg:94.58ms
step:539/1750 train_time:50979ms step_avg:94.58ms
step:540/1750 train_time:51076ms step_avg:94.59ms
step:541/1750 train_time:51173ms step_avg:94.59ms
step:542/1750 train_time:51269ms step_avg:94.59ms
step:543/1750 train_time:51366ms step_avg:94.60ms
step:544/1750 train_time:51464ms step_avg:94.60ms
step:545/1750 train_time:51560ms step_avg:94.61ms
step:546/1750 train_time:51658ms step_avg:94.61ms
step:547/1750 train_time:51755ms step_avg:94.62ms
step:548/1750 train_time:51853ms step_avg:94.62ms
step:549/1750 train_time:51949ms step_avg:94.62ms
step:550/1750 train_time:52046ms step_avg:94.63ms
step:551/1750 train_time:52142ms step_avg:94.63ms
step:552/1750 train_time:52239ms step_avg:94.64ms
step:553/1750 train_time:52336ms step_avg:94.64ms
step:554/1750 train_time:52433ms step_avg:94.64ms
step:555/1750 train_time:52529ms step_avg:94.65ms
step:556/1750 train_time:52627ms step_avg:94.65ms
step:557/1750 train_time:52724ms step_avg:94.66ms
step:558/1750 train_time:52821ms step_avg:94.66ms
step:559/1750 train_time:52919ms step_avg:94.67ms
step:560/1750 train_time:53017ms step_avg:94.67ms
step:561/1750 train_time:53114ms step_avg:94.68ms
step:562/1750 train_time:53210ms step_avg:94.68ms
step:563/1750 train_time:53307ms step_avg:94.68ms
step:564/1750 train_time:53404ms step_avg:94.69ms
step:565/1750 train_time:53501ms step_avg:94.69ms
step:566/1750 train_time:53597ms step_avg:94.70ms
step:567/1750 train_time:53694ms step_avg:94.70ms
step:568/1750 train_time:53792ms step_avg:94.70ms
step:569/1750 train_time:53889ms step_avg:94.71ms
step:570/1750 train_time:53986ms step_avg:94.71ms
step:571/1750 train_time:54083ms step_avg:94.72ms
step:572/1750 train_time:54178ms step_avg:94.72ms
step:573/1750 train_time:54275ms step_avg:94.72ms
step:574/1750 train_time:54371ms step_avg:94.72ms
step:575/1750 train_time:54468ms step_avg:94.73ms
step:576/1750 train_time:54565ms step_avg:94.73ms
step:577/1750 train_time:54663ms step_avg:94.74ms
step:578/1750 train_time:54759ms step_avg:94.74ms
step:579/1750 train_time:54857ms step_avg:94.74ms
step:580/1750 train_time:54953ms step_avg:94.75ms
step:581/1750 train_time:55051ms step_avg:94.75ms
step:582/1750 train_time:55148ms step_avg:94.76ms
step:583/1750 train_time:55244ms step_avg:94.76ms
step:584/1750 train_time:55341ms step_avg:94.76ms
step:585/1750 train_time:55438ms step_avg:94.77ms
step:586/1750 train_time:55534ms step_avg:94.77ms
step:587/1750 train_time:55631ms step_avg:94.77ms
step:588/1750 train_time:55728ms step_avg:94.78ms
step:589/1750 train_time:55825ms step_avg:94.78ms
step:590/1750 train_time:55923ms step_avg:94.78ms
step:591/1750 train_time:56019ms step_avg:94.79ms
step:592/1750 train_time:56117ms step_avg:94.79ms
step:593/1750 train_time:56215ms step_avg:94.80ms
step:594/1750 train_time:56312ms step_avg:94.80ms
step:595/1750 train_time:56408ms step_avg:94.80ms
step:596/1750 train_time:56504ms step_avg:94.81ms
step:597/1750 train_time:56601ms step_avg:94.81ms
step:598/1750 train_time:56699ms step_avg:94.81ms
step:599/1750 train_time:56796ms step_avg:94.82ms
step:600/1750 train_time:56893ms step_avg:94.82ms
step:601/1750 train_time:56990ms step_avg:94.82ms
step:602/1750 train_time:57087ms step_avg:94.83ms
step:603/1750 train_time:57184ms step_avg:94.83ms
step:604/1750 train_time:57281ms step_avg:94.84ms
step:605/1750 train_time:57378ms step_avg:94.84ms
step:606/1750 train_time:57475ms step_avg:94.84ms
step:607/1750 train_time:57572ms step_avg:94.85ms
step:608/1750 train_time:57669ms step_avg:94.85ms
step:609/1750 train_time:57765ms step_avg:94.85ms
step:610/1750 train_time:57861ms step_avg:94.85ms
step:611/1750 train_time:57959ms step_avg:94.86ms
step:612/1750 train_time:58056ms step_avg:94.86ms
step:613/1750 train_time:58154ms step_avg:94.87ms
step:614/1750 train_time:58251ms step_avg:94.87ms
step:615/1750 train_time:58348ms step_avg:94.87ms
step:616/1750 train_time:58444ms step_avg:94.88ms
step:617/1750 train_time:58541ms step_avg:94.88ms
step:618/1750 train_time:58639ms step_avg:94.88ms
step:619/1750 train_time:58736ms step_avg:94.89ms
step:620/1750 train_time:58832ms step_avg:94.89ms
step:621/1750 train_time:58930ms step_avg:94.90ms
step:622/1750 train_time:59026ms step_avg:94.90ms
step:623/1750 train_time:59123ms step_avg:94.90ms
step:624/1750 train_time:59219ms step_avg:94.90ms
step:625/1750 train_time:59317ms step_avg:94.91ms
step:625/1750 val_loss:3.6577 train_time:59409ms step_avg:95.05ms
step:626/1750 train_time:59436ms step_avg:94.95ms
step:627/1750 train_time:59521ms step_avg:94.93ms
step:628/1750 train_time:59618ms step_avg:94.93ms
step:629/1750 train_time:59716ms step_avg:94.94ms
step:630/1750 train_time:59812ms step_avg:94.94ms
step:631/1750 train_time:59908ms step_avg:94.94ms
step:632/1750 train_time:60004ms step_avg:94.94ms
step:633/1750 train_time:60100ms step_avg:94.95ms
step:634/1750 train_time:60196ms step_avg:94.95ms
step:635/1750 train_time:60292ms step_avg:94.95ms
step:636/1750 train_time:60389ms step_avg:94.95ms
step:637/1750 train_time:60487ms step_avg:94.96ms
step:638/1750 train_time:60584ms step_avg:94.96ms
step:639/1750 train_time:60681ms step_avg:94.96ms
step:640/1750 train_time:60778ms step_avg:94.97ms
step:641/1750 train_time:60874ms step_avg:94.97ms
step:642/1750 train_time:60971ms step_avg:94.97ms
step:643/1750 train_time:61068ms step_avg:94.97ms
step:644/1750 train_time:61165ms step_avg:94.98ms
step:645/1750 train_time:61262ms step_avg:94.98ms
step:646/1750 train_time:61358ms step_avg:94.98ms
step:647/1750 train_time:61455ms step_avg:94.98ms
step:648/1750 train_time:61552ms step_avg:94.99ms
step:649/1750 train_time:61650ms step_avg:94.99ms
step:650/1750 train_time:61748ms step_avg:95.00ms
step:651/1750 train_time:61847ms step_avg:95.00ms
step:652/1750 train_time:61946ms step_avg:95.01ms
step:653/1750 train_time:62044ms step_avg:95.01ms
step:654/1750 train_time:62142ms step_avg:95.02ms
step:655/1750 train_time:62240ms step_avg:95.02ms
step:656/1750 train_time:62338ms step_avg:95.03ms
step:657/1750 train_time:62436ms step_avg:95.03ms
step:658/1750 train_time:62535ms step_avg:95.04ms
step:659/1750 train_time:62634ms step_avg:95.04ms
step:660/1750 train_time:62732ms step_avg:95.05ms
step:661/1750 train_time:62831ms step_avg:95.05ms
step:662/1750 train_time:62930ms step_avg:95.06ms
step:663/1750 train_time:63028ms step_avg:95.07ms
step:664/1750 train_time:63128ms step_avg:95.07ms
step:665/1750 train_time:63225ms step_avg:95.08ms
step:666/1750 train_time:63324ms step_avg:95.08ms
step:667/1750 train_time:63422ms step_avg:95.09ms
step:668/1750 train_time:63522ms step_avg:95.09ms
step:669/1750 train_time:63620ms step_avg:95.10ms
step:670/1750 train_time:63718ms step_avg:95.10ms
step:671/1750 train_time:63817ms step_avg:95.11ms
step:672/1750 train_time:63915ms step_avg:95.11ms
step:673/1750 train_time:64014ms step_avg:95.12ms
step:674/1750 train_time:64113ms step_avg:95.12ms
step:675/1750 train_time:64211ms step_avg:95.13ms
step:676/1750 train_time:64310ms step_avg:95.13ms
step:677/1750 train_time:64410ms step_avg:95.14ms
step:678/1750 train_time:64509ms step_avg:95.15ms
step:679/1750 train_time:64608ms step_avg:95.15ms
step:680/1750 train_time:64707ms step_avg:95.16ms
step:681/1750 train_time:64807ms step_avg:95.16ms
step:682/1750 train_time:64907ms step_avg:95.17ms
step:683/1750 train_time:65007ms step_avg:95.18ms
step:684/1750 train_time:65105ms step_avg:95.18ms
step:685/1750 train_time:65203ms step_avg:95.19ms
step:686/1750 train_time:65301ms step_avg:95.19ms
step:687/1750 train_time:65399ms step_avg:95.19ms
step:688/1750 train_time:65497ms step_avg:95.20ms
step:689/1750 train_time:65595ms step_avg:95.20ms
step:690/1750 train_time:65694ms step_avg:95.21ms
step:691/1750 train_time:65792ms step_avg:95.21ms
step:692/1750 train_time:65891ms step_avg:95.22ms
step:693/1750 train_time:65989ms step_avg:95.22ms
step:694/1750 train_time:66088ms step_avg:95.23ms
step:695/1750 train_time:66186ms step_avg:95.23ms
step:696/1750 train_time:66284ms step_avg:95.24ms
step:697/1750 train_time:66383ms step_avg:95.24ms
step:698/1750 train_time:66481ms step_avg:95.25ms
step:699/1750 train_time:66581ms step_avg:95.25ms
step:700/1750 train_time:66679ms step_avg:95.26ms
step:701/1750 train_time:66777ms step_avg:95.26ms
step:702/1750 train_time:66876ms step_avg:95.26ms
step:703/1750 train_time:66975ms step_avg:95.27ms
step:704/1750 train_time:67075ms step_avg:95.28ms
step:705/1750 train_time:67174ms step_avg:95.28ms
step:706/1750 train_time:67273ms step_avg:95.29ms
step:707/1750 train_time:67372ms step_avg:95.29ms
step:708/1750 train_time:67471ms step_avg:95.30ms
step:709/1750 train_time:67571ms step_avg:95.30ms
step:710/1750 train_time:67671ms step_avg:95.31ms
step:711/1750 train_time:67770ms step_avg:95.32ms
step:712/1750 train_time:67869ms step_avg:95.32ms
step:713/1750 train_time:67968ms step_avg:95.33ms
step:714/1750 train_time:68068ms step_avg:95.33ms
step:715/1750 train_time:68167ms step_avg:95.34ms
step:716/1750 train_time:68265ms step_avg:95.34ms
step:717/1750 train_time:68364ms step_avg:95.35ms
step:718/1750 train_time:68462ms step_avg:95.35ms
step:719/1750 train_time:68561ms step_avg:95.36ms
step:720/1750 train_time:68660ms step_avg:95.36ms
step:721/1750 train_time:68758ms step_avg:95.37ms
step:722/1750 train_time:68857ms step_avg:95.37ms
step:723/1750 train_time:68955ms step_avg:95.37ms
step:724/1750 train_time:69053ms step_avg:95.38ms
step:725/1750 train_time:69151ms step_avg:95.38ms
step:726/1750 train_time:69250ms step_avg:95.39ms
step:727/1750 train_time:69348ms step_avg:95.39ms
step:728/1750 train_time:69446ms step_avg:95.39ms
step:729/1750 train_time:69547ms step_avg:95.40ms
step:730/1750 train_time:69645ms step_avg:95.40ms
step:731/1750 train_time:69743ms step_avg:95.41ms
step:732/1750 train_time:69842ms step_avg:95.41ms
step:733/1750 train_time:69941ms step_avg:95.42ms
step:734/1750 train_time:70040ms step_avg:95.42ms
step:735/1750 train_time:70138ms step_avg:95.43ms
step:736/1750 train_time:70237ms step_avg:95.43ms
step:737/1750 train_time:70335ms step_avg:95.43ms
step:738/1750 train_time:70434ms step_avg:95.44ms
step:739/1750 train_time:70532ms step_avg:95.44ms
step:740/1750 train_time:70631ms step_avg:95.45ms
step:741/1750 train_time:70731ms step_avg:95.45ms
step:742/1750 train_time:70831ms step_avg:95.46ms
step:743/1750 train_time:70930ms step_avg:95.46ms
step:744/1750 train_time:71029ms step_avg:95.47ms
step:745/1750 train_time:71128ms step_avg:95.47ms
step:746/1750 train_time:71227ms step_avg:95.48ms
step:747/1750 train_time:71327ms step_avg:95.48ms
step:748/1750 train_time:71426ms step_avg:95.49ms
step:749/1750 train_time:71524ms step_avg:95.49ms
step:750/1750 train_time:71622ms step_avg:95.50ms
step:750/1750 val_loss:3.5963 train_time:71715ms step_avg:95.62ms
step:751/1750 train_time:71742ms step_avg:95.53ms
step:752/1750 train_time:71828ms step_avg:95.52ms
step:753/1750 train_time:71928ms step_avg:95.52ms
step:754/1750 train_time:72026ms step_avg:95.53ms
step:755/1750 train_time:72124ms step_avg:95.53ms
step:756/1750 train_time:72222ms step_avg:95.53ms
step:757/1750 train_time:72319ms step_avg:95.53ms
step:758/1750 train_time:72417ms step_avg:95.54ms
step:759/1750 train_time:72515ms step_avg:95.54ms
step:760/1750 train_time:72613ms step_avg:95.54ms
step:761/1750 train_time:72711ms step_avg:95.55ms
step:762/1750 train_time:72811ms step_avg:95.55ms
step:763/1750 train_time:72911ms step_avg:95.56ms
step:764/1750 train_time:73010ms step_avg:95.56ms
step:765/1750 train_time:73108ms step_avg:95.57ms
step:766/1750 train_time:73206ms step_avg:95.57ms
step:767/1750 train_time:73304ms step_avg:95.57ms
step:768/1750 train_time:73403ms step_avg:95.58ms
step:769/1750 train_time:73501ms step_avg:95.58ms
step:770/1750 train_time:73599ms step_avg:95.58ms
step:771/1750 train_time:73699ms step_avg:95.59ms
step:772/1750 train_time:73796ms step_avg:95.59ms
step:773/1750 train_time:73895ms step_avg:95.60ms
step:774/1750 train_time:73994ms step_avg:95.60ms
step:775/1750 train_time:74094ms step_avg:95.60ms
step:776/1750 train_time:74192ms step_avg:95.61ms
step:777/1750 train_time:74291ms step_avg:95.61ms
step:778/1750 train_time:74389ms step_avg:95.62ms
step:779/1750 train_time:74488ms step_avg:95.62ms
step:780/1750 train_time:74587ms step_avg:95.62ms
step:781/1750 train_time:74685ms step_avg:95.63ms
step:782/1750 train_time:74785ms step_avg:95.63ms
step:783/1750 train_time:74884ms step_avg:95.64ms
step:784/1750 train_time:74984ms step_avg:95.64ms
step:785/1750 train_time:75083ms step_avg:95.65ms
step:786/1750 train_time:75183ms step_avg:95.65ms
step:787/1750 train_time:75282ms step_avg:95.66ms
step:788/1750 train_time:75381ms step_avg:95.66ms
step:789/1750 train_time:75480ms step_avg:95.67ms
step:790/1750 train_time:75579ms step_avg:95.67ms
step:791/1750 train_time:75677ms step_avg:95.67ms
step:792/1750 train_time:75776ms step_avg:95.68ms
step:793/1750 train_time:75874ms step_avg:95.68ms
step:794/1750 train_time:75973ms step_avg:95.68ms
step:795/1750 train_time:76072ms step_avg:95.69ms
step:796/1750 train_time:76171ms step_avg:95.69ms
step:797/1750 train_time:76270ms step_avg:95.70ms
step:798/1750 train_time:76369ms step_avg:95.70ms
step:799/1750 train_time:76467ms step_avg:95.70ms
step:800/1750 train_time:76566ms step_avg:95.71ms
step:801/1750 train_time:76666ms step_avg:95.71ms
step:802/1750 train_time:76766ms step_avg:95.72ms
step:803/1750 train_time:76865ms step_avg:95.72ms
step:804/1750 train_time:76964ms step_avg:95.73ms
step:805/1750 train_time:77064ms step_avg:95.73ms
step:806/1750 train_time:77163ms step_avg:95.74ms
step:807/1750 train_time:77263ms step_avg:95.74ms
step:808/1750 train_time:77362ms step_avg:95.75ms
step:809/1750 train_time:77462ms step_avg:95.75ms
step:810/1750 train_time:77561ms step_avg:95.75ms
step:811/1750 train_time:77659ms step_avg:95.76ms
step:812/1750 train_time:77758ms step_avg:95.76ms
step:813/1750 train_time:77857ms step_avg:95.77ms
step:814/1750 train_time:77956ms step_avg:95.77ms
step:815/1750 train_time:78056ms step_avg:95.77ms
step:816/1750 train_time:78156ms step_avg:95.78ms
step:817/1750 train_time:78256ms step_avg:95.78ms
step:818/1750 train_time:78355ms step_avg:95.79ms
step:819/1750 train_time:78454ms step_avg:95.79ms
step:820/1750 train_time:78553ms step_avg:95.80ms
step:821/1750 train_time:78652ms step_avg:95.80ms
step:822/1750 train_time:78751ms step_avg:95.80ms
step:823/1750 train_time:78851ms step_avg:95.81ms
step:824/1750 train_time:78948ms step_avg:95.81ms
step:825/1750 train_time:79047ms step_avg:95.81ms
step:826/1750 train_time:79146ms step_avg:95.82ms
step:827/1750 train_time:79246ms step_avg:95.82ms
step:828/1750 train_time:79345ms step_avg:95.83ms
step:829/1750 train_time:79445ms step_avg:95.83ms
step:830/1750 train_time:79545ms step_avg:95.84ms
step:831/1750 train_time:79644ms step_avg:95.84ms
step:832/1750 train_time:79743ms step_avg:95.84ms
step:833/1750 train_time:79842ms step_avg:95.85ms
step:834/1750 train_time:79940ms step_avg:95.85ms
step:835/1750 train_time:80039ms step_avg:95.86ms
step:836/1750 train_time:80138ms step_avg:95.86ms
step:837/1750 train_time:80237ms step_avg:95.86ms
step:838/1750 train_time:80335ms step_avg:95.87ms
step:839/1750 train_time:80434ms step_avg:95.87ms
step:840/1750 train_time:80533ms step_avg:95.87ms
step:841/1750 train_time:80631ms step_avg:95.88ms
step:842/1750 train_time:80729ms step_avg:95.88ms
step:843/1750 train_time:80828ms step_avg:95.88ms
step:844/1750 train_time:80926ms step_avg:95.88ms
step:845/1750 train_time:81026ms step_avg:95.89ms
step:846/1750 train_time:81125ms step_avg:95.89ms
step:847/1750 train_time:81223ms step_avg:95.90ms
step:848/1750 train_time:81322ms step_avg:95.90ms
step:849/1750 train_time:81421ms step_avg:95.90ms
step:850/1750 train_time:81521ms step_avg:95.91ms
step:851/1750 train_time:81621ms step_avg:95.91ms
step:852/1750 train_time:81720ms step_avg:95.92ms
step:853/1750 train_time:81818ms step_avg:95.92ms
step:854/1750 train_time:81917ms step_avg:95.92ms
step:855/1750 train_time:82015ms step_avg:95.92ms
step:856/1750 train_time:82114ms step_avg:95.93ms
step:857/1750 train_time:82213ms step_avg:95.93ms
step:858/1750 train_time:82311ms step_avg:95.93ms
step:859/1750 train_time:82409ms step_avg:95.94ms
step:860/1750 train_time:82508ms step_avg:95.94ms
step:861/1750 train_time:82608ms step_avg:95.94ms
step:862/1750 train_time:82707ms step_avg:95.95ms
step:863/1750 train_time:82807ms step_avg:95.95ms
step:864/1750 train_time:82906ms step_avg:95.96ms
step:865/1750 train_time:83006ms step_avg:95.96ms
step:866/1750 train_time:83105ms step_avg:95.96ms
step:867/1750 train_time:83204ms step_avg:95.97ms
step:868/1750 train_time:83303ms step_avg:95.97ms
step:869/1750 train_time:83403ms step_avg:95.98ms
step:870/1750 train_time:83501ms step_avg:95.98ms
step:871/1750 train_time:83600ms step_avg:95.98ms
step:872/1750 train_time:83700ms step_avg:95.99ms
step:873/1750 train_time:83798ms step_avg:95.99ms
step:874/1750 train_time:83896ms step_avg:95.99ms
step:875/1750 train_time:83995ms step_avg:95.99ms
step:875/1750 val_loss:3.5446 train_time:84088ms step_avg:96.10ms
step:876/1750 train_time:84115ms step_avg:96.02ms
step:877/1750 train_time:84202ms step_avg:96.01ms
step:878/1750 train_time:84301ms step_avg:96.01ms
step:879/1750 train_time:84399ms step_avg:96.02ms
step:880/1750 train_time:84498ms step_avg:96.02ms
step:881/1750 train_time:84597ms step_avg:96.02ms
step:882/1750 train_time:84695ms step_avg:96.03ms
step:883/1750 train_time:84794ms step_avg:96.03ms
step:884/1750 train_time:84892ms step_avg:96.03ms
step:885/1750 train_time:84990ms step_avg:96.03ms
step:886/1750 train_time:85091ms step_avg:96.04ms
step:887/1750 train_time:85191ms step_avg:96.04ms
step:888/1750 train_time:85293ms step_avg:96.05ms
step:889/1750 train_time:85392ms step_avg:96.05ms
step:890/1750 train_time:85492ms step_avg:96.06ms
step:891/1750 train_time:85591ms step_avg:96.06ms
step:892/1750 train_time:85690ms step_avg:96.07ms
step:893/1750 train_time:85790ms step_avg:96.07ms
step:894/1750 train_time:85887ms step_avg:96.07ms
step:895/1750 train_time:85986ms step_avg:96.07ms
step:896/1750 train_time:86086ms step_avg:96.08ms
step:897/1750 train_time:86186ms step_avg:96.08ms
step:898/1750 train_time:86285ms step_avg:96.09ms
step:899/1750 train_time:86384ms step_avg:96.09ms
step:900/1750 train_time:86484ms step_avg:96.09ms
step:901/1750 train_time:86583ms step_avg:96.10ms
step:902/1750 train_time:86682ms step_avg:96.10ms
step:903/1750 train_time:86781ms step_avg:96.10ms
step:904/1750 train_time:86880ms step_avg:96.11ms
step:905/1750 train_time:86979ms step_avg:96.11ms
step:906/1750 train_time:87077ms step_avg:96.11ms
step:907/1750 train_time:87176ms step_avg:96.12ms
step:908/1750 train_time:87275ms step_avg:96.12ms
step:909/1750 train_time:87375ms step_avg:96.12ms
step:910/1750 train_time:87477ms step_avg:96.13ms
step:911/1750 train_time:87577ms step_avg:96.13ms
step:912/1750 train_time:87677ms step_avg:96.14ms
step:913/1750 train_time:87778ms step_avg:96.14ms
step:914/1750 train_time:87878ms step_avg:96.15ms
step:915/1750 train_time:87978ms step_avg:96.15ms
step:916/1750 train_time:88077ms step_avg:96.15ms
step:917/1750 train_time:88180ms step_avg:96.16ms
step:918/1750 train_time:88279ms step_avg:96.16ms
step:919/1750 train_time:88380ms step_avg:96.17ms
step:920/1750 train_time:88481ms step_avg:96.17ms
step:921/1750 train_time:88581ms step_avg:96.18ms
step:922/1750 train_time:88681ms step_avg:96.18ms
step:923/1750 train_time:88781ms step_avg:96.19ms
step:924/1750 train_time:88882ms step_avg:96.19ms
step:925/1750 train_time:88981ms step_avg:96.20ms
step:926/1750 train_time:89082ms step_avg:96.20ms
step:927/1750 train_time:89182ms step_avg:96.20ms
step:928/1750 train_time:89281ms step_avg:96.21ms
step:929/1750 train_time:89381ms step_avg:96.21ms
step:930/1750 train_time:89481ms step_avg:96.22ms
step:931/1750 train_time:89581ms step_avg:96.22ms
step:932/1750 train_time:89682ms step_avg:96.22ms
step:933/1750 train_time:89782ms step_avg:96.23ms
step:934/1750 train_time:89882ms step_avg:96.23ms
step:935/1750 train_time:89982ms step_avg:96.24ms
step:936/1750 train_time:90083ms step_avg:96.24ms
step:937/1750 train_time:90183ms step_avg:96.25ms
step:938/1750 train_time:90283ms step_avg:96.25ms
step:939/1750 train_time:90384ms step_avg:96.26ms
step:940/1750 train_time:90484ms step_avg:96.26ms
step:941/1750 train_time:90585ms step_avg:96.27ms
step:942/1750 train_time:90685ms step_avg:96.27ms
step:943/1750 train_time:90786ms step_avg:96.27ms
step:944/1750 train_time:90885ms step_avg:96.28ms
step:945/1750 train_time:90986ms step_avg:96.28ms
step:946/1750 train_time:91087ms step_avg:96.29ms
step:947/1750 train_time:91187ms step_avg:96.29ms
step:948/1750 train_time:91288ms step_avg:96.29ms
step:949/1750 train_time:91388ms step_avg:96.30ms
step:950/1750 train_time:91489ms step_avg:96.30ms
step:951/1750 train_time:91589ms step_avg:96.31ms
step:952/1750 train_time:91690ms step_avg:96.31ms
step:953/1750 train_time:91792ms step_avg:96.32ms
step:954/1750 train_time:91892ms step_avg:96.32ms
step:955/1750 train_time:91993ms step_avg:96.33ms
step:956/1750 train_time:92093ms step_avg:96.33ms
step:957/1750 train_time:92194ms step_avg:96.34ms
step:958/1750 train_time:92295ms step_avg:96.34ms
step:959/1750 train_time:92394ms step_avg:96.34ms
step:960/1750 train_time:92495ms step_avg:96.35ms
step:961/1750 train_time:92596ms step_avg:96.35ms
step:962/1750 train_time:92697ms step_avg:96.36ms
step:963/1750 train_time:92799ms step_avg:96.36ms
step:964/1750 train_time:92898ms step_avg:96.37ms
step:965/1750 train_time:92999ms step_avg:96.37ms
step:966/1750 train_time:93099ms step_avg:96.38ms
step:967/1750 train_time:93200ms step_avg:96.38ms
step:968/1750 train_time:93302ms step_avg:96.39ms
step:969/1750 train_time:93401ms step_avg:96.39ms
step:970/1750 train_time:93502ms step_avg:96.39ms
step:971/1750 train_time:93602ms step_avg:96.40ms
step:972/1750 train_time:93702ms step_avg:96.40ms
step:973/1750 train_time:93803ms step_avg:96.41ms
step:974/1750 train_time:93903ms step_avg:96.41ms
step:975/1750 train_time:94003ms step_avg:96.41ms
step:976/1750 train_time:94103ms step_avg:96.42ms
step:977/1750 train_time:94204ms step_avg:96.42ms
step:978/1750 train_time:94304ms step_avg:96.43ms
step:979/1750 train_time:94405ms step_avg:96.43ms
step:980/1750 train_time:94504ms step_avg:96.43ms
step:981/1750 train_time:94605ms step_avg:96.44ms
step:982/1750 train_time:94706ms step_avg:96.44ms
step:983/1750 train_time:94806ms step_avg:96.45ms
step:984/1750 train_time:94906ms step_avg:96.45ms
step:985/1750 train_time:95006ms step_avg:96.45ms
step:986/1750 train_time:95108ms step_avg:96.46ms
step:987/1750 train_time:95209ms step_avg:96.46ms
step:988/1750 train_time:95310ms step_avg:96.47ms
step:989/1750 train_time:95411ms step_avg:96.47ms
step:990/1750 train_time:95511ms step_avg:96.48ms
step:991/1750 train_time:95611ms step_avg:96.48ms
step:992/1750 train_time:95713ms step_avg:96.49ms
step:993/1750 train_time:95814ms step_avg:96.49ms
step:994/1750 train_time:95916ms step_avg:96.49ms
step:995/1750 train_time:96016ms step_avg:96.50ms
step:996/1750 train_time:96116ms step_avg:96.50ms
step:997/1750 train_time:96217ms step_avg:96.51ms
step:998/1750 train_time:96316ms step_avg:96.51ms
step:999/1750 train_time:96416ms step_avg:96.51ms
step:1000/1750 train_time:96517ms step_avg:96.52ms
step:1000/1750 val_loss:3.5044 train_time:96612ms step_avg:96.61ms
step:1001/1750 train_time:96640ms step_avg:96.54ms
step:1002/1750 train_time:96726ms step_avg:96.53ms
step:1003/1750 train_time:96827ms step_avg:96.54ms
step:1004/1750 train_time:96927ms step_avg:96.54ms
step:1005/1750 train_time:97027ms step_avg:96.54ms
step:1006/1750 train_time:97126ms step_avg:96.55ms
step:1007/1750 train_time:97225ms step_avg:96.55ms
step:1008/1750 train_time:97325ms step_avg:96.55ms
step:1009/1750 train_time:97425ms step_avg:96.56ms
step:1010/1750 train_time:97526ms step_avg:96.56ms
step:1011/1750 train_time:97628ms step_avg:96.57ms
step:1012/1750 train_time:97731ms step_avg:96.57ms
step:1013/1750 train_time:97833ms step_avg:96.58ms
step:1014/1750 train_time:97934ms step_avg:96.58ms
step:1015/1750 train_time:98034ms step_avg:96.59ms
step:1016/1750 train_time:98135ms step_avg:96.59ms
step:1017/1750 train_time:98236ms step_avg:96.59ms
step:1018/1750 train_time:98336ms step_avg:96.60ms
step:1019/1750 train_time:98436ms step_avg:96.60ms
step:1020/1750 train_time:98537ms step_avg:96.61ms
step:1021/1750 train_time:98638ms step_avg:96.61ms
step:1022/1750 train_time:98739ms step_avg:96.61ms
step:1023/1750 train_time:98840ms step_avg:96.62ms
step:1024/1750 train_time:98941ms step_avg:96.62ms
step:1025/1750 train_time:99041ms step_avg:96.63ms
step:1026/1750 train_time:99141ms step_avg:96.63ms
step:1027/1750 train_time:99241ms step_avg:96.63ms
step:1028/1750 train_time:99340ms step_avg:96.63ms
step:1029/1750 train_time:99441ms step_avg:96.64ms
step:1030/1750 train_time:99541ms step_avg:96.64ms
step:1031/1750 train_time:99641ms step_avg:96.64ms
step:1032/1750 train_time:99741ms step_avg:96.65ms
step:1033/1750 train_time:99841ms step_avg:96.65ms
step:1034/1750 train_time:99941ms step_avg:96.66ms
step:1035/1750 train_time:100042ms step_avg:96.66ms
step:1036/1750 train_time:100142ms step_avg:96.66ms
step:1037/1750 train_time:100243ms step_avg:96.67ms
step:1038/1750 train_time:100343ms step_avg:96.67ms
step:1039/1750 train_time:100442ms step_avg:96.67ms
step:1040/1750 train_time:100543ms step_avg:96.68ms
step:1041/1750 train_time:100643ms step_avg:96.68ms
step:1042/1750 train_time:100745ms step_avg:96.68ms
step:1043/1750 train_time:100846ms step_avg:96.69ms
step:1044/1750 train_time:100946ms step_avg:96.69ms
step:1045/1750 train_time:101046ms step_avg:96.69ms
step:1046/1750 train_time:101147ms step_avg:96.70ms
step:1047/1750 train_time:101247ms step_avg:96.70ms
step:1048/1750 train_time:101348ms step_avg:96.71ms
step:1049/1750 train_time:101449ms step_avg:96.71ms
step:1050/1750 train_time:101550ms step_avg:96.71ms
step:1051/1750 train_time:101650ms step_avg:96.72ms
step:1052/1750 train_time:101751ms step_avg:96.72ms
step:1053/1750 train_time:101851ms step_avg:96.72ms
step:1054/1750 train_time:101952ms step_avg:96.73ms
step:1055/1750 train_time:102054ms step_avg:96.73ms
step:1056/1750 train_time:102155ms step_avg:96.74ms
step:1057/1750 train_time:102256ms step_avg:96.74ms
step:1058/1750 train_time:102356ms step_avg:96.74ms
step:1059/1750 train_time:102456ms step_avg:96.75ms
step:1060/1750 train_time:102557ms step_avg:96.75ms
step:1061/1750 train_time:102658ms step_avg:96.76ms
step:1062/1750 train_time:102758ms step_avg:96.76ms
step:1063/1750 train_time:102859ms step_avg:96.76ms
step:1064/1750 train_time:102959ms step_avg:96.77ms
step:1065/1750 train_time:103060ms step_avg:96.77ms
step:1066/1750 train_time:103160ms step_avg:96.77ms
step:1067/1750 train_time:103262ms step_avg:96.78ms
step:1068/1750 train_time:103362ms step_avg:96.78ms
step:1069/1750 train_time:103463ms step_avg:96.78ms
step:1070/1750 train_time:103563ms step_avg:96.79ms
step:1071/1750 train_time:103664ms step_avg:96.79ms
step:1072/1750 train_time:103764ms step_avg:96.79ms
step:1073/1750 train_time:103864ms step_avg:96.80ms
step:1074/1750 train_time:103965ms step_avg:96.80ms
step:1075/1750 train_time:104065ms step_avg:96.80ms
step:1076/1750 train_time:104165ms step_avg:96.81ms
step:1077/1750 train_time:104266ms step_avg:96.81ms
step:1078/1750 train_time:104366ms step_avg:96.81ms
step:1079/1750 train_time:104467ms step_avg:96.82ms
step:1080/1750 train_time:104568ms step_avg:96.82ms
step:1081/1750 train_time:104668ms step_avg:96.82ms
step:1082/1750 train_time:104769ms step_avg:96.83ms
step:1083/1750 train_time:104869ms step_avg:96.83ms
step:1084/1750 train_time:104970ms step_avg:96.84ms
step:1085/1750 train_time:105070ms step_avg:96.84ms
step:1086/1750 train_time:105171ms step_avg:96.84ms
step:1087/1750 train_time:105272ms step_avg:96.85ms
step:1088/1750 train_time:105373ms step_avg:96.85ms
step:1089/1750 train_time:105473ms step_avg:96.85ms
step:1090/1750 train_time:105574ms step_avg:96.86ms
step:1091/1750 train_time:105676ms step_avg:96.86ms
step:1092/1750 train_time:105777ms step_avg:96.87ms
step:1093/1750 train_time:105878ms step_avg:96.87ms
step:1094/1750 train_time:105978ms step_avg:96.87ms
step:1095/1750 train_time:106078ms step_avg:96.88ms
step:1096/1750 train_time:106179ms step_avg:96.88ms
step:1097/1750 train_time:106279ms step_avg:96.88ms
step:1098/1750 train_time:106379ms step_avg:96.88ms
step:1099/1750 train_time:106480ms step_avg:96.89ms
step:1100/1750 train_time:106581ms step_avg:96.89ms
step:1101/1750 train_time:106682ms step_avg:96.90ms
step:1102/1750 train_time:106782ms step_avg:96.90ms
step:1103/1750 train_time:106882ms step_avg:96.90ms
step:1104/1750 train_time:106983ms step_avg:96.91ms
step:1105/1750 train_time:107083ms step_avg:96.91ms
step:1106/1750 train_time:107184ms step_avg:96.91ms
step:1107/1750 train_time:107284ms step_avg:96.91ms
step:1108/1750 train_time:107384ms step_avg:96.92ms
step:1109/1750 train_time:107484ms step_avg:96.92ms
step:1110/1750 train_time:107584ms step_avg:96.92ms
step:1111/1750 train_time:107686ms step_avg:96.93ms
step:1112/1750 train_time:107786ms step_avg:96.93ms
step:1113/1750 train_time:107887ms step_avg:96.93ms
step:1114/1750 train_time:107987ms step_avg:96.94ms
step:1115/1750 train_time:108087ms step_avg:96.94ms
step:1116/1750 train_time:108188ms step_avg:96.94ms
step:1117/1750 train_time:108288ms step_avg:96.95ms
step:1118/1750 train_time:108389ms step_avg:96.95ms
step:1119/1750 train_time:108489ms step_avg:96.95ms
step:1120/1750 train_time:108589ms step_avg:96.95ms
step:1121/1750 train_time:108690ms step_avg:96.96ms
step:1122/1750 train_time:108790ms step_avg:96.96ms
step:1123/1750 train_time:108892ms step_avg:96.97ms
step:1124/1750 train_time:108992ms step_avg:96.97ms
step:1125/1750 train_time:109094ms step_avg:96.97ms
step:1125/1750 val_loss:3.4524 train_time:109189ms step_avg:97.06ms
step:1126/1750 train_time:109218ms step_avg:97.00ms
step:1127/1750 train_time:109303ms step_avg:96.99ms
step:1128/1750 train_time:109404ms step_avg:96.99ms
step:1129/1750 train_time:109505ms step_avg:96.99ms
step:1130/1750 train_time:109606ms step_avg:97.00ms
step:1131/1750 train_time:109706ms step_avg:97.00ms
step:1132/1750 train_time:109808ms step_avg:97.00ms
step:1133/1750 train_time:109908ms step_avg:97.01ms
step:1134/1750 train_time:110008ms step_avg:97.01ms
step:1135/1750 train_time:110107ms step_avg:97.01ms
step:1136/1750 train_time:110210ms step_avg:97.02ms
step:1137/1750 train_time:110313ms step_avg:97.02ms
step:1138/1750 train_time:110414ms step_avg:97.02ms
step:1139/1750 train_time:110515ms step_avg:97.03ms
step:1140/1750 train_time:110615ms step_avg:97.03ms
step:1141/1750 train_time:110715ms step_avg:97.03ms
step:1142/1750 train_time:110815ms step_avg:97.04ms
step:1143/1750 train_time:110915ms step_avg:97.04ms
step:1144/1750 train_time:111015ms step_avg:97.04ms
step:1145/1750 train_time:111116ms step_avg:97.04ms
step:1146/1750 train_time:111217ms step_avg:97.05ms
step:1147/1750 train_time:111317ms step_avg:97.05ms
step:1148/1750 train_time:111418ms step_avg:97.05ms
step:1149/1750 train_time:111518ms step_avg:97.06ms
step:1150/1750 train_time:111619ms step_avg:97.06ms
step:1151/1750 train_time:111718ms step_avg:97.06ms
step:1152/1750 train_time:111819ms step_avg:97.07ms
step:1153/1750 train_time:111921ms step_avg:97.07ms
step:1154/1750 train_time:112020ms step_avg:97.07ms
step:1155/1750 train_time:112120ms step_avg:97.07ms
step:1156/1750 train_time:112221ms step_avg:97.08ms
step:1157/1750 train_time:112322ms step_avg:97.08ms
step:1158/1750 train_time:112423ms step_avg:97.08ms
step:1159/1750 train_time:112525ms step_avg:97.09ms
step:1160/1750 train_time:112626ms step_avg:97.09ms
step:1161/1750 train_time:112727ms step_avg:97.09ms
step:1162/1750 train_time:112828ms step_avg:97.10ms
step:1163/1750 train_time:112929ms step_avg:97.10ms
step:1164/1750 train_time:113030ms step_avg:97.10ms
step:1165/1750 train_time:113131ms step_avg:97.11ms
step:1166/1750 train_time:113231ms step_avg:97.11ms
step:1167/1750 train_time:113331ms step_avg:97.11ms
step:1168/1750 train_time:113432ms step_avg:97.12ms
step:1169/1750 train_time:113534ms step_avg:97.12ms
step:1170/1750 train_time:113636ms step_avg:97.12ms
step:1171/1750 train_time:113738ms step_avg:97.13ms
step:1172/1750 train_time:113841ms step_avg:97.13ms
step:1173/1750 train_time:113942ms step_avg:97.14ms
step:1174/1750 train_time:114043ms step_avg:97.14ms
step:1175/1750 train_time:114145ms step_avg:97.14ms
step:1176/1750 train_time:114246ms step_avg:97.15ms
step:1177/1750 train_time:114348ms step_avg:97.15ms
step:1178/1750 train_time:114451ms step_avg:97.16ms
step:1179/1750 train_time:114552ms step_avg:97.16ms
step:1180/1750 train_time:114654ms step_avg:97.16ms
step:1181/1750 train_time:114756ms step_avg:97.17ms
step:1182/1750 train_time:114858ms step_avg:97.17ms
step:1183/1750 train_time:114959ms step_avg:97.18ms
step:1184/1750 train_time:115062ms step_avg:97.18ms
step:1185/1750 train_time:115165ms step_avg:97.19ms
step:1186/1750 train_time:115267ms step_avg:97.19ms
step:1187/1750 train_time:115368ms step_avg:97.19ms
step:1188/1750 train_time:115470ms step_avg:97.20ms
step:1189/1750 train_time:115572ms step_avg:97.20ms
step:1190/1750 train_time:115673ms step_avg:97.20ms
step:1191/1750 train_time:115776ms step_avg:97.21ms
step:1192/1750 train_time:115878ms step_avg:97.21ms
step:1193/1750 train_time:115980ms step_avg:97.22ms
step:1194/1750 train_time:116081ms step_avg:97.22ms
step:1195/1750 train_time:116182ms step_avg:97.22ms
step:1196/1750 train_time:116284ms step_avg:97.23ms
step:1197/1750 train_time:116386ms step_avg:97.23ms
step:1198/1750 train_time:116487ms step_avg:97.23ms
step:1199/1750 train_time:116589ms step_avg:97.24ms
step:1200/1750 train_time:116692ms step_avg:97.24ms
step:1201/1750 train_time:116794ms step_avg:97.25ms
step:1202/1750 train_time:116897ms step_avg:97.25ms
step:1203/1750 train_time:117000ms step_avg:97.26ms
step:1204/1750 train_time:117100ms step_avg:97.26ms
step:1205/1750 train_time:117200ms step_avg:97.26ms
step:1206/1750 train_time:117302ms step_avg:97.27ms
step:1207/1750 train_time:117404ms step_avg:97.27ms
step:1208/1750 train_time:117506ms step_avg:97.27ms
step:1209/1750 train_time:117608ms step_avg:97.28ms
step:1210/1750 train_time:117710ms step_avg:97.28ms
step:1211/1750 train_time:117812ms step_avg:97.28ms
step:1212/1750 train_time:117915ms step_avg:97.29ms
step:1213/1750 train_time:118017ms step_avg:97.29ms
step:1214/1750 train_time:118118ms step_avg:97.30ms
step:1215/1750 train_time:118219ms step_avg:97.30ms
step:1216/1750 train_time:118322ms step_avg:97.30ms
step:1217/1750 train_time:118424ms step_avg:97.31ms
step:1218/1750 train_time:118527ms step_avg:97.31ms
step:1219/1750 train_time:118630ms step_avg:97.32ms
step:1220/1750 train_time:118731ms step_avg:97.32ms
step:1221/1750 train_time:118833ms step_avg:97.32ms
step:1222/1750 train_time:118935ms step_avg:97.33ms
step:1223/1750 train_time:119037ms step_avg:97.33ms
step:1224/1750 train_time:119139ms step_avg:97.34ms
step:1225/1750 train_time:119241ms step_avg:97.34ms
step:1226/1750 train_time:119341ms step_avg:97.34ms
step:1227/1750 train_time:119443ms step_avg:97.35ms
step:1228/1750 train_time:119544ms step_avg:97.35ms
step:1229/1750 train_time:119648ms step_avg:97.35ms
step:1230/1750 train_time:119751ms step_avg:97.36ms
step:1231/1750 train_time:119852ms step_avg:97.36ms
step:1232/1750 train_time:119955ms step_avg:97.37ms
step:1233/1750 train_time:120056ms step_avg:97.37ms
step:1234/1750 train_time:120158ms step_avg:97.37ms
step:1235/1750 train_time:120259ms step_avg:97.38ms
step:1236/1750 train_time:120360ms step_avg:97.38ms
step:1237/1750 train_time:120462ms step_avg:97.38ms
step:1238/1750 train_time:120563ms step_avg:97.39ms
step:1239/1750 train_time:120665ms step_avg:97.39ms
step:1240/1750 train_time:120767ms step_avg:97.39ms
step:1241/1750 train_time:120870ms step_avg:97.40ms
step:1242/1750 train_time:120972ms step_avg:97.40ms
step:1243/1750 train_time:121074ms step_avg:97.40ms
step:1244/1750 train_time:121175ms step_avg:97.41ms
step:1245/1750 train_time:121277ms step_avg:97.41ms
step:1246/1750 train_time:121379ms step_avg:97.41ms
step:1247/1750 train_time:121480ms step_avg:97.42ms
step:1248/1750 train_time:121582ms step_avg:97.42ms
step:1249/1750 train_time:121684ms step_avg:97.42ms
step:1250/1750 train_time:121786ms step_avg:97.43ms
step:1250/1750 val_loss:3.4063 train_time:121883ms step_avg:97.51ms
step:1251/1750 train_time:121911ms step_avg:97.45ms
step:1252/1750 train_time:121997ms step_avg:97.44ms
step:1253/1750 train_time:122100ms step_avg:97.45ms
step:1254/1750 train_time:122202ms step_avg:97.45ms
step:1255/1750 train_time:122303ms step_avg:97.45ms
step:1256/1750 train_time:122404ms step_avg:97.46ms
step:1257/1750 train_time:122505ms step_avg:97.46ms
step:1258/1750 train_time:122606ms step_avg:97.46ms
step:1259/1750 train_time:122706ms step_avg:97.46ms
step:1260/1750 train_time:122808ms step_avg:97.47ms
step:1261/1750 train_time:122911ms step_avg:97.47ms
step:1262/1750 train_time:123015ms step_avg:97.48ms
step:1263/1750 train_time:123117ms step_avg:97.48ms
step:1264/1750 train_time:123218ms step_avg:97.48ms
step:1265/1750 train_time:123320ms step_avg:97.49ms
step:1266/1750 train_time:123420ms step_avg:97.49ms
step:1267/1750 train_time:123522ms step_avg:97.49ms
step:1268/1750 train_time:123623ms step_avg:97.49ms
step:1269/1750 train_time:123725ms step_avg:97.50ms
step:1270/1750 train_time:123826ms step_avg:97.50ms
step:1271/1750 train_time:123929ms step_avg:97.51ms
step:1272/1750 train_time:124031ms step_avg:97.51ms
step:1273/1750 train_time:124135ms step_avg:97.51ms
step:1274/1750 train_time:124238ms step_avg:97.52ms
step:1275/1750 train_time:124338ms step_avg:97.52ms
step:1276/1750 train_time:124441ms step_avg:97.52ms
step:1277/1750 train_time:124541ms step_avg:97.53ms
step:1278/1750 train_time:124642ms step_avg:97.53ms
step:1279/1750 train_time:124744ms step_avg:97.53ms
step:1280/1750 train_time:124846ms step_avg:97.54ms
step:1281/1750 train_time:124948ms step_avg:97.54ms
step:1282/1750 train_time:125051ms step_avg:97.54ms
step:1283/1750 train_time:125152ms step_avg:97.55ms
step:1284/1750 train_time:125254ms step_avg:97.55ms
step:1285/1750 train_time:125355ms step_avg:97.55ms
step:1286/1750 train_time:125457ms step_avg:97.56ms
step:1287/1750 train_time:125559ms step_avg:97.56ms
step:1288/1750 train_time:125660ms step_avg:97.56ms
step:1289/1750 train_time:125761ms step_avg:97.57ms
step:1290/1750 train_time:125863ms step_avg:97.57ms
step:1291/1750 train_time:125964ms step_avg:97.57ms
step:1292/1750 train_time:126067ms step_avg:97.58ms
step:1293/1750 train_time:126169ms step_avg:97.58ms
step:1294/1750 train_time:126272ms step_avg:97.58ms
step:1295/1750 train_time:126376ms step_avg:97.59ms
step:1296/1750 train_time:126478ms step_avg:97.59ms
step:1297/1750 train_time:126578ms step_avg:97.59ms
step:1298/1750 train_time:126680ms step_avg:97.60ms
step:1299/1750 train_time:126782ms step_avg:97.60ms
step:1300/1750 train_time:126883ms step_avg:97.60ms
step:1301/1750 train_time:126986ms step_avg:97.61ms
step:1302/1750 train_time:127088ms step_avg:97.61ms
step:1303/1750 train_time:127192ms step_avg:97.61ms
step:1304/1750 train_time:127294ms step_avg:97.62ms
step:1305/1750 train_time:127396ms step_avg:97.62ms
step:1306/1750 train_time:127498ms step_avg:97.63ms
step:1307/1750 train_time:127599ms step_avg:97.63ms
step:1308/1750 train_time:127701ms step_avg:97.63ms
step:1309/1750 train_time:127803ms step_avg:97.63ms
step:1310/1750 train_time:127906ms step_avg:97.64ms
step:1311/1750 train_time:128008ms step_avg:97.64ms
step:1312/1750 train_time:128109ms step_avg:97.64ms
step:1313/1750 train_time:128213ms step_avg:97.65ms
step:1314/1750 train_time:128315ms step_avg:97.65ms
step:1315/1750 train_time:128417ms step_avg:97.66ms
step:1316/1750 train_time:128518ms step_avg:97.66ms
step:1317/1750 train_time:128619ms step_avg:97.66ms
step:1318/1750 train_time:128721ms step_avg:97.66ms
step:1319/1750 train_time:128823ms step_avg:97.67ms
step:1320/1750 train_time:128925ms step_avg:97.67ms
step:1321/1750 train_time:129027ms step_avg:97.67ms
step:1322/1750 train_time:129128ms step_avg:97.68ms
step:1323/1750 train_time:129231ms step_avg:97.68ms
step:1324/1750 train_time:129334ms step_avg:97.68ms
step:1325/1750 train_time:129436ms step_avg:97.69ms
step:1326/1750 train_time:129538ms step_avg:97.69ms
step:1327/1750 train_time:129640ms step_avg:97.69ms
step:1328/1750 train_time:129741ms step_avg:97.70ms
step:1329/1750 train_time:129842ms step_avg:97.70ms
step:1330/1750 train_time:129944ms step_avg:97.70ms
step:1331/1750 train_time:130047ms step_avg:97.71ms
step:1332/1750 train_time:130150ms step_avg:97.71ms
step:1333/1750 train_time:130252ms step_avg:97.71ms
step:1334/1750 train_time:130354ms step_avg:97.72ms
step:1335/1750 train_time:130456ms step_avg:97.72ms
step:1336/1750 train_time:130559ms step_avg:97.72ms
step:1337/1750 train_time:130659ms step_avg:97.73ms
step:1338/1750 train_time:130761ms step_avg:97.73ms
step:1339/1750 train_time:130863ms step_avg:97.73ms
step:1340/1750 train_time:130964ms step_avg:97.73ms
step:1341/1750 train_time:131067ms step_avg:97.74ms
step:1342/1750 train_time:131168ms step_avg:97.74ms
step:1343/1750 train_time:131270ms step_avg:97.74ms
step:1344/1750 train_time:131373ms step_avg:97.75ms
step:1345/1750 train_time:131476ms step_avg:97.75ms
step:1346/1750 train_time:131578ms step_avg:97.76ms
step:1347/1750 train_time:131680ms step_avg:97.76ms
step:1348/1750 train_time:131781ms step_avg:97.76ms
step:1349/1750 train_time:131882ms step_avg:97.76ms
step:1350/1750 train_time:131984ms step_avg:97.77ms
step:1351/1750 train_time:132086ms step_avg:97.77ms
step:1352/1750 train_time:132187ms step_avg:97.77ms
step:1353/1750 train_time:132290ms step_avg:97.78ms
step:1354/1750 train_time:132393ms step_avg:97.78ms
step:1355/1750 train_time:132495ms step_avg:97.78ms
step:1356/1750 train_time:132597ms step_avg:97.79ms
step:1357/1750 train_time:132698ms step_avg:97.79ms
step:1358/1750 train_time:132800ms step_avg:97.79ms
step:1359/1750 train_time:132902ms step_avg:97.79ms
step:1360/1750 train_time:133004ms step_avg:97.80ms
step:1361/1750 train_time:133106ms step_avg:97.80ms
step:1362/1750 train_time:133207ms step_avg:97.80ms
step:1363/1750 train_time:133309ms step_avg:97.81ms
step:1364/1750 train_time:133413ms step_avg:97.81ms
step:1365/1750 train_time:133515ms step_avg:97.81ms
step:1366/1750 train_time:133617ms step_avg:97.82ms
step:1367/1750 train_time:133718ms step_avg:97.82ms
step:1368/1750 train_time:133820ms step_avg:97.82ms
step:1369/1750 train_time:133922ms step_avg:97.82ms
step:1370/1750 train_time:134024ms step_avg:97.83ms
step:1371/1750 train_time:134125ms step_avg:97.83ms
step:1372/1750 train_time:134227ms step_avg:97.83ms
step:1373/1750 train_time:134329ms step_avg:97.84ms
step:1374/1750 train_time:134432ms step_avg:97.84ms
step:1375/1750 train_time:134535ms step_avg:97.84ms
step:1375/1750 val_loss:3.3657 train_time:134631ms step_avg:97.91ms
step:1376/1750 train_time:134658ms step_avg:97.86ms
step:1377/1750 train_time:134748ms step_avg:97.86ms
step:1378/1750 train_time:134850ms step_avg:97.86ms
step:1379/1750 train_time:134952ms step_avg:97.86ms
step:1380/1750 train_time:135054ms step_avg:97.86ms
step:1381/1750 train_time:135156ms step_avg:97.87ms
step:1382/1750 train_time:135257ms step_avg:97.87ms
step:1383/1750 train_time:135358ms step_avg:97.87ms
step:1384/1750 train_time:135459ms step_avg:97.88ms
step:1385/1750 train_time:135561ms step_avg:97.88ms
step:1386/1750 train_time:135665ms step_avg:97.88ms
step:1387/1750 train_time:135766ms step_avg:97.88ms
step:1388/1750 train_time:135869ms step_avg:97.89ms
step:1389/1750 train_time:135970ms step_avg:97.89ms
step:1390/1750 train_time:136072ms step_avg:97.89ms
step:1391/1750 train_time:136174ms step_avg:97.90ms
step:1392/1750 train_time:136276ms step_avg:97.90ms
step:1393/1750 train_time:136378ms step_avg:97.90ms
step:1394/1750 train_time:136479ms step_avg:97.90ms
step:1395/1750 train_time:136582ms step_avg:97.91ms
step:1396/1750 train_time:136684ms step_avg:97.91ms
step:1397/1750 train_time:136787ms step_avg:97.91ms
step:1398/1750 train_time:136888ms step_avg:97.92ms
step:1399/1750 train_time:136991ms step_avg:97.92ms
step:1400/1750 train_time:137093ms step_avg:97.92ms
step:1401/1750 train_time:137195ms step_avg:97.93ms
step:1402/1750 train_time:137297ms step_avg:97.93ms
step:1403/1750 train_time:137399ms step_avg:97.93ms
step:1404/1750 train_time:137501ms step_avg:97.94ms
step:1405/1750 train_time:137603ms step_avg:97.94ms
step:1406/1750 train_time:137705ms step_avg:97.94ms
step:1407/1750 train_time:137808ms step_avg:97.94ms
step:1408/1750 train_time:137908ms step_avg:97.95ms
step:1409/1750 train_time:138011ms step_avg:97.95ms
step:1410/1750 train_time:138112ms step_avg:97.95ms
step:1411/1750 train_time:138215ms step_avg:97.96ms
step:1412/1750 train_time:138317ms step_avg:97.96ms
step:1413/1750 train_time:138419ms step_avg:97.96ms
step:1414/1750 train_time:138522ms step_avg:97.96ms
step:1415/1750 train_time:138625ms step_avg:97.97ms
step:1416/1750 train_time:138727ms step_avg:97.97ms
step:1417/1750 train_time:138829ms step_avg:97.97ms
step:1418/1750 train_time:138931ms step_avg:97.98ms
step:1419/1750 train_time:139032ms step_avg:97.98ms
step:1420/1750 train_time:139136ms step_avg:97.98ms
step:1421/1750 train_time:139236ms step_avg:97.98ms
step:1422/1750 train_time:139338ms step_avg:97.99ms
step:1423/1750 train_time:139441ms step_avg:97.99ms
step:1424/1750 train_time:139544ms step_avg:97.99ms
step:1425/1750 train_time:139646ms step_avg:98.00ms
step:1426/1750 train_time:139748ms step_avg:98.00ms
step:1427/1750 train_time:139849ms step_avg:98.00ms
step:1428/1750 train_time:139953ms step_avg:98.01ms
step:1429/1750 train_time:140055ms step_avg:98.01ms
step:1430/1750 train_time:140158ms step_avg:98.01ms
step:1431/1750 train_time:140260ms step_avg:98.02ms
step:1432/1750 train_time:140363ms step_avg:98.02ms
step:1433/1750 train_time:140466ms step_avg:98.02ms
step:1434/1750 train_time:140569ms step_avg:98.03ms
step:1435/1750 train_time:140674ms step_avg:98.03ms
step:1436/1750 train_time:140779ms step_avg:98.04ms
step:1437/1750 train_time:140881ms step_avg:98.04ms
step:1438/1750 train_time:140984ms step_avg:98.04ms
step:1439/1750 train_time:141088ms step_avg:98.05ms
step:1440/1750 train_time:141192ms step_avg:98.05ms
step:1441/1750 train_time:141297ms step_avg:98.05ms
step:1442/1750 train_time:141400ms step_avg:98.06ms
step:1443/1750 train_time:141503ms step_avg:98.06ms
step:1444/1750 train_time:141606ms step_avg:98.06ms
step:1445/1750 train_time:141708ms step_avg:98.07ms
step:1446/1750 train_time:141810ms step_avg:98.07ms
step:1447/1750 train_time:141913ms step_avg:98.07ms
step:1448/1750 train_time:142016ms step_avg:98.08ms
step:1449/1750 train_time:142118ms step_avg:98.08ms
step:1450/1750 train_time:142221ms step_avg:98.08ms
step:1451/1750 train_time:142324ms step_avg:98.09ms
step:1452/1750 train_time:142427ms step_avg:98.09ms
step:1453/1750 train_time:142531ms step_avg:98.09ms
step:1454/1750 train_time:142636ms step_avg:98.10ms
step:1455/1750 train_time:142738ms step_avg:98.10ms
step:1456/1750 train_time:142841ms step_avg:98.10ms
step:1457/1750 train_time:142944ms step_avg:98.11ms
step:1458/1750 train_time:143046ms step_avg:98.11ms
step:1459/1750 train_time:143149ms step_avg:98.11ms
step:1460/1750 train_time:143251ms step_avg:98.12ms
step:1461/1750 train_time:143354ms step_avg:98.12ms
step:1462/1750 train_time:143458ms step_avg:98.12ms
step:1463/1750 train_time:143562ms step_avg:98.13ms
step:1464/1750 train_time:143666ms step_avg:98.13ms
step:1465/1750 train_time:143768ms step_avg:98.14ms
step:1466/1750 train_time:143871ms step_avg:98.14ms
step:1467/1750 train_time:143973ms step_avg:98.14ms
step:1468/1750 train_time:144077ms step_avg:98.15ms
step:1469/1750 train_time:144180ms step_avg:98.15ms
step:1470/1750 train_time:144284ms step_avg:98.15ms
step:1471/1750 train_time:144386ms step_avg:98.15ms
step:1472/1750 train_time:144488ms step_avg:98.16ms
step:1473/1750 train_time:144592ms step_avg:98.16ms
step:1474/1750 train_time:144697ms step_avg:98.17ms
step:1475/1750 train_time:144800ms step_avg:98.17ms
step:1476/1750 train_time:144902ms step_avg:98.17ms
step:1477/1750 train_time:145007ms step_avg:98.18ms
step:1478/1750 train_time:145109ms step_avg:98.18ms
step:1479/1750 train_time:145212ms step_avg:98.18ms
step:1480/1750 train_time:145314ms step_avg:98.19ms
step:1481/1750 train_time:145417ms step_avg:98.19ms
step:1482/1750 train_time:145520ms step_avg:98.19ms
step:1483/1750 train_time:145624ms step_avg:98.20ms
step:1484/1750 train_time:145727ms step_avg:98.20ms
step:1485/1750 train_time:145830ms step_avg:98.20ms
step:1486/1750 train_time:145932ms step_avg:98.20ms
step:1487/1750 train_time:146036ms step_avg:98.21ms
step:1488/1750 train_time:146140ms step_avg:98.21ms
step:1489/1750 train_time:146243ms step_avg:98.22ms
step:1490/1750 train_time:146346ms step_avg:98.22ms
step:1491/1750 train_time:146449ms step_avg:98.22ms
step:1492/1750 train_time:146551ms step_avg:98.22ms
step:1493/1750 train_time:146653ms step_avg:98.23ms
step:1494/1750 train_time:146757ms step_avg:98.23ms
step:1495/1750 train_time:146859ms step_avg:98.23ms
step:1496/1750 train_time:146963ms step_avg:98.24ms
step:1497/1750 train_time:147064ms step_avg:98.24ms
step:1498/1750 train_time:147167ms step_avg:98.24ms
step:1499/1750 train_time:147269ms step_avg:98.24ms
step:1500/1750 train_time:147373ms step_avg:98.25ms
step:1500/1750 val_loss:3.3300 train_time:147471ms step_avg:98.31ms
step:1501/1750 train_time:147498ms step_avg:98.27ms
step:1502/1750 train_time:147586ms step_avg:98.26ms
step:1503/1750 train_time:147690ms step_avg:98.26ms
step:1504/1750 train_time:147792ms step_avg:98.27ms
step:1505/1750 train_time:147895ms step_avg:98.27ms
step:1506/1750 train_time:147997ms step_avg:98.27ms
step:1507/1750 train_time:148099ms step_avg:98.27ms
step:1508/1750 train_time:148201ms step_avg:98.28ms
step:1509/1750 train_time:148303ms step_avg:98.28ms
step:1510/1750 train_time:148406ms step_avg:98.28ms
step:1511/1750 train_time:148510ms step_avg:98.29ms
step:1512/1750 train_time:148614ms step_avg:98.29ms
step:1513/1750 train_time:148717ms step_avg:98.29ms
step:1514/1750 train_time:148822ms step_avg:98.30ms
step:1515/1750 train_time:148928ms step_avg:98.30ms
step:1516/1750 train_time:149031ms step_avg:98.31ms
step:1517/1750 train_time:149133ms step_avg:98.31ms
step:1518/1750 train_time:149237ms step_avg:98.31ms
step:1519/1750 train_time:149343ms step_avg:98.32ms
step:1520/1750 train_time:149444ms step_avg:98.32ms
step:1521/1750 train_time:149546ms step_avg:98.32ms
step:1522/1750 train_time:149649ms step_avg:98.32ms
step:1523/1750 train_time:149752ms step_avg:98.33ms
step:1524/1750 train_time:149856ms step_avg:98.33ms
step:1525/1750 train_time:149960ms step_avg:98.33ms
step:1526/1750 train_time:150065ms step_avg:98.34ms
step:1527/1750 train_time:150166ms step_avg:98.34ms
step:1528/1750 train_time:150271ms step_avg:98.34ms
step:1529/1750 train_time:150373ms step_avg:98.35ms
step:1530/1750 train_time:150476ms step_avg:98.35ms
step:1531/1750 train_time:150578ms step_avg:98.35ms
step:1532/1750 train_time:150682ms step_avg:98.36ms
step:1533/1750 train_time:150785ms step_avg:98.36ms
step:1534/1750 train_time:150888ms step_avg:98.36ms
step:1535/1750 train_time:150990ms step_avg:98.37ms
step:1536/1750 train_time:151093ms step_avg:98.37ms
step:1537/1750 train_time:151196ms step_avg:98.37ms
step:1538/1750 train_time:151300ms step_avg:98.37ms
step:1539/1750 train_time:151403ms step_avg:98.38ms
step:1540/1750 train_time:151505ms step_avg:98.38ms
step:1541/1750 train_time:151609ms step_avg:98.38ms
step:1542/1750 train_time:151713ms step_avg:98.39ms
step:1543/1750 train_time:151817ms step_avg:98.39ms
step:1544/1750 train_time:151920ms step_avg:98.39ms
step:1545/1750 train_time:152023ms step_avg:98.40ms
step:1546/1750 train_time:152125ms step_avg:98.40ms
step:1547/1750 train_time:152229ms step_avg:98.40ms
step:1548/1750 train_time:152332ms step_avg:98.41ms
step:1549/1750 train_time:152435ms step_avg:98.41ms
step:1550/1750 train_time:152539ms step_avg:98.41ms
step:1551/1750 train_time:152642ms step_avg:98.42ms
step:1552/1750 train_time:152745ms step_avg:98.42ms
step:1553/1750 train_time:152849ms step_avg:98.42ms
step:1554/1750 train_time:152951ms step_avg:98.42ms
step:1555/1750 train_time:153054ms step_avg:98.43ms
step:1556/1750 train_time:153159ms step_avg:98.43ms
step:1557/1750 train_time:153263ms step_avg:98.43ms
step:1558/1750 train_time:153367ms step_avg:98.44ms
step:1559/1750 train_time:153470ms step_avg:98.44ms
step:1560/1750 train_time:153573ms step_avg:98.44ms
step:1561/1750 train_time:153677ms step_avg:98.45ms
step:1562/1750 train_time:153780ms step_avg:98.45ms
step:1563/1750 train_time:153886ms step_avg:98.46ms
step:1564/1750 train_time:153989ms step_avg:98.46ms
step:1565/1750 train_time:154091ms step_avg:98.46ms
step:1566/1750 train_time:154194ms step_avg:98.46ms
step:1567/1750 train_time:154297ms step_avg:98.47ms
step:1568/1750 train_time:154400ms step_avg:98.47ms
step:1569/1750 train_time:154502ms step_avg:98.47ms
step:1570/1750 train_time:154606ms step_avg:98.48ms
step:1571/1750 train_time:154711ms step_avg:98.48ms
step:1572/1750 train_time:154814ms step_avg:98.48ms
step:1573/1750 train_time:154917ms step_avg:98.49ms
step:1574/1750 train_time:155021ms step_avg:98.49ms
step:1575/1750 train_time:155124ms step_avg:98.49ms
step:1576/1750 train_time:155227ms step_avg:98.49ms
step:1577/1750 train_time:155331ms step_avg:98.50ms
step:1578/1750 train_time:155434ms step_avg:98.50ms
step:1579/1750 train_time:155538ms step_avg:98.50ms
step:1580/1750 train_time:155641ms step_avg:98.51ms
step:1581/1750 train_time:155744ms step_avg:98.51ms
step:1582/1750 train_time:155847ms step_avg:98.51ms
step:1583/1750 train_time:155954ms step_avg:98.52ms
step:1584/1750 train_time:156059ms step_avg:98.52ms
step:1585/1750 train_time:156161ms step_avg:98.52ms
step:1586/1750 train_time:156264ms step_avg:98.53ms
step:1587/1750 train_time:156368ms step_avg:98.53ms
step:1588/1750 train_time:156471ms step_avg:98.53ms
step:1589/1750 train_time:156574ms step_avg:98.54ms
step:1590/1750 train_time:156677ms step_avg:98.54ms
step:1591/1750 train_time:156780ms step_avg:98.54ms
step:1592/1750 train_time:156884ms step_avg:98.55ms
step:1593/1750 train_time:156987ms step_avg:98.55ms
step:1594/1750 train_time:157092ms step_avg:98.55ms
step:1595/1750 train_time:157195ms step_avg:98.55ms
step:1596/1750 train_time:157297ms step_avg:98.56ms
step:1597/1750 train_time:157401ms step_avg:98.56ms
step:1598/1750 train_time:157504ms step_avg:98.56ms
step:1599/1750 train_time:157606ms step_avg:98.57ms
step:1600/1750 train_time:157710ms step_avg:98.57ms
step:1601/1750 train_time:157814ms step_avg:98.57ms
step:1602/1750 train_time:157917ms step_avg:98.57ms
step:1603/1750 train_time:158020ms step_avg:98.58ms
step:1604/1750 train_time:158123ms step_avg:98.58ms
step:1605/1750 train_time:158227ms step_avg:98.58ms
step:1606/1750 train_time:158329ms step_avg:98.59ms
step:1607/1750 train_time:158431ms step_avg:98.59ms
step:1608/1750 train_time:158535ms step_avg:98.59ms
step:1609/1750 train_time:158638ms step_avg:98.59ms
step:1610/1750 train_time:158744ms step_avg:98.60ms
step:1611/1750 train_time:158847ms step_avg:98.60ms
step:1612/1750 train_time:158951ms step_avg:98.60ms
step:1613/1750 train_time:159054ms step_avg:98.61ms
step:1614/1750 train_time:159158ms step_avg:98.61ms
step:1615/1750 train_time:159260ms step_avg:98.61ms
step:1616/1750 train_time:159363ms step_avg:98.62ms
step:1617/1750 train_time:159467ms step_avg:98.62ms
step:1618/1750 train_time:159569ms step_avg:98.62ms
step:1619/1750 train_time:159672ms step_avg:98.62ms
step:1620/1750 train_time:159776ms step_avg:98.63ms
step:1621/1750 train_time:159878ms step_avg:98.63ms
step:1622/1750 train_time:159981ms step_avg:98.63ms
step:1623/1750 train_time:160085ms step_avg:98.64ms
step:1624/1750 train_time:160189ms step_avg:98.64ms
step:1625/1750 train_time:160294ms step_avg:98.64ms
step:1625/1750 val_loss:3.2999 train_time:160390ms step_avg:98.70ms
step:1626/1750 train_time:160419ms step_avg:98.66ms
step:1627/1750 train_time:160505ms step_avg:98.65ms
step:1628/1750 train_time:160608ms step_avg:98.65ms
step:1629/1750 train_time:160711ms step_avg:98.66ms
step:1630/1750 train_time:160815ms step_avg:98.66ms
step:1631/1750 train_time:160917ms step_avg:98.66ms
step:1632/1750 train_time:161019ms step_avg:98.66ms
step:1633/1750 train_time:161121ms step_avg:98.67ms
step:1634/1750 train_time:161225ms step_avg:98.67ms
step:1635/1750 train_time:161327ms step_avg:98.67ms
step:1636/1750 train_time:161432ms step_avg:98.67ms
step:1637/1750 train_time:161535ms step_avg:98.68ms
step:1638/1750 train_time:161638ms step_avg:98.68ms
step:1639/1750 train_time:161740ms step_avg:98.68ms
step:1640/1750 train_time:161843ms step_avg:98.68ms
step:1641/1750 train_time:161946ms step_avg:98.69ms
step:1642/1750 train_time:162049ms step_avg:98.69ms
step:1643/1750 train_time:162151ms step_avg:98.69ms
step:1644/1750 train_time:162254ms step_avg:98.69ms
step:1645/1750 train_time:162357ms step_avg:98.70ms
step:1646/1750 train_time:162461ms step_avg:98.70ms
step:1647/1750 train_time:162567ms step_avg:98.71ms
step:1648/1750 train_time:162670ms step_avg:98.71ms
step:1649/1750 train_time:162774ms step_avg:98.71ms
step:1650/1750 train_time:162876ms step_avg:98.71ms
step:1651/1750 train_time:162980ms step_avg:98.72ms
step:1652/1750 train_time:163083ms step_avg:98.72ms
step:1653/1750 train_time:163186ms step_avg:98.72ms
step:1654/1750 train_time:163289ms step_avg:98.72ms
step:1655/1750 train_time:163394ms step_avg:98.73ms
step:1656/1750 train_time:163497ms step_avg:98.73ms
step:1657/1750 train_time:163599ms step_avg:98.73ms
step:1658/1750 train_time:163702ms step_avg:98.73ms
step:1659/1750 train_time:163810ms step_avg:98.74ms
step:1660/1750 train_time:163912ms step_avg:98.74ms
step:1661/1750 train_time:164017ms step_avg:98.75ms
step:1662/1750 train_time:164122ms step_avg:98.75ms
step:1663/1750 train_time:164226ms step_avg:98.75ms
step:1664/1750 train_time:164330ms step_avg:98.76ms
step:1665/1750 train_time:164434ms step_avg:98.76ms
step:1666/1750 train_time:164538ms step_avg:98.76ms
step:1667/1750 train_time:164639ms step_avg:98.76ms
step:1668/1750 train_time:164743ms step_avg:98.77ms
step:1669/1750 train_time:164847ms step_avg:98.77ms
step:1670/1750 train_time:164951ms step_avg:98.77ms
step:1671/1750 train_time:165054ms step_avg:98.78ms
step:1672/1750 train_time:165157ms step_avg:98.78ms
step:1673/1750 train_time:165259ms step_avg:98.78ms
step:1674/1750 train_time:165364ms step_avg:98.78ms
step:1675/1750 train_time:165467ms step_avg:98.79ms
step:1676/1750 train_time:165571ms step_avg:98.79ms
step:1677/1750 train_time:165674ms step_avg:98.79ms
step:1678/1750 train_time:165778ms step_avg:98.80ms
step:1679/1750 train_time:165881ms step_avg:98.80ms
step:1680/1750 train_time:165984ms step_avg:98.80ms
step:1681/1750 train_time:166088ms step_avg:98.80ms
step:1682/1750 train_time:166193ms step_avg:98.81ms
step:1683/1750 train_time:166297ms step_avg:98.81ms
step:1684/1750 train_time:166401ms step_avg:98.81ms
step:1685/1750 train_time:166504ms step_avg:98.82ms
step:1686/1750 train_time:166607ms step_avg:98.82ms
step:1687/1750 train_time:166711ms step_avg:98.82ms
step:1688/1750 train_time:166815ms step_avg:98.82ms
step:1689/1750 train_time:166918ms step_avg:98.83ms
step:1690/1750 train_time:167022ms step_avg:98.83ms
step:1691/1750 train_time:167125ms step_avg:98.83ms
step:1692/1750 train_time:167228ms step_avg:98.83ms
step:1693/1750 train_time:167332ms step_avg:98.84ms
step:1694/1750 train_time:167436ms step_avg:98.84ms
step:1695/1750 train_time:167541ms step_avg:98.84ms
step:1696/1750 train_time:167644ms step_avg:98.85ms
step:1697/1750 train_time:167752ms step_avg:98.85ms
step:1698/1750 train_time:167856ms step_avg:98.85ms
step:1699/1750 train_time:167959ms step_avg:98.86ms
step:1700/1750 train_time:168063ms step_avg:98.86ms
step:1701/1750 train_time:168166ms step_avg:98.86ms
step:1702/1750 train_time:168271ms step_avg:98.87ms
step:1703/1750 train_time:168376ms step_avg:98.87ms
step:1704/1750 train_time:168479ms step_avg:98.87ms
step:1705/1750 train_time:168583ms step_avg:98.88ms
step:1706/1750 train_time:168687ms step_avg:98.88ms
step:1707/1750 train_time:168792ms step_avg:98.88ms
step:1708/1750 train_time:168897ms step_avg:98.89ms
step:1709/1750 train_time:169000ms step_avg:98.89ms
step:1710/1750 train_time:169104ms step_avg:98.89ms
step:1711/1750 train_time:169209ms step_avg:98.89ms
step:1712/1750 train_time:169313ms step_avg:98.90ms
step:1713/1750 train_time:169418ms step_avg:98.90ms
step:1714/1750 train_time:169520ms step_avg:98.90ms
step:1715/1750 train_time:169627ms step_avg:98.91ms
step:1716/1750 train_time:169729ms step_avg:98.91ms
step:1717/1750 train_time:169833ms step_avg:98.91ms
step:1718/1750 train_time:169937ms step_avg:98.92ms
step:1719/1750 train_time:170044ms step_avg:98.92ms
step:1720/1750 train_time:170147ms step_avg:98.92ms
step:1721/1750 train_time:170251ms step_avg:98.93ms
step:1722/1750 train_time:170356ms step_avg:98.93ms
step:1723/1750 train_time:170460ms step_avg:98.93ms
step:1724/1750 train_time:170564ms step_avg:98.94ms
step:1725/1750 train_time:170669ms step_avg:98.94ms
step:1726/1750 train_time:170772ms step_avg:98.94ms
step:1727/1750 train_time:170876ms step_avg:98.94ms
step:1728/1750 train_time:170981ms step_avg:98.95ms
step:1729/1750 train_time:171085ms step_avg:98.95ms
step:1730/1750 train_time:171189ms step_avg:98.95ms
step:1731/1750 train_time:171294ms step_avg:98.96ms
step:1732/1750 train_time:171397ms step_avg:98.96ms
step:1733/1750 train_time:171500ms step_avg:98.96ms
step:1734/1750 train_time:171606ms step_avg:98.97ms
step:1735/1750 train_time:171710ms step_avg:98.97ms
step:1736/1750 train_time:171814ms step_avg:98.97ms
step:1737/1750 train_time:171918ms step_avg:98.97ms
step:1738/1750 train_time:172022ms step_avg:98.98ms
step:1739/1750 train_time:172127ms step_avg:98.98ms
step:1740/1750 train_time:172231ms step_avg:98.98ms
step:1741/1750 train_time:172339ms step_avg:98.99ms
step:1742/1750 train_time:172444ms step_avg:98.99ms
step:1743/1750 train_time:172548ms step_avg:99.00ms
step:1744/1750 train_time:172652ms step_avg:99.00ms
step:1745/1750 train_time:172756ms step_avg:99.00ms
step:1746/1750 train_time:172859ms step_avg:99.00ms
step:1747/1750 train_time:172964ms step_avg:99.01ms
step:1748/1750 train_time:173069ms step_avg:99.01ms
step:1749/1750 train_time:173172ms step_avg:99.01ms
step:1750/1750 train_time:173279ms step_avg:99.02ms
step:1750/1750 val_loss:3.2792 train_time:173377ms step_avg:99.07ms
peak memory allocated: 33430 MiB reserved: 48872 MiB

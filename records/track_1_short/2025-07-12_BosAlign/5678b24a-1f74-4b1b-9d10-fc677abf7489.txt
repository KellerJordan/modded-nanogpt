import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache, partial # Added partial for hook registration
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn, autocast
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
#import wandb

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)

        params = list(params)
        sizes = {p.shape for p in params}

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params,))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            grad_pad = [param.grad for param in params] + [torch.zeros_like(params[-1])] * self.world_size
            for base_i in range(0, len(params), self.world_size):
                if base_i + self.rank < len(params):
                    grad = params[base_i + self.rank].grad
                # This gives strange dynamo warnings
                reduce_scatter_futures.append(dist.reduce_scatter(grad, grad_pad[base_i:base_i + self.world_size], op=dist.ReduceOp.AVG, async_op=True).get_future())

        idx = 0
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            params_pad = params + [torch.empty_like(params[-1])] * self.world_size
            momentum = group["momentum"]
            for base_i in range(0, len(params), self.world_size):
                reduce_scatter_futures[idx].wait()
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    grad = p.grad
                    eff_lr = group["lr"] * max(1, p.size(-2) / p.size(-1)) ** 0.5 * getattr(p, "lr_mul", 1.0)
                    eff_weight_decay = group["lr"] * group["weight_decay"] * getattr(p, "wd_mul", 1.0)
                    state = self.state[p]
                    if len(state) == 0:
                        state["momentum_buffer"] = torch.zeros_like(grad)
                    momentum_buffer = state["momentum_buffer"]
                    p.mul_(1 - eff_weight_decay)
                    momentum_buffer.lerp_(grad, 1 - momentum)
                    grad = grad.lerp_(momentum_buffer, momentum)
                    v = zeropower_via_newtonschulz5(grad.bfloat16(), 5)
                    p.add_(other=v, alpha=-eff_lr)
                idx += 1
                futures.append(dist.all_gather(params_pad[base_i:base_i + self.world_size], params_pad[base_i + self.rank], async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01, rank: int = 0, world_size: int = 1):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        self.rank = rank
        self.world_size = world_size

        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(
                params=group_params,
            ))
        super().__init__(param_groups, defaults)

    @torch.compile
    @torch.no_grad()
    def step(self):
        futures: list[torch.Future] = []
        reduce_scatter_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            grad = torch.empty_like(params[-1])
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // self.world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // self.world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state['step'] = torch.tensor(0, dtype=torch.int64, device=p.device)
                    state['exp_avg'] = torch.zeros_like(p_slice)
                    state['exp_avg_sq'] = torch.zeros_like(p_slice)

                exp_avg = state['exp_avg']
                exp_avg_sq = state['exp_avg_sq']
                state['step'] += 1
                t = state['step']

                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)

                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t

                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        # TODO: Check if commenting it is dangerous
        torch.futures.collect_all(futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, lambdas: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = lambdas[0] * v + lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, lambdas: Tensor, sa_lambdas: Tensor, block_mask: BlockMask):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, sa_lambdas, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        for param in self.embed.parameters():
            param.lr_mul = 75.
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embeds in self.value_embeds:
            for param in self.value_embeds.parameters():
                param.lr_mul = 75.
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(model_dim**0.5)/448, w_s=24/448, grad_s=1/448)
        self.lm_head.weight.lr_mul = 27.5
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % world_size
        self.scalars = nn.Parameter(torch.cat([
            torch.ones(num_layers), # skip_weights
            *[torch.tensor([1.0, 0.0]) for _ in range(num_layers)], # block lambdas
            *[torch.tensor([0.5, 0.5]) for _ in range(num_layers)], # SA lambdas
            torch.ones(pad),
        ]))
        self.scalars.lr_mul = 5.0

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            #return causal_mask
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            if i >= n:
                x = x + skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, lambdas[i], sa_lambdas[i], block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / (7.5 * x.size(-1)**0.5))
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

# find world_size starting indicies, such that each begins with token 50256 and local_batches don't overlap
def find_batch_starts(tokens: Tensor, pos: int, world_size: int, local_batch_size: int, max_batch_span: int):
    boundary_mask = tokens[pos:pos+max_batch_span] == 50256
    boundary_positions = torch.nonzero(boundary_mask, as_tuple=False).squeeze(-1) + pos
    start = boundary_positions[0].item()

    starts = []
    batch_end=None
    for i in range(len(boundary_positions) - 1):
        end = boundary_positions[i + 1].item() 
        if end - start >= local_batch_size:
            starts.append(start) # append start once end pos is confirmed
            if len(starts) == world_size:
                batch_end = end
                break
            start = end
    assert batch_end is not None # increase max_batch_span if necessary
    batch_span = batch_end-pos
    return starts, batch_span

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int, align_to_bos: bool):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    batch_span = batch_size
    max_batch_span = 2*batch_size if align_to_bos else batch_size #provide buffer to handle samples up to length local_batch_size

    while True:
        if pos + max_batch_span + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        if align_to_bos:
            batch_starts, batch_span = find_batch_starts(tokens, pos, world_size, local_batch_size, max_batch_span)
            start_idx = batch_starts[rank]
        else:
            start_idx = pos + rank * local_batch_size
        buf = tokens[start_idx:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_span
        yield inputs, targets


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.45 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    train_align_to_bos = True # align local batch start indicies with next bos_token
    val_align_to_bos = False # False to maintain same eval as prior records
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

#if master_process:
#    wandb.init(project="modded-nanogpt-tiny", name=f"run-{os.path.basename(__file__)}", save_code=True)

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=True):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
if master_process:
    print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(vocab_size=next_multiple_of_n(50257, n=128), num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094

optimizer1 = DistAdam(scalar_params + head_params + embed_params, lr=0.008, betas=(0.8, 0.95), eps=1e-10, weight_decay=0.0, rank=rank, world_size=world_size)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]

for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

for n, p in model.named_parameters():
    wd_mul = getattr(p, "wd_mul", 1.0)
    lr_mul = getattr(p, "lr_mul", 1.0)

    print0(f"{n}: {p.shape} {p.dtype} {wd_mul} {lr_mul}")

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
embedding_params = sum(p.numel() for n, p in model.named_parameters() if "embed" in n)
non_embedding_params = total_params - embedding_params

print0(f"")
print0(f"Model parameters:")
print0(f"  Total parameters: {total_params:,}")
print0(f"  Embedding parameters: {embedding_params:,}")
print0(f"  Non-embedding parameters: {non_embedding_params:,}")

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    w = min((1 - x) / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.05
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
for _ in range(warmup_steps):
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(1)).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)

torch.cuda.synchronize()
dist.barrier()
with torch.profiler.profile() as prof:
    for _ in range(warmup_steps):
        inputs, targets = next(train_loader)
        model(inputs, targets, get_window_size_blocks(1)).backward()
        for opt in optimizers:
            opt.step()
    model.zero_grad(set_to_none=True)
    torch.cuda.synchronize()
    dist.barrier()
os.makedirs("traces", exist_ok=True)
prof.export_chrome_trace(f"traces/trace_{rank}.json")

model.load_state_dict(initial_state['model'])
for opt, opt_state in zip(optimizers, initial_state['optimizers']):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

train_loader = distributed_data_generator(args.train_files, world_size * args.seq_len, rank, world_size, args.train_align_to_bos)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size, args.val_align_to_bos)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        #if master_process:
        #    wandb.log({"val/loss": val_loss}, step=step)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.8.0.dev20250524+cu126 compiled for CUDA 12.6
Sun Jul 13 01:18:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    5856MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1517MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           77661      C   /usr/bin/python3                       1508MiB |
|    0   N/A  N/A           77662      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           77663      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           77664      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           77665      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           77666      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           77667      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A           77668      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A           77662      C   /usr/bin/python3                       1508MiB |
|    2   N/A  N/A           77663      C   /usr/bin/python3                       1508MiB |
|    3   N/A  N/A           77664      C   /usr/bin/python3                       1508MiB |
|    4   N/A  N/A           77665      C   /usr/bin/python3                       1508MiB |
|    5   N/A  N/A           77666      C   /usr/bin/python3                       1508MiB |
|    6   N/A  N/A           77667      C   /usr/bin/python3                       1508MiB |
|    7   N/A  N/A           77668      C   /usr/bin/python3                       1508MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
scalars: torch.Size([64]) torch.float32 1.0 5.0
embed.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.0.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.1.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
value_embeds.2.weight: torch.Size([50304, 768]) torch.bfloat16 1.0 75.0
blocks.0.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.0.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.0.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.1.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.1.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.1.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.2.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.2.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.2.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.3.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.3.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.3.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.4.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.4.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.4.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.5.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.5.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.5.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.6.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.6.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.6.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.7.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.7.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.8.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.8.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.8.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.9.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.9.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.9.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.10.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.10.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.10.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
blocks.11.attn.qkv_w: torch.Size([3, 768, 768]) torch.float32 1.0 1.0
blocks.11.attn.c_proj.weight: torch.Size([768, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_fc.weight: torch.Size([3072, 768]) torch.float32 1.0 1.0
blocks.11.mlp.c_proj.weight: torch.Size([768, 3072]) torch.float32 1.0 1.0
lm_head.weight: torch.Size([50304, 768]) torch.float32 1.0 27.5

Model parameters:
  Total parameters: 275,742,784
  Embedding parameters: 154,533,888
  Non-embedding parameters: 121,208,896
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:149ms step_avg:149.07ms
step:2/1750 train_time:176ms step_avg:87.79ms
step:3/1750 train_time:247ms step_avg:82.35ms
step:4/1750 train_time:340ms step_avg:84.97ms
step:5/1750 train_time:431ms step_avg:86.18ms
step:6/1750 train_time:523ms step_avg:87.20ms
step:7/1750 train_time:616ms step_avg:88.00ms
step:8/1750 train_time:708ms step_avg:88.56ms
step:9/1750 train_time:802ms step_avg:89.06ms
step:10/1750 train_time:894ms step_avg:89.37ms
step:11/1750 train_time:986ms step_avg:89.63ms
step:12/1750 train_time:1079ms step_avg:89.94ms
step:13/1750 train_time:1174ms step_avg:90.34ms
step:14/1750 train_time:1269ms step_avg:90.61ms
step:15/1750 train_time:1362ms step_avg:90.80ms
step:16/1750 train_time:1455ms step_avg:90.95ms
step:17/1750 train_time:1548ms step_avg:91.05ms
step:18/1750 train_time:1641ms step_avg:91.14ms
step:19/1750 train_time:1734ms step_avg:91.25ms
step:20/1750 train_time:1827ms step_avg:91.33ms
step:21/1750 train_time:1920ms step_avg:91.42ms
step:22/1750 train_time:2013ms step_avg:91.50ms
step:23/1750 train_time:2107ms step_avg:91.59ms
step:24/1750 train_time:2201ms step_avg:91.69ms
step:25/1750 train_time:2294ms step_avg:91.77ms
step:26/1750 train_time:2387ms step_avg:91.82ms
step:27/1750 train_time:2481ms step_avg:91.88ms
step:28/1750 train_time:2573ms step_avg:91.91ms
step:29/1750 train_time:2667ms step_avg:91.96ms
step:30/1750 train_time:2760ms step_avg:92.00ms
step:31/1750 train_time:2852ms step_avg:92.01ms
step:32/1750 train_time:2945ms step_avg:92.04ms
step:33/1750 train_time:3039ms step_avg:92.09ms
step:34/1750 train_time:3132ms step_avg:92.11ms
step:35/1750 train_time:3226ms step_avg:92.17ms
step:36/1750 train_time:3320ms step_avg:92.23ms
step:37/1750 train_time:3414ms step_avg:92.26ms
step:38/1750 train_time:3508ms step_avg:92.30ms
step:39/1750 train_time:3601ms step_avg:92.33ms
step:40/1750 train_time:3694ms step_avg:92.35ms
step:41/1750 train_time:3787ms step_avg:92.36ms
step:42/1750 train_time:3880ms step_avg:92.38ms
step:43/1750 train_time:3973ms step_avg:92.39ms
step:44/1750 train_time:4066ms step_avg:92.40ms
step:45/1750 train_time:4159ms step_avg:92.43ms
step:46/1750 train_time:4253ms step_avg:92.45ms
step:47/1750 train_time:4347ms step_avg:92.48ms
step:48/1750 train_time:4441ms step_avg:92.53ms
step:49/1750 train_time:4534ms step_avg:92.52ms
step:50/1750 train_time:4627ms step_avg:92.55ms
step:51/1750 train_time:4721ms step_avg:92.57ms
step:52/1750 train_time:4814ms step_avg:92.57ms
step:53/1750 train_time:4906ms step_avg:92.57ms
step:54/1750 train_time:5000ms step_avg:92.59ms
step:55/1750 train_time:5092ms step_avg:92.59ms
step:56/1750 train_time:5186ms step_avg:92.60ms
step:57/1750 train_time:5279ms step_avg:92.62ms
step:58/1750 train_time:5372ms step_avg:92.63ms
step:59/1750 train_time:5465ms step_avg:92.63ms
step:60/1750 train_time:5560ms step_avg:92.66ms
step:61/1750 train_time:5653ms step_avg:92.67ms
step:62/1750 train_time:5746ms step_avg:92.68ms
step:63/1750 train_time:5841ms step_avg:92.71ms
step:64/1750 train_time:5935ms step_avg:92.73ms
step:65/1750 train_time:6028ms step_avg:92.74ms
step:66/1750 train_time:6121ms step_avg:92.75ms
step:67/1750 train_time:6214ms step_avg:92.75ms
step:68/1750 train_time:6310ms step_avg:92.80ms
step:69/1750 train_time:6401ms step_avg:92.77ms
step:70/1750 train_time:6494ms step_avg:92.78ms
step:71/1750 train_time:6587ms step_avg:92.78ms
step:72/1750 train_time:6680ms step_avg:92.78ms
step:73/1750 train_time:6773ms step_avg:92.78ms
step:74/1750 train_time:6868ms step_avg:92.81ms
step:75/1750 train_time:6962ms step_avg:92.82ms
step:76/1750 train_time:7054ms step_avg:92.82ms
step:77/1750 train_time:7147ms step_avg:92.82ms
step:78/1750 train_time:7241ms step_avg:92.83ms
step:79/1750 train_time:7334ms step_avg:92.84ms
step:80/1750 train_time:7428ms step_avg:92.84ms
step:81/1750 train_time:7521ms step_avg:92.85ms
step:82/1750 train_time:7614ms step_avg:92.85ms
step:83/1750 train_time:7707ms step_avg:92.85ms
step:84/1750 train_time:7800ms step_avg:92.86ms
step:85/1750 train_time:7893ms step_avg:92.86ms
step:86/1750 train_time:7987ms step_avg:92.87ms
step:87/1750 train_time:8080ms step_avg:92.88ms
step:88/1750 train_time:8173ms step_avg:92.88ms
step:89/1750 train_time:8267ms step_avg:92.88ms
step:90/1750 train_time:8360ms step_avg:92.89ms
step:91/1750 train_time:8453ms step_avg:92.89ms
step:92/1750 train_time:8547ms step_avg:92.90ms
step:93/1750 train_time:8640ms step_avg:92.91ms
step:94/1750 train_time:8733ms step_avg:92.90ms
step:95/1750 train_time:8826ms step_avg:92.90ms
step:96/1750 train_time:8919ms step_avg:92.90ms
step:97/1750 train_time:9012ms step_avg:92.91ms
step:98/1750 train_time:9105ms step_avg:92.91ms
step:99/1750 train_time:9199ms step_avg:92.92ms
step:100/1750 train_time:9292ms step_avg:92.92ms
step:101/1750 train_time:9385ms step_avg:92.92ms
step:102/1750 train_time:9479ms step_avg:92.93ms
step:103/1750 train_time:9571ms step_avg:92.93ms
step:104/1750 train_time:9665ms step_avg:92.93ms
step:105/1750 train_time:9759ms step_avg:92.94ms
step:106/1750 train_time:9851ms step_avg:92.93ms
step:107/1750 train_time:9944ms step_avg:92.94ms
step:108/1750 train_time:10037ms step_avg:92.94ms
step:109/1750 train_time:10130ms step_avg:92.94ms
step:110/1750 train_time:10224ms step_avg:92.95ms
step:111/1750 train_time:10317ms step_avg:92.95ms
step:112/1750 train_time:10411ms step_avg:92.96ms
step:113/1750 train_time:10504ms step_avg:92.96ms
step:114/1750 train_time:10597ms step_avg:92.96ms
step:115/1750 train_time:10690ms step_avg:92.96ms
step:116/1750 train_time:10783ms step_avg:92.96ms
step:117/1750 train_time:10875ms step_avg:92.95ms
step:118/1750 train_time:10968ms step_avg:92.95ms
step:119/1750 train_time:11062ms step_avg:92.95ms
step:120/1750 train_time:11154ms step_avg:92.95ms
step:121/1750 train_time:11248ms step_avg:92.96ms
step:122/1750 train_time:11341ms step_avg:92.96ms
step:123/1750 train_time:11435ms step_avg:92.97ms
step:124/1750 train_time:11528ms step_avg:92.97ms
step:125/1750 train_time:11622ms step_avg:92.98ms
step:125/1750 val_loss:4.6314 train_time:11710ms step_avg:93.68ms
step:126/1750 train_time:11737ms step_avg:93.15ms
step:127/1750 train_time:11816ms step_avg:93.04ms
step:128/1750 train_time:11915ms step_avg:93.09ms
step:129/1750 train_time:12011ms step_avg:93.11ms
step:130/1750 train_time:12103ms step_avg:93.10ms
step:131/1750 train_time:12196ms step_avg:93.10ms
step:132/1750 train_time:12289ms step_avg:93.10ms
step:133/1750 train_time:12382ms step_avg:93.10ms
step:134/1750 train_time:12475ms step_avg:93.10ms
step:135/1750 train_time:12568ms step_avg:93.09ms
step:136/1750 train_time:12661ms step_avg:93.09ms
step:137/1750 train_time:12754ms step_avg:93.10ms
step:138/1750 train_time:12850ms step_avg:93.12ms
step:139/1750 train_time:12946ms step_avg:93.14ms
step:140/1750 train_time:13041ms step_avg:93.15ms
step:141/1750 train_time:13135ms step_avg:93.16ms
step:142/1750 train_time:13230ms step_avg:93.17ms
step:143/1750 train_time:13323ms step_avg:93.17ms
step:144/1750 train_time:13416ms step_avg:93.16ms
step:145/1750 train_time:13508ms step_avg:93.16ms
step:146/1750 train_time:13602ms step_avg:93.16ms
step:147/1750 train_time:13695ms step_avg:93.16ms
step:148/1750 train_time:13789ms step_avg:93.17ms
step:149/1750 train_time:13884ms step_avg:93.18ms
step:150/1750 train_time:13979ms step_avg:93.19ms
step:151/1750 train_time:14073ms step_avg:93.20ms
step:152/1750 train_time:14167ms step_avg:93.20ms
step:153/1750 train_time:14262ms step_avg:93.21ms
step:154/1750 train_time:14356ms step_avg:93.22ms
step:155/1750 train_time:14449ms step_avg:93.22ms
step:156/1750 train_time:14541ms step_avg:93.21ms
step:157/1750 train_time:14635ms step_avg:93.21ms
step:158/1750 train_time:14729ms step_avg:93.22ms
step:159/1750 train_time:14822ms step_avg:93.22ms
step:160/1750 train_time:14916ms step_avg:93.23ms
step:161/1750 train_time:15011ms step_avg:93.23ms
step:162/1750 train_time:15105ms step_avg:93.24ms
step:163/1750 train_time:15198ms step_avg:93.24ms
step:164/1750 train_time:15292ms step_avg:93.24ms
step:165/1750 train_time:15385ms step_avg:93.24ms
step:166/1750 train_time:15478ms step_avg:93.24ms
step:167/1750 train_time:15572ms step_avg:93.25ms
step:168/1750 train_time:15665ms step_avg:93.25ms
step:169/1750 train_time:15760ms step_avg:93.25ms
step:170/1750 train_time:15853ms step_avg:93.26ms
step:171/1750 train_time:15947ms step_avg:93.26ms
step:172/1750 train_time:16041ms step_avg:93.26ms
step:173/1750 train_time:16135ms step_avg:93.27ms
step:174/1750 train_time:16229ms step_avg:93.27ms
step:175/1750 train_time:16324ms step_avg:93.28ms
step:176/1750 train_time:16418ms step_avg:93.28ms
step:177/1750 train_time:16511ms step_avg:93.28ms
step:178/1750 train_time:16604ms step_avg:93.28ms
step:179/1750 train_time:16698ms step_avg:93.29ms
step:180/1750 train_time:16792ms step_avg:93.29ms
step:181/1750 train_time:16886ms step_avg:93.29ms
step:182/1750 train_time:16979ms step_avg:93.29ms
step:183/1750 train_time:17075ms step_avg:93.31ms
step:184/1750 train_time:17169ms step_avg:93.31ms
step:185/1750 train_time:17262ms step_avg:93.31ms
step:186/1750 train_time:17357ms step_avg:93.31ms
step:187/1750 train_time:17450ms step_avg:93.32ms
step:188/1750 train_time:17544ms step_avg:93.32ms
step:189/1750 train_time:17638ms step_avg:93.32ms
step:190/1750 train_time:17731ms step_avg:93.32ms
step:191/1750 train_time:17825ms step_avg:93.33ms
step:192/1750 train_time:17919ms step_avg:93.33ms
step:193/1750 train_time:18012ms step_avg:93.33ms
step:194/1750 train_time:18107ms step_avg:93.33ms
step:195/1750 train_time:18201ms step_avg:93.34ms
step:196/1750 train_time:18294ms step_avg:93.34ms
step:197/1750 train_time:18388ms step_avg:93.34ms
step:198/1750 train_time:18482ms step_avg:93.34ms
step:199/1750 train_time:18575ms step_avg:93.34ms
step:200/1750 train_time:18669ms step_avg:93.35ms
step:201/1750 train_time:18763ms step_avg:93.35ms
step:202/1750 train_time:18857ms step_avg:93.35ms
step:203/1750 train_time:18951ms step_avg:93.35ms
step:204/1750 train_time:19044ms step_avg:93.36ms
step:205/1750 train_time:19138ms step_avg:93.36ms
step:206/1750 train_time:19232ms step_avg:93.36ms
step:207/1750 train_time:19326ms step_avg:93.36ms
step:208/1750 train_time:19419ms step_avg:93.36ms
step:209/1750 train_time:19512ms step_avg:93.36ms
step:210/1750 train_time:19606ms step_avg:93.36ms
step:211/1750 train_time:19700ms step_avg:93.36ms
step:212/1750 train_time:19794ms step_avg:93.37ms
step:213/1750 train_time:19888ms step_avg:93.37ms
step:214/1750 train_time:19983ms step_avg:93.38ms
step:215/1750 train_time:20077ms step_avg:93.38ms
step:216/1750 train_time:20170ms step_avg:93.38ms
step:217/1750 train_time:20265ms step_avg:93.39ms
step:218/1750 train_time:20359ms step_avg:93.39ms
step:219/1750 train_time:20453ms step_avg:93.39ms
step:220/1750 train_time:20546ms step_avg:93.39ms
step:221/1750 train_time:20640ms step_avg:93.39ms
step:222/1750 train_time:20733ms step_avg:93.39ms
step:223/1750 train_time:20828ms step_avg:93.40ms
step:224/1750 train_time:20923ms step_avg:93.40ms
step:225/1750 train_time:21016ms step_avg:93.40ms
step:226/1750 train_time:21109ms step_avg:93.40ms
step:227/1750 train_time:21204ms step_avg:93.41ms
step:228/1750 train_time:21298ms step_avg:93.41ms
step:229/1750 train_time:21392ms step_avg:93.41ms
step:230/1750 train_time:21485ms step_avg:93.41ms
step:231/1750 train_time:21578ms step_avg:93.41ms
step:232/1750 train_time:21672ms step_avg:93.41ms
step:233/1750 train_time:21766ms step_avg:93.42ms
step:234/1750 train_time:21859ms step_avg:93.42ms
step:235/1750 train_time:21953ms step_avg:93.42ms
step:236/1750 train_time:22047ms step_avg:93.42ms
step:237/1750 train_time:22140ms step_avg:93.42ms
step:238/1750 train_time:22234ms step_avg:93.42ms
step:239/1750 train_time:22329ms step_avg:93.42ms
step:240/1750 train_time:22422ms step_avg:93.43ms
step:241/1750 train_time:22516ms step_avg:93.43ms
step:242/1750 train_time:22610ms step_avg:93.43ms
step:243/1750 train_time:22704ms step_avg:93.43ms
step:244/1750 train_time:22797ms step_avg:93.43ms
step:245/1750 train_time:22891ms step_avg:93.43ms
step:246/1750 train_time:22985ms step_avg:93.43ms
step:247/1750 train_time:23078ms step_avg:93.44ms
step:248/1750 train_time:23174ms step_avg:93.44ms
step:249/1750 train_time:23268ms step_avg:93.45ms
step:250/1750 train_time:23361ms step_avg:93.45ms
step:250/1750 val_loss:4.0987 train_time:23450ms step_avg:93.80ms
step:251/1750 train_time:23477ms step_avg:93.53ms
step:252/1750 train_time:23556ms step_avg:93.47ms
step:253/1750 train_time:23653ms step_avg:93.49ms
step:254/1750 train_time:23748ms step_avg:93.50ms
step:255/1750 train_time:23842ms step_avg:93.50ms
step:256/1750 train_time:23935ms step_avg:93.49ms
step:257/1750 train_time:24027ms step_avg:93.49ms
step:258/1750 train_time:24120ms step_avg:93.49ms
step:259/1750 train_time:24213ms step_avg:93.49ms
step:260/1750 train_time:24306ms step_avg:93.48ms
step:261/1750 train_time:24399ms step_avg:93.48ms
step:262/1750 train_time:24493ms step_avg:93.48ms
step:263/1750 train_time:24589ms step_avg:93.49ms
step:264/1750 train_time:24684ms step_avg:93.50ms
step:265/1750 train_time:24779ms step_avg:93.51ms
step:266/1750 train_time:24874ms step_avg:93.51ms
step:267/1750 train_time:24968ms step_avg:93.51ms
step:268/1750 train_time:25062ms step_avg:93.51ms
step:269/1750 train_time:25156ms step_avg:93.52ms
step:270/1750 train_time:25249ms step_avg:93.51ms
step:271/1750 train_time:25342ms step_avg:93.51ms
step:272/1750 train_time:25436ms step_avg:93.52ms
step:273/1750 train_time:25531ms step_avg:93.52ms
step:274/1750 train_time:25625ms step_avg:93.52ms
step:275/1750 train_time:25720ms step_avg:93.53ms
step:276/1750 train_time:25814ms step_avg:93.53ms
step:277/1750 train_time:25909ms step_avg:93.53ms
step:278/1750 train_time:26003ms step_avg:93.53ms
step:279/1750 train_time:26098ms step_avg:93.54ms
step:280/1750 train_time:26192ms step_avg:93.54ms
step:281/1750 train_time:26286ms step_avg:93.54ms
step:282/1750 train_time:26379ms step_avg:93.54ms
step:283/1750 train_time:26474ms step_avg:93.55ms
step:284/1750 train_time:26567ms step_avg:93.55ms
step:285/1750 train_time:26662ms step_avg:93.55ms
step:286/1750 train_time:26757ms step_avg:93.56ms
step:287/1750 train_time:26851ms step_avg:93.56ms
step:288/1750 train_time:26945ms step_avg:93.56ms
step:289/1750 train_time:27039ms step_avg:93.56ms
step:290/1750 train_time:27133ms step_avg:93.56ms
step:291/1750 train_time:27226ms step_avg:93.56ms
step:292/1750 train_time:27320ms step_avg:93.56ms
step:293/1750 train_time:27415ms step_avg:93.57ms
step:294/1750 train_time:27509ms step_avg:93.57ms
step:295/1750 train_time:27603ms step_avg:93.57ms
step:296/1750 train_time:27698ms step_avg:93.57ms
step:297/1750 train_time:27793ms step_avg:93.58ms
step:298/1750 train_time:27886ms step_avg:93.58ms
step:299/1750 train_time:27980ms step_avg:93.58ms
step:300/1750 train_time:28074ms step_avg:93.58ms
step:301/1750 train_time:28168ms step_avg:93.58ms
step:302/1750 train_time:28262ms step_avg:93.58ms
step:303/1750 train_time:28356ms step_avg:93.59ms
step:304/1750 train_time:28450ms step_avg:93.59ms
step:305/1750 train_time:28544ms step_avg:93.59ms
step:306/1750 train_time:28639ms step_avg:93.59ms
step:307/1750 train_time:28733ms step_avg:93.59ms
step:308/1750 train_time:28827ms step_avg:93.59ms
step:309/1750 train_time:28921ms step_avg:93.60ms
step:310/1750 train_time:29016ms step_avg:93.60ms
step:311/1750 train_time:29110ms step_avg:93.60ms
step:312/1750 train_time:29204ms step_avg:93.60ms
step:313/1750 train_time:29299ms step_avg:93.61ms
step:314/1750 train_time:29393ms step_avg:93.61ms
step:315/1750 train_time:29487ms step_avg:93.61ms
step:316/1750 train_time:29581ms step_avg:93.61ms
step:317/1750 train_time:29675ms step_avg:93.61ms
step:318/1750 train_time:29770ms step_avg:93.62ms
step:319/1750 train_time:29864ms step_avg:93.62ms
step:320/1750 train_time:29958ms step_avg:93.62ms
step:321/1750 train_time:30052ms step_avg:93.62ms
step:322/1750 train_time:30147ms step_avg:93.62ms
step:323/1750 train_time:30241ms step_avg:93.63ms
step:324/1750 train_time:30336ms step_avg:93.63ms
step:325/1750 train_time:30430ms step_avg:93.63ms
step:326/1750 train_time:30525ms step_avg:93.63ms
step:327/1750 train_time:30618ms step_avg:93.63ms
step:328/1750 train_time:30712ms step_avg:93.63ms
step:329/1750 train_time:30806ms step_avg:93.64ms
step:330/1750 train_time:30900ms step_avg:93.64ms
step:331/1750 train_time:30995ms step_avg:93.64ms
step:332/1750 train_time:31089ms step_avg:93.64ms
step:333/1750 train_time:31184ms step_avg:93.65ms
step:334/1750 train_time:31279ms step_avg:93.65ms
step:335/1750 train_time:31373ms step_avg:93.65ms
step:336/1750 train_time:31467ms step_avg:93.65ms
step:337/1750 train_time:31561ms step_avg:93.65ms
step:338/1750 train_time:31655ms step_avg:93.65ms
step:339/1750 train_time:31750ms step_avg:93.66ms
step:340/1750 train_time:31843ms step_avg:93.66ms
step:341/1750 train_time:31937ms step_avg:93.66ms
step:342/1750 train_time:32031ms step_avg:93.66ms
step:343/1750 train_time:32125ms step_avg:93.66ms
step:344/1750 train_time:32220ms step_avg:93.66ms
step:345/1750 train_time:32314ms step_avg:93.66ms
step:346/1750 train_time:32408ms step_avg:93.67ms
step:347/1750 train_time:32502ms step_avg:93.67ms
step:348/1750 train_time:32597ms step_avg:93.67ms
step:349/1750 train_time:32691ms step_avg:93.67ms
step:350/1750 train_time:32785ms step_avg:93.67ms
step:351/1750 train_time:32879ms step_avg:93.67ms
step:352/1750 train_time:32973ms step_avg:93.67ms
step:353/1750 train_time:33066ms step_avg:93.67ms
step:354/1750 train_time:33161ms step_avg:93.67ms
step:355/1750 train_time:33255ms step_avg:93.68ms
step:356/1750 train_time:33350ms step_avg:93.68ms
step:357/1750 train_time:33443ms step_avg:93.68ms
step:358/1750 train_time:33538ms step_avg:93.68ms
step:359/1750 train_time:33631ms step_avg:93.68ms
step:360/1750 train_time:33725ms step_avg:93.68ms
step:361/1750 train_time:33819ms step_avg:93.68ms
step:362/1750 train_time:33913ms step_avg:93.68ms
step:363/1750 train_time:34007ms step_avg:93.68ms
step:364/1750 train_time:34101ms step_avg:93.68ms
step:365/1750 train_time:34196ms step_avg:93.69ms
step:366/1750 train_time:34291ms step_avg:93.69ms
step:367/1750 train_time:34384ms step_avg:93.69ms
step:368/1750 train_time:34479ms step_avg:93.69ms
step:369/1750 train_time:34573ms step_avg:93.69ms
step:370/1750 train_time:34667ms step_avg:93.69ms
step:371/1750 train_time:34761ms step_avg:93.70ms
step:372/1750 train_time:34855ms step_avg:93.70ms
step:373/1750 train_time:34949ms step_avg:93.70ms
step:374/1750 train_time:35043ms step_avg:93.70ms
step:375/1750 train_time:35137ms step_avg:93.70ms
step:375/1750 val_loss:3.9028 train_time:35226ms step_avg:93.94ms
step:376/1750 train_time:35256ms step_avg:93.77ms
step:377/1750 train_time:35333ms step_avg:93.72ms
step:378/1750 train_time:35431ms step_avg:93.73ms
step:379/1750 train_time:35526ms step_avg:93.74ms
step:380/1750 train_time:35619ms step_avg:93.73ms
step:381/1750 train_time:35713ms step_avg:93.73ms
step:382/1750 train_time:35806ms step_avg:93.73ms
step:383/1750 train_time:35899ms step_avg:93.73ms
step:384/1750 train_time:35993ms step_avg:93.73ms
step:385/1750 train_time:36086ms step_avg:93.73ms
step:386/1750 train_time:36180ms step_avg:93.73ms
step:387/1750 train_time:36275ms step_avg:93.73ms
step:388/1750 train_time:36370ms step_avg:93.74ms
step:389/1750 train_time:36465ms step_avg:93.74ms
step:390/1750 train_time:36559ms step_avg:93.74ms
step:391/1750 train_time:36656ms step_avg:93.75ms
step:392/1750 train_time:36752ms step_avg:93.76ms
step:393/1750 train_time:36848ms step_avg:93.76ms
step:394/1750 train_time:36944ms step_avg:93.77ms
step:395/1750 train_time:37040ms step_avg:93.77ms
step:396/1750 train_time:37135ms step_avg:93.78ms
step:397/1750 train_time:37232ms step_avg:93.78ms
step:398/1750 train_time:37329ms step_avg:93.79ms
step:399/1750 train_time:37426ms step_avg:93.80ms
step:400/1750 train_time:37522ms step_avg:93.80ms
step:401/1750 train_time:37618ms step_avg:93.81ms
step:402/1750 train_time:37714ms step_avg:93.82ms
step:403/1750 train_time:37809ms step_avg:93.82ms
step:404/1750 train_time:37905ms step_avg:93.82ms
step:405/1750 train_time:38001ms step_avg:93.83ms
step:406/1750 train_time:38097ms step_avg:93.84ms
step:407/1750 train_time:38194ms step_avg:93.84ms
step:408/1750 train_time:38290ms step_avg:93.85ms
step:409/1750 train_time:38386ms step_avg:93.85ms
step:410/1750 train_time:38483ms step_avg:93.86ms
step:411/1750 train_time:38581ms step_avg:93.87ms
step:412/1750 train_time:38678ms step_avg:93.88ms
step:413/1750 train_time:38774ms step_avg:93.88ms
step:414/1750 train_time:38869ms step_avg:93.89ms
step:415/1750 train_time:38965ms step_avg:93.89ms
step:416/1750 train_time:39061ms step_avg:93.90ms
step:417/1750 train_time:39157ms step_avg:93.90ms
step:418/1750 train_time:39253ms step_avg:93.91ms
step:419/1750 train_time:39349ms step_avg:93.91ms
step:420/1750 train_time:39445ms step_avg:93.92ms
step:421/1750 train_time:39542ms step_avg:93.92ms
step:422/1750 train_time:39640ms step_avg:93.93ms
step:423/1750 train_time:39735ms step_avg:93.94ms
step:424/1750 train_time:39831ms step_avg:93.94ms
step:425/1750 train_time:39928ms step_avg:93.95ms
step:426/1750 train_time:40024ms step_avg:93.95ms
step:427/1750 train_time:40120ms step_avg:93.96ms
step:428/1750 train_time:40215ms step_avg:93.96ms
step:429/1750 train_time:40312ms step_avg:93.97ms
step:430/1750 train_time:40408ms step_avg:93.97ms
step:431/1750 train_time:40506ms step_avg:93.98ms
step:432/1750 train_time:40603ms step_avg:93.99ms
step:433/1750 train_time:40699ms step_avg:93.99ms
step:434/1750 train_time:40795ms step_avg:94.00ms
step:435/1750 train_time:40891ms step_avg:94.00ms
step:436/1750 train_time:40988ms step_avg:94.01ms
step:437/1750 train_time:41083ms step_avg:94.01ms
step:438/1750 train_time:41179ms step_avg:94.02ms
step:439/1750 train_time:41275ms step_avg:94.02ms
step:440/1750 train_time:41370ms step_avg:94.02ms
step:441/1750 train_time:41467ms step_avg:94.03ms
step:442/1750 train_time:41562ms step_avg:94.03ms
step:443/1750 train_time:41658ms step_avg:94.04ms
step:444/1750 train_time:41755ms step_avg:94.04ms
step:445/1750 train_time:41851ms step_avg:94.05ms
step:446/1750 train_time:41948ms step_avg:94.05ms
step:447/1750 train_time:42045ms step_avg:94.06ms
step:448/1750 train_time:42141ms step_avg:94.07ms
step:449/1750 train_time:42237ms step_avg:94.07ms
step:450/1750 train_time:42333ms step_avg:94.07ms
step:451/1750 train_time:42428ms step_avg:94.08ms
step:452/1750 train_time:42524ms step_avg:94.08ms
step:453/1750 train_time:42621ms step_avg:94.09ms
step:454/1750 train_time:42717ms step_avg:94.09ms
step:455/1750 train_time:42813ms step_avg:94.10ms
step:456/1750 train_time:42909ms step_avg:94.10ms
step:457/1750 train_time:43005ms step_avg:94.10ms
step:458/1750 train_time:43102ms step_avg:94.11ms
step:459/1750 train_time:43198ms step_avg:94.11ms
step:460/1750 train_time:43295ms step_avg:94.12ms
step:461/1750 train_time:43391ms step_avg:94.12ms
step:462/1750 train_time:43487ms step_avg:94.13ms
step:463/1750 train_time:43582ms step_avg:94.13ms
step:464/1750 train_time:43679ms step_avg:94.14ms
step:465/1750 train_time:43776ms step_avg:94.14ms
step:466/1750 train_time:43872ms step_avg:94.15ms
step:467/1750 train_time:43968ms step_avg:94.15ms
step:468/1750 train_time:44063ms step_avg:94.15ms
step:469/1750 train_time:44160ms step_avg:94.16ms
step:470/1750 train_time:44256ms step_avg:94.16ms
step:471/1750 train_time:44352ms step_avg:94.17ms
step:472/1750 train_time:44448ms step_avg:94.17ms
step:473/1750 train_time:44544ms step_avg:94.17ms
step:474/1750 train_time:44641ms step_avg:94.18ms
step:475/1750 train_time:44737ms step_avg:94.18ms
step:476/1750 train_time:44833ms step_avg:94.19ms
step:477/1750 train_time:44929ms step_avg:94.19ms
step:478/1750 train_time:45025ms step_avg:94.20ms
step:479/1750 train_time:45121ms step_avg:94.20ms
step:480/1750 train_time:45218ms step_avg:94.20ms
step:481/1750 train_time:45314ms step_avg:94.21ms
step:482/1750 train_time:45410ms step_avg:94.21ms
step:483/1750 train_time:45507ms step_avg:94.22ms
step:484/1750 train_time:45603ms step_avg:94.22ms
step:485/1750 train_time:45701ms step_avg:94.23ms
step:486/1750 train_time:45797ms step_avg:94.23ms
step:487/1750 train_time:45893ms step_avg:94.24ms
step:488/1750 train_time:45989ms step_avg:94.24ms
step:489/1750 train_time:46085ms step_avg:94.24ms
step:490/1750 train_time:46181ms step_avg:94.25ms
step:491/1750 train_time:46277ms step_avg:94.25ms
step:492/1750 train_time:46373ms step_avg:94.25ms
step:493/1750 train_time:46469ms step_avg:94.26ms
step:494/1750 train_time:46564ms step_avg:94.26ms
step:495/1750 train_time:46661ms step_avg:94.26ms
step:496/1750 train_time:46757ms step_avg:94.27ms
step:497/1750 train_time:46853ms step_avg:94.27ms
step:498/1750 train_time:46948ms step_avg:94.27ms
step:499/1750 train_time:47044ms step_avg:94.28ms
step:500/1750 train_time:47140ms step_avg:94.28ms
step:500/1750 val_loss:3.7536 train_time:47231ms step_avg:94.46ms
step:501/1750 train_time:47258ms step_avg:94.33ms
step:502/1750 train_time:47340ms step_avg:94.30ms
step:503/1750 train_time:47438ms step_avg:94.31ms
step:504/1750 train_time:47535ms step_avg:94.32ms
step:505/1750 train_time:47631ms step_avg:94.32ms
step:506/1750 train_time:47728ms step_avg:94.32ms
step:507/1750 train_time:47824ms step_avg:94.33ms
step:508/1750 train_time:47919ms step_avg:94.33ms
step:509/1750 train_time:48015ms step_avg:94.33ms
step:510/1750 train_time:48111ms step_avg:94.33ms
step:511/1750 train_time:48206ms step_avg:94.34ms
step:512/1750 train_time:48304ms step_avg:94.34ms
step:513/1750 train_time:48401ms step_avg:94.35ms
step:514/1750 train_time:48498ms step_avg:94.35ms
step:515/1750 train_time:48595ms step_avg:94.36ms
step:516/1750 train_time:48691ms step_avg:94.36ms
step:517/1750 train_time:48788ms step_avg:94.37ms
step:518/1750 train_time:48884ms step_avg:94.37ms
step:519/1750 train_time:48981ms step_avg:94.38ms
step:520/1750 train_time:49076ms step_avg:94.38ms
step:521/1750 train_time:49172ms step_avg:94.38ms
step:522/1750 train_time:49269ms step_avg:94.39ms
step:523/1750 train_time:49366ms step_avg:94.39ms
step:524/1750 train_time:49463ms step_avg:94.39ms
step:525/1750 train_time:49560ms step_avg:94.40ms
step:526/1750 train_time:49657ms step_avg:94.40ms
step:527/1750 train_time:49753ms step_avg:94.41ms
step:528/1750 train_time:49849ms step_avg:94.41ms
step:529/1750 train_time:49945ms step_avg:94.41ms
step:530/1750 train_time:50042ms step_avg:94.42ms
step:531/1750 train_time:50139ms step_avg:94.42ms
step:532/1750 train_time:50236ms step_avg:94.43ms
step:533/1750 train_time:50333ms step_avg:94.43ms
step:534/1750 train_time:50430ms step_avg:94.44ms
step:535/1750 train_time:50526ms step_avg:94.44ms
step:536/1750 train_time:50623ms step_avg:94.45ms
step:537/1750 train_time:50720ms step_avg:94.45ms
step:538/1750 train_time:50817ms step_avg:94.46ms
step:539/1750 train_time:50914ms step_avg:94.46ms
step:540/1750 train_time:51011ms step_avg:94.46ms
step:541/1750 train_time:51107ms step_avg:94.47ms
step:542/1750 train_time:51204ms step_avg:94.47ms
step:543/1750 train_time:51300ms step_avg:94.48ms
step:544/1750 train_time:51398ms step_avg:94.48ms
step:545/1750 train_time:51495ms step_avg:94.49ms
step:546/1750 train_time:51592ms step_avg:94.49ms
step:547/1750 train_time:51689ms step_avg:94.50ms
step:548/1750 train_time:51786ms step_avg:94.50ms
step:549/1750 train_time:51883ms step_avg:94.50ms
step:550/1750 train_time:51980ms step_avg:94.51ms
step:551/1750 train_time:52077ms step_avg:94.51ms
step:552/1750 train_time:52174ms step_avg:94.52ms
step:553/1750 train_time:52271ms step_avg:94.52ms
step:554/1750 train_time:52367ms step_avg:94.52ms
step:555/1750 train_time:52463ms step_avg:94.53ms
step:556/1750 train_time:52560ms step_avg:94.53ms
step:557/1750 train_time:52658ms step_avg:94.54ms
step:558/1750 train_time:52755ms step_avg:94.54ms
step:559/1750 train_time:52852ms step_avg:94.55ms
step:560/1750 train_time:52949ms step_avg:94.55ms
step:561/1750 train_time:53045ms step_avg:94.55ms
step:562/1750 train_time:53141ms step_avg:94.56ms
step:563/1750 train_time:53238ms step_avg:94.56ms
step:564/1750 train_time:53335ms step_avg:94.57ms
step:565/1750 train_time:53431ms step_avg:94.57ms
step:566/1750 train_time:53527ms step_avg:94.57ms
step:567/1750 train_time:53624ms step_avg:94.57ms
step:568/1750 train_time:53722ms step_avg:94.58ms
step:569/1750 train_time:53819ms step_avg:94.59ms
step:570/1750 train_time:53917ms step_avg:94.59ms
step:571/1750 train_time:54013ms step_avg:94.59ms
step:572/1750 train_time:54110ms step_avg:94.60ms
step:573/1750 train_time:54206ms step_avg:94.60ms
step:574/1750 train_time:54303ms step_avg:94.60ms
step:575/1750 train_time:54399ms step_avg:94.61ms
step:576/1750 train_time:54496ms step_avg:94.61ms
step:577/1750 train_time:54593ms step_avg:94.61ms
step:578/1750 train_time:54689ms step_avg:94.62ms
step:579/1750 train_time:54786ms step_avg:94.62ms
step:580/1750 train_time:54883ms step_avg:94.63ms
step:581/1750 train_time:54980ms step_avg:94.63ms
step:582/1750 train_time:55077ms step_avg:94.63ms
step:583/1750 train_time:55174ms step_avg:94.64ms
step:584/1750 train_time:55271ms step_avg:94.64ms
step:585/1750 train_time:55367ms step_avg:94.65ms
step:586/1750 train_time:55464ms step_avg:94.65ms
step:587/1750 train_time:55561ms step_avg:94.65ms
step:588/1750 train_time:55658ms step_avg:94.66ms
step:589/1750 train_time:55755ms step_avg:94.66ms
step:590/1750 train_time:55852ms step_avg:94.67ms
step:591/1750 train_time:55949ms step_avg:94.67ms
step:592/1750 train_time:56045ms step_avg:94.67ms
step:593/1750 train_time:56141ms step_avg:94.67ms
step:594/1750 train_time:56238ms step_avg:94.68ms
step:595/1750 train_time:56336ms step_avg:94.68ms
step:596/1750 train_time:56432ms step_avg:94.68ms
step:597/1750 train_time:56529ms step_avg:94.69ms
step:598/1750 train_time:56625ms step_avg:94.69ms
step:599/1750 train_time:56722ms step_avg:94.69ms
step:600/1750 train_time:56819ms step_avg:94.70ms
step:601/1750 train_time:56915ms step_avg:94.70ms
step:602/1750 train_time:57012ms step_avg:94.70ms
step:603/1750 train_time:57109ms step_avg:94.71ms
step:604/1750 train_time:57206ms step_avg:94.71ms
step:605/1750 train_time:57303ms step_avg:94.72ms
step:606/1750 train_time:57399ms step_avg:94.72ms
step:607/1750 train_time:57496ms step_avg:94.72ms
step:608/1750 train_time:57592ms step_avg:94.72ms
step:609/1750 train_time:57690ms step_avg:94.73ms
step:610/1750 train_time:57786ms step_avg:94.73ms
step:611/1750 train_time:57883ms step_avg:94.73ms
step:612/1750 train_time:57979ms step_avg:94.74ms
step:613/1750 train_time:58076ms step_avg:94.74ms
step:614/1750 train_time:58173ms step_avg:94.74ms
step:615/1750 train_time:58271ms step_avg:94.75ms
step:616/1750 train_time:58367ms step_avg:94.75ms
step:617/1750 train_time:58463ms step_avg:94.75ms
step:618/1750 train_time:58559ms step_avg:94.76ms
step:619/1750 train_time:58656ms step_avg:94.76ms
step:620/1750 train_time:58753ms step_avg:94.76ms
step:621/1750 train_time:58850ms step_avg:94.77ms
step:622/1750 train_time:58946ms step_avg:94.77ms
step:623/1750 train_time:59042ms step_avg:94.77ms
step:624/1750 train_time:59139ms step_avg:94.77ms
step:625/1750 train_time:59238ms step_avg:94.78ms
step:625/1750 val_loss:3.6661 train_time:59329ms step_avg:94.93ms
step:626/1750 train_time:59356ms step_avg:94.82ms
step:627/1750 train_time:59439ms step_avg:94.80ms
step:628/1750 train_time:59540ms step_avg:94.81ms
step:629/1750 train_time:59637ms step_avg:94.81ms
step:630/1750 train_time:59734ms step_avg:94.82ms
step:631/1750 train_time:59830ms step_avg:94.82ms
step:632/1750 train_time:59926ms step_avg:94.82ms
step:633/1750 train_time:60022ms step_avg:94.82ms
step:634/1750 train_time:60118ms step_avg:94.82ms
step:635/1750 train_time:60214ms step_avg:94.83ms
step:636/1750 train_time:60311ms step_avg:94.83ms
step:637/1750 train_time:60409ms step_avg:94.83ms
step:638/1750 train_time:60506ms step_avg:94.84ms
step:639/1750 train_time:60604ms step_avg:94.84ms
step:640/1750 train_time:60702ms step_avg:94.85ms
step:641/1750 train_time:60799ms step_avg:94.85ms
step:642/1750 train_time:60895ms step_avg:94.85ms
step:643/1750 train_time:60992ms step_avg:94.86ms
step:644/1750 train_time:61089ms step_avg:94.86ms
step:645/1750 train_time:61184ms step_avg:94.86ms
step:646/1750 train_time:61281ms step_avg:94.86ms
step:647/1750 train_time:61379ms step_avg:94.87ms
step:648/1750 train_time:61476ms step_avg:94.87ms
step:649/1750 train_time:61573ms step_avg:94.87ms
step:650/1750 train_time:61670ms step_avg:94.88ms
step:651/1750 train_time:61769ms step_avg:94.88ms
step:652/1750 train_time:61867ms step_avg:94.89ms
step:653/1750 train_time:61965ms step_avg:94.89ms
step:654/1750 train_time:62064ms step_avg:94.90ms
step:655/1750 train_time:62162ms step_avg:94.90ms
step:656/1750 train_time:62261ms step_avg:94.91ms
step:657/1750 train_time:62360ms step_avg:94.92ms
step:658/1750 train_time:62458ms step_avg:94.92ms
step:659/1750 train_time:62557ms step_avg:94.93ms
step:660/1750 train_time:62656ms step_avg:94.93ms
step:661/1750 train_time:62755ms step_avg:94.94ms
step:662/1750 train_time:62854ms step_avg:94.95ms
step:663/1750 train_time:62952ms step_avg:94.95ms
step:664/1750 train_time:63050ms step_avg:94.96ms
step:665/1750 train_time:63149ms step_avg:94.96ms
step:666/1750 train_time:63248ms step_avg:94.97ms
step:667/1750 train_time:63347ms step_avg:94.97ms
step:668/1750 train_time:63446ms step_avg:94.98ms
step:669/1750 train_time:63544ms step_avg:94.98ms
step:670/1750 train_time:63642ms step_avg:94.99ms
step:671/1750 train_time:63741ms step_avg:94.99ms
step:672/1750 train_time:63839ms step_avg:95.00ms
step:673/1750 train_time:63938ms step_avg:95.00ms
step:674/1750 train_time:64037ms step_avg:95.01ms
step:675/1750 train_time:64136ms step_avg:95.02ms
step:676/1750 train_time:64235ms step_avg:95.02ms
step:677/1750 train_time:64334ms step_avg:95.03ms
step:678/1750 train_time:64432ms step_avg:95.03ms
step:679/1750 train_time:64530ms step_avg:95.04ms
step:680/1750 train_time:64628ms step_avg:95.04ms
step:681/1750 train_time:64727ms step_avg:95.05ms
step:682/1750 train_time:64825ms step_avg:95.05ms
step:683/1750 train_time:64925ms step_avg:95.06ms
step:684/1750 train_time:65023ms step_avg:95.06ms
step:685/1750 train_time:65123ms step_avg:95.07ms
step:686/1750 train_time:65222ms step_avg:95.08ms
step:687/1750 train_time:65321ms step_avg:95.08ms
step:688/1750 train_time:65421ms step_avg:95.09ms
step:689/1750 train_time:65520ms step_avg:95.09ms
step:690/1750 train_time:65619ms step_avg:95.10ms
step:691/1750 train_time:65717ms step_avg:95.10ms
step:692/1750 train_time:65816ms step_avg:95.11ms
step:693/1750 train_time:65916ms step_avg:95.12ms
step:694/1750 train_time:66015ms step_avg:95.12ms
step:695/1750 train_time:66114ms step_avg:95.13ms
step:696/1750 train_time:66214ms step_avg:95.13ms
step:697/1750 train_time:66312ms step_avg:95.14ms
step:698/1750 train_time:66410ms step_avg:95.14ms
step:699/1750 train_time:66508ms step_avg:95.15ms
step:700/1750 train_time:66606ms step_avg:95.15ms
step:701/1750 train_time:66705ms step_avg:95.16ms
step:702/1750 train_time:66803ms step_avg:95.16ms
step:703/1750 train_time:66902ms step_avg:95.17ms
step:704/1750 train_time:67001ms step_avg:95.17ms
step:705/1750 train_time:67100ms step_avg:95.18ms
step:706/1750 train_time:67199ms step_avg:95.18ms
step:707/1750 train_time:67298ms step_avg:95.19ms
step:708/1750 train_time:67396ms step_avg:95.19ms
step:709/1750 train_time:67495ms step_avg:95.20ms
step:710/1750 train_time:67594ms step_avg:95.20ms
step:711/1750 train_time:67693ms step_avg:95.21ms
step:712/1750 train_time:67792ms step_avg:95.21ms
step:713/1750 train_time:67890ms step_avg:95.22ms
step:714/1750 train_time:67988ms step_avg:95.22ms
step:715/1750 train_time:68087ms step_avg:95.23ms
step:716/1750 train_time:68186ms step_avg:95.23ms
step:717/1750 train_time:68284ms step_avg:95.24ms
step:718/1750 train_time:68382ms step_avg:95.24ms
step:719/1750 train_time:68481ms step_avg:95.24ms
step:720/1750 train_time:68581ms step_avg:95.25ms
step:721/1750 train_time:68680ms step_avg:95.26ms
step:722/1750 train_time:68779ms step_avg:95.26ms
step:723/1750 train_time:68878ms step_avg:95.27ms
step:724/1750 train_time:68977ms step_avg:95.27ms
step:725/1750 train_time:69075ms step_avg:95.28ms
step:726/1750 train_time:69174ms step_avg:95.28ms
step:727/1750 train_time:69273ms step_avg:95.29ms
step:728/1750 train_time:69371ms step_avg:95.29ms
step:729/1750 train_time:69468ms step_avg:95.29ms
step:730/1750 train_time:69566ms step_avg:95.30ms
step:731/1750 train_time:69664ms step_avg:95.30ms
step:732/1750 train_time:69763ms step_avg:95.30ms
step:733/1750 train_time:69862ms step_avg:95.31ms
step:734/1750 train_time:69960ms step_avg:95.31ms
step:735/1750 train_time:70059ms step_avg:95.32ms
step:736/1750 train_time:70158ms step_avg:95.32ms
step:737/1750 train_time:70257ms step_avg:95.33ms
step:738/1750 train_time:70356ms step_avg:95.33ms
step:739/1750 train_time:70455ms step_avg:95.34ms
step:740/1750 train_time:70553ms step_avg:95.34ms
step:741/1750 train_time:70651ms step_avg:95.35ms
step:742/1750 train_time:70749ms step_avg:95.35ms
step:743/1750 train_time:70849ms step_avg:95.35ms
step:744/1750 train_time:70947ms step_avg:95.36ms
step:745/1750 train_time:71046ms step_avg:95.36ms
step:746/1750 train_time:71145ms step_avg:95.37ms
step:747/1750 train_time:71243ms step_avg:95.37ms
step:748/1750 train_time:71342ms step_avg:95.38ms
step:749/1750 train_time:71440ms step_avg:95.38ms
step:750/1750 train_time:71539ms step_avg:95.38ms
step:750/1750 val_loss:3.6011 train_time:71632ms step_avg:95.51ms
step:751/1750 train_time:71659ms step_avg:95.42ms
step:752/1750 train_time:71744ms step_avg:95.40ms
step:753/1750 train_time:71844ms step_avg:95.41ms
step:754/1750 train_time:71942ms step_avg:95.41ms
step:755/1750 train_time:72040ms step_avg:95.42ms
step:756/1750 train_time:72138ms step_avg:95.42ms
step:757/1750 train_time:72236ms step_avg:95.42ms
step:758/1750 train_time:72334ms step_avg:95.43ms
step:759/1750 train_time:72431ms step_avg:95.43ms
step:760/1750 train_time:72528ms step_avg:95.43ms
step:761/1750 train_time:72626ms step_avg:95.44ms
step:762/1750 train_time:72726ms step_avg:95.44ms
step:763/1750 train_time:72825ms step_avg:95.45ms
step:764/1750 train_time:72923ms step_avg:95.45ms
step:765/1750 train_time:73022ms step_avg:95.45ms
step:766/1750 train_time:73120ms step_avg:95.46ms
step:767/1750 train_time:73218ms step_avg:95.46ms
step:768/1750 train_time:73315ms step_avg:95.46ms
step:769/1750 train_time:73414ms step_avg:95.47ms
step:770/1750 train_time:73512ms step_avg:95.47ms
step:771/1750 train_time:73610ms step_avg:95.47ms
step:772/1750 train_time:73709ms step_avg:95.48ms
step:773/1750 train_time:73807ms step_avg:95.48ms
step:774/1750 train_time:73906ms step_avg:95.49ms
step:775/1750 train_time:74003ms step_avg:95.49ms
step:776/1750 train_time:74102ms step_avg:95.49ms
step:777/1750 train_time:74200ms step_avg:95.50ms
step:778/1750 train_time:74299ms step_avg:95.50ms
step:779/1750 train_time:74398ms step_avg:95.50ms
step:780/1750 train_time:74496ms step_avg:95.51ms
step:781/1750 train_time:74595ms step_avg:95.51ms
step:782/1750 train_time:74695ms step_avg:95.52ms
step:783/1750 train_time:74793ms step_avg:95.52ms
step:784/1750 train_time:74893ms step_avg:95.53ms
step:785/1750 train_time:74993ms step_avg:95.53ms
step:786/1750 train_time:75092ms step_avg:95.54ms
step:787/1750 train_time:75191ms step_avg:95.54ms
step:788/1750 train_time:75290ms step_avg:95.55ms
step:789/1750 train_time:75388ms step_avg:95.55ms
step:790/1750 train_time:75488ms step_avg:95.55ms
step:791/1750 train_time:75586ms step_avg:95.56ms
step:792/1750 train_time:75685ms step_avg:95.56ms
step:793/1750 train_time:75784ms step_avg:95.57ms
step:794/1750 train_time:75883ms step_avg:95.57ms
step:795/1750 train_time:75982ms step_avg:95.57ms
step:796/1750 train_time:76080ms step_avg:95.58ms
step:797/1750 train_time:76179ms step_avg:95.58ms
step:798/1750 train_time:76278ms step_avg:95.59ms
step:799/1750 train_time:76377ms step_avg:95.59ms
step:800/1750 train_time:76476ms step_avg:95.59ms
step:801/1750 train_time:76575ms step_avg:95.60ms
step:802/1750 train_time:76674ms step_avg:95.60ms
step:803/1750 train_time:76773ms step_avg:95.61ms
step:804/1750 train_time:76872ms step_avg:95.61ms
step:805/1750 train_time:76972ms step_avg:95.62ms
step:806/1750 train_time:77070ms step_avg:95.62ms
step:807/1750 train_time:77170ms step_avg:95.63ms
step:808/1750 train_time:77270ms step_avg:95.63ms
step:809/1750 train_time:77371ms step_avg:95.64ms
step:810/1750 train_time:77470ms step_avg:95.64ms
step:811/1750 train_time:77570ms step_avg:95.65ms
step:812/1750 train_time:77668ms step_avg:95.65ms
step:813/1750 train_time:77767ms step_avg:95.65ms
step:814/1750 train_time:77866ms step_avg:95.66ms
step:815/1750 train_time:77965ms step_avg:95.66ms
step:816/1750 train_time:78064ms step_avg:95.67ms
step:817/1750 train_time:78163ms step_avg:95.67ms
step:818/1750 train_time:78261ms step_avg:95.67ms
step:819/1750 train_time:78360ms step_avg:95.68ms
step:820/1750 train_time:78458ms step_avg:95.68ms
step:821/1750 train_time:78557ms step_avg:95.68ms
step:822/1750 train_time:78655ms step_avg:95.69ms
step:823/1750 train_time:78754ms step_avg:95.69ms
step:824/1750 train_time:78852ms step_avg:95.69ms
step:825/1750 train_time:78952ms step_avg:95.70ms
step:826/1750 train_time:79051ms step_avg:95.70ms
step:827/1750 train_time:79150ms step_avg:95.71ms
step:828/1750 train_time:79248ms step_avg:95.71ms
step:829/1750 train_time:79348ms step_avg:95.72ms
step:830/1750 train_time:79448ms step_avg:95.72ms
step:831/1750 train_time:79547ms step_avg:95.72ms
step:832/1750 train_time:79646ms step_avg:95.73ms
step:833/1750 train_time:79745ms step_avg:95.73ms
step:834/1750 train_time:79844ms step_avg:95.74ms
step:835/1750 train_time:79943ms step_avg:95.74ms
step:836/1750 train_time:80041ms step_avg:95.74ms
step:837/1750 train_time:80140ms step_avg:95.75ms
step:838/1750 train_time:80240ms step_avg:95.75ms
step:839/1750 train_time:80339ms step_avg:95.76ms
step:840/1750 train_time:80437ms step_avg:95.76ms
step:841/1750 train_time:80537ms step_avg:95.76ms
step:842/1750 train_time:80635ms step_avg:95.77ms
step:843/1750 train_time:80735ms step_avg:95.77ms
step:844/1750 train_time:80834ms step_avg:95.77ms
step:845/1750 train_time:80934ms step_avg:95.78ms
step:846/1750 train_time:81034ms step_avg:95.78ms
step:847/1750 train_time:81133ms step_avg:95.79ms
step:848/1750 train_time:81232ms step_avg:95.79ms
step:849/1750 train_time:81331ms step_avg:95.80ms
step:850/1750 train_time:81429ms step_avg:95.80ms
step:851/1750 train_time:81529ms step_avg:95.80ms
step:852/1750 train_time:81628ms step_avg:95.81ms
step:853/1750 train_time:81727ms step_avg:95.81ms
step:854/1750 train_time:81825ms step_avg:95.81ms
step:855/1750 train_time:81924ms step_avg:95.82ms
step:856/1750 train_time:82022ms step_avg:95.82ms
step:857/1750 train_time:82120ms step_avg:95.82ms
step:858/1750 train_time:82219ms step_avg:95.83ms
step:859/1750 train_time:82318ms step_avg:95.83ms
step:860/1750 train_time:82417ms step_avg:95.83ms
step:861/1750 train_time:82517ms step_avg:95.84ms
step:862/1750 train_time:82615ms step_avg:95.84ms
step:863/1750 train_time:82715ms step_avg:95.85ms
step:864/1750 train_time:82815ms step_avg:95.85ms
step:865/1750 train_time:82914ms step_avg:95.85ms
step:866/1750 train_time:83014ms step_avg:95.86ms
step:867/1750 train_time:83112ms step_avg:95.86ms
step:868/1750 train_time:83210ms step_avg:95.86ms
step:869/1750 train_time:83310ms step_avg:95.87ms
step:870/1750 train_time:83408ms step_avg:95.87ms
step:871/1750 train_time:83508ms step_avg:95.88ms
step:872/1750 train_time:83606ms step_avg:95.88ms
step:873/1750 train_time:83706ms step_avg:95.88ms
step:874/1750 train_time:83806ms step_avg:95.89ms
step:875/1750 train_time:83905ms step_avg:95.89ms
step:875/1750 val_loss:3.5527 train_time:83998ms step_avg:96.00ms
step:876/1750 train_time:84026ms step_avg:95.92ms
step:877/1750 train_time:84113ms step_avg:95.91ms
step:878/1750 train_time:84213ms step_avg:95.91ms
step:879/1750 train_time:84312ms step_avg:95.92ms
step:880/1750 train_time:84410ms step_avg:95.92ms
step:881/1750 train_time:84509ms step_avg:95.92ms
step:882/1750 train_time:84607ms step_avg:95.93ms
step:883/1750 train_time:84705ms step_avg:95.93ms
step:884/1750 train_time:84803ms step_avg:95.93ms
step:885/1750 train_time:84901ms step_avg:95.93ms
step:886/1750 train_time:84999ms step_avg:95.94ms
step:887/1750 train_time:85099ms step_avg:95.94ms
step:888/1750 train_time:85199ms step_avg:95.94ms
step:889/1750 train_time:85299ms step_avg:95.95ms
step:890/1750 train_time:85399ms step_avg:95.95ms
step:891/1750 train_time:85498ms step_avg:95.96ms
step:892/1750 train_time:85597ms step_avg:95.96ms
step:893/1750 train_time:85696ms step_avg:95.96ms
step:894/1750 train_time:85794ms step_avg:95.97ms
step:895/1750 train_time:85892ms step_avg:95.97ms
step:896/1750 train_time:85990ms step_avg:95.97ms
step:897/1750 train_time:86089ms step_avg:95.97ms
step:898/1750 train_time:86188ms step_avg:95.98ms
step:899/1750 train_time:86288ms step_avg:95.98ms
step:900/1750 train_time:86388ms step_avg:95.99ms
step:901/1750 train_time:86487ms step_avg:95.99ms
step:902/1750 train_time:86587ms step_avg:95.99ms
step:903/1750 train_time:86686ms step_avg:96.00ms
step:904/1750 train_time:86784ms step_avg:96.00ms
step:905/1750 train_time:86883ms step_avg:96.00ms
step:906/1750 train_time:86982ms step_avg:96.01ms
step:907/1750 train_time:87081ms step_avg:96.01ms
step:908/1750 train_time:87180ms step_avg:96.01ms
step:909/1750 train_time:87279ms step_avg:96.02ms
step:910/1750 train_time:87380ms step_avg:96.02ms
step:911/1750 train_time:87480ms step_avg:96.03ms
step:912/1750 train_time:87581ms step_avg:96.03ms
step:913/1750 train_time:87681ms step_avg:96.04ms
step:914/1750 train_time:87781ms step_avg:96.04ms
step:915/1750 train_time:87881ms step_avg:96.05ms
step:916/1750 train_time:87981ms step_avg:96.05ms
step:917/1750 train_time:88081ms step_avg:96.05ms
step:918/1750 train_time:88182ms step_avg:96.06ms
step:919/1750 train_time:88283ms step_avg:96.06ms
step:920/1750 train_time:88383ms step_avg:96.07ms
step:921/1750 train_time:88484ms step_avg:96.07ms
step:922/1750 train_time:88584ms step_avg:96.08ms
step:923/1750 train_time:88684ms step_avg:96.08ms
step:924/1750 train_time:88785ms step_avg:96.09ms
step:925/1750 train_time:88884ms step_avg:96.09ms
step:926/1750 train_time:88984ms step_avg:96.10ms
step:927/1750 train_time:89084ms step_avg:96.10ms
step:928/1750 train_time:89185ms step_avg:96.10ms
step:929/1750 train_time:89285ms step_avg:96.11ms
step:930/1750 train_time:89386ms step_avg:96.11ms
step:931/1750 train_time:89487ms step_avg:96.12ms
step:932/1750 train_time:89588ms step_avg:96.12ms
step:933/1750 train_time:89688ms step_avg:96.13ms
step:934/1750 train_time:89789ms step_avg:96.13ms
step:935/1750 train_time:89890ms step_avg:96.14ms
step:936/1750 train_time:89990ms step_avg:96.14ms
step:937/1750 train_time:90091ms step_avg:96.15ms
step:938/1750 train_time:90192ms step_avg:96.15ms
step:939/1750 train_time:90292ms step_avg:96.16ms
step:940/1750 train_time:90393ms step_avg:96.16ms
step:941/1750 train_time:90494ms step_avg:96.17ms
step:942/1750 train_time:90594ms step_avg:96.17ms
step:943/1750 train_time:90694ms step_avg:96.18ms
step:944/1750 train_time:90794ms step_avg:96.18ms
step:945/1750 train_time:90894ms step_avg:96.18ms
step:946/1750 train_time:90994ms step_avg:96.19ms
step:947/1750 train_time:91094ms step_avg:96.19ms
step:948/1750 train_time:91195ms step_avg:96.20ms
step:949/1750 train_time:91295ms step_avg:96.20ms
step:950/1750 train_time:91395ms step_avg:96.21ms
step:951/1750 train_time:91495ms step_avg:96.21ms
step:952/1750 train_time:91596ms step_avg:96.21ms
step:953/1750 train_time:91696ms step_avg:96.22ms
step:954/1750 train_time:91795ms step_avg:96.22ms
step:955/1750 train_time:91895ms step_avg:96.23ms
step:956/1750 train_time:91996ms step_avg:96.23ms
step:957/1750 train_time:92096ms step_avg:96.23ms
step:958/1750 train_time:92196ms step_avg:96.24ms
step:959/1750 train_time:92296ms step_avg:96.24ms
step:960/1750 train_time:92396ms step_avg:96.25ms
step:961/1750 train_time:92497ms step_avg:96.25ms
step:962/1750 train_time:92597ms step_avg:96.25ms
step:963/1750 train_time:92697ms step_avg:96.26ms
step:964/1750 train_time:92797ms step_avg:96.26ms
step:965/1750 train_time:92897ms step_avg:96.27ms
step:966/1750 train_time:92997ms step_avg:96.27ms
step:967/1750 train_time:93097ms step_avg:96.27ms
step:968/1750 train_time:93197ms step_avg:96.28ms
step:969/1750 train_time:93298ms step_avg:96.28ms
step:970/1750 train_time:93399ms step_avg:96.29ms
step:971/1750 train_time:93499ms step_avg:96.29ms
step:972/1750 train_time:93599ms step_avg:96.30ms
step:973/1750 train_time:93700ms step_avg:96.30ms
step:974/1750 train_time:93800ms step_avg:96.30ms
step:975/1750 train_time:93902ms step_avg:96.31ms
step:976/1750 train_time:94002ms step_avg:96.31ms
step:977/1750 train_time:94101ms step_avg:96.32ms
step:978/1750 train_time:94201ms step_avg:96.32ms
step:979/1750 train_time:94301ms step_avg:96.32ms
step:980/1750 train_time:94401ms step_avg:96.33ms
step:981/1750 train_time:94501ms step_avg:96.33ms
step:982/1750 train_time:94601ms step_avg:96.34ms
step:983/1750 train_time:94701ms step_avg:96.34ms
step:984/1750 train_time:94802ms step_avg:96.34ms
step:985/1750 train_time:94902ms step_avg:96.35ms
step:986/1750 train_time:95003ms step_avg:96.35ms
step:987/1750 train_time:95105ms step_avg:96.36ms
step:988/1750 train_time:95206ms step_avg:96.36ms
step:989/1750 train_time:95306ms step_avg:96.37ms
step:990/1750 train_time:95406ms step_avg:96.37ms
step:991/1750 train_time:95506ms step_avg:96.37ms
step:992/1750 train_time:95606ms step_avg:96.38ms
step:993/1750 train_time:95706ms step_avg:96.38ms
step:994/1750 train_time:95807ms step_avg:96.39ms
step:995/1750 train_time:95909ms step_avg:96.39ms
step:996/1750 train_time:96010ms step_avg:96.40ms
step:997/1750 train_time:96112ms step_avg:96.40ms
step:998/1750 train_time:96212ms step_avg:96.40ms
step:999/1750 train_time:96313ms step_avg:96.41ms
step:1000/1750 train_time:96413ms step_avg:96.41ms
step:1000/1750 val_loss:3.5091 train_time:96508ms step_avg:96.51ms
step:1001/1750 train_time:96539ms step_avg:96.44ms
step:1002/1750 train_time:96620ms step_avg:96.43ms
step:1003/1750 train_time:96721ms step_avg:96.43ms
step:1004/1750 train_time:96822ms step_avg:96.44ms
step:1005/1750 train_time:96922ms step_avg:96.44ms
step:1006/1750 train_time:97021ms step_avg:96.44ms
step:1007/1750 train_time:97121ms step_avg:96.45ms
step:1008/1750 train_time:97221ms step_avg:96.45ms
step:1009/1750 train_time:97320ms step_avg:96.45ms
step:1010/1750 train_time:97420ms step_avg:96.46ms
step:1011/1750 train_time:97522ms step_avg:96.46ms
step:1012/1750 train_time:97624ms step_avg:96.47ms
step:1013/1750 train_time:97726ms step_avg:96.47ms
step:1014/1750 train_time:97826ms step_avg:96.48ms
step:1015/1750 train_time:97926ms step_avg:96.48ms
step:1016/1750 train_time:98026ms step_avg:96.48ms
step:1017/1750 train_time:98126ms step_avg:96.49ms
step:1018/1750 train_time:98227ms step_avg:96.49ms
step:1019/1750 train_time:98326ms step_avg:96.49ms
step:1020/1750 train_time:98427ms step_avg:96.50ms
step:1021/1750 train_time:98527ms step_avg:96.50ms
step:1022/1750 train_time:98628ms step_avg:96.51ms
step:1023/1750 train_time:98729ms step_avg:96.51ms
step:1024/1750 train_time:98830ms step_avg:96.51ms
step:1025/1750 train_time:98930ms step_avg:96.52ms
step:1026/1750 train_time:99031ms step_avg:96.52ms
step:1027/1750 train_time:99131ms step_avg:96.52ms
step:1028/1750 train_time:99230ms step_avg:96.53ms
step:1029/1750 train_time:99331ms step_avg:96.53ms
step:1030/1750 train_time:99432ms step_avg:96.54ms
step:1031/1750 train_time:99533ms step_avg:96.54ms
step:1032/1750 train_time:99633ms step_avg:96.54ms
step:1033/1750 train_time:99733ms step_avg:96.55ms
step:1034/1750 train_time:99833ms step_avg:96.55ms
step:1035/1750 train_time:99933ms step_avg:96.55ms
step:1036/1750 train_time:100033ms step_avg:96.56ms
step:1037/1750 train_time:100134ms step_avg:96.56ms
step:1038/1750 train_time:100234ms step_avg:96.56ms
step:1039/1750 train_time:100335ms step_avg:96.57ms
step:1040/1750 train_time:100435ms step_avg:96.57ms
step:1041/1750 train_time:100536ms step_avg:96.58ms
step:1042/1750 train_time:100638ms step_avg:96.58ms
step:1043/1750 train_time:100739ms step_avg:96.59ms
step:1044/1750 train_time:100840ms step_avg:96.59ms
step:1045/1750 train_time:100941ms step_avg:96.59ms
step:1046/1750 train_time:101042ms step_avg:96.60ms
step:1047/1750 train_time:101144ms step_avg:96.60ms
step:1048/1750 train_time:101245ms step_avg:96.61ms
step:1049/1750 train_time:101346ms step_avg:96.61ms
step:1050/1750 train_time:101447ms step_avg:96.62ms
step:1051/1750 train_time:101548ms step_avg:96.62ms
step:1052/1750 train_time:101650ms step_avg:96.63ms
step:1053/1750 train_time:101750ms step_avg:96.63ms
step:1054/1750 train_time:101850ms step_avg:96.63ms
step:1055/1750 train_time:101951ms step_avg:96.64ms
step:1056/1750 train_time:102051ms step_avg:96.64ms
step:1057/1750 train_time:102152ms step_avg:96.64ms
step:1058/1750 train_time:102252ms step_avg:96.65ms
step:1059/1750 train_time:102353ms step_avg:96.65ms
step:1060/1750 train_time:102453ms step_avg:96.65ms
step:1061/1750 train_time:102554ms step_avg:96.66ms
step:1062/1750 train_time:102654ms step_avg:96.66ms
step:1063/1750 train_time:102754ms step_avg:96.66ms
step:1064/1750 train_time:102855ms step_avg:96.67ms
step:1065/1750 train_time:102957ms step_avg:96.67ms
step:1066/1750 train_time:103058ms step_avg:96.68ms
step:1067/1750 train_time:103159ms step_avg:96.68ms
step:1068/1750 train_time:103259ms step_avg:96.68ms
step:1069/1750 train_time:103360ms step_avg:96.69ms
step:1070/1750 train_time:103461ms step_avg:96.69ms
step:1071/1750 train_time:103561ms step_avg:96.70ms
step:1072/1750 train_time:103662ms step_avg:96.70ms
step:1073/1750 train_time:103762ms step_avg:96.70ms
step:1074/1750 train_time:103863ms step_avg:96.71ms
step:1075/1750 train_time:103965ms step_avg:96.71ms
step:1076/1750 train_time:104067ms step_avg:96.72ms
step:1077/1750 train_time:104167ms step_avg:96.72ms
step:1078/1750 train_time:104267ms step_avg:96.72ms
step:1079/1750 train_time:104368ms step_avg:96.73ms
step:1080/1750 train_time:104468ms step_avg:96.73ms
step:1081/1750 train_time:104569ms step_avg:96.73ms
step:1082/1750 train_time:104669ms step_avg:96.74ms
step:1083/1750 train_time:104769ms step_avg:96.74ms
step:1084/1750 train_time:104870ms step_avg:96.74ms
step:1085/1750 train_time:104970ms step_avg:96.75ms
step:1086/1750 train_time:105071ms step_avg:96.75ms
step:1087/1750 train_time:105171ms step_avg:96.75ms
step:1088/1750 train_time:105271ms step_avg:96.76ms
step:1089/1750 train_time:105372ms step_avg:96.76ms
step:1090/1750 train_time:105472ms step_avg:96.76ms
step:1091/1750 train_time:105572ms step_avg:96.77ms
step:1092/1750 train_time:105673ms step_avg:96.77ms
step:1093/1750 train_time:105772ms step_avg:96.77ms
step:1094/1750 train_time:105874ms step_avg:96.78ms
step:1095/1750 train_time:105974ms step_avg:96.78ms
step:1096/1750 train_time:106075ms step_avg:96.78ms
step:1097/1750 train_time:106175ms step_avg:96.79ms
step:1098/1750 train_time:106276ms step_avg:96.79ms
step:1099/1750 train_time:106377ms step_avg:96.79ms
step:1100/1750 train_time:106476ms step_avg:96.80ms
step:1101/1750 train_time:106577ms step_avg:96.80ms
step:1102/1750 train_time:106679ms step_avg:96.80ms
step:1103/1750 train_time:106780ms step_avg:96.81ms
step:1104/1750 train_time:106881ms step_avg:96.81ms
step:1105/1750 train_time:106980ms step_avg:96.81ms
step:1106/1750 train_time:107082ms step_avg:96.82ms
step:1107/1750 train_time:107183ms step_avg:96.82ms
step:1108/1750 train_time:107284ms step_avg:96.83ms
step:1109/1750 train_time:107385ms step_avg:96.83ms
step:1110/1750 train_time:107485ms step_avg:96.83ms
step:1111/1750 train_time:107586ms step_avg:96.84ms
step:1112/1750 train_time:107687ms step_avg:96.84ms
step:1113/1750 train_time:107789ms step_avg:96.85ms
step:1114/1750 train_time:107889ms step_avg:96.85ms
step:1115/1750 train_time:107989ms step_avg:96.85ms
step:1116/1750 train_time:108090ms step_avg:96.85ms
step:1117/1750 train_time:108191ms step_avg:96.86ms
step:1118/1750 train_time:108292ms step_avg:96.86ms
step:1119/1750 train_time:108392ms step_avg:96.86ms
step:1120/1750 train_time:108492ms step_avg:96.87ms
step:1121/1750 train_time:108593ms step_avg:96.87ms
step:1122/1750 train_time:108694ms step_avg:96.87ms
step:1123/1750 train_time:108794ms step_avg:96.88ms
step:1124/1750 train_time:108894ms step_avg:96.88ms
step:1125/1750 train_time:108995ms step_avg:96.88ms
step:1125/1750 val_loss:3.4566 train_time:109090ms step_avg:96.97ms
step:1126/1750 train_time:109117ms step_avg:96.91ms
step:1127/1750 train_time:109202ms step_avg:96.90ms
step:1128/1750 train_time:109304ms step_avg:96.90ms
step:1129/1750 train_time:109404ms step_avg:96.90ms
step:1130/1750 train_time:109504ms step_avg:96.91ms
step:1131/1750 train_time:109604ms step_avg:96.91ms
step:1132/1750 train_time:109704ms step_avg:96.91ms
step:1133/1750 train_time:109804ms step_avg:96.91ms
step:1134/1750 train_time:109903ms step_avg:96.92ms
step:1135/1750 train_time:110003ms step_avg:96.92ms
step:1136/1750 train_time:110105ms step_avg:96.92ms
step:1137/1750 train_time:110207ms step_avg:96.93ms
step:1138/1750 train_time:110308ms step_avg:96.93ms
step:1139/1750 train_time:110409ms step_avg:96.93ms
step:1140/1750 train_time:110509ms step_avg:96.94ms
step:1141/1750 train_time:110610ms step_avg:96.94ms
step:1142/1750 train_time:110711ms step_avg:96.94ms
step:1143/1750 train_time:110811ms step_avg:96.95ms
step:1144/1750 train_time:110911ms step_avg:96.95ms
step:1145/1750 train_time:111012ms step_avg:96.95ms
step:1146/1750 train_time:111113ms step_avg:96.96ms
step:1147/1750 train_time:111214ms step_avg:96.96ms
step:1148/1750 train_time:111316ms step_avg:96.97ms
step:1149/1750 train_time:111418ms step_avg:96.97ms
step:1150/1750 train_time:111518ms step_avg:96.97ms
step:1151/1750 train_time:111619ms step_avg:96.98ms
step:1152/1750 train_time:111719ms step_avg:96.98ms
step:1153/1750 train_time:111820ms step_avg:96.98ms
step:1154/1750 train_time:111921ms step_avg:96.98ms
step:1155/1750 train_time:112022ms step_avg:96.99ms
step:1156/1750 train_time:112122ms step_avg:96.99ms
step:1157/1750 train_time:112222ms step_avg:96.99ms
step:1158/1750 train_time:112323ms step_avg:97.00ms
step:1159/1750 train_time:112424ms step_avg:97.00ms
step:1160/1750 train_time:112525ms step_avg:97.00ms
step:1161/1750 train_time:112625ms step_avg:97.01ms
step:1162/1750 train_time:112726ms step_avg:97.01ms
step:1163/1750 train_time:112827ms step_avg:97.01ms
step:1164/1750 train_time:112927ms step_avg:97.02ms
step:1165/1750 train_time:113028ms step_avg:97.02ms
step:1166/1750 train_time:113129ms step_avg:97.02ms
step:1167/1750 train_time:113229ms step_avg:97.03ms
step:1168/1750 train_time:113330ms step_avg:97.03ms
step:1169/1750 train_time:113432ms step_avg:97.03ms
step:1170/1750 train_time:113535ms step_avg:97.04ms
step:1171/1750 train_time:113636ms step_avg:97.04ms
step:1172/1750 train_time:113740ms step_avg:97.05ms
step:1173/1750 train_time:113843ms step_avg:97.05ms
step:1174/1750 train_time:113944ms step_avg:97.06ms
step:1175/1750 train_time:114045ms step_avg:97.06ms
step:1176/1750 train_time:114146ms step_avg:97.06ms
step:1177/1750 train_time:114247ms step_avg:97.07ms
step:1178/1750 train_time:114349ms step_avg:97.07ms
step:1179/1750 train_time:114453ms step_avg:97.08ms
step:1180/1750 train_time:114555ms step_avg:97.08ms
step:1181/1750 train_time:114657ms step_avg:97.08ms
step:1182/1750 train_time:114760ms step_avg:97.09ms
step:1183/1750 train_time:114862ms step_avg:97.09ms
step:1184/1750 train_time:114965ms step_avg:97.10ms
step:1185/1750 train_time:115066ms step_avg:97.10ms
step:1186/1750 train_time:115168ms step_avg:97.11ms
step:1187/1750 train_time:115269ms step_avg:97.11ms
step:1188/1750 train_time:115370ms step_avg:97.11ms
step:1189/1750 train_time:115471ms step_avg:97.12ms
step:1190/1750 train_time:115574ms step_avg:97.12ms
step:1191/1750 train_time:115676ms step_avg:97.12ms
step:1192/1750 train_time:115777ms step_avg:97.13ms
step:1193/1750 train_time:115880ms step_avg:97.13ms
step:1194/1750 train_time:115983ms step_avg:97.14ms
step:1195/1750 train_time:116085ms step_avg:97.14ms
step:1196/1750 train_time:116185ms step_avg:97.14ms
step:1197/1750 train_time:116287ms step_avg:97.15ms
step:1198/1750 train_time:116389ms step_avg:97.15ms
step:1199/1750 train_time:116491ms step_avg:97.16ms
step:1200/1750 train_time:116592ms step_avg:97.16ms
step:1201/1750 train_time:116695ms step_avg:97.16ms
step:1202/1750 train_time:116799ms step_avg:97.17ms
step:1203/1750 train_time:116901ms step_avg:97.17ms
step:1204/1750 train_time:117002ms step_avg:97.18ms
step:1205/1750 train_time:117103ms step_avg:97.18ms
step:1206/1750 train_time:117205ms step_avg:97.18ms
step:1207/1750 train_time:117306ms step_avg:97.19ms
step:1208/1750 train_time:117407ms step_avg:97.19ms
step:1209/1750 train_time:117508ms step_avg:97.19ms
step:1210/1750 train_time:117610ms step_avg:97.20ms
step:1211/1750 train_time:117712ms step_avg:97.20ms
step:1212/1750 train_time:117815ms step_avg:97.21ms
step:1213/1750 train_time:117918ms step_avg:97.21ms
step:1214/1750 train_time:118019ms step_avg:97.22ms
step:1215/1750 train_time:118122ms step_avg:97.22ms
step:1216/1750 train_time:118224ms step_avg:97.22ms
step:1217/1750 train_time:118325ms step_avg:97.23ms
step:1218/1750 train_time:118428ms step_avg:97.23ms
step:1219/1750 train_time:118530ms step_avg:97.24ms
step:1220/1750 train_time:118631ms step_avg:97.24ms
step:1221/1750 train_time:118733ms step_avg:97.24ms
step:1222/1750 train_time:118836ms step_avg:97.25ms
step:1223/1750 train_time:118938ms step_avg:97.25ms
step:1224/1750 train_time:119040ms step_avg:97.25ms
step:1225/1750 train_time:119142ms step_avg:97.26ms
step:1226/1750 train_time:119243ms step_avg:97.26ms
step:1227/1750 train_time:119345ms step_avg:97.27ms
step:1228/1750 train_time:119447ms step_avg:97.27ms
step:1229/1750 train_time:119548ms step_avg:97.27ms
step:1230/1750 train_time:119650ms step_avg:97.28ms
step:1231/1750 train_time:119751ms step_avg:97.28ms
step:1232/1750 train_time:119855ms step_avg:97.28ms
step:1233/1750 train_time:119957ms step_avg:97.29ms
step:1234/1750 train_time:120058ms step_avg:97.29ms
step:1235/1750 train_time:120160ms step_avg:97.30ms
step:1236/1750 train_time:120263ms step_avg:97.30ms
step:1237/1750 train_time:120365ms step_avg:97.30ms
step:1238/1750 train_time:120466ms step_avg:97.31ms
step:1239/1750 train_time:120568ms step_avg:97.31ms
step:1240/1750 train_time:120669ms step_avg:97.31ms
step:1241/1750 train_time:120771ms step_avg:97.32ms
step:1242/1750 train_time:120872ms step_avg:97.32ms
step:1243/1750 train_time:120973ms step_avg:97.32ms
step:1244/1750 train_time:121076ms step_avg:97.33ms
step:1245/1750 train_time:121178ms step_avg:97.33ms
step:1246/1750 train_time:121281ms step_avg:97.34ms
step:1247/1750 train_time:121382ms step_avg:97.34ms
step:1248/1750 train_time:121485ms step_avg:97.34ms
step:1249/1750 train_time:121585ms step_avg:97.35ms
step:1250/1750 train_time:121687ms step_avg:97.35ms
step:1250/1750 val_loss:3.4101 train_time:121782ms step_avg:97.43ms
step:1251/1750 train_time:121809ms step_avg:97.37ms
step:1252/1750 train_time:121899ms step_avg:97.36ms
step:1253/1750 train_time:122000ms step_avg:97.37ms
step:1254/1750 train_time:122103ms step_avg:97.37ms
step:1255/1750 train_time:122204ms step_avg:97.37ms
step:1256/1750 train_time:122304ms step_avg:97.38ms
step:1257/1750 train_time:122406ms step_avg:97.38ms
step:1258/1750 train_time:122506ms step_avg:97.38ms
step:1259/1750 train_time:122607ms step_avg:97.38ms
step:1260/1750 train_time:122708ms step_avg:97.39ms
step:1261/1750 train_time:122811ms step_avg:97.39ms
step:1262/1750 train_time:122914ms step_avg:97.40ms
step:1263/1750 train_time:123017ms step_avg:97.40ms
step:1264/1750 train_time:123118ms step_avg:97.40ms
step:1265/1750 train_time:123220ms step_avg:97.41ms
step:1266/1750 train_time:123321ms step_avg:97.41ms
step:1267/1750 train_time:123422ms step_avg:97.41ms
step:1268/1750 train_time:123524ms step_avg:97.42ms
step:1269/1750 train_time:123626ms step_avg:97.42ms
step:1270/1750 train_time:123728ms step_avg:97.42ms
step:1271/1750 train_time:123832ms step_avg:97.43ms
step:1272/1750 train_time:123933ms step_avg:97.43ms
step:1273/1750 train_time:124034ms step_avg:97.43ms
step:1274/1750 train_time:124136ms step_avg:97.44ms
step:1275/1750 train_time:124237ms step_avg:97.44ms
step:1276/1750 train_time:124338ms step_avg:97.44ms
step:1277/1750 train_time:124440ms step_avg:97.45ms
step:1278/1750 train_time:124542ms step_avg:97.45ms
step:1279/1750 train_time:124645ms step_avg:97.45ms
step:1280/1750 train_time:124747ms step_avg:97.46ms
step:1281/1750 train_time:124848ms step_avg:97.46ms
step:1282/1750 train_time:124949ms step_avg:97.46ms
step:1283/1750 train_time:125050ms step_avg:97.47ms
step:1284/1750 train_time:125152ms step_avg:97.47ms
step:1285/1750 train_time:125253ms step_avg:97.47ms
step:1286/1750 train_time:125355ms step_avg:97.48ms
step:1287/1750 train_time:125456ms step_avg:97.48ms
step:1288/1750 train_time:125559ms step_avg:97.48ms
step:1289/1750 train_time:125661ms step_avg:97.49ms
step:1290/1750 train_time:125763ms step_avg:97.49ms
step:1291/1750 train_time:125865ms step_avg:97.49ms
step:1292/1750 train_time:125967ms step_avg:97.50ms
step:1293/1750 train_time:126068ms step_avg:97.50ms
step:1294/1750 train_time:126171ms step_avg:97.50ms
step:1295/1750 train_time:126273ms step_avg:97.51ms
step:1296/1750 train_time:126374ms step_avg:97.51ms
step:1297/1750 train_time:126476ms step_avg:97.51ms
step:1298/1750 train_time:126578ms step_avg:97.52ms
step:1299/1750 train_time:126680ms step_avg:97.52ms
step:1300/1750 train_time:126783ms step_avg:97.53ms
step:1301/1750 train_time:126885ms step_avg:97.53ms
step:1302/1750 train_time:126986ms step_avg:97.53ms
step:1303/1750 train_time:127088ms step_avg:97.54ms
step:1304/1750 train_time:127190ms step_avg:97.54ms
step:1305/1750 train_time:127292ms step_avg:97.54ms
step:1306/1750 train_time:127393ms step_avg:97.54ms
step:1307/1750 train_time:127495ms step_avg:97.55ms
step:1308/1750 train_time:127596ms step_avg:97.55ms
step:1309/1750 train_time:127698ms step_avg:97.55ms
step:1310/1750 train_time:127801ms step_avg:97.56ms
step:1311/1750 train_time:127903ms step_avg:97.56ms
step:1312/1750 train_time:128006ms step_avg:97.57ms
step:1313/1750 train_time:128109ms step_avg:97.57ms
step:1314/1750 train_time:128210ms step_avg:97.57ms
step:1315/1750 train_time:128312ms step_avg:97.58ms
step:1316/1750 train_time:128413ms step_avg:97.58ms
step:1317/1750 train_time:128514ms step_avg:97.58ms
step:1318/1750 train_time:128616ms step_avg:97.58ms
step:1319/1750 train_time:128718ms step_avg:97.59ms
step:1320/1750 train_time:128822ms step_avg:97.59ms
step:1321/1750 train_time:128924ms step_avg:97.60ms
step:1322/1750 train_time:129026ms step_avg:97.60ms
step:1323/1750 train_time:129128ms step_avg:97.60ms
step:1324/1750 train_time:129230ms step_avg:97.61ms
step:1325/1750 train_time:129332ms step_avg:97.61ms
step:1326/1750 train_time:129433ms step_avg:97.61ms
step:1327/1750 train_time:129535ms step_avg:97.62ms
step:1328/1750 train_time:129637ms step_avg:97.62ms
step:1329/1750 train_time:129739ms step_avg:97.62ms
step:1330/1750 train_time:129842ms step_avg:97.63ms
step:1331/1750 train_time:129945ms step_avg:97.63ms
step:1332/1750 train_time:130047ms step_avg:97.63ms
step:1333/1750 train_time:130148ms step_avg:97.64ms
step:1334/1750 train_time:130249ms step_avg:97.64ms
step:1335/1750 train_time:130351ms step_avg:97.64ms
step:1336/1750 train_time:130453ms step_avg:97.64ms
step:1337/1750 train_time:130556ms step_avg:97.65ms
step:1338/1750 train_time:130656ms step_avg:97.65ms
step:1339/1750 train_time:130759ms step_avg:97.65ms
step:1340/1750 train_time:130861ms step_avg:97.66ms
step:1341/1750 train_time:130963ms step_avg:97.66ms
step:1342/1750 train_time:131066ms step_avg:97.66ms
step:1343/1750 train_time:131168ms step_avg:97.67ms
step:1344/1750 train_time:131270ms step_avg:97.67ms
step:1345/1750 train_time:131371ms step_avg:97.67ms
step:1346/1750 train_time:131474ms step_avg:97.68ms
step:1347/1750 train_time:131576ms step_avg:97.68ms
step:1348/1750 train_time:131679ms step_avg:97.68ms
step:1349/1750 train_time:131780ms step_avg:97.69ms
step:1350/1750 train_time:131883ms step_avg:97.69ms
step:1351/1750 train_time:131985ms step_avg:97.69ms
step:1352/1750 train_time:132087ms step_avg:97.70ms
step:1353/1750 train_time:132188ms step_avg:97.70ms
step:1354/1750 train_time:132290ms step_avg:97.70ms
step:1355/1750 train_time:132391ms step_avg:97.71ms
step:1356/1750 train_time:132493ms step_avg:97.71ms
step:1357/1750 train_time:132594ms step_avg:97.71ms
step:1358/1750 train_time:132696ms step_avg:97.71ms
step:1359/1750 train_time:132799ms step_avg:97.72ms
step:1360/1750 train_time:132902ms step_avg:97.72ms
step:1361/1750 train_time:133003ms step_avg:97.72ms
step:1362/1750 train_time:133106ms step_avg:97.73ms
step:1363/1750 train_time:133209ms step_avg:97.73ms
step:1364/1750 train_time:133310ms step_avg:97.73ms
step:1365/1750 train_time:133411ms step_avg:97.74ms
step:1366/1750 train_time:133513ms step_avg:97.74ms
step:1367/1750 train_time:133614ms step_avg:97.74ms
step:1368/1750 train_time:133717ms step_avg:97.75ms
step:1369/1750 train_time:133819ms step_avg:97.75ms
step:1370/1750 train_time:133921ms step_avg:97.75ms
step:1371/1750 train_time:134022ms step_avg:97.75ms
step:1372/1750 train_time:134124ms step_avg:97.76ms
step:1373/1750 train_time:134227ms step_avg:97.76ms
step:1374/1750 train_time:134329ms step_avg:97.77ms
step:1375/1750 train_time:134432ms step_avg:97.77ms
step:1375/1750 val_loss:3.3696 train_time:134528ms step_avg:97.84ms
step:1376/1750 train_time:134555ms step_avg:97.79ms
step:1377/1750 train_time:134643ms step_avg:97.78ms
step:1378/1750 train_time:134746ms step_avg:97.78ms
step:1379/1750 train_time:134847ms step_avg:97.79ms
step:1380/1750 train_time:134949ms step_avg:97.79ms
step:1381/1750 train_time:135051ms step_avg:97.79ms
step:1382/1750 train_time:135152ms step_avg:97.79ms
step:1383/1750 train_time:135252ms step_avg:97.80ms
step:1384/1750 train_time:135353ms step_avg:97.80ms
step:1385/1750 train_time:135455ms step_avg:97.80ms
step:1386/1750 train_time:135558ms step_avg:97.81ms
step:1387/1750 train_time:135661ms step_avg:97.81ms
step:1388/1750 train_time:135763ms step_avg:97.81ms
step:1389/1750 train_time:135866ms step_avg:97.82ms
step:1390/1750 train_time:135968ms step_avg:97.82ms
step:1391/1750 train_time:136070ms step_avg:97.82ms
step:1392/1750 train_time:136171ms step_avg:97.82ms
step:1393/1750 train_time:136273ms step_avg:97.83ms
step:1394/1750 train_time:136374ms step_avg:97.83ms
step:1395/1750 train_time:136475ms step_avg:97.83ms
step:1396/1750 train_time:136578ms step_avg:97.84ms
step:1397/1750 train_time:136680ms step_avg:97.84ms
step:1398/1750 train_time:136782ms step_avg:97.84ms
step:1399/1750 train_time:136884ms step_avg:97.84ms
step:1400/1750 train_time:136986ms step_avg:97.85ms
step:1401/1750 train_time:137088ms step_avg:97.85ms
step:1402/1750 train_time:137190ms step_avg:97.85ms
step:1403/1750 train_time:137292ms step_avg:97.86ms
step:1404/1750 train_time:137394ms step_avg:97.86ms
step:1405/1750 train_time:137495ms step_avg:97.86ms
step:1406/1750 train_time:137597ms step_avg:97.86ms
step:1407/1750 train_time:137698ms step_avg:97.87ms
step:1408/1750 train_time:137801ms step_avg:97.87ms
step:1409/1750 train_time:137905ms step_avg:97.87ms
step:1410/1750 train_time:138007ms step_avg:97.88ms
step:1411/1750 train_time:138109ms step_avg:97.88ms
step:1412/1750 train_time:138211ms step_avg:97.88ms
step:1413/1750 train_time:138312ms step_avg:97.89ms
step:1414/1750 train_time:138413ms step_avg:97.89ms
step:1415/1750 train_time:138515ms step_avg:97.89ms
step:1416/1750 train_time:138618ms step_avg:97.89ms
step:1417/1750 train_time:138719ms step_avg:97.90ms
step:1418/1750 train_time:138821ms step_avg:97.90ms
step:1419/1750 train_time:138924ms step_avg:97.90ms
step:1420/1750 train_time:139027ms step_avg:97.91ms
step:1421/1750 train_time:139128ms step_avg:97.91ms
step:1422/1750 train_time:139231ms step_avg:97.91ms
step:1423/1750 train_time:139332ms step_avg:97.91ms
step:1424/1750 train_time:139434ms step_avg:97.92ms
step:1425/1750 train_time:139536ms step_avg:97.92ms
step:1426/1750 train_time:139638ms step_avg:97.92ms
step:1427/1750 train_time:139740ms step_avg:97.93ms
step:1428/1750 train_time:139844ms step_avg:97.93ms
step:1429/1750 train_time:139946ms step_avg:97.93ms
step:1430/1750 train_time:140049ms step_avg:97.94ms
step:1431/1750 train_time:140152ms step_avg:97.94ms
step:1432/1750 train_time:140255ms step_avg:97.94ms
step:1433/1750 train_time:140358ms step_avg:97.95ms
step:1434/1750 train_time:140460ms step_avg:97.95ms
step:1435/1750 train_time:140565ms step_avg:97.95ms
step:1436/1750 train_time:140669ms step_avg:97.96ms
step:1437/1750 train_time:140772ms step_avg:97.96ms
step:1438/1750 train_time:140875ms step_avg:97.97ms
step:1439/1750 train_time:140979ms step_avg:97.97ms
step:1440/1750 train_time:141082ms step_avg:97.97ms
step:1441/1750 train_time:141187ms step_avg:97.98ms
step:1442/1750 train_time:141291ms step_avg:97.98ms
step:1443/1750 train_time:141393ms step_avg:97.99ms
step:1444/1750 train_time:141496ms step_avg:97.99ms
step:1445/1750 train_time:141598ms step_avg:97.99ms
step:1446/1750 train_time:141700ms step_avg:97.99ms
step:1447/1750 train_time:141803ms step_avg:98.00ms
step:1448/1750 train_time:141907ms step_avg:98.00ms
step:1449/1750 train_time:142009ms step_avg:98.00ms
step:1450/1750 train_time:142112ms step_avg:98.01ms
step:1451/1750 train_time:142215ms step_avg:98.01ms
step:1452/1750 train_time:142318ms step_avg:98.02ms
step:1453/1750 train_time:142423ms step_avg:98.02ms
step:1454/1750 train_time:142527ms step_avg:98.02ms
step:1455/1750 train_time:142630ms step_avg:98.03ms
step:1456/1750 train_time:142732ms step_avg:98.03ms
step:1457/1750 train_time:142835ms step_avg:98.03ms
step:1458/1750 train_time:142939ms step_avg:98.04ms
step:1459/1750 train_time:143042ms step_avg:98.04ms
step:1460/1750 train_time:143144ms step_avg:98.04ms
step:1461/1750 train_time:143247ms step_avg:98.05ms
step:1462/1750 train_time:143350ms step_avg:98.05ms
step:1463/1750 train_time:143454ms step_avg:98.05ms
step:1464/1750 train_time:143558ms step_avg:98.06ms
step:1465/1750 train_time:143661ms step_avg:98.06ms
step:1466/1750 train_time:143763ms step_avg:98.07ms
step:1467/1750 train_time:143866ms step_avg:98.07ms
step:1468/1750 train_time:143970ms step_avg:98.07ms
step:1469/1750 train_time:144073ms step_avg:98.08ms
step:1470/1750 train_time:144175ms step_avg:98.08ms
step:1471/1750 train_time:144278ms step_avg:98.08ms
step:1472/1750 train_time:144381ms step_avg:98.08ms
step:1473/1750 train_time:144484ms step_avg:98.09ms
step:1474/1750 train_time:144589ms step_avg:98.09ms
step:1475/1750 train_time:144691ms step_avg:98.10ms
step:1476/1750 train_time:144794ms step_avg:98.10ms
step:1477/1750 train_time:144897ms step_avg:98.10ms
step:1478/1750 train_time:145000ms step_avg:98.11ms
step:1479/1750 train_time:145103ms step_avg:98.11ms
step:1480/1750 train_time:145205ms step_avg:98.11ms
step:1481/1750 train_time:145310ms step_avg:98.12ms
step:1482/1750 train_time:145413ms step_avg:98.12ms
step:1483/1750 train_time:145515ms step_avg:98.12ms
step:1484/1750 train_time:145619ms step_avg:98.13ms
step:1485/1750 train_time:145723ms step_avg:98.13ms
step:1486/1750 train_time:145826ms step_avg:98.13ms
step:1487/1750 train_time:145929ms step_avg:98.14ms
step:1488/1750 train_time:146032ms step_avg:98.14ms
step:1489/1750 train_time:146135ms step_avg:98.14ms
step:1490/1750 train_time:146238ms step_avg:98.15ms
step:1491/1750 train_time:146342ms step_avg:98.15ms
step:1492/1750 train_time:146445ms step_avg:98.15ms
step:1493/1750 train_time:146549ms step_avg:98.16ms
step:1494/1750 train_time:146652ms step_avg:98.16ms
step:1495/1750 train_time:146755ms step_avg:98.16ms
step:1496/1750 train_time:146858ms step_avg:98.17ms
step:1497/1750 train_time:146959ms step_avg:98.17ms
step:1498/1750 train_time:147064ms step_avg:98.17ms
step:1499/1750 train_time:147167ms step_avg:98.18ms
step:1500/1750 train_time:147269ms step_avg:98.18ms
step:1500/1750 val_loss:3.3330 train_time:147367ms step_avg:98.24ms
step:1501/1750 train_time:147394ms step_avg:98.20ms
step:1502/1750 train_time:147484ms step_avg:98.19ms
step:1503/1750 train_time:147587ms step_avg:98.20ms
step:1504/1750 train_time:147690ms step_avg:98.20ms
step:1505/1750 train_time:147792ms step_avg:98.20ms
step:1506/1750 train_time:147894ms step_avg:98.20ms
step:1507/1750 train_time:147996ms step_avg:98.21ms
step:1508/1750 train_time:148098ms step_avg:98.21ms
step:1509/1750 train_time:148200ms step_avg:98.21ms
step:1510/1750 train_time:148303ms step_avg:98.21ms
step:1511/1750 train_time:148409ms step_avg:98.22ms
step:1512/1750 train_time:148512ms step_avg:98.22ms
step:1513/1750 train_time:148614ms step_avg:98.23ms
step:1514/1750 train_time:148719ms step_avg:98.23ms
step:1515/1750 train_time:148825ms step_avg:98.23ms
step:1516/1750 train_time:148928ms step_avg:98.24ms
step:1517/1750 train_time:149030ms step_avg:98.24ms
step:1518/1750 train_time:149133ms step_avg:98.24ms
step:1519/1750 train_time:149237ms step_avg:98.25ms
step:1520/1750 train_time:149338ms step_avg:98.25ms
step:1521/1750 train_time:149442ms step_avg:98.25ms
step:1522/1750 train_time:149545ms step_avg:98.26ms
step:1523/1750 train_time:149648ms step_avg:98.26ms
step:1524/1750 train_time:149752ms step_avg:98.26ms
step:1525/1750 train_time:149856ms step_avg:98.27ms
step:1526/1750 train_time:149959ms step_avg:98.27ms
step:1527/1750 train_time:150062ms step_avg:98.27ms
step:1528/1750 train_time:150167ms step_avg:98.28ms
step:1529/1750 train_time:150269ms step_avg:98.28ms
step:1530/1750 train_time:150373ms step_avg:98.28ms
step:1531/1750 train_time:150475ms step_avg:98.29ms
step:1532/1750 train_time:150577ms step_avg:98.29ms
step:1533/1750 train_time:150680ms step_avg:98.29ms
step:1534/1750 train_time:150783ms step_avg:98.29ms
step:1535/1750 train_time:150887ms step_avg:98.30ms
step:1536/1750 train_time:150989ms step_avg:98.30ms
step:1537/1750 train_time:151092ms step_avg:98.30ms
step:1538/1750 train_time:151195ms step_avg:98.31ms
step:1539/1750 train_time:151297ms step_avg:98.31ms
step:1540/1750 train_time:151400ms step_avg:98.31ms
step:1541/1750 train_time:151503ms step_avg:98.31ms
step:1542/1750 train_time:151607ms step_avg:98.32ms
step:1543/1750 train_time:151711ms step_avg:98.32ms
step:1544/1750 train_time:151814ms step_avg:98.32ms
step:1545/1750 train_time:151916ms step_avg:98.33ms
step:1546/1750 train_time:152020ms step_avg:98.33ms
step:1547/1750 train_time:152124ms step_avg:98.33ms
step:1548/1750 train_time:152228ms step_avg:98.34ms
step:1549/1750 train_time:152332ms step_avg:98.34ms
step:1550/1750 train_time:152435ms step_avg:98.35ms
step:1551/1750 train_time:152539ms step_avg:98.35ms
step:1552/1750 train_time:152641ms step_avg:98.35ms
step:1553/1750 train_time:152743ms step_avg:98.35ms
step:1554/1750 train_time:152846ms step_avg:98.36ms
step:1555/1750 train_time:152949ms step_avg:98.36ms
step:1556/1750 train_time:153052ms step_avg:98.36ms
step:1557/1750 train_time:153156ms step_avg:98.37ms
step:1558/1750 train_time:153260ms step_avg:98.37ms
step:1559/1750 train_time:153364ms step_avg:98.37ms
step:1560/1750 train_time:153468ms step_avg:98.38ms
step:1561/1750 train_time:153571ms step_avg:98.38ms
step:1562/1750 train_time:153674ms step_avg:98.38ms
step:1563/1750 train_time:153780ms step_avg:98.39ms
step:1564/1750 train_time:153882ms step_avg:98.39ms
step:1565/1750 train_time:153984ms step_avg:98.39ms
step:1566/1750 train_time:154087ms step_avg:98.40ms
step:1567/1750 train_time:154190ms step_avg:98.40ms
step:1568/1750 train_time:154293ms step_avg:98.40ms
step:1569/1750 train_time:154395ms step_avg:98.40ms
step:1570/1750 train_time:154502ms step_avg:98.41ms
step:1571/1750 train_time:154605ms step_avg:98.41ms
step:1572/1750 train_time:154708ms step_avg:98.41ms
step:1573/1750 train_time:154811ms step_avg:98.42ms
step:1574/1750 train_time:154913ms step_avg:98.42ms
step:1575/1750 train_time:155016ms step_avg:98.42ms
step:1576/1750 train_time:155119ms step_avg:98.43ms
step:1577/1750 train_time:155225ms step_avg:98.43ms
step:1578/1750 train_time:155328ms step_avg:98.43ms
step:1579/1750 train_time:155432ms step_avg:98.44ms
step:1580/1750 train_time:155535ms step_avg:98.44ms
step:1581/1750 train_time:155638ms step_avg:98.44ms
step:1582/1750 train_time:155741ms step_avg:98.45ms
step:1583/1750 train_time:155845ms step_avg:98.45ms
step:1584/1750 train_time:155951ms step_avg:98.45ms
step:1585/1750 train_time:156055ms step_avg:98.46ms
step:1586/1750 train_time:156159ms step_avg:98.46ms
step:1587/1750 train_time:156261ms step_avg:98.46ms
step:1588/1750 train_time:156364ms step_avg:98.47ms
step:1589/1750 train_time:156468ms step_avg:98.47ms
step:1590/1750 train_time:156571ms step_avg:98.47ms
step:1591/1750 train_time:156675ms step_avg:98.48ms
step:1592/1750 train_time:156779ms step_avg:98.48ms
step:1593/1750 train_time:156882ms step_avg:98.48ms
step:1594/1750 train_time:156988ms step_avg:98.49ms
step:1595/1750 train_time:157090ms step_avg:98.49ms
step:1596/1750 train_time:157192ms step_avg:98.49ms
step:1597/1750 train_time:157295ms step_avg:98.49ms
step:1598/1750 train_time:157399ms step_avg:98.50ms
step:1599/1750 train_time:157501ms step_avg:98.50ms
step:1600/1750 train_time:157606ms step_avg:98.50ms
step:1601/1750 train_time:157710ms step_avg:98.51ms
step:1602/1750 train_time:157813ms step_avg:98.51ms
step:1603/1750 train_time:157916ms step_avg:98.51ms
step:1604/1750 train_time:158019ms step_avg:98.52ms
step:1605/1750 train_time:158124ms step_avg:98.52ms
step:1606/1750 train_time:158226ms step_avg:98.52ms
step:1607/1750 train_time:158329ms step_avg:98.52ms
step:1608/1750 train_time:158432ms step_avg:98.53ms
step:1609/1750 train_time:158536ms step_avg:98.53ms
step:1610/1750 train_time:158641ms step_avg:98.53ms
step:1611/1750 train_time:158744ms step_avg:98.54ms
step:1612/1750 train_time:158849ms step_avg:98.54ms
step:1613/1750 train_time:158952ms step_avg:98.54ms
step:1614/1750 train_time:159054ms step_avg:98.55ms
step:1615/1750 train_time:159156ms step_avg:98.55ms
step:1616/1750 train_time:159260ms step_avg:98.55ms
step:1617/1750 train_time:159363ms step_avg:98.55ms
step:1618/1750 train_time:159467ms step_avg:98.56ms
step:1619/1750 train_time:159570ms step_avg:98.56ms
step:1620/1750 train_time:159674ms step_avg:98.56ms
step:1621/1750 train_time:159777ms step_avg:98.57ms
step:1622/1750 train_time:159880ms step_avg:98.57ms
step:1623/1750 train_time:159983ms step_avg:98.57ms
step:1624/1750 train_time:160088ms step_avg:98.58ms
step:1625/1750 train_time:160192ms step_avg:98.58ms
step:1625/1750 val_loss:3.3032 train_time:160290ms step_avg:98.64ms
step:1626/1750 train_time:160316ms step_avg:98.60ms
step:1627/1750 train_time:160408ms step_avg:98.59ms
step:1628/1750 train_time:160511ms step_avg:98.59ms
step:1629/1750 train_time:160613ms step_avg:98.60ms
step:1630/1750 train_time:160717ms step_avg:98.60ms
step:1631/1750 train_time:160819ms step_avg:98.60ms
step:1632/1750 train_time:160923ms step_avg:98.60ms
step:1633/1750 train_time:161025ms step_avg:98.61ms
step:1634/1750 train_time:161130ms step_avg:98.61ms
step:1635/1750 train_time:161232ms step_avg:98.61ms
step:1636/1750 train_time:161337ms step_avg:98.62ms
step:1637/1750 train_time:161441ms step_avg:98.62ms
step:1638/1750 train_time:161544ms step_avg:98.62ms
step:1639/1750 train_time:161647ms step_avg:98.63ms
step:1640/1750 train_time:161750ms step_avg:98.63ms
step:1641/1750 train_time:161852ms step_avg:98.63ms
step:1642/1750 train_time:161954ms step_avg:98.63ms
step:1643/1750 train_time:162057ms step_avg:98.63ms
step:1644/1750 train_time:162160ms step_avg:98.64ms
step:1645/1750 train_time:162263ms step_avg:98.64ms
step:1646/1750 train_time:162366ms step_avg:98.64ms
step:1647/1750 train_time:162471ms step_avg:98.65ms
step:1648/1750 train_time:162575ms step_avg:98.65ms
step:1649/1750 train_time:162677ms step_avg:98.65ms
step:1650/1750 train_time:162781ms step_avg:98.66ms
step:1651/1750 train_time:162884ms step_avg:98.66ms
step:1652/1750 train_time:162986ms step_avg:98.66ms
step:1653/1750 train_time:163090ms step_avg:98.66ms
step:1654/1750 train_time:163192ms step_avg:98.67ms
step:1655/1750 train_time:163295ms step_avg:98.67ms
step:1656/1750 train_time:163400ms step_avg:98.67ms
step:1657/1750 train_time:163503ms step_avg:98.67ms
step:1658/1750 train_time:163607ms step_avg:98.68ms
step:1659/1750 train_time:163713ms step_avg:98.68ms
step:1660/1750 train_time:163816ms step_avg:98.68ms
step:1661/1750 train_time:163921ms step_avg:98.69ms
step:1662/1750 train_time:164025ms step_avg:98.69ms
step:1663/1750 train_time:164128ms step_avg:98.69ms
step:1664/1750 train_time:164231ms step_avg:98.70ms
step:1665/1750 train_time:164336ms step_avg:98.70ms
step:1666/1750 train_time:164439ms step_avg:98.70ms
step:1667/1750 train_time:164542ms step_avg:98.71ms
step:1668/1750 train_time:164646ms step_avg:98.71ms
step:1669/1750 train_time:164751ms step_avg:98.71ms
step:1670/1750 train_time:164854ms step_avg:98.71ms
step:1671/1750 train_time:164957ms step_avg:98.72ms
step:1672/1750 train_time:165061ms step_avg:98.72ms
step:1673/1750 train_time:165164ms step_avg:98.72ms
step:1674/1750 train_time:165267ms step_avg:98.73ms
step:1675/1750 train_time:165370ms step_avg:98.73ms
step:1676/1750 train_time:165475ms step_avg:98.73ms
step:1677/1750 train_time:165577ms step_avg:98.73ms
step:1678/1750 train_time:165680ms step_avg:98.74ms
step:1679/1750 train_time:165784ms step_avg:98.74ms
step:1680/1750 train_time:165888ms step_avg:98.74ms
step:1681/1750 train_time:165991ms step_avg:98.75ms
step:1682/1750 train_time:166097ms step_avg:98.75ms
step:1683/1750 train_time:166200ms step_avg:98.75ms
step:1684/1750 train_time:166304ms step_avg:98.76ms
step:1685/1750 train_time:166407ms step_avg:98.76ms
step:1686/1750 train_time:166509ms step_avg:98.76ms
step:1687/1750 train_time:166613ms step_avg:98.76ms
step:1688/1750 train_time:166717ms step_avg:98.77ms
step:1689/1750 train_time:166821ms step_avg:98.77ms
step:1690/1750 train_time:166925ms step_avg:98.77ms
step:1691/1750 train_time:167029ms step_avg:98.78ms
step:1692/1750 train_time:167133ms step_avg:98.78ms
step:1693/1750 train_time:167237ms step_avg:98.78ms
step:1694/1750 train_time:167342ms step_avg:98.78ms
step:1695/1750 train_time:167446ms step_avg:98.79ms
step:1696/1750 train_time:167550ms step_avg:98.79ms
step:1697/1750 train_time:167657ms step_avg:98.80ms
step:1698/1750 train_time:167761ms step_avg:98.80ms
step:1699/1750 train_time:167865ms step_avg:98.80ms
step:1700/1750 train_time:167969ms step_avg:98.81ms
step:1701/1750 train_time:168072ms step_avg:98.81ms
step:1702/1750 train_time:168179ms step_avg:98.81ms
step:1703/1750 train_time:168282ms step_avg:98.82ms
step:1704/1750 train_time:168386ms step_avg:98.82ms
step:1705/1750 train_time:168489ms step_avg:98.82ms
step:1706/1750 train_time:168593ms step_avg:98.82ms
step:1707/1750 train_time:168699ms step_avg:98.83ms
step:1708/1750 train_time:168804ms step_avg:98.83ms
step:1709/1750 train_time:168908ms step_avg:98.83ms
step:1710/1750 train_time:169013ms step_avg:98.84ms
step:1711/1750 train_time:169118ms step_avg:98.84ms
step:1712/1750 train_time:169221ms step_avg:98.84ms
step:1713/1750 train_time:169327ms step_avg:98.85ms
step:1714/1750 train_time:169429ms step_avg:98.85ms
step:1715/1750 train_time:169535ms step_avg:98.85ms
step:1716/1750 train_time:169639ms step_avg:98.86ms
step:1717/1750 train_time:169743ms step_avg:98.86ms
step:1718/1750 train_time:169848ms step_avg:98.86ms
step:1719/1750 train_time:169954ms step_avg:98.87ms
step:1720/1750 train_time:170057ms step_avg:98.87ms
step:1721/1750 train_time:170162ms step_avg:98.87ms
step:1722/1750 train_time:170267ms step_avg:98.88ms
step:1723/1750 train_time:170371ms step_avg:98.88ms
step:1724/1750 train_time:170476ms step_avg:98.88ms
step:1725/1750 train_time:170581ms step_avg:98.89ms
step:1726/1750 train_time:170685ms step_avg:98.89ms
step:1727/1750 train_time:170790ms step_avg:98.89ms
step:1728/1750 train_time:170895ms step_avg:98.90ms
step:1729/1750 train_time:170998ms step_avg:98.90ms
step:1730/1750 train_time:171102ms step_avg:98.90ms
step:1731/1750 train_time:171206ms step_avg:98.91ms
step:1732/1750 train_time:171310ms step_avg:98.91ms
step:1733/1750 train_time:171414ms step_avg:98.91ms
step:1734/1750 train_time:171519ms step_avg:98.92ms
step:1735/1750 train_time:171622ms step_avg:98.92ms
step:1736/1750 train_time:171727ms step_avg:98.92ms
step:1737/1750 train_time:171831ms step_avg:98.92ms
step:1738/1750 train_time:171935ms step_avg:98.93ms
step:1739/1750 train_time:172038ms step_avg:98.93ms
step:1740/1750 train_time:172143ms step_avg:98.93ms
step:1741/1750 train_time:172253ms step_avg:98.94ms
step:1742/1750 train_time:172357ms step_avg:98.94ms
step:1743/1750 train_time:172462ms step_avg:98.95ms
step:1744/1750 train_time:172566ms step_avg:98.95ms
step:1745/1750 train_time:172671ms step_avg:98.95ms
step:1746/1750 train_time:172774ms step_avg:98.95ms
step:1747/1750 train_time:172878ms step_avg:98.96ms
step:1748/1750 train_time:172982ms step_avg:98.96ms
step:1749/1750 train_time:173085ms step_avg:98.96ms
step:1750/1750 train_time:173192ms step_avg:98.97ms
step:1750/1750 val_loss:3.2819 train_time:173291ms step_avg:99.02ms
peak memory allocated: 33277 MiB reserved: 48892 MiB

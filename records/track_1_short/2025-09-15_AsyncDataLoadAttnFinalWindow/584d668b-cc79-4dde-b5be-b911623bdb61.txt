import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload=False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            #t0 = time.perf_counter()
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            #t1 = time.perf_counter()
            #print(f'{t1-t0} slowload')
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    def __init__(self, file_iter, world_size):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
      #loading in a whole shard will be slow...
      #BosFinder tracks its self.i index. I can kickoff with a smaller set. then have the 
      finder = BOSFinder(tokens, world_size=world_size, quickload=True)
      preloader = DataPreloader(file_iter, world_size)
      preloader.start()
    else:
      pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                #tokens = _load_data_shard(next(file_iter))
                #finder = BOSFinder(tokens, world_size=world_size)
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main
@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1660 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"data_threading_1660/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer is highly indifferent to length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Tue Sep 16 03:54:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   31C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          197242      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          197243      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          197244      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          197245      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          197246      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          197247      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          197248      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          197249      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          197243      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          197244      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          197245      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          197246      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          197247      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          197248      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          197249      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1660 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1660 train_time:145ms step_avg:145.00ms
step:2/1660 train_time:166ms step_avg:82.86ms
step:3/1660 train_time:232ms step_avg:77.41ms
step:4/1660 train_time:321ms step_avg:80.27ms
step:5/1660 train_time:411ms step_avg:82.26ms
step:6/1660 train_time:501ms step_avg:83.55ms
step:7/1660 train_time:592ms step_avg:84.53ms
step:8/1660 train_time:682ms step_avg:85.30ms
step:9/1660 train_time:774ms step_avg:85.96ms
step:10/1660 train_time:864ms step_avg:86.43ms
step:11/1660 train_time:955ms step_avg:86.85ms
step:12/1660 train_time:1048ms step_avg:87.32ms
step:13/1660 train_time:1143ms step_avg:87.94ms
step:14/1660 train_time:1239ms step_avg:88.49ms
step:15/1660 train_time:1332ms step_avg:88.78ms
step:16/1660 train_time:1423ms step_avg:88.94ms
step:17/1660 train_time:1515ms step_avg:89.12ms
step:18/1660 train_time:1606ms step_avg:89.20ms
step:19/1660 train_time:1697ms step_avg:89.31ms
step:20/1660 train_time:1788ms step_avg:89.41ms
step:21/1660 train_time:1879ms step_avg:89.46ms
step:22/1660 train_time:1971ms step_avg:89.60ms
step:23/1660 train_time:2064ms step_avg:89.73ms
step:24/1660 train_time:2156ms step_avg:89.84ms
step:25/1660 train_time:2249ms step_avg:89.95ms
step:26/1660 train_time:2342ms step_avg:90.07ms
step:27/1660 train_time:2434ms step_avg:90.16ms
step:28/1660 train_time:2527ms step_avg:90.25ms
step:29/1660 train_time:2619ms step_avg:90.29ms
step:30/1660 train_time:2711ms step_avg:90.35ms
step:31/1660 train_time:2801ms step_avg:90.37ms
step:32/1660 train_time:2893ms step_avg:90.40ms
step:33/1660 train_time:2985ms step_avg:90.45ms
step:34/1660 train_time:3077ms step_avg:90.49ms
step:35/1660 train_time:3169ms step_avg:90.55ms
step:36/1660 train_time:3261ms step_avg:90.59ms
step:37/1660 train_time:3354ms step_avg:90.65ms
step:38/1660 train_time:3446ms step_avg:90.67ms
step:39/1660 train_time:3538ms step_avg:90.71ms
step:40/1660 train_time:3630ms step_avg:90.74ms
step:41/1660 train_time:3721ms step_avg:90.75ms
step:42/1660 train_time:3812ms step_avg:90.77ms
step:43/1660 train_time:3904ms step_avg:90.78ms
step:44/1660 train_time:3996ms step_avg:90.82ms
step:45/1660 train_time:4089ms step_avg:90.86ms
step:46/1660 train_time:4180ms step_avg:90.88ms
step:47/1660 train_time:4272ms step_avg:90.90ms
step:48/1660 train_time:4365ms step_avg:90.94ms
step:49/1660 train_time:4458ms step_avg:90.98ms
step:50/1660 train_time:4550ms step_avg:91.00ms
step:51/1660 train_time:4642ms step_avg:91.02ms
step:52/1660 train_time:4734ms step_avg:91.04ms
step:53/1660 train_time:4825ms step_avg:91.04ms
step:54/1660 train_time:4917ms step_avg:91.05ms
step:55/1660 train_time:5008ms step_avg:91.06ms
step:56/1660 train_time:5100ms step_avg:91.08ms
step:57/1660 train_time:5192ms step_avg:91.09ms
step:58/1660 train_time:5284ms step_avg:91.10ms
step:59/1660 train_time:5376ms step_avg:91.11ms
step:60/1660 train_time:5467ms step_avg:91.12ms
step:61/1660 train_time:5558ms step_avg:91.12ms
step:62/1660 train_time:5650ms step_avg:91.13ms
step:63/1660 train_time:5742ms step_avg:91.14ms
step:64/1660 train_time:5834ms step_avg:91.15ms
step:65/1660 train_time:5925ms step_avg:91.16ms
step:66/1660 train_time:6017ms step_avg:91.17ms
step:67/1660 train_time:6109ms step_avg:91.19ms
step:68/1660 train_time:6201ms step_avg:91.19ms
step:69/1660 train_time:6293ms step_avg:91.20ms
step:70/1660 train_time:6385ms step_avg:91.22ms
step:71/1660 train_time:6477ms step_avg:91.23ms
step:72/1660 train_time:6568ms step_avg:91.22ms
step:73/1660 train_time:6659ms step_avg:91.22ms
step:74/1660 train_time:6751ms step_avg:91.22ms
step:75/1660 train_time:6842ms step_avg:91.22ms
step:76/1660 train_time:6934ms step_avg:91.23ms
step:77/1660 train_time:7025ms step_avg:91.23ms
step:78/1660 train_time:7119ms step_avg:91.27ms
step:79/1660 train_time:7211ms step_avg:91.28ms
step:80/1660 train_time:7302ms step_avg:91.28ms
step:81/1660 train_time:7394ms step_avg:91.29ms
step:82/1660 train_time:7486ms step_avg:91.29ms
step:83/1660 train_time:7578ms step_avg:91.30ms
step:84/1660 train_time:7669ms step_avg:91.29ms
step:85/1660 train_time:7759ms step_avg:91.29ms
step:86/1660 train_time:7850ms step_avg:91.28ms
step:87/1660 train_time:7941ms step_avg:91.28ms
step:88/1660 train_time:8034ms step_avg:91.29ms
step:89/1660 train_time:8125ms step_avg:91.30ms
step:90/1660 train_time:8218ms step_avg:91.31ms
step:91/1660 train_time:8310ms step_avg:91.32ms
step:92/1660 train_time:8402ms step_avg:91.32ms
step:93/1660 train_time:8493ms step_avg:91.33ms
step:94/1660 train_time:8585ms step_avg:91.33ms
step:95/1660 train_time:8677ms step_avg:91.33ms
step:96/1660 train_time:8768ms step_avg:91.33ms
step:97/1660 train_time:8859ms step_avg:91.33ms
step:98/1660 train_time:8950ms step_avg:91.33ms
step:99/1660 train_time:9042ms step_avg:91.33ms
step:100/1660 train_time:9134ms step_avg:91.34ms
step:101/1660 train_time:9226ms step_avg:91.35ms
step:102/1660 train_time:9318ms step_avg:91.36ms
step:103/1660 train_time:9410ms step_avg:91.36ms
step:104/1660 train_time:9501ms step_avg:91.35ms
step:105/1660 train_time:9592ms step_avg:91.36ms
step:106/1660 train_time:9684ms step_avg:91.36ms
step:107/1660 train_time:9775ms step_avg:91.35ms
step:108/1660 train_time:9865ms step_avg:91.34ms
step:109/1660 train_time:9956ms step_avg:91.34ms
step:110/1660 train_time:10048ms step_avg:91.34ms
step:111/1660 train_time:10140ms step_avg:91.35ms
step:112/1660 train_time:10233ms step_avg:91.37ms
step:113/1660 train_time:10324ms step_avg:91.37ms
step:114/1660 train_time:10417ms step_avg:91.38ms
step:115/1660 train_time:10510ms step_avg:91.39ms
step:116/1660 train_time:10602ms step_avg:91.39ms
step:117/1660 train_time:10692ms step_avg:91.39ms
step:118/1660 train_time:10784ms step_avg:91.39ms
step:119/1660 train_time:10875ms step_avg:91.38ms
step:120/1660 train_time:10966ms step_avg:91.39ms
step:121/1660 train_time:11058ms step_avg:91.38ms
step:122/1660 train_time:11149ms step_avg:91.39ms
step:123/1660 train_time:11241ms step_avg:91.39ms
step:124/1660 train_time:11333ms step_avg:91.39ms
step:125/1660 train_time:11425ms step_avg:91.40ms
step:125/1660 val_loss:4.2980 train_time:11521ms step_avg:92.17ms
step:126/1660 train_time:11544ms step_avg:91.62ms
step:127/1660 train_time:11617ms step_avg:91.48ms
step:128/1660 train_time:11717ms step_avg:91.54ms
step:129/1660 train_time:11811ms step_avg:91.56ms
step:130/1660 train_time:11903ms step_avg:91.56ms
step:131/1660 train_time:11993ms step_avg:91.55ms
step:132/1660 train_time:12083ms step_avg:91.54ms
step:133/1660 train_time:12173ms step_avg:91.53ms
step:134/1660 train_time:12264ms step_avg:91.52ms
step:135/1660 train_time:12354ms step_avg:91.51ms
step:136/1660 train_time:12444ms step_avg:91.50ms
step:137/1660 train_time:12536ms step_avg:91.51ms
step:138/1660 train_time:12630ms step_avg:91.52ms
step:139/1660 train_time:12724ms step_avg:91.54ms
step:140/1660 train_time:12817ms step_avg:91.55ms
step:141/1660 train_time:12910ms step_avg:91.56ms
step:142/1660 train_time:13000ms step_avg:91.55ms
step:143/1660 train_time:13091ms step_avg:91.54ms
step:144/1660 train_time:13181ms step_avg:91.54ms
step:145/1660 train_time:13272ms step_avg:91.53ms
step:146/1660 train_time:13363ms step_avg:91.53ms
step:147/1660 train_time:13454ms step_avg:91.52ms
step:148/1660 train_time:13546ms step_avg:91.52ms
step:149/1660 train_time:13638ms step_avg:91.53ms
step:150/1660 train_time:13730ms step_avg:91.53ms
step:151/1660 train_time:13823ms step_avg:91.54ms
step:152/1660 train_time:13914ms step_avg:91.54ms
step:153/1660 train_time:14006ms step_avg:91.54ms
step:154/1660 train_time:14097ms step_avg:91.54ms
step:155/1660 train_time:14188ms step_avg:91.54ms
step:156/1660 train_time:14279ms step_avg:91.53ms
step:157/1660 train_time:14370ms step_avg:91.53ms
step:158/1660 train_time:14461ms step_avg:91.53ms
step:159/1660 train_time:14552ms step_avg:91.52ms
step:160/1660 train_time:14646ms step_avg:91.54ms
step:161/1660 train_time:14739ms step_avg:91.54ms
step:162/1660 train_time:14830ms step_avg:91.55ms
step:163/1660 train_time:14922ms step_avg:91.55ms
step:164/1660 train_time:15014ms step_avg:91.55ms
step:165/1660 train_time:15105ms step_avg:91.55ms
step:166/1660 train_time:15196ms step_avg:91.54ms
step:167/1660 train_time:15287ms step_avg:91.54ms
step:168/1660 train_time:15378ms step_avg:91.54ms
step:169/1660 train_time:15469ms step_avg:91.53ms
step:170/1660 train_time:15560ms step_avg:91.53ms
step:171/1660 train_time:15652ms step_avg:91.53ms
step:172/1660 train_time:15744ms step_avg:91.53ms
step:173/1660 train_time:15835ms step_avg:91.53ms
step:174/1660 train_time:15927ms step_avg:91.54ms
step:175/1660 train_time:16020ms step_avg:91.54ms
step:176/1660 train_time:16111ms step_avg:91.54ms
step:177/1660 train_time:16202ms step_avg:91.54ms
step:178/1660 train_time:16293ms step_avg:91.54ms
step:179/1660 train_time:16386ms step_avg:91.54ms
step:180/1660 train_time:16478ms step_avg:91.55ms
step:181/1660 train_time:16569ms step_avg:91.54ms
step:182/1660 train_time:16661ms step_avg:91.54ms
step:183/1660 train_time:16752ms step_avg:91.54ms
step:184/1660 train_time:16844ms step_avg:91.54ms
step:185/1660 train_time:16936ms step_avg:91.54ms
step:186/1660 train_time:17027ms step_avg:91.54ms
step:187/1660 train_time:17119ms step_avg:91.55ms
step:188/1660 train_time:17211ms step_avg:91.55ms
step:189/1660 train_time:17302ms step_avg:91.54ms
step:190/1660 train_time:17392ms step_avg:91.54ms
step:191/1660 train_time:17484ms step_avg:91.54ms
step:192/1660 train_time:17575ms step_avg:91.54ms
step:193/1660 train_time:17668ms step_avg:91.54ms
step:194/1660 train_time:17759ms step_avg:91.54ms
step:195/1660 train_time:17850ms step_avg:91.54ms
step:196/1660 train_time:17942ms step_avg:91.54ms
step:197/1660 train_time:18033ms step_avg:91.54ms
step:198/1660 train_time:18125ms step_avg:91.54ms
step:199/1660 train_time:18217ms step_avg:91.54ms
step:200/1660 train_time:18308ms step_avg:91.54ms
step:201/1660 train_time:18399ms step_avg:91.54ms
step:202/1660 train_time:18489ms step_avg:91.53ms
step:203/1660 train_time:18582ms step_avg:91.54ms
step:204/1660 train_time:18673ms step_avg:91.54ms
step:205/1660 train_time:18765ms step_avg:91.54ms
step:206/1660 train_time:18856ms step_avg:91.54ms
step:207/1660 train_time:18948ms step_avg:91.54ms
step:208/1660 train_time:19039ms step_avg:91.54ms
step:209/1660 train_time:19131ms step_avg:91.54ms
step:210/1660 train_time:19223ms step_avg:91.54ms
step:211/1660 train_time:19314ms step_avg:91.53ms
step:212/1660 train_time:19405ms step_avg:91.53ms
step:213/1660 train_time:19496ms step_avg:91.53ms
step:214/1660 train_time:19588ms step_avg:91.53ms
step:215/1660 train_time:19680ms step_avg:91.54ms
step:216/1660 train_time:19772ms step_avg:91.54ms
step:217/1660 train_time:19864ms step_avg:91.54ms
step:218/1660 train_time:19955ms step_avg:91.54ms
step:219/1660 train_time:20046ms step_avg:91.54ms
step:220/1660 train_time:20139ms step_avg:91.54ms
step:221/1660 train_time:20230ms step_avg:91.54ms
step:222/1660 train_time:20321ms step_avg:91.54ms
step:223/1660 train_time:20412ms step_avg:91.53ms
step:224/1660 train_time:20503ms step_avg:91.53ms
step:225/1660 train_time:20594ms step_avg:91.53ms
step:226/1660 train_time:20687ms step_avg:91.54ms
step:227/1660 train_time:20779ms step_avg:91.54ms
step:228/1660 train_time:20871ms step_avg:91.54ms
step:229/1660 train_time:20962ms step_avg:91.54ms
step:230/1660 train_time:21053ms step_avg:91.53ms
step:231/1660 train_time:21144ms step_avg:91.53ms
step:232/1660 train_time:21235ms step_avg:91.53ms
step:233/1660 train_time:21327ms step_avg:91.53ms
step:234/1660 train_time:21418ms step_avg:91.53ms
step:235/1660 train_time:21509ms step_avg:91.53ms
step:236/1660 train_time:21601ms step_avg:91.53ms
step:237/1660 train_time:21692ms step_avg:91.53ms
step:238/1660 train_time:21786ms step_avg:91.54ms
step:239/1660 train_time:21877ms step_avg:91.54ms
step:240/1660 train_time:21969ms step_avg:91.54ms
step:241/1660 train_time:22062ms step_avg:91.54ms
step:242/1660 train_time:22153ms step_avg:91.54ms
step:243/1660 train_time:22245ms step_avg:91.54ms
step:244/1660 train_time:22336ms step_avg:91.54ms
step:245/1660 train_time:22428ms step_avg:91.54ms
step:246/1660 train_time:22519ms step_avg:91.54ms
step:247/1660 train_time:22610ms step_avg:91.54ms
step:248/1660 train_time:22702ms step_avg:91.54ms
step:249/1660 train_time:22793ms step_avg:91.54ms
step:250/1660 train_time:22885ms step_avg:91.54ms
step:250/1660 val_loss:3.9698 train_time:22978ms step_avg:91.91ms
step:251/1660 train_time:22998ms step_avg:91.63ms
step:252/1660 train_time:23071ms step_avg:91.55ms
step:253/1660 train_time:23167ms step_avg:91.57ms
step:254/1660 train_time:23260ms step_avg:91.57ms
step:255/1660 train_time:23350ms step_avg:91.57ms
step:256/1660 train_time:23440ms step_avg:91.56ms
step:257/1660 train_time:23530ms step_avg:91.56ms
step:258/1660 train_time:23621ms step_avg:91.55ms
step:259/1660 train_time:23711ms step_avg:91.55ms
step:260/1660 train_time:23801ms step_avg:91.54ms
step:261/1660 train_time:23893ms step_avg:91.54ms
step:262/1660 train_time:23985ms step_avg:91.54ms
step:263/1660 train_time:24078ms step_avg:91.55ms
step:264/1660 train_time:24172ms step_avg:91.56ms
step:265/1660 train_time:24264ms step_avg:91.56ms
step:266/1660 train_time:24355ms step_avg:91.56ms
step:267/1660 train_time:24447ms step_avg:91.56ms
step:268/1660 train_time:24537ms step_avg:91.56ms
step:269/1660 train_time:24627ms step_avg:91.55ms
step:270/1660 train_time:24718ms step_avg:91.55ms
step:271/1660 train_time:24810ms step_avg:91.55ms
step:272/1660 train_time:24902ms step_avg:91.55ms
step:273/1660 train_time:24994ms step_avg:91.55ms
step:274/1660 train_time:25086ms step_avg:91.56ms
step:275/1660 train_time:25178ms step_avg:91.56ms
step:276/1660 train_time:25270ms step_avg:91.56ms
step:277/1660 train_time:25361ms step_avg:91.56ms
step:278/1660 train_time:25452ms step_avg:91.55ms
step:279/1660 train_time:25542ms step_avg:91.55ms
step:280/1660 train_time:25633ms step_avg:91.55ms
step:281/1660 train_time:25724ms step_avg:91.54ms
step:282/1660 train_time:25814ms step_avg:91.54ms
step:283/1660 train_time:25905ms step_avg:91.54ms
step:284/1660 train_time:25997ms step_avg:91.54ms
step:285/1660 train_time:26090ms step_avg:91.54ms
step:286/1660 train_time:26183ms step_avg:91.55ms
step:287/1660 train_time:26275ms step_avg:91.55ms
step:288/1660 train_time:26366ms step_avg:91.55ms
step:289/1660 train_time:26458ms step_avg:91.55ms
step:290/1660 train_time:26549ms step_avg:91.55ms
step:291/1660 train_time:26639ms step_avg:91.54ms
step:292/1660 train_time:26731ms step_avg:91.54ms
step:293/1660 train_time:26821ms step_avg:91.54ms
step:294/1660 train_time:26913ms step_avg:91.54ms
step:295/1660 train_time:27004ms step_avg:91.54ms
step:296/1660 train_time:27097ms step_avg:91.54ms
step:297/1660 train_time:27188ms step_avg:91.54ms
step:298/1660 train_time:27280ms step_avg:91.54ms
step:299/1660 train_time:27372ms step_avg:91.55ms
step:300/1660 train_time:27463ms step_avg:91.54ms
step:301/1660 train_time:27554ms step_avg:91.54ms
step:302/1660 train_time:27645ms step_avg:91.54ms
step:303/1660 train_time:27736ms step_avg:91.54ms
step:304/1660 train_time:27827ms step_avg:91.53ms
step:305/1660 train_time:27918ms step_avg:91.53ms
step:306/1660 train_time:28009ms step_avg:91.53ms
step:307/1660 train_time:28101ms step_avg:91.54ms
step:308/1660 train_time:28193ms step_avg:91.54ms
step:309/1660 train_time:28285ms step_avg:91.54ms
step:310/1660 train_time:28376ms step_avg:91.53ms
step:311/1660 train_time:28467ms step_avg:91.54ms
step:312/1660 train_time:28558ms step_avg:91.53ms
step:313/1660 train_time:28650ms step_avg:91.53ms
step:314/1660 train_time:28742ms step_avg:91.53ms
step:315/1660 train_time:28833ms step_avg:91.53ms
step:316/1660 train_time:28923ms step_avg:91.53ms
step:317/1660 train_time:29015ms step_avg:91.53ms
step:318/1660 train_time:29106ms step_avg:91.53ms
step:319/1660 train_time:29197ms step_avg:91.53ms
step:320/1660 train_time:29289ms step_avg:91.53ms
step:321/1660 train_time:29381ms step_avg:91.53ms
step:322/1660 train_time:29473ms step_avg:91.53ms
step:323/1660 train_time:29565ms step_avg:91.53ms
step:324/1660 train_time:29655ms step_avg:91.53ms
step:325/1660 train_time:29746ms step_avg:91.53ms
step:326/1660 train_time:29837ms step_avg:91.53ms
step:327/1660 train_time:29928ms step_avg:91.52ms
step:328/1660 train_time:30021ms step_avg:91.53ms
step:329/1660 train_time:30112ms step_avg:91.53ms
step:330/1660 train_time:30204ms step_avg:91.53ms
step:331/1660 train_time:30295ms step_avg:91.53ms
step:332/1660 train_time:30387ms step_avg:91.53ms
step:333/1660 train_time:30479ms step_avg:91.53ms
step:334/1660 train_time:30570ms step_avg:91.53ms
step:335/1660 train_time:30661ms step_avg:91.53ms
step:336/1660 train_time:30752ms step_avg:91.52ms
step:337/1660 train_time:30843ms step_avg:91.52ms
step:338/1660 train_time:30935ms step_avg:91.52ms
step:339/1660 train_time:31026ms step_avg:91.52ms
step:340/1660 train_time:31117ms step_avg:91.52ms
step:341/1660 train_time:31209ms step_avg:91.52ms
step:342/1660 train_time:31300ms step_avg:91.52ms
step:343/1660 train_time:31393ms step_avg:91.52ms
step:344/1660 train_time:31484ms step_avg:91.52ms
step:345/1660 train_time:31575ms step_avg:91.52ms
step:346/1660 train_time:31667ms step_avg:91.52ms
step:347/1660 train_time:31758ms step_avg:91.52ms
step:348/1660 train_time:31849ms step_avg:91.52ms
step:349/1660 train_time:31940ms step_avg:91.52ms
step:350/1660 train_time:32031ms step_avg:91.52ms
step:351/1660 train_time:32123ms step_avg:91.52ms
step:352/1660 train_time:32215ms step_avg:91.52ms
step:353/1660 train_time:32306ms step_avg:91.52ms
step:354/1660 train_time:32398ms step_avg:91.52ms
step:355/1660 train_time:32489ms step_avg:91.52ms
step:356/1660 train_time:32581ms step_avg:91.52ms
step:357/1660 train_time:32673ms step_avg:91.52ms
step:358/1660 train_time:32763ms step_avg:91.52ms
step:359/1660 train_time:32854ms step_avg:91.52ms
step:360/1660 train_time:32945ms step_avg:91.51ms
step:361/1660 train_time:33037ms step_avg:91.51ms
step:362/1660 train_time:33128ms step_avg:91.51ms
step:363/1660 train_time:33220ms step_avg:91.51ms
step:364/1660 train_time:33312ms step_avg:91.52ms
step:365/1660 train_time:33403ms step_avg:91.52ms
step:366/1660 train_time:33495ms step_avg:91.52ms
step:367/1660 train_time:33586ms step_avg:91.52ms
step:368/1660 train_time:33677ms step_avg:91.51ms
step:369/1660 train_time:33769ms step_avg:91.51ms
step:370/1660 train_time:33859ms step_avg:91.51ms
step:371/1660 train_time:33950ms step_avg:91.51ms
step:372/1660 train_time:34042ms step_avg:91.51ms
step:373/1660 train_time:34133ms step_avg:91.51ms
step:374/1660 train_time:34224ms step_avg:91.51ms
step:375/1660 train_time:34315ms step_avg:91.51ms
step:375/1660 val_loss:3.8141 train_time:34408ms step_avg:91.75ms
step:376/1660 train_time:34428ms step_avg:91.56ms
step:377/1660 train_time:34504ms step_avg:91.52ms
step:378/1660 train_time:34599ms step_avg:91.53ms
step:379/1660 train_time:34692ms step_avg:91.54ms
step:380/1660 train_time:34783ms step_avg:91.53ms
step:381/1660 train_time:34873ms step_avg:91.53ms
step:382/1660 train_time:34964ms step_avg:91.53ms
step:383/1660 train_time:35054ms step_avg:91.53ms
step:384/1660 train_time:35145ms step_avg:91.52ms
step:385/1660 train_time:35234ms step_avg:91.52ms
step:386/1660 train_time:35325ms step_avg:91.52ms
step:387/1660 train_time:35417ms step_avg:91.52ms
step:388/1660 train_time:35510ms step_avg:91.52ms
step:389/1660 train_time:35603ms step_avg:91.52ms
step:390/1660 train_time:35696ms step_avg:91.53ms
step:391/1660 train_time:35788ms step_avg:91.53ms
step:392/1660 train_time:35879ms step_avg:91.53ms
step:393/1660 train_time:35970ms step_avg:91.53ms
step:394/1660 train_time:36060ms step_avg:91.52ms
step:395/1660 train_time:36150ms step_avg:91.52ms
step:396/1660 train_time:36240ms step_avg:91.52ms
step:397/1660 train_time:36331ms step_avg:91.51ms
step:398/1660 train_time:36423ms step_avg:91.51ms
step:399/1660 train_time:36515ms step_avg:91.52ms
step:400/1660 train_time:36608ms step_avg:91.52ms
step:401/1660 train_time:36700ms step_avg:91.52ms
step:402/1660 train_time:36792ms step_avg:91.52ms
step:403/1660 train_time:36883ms step_avg:91.52ms
step:404/1660 train_time:36974ms step_avg:91.52ms
step:405/1660 train_time:37065ms step_avg:91.52ms
step:406/1660 train_time:37156ms step_avg:91.52ms
step:407/1660 train_time:37246ms step_avg:91.51ms
step:408/1660 train_time:37337ms step_avg:91.51ms
step:409/1660 train_time:37428ms step_avg:91.51ms
step:410/1660 train_time:37520ms step_avg:91.51ms
step:411/1660 train_time:37612ms step_avg:91.51ms
step:412/1660 train_time:37703ms step_avg:91.51ms
step:413/1660 train_time:37795ms step_avg:91.51ms
step:414/1660 train_time:37886ms step_avg:91.51ms
step:415/1660 train_time:37977ms step_avg:91.51ms
step:416/1660 train_time:38068ms step_avg:91.51ms
step:417/1660 train_time:38159ms step_avg:91.51ms
step:418/1660 train_time:38250ms step_avg:91.51ms
step:419/1660 train_time:38341ms step_avg:91.51ms
step:420/1660 train_time:38432ms step_avg:91.51ms
step:421/1660 train_time:38524ms step_avg:91.51ms
step:422/1660 train_time:38616ms step_avg:91.51ms
step:423/1660 train_time:38707ms step_avg:91.51ms
step:424/1660 train_time:38799ms step_avg:91.51ms
step:425/1660 train_time:38890ms step_avg:91.51ms
step:426/1660 train_time:38981ms step_avg:91.50ms
step:427/1660 train_time:39072ms step_avg:91.50ms
step:428/1660 train_time:39163ms step_avg:91.50ms
step:429/1660 train_time:39253ms step_avg:91.50ms
step:430/1660 train_time:39345ms step_avg:91.50ms
step:431/1660 train_time:39436ms step_avg:91.50ms
step:432/1660 train_time:39528ms step_avg:91.50ms
step:433/1660 train_time:39619ms step_avg:91.50ms
step:434/1660 train_time:39711ms step_avg:91.50ms
step:435/1660 train_time:39803ms step_avg:91.50ms
step:436/1660 train_time:39894ms step_avg:91.50ms
step:437/1660 train_time:39986ms step_avg:91.50ms
step:438/1660 train_time:40077ms step_avg:91.50ms
step:439/1660 train_time:40169ms step_avg:91.50ms
step:440/1660 train_time:40260ms step_avg:91.50ms
step:441/1660 train_time:40351ms step_avg:91.50ms
step:442/1660 train_time:40442ms step_avg:91.50ms
step:443/1660 train_time:40534ms step_avg:91.50ms
step:444/1660 train_time:40626ms step_avg:91.50ms
step:445/1660 train_time:40717ms step_avg:91.50ms
step:446/1660 train_time:40808ms step_avg:91.50ms
step:447/1660 train_time:40899ms step_avg:91.50ms
step:448/1660 train_time:40991ms step_avg:91.50ms
step:449/1660 train_time:41082ms step_avg:91.50ms
step:450/1660 train_time:41174ms step_avg:91.50ms
step:451/1660 train_time:41265ms step_avg:91.50ms
step:452/1660 train_time:41356ms step_avg:91.50ms
step:453/1660 train_time:41447ms step_avg:91.49ms
step:454/1660 train_time:41538ms step_avg:91.49ms
step:455/1660 train_time:41630ms step_avg:91.49ms
step:456/1660 train_time:41722ms step_avg:91.50ms
step:457/1660 train_time:41813ms step_avg:91.49ms
step:458/1660 train_time:41904ms step_avg:91.49ms
step:459/1660 train_time:41996ms step_avg:91.49ms
step:460/1660 train_time:42088ms step_avg:91.50ms
step:461/1660 train_time:42179ms step_avg:91.50ms
step:462/1660 train_time:42272ms step_avg:91.50ms
step:463/1660 train_time:42363ms step_avg:91.50ms
step:464/1660 train_time:42454ms step_avg:91.50ms
step:465/1660 train_time:42545ms step_avg:91.49ms
step:466/1660 train_time:42636ms step_avg:91.49ms
step:467/1660 train_time:42727ms step_avg:91.49ms
step:468/1660 train_time:42819ms step_avg:91.49ms
step:469/1660 train_time:42909ms step_avg:91.49ms
step:470/1660 train_time:43002ms step_avg:91.49ms
step:471/1660 train_time:43093ms step_avg:91.49ms
step:472/1660 train_time:43185ms step_avg:91.49ms
step:473/1660 train_time:43276ms step_avg:91.49ms
step:474/1660 train_time:43367ms step_avg:91.49ms
step:475/1660 train_time:43459ms step_avg:91.49ms
step:476/1660 train_time:43550ms step_avg:91.49ms
step:477/1660 train_time:43642ms step_avg:91.49ms
step:478/1660 train_time:43734ms step_avg:91.49ms
step:479/1660 train_time:43825ms step_avg:91.49ms
step:480/1660 train_time:43917ms step_avg:91.49ms
step:481/1660 train_time:44008ms step_avg:91.49ms
step:482/1660 train_time:44099ms step_avg:91.49ms
step:483/1660 train_time:44191ms step_avg:91.49ms
step:484/1660 train_time:44283ms step_avg:91.49ms
step:485/1660 train_time:44375ms step_avg:91.49ms
step:486/1660 train_time:44467ms step_avg:91.49ms
step:487/1660 train_time:44557ms step_avg:91.49ms
step:488/1660 train_time:44649ms step_avg:91.49ms
step:489/1660 train_time:44740ms step_avg:91.49ms
step:490/1660 train_time:44831ms step_avg:91.49ms
step:491/1660 train_time:44922ms step_avg:91.49ms
step:492/1660 train_time:45014ms step_avg:91.49ms
step:493/1660 train_time:45105ms step_avg:91.49ms
step:494/1660 train_time:45196ms step_avg:91.49ms
step:495/1660 train_time:45288ms step_avg:91.49ms
step:496/1660 train_time:45380ms step_avg:91.49ms
step:497/1660 train_time:45471ms step_avg:91.49ms
step:498/1660 train_time:45562ms step_avg:91.49ms
step:499/1660 train_time:45653ms step_avg:91.49ms
step:500/1660 train_time:45745ms step_avg:91.49ms
step:500/1660 val_loss:3.7144 train_time:45838ms step_avg:91.68ms
step:501/1660 train_time:45858ms step_avg:91.53ms
step:502/1660 train_time:45932ms step_avg:91.50ms
step:503/1660 train_time:46029ms step_avg:91.51ms
step:504/1660 train_time:46123ms step_avg:91.51ms
step:505/1660 train_time:46214ms step_avg:91.51ms
step:506/1660 train_time:46304ms step_avg:91.51ms
step:507/1660 train_time:46394ms step_avg:91.51ms
step:508/1660 train_time:46485ms step_avg:91.51ms
step:509/1660 train_time:46576ms step_avg:91.50ms
step:510/1660 train_time:46667ms step_avg:91.50ms
step:511/1660 train_time:46758ms step_avg:91.50ms
step:512/1660 train_time:46849ms step_avg:91.50ms
step:513/1660 train_time:46943ms step_avg:91.51ms
step:514/1660 train_time:47036ms step_avg:91.51ms
step:515/1660 train_time:47128ms step_avg:91.51ms
step:516/1660 train_time:47220ms step_avg:91.51ms
step:517/1660 train_time:47311ms step_avg:91.51ms
step:518/1660 train_time:47402ms step_avg:91.51ms
step:519/1660 train_time:47492ms step_avg:91.51ms
step:520/1660 train_time:47583ms step_avg:91.51ms
step:521/1660 train_time:47674ms step_avg:91.50ms
step:522/1660 train_time:47765ms step_avg:91.50ms
step:523/1660 train_time:47857ms step_avg:91.50ms
step:524/1660 train_time:47949ms step_avg:91.51ms
step:525/1660 train_time:48043ms step_avg:91.51ms
step:526/1660 train_time:48135ms step_avg:91.51ms
step:527/1660 train_time:48227ms step_avg:91.51ms
step:528/1660 train_time:48319ms step_avg:91.51ms
step:529/1660 train_time:48410ms step_avg:91.51ms
step:530/1660 train_time:48500ms step_avg:91.51ms
step:531/1660 train_time:48591ms step_avg:91.51ms
step:532/1660 train_time:48683ms step_avg:91.51ms
step:533/1660 train_time:48774ms step_avg:91.51ms
step:534/1660 train_time:48866ms step_avg:91.51ms
step:535/1660 train_time:48958ms step_avg:91.51ms
step:536/1660 train_time:49050ms step_avg:91.51ms
step:537/1660 train_time:49142ms step_avg:91.51ms
step:538/1660 train_time:49234ms step_avg:91.51ms
step:539/1660 train_time:49327ms step_avg:91.52ms
step:540/1660 train_time:49419ms step_avg:91.52ms
step:541/1660 train_time:49509ms step_avg:91.51ms
step:542/1660 train_time:49600ms step_avg:91.51ms
step:543/1660 train_time:49690ms step_avg:91.51ms
step:544/1660 train_time:49781ms step_avg:91.51ms
step:545/1660 train_time:49872ms step_avg:91.51ms
step:546/1660 train_time:49963ms step_avg:91.51ms
step:547/1660 train_time:50055ms step_avg:91.51ms
step:548/1660 train_time:50148ms step_avg:91.51ms
step:549/1660 train_time:50240ms step_avg:91.51ms
step:550/1660 train_time:50331ms step_avg:91.51ms
step:551/1660 train_time:50422ms step_avg:91.51ms
step:552/1660 train_time:50513ms step_avg:91.51ms
step:553/1660 train_time:50604ms step_avg:91.51ms
step:554/1660 train_time:50695ms step_avg:91.51ms
step:555/1660 train_time:50786ms step_avg:91.51ms
step:556/1660 train_time:50879ms step_avg:91.51ms
step:557/1660 train_time:50973ms step_avg:91.51ms
step:558/1660 train_time:51067ms step_avg:91.52ms
step:559/1660 train_time:51161ms step_avg:91.52ms
step:560/1660 train_time:51253ms step_avg:91.52ms
step:561/1660 train_time:51347ms step_avg:91.53ms
step:562/1660 train_time:51440ms step_avg:91.53ms
step:563/1660 train_time:51532ms step_avg:91.53ms
step:564/1660 train_time:51625ms step_avg:91.53ms
step:565/1660 train_time:51718ms step_avg:91.54ms
step:566/1660 train_time:51810ms step_avg:91.54ms
step:567/1660 train_time:51903ms step_avg:91.54ms
step:568/1660 train_time:51996ms step_avg:91.54ms
step:569/1660 train_time:52088ms step_avg:91.54ms
step:570/1660 train_time:52181ms step_avg:91.55ms
step:571/1660 train_time:52275ms step_avg:91.55ms
step:572/1660 train_time:52368ms step_avg:91.55ms
step:573/1660 train_time:52461ms step_avg:91.55ms
step:574/1660 train_time:52553ms step_avg:91.56ms
step:575/1660 train_time:52647ms step_avg:91.56ms
step:576/1660 train_time:52740ms step_avg:91.56ms
step:577/1660 train_time:52833ms step_avg:91.56ms
step:578/1660 train_time:52925ms step_avg:91.57ms
step:579/1660 train_time:53017ms step_avg:91.57ms
step:580/1660 train_time:53109ms step_avg:91.57ms
step:581/1660 train_time:53203ms step_avg:91.57ms
step:582/1660 train_time:53296ms step_avg:91.57ms
step:583/1660 train_time:53389ms step_avg:91.58ms
step:584/1660 train_time:53481ms step_avg:91.58ms
step:585/1660 train_time:53574ms step_avg:91.58ms
step:586/1660 train_time:53667ms step_avg:91.58ms
step:587/1660 train_time:53760ms step_avg:91.58ms
step:588/1660 train_time:53852ms step_avg:91.58ms
step:589/1660 train_time:53946ms step_avg:91.59ms
step:590/1660 train_time:54039ms step_avg:91.59ms
step:591/1660 train_time:54131ms step_avg:91.59ms
step:592/1660 train_time:54224ms step_avg:91.59ms
step:593/1660 train_time:54317ms step_avg:91.60ms
step:594/1660 train_time:54410ms step_avg:91.60ms
step:595/1660 train_time:54503ms step_avg:91.60ms
step:596/1660 train_time:54596ms step_avg:91.60ms
step:597/1660 train_time:54689ms step_avg:91.61ms
step:598/1660 train_time:54781ms step_avg:91.61ms
step:599/1660 train_time:54873ms step_avg:91.61ms
step:600/1660 train_time:54967ms step_avg:91.61ms
step:601/1660 train_time:55060ms step_avg:91.61ms
step:602/1660 train_time:55152ms step_avg:91.61ms
step:603/1660 train_time:55245ms step_avg:91.62ms
step:604/1660 train_time:55339ms step_avg:91.62ms
step:605/1660 train_time:55431ms step_avg:91.62ms
step:606/1660 train_time:55524ms step_avg:91.62ms
step:607/1660 train_time:55617ms step_avg:91.63ms
step:608/1660 train_time:55710ms step_avg:91.63ms
step:609/1660 train_time:55803ms step_avg:91.63ms
step:610/1660 train_time:55895ms step_avg:91.63ms
step:611/1660 train_time:55988ms step_avg:91.63ms
step:612/1660 train_time:56080ms step_avg:91.63ms
step:613/1660 train_time:56172ms step_avg:91.63ms
step:614/1660 train_time:56265ms step_avg:91.64ms
step:615/1660 train_time:56358ms step_avg:91.64ms
step:616/1660 train_time:56450ms step_avg:91.64ms
step:617/1660 train_time:56544ms step_avg:91.64ms
step:618/1660 train_time:56637ms step_avg:91.65ms
step:619/1660 train_time:56729ms step_avg:91.65ms
step:620/1660 train_time:56822ms step_avg:91.65ms
step:621/1660 train_time:56914ms step_avg:91.65ms
step:622/1660 train_time:57008ms step_avg:91.65ms
step:623/1660 train_time:57100ms step_avg:91.65ms
step:624/1660 train_time:57193ms step_avg:91.66ms
step:625/1660 train_time:57286ms step_avg:91.66ms
step:625/1660 val_loss:3.6126 train_time:57381ms step_avg:91.81ms
step:626/1660 train_time:57401ms step_avg:91.70ms
step:627/1660 train_time:57478ms step_avg:91.67ms
step:628/1660 train_time:57576ms step_avg:91.68ms
step:629/1660 train_time:57669ms step_avg:91.68ms
step:630/1660 train_time:57761ms step_avg:91.68ms
step:631/1660 train_time:57852ms step_avg:91.68ms
step:632/1660 train_time:57943ms step_avg:91.68ms
step:633/1660 train_time:58034ms step_avg:91.68ms
step:634/1660 train_time:58125ms step_avg:91.68ms
step:635/1660 train_time:58217ms step_avg:91.68ms
step:636/1660 train_time:58312ms step_avg:91.69ms
step:637/1660 train_time:58408ms step_avg:91.69ms
step:638/1660 train_time:58503ms step_avg:91.70ms
step:639/1660 train_time:58597ms step_avg:91.70ms
step:640/1660 train_time:58689ms step_avg:91.70ms
step:641/1660 train_time:58782ms step_avg:91.70ms
step:642/1660 train_time:58874ms step_avg:91.70ms
step:643/1660 train_time:58966ms step_avg:91.70ms
step:644/1660 train_time:59057ms step_avg:91.70ms
step:645/1660 train_time:59149ms step_avg:91.70ms
step:646/1660 train_time:59241ms step_avg:91.70ms
step:647/1660 train_time:59334ms step_avg:91.71ms
step:648/1660 train_time:59432ms step_avg:91.72ms
step:649/1660 train_time:59527ms step_avg:91.72ms
step:650/1660 train_time:59620ms step_avg:91.72ms
step:651/1660 train_time:59712ms step_avg:91.72ms
step:652/1660 train_time:59805ms step_avg:91.73ms
step:653/1660 train_time:59897ms step_avg:91.73ms
step:654/1660 train_time:59990ms step_avg:91.73ms
step:655/1660 train_time:60082ms step_avg:91.73ms
step:656/1660 train_time:60175ms step_avg:91.73ms
step:657/1660 train_time:60268ms step_avg:91.73ms
step:658/1660 train_time:60362ms step_avg:91.74ms
step:659/1660 train_time:60456ms step_avg:91.74ms
step:660/1660 train_time:60552ms step_avg:91.74ms
step:661/1660 train_time:60645ms step_avg:91.75ms
step:662/1660 train_time:60737ms step_avg:91.75ms
step:663/1660 train_time:60830ms step_avg:91.75ms
step:664/1660 train_time:60922ms step_avg:91.75ms
step:665/1660 train_time:61014ms step_avg:91.75ms
step:666/1660 train_time:61106ms step_avg:91.75ms
step:667/1660 train_time:61198ms step_avg:91.75ms
step:668/1660 train_time:61291ms step_avg:91.75ms
step:669/1660 train_time:61385ms step_avg:91.76ms
step:670/1660 train_time:61478ms step_avg:91.76ms
step:671/1660 train_time:61572ms step_avg:91.76ms
step:672/1660 train_time:61665ms step_avg:91.76ms
step:673/1660 train_time:61757ms step_avg:91.76ms
step:674/1660 train_time:61850ms step_avg:91.77ms
step:675/1660 train_time:61942ms step_avg:91.77ms
step:676/1660 train_time:62033ms step_avg:91.77ms
step:677/1660 train_time:62126ms step_avg:91.77ms
step:678/1660 train_time:62218ms step_avg:91.77ms
step:679/1660 train_time:62311ms step_avg:91.77ms
step:680/1660 train_time:62404ms step_avg:91.77ms
step:681/1660 train_time:62497ms step_avg:91.77ms
step:682/1660 train_time:62590ms step_avg:91.77ms
step:683/1660 train_time:62683ms step_avg:91.78ms
step:684/1660 train_time:62776ms step_avg:91.78ms
step:685/1660 train_time:62869ms step_avg:91.78ms
step:686/1660 train_time:62962ms step_avg:91.78ms
step:687/1660 train_time:63054ms step_avg:91.78ms
step:688/1660 train_time:63147ms step_avg:91.78ms
step:689/1660 train_time:63239ms step_avg:91.78ms
step:690/1660 train_time:63331ms step_avg:91.78ms
step:691/1660 train_time:63425ms step_avg:91.79ms
step:692/1660 train_time:63517ms step_avg:91.79ms
step:693/1660 train_time:63610ms step_avg:91.79ms
step:694/1660 train_time:63702ms step_avg:91.79ms
step:695/1660 train_time:63795ms step_avg:91.79ms
step:696/1660 train_time:63888ms step_avg:91.79ms
step:697/1660 train_time:63981ms step_avg:91.80ms
step:698/1660 train_time:64074ms step_avg:91.80ms
step:699/1660 train_time:64166ms step_avg:91.80ms
step:700/1660 train_time:64259ms step_avg:91.80ms
step:701/1660 train_time:64351ms step_avg:91.80ms
step:702/1660 train_time:64445ms step_avg:91.80ms
step:703/1660 train_time:64537ms step_avg:91.80ms
step:704/1660 train_time:64631ms step_avg:91.81ms
step:705/1660 train_time:64725ms step_avg:91.81ms
step:706/1660 train_time:64817ms step_avg:91.81ms
step:707/1660 train_time:64911ms step_avg:91.81ms
step:708/1660 train_time:65004ms step_avg:91.81ms
step:709/1660 train_time:65096ms step_avg:91.81ms
step:710/1660 train_time:65189ms step_avg:91.82ms
step:711/1660 train_time:65282ms step_avg:91.82ms
step:712/1660 train_time:65374ms step_avg:91.82ms
step:713/1660 train_time:65466ms step_avg:91.82ms
step:714/1660 train_time:65558ms step_avg:91.82ms
step:715/1660 train_time:65651ms step_avg:91.82ms
step:716/1660 train_time:65744ms step_avg:91.82ms
step:717/1660 train_time:65837ms step_avg:91.82ms
step:718/1660 train_time:65930ms step_avg:91.82ms
step:719/1660 train_time:66023ms step_avg:91.83ms
step:720/1660 train_time:66116ms step_avg:91.83ms
step:721/1660 train_time:66209ms step_avg:91.83ms
step:722/1660 train_time:66302ms step_avg:91.83ms
step:723/1660 train_time:66394ms step_avg:91.83ms
step:724/1660 train_time:66486ms step_avg:91.83ms
step:725/1660 train_time:66579ms step_avg:91.83ms
step:726/1660 train_time:66672ms step_avg:91.83ms
step:727/1660 train_time:66764ms step_avg:91.84ms
step:728/1660 train_time:66857ms step_avg:91.84ms
step:729/1660 train_time:66951ms step_avg:91.84ms
step:730/1660 train_time:67044ms step_avg:91.84ms
step:731/1660 train_time:67137ms step_avg:91.84ms
step:732/1660 train_time:67230ms step_avg:91.84ms
step:733/1660 train_time:67323ms step_avg:91.85ms
step:734/1660 train_time:67415ms step_avg:91.85ms
step:735/1660 train_time:67508ms step_avg:91.85ms
step:736/1660 train_time:67601ms step_avg:91.85ms
step:737/1660 train_time:67694ms step_avg:91.85ms
step:738/1660 train_time:67786ms step_avg:91.85ms
step:739/1660 train_time:67880ms step_avg:91.85ms
step:740/1660 train_time:67972ms step_avg:91.85ms
step:741/1660 train_time:68065ms step_avg:91.86ms
step:742/1660 train_time:68158ms step_avg:91.86ms
step:743/1660 train_time:68250ms step_avg:91.86ms
step:744/1660 train_time:68343ms step_avg:91.86ms
step:745/1660 train_time:68435ms step_avg:91.86ms
step:746/1660 train_time:68527ms step_avg:91.86ms
step:747/1660 train_time:68620ms step_avg:91.86ms
step:748/1660 train_time:68712ms step_avg:91.86ms
step:749/1660 train_time:68804ms step_avg:91.86ms
step:750/1660 train_time:68897ms step_avg:91.86ms
step:750/1660 val_loss:3.5588 train_time:68991ms step_avg:91.99ms
step:751/1660 train_time:69011ms step_avg:91.89ms
step:752/1660 train_time:69087ms step_avg:91.87ms
step:753/1660 train_time:69183ms step_avg:91.88ms
step:754/1660 train_time:69277ms step_avg:91.88ms
step:755/1660 train_time:69368ms step_avg:91.88ms
step:756/1660 train_time:69460ms step_avg:91.88ms
step:757/1660 train_time:69552ms step_avg:91.88ms
step:758/1660 train_time:69644ms step_avg:91.88ms
step:759/1660 train_time:69735ms step_avg:91.88ms
step:760/1660 train_time:69827ms step_avg:91.88ms
step:761/1660 train_time:69920ms step_avg:91.88ms
step:762/1660 train_time:70014ms step_avg:91.88ms
step:763/1660 train_time:70109ms step_avg:91.89ms
step:764/1660 train_time:70204ms step_avg:91.89ms
step:765/1660 train_time:70297ms step_avg:91.89ms
step:766/1660 train_time:70390ms step_avg:91.89ms
step:767/1660 train_time:70483ms step_avg:91.89ms
step:768/1660 train_time:70575ms step_avg:91.89ms
step:769/1660 train_time:70667ms step_avg:91.89ms
step:770/1660 train_time:70758ms step_avg:91.89ms
step:771/1660 train_time:70851ms step_avg:91.90ms
step:772/1660 train_time:70944ms step_avg:91.90ms
step:773/1660 train_time:71039ms step_avg:91.90ms
step:774/1660 train_time:71134ms step_avg:91.90ms
step:775/1660 train_time:71227ms step_avg:91.91ms
step:776/1660 train_time:71320ms step_avg:91.91ms
step:777/1660 train_time:71413ms step_avg:91.91ms
step:778/1660 train_time:71505ms step_avg:91.91ms
step:779/1660 train_time:71598ms step_avg:91.91ms
step:780/1660 train_time:71690ms step_avg:91.91ms
step:781/1660 train_time:71783ms step_avg:91.91ms
step:782/1660 train_time:71875ms step_avg:91.91ms
step:783/1660 train_time:71967ms step_avg:91.91ms
step:784/1660 train_time:72061ms step_avg:91.91ms
step:785/1660 train_time:72155ms step_avg:91.92ms
step:786/1660 train_time:72248ms step_avg:91.92ms
step:787/1660 train_time:72340ms step_avg:91.92ms
step:788/1660 train_time:72433ms step_avg:91.92ms
step:789/1660 train_time:72526ms step_avg:91.92ms
step:790/1660 train_time:72620ms step_avg:91.92ms
step:791/1660 train_time:72712ms step_avg:91.92ms
step:792/1660 train_time:72804ms step_avg:91.92ms
step:793/1660 train_time:72897ms step_avg:91.93ms
step:794/1660 train_time:72989ms step_avg:91.93ms
step:795/1660 train_time:73083ms step_avg:91.93ms
step:796/1660 train_time:73175ms step_avg:91.93ms
step:797/1660 train_time:73268ms step_avg:91.93ms
step:798/1660 train_time:73361ms step_avg:91.93ms
step:799/1660 train_time:73454ms step_avg:91.93ms
step:800/1660 train_time:73546ms step_avg:91.93ms
step:801/1660 train_time:73639ms step_avg:91.93ms
step:802/1660 train_time:73731ms step_avg:91.93ms
step:803/1660 train_time:73825ms step_avg:91.94ms
step:804/1660 train_time:73918ms step_avg:91.94ms
step:805/1660 train_time:74010ms step_avg:91.94ms
step:806/1660 train_time:74104ms step_avg:91.94ms
step:807/1660 train_time:74197ms step_avg:91.94ms
step:808/1660 train_time:74290ms step_avg:91.94ms
step:809/1660 train_time:74383ms step_avg:91.94ms
step:810/1660 train_time:74475ms step_avg:91.94ms
step:811/1660 train_time:74567ms step_avg:91.94ms
step:812/1660 train_time:74660ms step_avg:91.95ms
step:813/1660 train_time:74752ms step_avg:91.95ms
step:814/1660 train_time:74845ms step_avg:91.95ms
step:815/1660 train_time:74937ms step_avg:91.95ms
step:816/1660 train_time:75030ms step_avg:91.95ms
step:817/1660 train_time:75123ms step_avg:91.95ms
step:818/1660 train_time:75216ms step_avg:91.95ms
step:819/1660 train_time:75309ms step_avg:91.95ms
step:820/1660 train_time:75402ms step_avg:91.95ms
step:821/1660 train_time:75496ms step_avg:91.96ms
step:822/1660 train_time:75589ms step_avg:91.96ms
step:823/1660 train_time:75682ms step_avg:91.96ms
step:824/1660 train_time:75775ms step_avg:91.96ms
step:825/1660 train_time:75867ms step_avg:91.96ms
step:826/1660 train_time:75959ms step_avg:91.96ms
step:827/1660 train_time:76052ms step_avg:91.96ms
step:828/1660 train_time:76144ms step_avg:91.96ms
step:829/1660 train_time:76237ms step_avg:91.96ms
step:830/1660 train_time:76330ms step_avg:91.96ms
step:831/1660 train_time:76423ms step_avg:91.96ms
step:832/1660 train_time:76515ms step_avg:91.97ms
step:833/1660 train_time:76608ms step_avg:91.97ms
step:834/1660 train_time:76702ms step_avg:91.97ms
step:835/1660 train_time:76795ms step_avg:91.97ms
step:836/1660 train_time:76887ms step_avg:91.97ms
step:837/1660 train_time:76979ms step_avg:91.97ms
step:838/1660 train_time:77072ms step_avg:91.97ms
step:839/1660 train_time:77165ms step_avg:91.97ms
step:840/1660 train_time:77259ms step_avg:91.98ms
step:841/1660 train_time:77351ms step_avg:91.98ms
step:842/1660 train_time:77445ms step_avg:91.98ms
step:843/1660 train_time:77538ms step_avg:91.98ms
step:844/1660 train_time:77630ms step_avg:91.98ms
step:845/1660 train_time:77723ms step_avg:91.98ms
step:846/1660 train_time:77816ms step_avg:91.98ms
step:847/1660 train_time:77908ms step_avg:91.98ms
step:848/1660 train_time:78001ms step_avg:91.98ms
step:849/1660 train_time:78093ms step_avg:91.98ms
step:850/1660 train_time:78186ms step_avg:91.98ms
step:851/1660 train_time:78279ms step_avg:91.98ms
step:852/1660 train_time:78372ms step_avg:91.99ms
step:853/1660 train_time:78465ms step_avg:91.99ms
step:854/1660 train_time:78558ms step_avg:91.99ms
step:855/1660 train_time:78651ms step_avg:91.99ms
step:856/1660 train_time:78745ms step_avg:91.99ms
step:857/1660 train_time:78837ms step_avg:91.99ms
step:858/1660 train_time:78930ms step_avg:91.99ms
step:859/1660 train_time:79022ms step_avg:91.99ms
step:860/1660 train_time:79115ms step_avg:91.99ms
step:861/1660 train_time:79207ms step_avg:91.99ms
step:862/1660 train_time:79300ms step_avg:92.00ms
step:863/1660 train_time:79393ms step_avg:92.00ms
step:864/1660 train_time:79485ms step_avg:92.00ms
step:865/1660 train_time:79579ms step_avg:92.00ms
step:866/1660 train_time:79671ms step_avg:92.00ms
step:867/1660 train_time:79765ms step_avg:92.00ms
step:868/1660 train_time:79858ms step_avg:92.00ms
step:869/1660 train_time:79950ms step_avg:92.00ms
step:870/1660 train_time:80043ms step_avg:92.00ms
step:871/1660 train_time:80136ms step_avg:92.00ms
step:872/1660 train_time:80229ms step_avg:92.01ms
step:873/1660 train_time:80322ms step_avg:92.01ms
step:874/1660 train_time:80415ms step_avg:92.01ms
step:875/1660 train_time:80507ms step_avg:92.01ms
step:875/1660 val_loss:3.5150 train_time:80601ms step_avg:92.12ms
step:876/1660 train_time:80621ms step_avg:92.03ms
step:877/1660 train_time:80697ms step_avg:92.02ms
step:878/1660 train_time:80796ms step_avg:92.02ms
step:879/1660 train_time:80889ms step_avg:92.02ms
step:880/1660 train_time:80982ms step_avg:92.02ms
step:881/1660 train_time:81073ms step_avg:92.02ms
step:882/1660 train_time:81165ms step_avg:92.02ms
step:883/1660 train_time:81256ms step_avg:92.02ms
step:884/1660 train_time:81347ms step_avg:92.02ms
step:885/1660 train_time:81439ms step_avg:92.02ms
step:886/1660 train_time:81531ms step_avg:92.02ms
step:887/1660 train_time:81626ms step_avg:92.02ms
step:888/1660 train_time:81721ms step_avg:92.03ms
step:889/1660 train_time:81816ms step_avg:92.03ms
step:890/1660 train_time:81909ms step_avg:92.03ms
step:891/1660 train_time:82002ms step_avg:92.03ms
step:892/1660 train_time:82094ms step_avg:92.03ms
step:893/1660 train_time:82186ms step_avg:92.03ms
step:894/1660 train_time:82278ms step_avg:92.03ms
step:895/1660 train_time:82369ms step_avg:92.03ms
step:896/1660 train_time:82462ms step_avg:92.03ms
step:897/1660 train_time:82554ms step_avg:92.03ms
step:898/1660 train_time:82648ms step_avg:92.04ms
step:899/1660 train_time:82743ms step_avg:92.04ms
step:900/1660 train_time:82837ms step_avg:92.04ms
step:901/1660 train_time:82930ms step_avg:92.04ms
step:902/1660 train_time:83022ms step_avg:92.04ms
step:903/1660 train_time:83114ms step_avg:92.04ms
step:904/1660 train_time:83207ms step_avg:92.04ms
step:905/1660 train_time:83300ms step_avg:92.04ms
step:906/1660 train_time:83391ms step_avg:92.04ms
step:907/1660 train_time:83483ms step_avg:92.04ms
step:908/1660 train_time:83577ms step_avg:92.04ms
step:909/1660 train_time:83670ms step_avg:92.05ms
step:910/1660 train_time:83765ms step_avg:92.05ms
step:911/1660 train_time:83858ms step_avg:92.05ms
step:912/1660 train_time:83951ms step_avg:92.05ms
step:913/1660 train_time:84043ms step_avg:92.05ms
step:914/1660 train_time:84136ms step_avg:92.05ms
step:915/1660 train_time:84229ms step_avg:92.05ms
step:916/1660 train_time:84322ms step_avg:92.05ms
step:917/1660 train_time:84413ms step_avg:92.05ms
step:918/1660 train_time:84507ms step_avg:92.06ms
step:919/1660 train_time:84600ms step_avg:92.06ms
step:920/1660 train_time:84692ms step_avg:92.06ms
step:921/1660 train_time:84786ms step_avg:92.06ms
step:922/1660 train_time:84880ms step_avg:92.06ms
step:923/1660 train_time:84973ms step_avg:92.06ms
step:924/1660 train_time:85066ms step_avg:92.06ms
step:925/1660 train_time:85158ms step_avg:92.06ms
step:926/1660 train_time:85250ms step_avg:92.06ms
step:927/1660 train_time:85343ms step_avg:92.06ms
step:928/1660 train_time:85436ms step_avg:92.07ms
step:929/1660 train_time:85529ms step_avg:92.07ms
step:930/1660 train_time:85622ms step_avg:92.07ms
step:931/1660 train_time:85715ms step_avg:92.07ms
step:932/1660 train_time:85808ms step_avg:92.07ms
step:933/1660 train_time:85901ms step_avg:92.07ms
step:934/1660 train_time:85993ms step_avg:92.07ms
step:935/1660 train_time:86087ms step_avg:92.07ms
step:936/1660 train_time:86180ms step_avg:92.07ms
step:937/1660 train_time:86273ms step_avg:92.07ms
step:938/1660 train_time:86366ms step_avg:92.07ms
step:939/1660 train_time:86459ms step_avg:92.08ms
step:940/1660 train_time:86552ms step_avg:92.08ms
step:941/1660 train_time:86644ms step_avg:92.08ms
step:942/1660 train_time:86737ms step_avg:92.08ms
step:943/1660 train_time:86830ms step_avg:92.08ms
step:944/1660 train_time:86922ms step_avg:92.08ms
step:945/1660 train_time:87015ms step_avg:92.08ms
step:946/1660 train_time:87108ms step_avg:92.08ms
step:947/1660 train_time:87203ms step_avg:92.08ms
step:948/1660 train_time:87295ms step_avg:92.08ms
step:949/1660 train_time:87388ms step_avg:92.08ms
step:950/1660 train_time:87481ms step_avg:92.08ms
step:951/1660 train_time:87573ms step_avg:92.09ms
step:952/1660 train_time:87666ms step_avg:92.09ms
step:953/1660 train_time:87759ms step_avg:92.09ms
step:954/1660 train_time:87852ms step_avg:92.09ms
step:955/1660 train_time:87945ms step_avg:92.09ms
step:956/1660 train_time:88038ms step_avg:92.09ms
step:957/1660 train_time:88131ms step_avg:92.09ms
step:958/1660 train_time:88225ms step_avg:92.09ms
step:959/1660 train_time:88317ms step_avg:92.09ms
step:960/1660 train_time:88410ms step_avg:92.09ms
step:961/1660 train_time:88504ms step_avg:92.10ms
step:962/1660 train_time:88598ms step_avg:92.10ms
step:963/1660 train_time:88690ms step_avg:92.10ms
step:964/1660 train_time:88782ms step_avg:92.10ms
step:965/1660 train_time:88875ms step_avg:92.10ms
step:966/1660 train_time:88967ms step_avg:92.10ms
step:967/1660 train_time:89060ms step_avg:92.10ms
step:968/1660 train_time:89153ms step_avg:92.10ms
step:969/1660 train_time:89246ms step_avg:92.10ms
step:970/1660 train_time:89339ms step_avg:92.10ms
step:971/1660 train_time:89432ms step_avg:92.10ms
step:972/1660 train_time:89526ms step_avg:92.10ms
step:973/1660 train_time:89618ms step_avg:92.10ms
step:974/1660 train_time:89711ms step_avg:92.11ms
step:975/1660 train_time:89804ms step_avg:92.11ms
step:976/1660 train_time:89896ms step_avg:92.11ms
step:977/1660 train_time:89989ms step_avg:92.11ms
step:978/1660 train_time:90082ms step_avg:92.11ms
step:979/1660 train_time:90175ms step_avg:92.11ms
step:980/1660 train_time:90268ms step_avg:92.11ms
step:981/1660 train_time:90361ms step_avg:92.11ms
step:982/1660 train_time:90454ms step_avg:92.11ms
step:983/1660 train_time:90547ms step_avg:92.11ms
step:984/1660 train_time:90640ms step_avg:92.11ms
step:985/1660 train_time:90733ms step_avg:92.11ms
step:986/1660 train_time:90826ms step_avg:92.12ms
step:987/1660 train_time:90919ms step_avg:92.12ms
step:988/1660 train_time:91012ms step_avg:92.12ms
step:989/1660 train_time:91105ms step_avg:92.12ms
step:990/1660 train_time:91199ms step_avg:92.12ms
step:991/1660 train_time:91291ms step_avg:92.12ms
step:992/1660 train_time:91384ms step_avg:92.12ms
step:993/1660 train_time:91477ms step_avg:92.12ms
step:994/1660 train_time:91570ms step_avg:92.12ms
step:995/1660 train_time:91662ms step_avg:92.12ms
step:996/1660 train_time:91755ms step_avg:92.12ms
step:997/1660 train_time:91848ms step_avg:92.12ms
step:998/1660 train_time:91942ms step_avg:92.13ms
step:999/1660 train_time:92034ms step_avg:92.13ms
step:1000/1660 train_time:92128ms step_avg:92.13ms
step:1000/1660 val_loss:3.4647 train_time:92222ms step_avg:92.22ms
step:1001/1660 train_time:92243ms step_avg:92.15ms
step:1002/1660 train_time:92320ms step_avg:92.14ms
step:1003/1660 train_time:92419ms step_avg:92.14ms
step:1004/1660 train_time:92512ms step_avg:92.14ms
step:1005/1660 train_time:92604ms step_avg:92.14ms
step:1006/1660 train_time:92695ms step_avg:92.14ms
step:1007/1660 train_time:92786ms step_avg:92.14ms
step:1008/1660 train_time:92878ms step_avg:92.14ms
step:1009/1660 train_time:92969ms step_avg:92.14ms
step:1010/1660 train_time:93061ms step_avg:92.14ms
step:1011/1660 train_time:93153ms step_avg:92.14ms
step:1012/1660 train_time:93248ms step_avg:92.14ms
step:1013/1660 train_time:93345ms step_avg:92.15ms
step:1014/1660 train_time:93440ms step_avg:92.15ms
step:1015/1660 train_time:93533ms step_avg:92.15ms
step:1016/1660 train_time:93626ms step_avg:92.15ms
step:1017/1660 train_time:93719ms step_avg:92.15ms
step:1018/1660 train_time:93810ms step_avg:92.15ms
step:1019/1660 train_time:93902ms step_avg:92.15ms
step:1020/1660 train_time:93994ms step_avg:92.15ms
step:1021/1660 train_time:94085ms step_avg:92.15ms
step:1022/1660 train_time:94178ms step_avg:92.15ms
step:1023/1660 train_time:94273ms step_avg:92.15ms
step:1024/1660 train_time:94369ms step_avg:92.16ms
step:1025/1660 train_time:94464ms step_avg:92.16ms
step:1026/1660 train_time:94556ms step_avg:92.16ms
step:1027/1660 train_time:94649ms step_avg:92.16ms
step:1028/1660 train_time:94741ms step_avg:92.16ms
step:1029/1660 train_time:94834ms step_avg:92.16ms
step:1030/1660 train_time:94926ms step_avg:92.16ms
step:1031/1660 train_time:95017ms step_avg:92.16ms
step:1032/1660 train_time:95109ms step_avg:92.16ms
step:1033/1660 train_time:95201ms step_avg:92.16ms
step:1034/1660 train_time:95296ms step_avg:92.16ms
step:1035/1660 train_time:95389ms step_avg:92.16ms
step:1036/1660 train_time:95482ms step_avg:92.16ms
step:1037/1660 train_time:95575ms step_avg:92.16ms
step:1038/1660 train_time:95668ms step_avg:92.17ms
step:1039/1660 train_time:95761ms step_avg:92.17ms
step:1040/1660 train_time:95853ms step_avg:92.17ms
step:1041/1660 train_time:95947ms step_avg:92.17ms
step:1042/1660 train_time:96039ms step_avg:92.17ms
step:1043/1660 train_time:96131ms step_avg:92.17ms
step:1044/1660 train_time:96225ms step_avg:92.17ms
step:1045/1660 train_time:96318ms step_avg:92.17ms
step:1046/1660 train_time:96411ms step_avg:92.17ms
step:1047/1660 train_time:96505ms step_avg:92.17ms
step:1048/1660 train_time:96599ms step_avg:92.17ms
step:1049/1660 train_time:96691ms step_avg:92.17ms
step:1050/1660 train_time:96784ms step_avg:92.17ms
step:1051/1660 train_time:96876ms step_avg:92.17ms
step:1052/1660 train_time:96968ms step_avg:92.18ms
step:1053/1660 train_time:97061ms step_avg:92.18ms
step:1054/1660 train_time:97154ms step_avg:92.18ms
step:1055/1660 train_time:97248ms step_avg:92.18ms
step:1056/1660 train_time:97341ms step_avg:92.18ms
step:1057/1660 train_time:97434ms step_avg:92.18ms
step:1058/1660 train_time:97527ms step_avg:92.18ms
step:1059/1660 train_time:97620ms step_avg:92.18ms
step:1060/1660 train_time:97714ms step_avg:92.18ms
step:1061/1660 train_time:97806ms step_avg:92.18ms
step:1062/1660 train_time:97899ms step_avg:92.18ms
step:1063/1660 train_time:97991ms step_avg:92.18ms
step:1064/1660 train_time:98083ms step_avg:92.18ms
step:1065/1660 train_time:98176ms step_avg:92.18ms
step:1066/1660 train_time:98269ms step_avg:92.18ms
step:1067/1660 train_time:98362ms step_avg:92.19ms
step:1068/1660 train_time:98455ms step_avg:92.19ms
step:1069/1660 train_time:98549ms step_avg:92.19ms
step:1070/1660 train_time:98643ms step_avg:92.19ms
step:1071/1660 train_time:98735ms step_avg:92.19ms
step:1072/1660 train_time:98828ms step_avg:92.19ms
step:1073/1660 train_time:98920ms step_avg:92.19ms
step:1074/1660 train_time:99012ms step_avg:92.19ms
step:1075/1660 train_time:99104ms step_avg:92.19ms
step:1076/1660 train_time:99197ms step_avg:92.19ms
step:1077/1660 train_time:99289ms step_avg:92.19ms
step:1078/1660 train_time:99383ms step_avg:92.19ms
step:1079/1660 train_time:99475ms step_avg:92.19ms
step:1080/1660 train_time:99569ms step_avg:92.19ms
step:1081/1660 train_time:99662ms step_avg:92.19ms
step:1082/1660 train_time:99754ms step_avg:92.19ms
step:1083/1660 train_time:99848ms step_avg:92.20ms
step:1084/1660 train_time:99940ms step_avg:92.20ms
step:1085/1660 train_time:100032ms step_avg:92.20ms
step:1086/1660 train_time:100125ms step_avg:92.20ms
step:1087/1660 train_time:100219ms step_avg:92.20ms
step:1088/1660 train_time:100312ms step_avg:92.20ms
step:1089/1660 train_time:100404ms step_avg:92.20ms
step:1090/1660 train_time:100497ms step_avg:92.20ms
step:1091/1660 train_time:100590ms step_avg:92.20ms
step:1092/1660 train_time:100683ms step_avg:92.20ms
step:1093/1660 train_time:100776ms step_avg:92.20ms
step:1094/1660 train_time:100869ms step_avg:92.20ms
step:1095/1660 train_time:100961ms step_avg:92.20ms
step:1096/1660 train_time:101053ms step_avg:92.20ms
step:1097/1660 train_time:101146ms step_avg:92.20ms
step:1098/1660 train_time:101239ms step_avg:92.20ms
step:1099/1660 train_time:101332ms step_avg:92.20ms
step:1100/1660 train_time:101426ms step_avg:92.21ms
step:1101/1660 train_time:101519ms step_avg:92.21ms
step:1102/1660 train_time:101611ms step_avg:92.21ms
step:1103/1660 train_time:101703ms step_avg:92.21ms
step:1104/1660 train_time:101797ms step_avg:92.21ms
step:1105/1660 train_time:101889ms step_avg:92.21ms
step:1106/1660 train_time:101981ms step_avg:92.21ms
step:1107/1660 train_time:102073ms step_avg:92.21ms
step:1108/1660 train_time:102166ms step_avg:92.21ms
step:1109/1660 train_time:102259ms step_avg:92.21ms
step:1110/1660 train_time:102353ms step_avg:92.21ms
step:1111/1660 train_time:102447ms step_avg:92.21ms
step:1112/1660 train_time:102541ms step_avg:92.21ms
step:1113/1660 train_time:102633ms step_avg:92.21ms
step:1114/1660 train_time:102728ms step_avg:92.22ms
step:1115/1660 train_time:102822ms step_avg:92.22ms
step:1116/1660 train_time:102916ms step_avg:92.22ms
step:1117/1660 train_time:103009ms step_avg:92.22ms
step:1118/1660 train_time:103101ms step_avg:92.22ms
step:1119/1660 train_time:103194ms step_avg:92.22ms
step:1120/1660 train_time:103288ms step_avg:92.22ms
step:1121/1660 train_time:103381ms step_avg:92.22ms
step:1122/1660 train_time:103475ms step_avg:92.22ms
step:1123/1660 train_time:103568ms step_avg:92.22ms
step:1124/1660 train_time:103662ms step_avg:92.23ms
step:1125/1660 train_time:103755ms step_avg:92.23ms
step:1125/1660 val_loss:3.4123 train_time:103851ms step_avg:92.31ms
step:1126/1660 train_time:103871ms step_avg:92.25ms
step:1127/1660 train_time:103950ms step_avg:92.24ms
step:1128/1660 train_time:104052ms step_avg:92.24ms
step:1129/1660 train_time:104147ms step_avg:92.25ms
step:1130/1660 train_time:104239ms step_avg:92.25ms
step:1131/1660 train_time:104332ms step_avg:92.25ms
step:1132/1660 train_time:104423ms step_avg:92.25ms
step:1133/1660 train_time:104516ms step_avg:92.25ms
step:1134/1660 train_time:104608ms step_avg:92.25ms
step:1135/1660 train_time:104700ms step_avg:92.25ms
step:1136/1660 train_time:104792ms step_avg:92.25ms
step:1137/1660 train_time:104886ms step_avg:92.25ms
step:1138/1660 train_time:104981ms step_avg:92.25ms
step:1139/1660 train_time:105077ms step_avg:92.25ms
step:1140/1660 train_time:105171ms step_avg:92.26ms
step:1141/1660 train_time:105264ms step_avg:92.26ms
step:1142/1660 train_time:105356ms step_avg:92.26ms
step:1143/1660 train_time:105448ms step_avg:92.26ms
step:1144/1660 train_time:105541ms step_avg:92.26ms
step:1145/1660 train_time:105634ms step_avg:92.26ms
step:1146/1660 train_time:105726ms step_avg:92.26ms
step:1147/1660 train_time:105819ms step_avg:92.26ms
step:1148/1660 train_time:105913ms step_avg:92.26ms
step:1149/1660 train_time:106007ms step_avg:92.26ms
step:1150/1660 train_time:106102ms step_avg:92.26ms
step:1151/1660 train_time:106196ms step_avg:92.26ms
step:1152/1660 train_time:106290ms step_avg:92.27ms
step:1153/1660 train_time:106383ms step_avg:92.27ms
step:1154/1660 train_time:106477ms step_avg:92.27ms
step:1155/1660 train_time:106571ms step_avg:92.27ms
step:1156/1660 train_time:106663ms step_avg:92.27ms
step:1157/1660 train_time:106755ms step_avg:92.27ms
step:1158/1660 train_time:106848ms step_avg:92.27ms
step:1159/1660 train_time:106942ms step_avg:92.27ms
step:1160/1660 train_time:107036ms step_avg:92.27ms
step:1161/1660 train_time:107130ms step_avg:92.27ms
step:1162/1660 train_time:107223ms step_avg:92.27ms
step:1163/1660 train_time:107318ms step_avg:92.28ms
step:1164/1660 train_time:107411ms step_avg:92.28ms
step:1165/1660 train_time:107504ms step_avg:92.28ms
step:1166/1660 train_time:107598ms step_avg:92.28ms
step:1167/1660 train_time:107691ms step_avg:92.28ms
step:1168/1660 train_time:107784ms step_avg:92.28ms
step:1169/1660 train_time:107876ms step_avg:92.28ms
step:1170/1660 train_time:107970ms step_avg:92.28ms
step:1171/1660 train_time:108064ms step_avg:92.28ms
step:1172/1660 train_time:108157ms step_avg:92.28ms
step:1173/1660 train_time:108251ms step_avg:92.29ms
step:1174/1660 train_time:108344ms step_avg:92.29ms
step:1175/1660 train_time:108437ms step_avg:92.29ms
step:1176/1660 train_time:108531ms step_avg:92.29ms
step:1177/1660 train_time:108624ms step_avg:92.29ms
step:1178/1660 train_time:108716ms step_avg:92.29ms
step:1179/1660 train_time:108809ms step_avg:92.29ms
step:1180/1660 train_time:108902ms step_avg:92.29ms
step:1181/1660 train_time:108997ms step_avg:92.29ms
step:1182/1660 train_time:109091ms step_avg:92.29ms
step:1183/1660 train_time:109184ms step_avg:92.29ms
step:1184/1660 train_time:109278ms step_avg:92.30ms
step:1185/1660 train_time:109371ms step_avg:92.30ms
step:1186/1660 train_time:109464ms step_avg:92.30ms
step:1187/1660 train_time:109557ms step_avg:92.30ms
step:1188/1660 train_time:109650ms step_avg:92.30ms
step:1189/1660 train_time:109743ms step_avg:92.30ms
step:1190/1660 train_time:109838ms step_avg:92.30ms
step:1191/1660 train_time:109932ms step_avg:92.30ms
step:1192/1660 train_time:110025ms step_avg:92.30ms
step:1193/1660 train_time:110119ms step_avg:92.30ms
step:1194/1660 train_time:110212ms step_avg:92.30ms
step:1195/1660 train_time:110304ms step_avg:92.30ms
step:1196/1660 train_time:110398ms step_avg:92.31ms
step:1197/1660 train_time:110491ms step_avg:92.31ms
step:1198/1660 train_time:110584ms step_avg:92.31ms
step:1199/1660 train_time:110678ms step_avg:92.31ms
step:1200/1660 train_time:110771ms step_avg:92.31ms
step:1201/1660 train_time:110864ms step_avg:92.31ms
step:1202/1660 train_time:110957ms step_avg:92.31ms
step:1203/1660 train_time:111050ms step_avg:92.31ms
step:1204/1660 train_time:111143ms step_avg:92.31ms
step:1205/1660 train_time:111237ms step_avg:92.31ms
step:1206/1660 train_time:111331ms step_avg:92.31ms
step:1207/1660 train_time:111425ms step_avg:92.32ms
step:1208/1660 train_time:111517ms step_avg:92.32ms
step:1209/1660 train_time:111610ms step_avg:92.32ms
step:1210/1660 train_time:111704ms step_avg:92.32ms
step:1211/1660 train_time:111797ms step_avg:92.32ms
step:1212/1660 train_time:111891ms step_avg:92.32ms
step:1213/1660 train_time:111984ms step_avg:92.32ms
step:1214/1660 train_time:112077ms step_avg:92.32ms
step:1215/1660 train_time:112171ms step_avg:92.32ms
step:1216/1660 train_time:112264ms step_avg:92.32ms
step:1217/1660 train_time:112358ms step_avg:92.32ms
step:1218/1660 train_time:112451ms step_avg:92.32ms
step:1219/1660 train_time:112543ms step_avg:92.32ms
step:1220/1660 train_time:112637ms step_avg:92.33ms
step:1221/1660 train_time:112730ms step_avg:92.33ms
step:1222/1660 train_time:112824ms step_avg:92.33ms
step:1223/1660 train_time:112917ms step_avg:92.33ms
step:1224/1660 train_time:113011ms step_avg:92.33ms
step:1225/1660 train_time:113104ms step_avg:92.33ms
step:1226/1660 train_time:113198ms step_avg:92.33ms
step:1227/1660 train_time:113291ms step_avg:92.33ms
step:1228/1660 train_time:113385ms step_avg:92.33ms
step:1229/1660 train_time:113478ms step_avg:92.33ms
step:1230/1660 train_time:113572ms step_avg:92.33ms
step:1231/1660 train_time:113665ms step_avg:92.34ms
step:1232/1660 train_time:113757ms step_avg:92.34ms
step:1233/1660 train_time:113850ms step_avg:92.34ms
step:1234/1660 train_time:113943ms step_avg:92.34ms
step:1235/1660 train_time:114036ms step_avg:92.34ms
step:1236/1660 train_time:114130ms step_avg:92.34ms
step:1237/1660 train_time:114224ms step_avg:92.34ms
step:1238/1660 train_time:114318ms step_avg:92.34ms
step:1239/1660 train_time:114411ms step_avg:92.34ms
step:1240/1660 train_time:114504ms step_avg:92.34ms
step:1241/1660 train_time:114598ms step_avg:92.34ms
step:1242/1660 train_time:114691ms step_avg:92.34ms
step:1243/1660 train_time:114784ms step_avg:92.34ms
step:1244/1660 train_time:114877ms step_avg:92.34ms
step:1245/1660 train_time:114971ms step_avg:92.35ms
step:1246/1660 train_time:115064ms step_avg:92.35ms
step:1247/1660 train_time:115157ms step_avg:92.35ms
step:1248/1660 train_time:115250ms step_avg:92.35ms
step:1249/1660 train_time:115343ms step_avg:92.35ms
step:1250/1660 train_time:115437ms step_avg:92.35ms
step:1250/1660 val_loss:3.3738 train_time:115532ms step_avg:92.43ms
step:1251/1660 train_time:115553ms step_avg:92.37ms
step:1252/1660 train_time:115628ms step_avg:92.35ms
step:1253/1660 train_time:115728ms step_avg:92.36ms
step:1254/1660 train_time:115822ms step_avg:92.36ms
step:1255/1660 train_time:115913ms step_avg:92.36ms
step:1256/1660 train_time:116006ms step_avg:92.36ms
step:1257/1660 train_time:116097ms step_avg:92.36ms
step:1258/1660 train_time:116190ms step_avg:92.36ms
step:1259/1660 train_time:116282ms step_avg:92.36ms
step:1260/1660 train_time:116374ms step_avg:92.36ms
step:1261/1660 train_time:116468ms step_avg:92.36ms
step:1262/1660 train_time:116565ms step_avg:92.37ms
step:1263/1660 train_time:116660ms step_avg:92.37ms
step:1264/1660 train_time:116754ms step_avg:92.37ms
step:1265/1660 train_time:116847ms step_avg:92.37ms
step:1266/1660 train_time:116940ms step_avg:92.37ms
step:1267/1660 train_time:117032ms step_avg:92.37ms
step:1268/1660 train_time:117124ms step_avg:92.37ms
step:1269/1660 train_time:117217ms step_avg:92.37ms
step:1270/1660 train_time:117309ms step_avg:92.37ms
step:1271/1660 train_time:117402ms step_avg:92.37ms
step:1272/1660 train_time:117494ms step_avg:92.37ms
step:1273/1660 train_time:117590ms step_avg:92.37ms
step:1274/1660 train_time:117686ms step_avg:92.37ms
step:1275/1660 train_time:117779ms step_avg:92.38ms
step:1276/1660 train_time:117874ms step_avg:92.38ms
step:1277/1660 train_time:117967ms step_avg:92.38ms
step:1278/1660 train_time:118059ms step_avg:92.38ms
step:1279/1660 train_time:118151ms step_avg:92.38ms
step:1280/1660 train_time:118244ms step_avg:92.38ms
step:1281/1660 train_time:118336ms step_avg:92.38ms
step:1282/1660 train_time:118429ms step_avg:92.38ms
step:1283/1660 train_time:118523ms step_avg:92.38ms
step:1284/1660 train_time:118618ms step_avg:92.38ms
step:1285/1660 train_time:118711ms step_avg:92.38ms
step:1286/1660 train_time:118806ms step_avg:92.38ms
step:1287/1660 train_time:118898ms step_avg:92.38ms
step:1288/1660 train_time:118992ms step_avg:92.39ms
step:1289/1660 train_time:119085ms step_avg:92.39ms
step:1290/1660 train_time:119177ms step_avg:92.39ms
step:1291/1660 train_time:119270ms step_avg:92.39ms
step:1292/1660 train_time:119363ms step_avg:92.39ms
step:1293/1660 train_time:119456ms step_avg:92.39ms
step:1294/1660 train_time:119549ms step_avg:92.39ms
step:1295/1660 train_time:119643ms step_avg:92.39ms
step:1296/1660 train_time:119736ms step_avg:92.39ms
step:1297/1660 train_time:119830ms step_avg:92.39ms
step:1298/1660 train_time:119924ms step_avg:92.39ms
step:1299/1660 train_time:120017ms step_avg:92.39ms
step:1300/1660 train_time:120109ms step_avg:92.39ms
step:1301/1660 train_time:120203ms step_avg:92.39ms
step:1302/1660 train_time:120295ms step_avg:92.39ms
step:1303/1660 train_time:120387ms step_avg:92.39ms
step:1304/1660 train_time:120480ms step_avg:92.39ms
step:1305/1660 train_time:120575ms step_avg:92.39ms
step:1306/1660 train_time:120670ms step_avg:92.40ms
step:1307/1660 train_time:120762ms step_avg:92.40ms
step:1308/1660 train_time:120857ms step_avg:92.40ms
step:1309/1660 train_time:120950ms step_avg:92.40ms
step:1310/1660 train_time:121043ms step_avg:92.40ms
step:1311/1660 train_time:121136ms step_avg:92.40ms
step:1312/1660 train_time:121229ms step_avg:92.40ms
step:1313/1660 train_time:121322ms step_avg:92.40ms
step:1314/1660 train_time:121415ms step_avg:92.40ms
step:1315/1660 train_time:121508ms step_avg:92.40ms
step:1316/1660 train_time:121601ms step_avg:92.40ms
step:1317/1660 train_time:121694ms step_avg:92.40ms
step:1318/1660 train_time:121787ms step_avg:92.40ms
step:1319/1660 train_time:121880ms step_avg:92.40ms
step:1320/1660 train_time:121975ms step_avg:92.41ms
step:1321/1660 train_time:122068ms step_avg:92.41ms
step:1322/1660 train_time:122161ms step_avg:92.41ms
step:1323/1660 train_time:122254ms step_avg:92.41ms
step:1324/1660 train_time:122348ms step_avg:92.41ms
step:1325/1660 train_time:122441ms step_avg:92.41ms
step:1326/1660 train_time:122534ms step_avg:92.41ms
step:1327/1660 train_time:122628ms step_avg:92.41ms
step:1328/1660 train_time:122722ms step_avg:92.41ms
step:1329/1660 train_time:122815ms step_avg:92.41ms
step:1330/1660 train_time:122908ms step_avg:92.41ms
step:1331/1660 train_time:123002ms step_avg:92.41ms
step:1332/1660 train_time:123096ms step_avg:92.41ms
step:1333/1660 train_time:123189ms step_avg:92.41ms
step:1334/1660 train_time:123281ms step_avg:92.41ms
step:1335/1660 train_time:123375ms step_avg:92.42ms
step:1336/1660 train_time:123468ms step_avg:92.42ms
step:1337/1660 train_time:123562ms step_avg:92.42ms
step:1338/1660 train_time:123654ms step_avg:92.42ms
step:1339/1660 train_time:123748ms step_avg:92.42ms
step:1340/1660 train_time:123841ms step_avg:92.42ms
step:1341/1660 train_time:123935ms step_avg:92.42ms
step:1342/1660 train_time:124028ms step_avg:92.42ms
step:1343/1660 train_time:124121ms step_avg:92.42ms
step:1344/1660 train_time:124214ms step_avg:92.42ms
step:1345/1660 train_time:124307ms step_avg:92.42ms
step:1346/1660 train_time:124400ms step_avg:92.42ms
step:1347/1660 train_time:124494ms step_avg:92.42ms
step:1348/1660 train_time:124586ms step_avg:92.42ms
step:1349/1660 train_time:124679ms step_avg:92.42ms
step:1350/1660 train_time:124772ms step_avg:92.42ms
step:1351/1660 train_time:124867ms step_avg:92.43ms
step:1352/1660 train_time:124959ms step_avg:92.43ms
step:1353/1660 train_time:125054ms step_avg:92.43ms
step:1354/1660 train_time:125147ms step_avg:92.43ms
step:1355/1660 train_time:125239ms step_avg:92.43ms
step:1356/1660 train_time:125333ms step_avg:92.43ms
step:1357/1660 train_time:125426ms step_avg:92.43ms
step:1358/1660 train_time:125519ms step_avg:92.43ms
step:1359/1660 train_time:125612ms step_avg:92.43ms
step:1360/1660 train_time:125705ms step_avg:92.43ms
step:1361/1660 train_time:125798ms step_avg:92.43ms
step:1362/1660 train_time:125892ms step_avg:92.43ms
step:1363/1660 train_time:125986ms step_avg:92.43ms
step:1364/1660 train_time:126078ms step_avg:92.43ms
step:1365/1660 train_time:126173ms step_avg:92.43ms
step:1366/1660 train_time:126266ms step_avg:92.43ms
step:1367/1660 train_time:126359ms step_avg:92.44ms
step:1368/1660 train_time:126453ms step_avg:92.44ms
step:1369/1660 train_time:126546ms step_avg:92.44ms
step:1370/1660 train_time:126639ms step_avg:92.44ms
step:1371/1660 train_time:126732ms step_avg:92.44ms
step:1372/1660 train_time:126826ms step_avg:92.44ms
step:1373/1660 train_time:126920ms step_avg:92.44ms
step:1374/1660 train_time:127013ms step_avg:92.44ms
step:1375/1660 train_time:127106ms step_avg:92.44ms
step:1375/1660 val_loss:3.3400 train_time:127200ms step_avg:92.51ms
step:1376/1660 train_time:127221ms step_avg:92.46ms
step:1377/1660 train_time:127297ms step_avg:92.44ms
step:1378/1660 train_time:127393ms step_avg:92.45ms
step:1379/1660 train_time:127487ms step_avg:92.45ms
step:1380/1660 train_time:127579ms step_avg:92.45ms
step:1381/1660 train_time:127671ms step_avg:92.45ms
step:1382/1660 train_time:127763ms step_avg:92.45ms
step:1383/1660 train_time:127856ms step_avg:92.45ms
step:1384/1660 train_time:127947ms step_avg:92.45ms
step:1385/1660 train_time:128039ms step_avg:92.45ms
step:1386/1660 train_time:128133ms step_avg:92.45ms
step:1387/1660 train_time:128227ms step_avg:92.45ms
step:1388/1660 train_time:128323ms step_avg:92.45ms
step:1389/1660 train_time:128417ms step_avg:92.45ms
step:1390/1660 train_time:128511ms step_avg:92.45ms
step:1391/1660 train_time:128604ms step_avg:92.45ms
step:1392/1660 train_time:128697ms step_avg:92.46ms
step:1393/1660 train_time:128790ms step_avg:92.45ms
step:1394/1660 train_time:128882ms step_avg:92.45ms
step:1395/1660 train_time:128974ms step_avg:92.45ms
step:1396/1660 train_time:129067ms step_avg:92.46ms
step:1397/1660 train_time:129161ms step_avg:92.46ms
step:1398/1660 train_time:129255ms step_avg:92.46ms
step:1399/1660 train_time:129350ms step_avg:92.46ms
step:1400/1660 train_time:129444ms step_avg:92.46ms
step:1401/1660 train_time:129537ms step_avg:92.46ms
step:1402/1660 train_time:129630ms step_avg:92.46ms
step:1403/1660 train_time:129725ms step_avg:92.46ms
step:1404/1660 train_time:129817ms step_avg:92.46ms
step:1405/1660 train_time:129910ms step_avg:92.46ms
step:1406/1660 train_time:130002ms step_avg:92.46ms
step:1407/1660 train_time:130095ms step_avg:92.46ms
step:1408/1660 train_time:130188ms step_avg:92.46ms
step:1409/1660 train_time:130284ms step_avg:92.47ms
step:1410/1660 train_time:130378ms step_avg:92.47ms
step:1411/1660 train_time:130471ms step_avg:92.47ms
step:1412/1660 train_time:130564ms step_avg:92.47ms
step:1413/1660 train_time:130657ms step_avg:92.47ms
step:1414/1660 train_time:130750ms step_avg:92.47ms
step:1415/1660 train_time:130843ms step_avg:92.47ms
step:1416/1660 train_time:130935ms step_avg:92.47ms
step:1417/1660 train_time:131028ms step_avg:92.47ms
step:1418/1660 train_time:131121ms step_avg:92.47ms
step:1419/1660 train_time:131214ms step_avg:92.47ms
step:1420/1660 train_time:131309ms step_avg:92.47ms
step:1421/1660 train_time:131402ms step_avg:92.47ms
step:1422/1660 train_time:131496ms step_avg:92.47ms
step:1423/1660 train_time:131589ms step_avg:92.47ms
step:1424/1660 train_time:131683ms step_avg:92.47ms
step:1425/1660 train_time:131776ms step_avg:92.47ms
step:1426/1660 train_time:131869ms step_avg:92.48ms
step:1427/1660 train_time:131962ms step_avg:92.48ms
step:1428/1660 train_time:132056ms step_avg:92.48ms
step:1429/1660 train_time:132149ms step_avg:92.48ms
step:1430/1660 train_time:132243ms step_avg:92.48ms
step:1431/1660 train_time:132336ms step_avg:92.48ms
step:1432/1660 train_time:132429ms step_avg:92.48ms
step:1433/1660 train_time:132523ms step_avg:92.48ms
step:1434/1660 train_time:132617ms step_avg:92.48ms
step:1435/1660 train_time:132710ms step_avg:92.48ms
step:1436/1660 train_time:132803ms step_avg:92.48ms
step:1437/1660 train_time:132897ms step_avg:92.48ms
step:1438/1660 train_time:132990ms step_avg:92.48ms
step:1439/1660 train_time:133083ms step_avg:92.48ms
step:1440/1660 train_time:133177ms step_avg:92.48ms
step:1441/1660 train_time:133270ms step_avg:92.48ms
step:1442/1660 train_time:133364ms step_avg:92.49ms
step:1443/1660 train_time:133457ms step_avg:92.49ms
step:1444/1660 train_time:133550ms step_avg:92.49ms
step:1445/1660 train_time:133644ms step_avg:92.49ms
step:1446/1660 train_time:133738ms step_avg:92.49ms
step:1447/1660 train_time:133831ms step_avg:92.49ms
step:1448/1660 train_time:133925ms step_avg:92.49ms
step:1449/1660 train_time:134017ms step_avg:92.49ms
step:1450/1660 train_time:134110ms step_avg:92.49ms
step:1451/1660 train_time:134204ms step_avg:92.49ms
step:1452/1660 train_time:134298ms step_avg:92.49ms
step:1453/1660 train_time:134391ms step_avg:92.49ms
step:1454/1660 train_time:134486ms step_avg:92.49ms
step:1455/1660 train_time:134580ms step_avg:92.49ms
step:1456/1660 train_time:134672ms step_avg:92.49ms
step:1457/1660 train_time:134765ms step_avg:92.50ms
step:1458/1660 train_time:134859ms step_avg:92.50ms
step:1459/1660 train_time:134952ms step_avg:92.50ms
step:1460/1660 train_time:135045ms step_avg:92.50ms
step:1461/1660 train_time:135139ms step_avg:92.50ms
step:1462/1660 train_time:135232ms step_avg:92.50ms
step:1463/1660 train_time:135326ms step_avg:92.50ms
step:1464/1660 train_time:135420ms step_avg:92.50ms
step:1465/1660 train_time:135514ms step_avg:92.50ms
step:1466/1660 train_time:135607ms step_avg:92.50ms
step:1467/1660 train_time:135701ms step_avg:92.50ms
step:1468/1660 train_time:135794ms step_avg:92.50ms
step:1469/1660 train_time:135886ms step_avg:92.50ms
step:1470/1660 train_time:135980ms step_avg:92.50ms
step:1471/1660 train_time:136073ms step_avg:92.50ms
step:1472/1660 train_time:136166ms step_avg:92.50ms
step:1473/1660 train_time:136259ms step_avg:92.50ms
step:1474/1660 train_time:136353ms step_avg:92.51ms
step:1475/1660 train_time:136447ms step_avg:92.51ms
step:1476/1660 train_time:136539ms step_avg:92.51ms
step:1477/1660 train_time:136633ms step_avg:92.51ms
step:1478/1660 train_time:136727ms step_avg:92.51ms
step:1479/1660 train_time:136820ms step_avg:92.51ms
step:1480/1660 train_time:136912ms step_avg:92.51ms
step:1481/1660 train_time:137006ms step_avg:92.51ms
step:1482/1660 train_time:137100ms step_avg:92.51ms
step:1483/1660 train_time:137193ms step_avg:92.51ms
step:1484/1660 train_time:137286ms step_avg:92.51ms
step:1485/1660 train_time:137380ms step_avg:92.51ms
step:1486/1660 train_time:137472ms step_avg:92.51ms
step:1487/1660 train_time:137566ms step_avg:92.51ms
step:1488/1660 train_time:137660ms step_avg:92.51ms
step:1489/1660 train_time:137753ms step_avg:92.51ms
step:1490/1660 train_time:137846ms step_avg:92.51ms
step:1491/1660 train_time:137939ms step_avg:92.51ms
step:1492/1660 train_time:138033ms step_avg:92.52ms
step:1493/1660 train_time:138126ms step_avg:92.52ms
step:1494/1660 train_time:138219ms step_avg:92.52ms
step:1495/1660 train_time:138312ms step_avg:92.52ms
step:1496/1660 train_time:138407ms step_avg:92.52ms
step:1497/1660 train_time:138500ms step_avg:92.52ms
step:1498/1660 train_time:138594ms step_avg:92.52ms
step:1499/1660 train_time:138687ms step_avg:92.52ms
step:1500/1660 train_time:138780ms step_avg:92.52ms
step:1500/1660 val_loss:3.3098 train_time:138875ms step_avg:92.58ms
step:1501/1660 train_time:138895ms step_avg:92.54ms
step:1502/1660 train_time:138974ms step_avg:92.53ms
step:1503/1660 train_time:139072ms step_avg:92.53ms
step:1504/1660 train_time:139165ms step_avg:92.53ms
step:1505/1660 train_time:139258ms step_avg:92.53ms
step:1506/1660 train_time:139350ms step_avg:92.53ms
step:1507/1660 train_time:139442ms step_avg:92.53ms
step:1508/1660 train_time:139534ms step_avg:92.53ms
step:1509/1660 train_time:139626ms step_avg:92.53ms
step:1510/1660 train_time:139718ms step_avg:92.53ms
step:1511/1660 train_time:139811ms step_avg:92.53ms
step:1512/1660 train_time:139908ms step_avg:92.53ms
step:1513/1660 train_time:140003ms step_avg:92.53ms
step:1514/1660 train_time:140099ms step_avg:92.54ms
step:1515/1660 train_time:140193ms step_avg:92.54ms
step:1516/1660 train_time:140286ms step_avg:92.54ms
step:1517/1660 train_time:140378ms step_avg:92.54ms
step:1518/1660 train_time:140470ms step_avg:92.54ms
step:1519/1660 train_time:140562ms step_avg:92.54ms
step:1520/1660 train_time:140655ms step_avg:92.54ms
step:1521/1660 train_time:140748ms step_avg:92.54ms
step:1522/1660 train_time:140841ms step_avg:92.54ms
step:1523/1660 train_time:140936ms step_avg:92.54ms
step:1524/1660 train_time:141031ms step_avg:92.54ms
step:1525/1660 train_time:141125ms step_avg:92.54ms
step:1526/1660 train_time:141219ms step_avg:92.54ms
step:1527/1660 train_time:141312ms step_avg:92.54ms
step:1528/1660 train_time:141405ms step_avg:92.54ms
step:1529/1660 train_time:141497ms step_avg:92.54ms
step:1530/1660 train_time:141589ms step_avg:92.54ms
step:1531/1660 train_time:141681ms step_avg:92.54ms
step:1532/1660 train_time:141774ms step_avg:92.54ms
step:1533/1660 train_time:141868ms step_avg:92.54ms
step:1534/1660 train_time:141962ms step_avg:92.54ms
step:1535/1660 train_time:142056ms step_avg:92.54ms
step:1536/1660 train_time:142152ms step_avg:92.55ms
step:1537/1660 train_time:142246ms step_avg:92.55ms
step:1538/1660 train_time:142339ms step_avg:92.55ms
step:1539/1660 train_time:142432ms step_avg:92.55ms
step:1540/1660 train_time:142525ms step_avg:92.55ms
step:1541/1660 train_time:142618ms step_avg:92.55ms
step:1542/1660 train_time:142711ms step_avg:92.55ms
step:1543/1660 train_time:142803ms step_avg:92.55ms
step:1544/1660 train_time:142897ms step_avg:92.55ms
step:1545/1660 train_time:142991ms step_avg:92.55ms
step:1546/1660 train_time:143085ms step_avg:92.55ms
step:1547/1660 train_time:143179ms step_avg:92.55ms
step:1548/1660 train_time:143272ms step_avg:92.55ms
step:1549/1660 train_time:143365ms step_avg:92.55ms
step:1550/1660 train_time:143459ms step_avg:92.55ms
step:1551/1660 train_time:143552ms step_avg:92.55ms
step:1552/1660 train_time:143644ms step_avg:92.55ms
step:1553/1660 train_time:143737ms step_avg:92.55ms
step:1554/1660 train_time:143830ms step_avg:92.55ms
step:1555/1660 train_time:143923ms step_avg:92.56ms
step:1556/1660 train_time:144016ms step_avg:92.56ms
step:1557/1660 train_time:144110ms step_avg:92.56ms
step:1558/1660 train_time:144204ms step_avg:92.56ms
step:1559/1660 train_time:144298ms step_avg:92.56ms
step:1560/1660 train_time:144392ms step_avg:92.56ms
step:1561/1660 train_time:144484ms step_avg:92.56ms
step:1562/1660 train_time:144577ms step_avg:92.56ms
step:1563/1660 train_time:144670ms step_avg:92.56ms
step:1564/1660 train_time:144763ms step_avg:92.56ms
step:1565/1660 train_time:144856ms step_avg:92.56ms
step:1566/1660 train_time:144950ms step_avg:92.56ms
step:1567/1660 train_time:145042ms step_avg:92.56ms
step:1568/1660 train_time:145137ms step_avg:92.56ms
step:1569/1660 train_time:145230ms step_avg:92.56ms
step:1570/1660 train_time:145324ms step_avg:92.56ms
step:1571/1660 train_time:145417ms step_avg:92.56ms
step:1572/1660 train_time:145510ms step_avg:92.56ms
step:1573/1660 train_time:145603ms step_avg:92.56ms
step:1574/1660 train_time:145696ms step_avg:92.56ms
step:1575/1660 train_time:145789ms step_avg:92.56ms
step:1576/1660 train_time:145881ms step_avg:92.56ms
step:1577/1660 train_time:145974ms step_avg:92.56ms
step:1578/1660 train_time:146067ms step_avg:92.56ms
step:1579/1660 train_time:146161ms step_avg:92.57ms
step:1580/1660 train_time:146255ms step_avg:92.57ms
step:1581/1660 train_time:146349ms step_avg:92.57ms
step:1582/1660 train_time:146441ms step_avg:92.57ms
step:1583/1660 train_time:146534ms step_avg:92.57ms
step:1584/1660 train_time:146627ms step_avg:92.57ms
step:1585/1660 train_time:146720ms step_avg:92.57ms
step:1586/1660 train_time:146813ms step_avg:92.57ms
step:1587/1660 train_time:146906ms step_avg:92.57ms
step:1588/1660 train_time:147000ms step_avg:92.57ms
step:1589/1660 train_time:147094ms step_avg:92.57ms
step:1590/1660 train_time:147187ms step_avg:92.57ms
step:1591/1660 train_time:147280ms step_avg:92.57ms
step:1592/1660 train_time:147373ms step_avg:92.57ms
step:1593/1660 train_time:147466ms step_avg:92.57ms
step:1594/1660 train_time:147560ms step_avg:92.57ms
step:1595/1660 train_time:147653ms step_avg:92.57ms
step:1596/1660 train_time:147746ms step_avg:92.57ms
step:1597/1660 train_time:147840ms step_avg:92.57ms
step:1598/1660 train_time:147933ms step_avg:92.57ms
step:1599/1660 train_time:148027ms step_avg:92.57ms
step:1600/1660 train_time:148119ms step_avg:92.57ms
step:1601/1660 train_time:148213ms step_avg:92.58ms
step:1602/1660 train_time:148305ms step_avg:92.58ms
step:1603/1660 train_time:148400ms step_avg:92.58ms
step:1604/1660 train_time:148493ms step_avg:92.58ms
step:1605/1660 train_time:148586ms step_avg:92.58ms
step:1606/1660 train_time:148679ms step_avg:92.58ms
step:1607/1660 train_time:148772ms step_avg:92.58ms
step:1608/1660 train_time:148865ms step_avg:92.58ms
step:1609/1660 train_time:148959ms step_avg:92.58ms
step:1610/1660 train_time:149052ms step_avg:92.58ms
step:1611/1660 train_time:149145ms step_avg:92.58ms
step:1612/1660 train_time:149238ms step_avg:92.58ms
step:1613/1660 train_time:149332ms step_avg:92.58ms
step:1614/1660 train_time:149425ms step_avg:92.58ms
step:1615/1660 train_time:149519ms step_avg:92.58ms
step:1616/1660 train_time:149612ms step_avg:92.58ms
step:1617/1660 train_time:149705ms step_avg:92.58ms
step:1618/1660 train_time:149799ms step_avg:92.58ms
step:1619/1660 train_time:149893ms step_avg:92.58ms
step:1620/1660 train_time:149986ms step_avg:92.58ms
step:1621/1660 train_time:150079ms step_avg:92.58ms
step:1622/1660 train_time:150173ms step_avg:92.58ms
step:1623/1660 train_time:150266ms step_avg:92.59ms
step:1624/1660 train_time:150359ms step_avg:92.59ms
step:1625/1660 train_time:150452ms step_avg:92.59ms
step:1625/1660 val_loss:3.2853 train_time:150547ms step_avg:92.64ms
step:1626/1660 train_time:150567ms step_avg:92.60ms
step:1627/1660 train_time:150644ms step_avg:92.59ms
step:1628/1660 train_time:150743ms step_avg:92.59ms
step:1629/1660 train_time:150835ms step_avg:92.59ms
step:1630/1660 train_time:150928ms step_avg:92.59ms
step:1631/1660 train_time:151020ms step_avg:92.59ms
step:1632/1660 train_time:151112ms step_avg:92.59ms
step:1633/1660 train_time:151204ms step_avg:92.59ms
step:1634/1660 train_time:151296ms step_avg:92.59ms
step:1635/1660 train_time:151388ms step_avg:92.59ms
step:1636/1660 train_time:151481ms step_avg:92.59ms
step:1637/1660 train_time:151577ms step_avg:92.59ms
step:1638/1660 train_time:151675ms step_avg:92.60ms
step:1639/1660 train_time:151770ms step_avg:92.60ms
step:1640/1660 train_time:151863ms step_avg:92.60ms
step:1641/1660 train_time:151956ms step_avg:92.60ms
step:1642/1660 train_time:152049ms step_avg:92.60ms
step:1643/1660 train_time:152141ms step_avg:92.60ms
step:1644/1660 train_time:152233ms step_avg:92.60ms
step:1645/1660 train_time:152326ms step_avg:92.60ms
step:1646/1660 train_time:152418ms step_avg:92.60ms
step:1647/1660 train_time:152512ms step_avg:92.60ms
step:1648/1660 train_time:152607ms step_avg:92.60ms
step:1649/1660 train_time:152702ms step_avg:92.60ms
step:1650/1660 train_time:152795ms step_avg:92.60ms
step:1651/1660 train_time:152889ms step_avg:92.60ms
step:1652/1660 train_time:152983ms step_avg:92.60ms
step:1653/1660 train_time:153076ms step_avg:92.60ms
step:1654/1660 train_time:153169ms step_avg:92.61ms
step:1655/1660 train_time:153261ms step_avg:92.60ms
step:1656/1660 train_time:153353ms step_avg:92.60ms
step:1657/1660 train_time:153447ms step_avg:92.61ms
step:1658/1660 train_time:153542ms step_avg:92.61ms
step:1659/1660 train_time:153637ms step_avg:92.61ms
step:1660/1660 train_time:153733ms step_avg:92.61ms
step:1660/1660 val_loss:3.2774 train_time:153828ms step_avg:92.67ms
peak memory allocated: 32002 MiB reserved: 47016 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload=False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            #t0 = time.perf_counter()
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            #t1 = time.perf_counter()
            #print(f'{t1-t0} slowload')
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    def __init__(self, file_iter, world_size):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
      #loading in a whole shard will be slow...
      #BosFinder tracks its self.i index. I can kickoff with a smaller set. then have the 
      finder = BOSFinder(tokens, world_size=world_size, quickload=True)
      preloader = DataPreloader(file_iter, world_size)
      preloader.start()
    else:
      pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                #tokens = _load_data_shard(next(file_iter))
                #finder = BOSFinder(tokens, world_size=world_size)
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main
@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1660 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"data_threading_1660/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer is highly indifferent to length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Tue Sep 16 03:33:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   39C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   37C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   35C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          189042      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          189043      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          189044      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          189045      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          189046      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          189047      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          189048      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          189049      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          189043      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          189044      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          189045      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          189046      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          189047      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          189048      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          189049      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1660 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1660 train_time:146ms step_avg:145.73ms
step:2/1660 train_time:166ms step_avg:82.77ms
step:3/1660 train_time:234ms step_avg:77.88ms
step:4/1660 train_time:323ms step_avg:80.79ms
step:5/1660 train_time:414ms step_avg:82.78ms
step:6/1660 train_time:505ms step_avg:84.09ms
step:7/1660 train_time:596ms step_avg:85.16ms
step:8/1660 train_time:687ms step_avg:85.91ms
step:9/1660 train_time:779ms step_avg:86.51ms
step:10/1660 train_time:869ms step_avg:86.89ms
step:11/1660 train_time:960ms step_avg:87.24ms
step:12/1660 train_time:1052ms step_avg:87.70ms
step:13/1660 train_time:1148ms step_avg:88.33ms
step:14/1660 train_time:1242ms step_avg:88.71ms
step:15/1660 train_time:1333ms step_avg:88.88ms
step:16/1660 train_time:1425ms step_avg:89.07ms
step:17/1660 train_time:1517ms step_avg:89.21ms
step:18/1660 train_time:1608ms step_avg:89.33ms
step:19/1660 train_time:1699ms step_avg:89.43ms
step:20/1660 train_time:1791ms step_avg:89.53ms
step:21/1660 train_time:1881ms step_avg:89.59ms
step:22/1660 train_time:1972ms step_avg:89.65ms
step:23/1660 train_time:2065ms step_avg:89.78ms
step:24/1660 train_time:2157ms step_avg:89.88ms
step:25/1660 train_time:2250ms step_avg:89.98ms
step:26/1660 train_time:2343ms step_avg:90.10ms
step:27/1660 train_time:2435ms step_avg:90.17ms
step:28/1660 train_time:2526ms step_avg:90.22ms
step:29/1660 train_time:2618ms step_avg:90.28ms
step:30/1660 train_time:2709ms step_avg:90.29ms
step:31/1660 train_time:2800ms step_avg:90.31ms
step:32/1660 train_time:2891ms step_avg:90.35ms
step:33/1660 train_time:2983ms step_avg:90.40ms
step:34/1660 train_time:3076ms step_avg:90.46ms
step:35/1660 train_time:3168ms step_avg:90.52ms
step:36/1660 train_time:3261ms step_avg:90.57ms
step:37/1660 train_time:3352ms step_avg:90.61ms
step:38/1660 train_time:3444ms step_avg:90.64ms
step:39/1660 train_time:3535ms step_avg:90.65ms
step:40/1660 train_time:3627ms step_avg:90.67ms
step:41/1660 train_time:3719ms step_avg:90.71ms
step:42/1660 train_time:3811ms step_avg:90.73ms
step:43/1660 train_time:3902ms step_avg:90.74ms
step:44/1660 train_time:3993ms step_avg:90.75ms
step:45/1660 train_time:4086ms step_avg:90.80ms
step:46/1660 train_time:4178ms step_avg:90.82ms
step:47/1660 train_time:4269ms step_avg:90.83ms
step:48/1660 train_time:4362ms step_avg:90.87ms
step:49/1660 train_time:4454ms step_avg:90.89ms
step:50/1660 train_time:4545ms step_avg:90.91ms
step:51/1660 train_time:4636ms step_avg:90.91ms
step:52/1660 train_time:4728ms step_avg:90.93ms
step:53/1660 train_time:4820ms step_avg:90.95ms
step:54/1660 train_time:4912ms step_avg:90.97ms
step:55/1660 train_time:5005ms step_avg:90.99ms
step:56/1660 train_time:5099ms step_avg:91.04ms
step:57/1660 train_time:5190ms step_avg:91.05ms
step:58/1660 train_time:5282ms step_avg:91.06ms
step:59/1660 train_time:5373ms step_avg:91.06ms
step:60/1660 train_time:5464ms step_avg:91.07ms
step:61/1660 train_time:5556ms step_avg:91.08ms
step:62/1660 train_time:5648ms step_avg:91.09ms
step:63/1660 train_time:5739ms step_avg:91.10ms
step:64/1660 train_time:5831ms step_avg:91.10ms
step:65/1660 train_time:5923ms step_avg:91.12ms
step:66/1660 train_time:6015ms step_avg:91.14ms
step:67/1660 train_time:6108ms step_avg:91.16ms
step:68/1660 train_time:6200ms step_avg:91.18ms
step:69/1660 train_time:6291ms step_avg:91.18ms
step:70/1660 train_time:6384ms step_avg:91.20ms
step:71/1660 train_time:6475ms step_avg:91.20ms
step:72/1660 train_time:6566ms step_avg:91.20ms
step:73/1660 train_time:6658ms step_avg:91.21ms
step:74/1660 train_time:6749ms step_avg:91.20ms
step:75/1660 train_time:6840ms step_avg:91.20ms
step:76/1660 train_time:6931ms step_avg:91.20ms
step:77/1660 train_time:7025ms step_avg:91.23ms
step:78/1660 train_time:7118ms step_avg:91.25ms
step:79/1660 train_time:7209ms step_avg:91.25ms
step:80/1660 train_time:7301ms step_avg:91.26ms
step:81/1660 train_time:7392ms step_avg:91.26ms
step:82/1660 train_time:7484ms step_avg:91.27ms
step:83/1660 train_time:7575ms step_avg:91.27ms
step:84/1660 train_time:7666ms step_avg:91.27ms
step:85/1660 train_time:7758ms step_avg:91.26ms
step:86/1660 train_time:7849ms step_avg:91.27ms
step:87/1660 train_time:7941ms step_avg:91.28ms
step:88/1660 train_time:8032ms step_avg:91.28ms
step:89/1660 train_time:8125ms step_avg:91.29ms
step:90/1660 train_time:8218ms step_avg:91.31ms
step:91/1660 train_time:8310ms step_avg:91.32ms
step:92/1660 train_time:8402ms step_avg:91.32ms
step:93/1660 train_time:8493ms step_avg:91.33ms
step:94/1660 train_time:8585ms step_avg:91.33ms
step:95/1660 train_time:8677ms step_avg:91.33ms
step:96/1660 train_time:8767ms step_avg:91.33ms
step:97/1660 train_time:8859ms step_avg:91.33ms
step:98/1660 train_time:8950ms step_avg:91.33ms
step:99/1660 train_time:9042ms step_avg:91.33ms
step:100/1660 train_time:9133ms step_avg:91.33ms
step:101/1660 train_time:9226ms step_avg:91.34ms
step:102/1660 train_time:9318ms step_avg:91.35ms
step:103/1660 train_time:9409ms step_avg:91.35ms
step:104/1660 train_time:9501ms step_avg:91.36ms
step:105/1660 train_time:9592ms step_avg:91.36ms
step:106/1660 train_time:9684ms step_avg:91.36ms
step:107/1660 train_time:9776ms step_avg:91.36ms
step:108/1660 train_time:9867ms step_avg:91.36ms
step:109/1660 train_time:9957ms step_avg:91.35ms
step:110/1660 train_time:10048ms step_avg:91.35ms
step:111/1660 train_time:10140ms step_avg:91.35ms
step:112/1660 train_time:10231ms step_avg:91.35ms
step:113/1660 train_time:10323ms step_avg:91.36ms
step:114/1660 train_time:10415ms step_avg:91.36ms
step:115/1660 train_time:10507ms step_avg:91.36ms
step:116/1660 train_time:10598ms step_avg:91.36ms
step:117/1660 train_time:10689ms step_avg:91.36ms
step:118/1660 train_time:10781ms step_avg:91.37ms
step:119/1660 train_time:10872ms step_avg:91.36ms
step:120/1660 train_time:10963ms step_avg:91.36ms
step:121/1660 train_time:11055ms step_avg:91.36ms
step:122/1660 train_time:11146ms step_avg:91.36ms
step:123/1660 train_time:11238ms step_avg:91.36ms
step:124/1660 train_time:11329ms step_avg:91.36ms
step:125/1660 train_time:11421ms step_avg:91.37ms
step:125/1660 val_loss:4.3232 train_time:11514ms step_avg:92.11ms
step:126/1660 train_time:11535ms step_avg:91.55ms
step:127/1660 train_time:11610ms step_avg:91.42ms
step:128/1660 train_time:11712ms step_avg:91.50ms
step:129/1660 train_time:11806ms step_avg:91.52ms
step:130/1660 train_time:11897ms step_avg:91.52ms
step:131/1660 train_time:11988ms step_avg:91.51ms
step:132/1660 train_time:12078ms step_avg:91.50ms
step:133/1660 train_time:12168ms step_avg:91.49ms
step:134/1660 train_time:12259ms step_avg:91.48ms
step:135/1660 train_time:12349ms step_avg:91.47ms
step:136/1660 train_time:12440ms step_avg:91.47ms
step:137/1660 train_time:12532ms step_avg:91.47ms
step:138/1660 train_time:12626ms step_avg:91.49ms
step:139/1660 train_time:12720ms step_avg:91.51ms
step:140/1660 train_time:12812ms step_avg:91.52ms
step:141/1660 train_time:12905ms step_avg:91.52ms
step:142/1660 train_time:12996ms step_avg:91.52ms
step:143/1660 train_time:13087ms step_avg:91.51ms
step:144/1660 train_time:13176ms step_avg:91.50ms
step:145/1660 train_time:13267ms step_avg:91.50ms
step:146/1660 train_time:13358ms step_avg:91.49ms
step:147/1660 train_time:13449ms step_avg:91.49ms
step:148/1660 train_time:13540ms step_avg:91.49ms
step:149/1660 train_time:13633ms step_avg:91.49ms
step:150/1660 train_time:13726ms step_avg:91.51ms
step:151/1660 train_time:13819ms step_avg:91.51ms
step:152/1660 train_time:13910ms step_avg:91.51ms
step:153/1660 train_time:14001ms step_avg:91.51ms
step:154/1660 train_time:14092ms step_avg:91.51ms
step:155/1660 train_time:14183ms step_avg:91.50ms
step:156/1660 train_time:14273ms step_avg:91.50ms
step:157/1660 train_time:14365ms step_avg:91.50ms
step:158/1660 train_time:14457ms step_avg:91.50ms
step:159/1660 train_time:14548ms step_avg:91.50ms
step:160/1660 train_time:14640ms step_avg:91.50ms
step:161/1660 train_time:14731ms step_avg:91.50ms
step:162/1660 train_time:14824ms step_avg:91.50ms
step:163/1660 train_time:14915ms step_avg:91.50ms
step:164/1660 train_time:15006ms step_avg:91.50ms
step:165/1660 train_time:15098ms step_avg:91.50ms
step:166/1660 train_time:15188ms step_avg:91.50ms
step:167/1660 train_time:15279ms step_avg:91.49ms
step:168/1660 train_time:15370ms step_avg:91.49ms
step:169/1660 train_time:15462ms step_avg:91.49ms
step:170/1660 train_time:15553ms step_avg:91.49ms
step:171/1660 train_time:15644ms step_avg:91.49ms
step:172/1660 train_time:15736ms step_avg:91.49ms
step:173/1660 train_time:15829ms step_avg:91.50ms
step:174/1660 train_time:15920ms step_avg:91.50ms
step:175/1660 train_time:16011ms step_avg:91.49ms
step:176/1660 train_time:16103ms step_avg:91.49ms
step:177/1660 train_time:16194ms step_avg:91.49ms
step:178/1660 train_time:16286ms step_avg:91.50ms
step:179/1660 train_time:16378ms step_avg:91.49ms
step:180/1660 train_time:16469ms step_avg:91.49ms
step:181/1660 train_time:16561ms step_avg:91.50ms
step:182/1660 train_time:16652ms step_avg:91.50ms
step:183/1660 train_time:16745ms step_avg:91.50ms
step:184/1660 train_time:16837ms step_avg:91.51ms
step:185/1660 train_time:16928ms step_avg:91.51ms
step:186/1660 train_time:17019ms step_avg:91.50ms
step:187/1660 train_time:17111ms step_avg:91.50ms
step:188/1660 train_time:17202ms step_avg:91.50ms
step:189/1660 train_time:17293ms step_avg:91.50ms
step:190/1660 train_time:17386ms step_avg:91.50ms
step:191/1660 train_time:17477ms step_avg:91.50ms
step:192/1660 train_time:17569ms step_avg:91.50ms
step:193/1660 train_time:17660ms step_avg:91.50ms
step:194/1660 train_time:17752ms step_avg:91.50ms
step:195/1660 train_time:17844ms step_avg:91.51ms
step:196/1660 train_time:17935ms step_avg:91.50ms
step:197/1660 train_time:18026ms step_avg:91.50ms
step:198/1660 train_time:18117ms step_avg:91.50ms
step:199/1660 train_time:18209ms step_avg:91.50ms
step:200/1660 train_time:18300ms step_avg:91.50ms
step:201/1660 train_time:18392ms step_avg:91.50ms
step:202/1660 train_time:18483ms step_avg:91.50ms
step:203/1660 train_time:18574ms step_avg:91.50ms
step:204/1660 train_time:18668ms step_avg:91.51ms
step:205/1660 train_time:18760ms step_avg:91.51ms
step:206/1660 train_time:18851ms step_avg:91.51ms
step:207/1660 train_time:18943ms step_avg:91.51ms
step:208/1660 train_time:19034ms step_avg:91.51ms
step:209/1660 train_time:19125ms step_avg:91.51ms
step:210/1660 train_time:19216ms step_avg:91.51ms
step:211/1660 train_time:19308ms step_avg:91.51ms
step:212/1660 train_time:19400ms step_avg:91.51ms
step:213/1660 train_time:19491ms step_avg:91.51ms
step:214/1660 train_time:19583ms step_avg:91.51ms
step:215/1660 train_time:19674ms step_avg:91.51ms
step:216/1660 train_time:19767ms step_avg:91.51ms
step:217/1660 train_time:19858ms step_avg:91.51ms
step:218/1660 train_time:19950ms step_avg:91.51ms
step:219/1660 train_time:20041ms step_avg:91.51ms
step:220/1660 train_time:20132ms step_avg:91.51ms
step:221/1660 train_time:20224ms step_avg:91.51ms
step:222/1660 train_time:20315ms step_avg:91.51ms
step:223/1660 train_time:20405ms step_avg:91.50ms
step:224/1660 train_time:20496ms step_avg:91.50ms
step:225/1660 train_time:20587ms step_avg:91.50ms
step:226/1660 train_time:20679ms step_avg:91.50ms
step:227/1660 train_time:20770ms step_avg:91.50ms
step:228/1660 train_time:20861ms step_avg:91.50ms
step:229/1660 train_time:20952ms step_avg:91.49ms
step:230/1660 train_time:21043ms step_avg:91.49ms
step:231/1660 train_time:21134ms step_avg:91.49ms
step:232/1660 train_time:21225ms step_avg:91.49ms
step:233/1660 train_time:21317ms step_avg:91.49ms
step:234/1660 train_time:21408ms step_avg:91.49ms
step:235/1660 train_time:21499ms step_avg:91.49ms
step:236/1660 train_time:21591ms step_avg:91.49ms
step:237/1660 train_time:21682ms step_avg:91.48ms
step:238/1660 train_time:21773ms step_avg:91.48ms
step:239/1660 train_time:21865ms step_avg:91.48ms
step:240/1660 train_time:21955ms step_avg:91.48ms
step:241/1660 train_time:22048ms step_avg:91.48ms
step:242/1660 train_time:22140ms step_avg:91.49ms
step:243/1660 train_time:22231ms step_avg:91.49ms
step:244/1660 train_time:22322ms step_avg:91.49ms
step:245/1660 train_time:22413ms step_avg:91.48ms
step:246/1660 train_time:22504ms step_avg:91.48ms
step:247/1660 train_time:22596ms step_avg:91.48ms
step:248/1660 train_time:22687ms step_avg:91.48ms
step:249/1660 train_time:22778ms step_avg:91.48ms
step:250/1660 train_time:22870ms step_avg:91.48ms
step:250/1660 val_loss:3.9761 train_time:22963ms step_avg:91.85ms
step:251/1660 train_time:22982ms step_avg:91.56ms
step:252/1660 train_time:23059ms step_avg:91.50ms
step:253/1660 train_time:23155ms step_avg:91.52ms
step:254/1660 train_time:23247ms step_avg:91.52ms
step:255/1660 train_time:23337ms step_avg:91.52ms
step:256/1660 train_time:23428ms step_avg:91.51ms
step:257/1660 train_time:23517ms step_avg:91.51ms
step:258/1660 train_time:23608ms step_avg:91.51ms
step:259/1660 train_time:23698ms step_avg:91.50ms
step:260/1660 train_time:23788ms step_avg:91.49ms
step:261/1660 train_time:23880ms step_avg:91.49ms
step:262/1660 train_time:23973ms step_avg:91.50ms
step:263/1660 train_time:24066ms step_avg:91.51ms
step:264/1660 train_time:24159ms step_avg:91.51ms
step:265/1660 train_time:24251ms step_avg:91.51ms
step:266/1660 train_time:24342ms step_avg:91.51ms
step:267/1660 train_time:24434ms step_avg:91.51ms
step:268/1660 train_time:24524ms step_avg:91.51ms
step:269/1660 train_time:24616ms step_avg:91.51ms
step:270/1660 train_time:24708ms step_avg:91.51ms
step:271/1660 train_time:24798ms step_avg:91.51ms
step:272/1660 train_time:24890ms step_avg:91.51ms
step:273/1660 train_time:24981ms step_avg:91.51ms
step:274/1660 train_time:25074ms step_avg:91.51ms
step:275/1660 train_time:25166ms step_avg:91.51ms
step:276/1660 train_time:25258ms step_avg:91.51ms
step:277/1660 train_time:25350ms step_avg:91.52ms
step:278/1660 train_time:25440ms step_avg:91.51ms
step:279/1660 train_time:25531ms step_avg:91.51ms
step:280/1660 train_time:25621ms step_avg:91.51ms
step:281/1660 train_time:25713ms step_avg:91.51ms
step:282/1660 train_time:25805ms step_avg:91.51ms
step:283/1660 train_time:25896ms step_avg:91.51ms
step:284/1660 train_time:25987ms step_avg:91.50ms
step:285/1660 train_time:26079ms step_avg:91.50ms
step:286/1660 train_time:26171ms step_avg:91.51ms
step:287/1660 train_time:26262ms step_avg:91.50ms
step:288/1660 train_time:26353ms step_avg:91.50ms
step:289/1660 train_time:26444ms step_avg:91.50ms
step:290/1660 train_time:26535ms step_avg:91.50ms
step:291/1660 train_time:26626ms step_avg:91.50ms
step:292/1660 train_time:26717ms step_avg:91.50ms
step:293/1660 train_time:26808ms step_avg:91.49ms
step:294/1660 train_time:26899ms step_avg:91.49ms
step:295/1660 train_time:26991ms step_avg:91.49ms
step:296/1660 train_time:27082ms step_avg:91.49ms
step:297/1660 train_time:27174ms step_avg:91.50ms
step:298/1660 train_time:27266ms step_avg:91.50ms
step:299/1660 train_time:27358ms step_avg:91.50ms
step:300/1660 train_time:27449ms step_avg:91.50ms
step:301/1660 train_time:27540ms step_avg:91.49ms
step:302/1660 train_time:27631ms step_avg:91.49ms
step:303/1660 train_time:27721ms step_avg:91.49ms
step:304/1660 train_time:27813ms step_avg:91.49ms
step:305/1660 train_time:27905ms step_avg:91.49ms
step:306/1660 train_time:27997ms step_avg:91.49ms
step:307/1660 train_time:28088ms step_avg:91.49ms
step:308/1660 train_time:28180ms step_avg:91.49ms
step:309/1660 train_time:28271ms step_avg:91.49ms
step:310/1660 train_time:28362ms step_avg:91.49ms
step:311/1660 train_time:28453ms step_avg:91.49ms
step:312/1660 train_time:28545ms step_avg:91.49ms
step:313/1660 train_time:28636ms step_avg:91.49ms
step:314/1660 train_time:28727ms step_avg:91.49ms
step:315/1660 train_time:28818ms step_avg:91.48ms
step:316/1660 train_time:28909ms step_avg:91.49ms
step:317/1660 train_time:29001ms step_avg:91.49ms
step:318/1660 train_time:29094ms step_avg:91.49ms
step:319/1660 train_time:29185ms step_avg:91.49ms
step:320/1660 train_time:29276ms step_avg:91.49ms
step:321/1660 train_time:29368ms step_avg:91.49ms
step:322/1660 train_time:29459ms step_avg:91.49ms
step:323/1660 train_time:29550ms step_avg:91.49ms
step:324/1660 train_time:29642ms step_avg:91.49ms
step:325/1660 train_time:29733ms step_avg:91.49ms
step:326/1660 train_time:29824ms step_avg:91.49ms
step:327/1660 train_time:29916ms step_avg:91.49ms
step:328/1660 train_time:30008ms step_avg:91.49ms
step:329/1660 train_time:30099ms step_avg:91.49ms
step:330/1660 train_time:30190ms step_avg:91.49ms
step:331/1660 train_time:30282ms step_avg:91.49ms
step:332/1660 train_time:30373ms step_avg:91.49ms
step:333/1660 train_time:30464ms step_avg:91.48ms
step:334/1660 train_time:30555ms step_avg:91.48ms
step:335/1660 train_time:30646ms step_avg:91.48ms
step:336/1660 train_time:30738ms step_avg:91.48ms
step:337/1660 train_time:30828ms step_avg:91.48ms
step:338/1660 train_time:30919ms step_avg:91.48ms
step:339/1660 train_time:31011ms step_avg:91.48ms
step:340/1660 train_time:31103ms step_avg:91.48ms
step:341/1660 train_time:31195ms step_avg:91.48ms
step:342/1660 train_time:31287ms step_avg:91.48ms
step:343/1660 train_time:31378ms step_avg:91.48ms
step:344/1660 train_time:31470ms step_avg:91.48ms
step:345/1660 train_time:31560ms step_avg:91.48ms
step:346/1660 train_time:31651ms step_avg:91.48ms
step:347/1660 train_time:31742ms step_avg:91.48ms
step:348/1660 train_time:31834ms step_avg:91.48ms
step:349/1660 train_time:31925ms step_avg:91.47ms
step:350/1660 train_time:32016ms step_avg:91.47ms
step:351/1660 train_time:32108ms step_avg:91.48ms
step:352/1660 train_time:32199ms step_avg:91.48ms
step:353/1660 train_time:32292ms step_avg:91.48ms
step:354/1660 train_time:32384ms step_avg:91.48ms
step:355/1660 train_time:32476ms step_avg:91.48ms
step:356/1660 train_time:32567ms step_avg:91.48ms
step:357/1660 train_time:32658ms step_avg:91.48ms
step:358/1660 train_time:32749ms step_avg:91.48ms
step:359/1660 train_time:32840ms step_avg:91.48ms
step:360/1660 train_time:32931ms step_avg:91.47ms
step:361/1660 train_time:33022ms step_avg:91.47ms
step:362/1660 train_time:33115ms step_avg:91.48ms
step:363/1660 train_time:33207ms step_avg:91.48ms
step:364/1660 train_time:33298ms step_avg:91.48ms
step:365/1660 train_time:33390ms step_avg:91.48ms
step:366/1660 train_time:33482ms step_avg:91.48ms
step:367/1660 train_time:33574ms step_avg:91.48ms
step:368/1660 train_time:33665ms step_avg:91.48ms
step:369/1660 train_time:33756ms step_avg:91.48ms
step:370/1660 train_time:33846ms step_avg:91.48ms
step:371/1660 train_time:33937ms step_avg:91.48ms
step:372/1660 train_time:34028ms step_avg:91.47ms
step:373/1660 train_time:34120ms step_avg:91.47ms
step:374/1660 train_time:34212ms step_avg:91.48ms
step:375/1660 train_time:34304ms step_avg:91.48ms
step:375/1660 val_loss:3.8184 train_time:34398ms step_avg:91.73ms
step:376/1660 train_time:34417ms step_avg:91.53ms
step:377/1660 train_time:34493ms step_avg:91.49ms
step:378/1660 train_time:34590ms step_avg:91.51ms
step:379/1660 train_time:34682ms step_avg:91.51ms
step:380/1660 train_time:34773ms step_avg:91.51ms
step:381/1660 train_time:34865ms step_avg:91.51ms
step:382/1660 train_time:34956ms step_avg:91.51ms
step:383/1660 train_time:35046ms step_avg:91.50ms
step:384/1660 train_time:35136ms step_avg:91.50ms
step:385/1660 train_time:35227ms step_avg:91.50ms
step:386/1660 train_time:35318ms step_avg:91.50ms
step:387/1660 train_time:35409ms step_avg:91.50ms
step:388/1660 train_time:35502ms step_avg:91.50ms
step:389/1660 train_time:35595ms step_avg:91.50ms
step:390/1660 train_time:35688ms step_avg:91.51ms
step:391/1660 train_time:35779ms step_avg:91.51ms
step:392/1660 train_time:35870ms step_avg:91.50ms
step:393/1660 train_time:35961ms step_avg:91.50ms
step:394/1660 train_time:36052ms step_avg:91.50ms
step:395/1660 train_time:36143ms step_avg:91.50ms
step:396/1660 train_time:36233ms step_avg:91.50ms
step:397/1660 train_time:36324ms step_avg:91.50ms
step:398/1660 train_time:36416ms step_avg:91.50ms
step:399/1660 train_time:36509ms step_avg:91.50ms
step:400/1660 train_time:36600ms step_avg:91.50ms
step:401/1660 train_time:36692ms step_avg:91.50ms
step:402/1660 train_time:36784ms step_avg:91.50ms
step:403/1660 train_time:36875ms step_avg:91.50ms
step:404/1660 train_time:36966ms step_avg:91.50ms
step:405/1660 train_time:37057ms step_avg:91.50ms
step:406/1660 train_time:37149ms step_avg:91.50ms
step:407/1660 train_time:37240ms step_avg:91.50ms
step:408/1660 train_time:37331ms step_avg:91.50ms
step:409/1660 train_time:37422ms step_avg:91.50ms
step:410/1660 train_time:37514ms step_avg:91.50ms
step:411/1660 train_time:37606ms step_avg:91.50ms
step:412/1660 train_time:37698ms step_avg:91.50ms
step:413/1660 train_time:37789ms step_avg:91.50ms
step:414/1660 train_time:37881ms step_avg:91.50ms
step:415/1660 train_time:37972ms step_avg:91.50ms
step:416/1660 train_time:38063ms step_avg:91.50ms
step:417/1660 train_time:38154ms step_avg:91.50ms
step:418/1660 train_time:38247ms step_avg:91.50ms
step:419/1660 train_time:38339ms step_avg:91.50ms
step:420/1660 train_time:38430ms step_avg:91.50ms
step:421/1660 train_time:38522ms step_avg:91.50ms
step:422/1660 train_time:38613ms step_avg:91.50ms
step:423/1660 train_time:38705ms step_avg:91.50ms
step:424/1660 train_time:38796ms step_avg:91.50ms
step:425/1660 train_time:38887ms step_avg:91.50ms
step:426/1660 train_time:38979ms step_avg:91.50ms
step:427/1660 train_time:39071ms step_avg:91.50ms
step:428/1660 train_time:39162ms step_avg:91.50ms
step:429/1660 train_time:39252ms step_avg:91.50ms
step:430/1660 train_time:39344ms step_avg:91.50ms
step:431/1660 train_time:39436ms step_avg:91.50ms
step:432/1660 train_time:39528ms step_avg:91.50ms
step:433/1660 train_time:39619ms step_avg:91.50ms
step:434/1660 train_time:39710ms step_avg:91.50ms
step:435/1660 train_time:39802ms step_avg:91.50ms
step:436/1660 train_time:39893ms step_avg:91.50ms
step:437/1660 train_time:39986ms step_avg:91.50ms
step:438/1660 train_time:40077ms step_avg:91.50ms
step:439/1660 train_time:40168ms step_avg:91.50ms
step:440/1660 train_time:40259ms step_avg:91.50ms
step:441/1660 train_time:40350ms step_avg:91.50ms
step:442/1660 train_time:40442ms step_avg:91.50ms
step:443/1660 train_time:40534ms step_avg:91.50ms
step:444/1660 train_time:40625ms step_avg:91.50ms
step:445/1660 train_time:40717ms step_avg:91.50ms
step:446/1660 train_time:40808ms step_avg:91.50ms
step:447/1660 train_time:40899ms step_avg:91.50ms
step:448/1660 train_time:40991ms step_avg:91.50ms
step:449/1660 train_time:41082ms step_avg:91.50ms
step:450/1660 train_time:41173ms step_avg:91.50ms
step:451/1660 train_time:41266ms step_avg:91.50ms
step:452/1660 train_time:41358ms step_avg:91.50ms
step:453/1660 train_time:41449ms step_avg:91.50ms
step:454/1660 train_time:41540ms step_avg:91.50ms
step:455/1660 train_time:41632ms step_avg:91.50ms
step:456/1660 train_time:41724ms step_avg:91.50ms
step:457/1660 train_time:41815ms step_avg:91.50ms
step:458/1660 train_time:41907ms step_avg:91.50ms
step:459/1660 train_time:41999ms step_avg:91.50ms
step:460/1660 train_time:42090ms step_avg:91.50ms
step:461/1660 train_time:42182ms step_avg:91.50ms
step:462/1660 train_time:42272ms step_avg:91.50ms
step:463/1660 train_time:42364ms step_avg:91.50ms
step:464/1660 train_time:42457ms step_avg:91.50ms
step:465/1660 train_time:42548ms step_avg:91.50ms
step:466/1660 train_time:42640ms step_avg:91.50ms
step:467/1660 train_time:42731ms step_avg:91.50ms
step:468/1660 train_time:42823ms step_avg:91.50ms
step:469/1660 train_time:42914ms step_avg:91.50ms
step:470/1660 train_time:43006ms step_avg:91.50ms
step:471/1660 train_time:43097ms step_avg:91.50ms
step:472/1660 train_time:43188ms step_avg:91.50ms
step:473/1660 train_time:43279ms step_avg:91.50ms
step:474/1660 train_time:43370ms step_avg:91.50ms
step:475/1660 train_time:43463ms step_avg:91.50ms
step:476/1660 train_time:43555ms step_avg:91.50ms
step:477/1660 train_time:43647ms step_avg:91.50ms
step:478/1660 train_time:43738ms step_avg:91.50ms
step:479/1660 train_time:43829ms step_avg:91.50ms
step:480/1660 train_time:43920ms step_avg:91.50ms
step:481/1660 train_time:44012ms step_avg:91.50ms
step:482/1660 train_time:44103ms step_avg:91.50ms
step:483/1660 train_time:44195ms step_avg:91.50ms
step:484/1660 train_time:44286ms step_avg:91.50ms
step:485/1660 train_time:44378ms step_avg:91.50ms
step:486/1660 train_time:44469ms step_avg:91.50ms
step:487/1660 train_time:44561ms step_avg:91.50ms
step:488/1660 train_time:44653ms step_avg:91.50ms
step:489/1660 train_time:44743ms step_avg:91.50ms
step:490/1660 train_time:44835ms step_avg:91.50ms
step:491/1660 train_time:44927ms step_avg:91.50ms
step:492/1660 train_time:45018ms step_avg:91.50ms
step:493/1660 train_time:45110ms step_avg:91.50ms
step:494/1660 train_time:45200ms step_avg:91.50ms
step:495/1660 train_time:45292ms step_avg:91.50ms
step:496/1660 train_time:45383ms step_avg:91.50ms
step:497/1660 train_time:45474ms step_avg:91.50ms
step:498/1660 train_time:45567ms step_avg:91.50ms
step:499/1660 train_time:45658ms step_avg:91.50ms
step:500/1660 train_time:45749ms step_avg:91.50ms
step:500/1660 val_loss:3.7164 train_time:45842ms step_avg:91.68ms
step:501/1660 train_time:45862ms step_avg:91.54ms
step:502/1660 train_time:45937ms step_avg:91.51ms
step:503/1660 train_time:46033ms step_avg:91.52ms
step:504/1660 train_time:46125ms step_avg:91.52ms
step:505/1660 train_time:46216ms step_avg:91.52ms
step:506/1660 train_time:46308ms step_avg:91.52ms
step:507/1660 train_time:46398ms step_avg:91.52ms
step:508/1660 train_time:46488ms step_avg:91.51ms
step:509/1660 train_time:46579ms step_avg:91.51ms
step:510/1660 train_time:46669ms step_avg:91.51ms
step:511/1660 train_time:46760ms step_avg:91.51ms
step:512/1660 train_time:46853ms step_avg:91.51ms
step:513/1660 train_time:46945ms step_avg:91.51ms
step:514/1660 train_time:47039ms step_avg:91.52ms
step:515/1660 train_time:47131ms step_avg:91.52ms
step:516/1660 train_time:47223ms step_avg:91.52ms
step:517/1660 train_time:47314ms step_avg:91.52ms
step:518/1660 train_time:47405ms step_avg:91.51ms
step:519/1660 train_time:47495ms step_avg:91.51ms
step:520/1660 train_time:47585ms step_avg:91.51ms
step:521/1660 train_time:47675ms step_avg:91.51ms
step:522/1660 train_time:47766ms step_avg:91.51ms
step:523/1660 train_time:47858ms step_avg:91.51ms
step:524/1660 train_time:47950ms step_avg:91.51ms
step:525/1660 train_time:48043ms step_avg:91.51ms
step:526/1660 train_time:48134ms step_avg:91.51ms
step:527/1660 train_time:48226ms step_avg:91.51ms
step:528/1660 train_time:48318ms step_avg:91.51ms
step:529/1660 train_time:48408ms step_avg:91.51ms
step:530/1660 train_time:48499ms step_avg:91.51ms
step:531/1660 train_time:48589ms step_avg:91.51ms
step:532/1660 train_time:48680ms step_avg:91.50ms
step:533/1660 train_time:48770ms step_avg:91.50ms
step:534/1660 train_time:48862ms step_avg:91.50ms
step:535/1660 train_time:48953ms step_avg:91.50ms
step:536/1660 train_time:49044ms step_avg:91.50ms
step:537/1660 train_time:49136ms step_avg:91.50ms
step:538/1660 train_time:49227ms step_avg:91.50ms
step:539/1660 train_time:49319ms step_avg:91.50ms
step:540/1660 train_time:49410ms step_avg:91.50ms
step:541/1660 train_time:49501ms step_avg:91.50ms
step:542/1660 train_time:49593ms step_avg:91.50ms
step:543/1660 train_time:49683ms step_avg:91.50ms
step:544/1660 train_time:49773ms step_avg:91.50ms
step:545/1660 train_time:49864ms step_avg:91.49ms
step:546/1660 train_time:49956ms step_avg:91.50ms
step:547/1660 train_time:50047ms step_avg:91.49ms
step:548/1660 train_time:50139ms step_avg:91.49ms
step:549/1660 train_time:50231ms step_avg:91.49ms
step:550/1660 train_time:50322ms step_avg:91.49ms
step:551/1660 train_time:50413ms step_avg:91.49ms
step:552/1660 train_time:50504ms step_avg:91.49ms
step:553/1660 train_time:50596ms step_avg:91.49ms
step:554/1660 train_time:50687ms step_avg:91.49ms
step:555/1660 train_time:50778ms step_avg:91.49ms
step:556/1660 train_time:50871ms step_avg:91.49ms
step:557/1660 train_time:50964ms step_avg:91.50ms
step:558/1660 train_time:51056ms step_avg:91.50ms
step:559/1660 train_time:51149ms step_avg:91.50ms
step:560/1660 train_time:51241ms step_avg:91.50ms
step:561/1660 train_time:51334ms step_avg:91.50ms
step:562/1660 train_time:51426ms step_avg:91.51ms
step:563/1660 train_time:51521ms step_avg:91.51ms
step:564/1660 train_time:51614ms step_avg:91.51ms
step:565/1660 train_time:51706ms step_avg:91.51ms
step:566/1660 train_time:51799ms step_avg:91.52ms
step:567/1660 train_time:51892ms step_avg:91.52ms
step:568/1660 train_time:51984ms step_avg:91.52ms
step:569/1660 train_time:52076ms step_avg:91.52ms
step:570/1660 train_time:52168ms step_avg:91.52ms
step:571/1660 train_time:52261ms step_avg:91.52ms
step:572/1660 train_time:52353ms step_avg:91.53ms
step:573/1660 train_time:52446ms step_avg:91.53ms
step:574/1660 train_time:52539ms step_avg:91.53ms
step:575/1660 train_time:52632ms step_avg:91.53ms
step:576/1660 train_time:52725ms step_avg:91.54ms
step:577/1660 train_time:52819ms step_avg:91.54ms
step:578/1660 train_time:52911ms step_avg:91.54ms
step:579/1660 train_time:53003ms step_avg:91.54ms
step:580/1660 train_time:53095ms step_avg:91.54ms
step:581/1660 train_time:53189ms step_avg:91.55ms
step:582/1660 train_time:53281ms step_avg:91.55ms
step:583/1660 train_time:53373ms step_avg:91.55ms
step:584/1660 train_time:53465ms step_avg:91.55ms
step:585/1660 train_time:53559ms step_avg:91.55ms
step:586/1660 train_time:53652ms step_avg:91.56ms
step:587/1660 train_time:53745ms step_avg:91.56ms
step:588/1660 train_time:53838ms step_avg:91.56ms
step:589/1660 train_time:53931ms step_avg:91.56ms
step:590/1660 train_time:54023ms step_avg:91.56ms
step:591/1660 train_time:54115ms step_avg:91.57ms
step:592/1660 train_time:54207ms step_avg:91.57ms
step:593/1660 train_time:54299ms step_avg:91.57ms
step:594/1660 train_time:54392ms step_avg:91.57ms
step:595/1660 train_time:54485ms step_avg:91.57ms
step:596/1660 train_time:54578ms step_avg:91.57ms
step:597/1660 train_time:54670ms step_avg:91.57ms
step:598/1660 train_time:54763ms step_avg:91.58ms
step:599/1660 train_time:54855ms step_avg:91.58ms
step:600/1660 train_time:54947ms step_avg:91.58ms
step:601/1660 train_time:55040ms step_avg:91.58ms
step:602/1660 train_time:55133ms step_avg:91.58ms
step:603/1660 train_time:55226ms step_avg:91.58ms
step:604/1660 train_time:55318ms step_avg:91.59ms
step:605/1660 train_time:55411ms step_avg:91.59ms
step:606/1660 train_time:55503ms step_avg:91.59ms
step:607/1660 train_time:55595ms step_avg:91.59ms
step:608/1660 train_time:55687ms step_avg:91.59ms
step:609/1660 train_time:55781ms step_avg:91.59ms
step:610/1660 train_time:55873ms step_avg:91.59ms
step:611/1660 train_time:55965ms step_avg:91.60ms
step:612/1660 train_time:56057ms step_avg:91.60ms
step:613/1660 train_time:56150ms step_avg:91.60ms
step:614/1660 train_time:56242ms step_avg:91.60ms
step:615/1660 train_time:56334ms step_avg:91.60ms
step:616/1660 train_time:56426ms step_avg:91.60ms
step:617/1660 train_time:56519ms step_avg:91.60ms
step:618/1660 train_time:56612ms step_avg:91.60ms
step:619/1660 train_time:56704ms step_avg:91.61ms
step:620/1660 train_time:56797ms step_avg:91.61ms
step:621/1660 train_time:56889ms step_avg:91.61ms
step:622/1660 train_time:56982ms step_avg:91.61ms
step:623/1660 train_time:57075ms step_avg:91.61ms
step:624/1660 train_time:57167ms step_avg:91.61ms
step:625/1660 train_time:57259ms step_avg:91.61ms
step:625/1660 val_loss:3.6137 train_time:57354ms step_avg:91.77ms
step:626/1660 train_time:57373ms step_avg:91.65ms
step:627/1660 train_time:57454ms step_avg:91.63ms
step:628/1660 train_time:57553ms step_avg:91.64ms
step:629/1660 train_time:57647ms step_avg:91.65ms
step:630/1660 train_time:57739ms step_avg:91.65ms
step:631/1660 train_time:57829ms step_avg:91.65ms
step:632/1660 train_time:57920ms step_avg:91.65ms
step:633/1660 train_time:58012ms step_avg:91.65ms
step:634/1660 train_time:58103ms step_avg:91.65ms
step:635/1660 train_time:58194ms step_avg:91.64ms
step:636/1660 train_time:58287ms step_avg:91.65ms
step:637/1660 train_time:58384ms step_avg:91.65ms
step:638/1660 train_time:58483ms step_avg:91.67ms
step:639/1660 train_time:58579ms step_avg:91.67ms
step:640/1660 train_time:58673ms step_avg:91.68ms
step:641/1660 train_time:58765ms step_avg:91.68ms
step:642/1660 train_time:58857ms step_avg:91.68ms
step:643/1660 train_time:58948ms step_avg:91.68ms
step:644/1660 train_time:59040ms step_avg:91.68ms
step:645/1660 train_time:59132ms step_avg:91.68ms
step:646/1660 train_time:59223ms step_avg:91.68ms
step:647/1660 train_time:59316ms step_avg:91.68ms
step:648/1660 train_time:59409ms step_avg:91.68ms
step:649/1660 train_time:59504ms step_avg:91.69ms
step:650/1660 train_time:59599ms step_avg:91.69ms
step:651/1660 train_time:59692ms step_avg:91.69ms
step:652/1660 train_time:59785ms step_avg:91.69ms
step:653/1660 train_time:59877ms step_avg:91.70ms
step:654/1660 train_time:59970ms step_avg:91.70ms
step:655/1660 train_time:60061ms step_avg:91.70ms
step:656/1660 train_time:60152ms step_avg:91.70ms
step:657/1660 train_time:60244ms step_avg:91.70ms
step:658/1660 train_time:60337ms step_avg:91.70ms
step:659/1660 train_time:60429ms step_avg:91.70ms
step:660/1660 train_time:60523ms step_avg:91.70ms
step:661/1660 train_time:60617ms step_avg:91.70ms
step:662/1660 train_time:60709ms step_avg:91.71ms
step:663/1660 train_time:60802ms step_avg:91.71ms
step:664/1660 train_time:60895ms step_avg:91.71ms
step:665/1660 train_time:60987ms step_avg:91.71ms
step:666/1660 train_time:61079ms step_avg:91.71ms
step:667/1660 train_time:61170ms step_avg:91.71ms
step:668/1660 train_time:61262ms step_avg:91.71ms
step:669/1660 train_time:61355ms step_avg:91.71ms
step:670/1660 train_time:61447ms step_avg:91.71ms
step:671/1660 train_time:61541ms step_avg:91.71ms
step:672/1660 train_time:61633ms step_avg:91.72ms
step:673/1660 train_time:61725ms step_avg:91.72ms
step:674/1660 train_time:61819ms step_avg:91.72ms
step:675/1660 train_time:61911ms step_avg:91.72ms
step:676/1660 train_time:62003ms step_avg:91.72ms
step:677/1660 train_time:62096ms step_avg:91.72ms
step:678/1660 train_time:62187ms step_avg:91.72ms
step:679/1660 train_time:62280ms step_avg:91.72ms
step:680/1660 train_time:62373ms step_avg:91.72ms
step:681/1660 train_time:62466ms step_avg:91.73ms
step:682/1660 train_time:62560ms step_avg:91.73ms
step:683/1660 train_time:62653ms step_avg:91.73ms
step:684/1660 train_time:62745ms step_avg:91.73ms
step:685/1660 train_time:62840ms step_avg:91.74ms
step:686/1660 train_time:62932ms step_avg:91.74ms
step:687/1660 train_time:63023ms step_avg:91.74ms
step:688/1660 train_time:63116ms step_avg:91.74ms
step:689/1660 train_time:63208ms step_avg:91.74ms
step:690/1660 train_time:63301ms step_avg:91.74ms
step:691/1660 train_time:63394ms step_avg:91.74ms
step:692/1660 train_time:63487ms step_avg:91.74ms
step:693/1660 train_time:63581ms step_avg:91.75ms
step:694/1660 train_time:63674ms step_avg:91.75ms
step:695/1660 train_time:63767ms step_avg:91.75ms
step:696/1660 train_time:63861ms step_avg:91.75ms
step:697/1660 train_time:63953ms step_avg:91.75ms
step:698/1660 train_time:64045ms step_avg:91.75ms
step:699/1660 train_time:64137ms step_avg:91.76ms
step:700/1660 train_time:64229ms step_avg:91.76ms
step:701/1660 train_time:64322ms step_avg:91.76ms
step:702/1660 train_time:64414ms step_avg:91.76ms
step:703/1660 train_time:64508ms step_avg:91.76ms
step:704/1660 train_time:64602ms step_avg:91.76ms
step:705/1660 train_time:64695ms step_avg:91.77ms
step:706/1660 train_time:64787ms step_avg:91.77ms
step:707/1660 train_time:64881ms step_avg:91.77ms
step:708/1660 train_time:64974ms step_avg:91.77ms
step:709/1660 train_time:65066ms step_avg:91.77ms
step:710/1660 train_time:65159ms step_avg:91.77ms
step:711/1660 train_time:65251ms step_avg:91.77ms
step:712/1660 train_time:65343ms step_avg:91.77ms
step:713/1660 train_time:65435ms step_avg:91.77ms
step:714/1660 train_time:65528ms step_avg:91.78ms
step:715/1660 train_time:65621ms step_avg:91.78ms
step:716/1660 train_time:65714ms step_avg:91.78ms
step:717/1660 train_time:65807ms step_avg:91.78ms
step:718/1660 train_time:65900ms step_avg:91.78ms
step:719/1660 train_time:65993ms step_avg:91.78ms
step:720/1660 train_time:66085ms step_avg:91.78ms
step:721/1660 train_time:66177ms step_avg:91.79ms
step:722/1660 train_time:66270ms step_avg:91.79ms
step:723/1660 train_time:66363ms step_avg:91.79ms
step:724/1660 train_time:66455ms step_avg:91.79ms
step:725/1660 train_time:66548ms step_avg:91.79ms
step:726/1660 train_time:66640ms step_avg:91.79ms
step:727/1660 train_time:66734ms step_avg:91.79ms
step:728/1660 train_time:66827ms step_avg:91.79ms
step:729/1660 train_time:66920ms step_avg:91.80ms
step:730/1660 train_time:67013ms step_avg:91.80ms
step:731/1660 train_time:67105ms step_avg:91.80ms
step:732/1660 train_time:67198ms step_avg:91.80ms
step:733/1660 train_time:67290ms step_avg:91.80ms
step:734/1660 train_time:67382ms step_avg:91.80ms
step:735/1660 train_time:67475ms step_avg:91.80ms
step:736/1660 train_time:67567ms step_avg:91.80ms
step:737/1660 train_time:67660ms step_avg:91.80ms
step:738/1660 train_time:67753ms step_avg:91.81ms
step:739/1660 train_time:67845ms step_avg:91.81ms
step:740/1660 train_time:67938ms step_avg:91.81ms
step:741/1660 train_time:68030ms step_avg:91.81ms
step:742/1660 train_time:68123ms step_avg:91.81ms
step:743/1660 train_time:68215ms step_avg:91.81ms
step:744/1660 train_time:68307ms step_avg:91.81ms
step:745/1660 train_time:68401ms step_avg:91.81ms
step:746/1660 train_time:68494ms step_avg:91.81ms
step:747/1660 train_time:68586ms step_avg:91.81ms
step:748/1660 train_time:68679ms step_avg:91.82ms
step:749/1660 train_time:68772ms step_avg:91.82ms
step:750/1660 train_time:68865ms step_avg:91.82ms
step:750/1660 val_loss:3.5616 train_time:68960ms step_avg:91.95ms
step:751/1660 train_time:68979ms step_avg:91.85ms
step:752/1660 train_time:69055ms step_avg:91.83ms
step:753/1660 train_time:69152ms step_avg:91.84ms
step:754/1660 train_time:69245ms step_avg:91.84ms
step:755/1660 train_time:69337ms step_avg:91.84ms
step:756/1660 train_time:69427ms step_avg:91.84ms
step:757/1660 train_time:69519ms step_avg:91.83ms
step:758/1660 train_time:69611ms step_avg:91.83ms
step:759/1660 train_time:69702ms step_avg:91.83ms
step:760/1660 train_time:69793ms step_avg:91.83ms
step:761/1660 train_time:69886ms step_avg:91.83ms
step:762/1660 train_time:69981ms step_avg:91.84ms
step:763/1660 train_time:70077ms step_avg:91.84ms
step:764/1660 train_time:70171ms step_avg:91.85ms
step:765/1660 train_time:70263ms step_avg:91.85ms
step:766/1660 train_time:70357ms step_avg:91.85ms
step:767/1660 train_time:70448ms step_avg:91.85ms
step:768/1660 train_time:70540ms step_avg:91.85ms
step:769/1660 train_time:70633ms step_avg:91.85ms
step:770/1660 train_time:70724ms step_avg:91.85ms
step:771/1660 train_time:70816ms step_avg:91.85ms
step:772/1660 train_time:70909ms step_avg:91.85ms
step:773/1660 train_time:71003ms step_avg:91.85ms
step:774/1660 train_time:71097ms step_avg:91.86ms
step:775/1660 train_time:71190ms step_avg:91.86ms
step:776/1660 train_time:71283ms step_avg:91.86ms
step:777/1660 train_time:71375ms step_avg:91.86ms
step:778/1660 train_time:71467ms step_avg:91.86ms
step:779/1660 train_time:71559ms step_avg:91.86ms
step:780/1660 train_time:71650ms step_avg:91.86ms
step:781/1660 train_time:71742ms step_avg:91.86ms
step:782/1660 train_time:71835ms step_avg:91.86ms
step:783/1660 train_time:71927ms step_avg:91.86ms
step:784/1660 train_time:72022ms step_avg:91.86ms
step:785/1660 train_time:72114ms step_avg:91.87ms
step:786/1660 train_time:72207ms step_avg:91.87ms
step:787/1660 train_time:72300ms step_avg:91.87ms
step:788/1660 train_time:72393ms step_avg:91.87ms
step:789/1660 train_time:72485ms step_avg:91.87ms
step:790/1660 train_time:72578ms step_avg:91.87ms
step:791/1660 train_time:72670ms step_avg:91.87ms
step:792/1660 train_time:72763ms step_avg:91.87ms
step:793/1660 train_time:72854ms step_avg:91.87ms
step:794/1660 train_time:72946ms step_avg:91.87ms
step:795/1660 train_time:73040ms step_avg:91.87ms
step:796/1660 train_time:73133ms step_avg:91.88ms
step:797/1660 train_time:73225ms step_avg:91.88ms
step:798/1660 train_time:73319ms step_avg:91.88ms
step:799/1660 train_time:73412ms step_avg:91.88ms
step:800/1660 train_time:73504ms step_avg:91.88ms
step:801/1660 train_time:73596ms step_avg:91.88ms
step:802/1660 train_time:73688ms step_avg:91.88ms
step:803/1660 train_time:73780ms step_avg:91.88ms
step:804/1660 train_time:73873ms step_avg:91.88ms
step:805/1660 train_time:73966ms step_avg:91.88ms
step:806/1660 train_time:74059ms step_avg:91.88ms
step:807/1660 train_time:74152ms step_avg:91.89ms
step:808/1660 train_time:74245ms step_avg:91.89ms
step:809/1660 train_time:74338ms step_avg:91.89ms
step:810/1660 train_time:74431ms step_avg:91.89ms
step:811/1660 train_time:74523ms step_avg:91.89ms
step:812/1660 train_time:74615ms step_avg:91.89ms
step:813/1660 train_time:74707ms step_avg:91.89ms
step:814/1660 train_time:74800ms step_avg:91.89ms
step:815/1660 train_time:74892ms step_avg:91.89ms
step:816/1660 train_time:74984ms step_avg:91.89ms
step:817/1660 train_time:75077ms step_avg:91.89ms
step:818/1660 train_time:75170ms step_avg:91.89ms
step:819/1660 train_time:75263ms step_avg:91.90ms
step:820/1660 train_time:75355ms step_avg:91.90ms
step:821/1660 train_time:75447ms step_avg:91.90ms
step:822/1660 train_time:75539ms step_avg:91.90ms
step:823/1660 train_time:75633ms step_avg:91.90ms
step:824/1660 train_time:75725ms step_avg:91.90ms
step:825/1660 train_time:75818ms step_avg:91.90ms
step:826/1660 train_time:75910ms step_avg:91.90ms
step:827/1660 train_time:76002ms step_avg:91.90ms
step:828/1660 train_time:76095ms step_avg:91.90ms
step:829/1660 train_time:76187ms step_avg:91.90ms
step:830/1660 train_time:76280ms step_avg:91.90ms
step:831/1660 train_time:76372ms step_avg:91.90ms
step:832/1660 train_time:76464ms step_avg:91.90ms
step:833/1660 train_time:76556ms step_avg:91.90ms
step:834/1660 train_time:76649ms step_avg:91.90ms
step:835/1660 train_time:76741ms step_avg:91.91ms
step:836/1660 train_time:76834ms step_avg:91.91ms
step:837/1660 train_time:76926ms step_avg:91.91ms
step:838/1660 train_time:77019ms step_avg:91.91ms
step:839/1660 train_time:77111ms step_avg:91.91ms
step:840/1660 train_time:77204ms step_avg:91.91ms
step:841/1660 train_time:77297ms step_avg:91.91ms
step:842/1660 train_time:77389ms step_avg:91.91ms
step:843/1660 train_time:77482ms step_avg:91.91ms
step:844/1660 train_time:77574ms step_avg:91.91ms
step:845/1660 train_time:77667ms step_avg:91.91ms
step:846/1660 train_time:77759ms step_avg:91.91ms
step:847/1660 train_time:77852ms step_avg:91.92ms
step:848/1660 train_time:77943ms step_avg:91.91ms
step:849/1660 train_time:78037ms step_avg:91.92ms
step:850/1660 train_time:78130ms step_avg:91.92ms
step:851/1660 train_time:78223ms step_avg:91.92ms
step:852/1660 train_time:78315ms step_avg:91.92ms
step:853/1660 train_time:78408ms step_avg:91.92ms
step:854/1660 train_time:78501ms step_avg:91.92ms
step:855/1660 train_time:78593ms step_avg:91.92ms
step:856/1660 train_time:78685ms step_avg:91.92ms
step:857/1660 train_time:78777ms step_avg:91.92ms
step:858/1660 train_time:78869ms step_avg:91.92ms
step:859/1660 train_time:78962ms step_avg:91.92ms
step:860/1660 train_time:79054ms step_avg:91.92ms
step:861/1660 train_time:79146ms step_avg:91.92ms
step:862/1660 train_time:79240ms step_avg:91.93ms
step:863/1660 train_time:79333ms step_avg:91.93ms
step:864/1660 train_time:79425ms step_avg:91.93ms
step:865/1660 train_time:79519ms step_avg:91.93ms
step:866/1660 train_time:79612ms step_avg:91.93ms
step:867/1660 train_time:79704ms step_avg:91.93ms
step:868/1660 train_time:79796ms step_avg:91.93ms
step:869/1660 train_time:79889ms step_avg:91.93ms
step:870/1660 train_time:79982ms step_avg:91.93ms
step:871/1660 train_time:80075ms step_avg:91.93ms
step:872/1660 train_time:80168ms step_avg:91.94ms
step:873/1660 train_time:80260ms step_avg:91.94ms
step:874/1660 train_time:80353ms step_avg:91.94ms
step:875/1660 train_time:80445ms step_avg:91.94ms
step:875/1660 val_loss:3.5172 train_time:80540ms step_avg:92.05ms
step:876/1660 train_time:80559ms step_avg:91.96ms
step:877/1660 train_time:80640ms step_avg:91.95ms
step:878/1660 train_time:80739ms step_avg:91.96ms
step:879/1660 train_time:80832ms step_avg:91.96ms
step:880/1660 train_time:80923ms step_avg:91.96ms
step:881/1660 train_time:81014ms step_avg:91.96ms
step:882/1660 train_time:81106ms step_avg:91.96ms
step:883/1660 train_time:81197ms step_avg:91.96ms
step:884/1660 train_time:81288ms step_avg:91.96ms
step:885/1660 train_time:81380ms step_avg:91.96ms
step:886/1660 train_time:81473ms step_avg:91.96ms
step:887/1660 train_time:81568ms step_avg:91.96ms
step:888/1660 train_time:81664ms step_avg:91.96ms
step:889/1660 train_time:81759ms step_avg:91.97ms
step:890/1660 train_time:81853ms step_avg:91.97ms
step:891/1660 train_time:81946ms step_avg:91.97ms
step:892/1660 train_time:82039ms step_avg:91.97ms
step:893/1660 train_time:82131ms step_avg:91.97ms
step:894/1660 train_time:82222ms step_avg:91.97ms
step:895/1660 train_time:82314ms step_avg:91.97ms
step:896/1660 train_time:82405ms step_avg:91.97ms
step:897/1660 train_time:82499ms step_avg:91.97ms
step:898/1660 train_time:82593ms step_avg:91.97ms
step:899/1660 train_time:82686ms step_avg:91.98ms
step:900/1660 train_time:82781ms step_avg:91.98ms
step:901/1660 train_time:82876ms step_avg:91.98ms
step:902/1660 train_time:82968ms step_avg:91.98ms
step:903/1660 train_time:83060ms step_avg:91.98ms
step:904/1660 train_time:83152ms step_avg:91.98ms
step:905/1660 train_time:83243ms step_avg:91.98ms
step:906/1660 train_time:83336ms step_avg:91.98ms
step:907/1660 train_time:83428ms step_avg:91.98ms
step:908/1660 train_time:83521ms step_avg:91.98ms
step:909/1660 train_time:83614ms step_avg:91.98ms
step:910/1660 train_time:83707ms step_avg:91.99ms
step:911/1660 train_time:83801ms step_avg:91.99ms
step:912/1660 train_time:83895ms step_avg:91.99ms
step:913/1660 train_time:83987ms step_avg:91.99ms
step:914/1660 train_time:84080ms step_avg:91.99ms
step:915/1660 train_time:84172ms step_avg:91.99ms
step:916/1660 train_time:84264ms step_avg:91.99ms
step:917/1660 train_time:84356ms step_avg:91.99ms
step:918/1660 train_time:84450ms step_avg:91.99ms
step:919/1660 train_time:84542ms step_avg:91.99ms
step:920/1660 train_time:84635ms step_avg:92.00ms
step:921/1660 train_time:84728ms step_avg:92.00ms
step:922/1660 train_time:84822ms step_avg:92.00ms
step:923/1660 train_time:84916ms step_avg:92.00ms
step:924/1660 train_time:85008ms step_avg:92.00ms
step:925/1660 train_time:85102ms step_avg:92.00ms
step:926/1660 train_time:85194ms step_avg:92.00ms
step:927/1660 train_time:85286ms step_avg:92.00ms
step:928/1660 train_time:85379ms step_avg:92.00ms
step:929/1660 train_time:85472ms step_avg:92.00ms
step:930/1660 train_time:85564ms step_avg:92.00ms
step:931/1660 train_time:85657ms step_avg:92.00ms
step:932/1660 train_time:85749ms step_avg:92.01ms
step:933/1660 train_time:85843ms step_avg:92.01ms
step:934/1660 train_time:85937ms step_avg:92.01ms
step:935/1660 train_time:86030ms step_avg:92.01ms
step:936/1660 train_time:86122ms step_avg:92.01ms
step:937/1660 train_time:86215ms step_avg:92.01ms
step:938/1660 train_time:86307ms step_avg:92.01ms
step:939/1660 train_time:86399ms step_avg:92.01ms
step:940/1660 train_time:86492ms step_avg:92.01ms
step:941/1660 train_time:86584ms step_avg:92.01ms
step:942/1660 train_time:86676ms step_avg:92.01ms
step:943/1660 train_time:86769ms step_avg:92.01ms
step:944/1660 train_time:86862ms step_avg:92.01ms
step:945/1660 train_time:86955ms step_avg:92.02ms
step:946/1660 train_time:87048ms step_avg:92.02ms
step:947/1660 train_time:87140ms step_avg:92.02ms
step:948/1660 train_time:87233ms step_avg:92.02ms
step:949/1660 train_time:87325ms step_avg:92.02ms
step:950/1660 train_time:87417ms step_avg:92.02ms
step:951/1660 train_time:87510ms step_avg:92.02ms
step:952/1660 train_time:87603ms step_avg:92.02ms
step:953/1660 train_time:87696ms step_avg:92.02ms
step:954/1660 train_time:87788ms step_avg:92.02ms
step:955/1660 train_time:87882ms step_avg:92.02ms
step:956/1660 train_time:87976ms step_avg:92.02ms
step:957/1660 train_time:88068ms step_avg:92.03ms
step:958/1660 train_time:88161ms step_avg:92.03ms
step:959/1660 train_time:88254ms step_avg:92.03ms
step:960/1660 train_time:88347ms step_avg:92.03ms
step:961/1660 train_time:88440ms step_avg:92.03ms
step:962/1660 train_time:88533ms step_avg:92.03ms
step:963/1660 train_time:88625ms step_avg:92.03ms
step:964/1660 train_time:88717ms step_avg:92.03ms
step:965/1660 train_time:88810ms step_avg:92.03ms
step:966/1660 train_time:88903ms step_avg:92.03ms
step:967/1660 train_time:88998ms step_avg:92.03ms
step:968/1660 train_time:89090ms step_avg:92.04ms
step:969/1660 train_time:89183ms step_avg:92.04ms
step:970/1660 train_time:89277ms step_avg:92.04ms
step:971/1660 train_time:89370ms step_avg:92.04ms
step:972/1660 train_time:89462ms step_avg:92.04ms
step:973/1660 train_time:89555ms step_avg:92.04ms
step:974/1660 train_time:89647ms step_avg:92.04ms
step:975/1660 train_time:89740ms step_avg:92.04ms
step:976/1660 train_time:89833ms step_avg:92.04ms
step:977/1660 train_time:89926ms step_avg:92.04ms
step:978/1660 train_time:90019ms step_avg:92.04ms
step:979/1660 train_time:90111ms step_avg:92.04ms
step:980/1660 train_time:90204ms step_avg:92.05ms
step:981/1660 train_time:90298ms step_avg:92.05ms
step:982/1660 train_time:90390ms step_avg:92.05ms
step:983/1660 train_time:90482ms step_avg:92.05ms
step:984/1660 train_time:90576ms step_avg:92.05ms
step:985/1660 train_time:90669ms step_avg:92.05ms
step:986/1660 train_time:90762ms step_avg:92.05ms
step:987/1660 train_time:90855ms step_avg:92.05ms
step:988/1660 train_time:90948ms step_avg:92.05ms
step:989/1660 train_time:91041ms step_avg:92.05ms
step:990/1660 train_time:91133ms step_avg:92.05ms
step:991/1660 train_time:91225ms step_avg:92.05ms
step:992/1660 train_time:91319ms step_avg:92.06ms
step:993/1660 train_time:91411ms step_avg:92.06ms
step:994/1660 train_time:91504ms step_avg:92.06ms
step:995/1660 train_time:91597ms step_avg:92.06ms
step:996/1660 train_time:91689ms step_avg:92.06ms
step:997/1660 train_time:91782ms step_avg:92.06ms
step:998/1660 train_time:91876ms step_avg:92.06ms
step:999/1660 train_time:91969ms step_avg:92.06ms
step:1000/1660 train_time:92062ms step_avg:92.06ms
step:1000/1660 val_loss:3.4660 train_time:92155ms step_avg:92.16ms
step:1001/1660 train_time:92175ms step_avg:92.08ms
step:1002/1660 train_time:92251ms step_avg:92.07ms
step:1003/1660 train_time:92349ms step_avg:92.07ms
step:1004/1660 train_time:92442ms step_avg:92.07ms
step:1005/1660 train_time:92535ms step_avg:92.07ms
step:1006/1660 train_time:92627ms step_avg:92.07ms
step:1007/1660 train_time:92718ms step_avg:92.07ms
step:1008/1660 train_time:92810ms step_avg:92.07ms
step:1009/1660 train_time:92901ms step_avg:92.07ms
step:1010/1660 train_time:92992ms step_avg:92.07ms
step:1011/1660 train_time:93084ms step_avg:92.07ms
step:1012/1660 train_time:93179ms step_avg:92.07ms
step:1013/1660 train_time:93275ms step_avg:92.08ms
step:1014/1660 train_time:93371ms step_avg:92.08ms
step:1015/1660 train_time:93464ms step_avg:92.08ms
step:1016/1660 train_time:93557ms step_avg:92.08ms
step:1017/1660 train_time:93649ms step_avg:92.08ms
step:1018/1660 train_time:93741ms step_avg:92.08ms
step:1019/1660 train_time:93832ms step_avg:92.08ms
step:1020/1660 train_time:93923ms step_avg:92.08ms
step:1021/1660 train_time:94016ms step_avg:92.08ms
step:1022/1660 train_time:94108ms step_avg:92.08ms
step:1023/1660 train_time:94201ms step_avg:92.08ms
step:1024/1660 train_time:94297ms step_avg:92.09ms
step:1025/1660 train_time:94392ms step_avg:92.09ms
step:1026/1660 train_time:94485ms step_avg:92.09ms
step:1027/1660 train_time:94578ms step_avg:92.09ms
step:1028/1660 train_time:94670ms step_avg:92.09ms
step:1029/1660 train_time:94762ms step_avg:92.09ms
step:1030/1660 train_time:94853ms step_avg:92.09ms
step:1031/1660 train_time:94946ms step_avg:92.09ms
step:1032/1660 train_time:95038ms step_avg:92.09ms
step:1033/1660 train_time:95130ms step_avg:92.09ms
step:1034/1660 train_time:95223ms step_avg:92.09ms
step:1035/1660 train_time:95318ms step_avg:92.09ms
step:1036/1660 train_time:95411ms step_avg:92.10ms
step:1037/1660 train_time:95504ms step_avg:92.10ms
step:1038/1660 train_time:95598ms step_avg:92.10ms
step:1039/1660 train_time:95690ms step_avg:92.10ms
step:1040/1660 train_time:95782ms step_avg:92.10ms
step:1041/1660 train_time:95875ms step_avg:92.10ms
step:1042/1660 train_time:95967ms step_avg:92.10ms
step:1043/1660 train_time:96060ms step_avg:92.10ms
step:1044/1660 train_time:96152ms step_avg:92.10ms
step:1045/1660 train_time:96246ms step_avg:92.10ms
step:1046/1660 train_time:96339ms step_avg:92.10ms
step:1047/1660 train_time:96432ms step_avg:92.10ms
step:1048/1660 train_time:96525ms step_avg:92.10ms
step:1049/1660 train_time:96618ms step_avg:92.10ms
step:1050/1660 train_time:96711ms step_avg:92.11ms
step:1051/1660 train_time:96803ms step_avg:92.11ms
step:1052/1660 train_time:96895ms step_avg:92.11ms
step:1053/1660 train_time:96987ms step_avg:92.11ms
step:1054/1660 train_time:97079ms step_avg:92.11ms
step:1055/1660 train_time:97172ms step_avg:92.11ms
step:1056/1660 train_time:97265ms step_avg:92.11ms
step:1057/1660 train_time:97358ms step_avg:92.11ms
step:1058/1660 train_time:97451ms step_avg:92.11ms
step:1059/1660 train_time:97543ms step_avg:92.11ms
step:1060/1660 train_time:97637ms step_avg:92.11ms
step:1061/1660 train_time:97729ms step_avg:92.11ms
step:1062/1660 train_time:97822ms step_avg:92.11ms
step:1063/1660 train_time:97914ms step_avg:92.11ms
step:1064/1660 train_time:98006ms step_avg:92.11ms
step:1065/1660 train_time:98099ms step_avg:92.11ms
step:1066/1660 train_time:98192ms step_avg:92.11ms
step:1067/1660 train_time:98286ms step_avg:92.11ms
step:1068/1660 train_time:98378ms step_avg:92.11ms
step:1069/1660 train_time:98471ms step_avg:92.12ms
step:1070/1660 train_time:98564ms step_avg:92.12ms
step:1071/1660 train_time:98658ms step_avg:92.12ms
step:1072/1660 train_time:98751ms step_avg:92.12ms
step:1073/1660 train_time:98843ms step_avg:92.12ms
step:1074/1660 train_time:98936ms step_avg:92.12ms
step:1075/1660 train_time:99027ms step_avg:92.12ms
step:1076/1660 train_time:99120ms step_avg:92.12ms
step:1077/1660 train_time:99214ms step_avg:92.12ms
step:1078/1660 train_time:99307ms step_avg:92.12ms
step:1079/1660 train_time:99399ms step_avg:92.12ms
step:1080/1660 train_time:99491ms step_avg:92.12ms
step:1081/1660 train_time:99585ms step_avg:92.12ms
step:1082/1660 train_time:99679ms step_avg:92.12ms
step:1083/1660 train_time:99771ms step_avg:92.13ms
step:1084/1660 train_time:99863ms step_avg:92.12ms
step:1085/1660 train_time:99956ms step_avg:92.13ms
step:1086/1660 train_time:100048ms step_avg:92.13ms
step:1087/1660 train_time:100140ms step_avg:92.12ms
step:1088/1660 train_time:100233ms step_avg:92.13ms
step:1089/1660 train_time:100325ms step_avg:92.13ms
step:1090/1660 train_time:100420ms step_avg:92.13ms
step:1091/1660 train_time:100513ms step_avg:92.13ms
step:1092/1660 train_time:100606ms step_avg:92.13ms
step:1093/1660 train_time:100699ms step_avg:92.13ms
step:1094/1660 train_time:100792ms step_avg:92.13ms
step:1095/1660 train_time:100884ms step_avg:92.13ms
step:1096/1660 train_time:100977ms step_avg:92.13ms
step:1097/1660 train_time:101070ms step_avg:92.13ms
step:1098/1660 train_time:101162ms step_avg:92.13ms
step:1099/1660 train_time:101256ms step_avg:92.13ms
step:1100/1660 train_time:101349ms step_avg:92.14ms
step:1101/1660 train_time:101441ms step_avg:92.14ms
step:1102/1660 train_time:101535ms step_avg:92.14ms
step:1103/1660 train_time:101627ms step_avg:92.14ms
step:1104/1660 train_time:101720ms step_avg:92.14ms
step:1105/1660 train_time:101814ms step_avg:92.14ms
step:1106/1660 train_time:101907ms step_avg:92.14ms
step:1107/1660 train_time:101999ms step_avg:92.14ms
step:1108/1660 train_time:102092ms step_avg:92.14ms
step:1109/1660 train_time:102185ms step_avg:92.14ms
step:1110/1660 train_time:102279ms step_avg:92.14ms
step:1111/1660 train_time:102372ms step_avg:92.14ms
step:1112/1660 train_time:102465ms step_avg:92.14ms
step:1113/1660 train_time:102558ms step_avg:92.15ms
step:1114/1660 train_time:102652ms step_avg:92.15ms
step:1115/1660 train_time:102745ms step_avg:92.15ms
step:1116/1660 train_time:102840ms step_avg:92.15ms
step:1117/1660 train_time:102934ms step_avg:92.15ms
step:1118/1660 train_time:103027ms step_avg:92.15ms
step:1119/1660 train_time:103120ms step_avg:92.15ms
step:1120/1660 train_time:103213ms step_avg:92.15ms
step:1121/1660 train_time:103306ms step_avg:92.16ms
step:1122/1660 train_time:103399ms step_avg:92.16ms
step:1123/1660 train_time:103492ms step_avg:92.16ms
step:1124/1660 train_time:103585ms step_avg:92.16ms
step:1125/1660 train_time:103679ms step_avg:92.16ms
step:1125/1660 val_loss:3.4132 train_time:103774ms step_avg:92.24ms
step:1126/1660 train_time:103793ms step_avg:92.18ms
step:1127/1660 train_time:103871ms step_avg:92.17ms
step:1128/1660 train_time:103973ms step_avg:92.17ms
step:1129/1660 train_time:104067ms step_avg:92.18ms
step:1130/1660 train_time:104160ms step_avg:92.18ms
step:1131/1660 train_time:104252ms step_avg:92.18ms
step:1132/1660 train_time:104344ms step_avg:92.18ms
step:1133/1660 train_time:104436ms step_avg:92.18ms
step:1134/1660 train_time:104528ms step_avg:92.18ms
step:1135/1660 train_time:104620ms step_avg:92.18ms
step:1136/1660 train_time:104713ms step_avg:92.18ms
step:1137/1660 train_time:104808ms step_avg:92.18ms
step:1138/1660 train_time:104904ms step_avg:92.18ms
step:1139/1660 train_time:104999ms step_avg:92.19ms
step:1140/1660 train_time:105093ms step_avg:92.19ms
step:1141/1660 train_time:105186ms step_avg:92.19ms
step:1142/1660 train_time:105278ms step_avg:92.19ms
step:1143/1660 train_time:105371ms step_avg:92.19ms
step:1144/1660 train_time:105464ms step_avg:92.19ms
step:1145/1660 train_time:105556ms step_avg:92.19ms
step:1146/1660 train_time:105649ms step_avg:92.19ms
step:1147/1660 train_time:105741ms step_avg:92.19ms
step:1148/1660 train_time:105836ms step_avg:92.19ms
step:1149/1660 train_time:105933ms step_avg:92.20ms
step:1150/1660 train_time:106028ms step_avg:92.20ms
step:1151/1660 train_time:106122ms step_avg:92.20ms
step:1152/1660 train_time:106214ms step_avg:92.20ms
step:1153/1660 train_time:106307ms step_avg:92.20ms
step:1154/1660 train_time:106399ms step_avg:92.20ms
step:1155/1660 train_time:106492ms step_avg:92.20ms
step:1156/1660 train_time:106585ms step_avg:92.20ms
step:1157/1660 train_time:106677ms step_avg:92.20ms
step:1158/1660 train_time:106771ms step_avg:92.20ms
step:1159/1660 train_time:106866ms step_avg:92.21ms
step:1160/1660 train_time:106960ms step_avg:92.21ms
step:1161/1660 train_time:107056ms step_avg:92.21ms
step:1162/1660 train_time:107150ms step_avg:92.21ms
step:1163/1660 train_time:107242ms step_avg:92.21ms
step:1164/1660 train_time:107335ms step_avg:92.21ms
step:1165/1660 train_time:107429ms step_avg:92.21ms
step:1166/1660 train_time:107521ms step_avg:92.21ms
step:1167/1660 train_time:107614ms step_avg:92.21ms
step:1168/1660 train_time:107707ms step_avg:92.21ms
step:1169/1660 train_time:107800ms step_avg:92.22ms
step:1170/1660 train_time:107895ms step_avg:92.22ms
step:1171/1660 train_time:107990ms step_avg:92.22ms
step:1172/1660 train_time:108084ms step_avg:92.22ms
step:1173/1660 train_time:108177ms step_avg:92.22ms
step:1174/1660 train_time:108270ms step_avg:92.22ms
step:1175/1660 train_time:108363ms step_avg:92.22ms
step:1176/1660 train_time:108457ms step_avg:92.23ms
step:1177/1660 train_time:108550ms step_avg:92.23ms
step:1178/1660 train_time:108643ms step_avg:92.23ms
step:1179/1660 train_time:108736ms step_avg:92.23ms
step:1180/1660 train_time:108830ms step_avg:92.23ms
step:1181/1660 train_time:108924ms step_avg:92.23ms
step:1182/1660 train_time:109018ms step_avg:92.23ms
step:1183/1660 train_time:109113ms step_avg:92.23ms
step:1184/1660 train_time:109206ms step_avg:92.23ms
step:1185/1660 train_time:109300ms step_avg:92.24ms
step:1186/1660 train_time:109393ms step_avg:92.24ms
step:1187/1660 train_time:109487ms step_avg:92.24ms
step:1188/1660 train_time:109580ms step_avg:92.24ms
step:1189/1660 train_time:109673ms step_avg:92.24ms
step:1190/1660 train_time:109767ms step_avg:92.24ms
step:1191/1660 train_time:109860ms step_avg:92.24ms
step:1192/1660 train_time:109954ms step_avg:92.24ms
step:1193/1660 train_time:110048ms step_avg:92.24ms
step:1194/1660 train_time:110141ms step_avg:92.25ms
step:1195/1660 train_time:110235ms step_avg:92.25ms
step:1196/1660 train_time:110328ms step_avg:92.25ms
step:1197/1660 train_time:110421ms step_avg:92.25ms
step:1198/1660 train_time:110514ms step_avg:92.25ms
step:1199/1660 train_time:110607ms step_avg:92.25ms
step:1200/1660 train_time:110699ms step_avg:92.25ms
step:1201/1660 train_time:110794ms step_avg:92.25ms
step:1202/1660 train_time:110888ms step_avg:92.25ms
step:1203/1660 train_time:110981ms step_avg:92.25ms
step:1204/1660 train_time:111075ms step_avg:92.25ms
step:1205/1660 train_time:111168ms step_avg:92.26ms
step:1206/1660 train_time:111262ms step_avg:92.26ms
step:1207/1660 train_time:111356ms step_avg:92.26ms
step:1208/1660 train_time:111448ms step_avg:92.26ms
step:1209/1660 train_time:111542ms step_avg:92.26ms
step:1210/1660 train_time:111635ms step_avg:92.26ms
step:1211/1660 train_time:111728ms step_avg:92.26ms
step:1212/1660 train_time:111821ms step_avg:92.26ms
step:1213/1660 train_time:111914ms step_avg:92.26ms
step:1214/1660 train_time:112008ms step_avg:92.26ms
step:1215/1660 train_time:112101ms step_avg:92.26ms
step:1216/1660 train_time:112197ms step_avg:92.27ms
step:1217/1660 train_time:112293ms step_avg:92.27ms
step:1218/1660 train_time:112388ms step_avg:92.27ms
step:1219/1660 train_time:112480ms step_avg:92.27ms
step:1220/1660 train_time:112574ms step_avg:92.27ms
step:1221/1660 train_time:112667ms step_avg:92.27ms
step:1222/1660 train_time:112760ms step_avg:92.27ms
step:1223/1660 train_time:112853ms step_avg:92.28ms
step:1224/1660 train_time:112947ms step_avg:92.28ms
step:1225/1660 train_time:113040ms step_avg:92.28ms
step:1226/1660 train_time:113135ms step_avg:92.28ms
step:1227/1660 train_time:113230ms step_avg:92.28ms
step:1228/1660 train_time:113324ms step_avg:92.28ms
step:1229/1660 train_time:113417ms step_avg:92.28ms
step:1230/1660 train_time:113510ms step_avg:92.28ms
step:1231/1660 train_time:113603ms step_avg:92.29ms
step:1232/1660 train_time:113696ms step_avg:92.29ms
step:1233/1660 train_time:113790ms step_avg:92.29ms
step:1234/1660 train_time:113882ms step_avg:92.29ms
step:1235/1660 train_time:113975ms step_avg:92.29ms
step:1236/1660 train_time:114069ms step_avg:92.29ms
step:1237/1660 train_time:114163ms step_avg:92.29ms
step:1238/1660 train_time:114257ms step_avg:92.29ms
step:1239/1660 train_time:114353ms step_avg:92.29ms
step:1240/1660 train_time:114447ms step_avg:92.30ms
step:1241/1660 train_time:114539ms step_avg:92.30ms
step:1242/1660 train_time:114633ms step_avg:92.30ms
step:1243/1660 train_time:114727ms step_avg:92.30ms
step:1244/1660 train_time:114821ms step_avg:92.30ms
step:1245/1660 train_time:114915ms step_avg:92.30ms
step:1246/1660 train_time:115008ms step_avg:92.30ms
step:1247/1660 train_time:115101ms step_avg:92.30ms
step:1248/1660 train_time:115195ms step_avg:92.30ms
step:1249/1660 train_time:115288ms step_avg:92.30ms
step:1250/1660 train_time:115382ms step_avg:92.31ms
step:1250/1660 val_loss:3.3741 train_time:115476ms step_avg:92.38ms
step:1251/1660 train_time:115495ms step_avg:92.32ms
step:1252/1660 train_time:115574ms step_avg:92.31ms
step:1253/1660 train_time:115674ms step_avg:92.32ms
step:1254/1660 train_time:115767ms step_avg:92.32ms
step:1255/1660 train_time:115859ms step_avg:92.32ms
step:1256/1660 train_time:115952ms step_avg:92.32ms
step:1257/1660 train_time:116044ms step_avg:92.32ms
step:1258/1660 train_time:116136ms step_avg:92.32ms
step:1259/1660 train_time:116228ms step_avg:92.32ms
step:1260/1660 train_time:116320ms step_avg:92.32ms
step:1261/1660 train_time:116414ms step_avg:92.32ms
step:1262/1660 train_time:116509ms step_avg:92.32ms
step:1263/1660 train_time:116607ms step_avg:92.33ms
step:1264/1660 train_time:116703ms step_avg:92.33ms
step:1265/1660 train_time:116797ms step_avg:92.33ms
step:1266/1660 train_time:116889ms step_avg:92.33ms
step:1267/1660 train_time:116982ms step_avg:92.33ms
step:1268/1660 train_time:117075ms step_avg:92.33ms
step:1269/1660 train_time:117167ms step_avg:92.33ms
step:1270/1660 train_time:117259ms step_avg:92.33ms
step:1271/1660 train_time:117352ms step_avg:92.33ms
step:1272/1660 train_time:117445ms step_avg:92.33ms
step:1273/1660 train_time:117541ms step_avg:92.33ms
step:1274/1660 train_time:117637ms step_avg:92.34ms
step:1275/1660 train_time:117731ms step_avg:92.34ms
step:1276/1660 train_time:117825ms step_avg:92.34ms
step:1277/1660 train_time:117919ms step_avg:92.34ms
step:1278/1660 train_time:118011ms step_avg:92.34ms
step:1279/1660 train_time:118104ms step_avg:92.34ms
step:1280/1660 train_time:118196ms step_avg:92.34ms
step:1281/1660 train_time:118289ms step_avg:92.34ms
step:1282/1660 train_time:118382ms step_avg:92.34ms
step:1283/1660 train_time:118475ms step_avg:92.34ms
step:1284/1660 train_time:118569ms step_avg:92.34ms
step:1285/1660 train_time:118665ms step_avg:92.35ms
step:1286/1660 train_time:118758ms step_avg:92.35ms
step:1287/1660 train_time:118852ms step_avg:92.35ms
step:1288/1660 train_time:118945ms step_avg:92.35ms
step:1289/1660 train_time:119038ms step_avg:92.35ms
step:1290/1660 train_time:119130ms step_avg:92.35ms
step:1291/1660 train_time:119224ms step_avg:92.35ms
step:1292/1660 train_time:119317ms step_avg:92.35ms
step:1293/1660 train_time:119409ms step_avg:92.35ms
step:1294/1660 train_time:119503ms step_avg:92.35ms
step:1295/1660 train_time:119597ms step_avg:92.35ms
step:1296/1660 train_time:119691ms step_avg:92.35ms
step:1297/1660 train_time:119785ms step_avg:92.36ms
step:1298/1660 train_time:119878ms step_avg:92.36ms
step:1299/1660 train_time:119971ms step_avg:92.36ms
step:1300/1660 train_time:120063ms step_avg:92.36ms
step:1301/1660 train_time:120156ms step_avg:92.36ms
step:1302/1660 train_time:120249ms step_avg:92.36ms
step:1303/1660 train_time:120343ms step_avg:92.36ms
step:1304/1660 train_time:120435ms step_avg:92.36ms
step:1305/1660 train_time:120529ms step_avg:92.36ms
step:1306/1660 train_time:120624ms step_avg:92.36ms
step:1307/1660 train_time:120718ms step_avg:92.36ms
step:1308/1660 train_time:120811ms step_avg:92.36ms
step:1309/1660 train_time:120904ms step_avg:92.36ms
step:1310/1660 train_time:120997ms step_avg:92.36ms
step:1311/1660 train_time:121091ms step_avg:92.37ms
step:1312/1660 train_time:121184ms step_avg:92.37ms
step:1313/1660 train_time:121277ms step_avg:92.37ms
step:1314/1660 train_time:121369ms step_avg:92.37ms
step:1315/1660 train_time:121463ms step_avg:92.37ms
step:1316/1660 train_time:121557ms step_avg:92.37ms
step:1317/1660 train_time:121649ms step_avg:92.37ms
step:1318/1660 train_time:121744ms step_avg:92.37ms
step:1319/1660 train_time:121838ms step_avg:92.37ms
step:1320/1660 train_time:121930ms step_avg:92.37ms
step:1321/1660 train_time:122025ms step_avg:92.37ms
step:1322/1660 train_time:122121ms step_avg:92.38ms
step:1323/1660 train_time:122215ms step_avg:92.38ms
step:1324/1660 train_time:122307ms step_avg:92.38ms
step:1325/1660 train_time:122401ms step_avg:92.38ms
step:1326/1660 train_time:122495ms step_avg:92.38ms
step:1327/1660 train_time:122588ms step_avg:92.38ms
step:1328/1660 train_time:122682ms step_avg:92.38ms
step:1329/1660 train_time:122776ms step_avg:92.38ms
step:1330/1660 train_time:122869ms step_avg:92.38ms
step:1331/1660 train_time:122963ms step_avg:92.38ms
step:1332/1660 train_time:123058ms step_avg:92.39ms
step:1333/1660 train_time:123151ms step_avg:92.39ms
step:1334/1660 train_time:123244ms step_avg:92.39ms
step:1335/1660 train_time:123337ms step_avg:92.39ms
step:1336/1660 train_time:123430ms step_avg:92.39ms
step:1337/1660 train_time:123524ms step_avg:92.39ms
step:1338/1660 train_time:123617ms step_avg:92.39ms
step:1339/1660 train_time:123711ms step_avg:92.39ms
step:1340/1660 train_time:123805ms step_avg:92.39ms
step:1341/1660 train_time:123898ms step_avg:92.39ms
step:1342/1660 train_time:123990ms step_avg:92.39ms
step:1343/1660 train_time:124084ms step_avg:92.39ms
step:1344/1660 train_time:124178ms step_avg:92.39ms
step:1345/1660 train_time:124271ms step_avg:92.39ms
step:1346/1660 train_time:124364ms step_avg:92.40ms
step:1347/1660 train_time:124458ms step_avg:92.40ms
step:1348/1660 train_time:124551ms step_avg:92.40ms
step:1349/1660 train_time:124645ms step_avg:92.40ms
step:1350/1660 train_time:124738ms step_avg:92.40ms
step:1351/1660 train_time:124831ms step_avg:92.40ms
step:1352/1660 train_time:124925ms step_avg:92.40ms
step:1353/1660 train_time:125020ms step_avg:92.40ms
step:1354/1660 train_time:125115ms step_avg:92.40ms
step:1355/1660 train_time:125207ms step_avg:92.40ms
step:1356/1660 train_time:125300ms step_avg:92.40ms
step:1357/1660 train_time:125394ms step_avg:92.41ms
step:1358/1660 train_time:125487ms step_avg:92.41ms
step:1359/1660 train_time:125581ms step_avg:92.41ms
step:1360/1660 train_time:125674ms step_avg:92.41ms
step:1361/1660 train_time:125767ms step_avg:92.41ms
step:1362/1660 train_time:125860ms step_avg:92.41ms
step:1363/1660 train_time:125953ms step_avg:92.41ms
step:1364/1660 train_time:126046ms step_avg:92.41ms
step:1365/1660 train_time:126141ms step_avg:92.41ms
step:1366/1660 train_time:126235ms step_avg:92.41ms
step:1367/1660 train_time:126328ms step_avg:92.41ms
step:1368/1660 train_time:126423ms step_avg:92.41ms
step:1369/1660 train_time:126518ms step_avg:92.42ms
step:1370/1660 train_time:126611ms step_avg:92.42ms
step:1371/1660 train_time:126704ms step_avg:92.42ms
step:1372/1660 train_time:126797ms step_avg:92.42ms
step:1373/1660 train_time:126891ms step_avg:92.42ms
step:1374/1660 train_time:126984ms step_avg:92.42ms
step:1375/1660 train_time:127077ms step_avg:92.42ms
step:1375/1660 val_loss:3.3396 train_time:127171ms step_avg:92.49ms
step:1376/1660 train_time:127191ms step_avg:92.44ms
step:1377/1660 train_time:127270ms step_avg:92.43ms
step:1378/1660 train_time:127368ms step_avg:92.43ms
step:1379/1660 train_time:127462ms step_avg:92.43ms
step:1380/1660 train_time:127554ms step_avg:92.43ms
step:1381/1660 train_time:127646ms step_avg:92.43ms
step:1382/1660 train_time:127738ms step_avg:92.43ms
step:1383/1660 train_time:127831ms step_avg:92.43ms
step:1384/1660 train_time:127923ms step_avg:92.43ms
step:1385/1660 train_time:128016ms step_avg:92.43ms
step:1386/1660 train_time:128110ms step_avg:92.43ms
step:1387/1660 train_time:128205ms step_avg:92.43ms
step:1388/1660 train_time:128301ms step_avg:92.44ms
step:1389/1660 train_time:128396ms step_avg:92.44ms
step:1390/1660 train_time:128492ms step_avg:92.44ms
step:1391/1660 train_time:128586ms step_avg:92.44ms
step:1392/1660 train_time:128678ms step_avg:92.44ms
step:1393/1660 train_time:128772ms step_avg:92.44ms
step:1394/1660 train_time:128865ms step_avg:92.44ms
step:1395/1660 train_time:128957ms step_avg:92.44ms
step:1396/1660 train_time:129050ms step_avg:92.44ms
step:1397/1660 train_time:129143ms step_avg:92.44ms
step:1398/1660 train_time:129238ms step_avg:92.44ms
step:1399/1660 train_time:129332ms step_avg:92.45ms
step:1400/1660 train_time:129427ms step_avg:92.45ms
step:1401/1660 train_time:129521ms step_avg:92.45ms
step:1402/1660 train_time:129615ms step_avg:92.45ms
step:1403/1660 train_time:129708ms step_avg:92.45ms
step:1404/1660 train_time:129800ms step_avg:92.45ms
step:1405/1660 train_time:129893ms step_avg:92.45ms
step:1406/1660 train_time:129985ms step_avg:92.45ms
step:1407/1660 train_time:130078ms step_avg:92.45ms
step:1408/1660 train_time:130173ms step_avg:92.45ms
step:1409/1660 train_time:130266ms step_avg:92.45ms
step:1410/1660 train_time:130360ms step_avg:92.45ms
step:1411/1660 train_time:130454ms step_avg:92.45ms
step:1412/1660 train_time:130549ms step_avg:92.46ms
step:1413/1660 train_time:130642ms step_avg:92.46ms
step:1414/1660 train_time:130735ms step_avg:92.46ms
step:1415/1660 train_time:130827ms step_avg:92.46ms
step:1416/1660 train_time:130920ms step_avg:92.46ms
step:1417/1660 train_time:131013ms step_avg:92.46ms
step:1418/1660 train_time:131106ms step_avg:92.46ms
step:1419/1660 train_time:131199ms step_avg:92.46ms
step:1420/1660 train_time:131293ms step_avg:92.46ms
step:1421/1660 train_time:131387ms step_avg:92.46ms
step:1422/1660 train_time:131481ms step_avg:92.46ms
step:1423/1660 train_time:131574ms step_avg:92.46ms
step:1424/1660 train_time:131668ms step_avg:92.46ms
step:1425/1660 train_time:131761ms step_avg:92.46ms
step:1426/1660 train_time:131854ms step_avg:92.46ms
step:1427/1660 train_time:131947ms step_avg:92.46ms
step:1428/1660 train_time:132041ms step_avg:92.47ms
step:1429/1660 train_time:132136ms step_avg:92.47ms
step:1430/1660 train_time:132230ms step_avg:92.47ms
step:1431/1660 train_time:132324ms step_avg:92.47ms
step:1432/1660 train_time:132418ms step_avg:92.47ms
step:1433/1660 train_time:132512ms step_avg:92.47ms
step:1434/1660 train_time:132605ms step_avg:92.47ms
step:1435/1660 train_time:132698ms step_avg:92.47ms
step:1436/1660 train_time:132792ms step_avg:92.47ms
step:1437/1660 train_time:132886ms step_avg:92.47ms
step:1438/1660 train_time:132980ms step_avg:92.48ms
step:1439/1660 train_time:133074ms step_avg:92.48ms
step:1440/1660 train_time:133167ms step_avg:92.48ms
step:1441/1660 train_time:133260ms step_avg:92.48ms
step:1442/1660 train_time:133354ms step_avg:92.48ms
step:1443/1660 train_time:133449ms step_avg:92.48ms
step:1444/1660 train_time:133543ms step_avg:92.48ms
step:1445/1660 train_time:133635ms step_avg:92.48ms
step:1446/1660 train_time:133729ms step_avg:92.48ms
step:1447/1660 train_time:133822ms step_avg:92.48ms
step:1448/1660 train_time:133916ms step_avg:92.48ms
step:1449/1660 train_time:134010ms step_avg:92.48ms
step:1450/1660 train_time:134103ms step_avg:92.48ms
step:1451/1660 train_time:134196ms step_avg:92.48ms
step:1452/1660 train_time:134289ms step_avg:92.49ms
step:1453/1660 train_time:134383ms step_avg:92.49ms
step:1454/1660 train_time:134478ms step_avg:92.49ms
step:1455/1660 train_time:134571ms step_avg:92.49ms
step:1456/1660 train_time:134665ms step_avg:92.49ms
step:1457/1660 train_time:134757ms step_avg:92.49ms
step:1458/1660 train_time:134852ms step_avg:92.49ms
step:1459/1660 train_time:134946ms step_avg:92.49ms
step:1460/1660 train_time:135039ms step_avg:92.49ms
step:1461/1660 train_time:135134ms step_avg:92.49ms
step:1462/1660 train_time:135226ms step_avg:92.49ms
step:1463/1660 train_time:135320ms step_avg:92.49ms
step:1464/1660 train_time:135414ms step_avg:92.50ms
step:1465/1660 train_time:135507ms step_avg:92.50ms
step:1466/1660 train_time:135600ms step_avg:92.50ms
step:1467/1660 train_time:135693ms step_avg:92.50ms
step:1468/1660 train_time:135788ms step_avg:92.50ms
step:1469/1660 train_time:135882ms step_avg:92.50ms
step:1470/1660 train_time:135974ms step_avg:92.50ms
step:1471/1660 train_time:136069ms step_avg:92.50ms
step:1472/1660 train_time:136162ms step_avg:92.50ms
step:1473/1660 train_time:136255ms step_avg:92.50ms
step:1474/1660 train_time:136349ms step_avg:92.50ms
step:1475/1660 train_time:136443ms step_avg:92.50ms
step:1476/1660 train_time:136536ms step_avg:92.50ms
step:1477/1660 train_time:136629ms step_avg:92.50ms
step:1478/1660 train_time:136722ms step_avg:92.50ms
step:1479/1660 train_time:136816ms step_avg:92.51ms
step:1480/1660 train_time:136910ms step_avg:92.51ms
step:1481/1660 train_time:137004ms step_avg:92.51ms
step:1482/1660 train_time:137097ms step_avg:92.51ms
step:1483/1660 train_time:137191ms step_avg:92.51ms
step:1484/1660 train_time:137285ms step_avg:92.51ms
step:1485/1660 train_time:137379ms step_avg:92.51ms
step:1486/1660 train_time:137473ms step_avg:92.51ms
step:1487/1660 train_time:137566ms step_avg:92.51ms
step:1488/1660 train_time:137659ms step_avg:92.51ms
step:1489/1660 train_time:137753ms step_avg:92.51ms
step:1490/1660 train_time:137846ms step_avg:92.51ms
step:1491/1660 train_time:137940ms step_avg:92.52ms
step:1492/1660 train_time:138033ms step_avg:92.52ms
step:1493/1660 train_time:138126ms step_avg:92.52ms
step:1494/1660 train_time:138219ms step_avg:92.52ms
step:1495/1660 train_time:138313ms step_avg:92.52ms
step:1496/1660 train_time:138407ms step_avg:92.52ms
step:1497/1660 train_time:138499ms step_avg:92.52ms
step:1498/1660 train_time:138592ms step_avg:92.52ms
step:1499/1660 train_time:138686ms step_avg:92.52ms
step:1500/1660 train_time:138779ms step_avg:92.52ms
step:1500/1660 val_loss:3.3097 train_time:138874ms step_avg:92.58ms
step:1501/1660 train_time:138894ms step_avg:92.53ms
step:1502/1660 train_time:138971ms step_avg:92.52ms
step:1503/1660 train_time:139066ms step_avg:92.53ms
step:1504/1660 train_time:139158ms step_avg:92.53ms
step:1505/1660 train_time:139250ms step_avg:92.53ms
step:1506/1660 train_time:139342ms step_avg:92.52ms
step:1507/1660 train_time:139435ms step_avg:92.52ms
step:1508/1660 train_time:139527ms step_avg:92.52ms
step:1509/1660 train_time:139619ms step_avg:92.52ms
step:1510/1660 train_time:139713ms step_avg:92.52ms
step:1511/1660 train_time:139807ms step_avg:92.53ms
step:1512/1660 train_time:139902ms step_avg:92.53ms
step:1513/1660 train_time:139997ms step_avg:92.53ms
step:1514/1660 train_time:140091ms step_avg:92.53ms
step:1515/1660 train_time:140184ms step_avg:92.53ms
step:1516/1660 train_time:140277ms step_avg:92.53ms
step:1517/1660 train_time:140370ms step_avg:92.53ms
step:1518/1660 train_time:140462ms step_avg:92.53ms
step:1519/1660 train_time:140555ms step_avg:92.53ms
step:1520/1660 train_time:140648ms step_avg:92.53ms
step:1521/1660 train_time:140741ms step_avg:92.53ms
step:1522/1660 train_time:140836ms step_avg:92.53ms
step:1523/1660 train_time:140930ms step_avg:92.53ms
step:1524/1660 train_time:141023ms step_avg:92.53ms
step:1525/1660 train_time:141117ms step_avg:92.54ms
step:1526/1660 train_time:141211ms step_avg:92.54ms
step:1527/1660 train_time:141304ms step_avg:92.54ms
step:1528/1660 train_time:141397ms step_avg:92.54ms
step:1529/1660 train_time:141490ms step_avg:92.54ms
step:1530/1660 train_time:141582ms step_avg:92.54ms
step:1531/1660 train_time:141676ms step_avg:92.54ms
step:1532/1660 train_time:141770ms step_avg:92.54ms
step:1533/1660 train_time:141863ms step_avg:92.54ms
step:1534/1660 train_time:141957ms step_avg:92.54ms
step:1535/1660 train_time:142050ms step_avg:92.54ms
step:1536/1660 train_time:142144ms step_avg:92.54ms
step:1537/1660 train_time:142238ms step_avg:92.54ms
step:1538/1660 train_time:142331ms step_avg:92.54ms
step:1539/1660 train_time:142424ms step_avg:92.54ms
step:1540/1660 train_time:142517ms step_avg:92.54ms
step:1541/1660 train_time:142609ms step_avg:92.54ms
step:1542/1660 train_time:142702ms step_avg:92.54ms
step:1543/1660 train_time:142796ms step_avg:92.54ms
step:1544/1660 train_time:142889ms step_avg:92.54ms
step:1545/1660 train_time:142982ms step_avg:92.55ms
step:1546/1660 train_time:143076ms step_avg:92.55ms
step:1547/1660 train_time:143171ms step_avg:92.55ms
step:1548/1660 train_time:143265ms step_avg:92.55ms
step:1549/1660 train_time:143358ms step_avg:92.55ms
step:1550/1660 train_time:143451ms step_avg:92.55ms
step:1551/1660 train_time:143544ms step_avg:92.55ms
step:1552/1660 train_time:143638ms step_avg:92.55ms
step:1553/1660 train_time:143731ms step_avg:92.55ms
step:1554/1660 train_time:143823ms step_avg:92.55ms
step:1555/1660 train_time:143917ms step_avg:92.55ms
step:1556/1660 train_time:144011ms step_avg:92.55ms
step:1557/1660 train_time:144104ms step_avg:92.55ms
step:1558/1660 train_time:144198ms step_avg:92.55ms
step:1559/1660 train_time:144291ms step_avg:92.55ms
step:1560/1660 train_time:144384ms step_avg:92.55ms
step:1561/1660 train_time:144478ms step_avg:92.55ms
step:1562/1660 train_time:144571ms step_avg:92.55ms
step:1563/1660 train_time:144663ms step_avg:92.55ms
step:1564/1660 train_time:144756ms step_avg:92.56ms
step:1565/1660 train_time:144849ms step_avg:92.56ms
step:1566/1660 train_time:144942ms step_avg:92.56ms
step:1567/1660 train_time:145036ms step_avg:92.56ms
step:1568/1660 train_time:145130ms step_avg:92.56ms
step:1569/1660 train_time:145223ms step_avg:92.56ms
step:1570/1660 train_time:145317ms step_avg:92.56ms
step:1571/1660 train_time:145409ms step_avg:92.56ms
step:1572/1660 train_time:145503ms step_avg:92.56ms
step:1573/1660 train_time:145596ms step_avg:92.56ms
step:1574/1660 train_time:145689ms step_avg:92.56ms
step:1575/1660 train_time:145781ms step_avg:92.56ms
step:1576/1660 train_time:145875ms step_avg:92.56ms
step:1577/1660 train_time:145968ms step_avg:92.56ms
step:1578/1660 train_time:146061ms step_avg:92.56ms
step:1579/1660 train_time:146156ms step_avg:92.56ms
step:1580/1660 train_time:146249ms step_avg:92.56ms
step:1581/1660 train_time:146343ms step_avg:92.56ms
step:1582/1660 train_time:146437ms step_avg:92.56ms
step:1583/1660 train_time:146530ms step_avg:92.57ms
step:1584/1660 train_time:146623ms step_avg:92.56ms
step:1585/1660 train_time:146716ms step_avg:92.57ms
step:1586/1660 train_time:146810ms step_avg:92.57ms
step:1587/1660 train_time:146903ms step_avg:92.57ms
step:1588/1660 train_time:146997ms step_avg:92.57ms
step:1589/1660 train_time:147090ms step_avg:92.57ms
step:1590/1660 train_time:147183ms step_avg:92.57ms
step:1591/1660 train_time:147277ms step_avg:92.57ms
step:1592/1660 train_time:147370ms step_avg:92.57ms
step:1593/1660 train_time:147464ms step_avg:92.57ms
step:1594/1660 train_time:147557ms step_avg:92.57ms
step:1595/1660 train_time:147650ms step_avg:92.57ms
step:1596/1660 train_time:147743ms step_avg:92.57ms
step:1597/1660 train_time:147837ms step_avg:92.57ms
step:1598/1660 train_time:147931ms step_avg:92.57ms
step:1599/1660 train_time:148023ms step_avg:92.57ms
step:1600/1660 train_time:148116ms step_avg:92.57ms
step:1601/1660 train_time:148209ms step_avg:92.57ms
step:1602/1660 train_time:148302ms step_avg:92.57ms
step:1603/1660 train_time:148396ms step_avg:92.57ms
step:1604/1660 train_time:148490ms step_avg:92.57ms
step:1605/1660 train_time:148583ms step_avg:92.57ms
step:1606/1660 train_time:148676ms step_avg:92.58ms
step:1607/1660 train_time:148770ms step_avg:92.58ms
step:1608/1660 train_time:148863ms step_avg:92.58ms
step:1609/1660 train_time:148956ms step_avg:92.58ms
step:1610/1660 train_time:149050ms step_avg:92.58ms
step:1611/1660 train_time:149143ms step_avg:92.58ms
step:1612/1660 train_time:149237ms step_avg:92.58ms
step:1613/1660 train_time:149330ms step_avg:92.58ms
step:1614/1660 train_time:149423ms step_avg:92.58ms
step:1615/1660 train_time:149516ms step_avg:92.58ms
step:1616/1660 train_time:149609ms step_avg:92.58ms
step:1617/1660 train_time:149703ms step_avg:92.58ms
step:1618/1660 train_time:149796ms step_avg:92.58ms
step:1619/1660 train_time:149889ms step_avg:92.58ms
step:1620/1660 train_time:149982ms step_avg:92.58ms
step:1621/1660 train_time:150076ms step_avg:92.58ms
step:1622/1660 train_time:150170ms step_avg:92.58ms
step:1623/1660 train_time:150263ms step_avg:92.58ms
step:1624/1660 train_time:150356ms step_avg:92.58ms
step:1625/1660 train_time:150449ms step_avg:92.58ms
step:1625/1660 val_loss:3.2846 train_time:150544ms step_avg:92.64ms
step:1626/1660 train_time:150564ms step_avg:92.60ms
step:1627/1660 train_time:150642ms step_avg:92.59ms
step:1628/1660 train_time:150739ms step_avg:92.59ms
step:1629/1660 train_time:150832ms step_avg:92.59ms
step:1630/1660 train_time:150925ms step_avg:92.59ms
step:1631/1660 train_time:151016ms step_avg:92.59ms
step:1632/1660 train_time:151109ms step_avg:92.59ms
step:1633/1660 train_time:151200ms step_avg:92.59ms
step:1634/1660 train_time:151293ms step_avg:92.59ms
step:1635/1660 train_time:151385ms step_avg:92.59ms
step:1636/1660 train_time:151479ms step_avg:92.59ms
step:1637/1660 train_time:151575ms step_avg:92.59ms
step:1638/1660 train_time:151672ms step_avg:92.60ms
step:1639/1660 train_time:151768ms step_avg:92.60ms
step:1640/1660 train_time:151861ms step_avg:92.60ms
step:1641/1660 train_time:151953ms step_avg:92.60ms
step:1642/1660 train_time:152046ms step_avg:92.60ms
step:1643/1660 train_time:152138ms step_avg:92.60ms
step:1644/1660 train_time:152230ms step_avg:92.60ms
step:1645/1660 train_time:152323ms step_avg:92.60ms
step:1646/1660 train_time:152415ms step_avg:92.60ms
step:1647/1660 train_time:152510ms step_avg:92.60ms
step:1648/1660 train_time:152606ms step_avg:92.60ms
step:1649/1660 train_time:152700ms step_avg:92.60ms
step:1650/1660 train_time:152793ms step_avg:92.60ms
step:1651/1660 train_time:152887ms step_avg:92.60ms
step:1652/1660 train_time:152979ms step_avg:92.60ms
step:1653/1660 train_time:153072ms step_avg:92.60ms
step:1654/1660 train_time:153164ms step_avg:92.60ms
step:1655/1660 train_time:153257ms step_avg:92.60ms
step:1656/1660 train_time:153349ms step_avg:92.60ms
step:1657/1660 train_time:153443ms step_avg:92.60ms
step:1658/1660 train_time:153537ms step_avg:92.60ms
step:1659/1660 train_time:153631ms step_avg:92.60ms
step:1660/1660 train_time:153725ms step_avg:92.61ms
step:1660/1660 val_loss:3.2764 train_time:153820ms step_avg:92.66ms
peak memory allocated: 32002 MiB reserved: 46836 MiB

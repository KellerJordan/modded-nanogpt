import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload=False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            #t0 = time.perf_counter()
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            #t1 = time.perf_counter()
            #print(f'{t1-t0} slowload')
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    def __init__(self, file_iter, world_size):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
      #loading in a whole shard will be slow...
      #BosFinder tracks its self.i index. I can kickoff with a smaller set. then have the 
      finder = BOSFinder(tokens, world_size=world_size, quickload=True)
      preloader = DataPreloader(file_iter, world_size)
      preloader.start()
    else:
      pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                #tokens = _load_data_shard(next(file_iter))
                #finder = BOSFinder(tokens, world_size=world_size)
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main
@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1660 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"data_threading_1660/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer is highly indifferent to length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Tue Sep 16 03:46:40 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   27C    P0            116W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   27C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   27C    P0            111W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   29C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   27C    P0            116W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          194072      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          194073      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          194074      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          194075      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          194076      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          194077      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          194078      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          194079      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          194073      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          194074      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          194075      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          194076      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          194077      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          194078      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          194079      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1660 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1660 train_time:148ms step_avg:148.18ms
step:2/1660 train_time:171ms step_avg:85.50ms
step:3/1660 train_time:238ms step_avg:79.45ms
step:4/1660 train_time:328ms step_avg:81.94ms
step:5/1660 train_time:418ms step_avg:83.69ms
step:6/1660 train_time:509ms step_avg:84.78ms
step:7/1660 train_time:599ms step_avg:85.64ms
step:8/1660 train_time:690ms step_avg:86.25ms
step:9/1660 train_time:781ms step_avg:86.74ms
step:10/1660 train_time:871ms step_avg:87.14ms
step:11/1660 train_time:962ms step_avg:87.45ms
step:12/1660 train_time:1058ms step_avg:88.13ms
step:13/1660 train_time:1153ms step_avg:88.68ms
step:14/1660 train_time:1246ms step_avg:88.98ms
step:15/1660 train_time:1337ms step_avg:89.16ms
step:16/1660 train_time:1428ms step_avg:89.26ms
step:17/1660 train_time:1519ms step_avg:89.35ms
step:18/1660 train_time:1609ms step_avg:89.41ms
step:19/1660 train_time:1700ms step_avg:89.49ms
step:20/1660 train_time:1791ms step_avg:89.56ms
step:21/1660 train_time:1882ms step_avg:89.61ms
step:22/1660 train_time:1975ms step_avg:89.75ms
step:23/1660 train_time:2067ms step_avg:89.87ms
step:24/1660 train_time:2162ms step_avg:90.09ms
step:25/1660 train_time:2255ms step_avg:90.22ms
step:26/1660 train_time:2347ms step_avg:90.28ms
step:27/1660 train_time:2439ms step_avg:90.32ms
step:28/1660 train_time:2530ms step_avg:90.35ms
step:29/1660 train_time:2620ms step_avg:90.36ms
step:30/1660 train_time:2711ms step_avg:90.38ms
step:31/1660 train_time:2802ms step_avg:90.38ms
step:32/1660 train_time:2893ms step_avg:90.40ms
step:33/1660 train_time:2984ms step_avg:90.41ms
step:34/1660 train_time:3076ms step_avg:90.48ms
step:35/1660 train_time:3169ms step_avg:90.54ms
step:36/1660 train_time:3262ms step_avg:90.60ms
step:37/1660 train_time:3354ms step_avg:90.64ms
step:38/1660 train_time:3445ms step_avg:90.67ms
step:39/1660 train_time:3537ms step_avg:90.69ms
step:40/1660 train_time:3628ms step_avg:90.70ms
step:41/1660 train_time:3719ms step_avg:90.70ms
step:42/1660 train_time:3811ms step_avg:90.73ms
step:43/1660 train_time:3902ms step_avg:90.74ms
step:44/1660 train_time:3992ms step_avg:90.73ms
step:45/1660 train_time:4084ms step_avg:90.75ms
step:46/1660 train_time:4177ms step_avg:90.80ms
step:47/1660 train_time:4268ms step_avg:90.82ms
step:48/1660 train_time:4361ms step_avg:90.86ms
step:49/1660 train_time:4453ms step_avg:90.88ms
step:50/1660 train_time:4544ms step_avg:90.89ms
step:51/1660 train_time:4636ms step_avg:90.91ms
step:52/1660 train_time:4728ms step_avg:90.92ms
step:53/1660 train_time:4818ms step_avg:90.91ms
step:54/1660 train_time:4910ms step_avg:90.92ms
step:55/1660 train_time:5001ms step_avg:90.93ms
step:56/1660 train_time:5093ms step_avg:90.95ms
step:57/1660 train_time:5185ms step_avg:90.96ms
step:58/1660 train_time:5278ms step_avg:91.00ms
step:59/1660 train_time:5370ms step_avg:91.01ms
step:60/1660 train_time:5461ms step_avg:91.02ms
step:61/1660 train_time:5552ms step_avg:91.02ms
step:62/1660 train_time:5644ms step_avg:91.03ms
step:63/1660 train_time:5736ms step_avg:91.05ms
step:64/1660 train_time:5827ms step_avg:91.05ms
step:65/1660 train_time:5919ms step_avg:91.06ms
step:66/1660 train_time:6011ms step_avg:91.08ms
step:67/1660 train_time:6103ms step_avg:91.08ms
step:68/1660 train_time:6194ms step_avg:91.09ms
step:69/1660 train_time:6286ms step_avg:91.10ms
step:70/1660 train_time:6378ms step_avg:91.11ms
step:71/1660 train_time:6470ms step_avg:91.12ms
step:72/1660 train_time:6562ms step_avg:91.14ms
step:73/1660 train_time:6653ms step_avg:91.14ms
step:74/1660 train_time:6745ms step_avg:91.15ms
step:75/1660 train_time:6837ms step_avg:91.16ms
step:76/1660 train_time:6928ms step_avg:91.16ms
step:77/1660 train_time:7020ms step_avg:91.16ms
step:78/1660 train_time:7112ms step_avg:91.18ms
step:79/1660 train_time:7204ms step_avg:91.19ms
step:80/1660 train_time:7295ms step_avg:91.19ms
step:81/1660 train_time:7387ms step_avg:91.19ms
step:82/1660 train_time:7481ms step_avg:91.23ms
step:83/1660 train_time:7573ms step_avg:91.24ms
step:84/1660 train_time:7664ms step_avg:91.24ms
step:85/1660 train_time:7757ms step_avg:91.25ms
step:86/1660 train_time:7847ms step_avg:91.25ms
step:87/1660 train_time:7939ms step_avg:91.25ms
step:88/1660 train_time:8031ms step_avg:91.26ms
step:89/1660 train_time:8122ms step_avg:91.25ms
step:90/1660 train_time:8213ms step_avg:91.26ms
step:91/1660 train_time:8306ms step_avg:91.27ms
step:92/1660 train_time:8398ms step_avg:91.28ms
step:93/1660 train_time:8489ms step_avg:91.28ms
step:94/1660 train_time:8581ms step_avg:91.28ms
step:95/1660 train_time:8673ms step_avg:91.29ms
step:96/1660 train_time:8764ms step_avg:91.29ms
step:97/1660 train_time:8855ms step_avg:91.29ms
step:98/1660 train_time:8946ms step_avg:91.29ms
step:99/1660 train_time:9038ms step_avg:91.29ms
step:100/1660 train_time:9129ms step_avg:91.29ms
step:101/1660 train_time:9220ms step_avg:91.29ms
step:102/1660 train_time:9312ms step_avg:91.29ms
step:103/1660 train_time:9404ms step_avg:91.30ms
step:104/1660 train_time:9496ms step_avg:91.31ms
step:105/1660 train_time:9587ms step_avg:91.30ms
step:106/1660 train_time:9680ms step_avg:91.32ms
step:107/1660 train_time:9771ms step_avg:91.32ms
step:108/1660 train_time:9862ms step_avg:91.32ms
step:109/1660 train_time:9954ms step_avg:91.32ms
step:110/1660 train_time:10044ms step_avg:91.31ms
step:111/1660 train_time:10136ms step_avg:91.31ms
step:112/1660 train_time:10227ms step_avg:91.32ms
step:113/1660 train_time:10319ms step_avg:91.32ms
step:114/1660 train_time:10411ms step_avg:91.33ms
step:115/1660 train_time:10502ms step_avg:91.32ms
step:116/1660 train_time:10594ms step_avg:91.32ms
step:117/1660 train_time:10684ms step_avg:91.32ms
step:118/1660 train_time:10777ms step_avg:91.33ms
step:119/1660 train_time:10868ms step_avg:91.33ms
step:120/1660 train_time:10960ms step_avg:91.33ms
step:121/1660 train_time:11052ms step_avg:91.34ms
step:122/1660 train_time:11143ms step_avg:91.34ms
step:123/1660 train_time:11234ms step_avg:91.34ms
step:124/1660 train_time:11325ms step_avg:91.33ms
step:125/1660 train_time:11416ms step_avg:91.33ms
step:125/1660 val_loss:4.3163 train_time:11509ms step_avg:92.07ms
step:126/1660 train_time:11530ms step_avg:91.50ms
step:127/1660 train_time:11603ms step_avg:91.36ms
step:128/1660 train_time:11705ms step_avg:91.45ms
step:129/1660 train_time:11804ms step_avg:91.50ms
step:130/1660 train_time:11896ms step_avg:91.51ms
step:131/1660 train_time:11986ms step_avg:91.50ms
step:132/1660 train_time:12076ms step_avg:91.49ms
step:133/1660 train_time:12166ms step_avg:91.47ms
step:134/1660 train_time:12257ms step_avg:91.47ms
step:135/1660 train_time:12347ms step_avg:91.46ms
step:136/1660 train_time:12437ms step_avg:91.45ms
step:137/1660 train_time:12528ms step_avg:91.44ms
step:138/1660 train_time:12621ms step_avg:91.46ms
step:139/1660 train_time:12716ms step_avg:91.48ms
step:140/1660 train_time:12809ms step_avg:91.49ms
step:141/1660 train_time:12905ms step_avg:91.52ms
step:142/1660 train_time:12996ms step_avg:91.52ms
step:143/1660 train_time:13087ms step_avg:91.52ms
step:144/1660 train_time:13178ms step_avg:91.51ms
step:145/1660 train_time:13267ms step_avg:91.50ms
step:146/1660 train_time:13358ms step_avg:91.49ms
step:147/1660 train_time:13448ms step_avg:91.49ms
step:148/1660 train_time:13539ms step_avg:91.48ms
step:149/1660 train_time:13631ms step_avg:91.49ms
step:150/1660 train_time:13724ms step_avg:91.49ms
step:151/1660 train_time:13817ms step_avg:91.50ms
step:152/1660 train_time:13909ms step_avg:91.51ms
step:153/1660 train_time:14002ms step_avg:91.52ms
step:154/1660 train_time:14093ms step_avg:91.52ms
step:155/1660 train_time:14185ms step_avg:91.52ms
step:156/1660 train_time:14276ms step_avg:91.51ms
step:157/1660 train_time:14367ms step_avg:91.51ms
step:158/1660 train_time:14457ms step_avg:91.50ms
step:159/1660 train_time:14548ms step_avg:91.50ms
step:160/1660 train_time:14640ms step_avg:91.50ms
step:161/1660 train_time:14731ms step_avg:91.50ms
step:162/1660 train_time:14824ms step_avg:91.51ms
step:163/1660 train_time:14917ms step_avg:91.52ms
step:164/1660 train_time:15008ms step_avg:91.51ms
step:165/1660 train_time:15100ms step_avg:91.51ms
step:166/1660 train_time:15191ms step_avg:91.51ms
step:167/1660 train_time:15283ms step_avg:91.52ms
step:168/1660 train_time:15374ms step_avg:91.51ms
step:169/1660 train_time:15465ms step_avg:91.51ms
step:170/1660 train_time:15556ms step_avg:91.51ms
step:171/1660 train_time:15647ms step_avg:91.50ms
step:172/1660 train_time:15738ms step_avg:91.50ms
step:173/1660 train_time:15829ms step_avg:91.50ms
step:174/1660 train_time:15923ms step_avg:91.51ms
step:175/1660 train_time:16014ms step_avg:91.51ms
step:176/1660 train_time:16106ms step_avg:91.51ms
step:177/1660 train_time:16198ms step_avg:91.51ms
step:178/1660 train_time:16288ms step_avg:91.51ms
step:179/1660 train_time:16379ms step_avg:91.50ms
step:180/1660 train_time:16470ms step_avg:91.50ms
step:181/1660 train_time:16561ms step_avg:91.50ms
step:182/1660 train_time:16652ms step_avg:91.49ms
step:183/1660 train_time:16744ms step_avg:91.50ms
step:184/1660 train_time:16836ms step_avg:91.50ms
step:185/1660 train_time:16927ms step_avg:91.50ms
step:186/1660 train_time:17019ms step_avg:91.50ms
step:187/1660 train_time:17110ms step_avg:91.50ms
step:188/1660 train_time:17203ms step_avg:91.50ms
step:189/1660 train_time:17295ms step_avg:91.51ms
step:190/1660 train_time:17387ms step_avg:91.51ms
step:191/1660 train_time:17477ms step_avg:91.50ms
step:192/1660 train_time:17568ms step_avg:91.50ms
step:193/1660 train_time:17659ms step_avg:91.50ms
step:194/1660 train_time:17750ms step_avg:91.50ms
step:195/1660 train_time:17843ms step_avg:91.50ms
step:196/1660 train_time:17934ms step_avg:91.50ms
step:197/1660 train_time:18026ms step_avg:91.50ms
step:198/1660 train_time:18118ms step_avg:91.51ms
step:199/1660 train_time:18210ms step_avg:91.51ms
step:200/1660 train_time:18302ms step_avg:91.51ms
step:201/1660 train_time:18394ms step_avg:91.51ms
step:202/1660 train_time:18485ms step_avg:91.51ms
step:203/1660 train_time:18576ms step_avg:91.51ms
step:204/1660 train_time:18667ms step_avg:91.50ms
step:205/1660 train_time:18759ms step_avg:91.51ms
step:206/1660 train_time:18850ms step_avg:91.51ms
step:207/1660 train_time:18942ms step_avg:91.51ms
step:208/1660 train_time:19032ms step_avg:91.50ms
step:209/1660 train_time:19125ms step_avg:91.51ms
step:210/1660 train_time:19217ms step_avg:91.51ms
step:211/1660 train_time:19308ms step_avg:91.51ms
step:212/1660 train_time:19399ms step_avg:91.51ms
step:213/1660 train_time:19491ms step_avg:91.51ms
step:214/1660 train_time:19583ms step_avg:91.51ms
step:215/1660 train_time:19674ms step_avg:91.51ms
step:216/1660 train_time:19765ms step_avg:91.51ms
step:217/1660 train_time:19858ms step_avg:91.51ms
step:218/1660 train_time:19949ms step_avg:91.51ms
step:219/1660 train_time:20040ms step_avg:91.51ms
step:220/1660 train_time:20131ms step_avg:91.50ms
step:221/1660 train_time:20223ms step_avg:91.51ms
step:222/1660 train_time:20315ms step_avg:91.51ms
step:223/1660 train_time:20406ms step_avg:91.51ms
step:224/1660 train_time:20498ms step_avg:91.51ms
step:225/1660 train_time:20590ms step_avg:91.51ms
step:226/1660 train_time:20683ms step_avg:91.52ms
step:227/1660 train_time:20774ms step_avg:91.52ms
step:228/1660 train_time:20865ms step_avg:91.51ms
step:229/1660 train_time:20958ms step_avg:91.52ms
step:230/1660 train_time:21048ms step_avg:91.52ms
step:231/1660 train_time:21140ms step_avg:91.51ms
step:232/1660 train_time:21231ms step_avg:91.51ms
step:233/1660 train_time:21323ms step_avg:91.52ms
step:234/1660 train_time:21416ms step_avg:91.52ms
step:235/1660 train_time:21507ms step_avg:91.52ms
step:236/1660 train_time:21598ms step_avg:91.52ms
step:237/1660 train_time:21689ms step_avg:91.52ms
step:238/1660 train_time:21781ms step_avg:91.52ms
step:239/1660 train_time:21872ms step_avg:91.52ms
step:240/1660 train_time:21964ms step_avg:91.52ms
step:241/1660 train_time:22055ms step_avg:91.52ms
step:242/1660 train_time:22147ms step_avg:91.51ms
step:243/1660 train_time:22239ms step_avg:91.52ms
step:244/1660 train_time:22330ms step_avg:91.51ms
step:245/1660 train_time:22421ms step_avg:91.51ms
step:246/1660 train_time:22512ms step_avg:91.51ms
step:247/1660 train_time:22606ms step_avg:91.52ms
step:248/1660 train_time:22698ms step_avg:91.52ms
step:249/1660 train_time:22789ms step_avg:91.52ms
step:250/1660 train_time:22880ms step_avg:91.52ms
step:250/1660 val_loss:3.9765 train_time:22972ms step_avg:91.89ms
step:251/1660 train_time:22991ms step_avg:91.60ms
step:252/1660 train_time:23067ms step_avg:91.54ms
step:253/1660 train_time:23166ms step_avg:91.56ms
step:254/1660 train_time:23259ms step_avg:91.57ms
step:255/1660 train_time:23351ms step_avg:91.57ms
step:256/1660 train_time:23442ms step_avg:91.57ms
step:257/1660 train_time:23533ms step_avg:91.57ms
step:258/1660 train_time:23623ms step_avg:91.56ms
step:259/1660 train_time:23713ms step_avg:91.56ms
step:260/1660 train_time:23803ms step_avg:91.55ms
step:261/1660 train_time:23894ms step_avg:91.55ms
step:262/1660 train_time:23985ms step_avg:91.55ms
step:263/1660 train_time:24080ms step_avg:91.56ms
step:264/1660 train_time:24174ms step_avg:91.57ms
step:265/1660 train_time:24266ms step_avg:91.57ms
step:266/1660 train_time:24358ms step_avg:91.57ms
step:267/1660 train_time:24449ms step_avg:91.57ms
step:268/1660 train_time:24540ms step_avg:91.57ms
step:269/1660 train_time:24631ms step_avg:91.57ms
step:270/1660 train_time:24722ms step_avg:91.56ms
step:271/1660 train_time:24813ms step_avg:91.56ms
step:272/1660 train_time:24903ms step_avg:91.56ms
step:273/1660 train_time:24996ms step_avg:91.56ms
step:274/1660 train_time:25087ms step_avg:91.56ms
step:275/1660 train_time:25181ms step_avg:91.57ms
step:276/1660 train_time:25273ms step_avg:91.57ms
step:277/1660 train_time:25365ms step_avg:91.57ms
step:278/1660 train_time:25457ms step_avg:91.57ms
step:279/1660 train_time:25549ms step_avg:91.57ms
step:280/1660 train_time:25640ms step_avg:91.57ms
step:281/1660 train_time:25731ms step_avg:91.57ms
step:282/1660 train_time:25821ms step_avg:91.56ms
step:283/1660 train_time:25912ms step_avg:91.56ms
step:284/1660 train_time:26004ms step_avg:91.56ms
step:285/1660 train_time:26095ms step_avg:91.56ms
step:286/1660 train_time:26187ms step_avg:91.56ms
step:287/1660 train_time:26279ms step_avg:91.56ms
step:288/1660 train_time:26371ms step_avg:91.57ms
step:289/1660 train_time:26463ms step_avg:91.57ms
step:290/1660 train_time:26555ms step_avg:91.57ms
step:291/1660 train_time:26646ms step_avg:91.57ms
step:292/1660 train_time:26737ms step_avg:91.56ms
step:293/1660 train_time:26827ms step_avg:91.56ms
step:294/1660 train_time:26918ms step_avg:91.56ms
step:295/1660 train_time:27009ms step_avg:91.56ms
step:296/1660 train_time:27101ms step_avg:91.56ms
step:297/1660 train_time:27193ms step_avg:91.56ms
step:298/1660 train_time:27283ms step_avg:91.56ms
step:299/1660 train_time:27376ms step_avg:91.56ms
step:300/1660 train_time:27468ms step_avg:91.56ms
step:301/1660 train_time:27560ms step_avg:91.56ms
step:302/1660 train_time:27652ms step_avg:91.56ms
step:303/1660 train_time:27743ms step_avg:91.56ms
step:304/1660 train_time:27835ms step_avg:91.56ms
step:305/1660 train_time:27925ms step_avg:91.56ms
step:306/1660 train_time:28017ms step_avg:91.56ms
step:307/1660 train_time:28108ms step_avg:91.56ms
step:308/1660 train_time:28199ms step_avg:91.56ms
step:309/1660 train_time:28291ms step_avg:91.56ms
step:310/1660 train_time:28383ms step_avg:91.56ms
step:311/1660 train_time:28475ms step_avg:91.56ms
step:312/1660 train_time:28567ms step_avg:91.56ms
step:313/1660 train_time:28659ms step_avg:91.56ms
step:314/1660 train_time:28750ms step_avg:91.56ms
step:315/1660 train_time:28841ms step_avg:91.56ms
step:316/1660 train_time:28932ms step_avg:91.56ms
step:317/1660 train_time:29024ms step_avg:91.56ms
step:318/1660 train_time:29115ms step_avg:91.56ms
step:319/1660 train_time:29205ms step_avg:91.55ms
step:320/1660 train_time:29297ms step_avg:91.55ms
step:321/1660 train_time:29388ms step_avg:91.55ms
step:322/1660 train_time:29482ms step_avg:91.56ms
step:323/1660 train_time:29574ms step_avg:91.56ms
step:324/1660 train_time:29665ms step_avg:91.56ms
step:325/1660 train_time:29758ms step_avg:91.56ms
step:326/1660 train_time:29850ms step_avg:91.56ms
step:327/1660 train_time:29941ms step_avg:91.56ms
step:328/1660 train_time:30032ms step_avg:91.56ms
step:329/1660 train_time:30123ms step_avg:91.56ms
step:330/1660 train_time:30214ms step_avg:91.56ms
step:331/1660 train_time:30306ms step_avg:91.56ms
step:332/1660 train_time:30398ms step_avg:91.56ms
step:333/1660 train_time:30489ms step_avg:91.56ms
step:334/1660 train_time:30582ms step_avg:91.56ms
step:335/1660 train_time:30674ms step_avg:91.57ms
step:336/1660 train_time:30766ms step_avg:91.57ms
step:337/1660 train_time:30858ms step_avg:91.57ms
step:338/1660 train_time:30950ms step_avg:91.57ms
step:339/1660 train_time:31041ms step_avg:91.57ms
step:340/1660 train_time:31132ms step_avg:91.56ms
step:341/1660 train_time:31223ms step_avg:91.56ms
step:342/1660 train_time:31315ms step_avg:91.56ms
step:343/1660 train_time:31407ms step_avg:91.56ms
step:344/1660 train_time:31498ms step_avg:91.57ms
step:345/1660 train_time:31590ms step_avg:91.57ms
step:346/1660 train_time:31683ms step_avg:91.57ms
step:347/1660 train_time:31775ms step_avg:91.57ms
step:348/1660 train_time:31865ms step_avg:91.57ms
step:349/1660 train_time:31957ms step_avg:91.57ms
step:350/1660 train_time:32049ms step_avg:91.57ms
step:351/1660 train_time:32140ms step_avg:91.57ms
step:352/1660 train_time:32231ms step_avg:91.57ms
step:353/1660 train_time:32322ms step_avg:91.56ms
step:354/1660 train_time:32413ms step_avg:91.56ms
step:355/1660 train_time:32504ms step_avg:91.56ms
step:356/1660 train_time:32596ms step_avg:91.56ms
step:357/1660 train_time:32686ms step_avg:91.56ms
step:358/1660 train_time:32779ms step_avg:91.56ms
step:359/1660 train_time:32871ms step_avg:91.56ms
step:360/1660 train_time:32963ms step_avg:91.56ms
step:361/1660 train_time:33055ms step_avg:91.57ms
step:362/1660 train_time:33145ms step_avg:91.56ms
step:363/1660 train_time:33237ms step_avg:91.56ms
step:364/1660 train_time:33327ms step_avg:91.56ms
step:365/1660 train_time:33418ms step_avg:91.56ms
step:366/1660 train_time:33510ms step_avg:91.56ms
step:367/1660 train_time:33601ms step_avg:91.56ms
step:368/1660 train_time:33692ms step_avg:91.56ms
step:369/1660 train_time:33784ms step_avg:91.55ms
step:370/1660 train_time:33876ms step_avg:91.56ms
step:371/1660 train_time:33967ms step_avg:91.55ms
step:372/1660 train_time:34059ms step_avg:91.56ms
step:373/1660 train_time:34150ms step_avg:91.56ms
step:374/1660 train_time:34242ms step_avg:91.56ms
step:375/1660 train_time:34333ms step_avg:91.56ms
step:375/1660 val_loss:3.8187 train_time:34426ms step_avg:91.80ms
step:376/1660 train_time:34445ms step_avg:91.61ms
step:377/1660 train_time:34520ms step_avg:91.57ms
step:378/1660 train_time:34615ms step_avg:91.57ms
step:379/1660 train_time:34707ms step_avg:91.57ms
step:380/1660 train_time:34797ms step_avg:91.57ms
step:381/1660 train_time:34888ms step_avg:91.57ms
step:382/1660 train_time:34978ms step_avg:91.56ms
step:383/1660 train_time:35068ms step_avg:91.56ms
step:384/1660 train_time:35158ms step_avg:91.56ms
step:385/1660 train_time:35249ms step_avg:91.56ms
step:386/1660 train_time:35340ms step_avg:91.55ms
step:387/1660 train_time:35435ms step_avg:91.56ms
step:388/1660 train_time:35529ms step_avg:91.57ms
step:389/1660 train_time:35622ms step_avg:91.57ms
step:390/1660 train_time:35714ms step_avg:91.58ms
step:391/1660 train_time:35805ms step_avg:91.57ms
step:392/1660 train_time:35896ms step_avg:91.57ms
step:393/1660 train_time:35986ms step_avg:91.57ms
step:394/1660 train_time:36077ms step_avg:91.57ms
step:395/1660 train_time:36167ms step_avg:91.56ms
step:396/1660 train_time:36257ms step_avg:91.56ms
step:397/1660 train_time:36348ms step_avg:91.56ms
step:398/1660 train_time:36439ms step_avg:91.56ms
step:399/1660 train_time:36533ms step_avg:91.56ms
step:400/1660 train_time:36626ms step_avg:91.56ms
step:401/1660 train_time:36717ms step_avg:91.56ms
step:402/1660 train_time:36810ms step_avg:91.57ms
step:403/1660 train_time:36901ms step_avg:91.57ms
step:404/1660 train_time:36993ms step_avg:91.57ms
step:405/1660 train_time:37084ms step_avg:91.57ms
step:406/1660 train_time:37174ms step_avg:91.56ms
step:407/1660 train_time:37265ms step_avg:91.56ms
step:408/1660 train_time:37356ms step_avg:91.56ms
step:409/1660 train_time:37448ms step_avg:91.56ms
step:410/1660 train_time:37539ms step_avg:91.56ms
step:411/1660 train_time:37632ms step_avg:91.56ms
step:412/1660 train_time:37724ms step_avg:91.56ms
step:413/1660 train_time:37815ms step_avg:91.56ms
step:414/1660 train_time:37907ms step_avg:91.56ms
step:415/1660 train_time:37998ms step_avg:91.56ms
step:416/1660 train_time:38090ms step_avg:91.56ms
step:417/1660 train_time:38181ms step_avg:91.56ms
step:418/1660 train_time:38272ms step_avg:91.56ms
step:419/1660 train_time:38364ms step_avg:91.56ms
step:420/1660 train_time:38455ms step_avg:91.56ms
step:421/1660 train_time:38547ms step_avg:91.56ms
step:422/1660 train_time:38638ms step_avg:91.56ms
step:423/1660 train_time:38731ms step_avg:91.56ms
step:424/1660 train_time:38824ms step_avg:91.57ms
step:425/1660 train_time:38916ms step_avg:91.57ms
step:426/1660 train_time:39007ms step_avg:91.57ms
step:427/1660 train_time:39098ms step_avg:91.57ms
step:428/1660 train_time:39190ms step_avg:91.57ms
step:429/1660 train_time:39281ms step_avg:91.56ms
step:430/1660 train_time:39371ms step_avg:91.56ms
step:431/1660 train_time:39463ms step_avg:91.56ms
step:432/1660 train_time:39554ms step_avg:91.56ms
step:433/1660 train_time:39645ms step_avg:91.56ms
step:434/1660 train_time:39736ms step_avg:91.56ms
step:435/1660 train_time:39830ms step_avg:91.56ms
step:436/1660 train_time:39922ms step_avg:91.56ms
step:437/1660 train_time:40014ms step_avg:91.56ms
step:438/1660 train_time:40105ms step_avg:91.56ms
step:439/1660 train_time:40196ms step_avg:91.56ms
step:440/1660 train_time:40287ms step_avg:91.56ms
step:441/1660 train_time:40378ms step_avg:91.56ms
step:442/1660 train_time:40469ms step_avg:91.56ms
step:443/1660 train_time:40561ms step_avg:91.56ms
step:444/1660 train_time:40653ms step_avg:91.56ms
step:445/1660 train_time:40744ms step_avg:91.56ms
step:446/1660 train_time:40835ms step_avg:91.56ms
step:447/1660 train_time:40929ms step_avg:91.56ms
step:448/1660 train_time:41020ms step_avg:91.56ms
step:449/1660 train_time:41112ms step_avg:91.56ms
step:450/1660 train_time:41203ms step_avg:91.56ms
step:451/1660 train_time:41295ms step_avg:91.56ms
step:452/1660 train_time:41386ms step_avg:91.56ms
step:453/1660 train_time:41477ms step_avg:91.56ms
step:454/1660 train_time:41569ms step_avg:91.56ms
step:455/1660 train_time:41660ms step_avg:91.56ms
step:456/1660 train_time:41752ms step_avg:91.56ms
step:457/1660 train_time:41844ms step_avg:91.56ms
step:458/1660 train_time:41935ms step_avg:91.56ms
step:459/1660 train_time:42027ms step_avg:91.56ms
step:460/1660 train_time:42120ms step_avg:91.56ms
step:461/1660 train_time:42212ms step_avg:91.57ms
step:462/1660 train_time:42303ms step_avg:91.57ms
step:463/1660 train_time:42395ms step_avg:91.57ms
step:464/1660 train_time:42486ms step_avg:91.56ms
step:465/1660 train_time:42576ms step_avg:91.56ms
step:466/1660 train_time:42667ms step_avg:91.56ms
step:467/1660 train_time:42758ms step_avg:91.56ms
step:468/1660 train_time:42850ms step_avg:91.56ms
step:469/1660 train_time:42942ms step_avg:91.56ms
step:470/1660 train_time:43034ms step_avg:91.56ms
step:471/1660 train_time:43126ms step_avg:91.56ms
step:472/1660 train_time:43217ms step_avg:91.56ms
step:473/1660 train_time:43309ms step_avg:91.56ms
step:474/1660 train_time:43401ms step_avg:91.56ms
step:475/1660 train_time:43491ms step_avg:91.56ms
step:476/1660 train_time:43583ms step_avg:91.56ms
step:477/1660 train_time:43674ms step_avg:91.56ms
step:478/1660 train_time:43765ms step_avg:91.56ms
step:479/1660 train_time:43856ms step_avg:91.56ms
step:480/1660 train_time:43948ms step_avg:91.56ms
step:481/1660 train_time:44040ms step_avg:91.56ms
step:482/1660 train_time:44132ms step_avg:91.56ms
step:483/1660 train_time:44225ms step_avg:91.56ms
step:484/1660 train_time:44315ms step_avg:91.56ms
step:485/1660 train_time:44406ms step_avg:91.56ms
step:486/1660 train_time:44497ms step_avg:91.56ms
step:487/1660 train_time:44588ms step_avg:91.56ms
step:488/1660 train_time:44679ms step_avg:91.56ms
step:489/1660 train_time:44770ms step_avg:91.55ms
step:490/1660 train_time:44862ms step_avg:91.56ms
step:491/1660 train_time:44953ms step_avg:91.55ms
step:492/1660 train_time:45044ms step_avg:91.55ms
step:493/1660 train_time:45135ms step_avg:91.55ms
step:494/1660 train_time:45227ms step_avg:91.55ms
step:495/1660 train_time:45318ms step_avg:91.55ms
step:496/1660 train_time:45410ms step_avg:91.55ms
step:497/1660 train_time:45502ms step_avg:91.55ms
step:498/1660 train_time:45593ms step_avg:91.55ms
step:499/1660 train_time:45684ms step_avg:91.55ms
step:500/1660 train_time:45775ms step_avg:91.55ms
step:500/1660 val_loss:3.7178 train_time:45867ms step_avg:91.73ms
step:501/1660 train_time:45886ms step_avg:91.59ms
step:502/1660 train_time:45960ms step_avg:91.55ms
step:503/1660 train_time:46055ms step_avg:91.56ms
step:504/1660 train_time:46146ms step_avg:91.56ms
step:505/1660 train_time:46237ms step_avg:91.56ms
step:506/1660 train_time:46327ms step_avg:91.56ms
step:507/1660 train_time:46417ms step_avg:91.55ms
step:508/1660 train_time:46508ms step_avg:91.55ms
step:509/1660 train_time:46598ms step_avg:91.55ms
step:510/1660 train_time:46690ms step_avg:91.55ms
step:511/1660 train_time:46781ms step_avg:91.55ms
step:512/1660 train_time:46874ms step_avg:91.55ms
step:513/1660 train_time:46967ms step_avg:91.55ms
step:514/1660 train_time:47060ms step_avg:91.56ms
step:515/1660 train_time:47152ms step_avg:91.56ms
step:516/1660 train_time:47243ms step_avg:91.56ms
step:517/1660 train_time:47334ms step_avg:91.55ms
step:518/1660 train_time:47424ms step_avg:91.55ms
step:519/1660 train_time:47515ms step_avg:91.55ms
step:520/1660 train_time:47605ms step_avg:91.55ms
step:521/1660 train_time:47696ms step_avg:91.55ms
step:522/1660 train_time:47787ms step_avg:91.55ms
step:523/1660 train_time:47880ms step_avg:91.55ms
step:524/1660 train_time:47973ms step_avg:91.55ms
step:525/1660 train_time:48065ms step_avg:91.55ms
step:526/1660 train_time:48157ms step_avg:91.55ms
step:527/1660 train_time:48248ms step_avg:91.55ms
step:528/1660 train_time:48339ms step_avg:91.55ms
step:529/1660 train_time:48430ms step_avg:91.55ms
step:530/1660 train_time:48521ms step_avg:91.55ms
step:531/1660 train_time:48611ms step_avg:91.55ms
step:532/1660 train_time:48702ms step_avg:91.54ms
step:533/1660 train_time:48794ms step_avg:91.55ms
step:534/1660 train_time:48885ms step_avg:91.54ms
step:535/1660 train_time:48977ms step_avg:91.55ms
step:536/1660 train_time:49069ms step_avg:91.55ms
step:537/1660 train_time:49161ms step_avg:91.55ms
step:538/1660 train_time:49253ms step_avg:91.55ms
step:539/1660 train_time:49343ms step_avg:91.55ms
step:540/1660 train_time:49434ms step_avg:91.54ms
step:541/1660 train_time:49524ms step_avg:91.54ms
step:542/1660 train_time:49615ms step_avg:91.54ms
step:543/1660 train_time:49706ms step_avg:91.54ms
step:544/1660 train_time:49797ms step_avg:91.54ms
step:545/1660 train_time:49888ms step_avg:91.54ms
step:546/1660 train_time:49980ms step_avg:91.54ms
step:547/1660 train_time:50072ms step_avg:91.54ms
step:548/1660 train_time:50162ms step_avg:91.54ms
step:549/1660 train_time:50254ms step_avg:91.54ms
step:550/1660 train_time:50346ms step_avg:91.54ms
step:551/1660 train_time:50438ms step_avg:91.54ms
step:552/1660 train_time:50528ms step_avg:91.54ms
step:553/1660 train_time:50619ms step_avg:91.54ms
step:554/1660 train_time:50710ms step_avg:91.54ms
step:555/1660 train_time:50802ms step_avg:91.53ms
step:556/1660 train_time:50895ms step_avg:91.54ms
step:557/1660 train_time:50988ms step_avg:91.54ms
step:558/1660 train_time:51080ms step_avg:91.54ms
step:559/1660 train_time:51174ms step_avg:91.55ms
step:560/1660 train_time:51267ms step_avg:91.55ms
step:561/1660 train_time:51360ms step_avg:91.55ms
step:562/1660 train_time:51454ms step_avg:91.55ms
step:563/1660 train_time:51546ms step_avg:91.56ms
step:564/1660 train_time:51637ms step_avg:91.56ms
step:565/1660 train_time:51730ms step_avg:91.56ms
step:566/1660 train_time:51822ms step_avg:91.56ms
step:567/1660 train_time:51915ms step_avg:91.56ms
step:568/1660 train_time:52009ms step_avg:91.56ms
step:569/1660 train_time:52101ms step_avg:91.57ms
step:570/1660 train_time:52195ms step_avg:91.57ms
step:571/1660 train_time:52288ms step_avg:91.57ms
step:572/1660 train_time:52380ms step_avg:91.57ms
step:573/1660 train_time:52472ms step_avg:91.57ms
step:574/1660 train_time:52564ms step_avg:91.58ms
step:575/1660 train_time:52657ms step_avg:91.58ms
step:576/1660 train_time:52750ms step_avg:91.58ms
step:577/1660 train_time:52842ms step_avg:91.58ms
step:578/1660 train_time:52934ms step_avg:91.58ms
step:579/1660 train_time:53026ms step_avg:91.58ms
step:580/1660 train_time:53119ms step_avg:91.58ms
step:581/1660 train_time:53212ms step_avg:91.59ms
step:582/1660 train_time:53304ms step_avg:91.59ms
step:583/1660 train_time:53398ms step_avg:91.59ms
step:584/1660 train_time:53490ms step_avg:91.59ms
step:585/1660 train_time:53582ms step_avg:91.59ms
step:586/1660 train_time:53676ms step_avg:91.60ms
step:587/1660 train_time:53768ms step_avg:91.60ms
step:588/1660 train_time:53860ms step_avg:91.60ms
step:589/1660 train_time:53953ms step_avg:91.60ms
step:590/1660 train_time:54045ms step_avg:91.60ms
step:591/1660 train_time:54138ms step_avg:91.60ms
step:592/1660 train_time:54231ms step_avg:91.61ms
step:593/1660 train_time:54323ms step_avg:91.61ms
step:594/1660 train_time:54416ms step_avg:91.61ms
step:595/1660 train_time:54510ms step_avg:91.61ms
step:596/1660 train_time:54602ms step_avg:91.61ms
step:597/1660 train_time:54695ms step_avg:91.62ms
step:598/1660 train_time:54787ms step_avg:91.62ms
step:599/1660 train_time:54880ms step_avg:91.62ms
step:600/1660 train_time:54973ms step_avg:91.62ms
step:601/1660 train_time:55065ms step_avg:91.62ms
step:602/1660 train_time:55160ms step_avg:91.63ms
step:603/1660 train_time:55252ms step_avg:91.63ms
step:604/1660 train_time:55344ms step_avg:91.63ms
step:605/1660 train_time:55438ms step_avg:91.63ms
step:606/1660 train_time:55530ms step_avg:91.63ms
step:607/1660 train_time:55622ms step_avg:91.63ms
step:608/1660 train_time:55715ms step_avg:91.64ms
step:609/1660 train_time:55807ms step_avg:91.64ms
step:610/1660 train_time:55900ms step_avg:91.64ms
step:611/1660 train_time:55992ms step_avg:91.64ms
step:612/1660 train_time:56083ms step_avg:91.64ms
step:613/1660 train_time:56177ms step_avg:91.64ms
step:614/1660 train_time:56270ms step_avg:91.65ms
step:615/1660 train_time:56363ms step_avg:91.65ms
step:616/1660 train_time:56456ms step_avg:91.65ms
step:617/1660 train_time:56549ms step_avg:91.65ms
step:618/1660 train_time:56641ms step_avg:91.65ms
step:619/1660 train_time:56734ms step_avg:91.65ms
step:620/1660 train_time:56826ms step_avg:91.66ms
step:621/1660 train_time:56919ms step_avg:91.66ms
step:622/1660 train_time:57012ms step_avg:91.66ms
step:623/1660 train_time:57104ms step_avg:91.66ms
step:624/1660 train_time:57198ms step_avg:91.66ms
step:625/1660 train_time:57291ms step_avg:91.67ms
step:625/1660 val_loss:3.6161 train_time:57385ms step_avg:91.82ms
step:626/1660 train_time:57404ms step_avg:91.70ms
step:627/1660 train_time:57480ms step_avg:91.67ms
step:628/1660 train_time:57577ms step_avg:91.68ms
step:629/1660 train_time:57671ms step_avg:91.69ms
step:630/1660 train_time:57763ms step_avg:91.69ms
step:631/1660 train_time:57854ms step_avg:91.69ms
step:632/1660 train_time:57945ms step_avg:91.68ms
step:633/1660 train_time:58035ms step_avg:91.68ms
step:634/1660 train_time:58126ms step_avg:91.68ms
step:635/1660 train_time:58217ms step_avg:91.68ms
step:636/1660 train_time:58311ms step_avg:91.68ms
step:637/1660 train_time:58407ms step_avg:91.69ms
step:638/1660 train_time:58502ms step_avg:91.70ms
step:639/1660 train_time:58597ms step_avg:91.70ms
step:640/1660 train_time:58691ms step_avg:91.70ms
step:641/1660 train_time:58784ms step_avg:91.71ms
step:642/1660 train_time:58876ms step_avg:91.71ms
step:643/1660 train_time:58968ms step_avg:91.71ms
step:644/1660 train_time:59059ms step_avg:91.71ms
step:645/1660 train_time:59150ms step_avg:91.71ms
step:646/1660 train_time:59241ms step_avg:91.70ms
step:647/1660 train_time:59335ms step_avg:91.71ms
step:648/1660 train_time:59430ms step_avg:91.71ms
step:649/1660 train_time:59524ms step_avg:91.72ms
step:650/1660 train_time:59616ms step_avg:91.72ms
step:651/1660 train_time:59710ms step_avg:91.72ms
step:652/1660 train_time:59803ms step_avg:91.72ms
step:653/1660 train_time:59895ms step_avg:91.72ms
step:654/1660 train_time:59988ms step_avg:91.72ms
step:655/1660 train_time:60079ms step_avg:91.72ms
step:656/1660 train_time:60170ms step_avg:91.72ms
step:657/1660 train_time:60262ms step_avg:91.72ms
step:658/1660 train_time:60355ms step_avg:91.73ms
step:659/1660 train_time:60449ms step_avg:91.73ms
step:660/1660 train_time:60542ms step_avg:91.73ms
step:661/1660 train_time:60635ms step_avg:91.73ms
step:662/1660 train_time:60730ms step_avg:91.74ms
step:663/1660 train_time:60823ms step_avg:91.74ms
step:664/1660 train_time:60915ms step_avg:91.74ms
step:665/1660 train_time:61007ms step_avg:91.74ms
step:666/1660 train_time:61098ms step_avg:91.74ms
step:667/1660 train_time:61191ms step_avg:91.74ms
step:668/1660 train_time:61283ms step_avg:91.74ms
step:669/1660 train_time:61375ms step_avg:91.74ms
step:670/1660 train_time:61468ms step_avg:91.74ms
step:671/1660 train_time:61561ms step_avg:91.74ms
step:672/1660 train_time:61655ms step_avg:91.75ms
step:673/1660 train_time:61749ms step_avg:91.75ms
step:674/1660 train_time:61841ms step_avg:91.75ms
step:675/1660 train_time:61934ms step_avg:91.75ms
step:676/1660 train_time:62026ms step_avg:91.75ms
step:677/1660 train_time:62118ms step_avg:91.75ms
step:678/1660 train_time:62210ms step_avg:91.76ms
step:679/1660 train_time:62303ms step_avg:91.76ms
step:680/1660 train_time:62395ms step_avg:91.76ms
step:681/1660 train_time:62488ms step_avg:91.76ms
step:682/1660 train_time:62582ms step_avg:91.76ms
step:683/1660 train_time:62674ms step_avg:91.76ms
step:684/1660 train_time:62767ms step_avg:91.76ms
step:685/1660 train_time:62859ms step_avg:91.77ms
step:686/1660 train_time:62952ms step_avg:91.77ms
step:687/1660 train_time:63045ms step_avg:91.77ms
step:688/1660 train_time:63136ms step_avg:91.77ms
step:689/1660 train_time:63231ms step_avg:91.77ms
step:690/1660 train_time:63323ms step_avg:91.77ms
step:691/1660 train_time:63414ms step_avg:91.77ms
step:692/1660 train_time:63507ms step_avg:91.77ms
step:693/1660 train_time:63600ms step_avg:91.77ms
step:694/1660 train_time:63694ms step_avg:91.78ms
step:695/1660 train_time:63787ms step_avg:91.78ms
step:696/1660 train_time:63879ms step_avg:91.78ms
step:697/1660 train_time:63971ms step_avg:91.78ms
step:698/1660 train_time:64064ms step_avg:91.78ms
step:699/1660 train_time:64156ms step_avg:91.78ms
step:700/1660 train_time:64250ms step_avg:91.79ms
step:701/1660 train_time:64342ms step_avg:91.79ms
step:702/1660 train_time:64435ms step_avg:91.79ms
step:703/1660 train_time:64528ms step_avg:91.79ms
step:704/1660 train_time:64620ms step_avg:91.79ms
step:705/1660 train_time:64713ms step_avg:91.79ms
step:706/1660 train_time:64805ms step_avg:91.79ms
step:707/1660 train_time:64897ms step_avg:91.79ms
step:708/1660 train_time:64991ms step_avg:91.79ms
step:709/1660 train_time:65084ms step_avg:91.80ms
step:710/1660 train_time:65175ms step_avg:91.80ms
step:711/1660 train_time:65268ms step_avg:91.80ms
step:712/1660 train_time:65360ms step_avg:91.80ms
step:713/1660 train_time:65453ms step_avg:91.80ms
step:714/1660 train_time:65545ms step_avg:91.80ms
step:715/1660 train_time:65638ms step_avg:91.80ms
step:716/1660 train_time:65732ms step_avg:91.80ms
step:717/1660 train_time:65824ms step_avg:91.81ms
step:718/1660 train_time:65916ms step_avg:91.81ms
step:719/1660 train_time:66009ms step_avg:91.81ms
step:720/1660 train_time:66101ms step_avg:91.81ms
step:721/1660 train_time:66193ms step_avg:91.81ms
step:722/1660 train_time:66286ms step_avg:91.81ms
step:723/1660 train_time:66377ms step_avg:91.81ms
step:724/1660 train_time:66470ms step_avg:91.81ms
step:725/1660 train_time:66562ms step_avg:91.81ms
step:726/1660 train_time:66654ms step_avg:91.81ms
step:727/1660 train_time:66746ms step_avg:91.81ms
step:728/1660 train_time:66837ms step_avg:91.81ms
step:729/1660 train_time:66931ms step_avg:91.81ms
step:730/1660 train_time:67023ms step_avg:91.81ms
step:731/1660 train_time:67115ms step_avg:91.81ms
step:732/1660 train_time:67208ms step_avg:91.81ms
step:733/1660 train_time:67301ms step_avg:91.82ms
step:734/1660 train_time:67393ms step_avg:91.82ms
step:735/1660 train_time:67486ms step_avg:91.82ms
step:736/1660 train_time:67578ms step_avg:91.82ms
step:737/1660 train_time:67671ms step_avg:91.82ms
step:738/1660 train_time:67763ms step_avg:91.82ms
step:739/1660 train_time:67856ms step_avg:91.82ms
step:740/1660 train_time:67949ms step_avg:91.82ms
step:741/1660 train_time:68041ms step_avg:91.82ms
step:742/1660 train_time:68134ms step_avg:91.82ms
step:743/1660 train_time:68227ms step_avg:91.83ms
step:744/1660 train_time:68318ms step_avg:91.83ms
step:745/1660 train_time:68411ms step_avg:91.83ms
step:746/1660 train_time:68503ms step_avg:91.83ms
step:747/1660 train_time:68595ms step_avg:91.83ms
step:748/1660 train_time:68689ms step_avg:91.83ms
step:749/1660 train_time:68780ms step_avg:91.83ms
step:750/1660 train_time:68873ms step_avg:91.83ms
step:750/1660 val_loss:3.5652 train_time:68968ms step_avg:91.96ms
step:751/1660 train_time:68987ms step_avg:91.86ms
step:752/1660 train_time:69063ms step_avg:91.84ms
step:753/1660 train_time:69159ms step_avg:91.84ms
step:754/1660 train_time:69252ms step_avg:91.85ms
step:755/1660 train_time:69344ms step_avg:91.85ms
step:756/1660 train_time:69435ms step_avg:91.84ms
step:757/1660 train_time:69526ms step_avg:91.84ms
step:758/1660 train_time:69618ms step_avg:91.84ms
step:759/1660 train_time:69710ms step_avg:91.84ms
step:760/1660 train_time:69803ms step_avg:91.85ms
step:761/1660 train_time:69896ms step_avg:91.85ms
step:762/1660 train_time:69990ms step_avg:91.85ms
step:763/1660 train_time:70085ms step_avg:91.85ms
step:764/1660 train_time:70177ms step_avg:91.86ms
step:765/1660 train_time:70272ms step_avg:91.86ms
step:766/1660 train_time:70364ms step_avg:91.86ms
step:767/1660 train_time:70456ms step_avg:91.86ms
step:768/1660 train_time:70548ms step_avg:91.86ms
step:769/1660 train_time:70639ms step_avg:91.86ms
step:770/1660 train_time:70732ms step_avg:91.86ms
step:771/1660 train_time:70825ms step_avg:91.86ms
step:772/1660 train_time:70917ms step_avg:91.86ms
step:773/1660 train_time:71011ms step_avg:91.86ms
step:774/1660 train_time:71105ms step_avg:91.87ms
step:775/1660 train_time:71198ms step_avg:91.87ms
step:776/1660 train_time:71291ms step_avg:91.87ms
step:777/1660 train_time:71383ms step_avg:91.87ms
step:778/1660 train_time:71475ms step_avg:91.87ms
step:779/1660 train_time:71568ms step_avg:91.87ms
step:780/1660 train_time:71660ms step_avg:91.87ms
step:781/1660 train_time:71752ms step_avg:91.87ms
step:782/1660 train_time:71845ms step_avg:91.87ms
step:783/1660 train_time:71938ms step_avg:91.87ms
step:784/1660 train_time:72032ms step_avg:91.88ms
step:785/1660 train_time:72125ms step_avg:91.88ms
step:786/1660 train_time:72218ms step_avg:91.88ms
step:787/1660 train_time:72311ms step_avg:91.88ms
step:788/1660 train_time:72404ms step_avg:91.88ms
step:789/1660 train_time:72496ms step_avg:91.88ms
step:790/1660 train_time:72589ms step_avg:91.88ms
step:791/1660 train_time:72681ms step_avg:91.89ms
step:792/1660 train_time:72772ms step_avg:91.88ms
step:793/1660 train_time:72866ms step_avg:91.89ms
step:794/1660 train_time:72958ms step_avg:91.89ms
step:795/1660 train_time:73051ms step_avg:91.89ms
step:796/1660 train_time:73145ms step_avg:91.89ms
step:797/1660 train_time:73237ms step_avg:91.89ms
step:798/1660 train_time:73331ms step_avg:91.89ms
step:799/1660 train_time:73424ms step_avg:91.89ms
step:800/1660 train_time:73515ms step_avg:91.89ms
step:801/1660 train_time:73607ms step_avg:91.89ms
step:802/1660 train_time:73700ms step_avg:91.89ms
step:803/1660 train_time:73791ms step_avg:91.89ms
step:804/1660 train_time:73884ms step_avg:91.90ms
step:805/1660 train_time:73976ms step_avg:91.90ms
step:806/1660 train_time:74070ms step_avg:91.90ms
step:807/1660 train_time:74162ms step_avg:91.90ms
step:808/1660 train_time:74254ms step_avg:91.90ms
step:809/1660 train_time:74348ms step_avg:91.90ms
step:810/1660 train_time:74442ms step_avg:91.90ms
step:811/1660 train_time:74534ms step_avg:91.90ms
step:812/1660 train_time:74627ms step_avg:91.90ms
step:813/1660 train_time:74719ms step_avg:91.91ms
step:814/1660 train_time:74811ms step_avg:91.90ms
step:815/1660 train_time:74903ms step_avg:91.91ms
step:816/1660 train_time:74995ms step_avg:91.91ms
step:817/1660 train_time:75088ms step_avg:91.91ms
step:818/1660 train_time:75181ms step_avg:91.91ms
step:819/1660 train_time:75274ms step_avg:91.91ms
step:820/1660 train_time:75368ms step_avg:91.91ms
step:821/1660 train_time:75461ms step_avg:91.91ms
step:822/1660 train_time:75553ms step_avg:91.91ms
step:823/1660 train_time:75647ms step_avg:91.92ms
step:824/1660 train_time:75739ms step_avg:91.92ms
step:825/1660 train_time:75831ms step_avg:91.92ms
step:826/1660 train_time:75923ms step_avg:91.92ms
step:827/1660 train_time:76015ms step_avg:91.92ms
step:828/1660 train_time:76107ms step_avg:91.92ms
step:829/1660 train_time:76199ms step_avg:91.92ms
step:830/1660 train_time:76291ms step_avg:91.92ms
step:831/1660 train_time:76384ms step_avg:91.92ms
step:832/1660 train_time:76476ms step_avg:91.92ms
step:833/1660 train_time:76569ms step_avg:91.92ms
step:834/1660 train_time:76662ms step_avg:91.92ms
step:835/1660 train_time:76754ms step_avg:91.92ms
step:836/1660 train_time:76847ms step_avg:91.92ms
step:837/1660 train_time:76940ms step_avg:91.92ms
step:838/1660 train_time:77032ms step_avg:91.92ms
step:839/1660 train_time:77124ms step_avg:91.92ms
step:840/1660 train_time:77216ms step_avg:91.92ms
step:841/1660 train_time:77310ms step_avg:91.93ms
step:842/1660 train_time:77403ms step_avg:91.93ms
step:843/1660 train_time:77494ms step_avg:91.93ms
step:844/1660 train_time:77587ms step_avg:91.93ms
step:845/1660 train_time:77680ms step_avg:91.93ms
step:846/1660 train_time:77773ms step_avg:91.93ms
step:847/1660 train_time:77866ms step_avg:91.93ms
step:848/1660 train_time:77960ms step_avg:91.93ms
step:849/1660 train_time:78052ms step_avg:91.93ms
step:850/1660 train_time:78144ms step_avg:91.93ms
step:851/1660 train_time:78235ms step_avg:91.93ms
step:852/1660 train_time:78329ms step_avg:91.94ms
step:853/1660 train_time:78423ms step_avg:91.94ms
step:854/1660 train_time:78514ms step_avg:91.94ms
step:855/1660 train_time:78608ms step_avg:91.94ms
step:856/1660 train_time:78700ms step_avg:91.94ms
step:857/1660 train_time:78793ms step_avg:91.94ms
step:858/1660 train_time:78887ms step_avg:91.94ms
step:859/1660 train_time:78980ms step_avg:91.94ms
step:860/1660 train_time:79073ms step_avg:91.94ms
step:861/1660 train_time:79166ms step_avg:91.95ms
step:862/1660 train_time:79257ms step_avg:91.95ms
step:863/1660 train_time:79351ms step_avg:91.95ms
step:864/1660 train_time:79444ms step_avg:91.95ms
step:865/1660 train_time:79536ms step_avg:91.95ms
step:866/1660 train_time:79630ms step_avg:91.95ms
step:867/1660 train_time:79723ms step_avg:91.95ms
step:868/1660 train_time:79815ms step_avg:91.95ms
step:869/1660 train_time:79908ms step_avg:91.95ms
step:870/1660 train_time:80000ms step_avg:91.95ms
step:871/1660 train_time:80092ms step_avg:91.95ms
step:872/1660 train_time:80184ms step_avg:91.95ms
step:873/1660 train_time:80276ms step_avg:91.95ms
step:874/1660 train_time:80369ms step_avg:91.96ms
step:875/1660 train_time:80462ms step_avg:91.96ms
step:875/1660 val_loss:3.5191 train_time:80555ms step_avg:92.06ms
step:876/1660 train_time:80575ms step_avg:91.98ms
step:877/1660 train_time:80650ms step_avg:91.96ms
step:878/1660 train_time:80751ms step_avg:91.97ms
step:879/1660 train_time:80844ms step_avg:91.97ms
step:880/1660 train_time:80935ms step_avg:91.97ms
step:881/1660 train_time:81027ms step_avg:91.97ms
step:882/1660 train_time:81118ms step_avg:91.97ms
step:883/1660 train_time:81209ms step_avg:91.97ms
step:884/1660 train_time:81301ms step_avg:91.97ms
step:885/1660 train_time:81392ms step_avg:91.97ms
step:886/1660 train_time:81485ms step_avg:91.97ms
step:887/1660 train_time:81579ms step_avg:91.97ms
step:888/1660 train_time:81674ms step_avg:91.98ms
step:889/1660 train_time:81770ms step_avg:91.98ms
step:890/1660 train_time:81864ms step_avg:91.98ms
step:891/1660 train_time:81956ms step_avg:91.98ms
step:892/1660 train_time:82048ms step_avg:91.98ms
step:893/1660 train_time:82140ms step_avg:91.98ms
step:894/1660 train_time:82231ms step_avg:91.98ms
step:895/1660 train_time:82323ms step_avg:91.98ms
step:896/1660 train_time:82414ms step_avg:91.98ms
step:897/1660 train_time:82508ms step_avg:91.98ms
step:898/1660 train_time:82602ms step_avg:91.98ms
step:899/1660 train_time:82695ms step_avg:91.99ms
step:900/1660 train_time:82788ms step_avg:91.99ms
step:901/1660 train_time:82882ms step_avg:91.99ms
step:902/1660 train_time:82974ms step_avg:91.99ms
step:903/1660 train_time:83067ms step_avg:91.99ms
step:904/1660 train_time:83159ms step_avg:91.99ms
step:905/1660 train_time:83251ms step_avg:91.99ms
step:906/1660 train_time:83342ms step_avg:91.99ms
step:907/1660 train_time:83434ms step_avg:91.99ms
step:908/1660 train_time:83528ms step_avg:91.99ms
step:909/1660 train_time:83621ms step_avg:91.99ms
step:910/1660 train_time:83714ms step_avg:91.99ms
step:911/1660 train_time:83809ms step_avg:92.00ms
step:912/1660 train_time:83904ms step_avg:92.00ms
step:913/1660 train_time:83997ms step_avg:92.00ms
step:914/1660 train_time:84089ms step_avg:92.00ms
step:915/1660 train_time:84181ms step_avg:92.00ms
step:916/1660 train_time:84272ms step_avg:92.00ms
step:917/1660 train_time:84365ms step_avg:92.00ms
step:918/1660 train_time:84458ms step_avg:92.00ms
step:919/1660 train_time:84550ms step_avg:92.00ms
step:920/1660 train_time:84643ms step_avg:92.00ms
step:921/1660 train_time:84736ms step_avg:92.00ms
step:922/1660 train_time:84829ms step_avg:92.01ms
step:923/1660 train_time:84923ms step_avg:92.01ms
step:924/1660 train_time:85015ms step_avg:92.01ms
step:925/1660 train_time:85107ms step_avg:92.01ms
step:926/1660 train_time:85199ms step_avg:92.01ms
step:927/1660 train_time:85291ms step_avg:92.01ms
step:928/1660 train_time:85383ms step_avg:92.01ms
step:929/1660 train_time:85474ms step_avg:92.01ms
step:930/1660 train_time:85568ms step_avg:92.01ms
step:931/1660 train_time:85661ms step_avg:92.01ms
step:932/1660 train_time:85754ms step_avg:92.01ms
step:933/1660 train_time:85848ms step_avg:92.01ms
step:934/1660 train_time:85941ms step_avg:92.01ms
step:935/1660 train_time:86033ms step_avg:92.01ms
step:936/1660 train_time:86127ms step_avg:92.02ms
step:937/1660 train_time:86220ms step_avg:92.02ms
step:938/1660 train_time:86311ms step_avg:92.02ms
step:939/1660 train_time:86404ms step_avg:92.02ms
step:940/1660 train_time:86496ms step_avg:92.02ms
step:941/1660 train_time:86589ms step_avg:92.02ms
step:942/1660 train_time:86681ms step_avg:92.02ms
step:943/1660 train_time:86774ms step_avg:92.02ms
step:944/1660 train_time:86867ms step_avg:92.02ms
step:945/1660 train_time:86960ms step_avg:92.02ms
step:946/1660 train_time:87052ms step_avg:92.02ms
step:947/1660 train_time:87145ms step_avg:92.02ms
step:948/1660 train_time:87238ms step_avg:92.02ms
step:949/1660 train_time:87330ms step_avg:92.02ms
step:950/1660 train_time:87422ms step_avg:92.02ms
step:951/1660 train_time:87514ms step_avg:92.02ms
step:952/1660 train_time:87608ms step_avg:92.03ms
step:953/1660 train_time:87701ms step_avg:92.03ms
step:954/1660 train_time:87793ms step_avg:92.03ms
step:955/1660 train_time:87886ms step_avg:92.03ms
step:956/1660 train_time:87980ms step_avg:92.03ms
step:957/1660 train_time:88072ms step_avg:92.03ms
step:958/1660 train_time:88165ms step_avg:92.03ms
step:959/1660 train_time:88257ms step_avg:92.03ms
step:960/1660 train_time:88349ms step_avg:92.03ms
step:961/1660 train_time:88441ms step_avg:92.03ms
step:962/1660 train_time:88534ms step_avg:92.03ms
step:963/1660 train_time:88627ms step_avg:92.03ms
step:964/1660 train_time:88720ms step_avg:92.03ms
step:965/1660 train_time:88812ms step_avg:92.03ms
step:966/1660 train_time:88905ms step_avg:92.03ms
step:967/1660 train_time:88998ms step_avg:92.04ms
step:968/1660 train_time:89090ms step_avg:92.04ms
step:969/1660 train_time:89182ms step_avg:92.04ms
step:970/1660 train_time:89275ms step_avg:92.04ms
step:971/1660 train_time:89366ms step_avg:92.04ms
step:972/1660 train_time:89459ms step_avg:92.04ms
step:973/1660 train_time:89551ms step_avg:92.04ms
step:974/1660 train_time:89644ms step_avg:92.04ms
step:975/1660 train_time:89737ms step_avg:92.04ms
step:976/1660 train_time:89829ms step_avg:92.04ms
step:977/1660 train_time:89921ms step_avg:92.04ms
step:978/1660 train_time:90013ms step_avg:92.04ms
step:979/1660 train_time:90106ms step_avg:92.04ms
step:980/1660 train_time:90199ms step_avg:92.04ms
step:981/1660 train_time:90291ms step_avg:92.04ms
step:982/1660 train_time:90384ms step_avg:92.04ms
step:983/1660 train_time:90476ms step_avg:92.04ms
step:984/1660 train_time:90569ms step_avg:92.04ms
step:985/1660 train_time:90662ms step_avg:92.04ms
step:986/1660 train_time:90753ms step_avg:92.04ms
step:987/1660 train_time:90845ms step_avg:92.04ms
step:988/1660 train_time:90939ms step_avg:92.04ms
step:989/1660 train_time:91031ms step_avg:92.04ms
step:990/1660 train_time:91124ms step_avg:92.04ms
step:991/1660 train_time:91216ms step_avg:92.04ms
step:992/1660 train_time:91309ms step_avg:92.05ms
step:993/1660 train_time:91402ms step_avg:92.05ms
step:994/1660 train_time:91494ms step_avg:92.05ms
step:995/1660 train_time:91585ms step_avg:92.05ms
step:996/1660 train_time:91678ms step_avg:92.05ms
step:997/1660 train_time:91771ms step_avg:92.05ms
step:998/1660 train_time:91864ms step_avg:92.05ms
step:999/1660 train_time:91957ms step_avg:92.05ms
step:1000/1660 train_time:92050ms step_avg:92.05ms
step:1000/1660 val_loss:3.4692 train_time:92144ms step_avg:92.14ms
step:1001/1660 train_time:92164ms step_avg:92.07ms
step:1002/1660 train_time:92239ms step_avg:92.06ms
step:1003/1660 train_time:92335ms step_avg:92.06ms
step:1004/1660 train_time:92428ms step_avg:92.06ms
step:1005/1660 train_time:92520ms step_avg:92.06ms
step:1006/1660 train_time:92612ms step_avg:92.06ms
step:1007/1660 train_time:92703ms step_avg:92.06ms
step:1008/1660 train_time:92794ms step_avg:92.06ms
step:1009/1660 train_time:92885ms step_avg:92.06ms
step:1010/1660 train_time:92977ms step_avg:92.06ms
step:1011/1660 train_time:93069ms step_avg:92.06ms
step:1012/1660 train_time:93164ms step_avg:92.06ms
step:1013/1660 train_time:93258ms step_avg:92.06ms
step:1014/1660 train_time:93352ms step_avg:92.06ms
step:1015/1660 train_time:93448ms step_avg:92.07ms
step:1016/1660 train_time:93542ms step_avg:92.07ms
step:1017/1660 train_time:93634ms step_avg:92.07ms
step:1018/1660 train_time:93726ms step_avg:92.07ms
step:1019/1660 train_time:93818ms step_avg:92.07ms
step:1020/1660 train_time:93909ms step_avg:92.07ms
step:1021/1660 train_time:94001ms step_avg:92.07ms
step:1022/1660 train_time:94094ms step_avg:92.07ms
step:1023/1660 train_time:94188ms step_avg:92.07ms
step:1024/1660 train_time:94282ms step_avg:92.07ms
step:1025/1660 train_time:94375ms step_avg:92.07ms
step:1026/1660 train_time:94469ms step_avg:92.08ms
step:1027/1660 train_time:94562ms step_avg:92.08ms
step:1028/1660 train_time:94653ms step_avg:92.07ms
step:1029/1660 train_time:94746ms step_avg:92.08ms
step:1030/1660 train_time:94838ms step_avg:92.08ms
step:1031/1660 train_time:94929ms step_avg:92.07ms
step:1032/1660 train_time:95022ms step_avg:92.08ms
step:1033/1660 train_time:95114ms step_avg:92.08ms
step:1034/1660 train_time:95209ms step_avg:92.08ms
step:1035/1660 train_time:95303ms step_avg:92.08ms
step:1036/1660 train_time:95395ms step_avg:92.08ms
step:1037/1660 train_time:95488ms step_avg:92.08ms
step:1038/1660 train_time:95581ms step_avg:92.08ms
step:1039/1660 train_time:95673ms step_avg:92.08ms
step:1040/1660 train_time:95766ms step_avg:92.08ms
step:1041/1660 train_time:95858ms step_avg:92.08ms
step:1042/1660 train_time:95950ms step_avg:92.08ms
step:1043/1660 train_time:96043ms step_avg:92.08ms
step:1044/1660 train_time:96135ms step_avg:92.08ms
step:1045/1660 train_time:96230ms step_avg:92.09ms
step:1046/1660 train_time:96323ms step_avg:92.09ms
step:1047/1660 train_time:96415ms step_avg:92.09ms
step:1048/1660 train_time:96508ms step_avg:92.09ms
step:1049/1660 train_time:96601ms step_avg:92.09ms
step:1050/1660 train_time:96693ms step_avg:92.09ms
step:1051/1660 train_time:96786ms step_avg:92.09ms
step:1052/1660 train_time:96878ms step_avg:92.09ms
step:1053/1660 train_time:96970ms step_avg:92.09ms
step:1054/1660 train_time:97062ms step_avg:92.09ms
step:1055/1660 train_time:97156ms step_avg:92.09ms
step:1056/1660 train_time:97250ms step_avg:92.09ms
step:1057/1660 train_time:97344ms step_avg:92.09ms
step:1058/1660 train_time:97437ms step_avg:92.10ms
step:1059/1660 train_time:97529ms step_avg:92.10ms
step:1060/1660 train_time:97621ms step_avg:92.10ms
step:1061/1660 train_time:97713ms step_avg:92.10ms
step:1062/1660 train_time:97808ms step_avg:92.10ms
step:1063/1660 train_time:97900ms step_avg:92.10ms
step:1064/1660 train_time:97992ms step_avg:92.10ms
step:1065/1660 train_time:98084ms step_avg:92.10ms
step:1066/1660 train_time:98177ms step_avg:92.10ms
step:1067/1660 train_time:98269ms step_avg:92.10ms
step:1068/1660 train_time:98362ms step_avg:92.10ms
step:1069/1660 train_time:98455ms step_avg:92.10ms
step:1070/1660 train_time:98548ms step_avg:92.10ms
step:1071/1660 train_time:98641ms step_avg:92.10ms
step:1072/1660 train_time:98733ms step_avg:92.10ms
step:1073/1660 train_time:98826ms step_avg:92.10ms
step:1074/1660 train_time:98919ms step_avg:92.10ms
step:1075/1660 train_time:99012ms step_avg:92.10ms
step:1076/1660 train_time:99104ms step_avg:92.10ms
step:1077/1660 train_time:99196ms step_avg:92.10ms
step:1078/1660 train_time:99288ms step_avg:92.10ms
step:1079/1660 train_time:99381ms step_avg:92.10ms
step:1080/1660 train_time:99474ms step_avg:92.11ms
step:1081/1660 train_time:99567ms step_avg:92.11ms
step:1082/1660 train_time:99660ms step_avg:92.11ms
step:1083/1660 train_time:99752ms step_avg:92.11ms
step:1084/1660 train_time:99846ms step_avg:92.11ms
step:1085/1660 train_time:99938ms step_avg:92.11ms
step:1086/1660 train_time:100031ms step_avg:92.11ms
step:1087/1660 train_time:100123ms step_avg:92.11ms
step:1088/1660 train_time:100215ms step_avg:92.11ms
step:1089/1660 train_time:100308ms step_avg:92.11ms
step:1090/1660 train_time:100401ms step_avg:92.11ms
step:1091/1660 train_time:100494ms step_avg:92.11ms
step:1092/1660 train_time:100587ms step_avg:92.11ms
step:1093/1660 train_time:100680ms step_avg:92.11ms
step:1094/1660 train_time:100772ms step_avg:92.11ms
step:1095/1660 train_time:100865ms step_avg:92.11ms
step:1096/1660 train_time:100958ms step_avg:92.12ms
step:1097/1660 train_time:101050ms step_avg:92.12ms
step:1098/1660 train_time:101143ms step_avg:92.12ms
step:1099/1660 train_time:101235ms step_avg:92.12ms
step:1100/1660 train_time:101328ms step_avg:92.12ms
step:1101/1660 train_time:101421ms step_avg:92.12ms
step:1102/1660 train_time:101513ms step_avg:92.12ms
step:1103/1660 train_time:101608ms step_avg:92.12ms
step:1104/1660 train_time:101701ms step_avg:92.12ms
step:1105/1660 train_time:101793ms step_avg:92.12ms
step:1106/1660 train_time:101886ms step_avg:92.12ms
step:1107/1660 train_time:101979ms step_avg:92.12ms
step:1108/1660 train_time:102072ms step_avg:92.12ms
step:1109/1660 train_time:102166ms step_avg:92.12ms
step:1110/1660 train_time:102259ms step_avg:92.13ms
step:1111/1660 train_time:102353ms step_avg:92.13ms
step:1112/1660 train_time:102447ms step_avg:92.13ms
step:1113/1660 train_time:102541ms step_avg:92.13ms
step:1114/1660 train_time:102633ms step_avg:92.13ms
step:1115/1660 train_time:102728ms step_avg:92.13ms
step:1116/1660 train_time:102822ms step_avg:92.13ms
step:1117/1660 train_time:102914ms step_avg:92.13ms
step:1118/1660 train_time:103008ms step_avg:92.14ms
step:1119/1660 train_time:103101ms step_avg:92.14ms
step:1120/1660 train_time:103194ms step_avg:92.14ms
step:1121/1660 train_time:103288ms step_avg:92.14ms
step:1122/1660 train_time:103381ms step_avg:92.14ms
step:1123/1660 train_time:103474ms step_avg:92.14ms
step:1124/1660 train_time:103568ms step_avg:92.14ms
step:1125/1660 train_time:103661ms step_avg:92.14ms
step:1125/1660 val_loss:3.4165 train_time:103755ms step_avg:92.23ms
step:1126/1660 train_time:103775ms step_avg:92.16ms
step:1127/1660 train_time:103851ms step_avg:92.15ms
step:1128/1660 train_time:103955ms step_avg:92.16ms
step:1129/1660 train_time:104050ms step_avg:92.16ms
step:1130/1660 train_time:104143ms step_avg:92.16ms
step:1131/1660 train_time:104235ms step_avg:92.16ms
step:1132/1660 train_time:104327ms step_avg:92.16ms
step:1133/1660 train_time:104420ms step_avg:92.16ms
step:1134/1660 train_time:104513ms step_avg:92.16ms
step:1135/1660 train_time:104605ms step_avg:92.16ms
step:1136/1660 train_time:104698ms step_avg:92.16ms
step:1137/1660 train_time:104792ms step_avg:92.17ms
step:1138/1660 train_time:104890ms step_avg:92.17ms
step:1139/1660 train_time:104986ms step_avg:92.17ms
step:1140/1660 train_time:105081ms step_avg:92.18ms
step:1141/1660 train_time:105173ms step_avg:92.18ms
step:1142/1660 train_time:105266ms step_avg:92.18ms
step:1143/1660 train_time:105358ms step_avg:92.18ms
step:1144/1660 train_time:105451ms step_avg:92.18ms
step:1145/1660 train_time:105544ms step_avg:92.18ms
step:1146/1660 train_time:105636ms step_avg:92.18ms
step:1147/1660 train_time:105729ms step_avg:92.18ms
step:1148/1660 train_time:105823ms step_avg:92.18ms
step:1149/1660 train_time:105917ms step_avg:92.18ms
step:1150/1660 train_time:106013ms step_avg:92.19ms
step:1151/1660 train_time:106108ms step_avg:92.19ms
step:1152/1660 train_time:106201ms step_avg:92.19ms
step:1153/1660 train_time:106293ms step_avg:92.19ms
step:1154/1660 train_time:106385ms step_avg:92.19ms
step:1155/1660 train_time:106477ms step_avg:92.19ms
step:1156/1660 train_time:106569ms step_avg:92.19ms
step:1157/1660 train_time:106662ms step_avg:92.19ms
step:1158/1660 train_time:106756ms step_avg:92.19ms
step:1159/1660 train_time:106851ms step_avg:92.19ms
step:1160/1660 train_time:106945ms step_avg:92.19ms
step:1161/1660 train_time:107039ms step_avg:92.20ms
step:1162/1660 train_time:107134ms step_avg:92.20ms
step:1163/1660 train_time:107227ms step_avg:92.20ms
step:1164/1660 train_time:107320ms step_avg:92.20ms
step:1165/1660 train_time:107412ms step_avg:92.20ms
step:1166/1660 train_time:107505ms step_avg:92.20ms
step:1167/1660 train_time:107597ms step_avg:92.20ms
step:1168/1660 train_time:107690ms step_avg:92.20ms
step:1169/1660 train_time:107783ms step_avg:92.20ms
step:1170/1660 train_time:107876ms step_avg:92.20ms
step:1171/1660 train_time:107970ms step_avg:92.20ms
step:1172/1660 train_time:108064ms step_avg:92.20ms
step:1173/1660 train_time:108156ms step_avg:92.20ms
step:1174/1660 train_time:108249ms step_avg:92.21ms
step:1175/1660 train_time:108342ms step_avg:92.21ms
step:1176/1660 train_time:108435ms step_avg:92.21ms
step:1177/1660 train_time:108528ms step_avg:92.21ms
step:1178/1660 train_time:108621ms step_avg:92.21ms
step:1179/1660 train_time:108714ms step_avg:92.21ms
step:1180/1660 train_time:108808ms step_avg:92.21ms
step:1181/1660 train_time:108903ms step_avg:92.21ms
step:1182/1660 train_time:108996ms step_avg:92.21ms
step:1183/1660 train_time:109089ms step_avg:92.21ms
step:1184/1660 train_time:109183ms step_avg:92.22ms
step:1185/1660 train_time:109276ms step_avg:92.22ms
step:1186/1660 train_time:109369ms step_avg:92.22ms
step:1187/1660 train_time:109462ms step_avg:92.22ms
step:1188/1660 train_time:109554ms step_avg:92.22ms
step:1189/1660 train_time:109648ms step_avg:92.22ms
step:1190/1660 train_time:109741ms step_avg:92.22ms
step:1191/1660 train_time:109834ms step_avg:92.22ms
step:1192/1660 train_time:109927ms step_avg:92.22ms
step:1193/1660 train_time:110022ms step_avg:92.22ms
step:1194/1660 train_time:110114ms step_avg:92.22ms
step:1195/1660 train_time:110208ms step_avg:92.22ms
step:1196/1660 train_time:110301ms step_avg:92.23ms
step:1197/1660 train_time:110394ms step_avg:92.23ms
step:1198/1660 train_time:110487ms step_avg:92.23ms
step:1199/1660 train_time:110580ms step_avg:92.23ms
step:1200/1660 train_time:110672ms step_avg:92.23ms
step:1201/1660 train_time:110766ms step_avg:92.23ms
step:1202/1660 train_time:110860ms step_avg:92.23ms
step:1203/1660 train_time:110952ms step_avg:92.23ms
step:1204/1660 train_time:111046ms step_avg:92.23ms
step:1205/1660 train_time:111140ms step_avg:92.23ms
step:1206/1660 train_time:111233ms step_avg:92.23ms
step:1207/1660 train_time:111327ms step_avg:92.23ms
step:1208/1660 train_time:111420ms step_avg:92.23ms
step:1209/1660 train_time:111512ms step_avg:92.24ms
step:1210/1660 train_time:111605ms step_avg:92.24ms
step:1211/1660 train_time:111699ms step_avg:92.24ms
step:1212/1660 train_time:111792ms step_avg:92.24ms
step:1213/1660 train_time:111886ms step_avg:92.24ms
step:1214/1660 train_time:111978ms step_avg:92.24ms
step:1215/1660 train_time:112071ms step_avg:92.24ms
step:1216/1660 train_time:112165ms step_avg:92.24ms
step:1217/1660 train_time:112259ms step_avg:92.24ms
step:1218/1660 train_time:112353ms step_avg:92.24ms
step:1219/1660 train_time:112446ms step_avg:92.24ms
step:1220/1660 train_time:112539ms step_avg:92.24ms
step:1221/1660 train_time:112632ms step_avg:92.25ms
step:1222/1660 train_time:112724ms step_avg:92.25ms
step:1223/1660 train_time:112817ms step_avg:92.25ms
step:1224/1660 train_time:112911ms step_avg:92.25ms
step:1225/1660 train_time:113004ms step_avg:92.25ms
step:1226/1660 train_time:113099ms step_avg:92.25ms
step:1227/1660 train_time:113191ms step_avg:92.25ms
step:1228/1660 train_time:113284ms step_avg:92.25ms
step:1229/1660 train_time:113376ms step_avg:92.25ms
step:1230/1660 train_time:113470ms step_avg:92.25ms
step:1231/1660 train_time:113564ms step_avg:92.25ms
step:1232/1660 train_time:113657ms step_avg:92.25ms
step:1233/1660 train_time:113751ms step_avg:92.26ms
step:1234/1660 train_time:113844ms step_avg:92.26ms
step:1235/1660 train_time:113936ms step_avg:92.26ms
step:1236/1660 train_time:114030ms step_avg:92.26ms
step:1237/1660 train_time:114124ms step_avg:92.26ms
step:1238/1660 train_time:114218ms step_avg:92.26ms
step:1239/1660 train_time:114310ms step_avg:92.26ms
step:1240/1660 train_time:114405ms step_avg:92.26ms
step:1241/1660 train_time:114498ms step_avg:92.26ms
step:1242/1660 train_time:114590ms step_avg:92.26ms
step:1243/1660 train_time:114684ms step_avg:92.26ms
step:1244/1660 train_time:114777ms step_avg:92.26ms
step:1245/1660 train_time:114871ms step_avg:92.27ms
step:1246/1660 train_time:114964ms step_avg:92.27ms
step:1247/1660 train_time:115057ms step_avg:92.27ms
step:1248/1660 train_time:115151ms step_avg:92.27ms
step:1249/1660 train_time:115244ms step_avg:92.27ms
step:1250/1660 train_time:115337ms step_avg:92.27ms
step:1250/1660 val_loss:3.3776 train_time:115432ms step_avg:92.35ms
step:1251/1660 train_time:115451ms step_avg:92.29ms
step:1252/1660 train_time:115528ms step_avg:92.27ms
step:1253/1660 train_time:115624ms step_avg:92.28ms
step:1254/1660 train_time:115716ms step_avg:92.28ms
step:1255/1660 train_time:115808ms step_avg:92.28ms
step:1256/1660 train_time:115900ms step_avg:92.28ms
step:1257/1660 train_time:115993ms step_avg:92.28ms
step:1258/1660 train_time:116086ms step_avg:92.28ms
step:1259/1660 train_time:116178ms step_avg:92.28ms
step:1260/1660 train_time:116272ms step_avg:92.28ms
step:1261/1660 train_time:116366ms step_avg:92.28ms
step:1262/1660 train_time:116460ms step_avg:92.28ms
step:1263/1660 train_time:116555ms step_avg:92.28ms
step:1264/1660 train_time:116649ms step_avg:92.29ms
step:1265/1660 train_time:116743ms step_avg:92.29ms
step:1266/1660 train_time:116835ms step_avg:92.29ms
step:1267/1660 train_time:116928ms step_avg:92.29ms
step:1268/1660 train_time:117020ms step_avg:92.29ms
step:1269/1660 train_time:117113ms step_avg:92.29ms
step:1270/1660 train_time:117207ms step_avg:92.29ms
step:1271/1660 train_time:117301ms step_avg:92.29ms
step:1272/1660 train_time:117396ms step_avg:92.29ms
step:1273/1660 train_time:117491ms step_avg:92.29ms
step:1274/1660 train_time:117585ms step_avg:92.30ms
step:1275/1660 train_time:117678ms step_avg:92.30ms
step:1276/1660 train_time:117773ms step_avg:92.30ms
step:1277/1660 train_time:117865ms step_avg:92.30ms
step:1278/1660 train_time:117957ms step_avg:92.30ms
step:1279/1660 train_time:118050ms step_avg:92.30ms
step:1280/1660 train_time:118143ms step_avg:92.30ms
step:1281/1660 train_time:118235ms step_avg:92.30ms
step:1282/1660 train_time:118330ms step_avg:92.30ms
step:1283/1660 train_time:118424ms step_avg:92.30ms
step:1284/1660 train_time:118517ms step_avg:92.30ms
step:1285/1660 train_time:118612ms step_avg:92.31ms
step:1286/1660 train_time:118707ms step_avg:92.31ms
step:1287/1660 train_time:118801ms step_avg:92.31ms
step:1288/1660 train_time:118894ms step_avg:92.31ms
step:1289/1660 train_time:118987ms step_avg:92.31ms
step:1290/1660 train_time:119079ms step_avg:92.31ms
step:1291/1660 train_time:119173ms step_avg:92.31ms
step:1292/1660 train_time:119267ms step_avg:92.31ms
step:1293/1660 train_time:119360ms step_avg:92.31ms
step:1294/1660 train_time:119454ms step_avg:92.31ms
step:1295/1660 train_time:119547ms step_avg:92.31ms
step:1296/1660 train_time:119641ms step_avg:92.32ms
step:1297/1660 train_time:119734ms step_avg:92.32ms
step:1298/1660 train_time:119828ms step_avg:92.32ms
step:1299/1660 train_time:119921ms step_avg:92.32ms
step:1300/1660 train_time:120014ms step_avg:92.32ms
step:1301/1660 train_time:120107ms step_avg:92.32ms
step:1302/1660 train_time:120200ms step_avg:92.32ms
step:1303/1660 train_time:120293ms step_avg:92.32ms
step:1304/1660 train_time:120386ms step_avg:92.32ms
step:1305/1660 train_time:120480ms step_avg:92.32ms
step:1306/1660 train_time:120576ms step_avg:92.32ms
step:1307/1660 train_time:120669ms step_avg:92.33ms
step:1308/1660 train_time:120763ms step_avg:92.33ms
step:1309/1660 train_time:120856ms step_avg:92.33ms
step:1310/1660 train_time:120949ms step_avg:92.33ms
step:1311/1660 train_time:121042ms step_avg:92.33ms
step:1312/1660 train_time:121135ms step_avg:92.33ms
step:1313/1660 train_time:121229ms step_avg:92.33ms
step:1314/1660 train_time:121323ms step_avg:92.33ms
step:1315/1660 train_time:121415ms step_avg:92.33ms
step:1316/1660 train_time:121509ms step_avg:92.33ms
step:1317/1660 train_time:121602ms step_avg:92.33ms
step:1318/1660 train_time:121696ms step_avg:92.33ms
step:1319/1660 train_time:121790ms step_avg:92.33ms
step:1320/1660 train_time:121883ms step_avg:92.34ms
step:1321/1660 train_time:121976ms step_avg:92.34ms
step:1322/1660 train_time:122071ms step_avg:92.34ms
step:1323/1660 train_time:122165ms step_avg:92.34ms
step:1324/1660 train_time:122258ms step_avg:92.34ms
step:1325/1660 train_time:122352ms step_avg:92.34ms
step:1326/1660 train_time:122446ms step_avg:92.34ms
step:1327/1660 train_time:122539ms step_avg:92.34ms
step:1328/1660 train_time:122633ms step_avg:92.34ms
step:1329/1660 train_time:122726ms step_avg:92.34ms
step:1330/1660 train_time:122819ms step_avg:92.35ms
step:1331/1660 train_time:122912ms step_avg:92.35ms
step:1332/1660 train_time:123006ms step_avg:92.35ms
step:1333/1660 train_time:123099ms step_avg:92.35ms
step:1334/1660 train_time:123192ms step_avg:92.35ms
step:1335/1660 train_time:123286ms step_avg:92.35ms
step:1336/1660 train_time:123380ms step_avg:92.35ms
step:1337/1660 train_time:123475ms step_avg:92.35ms
step:1338/1660 train_time:123569ms step_avg:92.35ms
step:1339/1660 train_time:123662ms step_avg:92.35ms
step:1340/1660 train_time:123756ms step_avg:92.35ms
step:1341/1660 train_time:123849ms step_avg:92.36ms
step:1342/1660 train_time:123942ms step_avg:92.36ms
step:1343/1660 train_time:124035ms step_avg:92.36ms
step:1344/1660 train_time:124129ms step_avg:92.36ms
step:1345/1660 train_time:124222ms step_avg:92.36ms
step:1346/1660 train_time:124315ms step_avg:92.36ms
step:1347/1660 train_time:124409ms step_avg:92.36ms
step:1348/1660 train_time:124502ms step_avg:92.36ms
step:1349/1660 train_time:124596ms step_avg:92.36ms
step:1350/1660 train_time:124690ms step_avg:92.36ms
step:1351/1660 train_time:124783ms step_avg:92.36ms
step:1352/1660 train_time:124876ms step_avg:92.36ms
step:1353/1660 train_time:124970ms step_avg:92.37ms
step:1354/1660 train_time:125064ms step_avg:92.37ms
step:1355/1660 train_time:125156ms step_avg:92.37ms
step:1356/1660 train_time:125250ms step_avg:92.37ms
step:1357/1660 train_time:125343ms step_avg:92.37ms
step:1358/1660 train_time:125436ms step_avg:92.37ms
step:1359/1660 train_time:125530ms step_avg:92.37ms
step:1360/1660 train_time:125623ms step_avg:92.37ms
step:1361/1660 train_time:125716ms step_avg:92.37ms
step:1362/1660 train_time:125809ms step_avg:92.37ms
step:1363/1660 train_time:125902ms step_avg:92.37ms
step:1364/1660 train_time:125995ms step_avg:92.37ms
step:1365/1660 train_time:126089ms step_avg:92.37ms
step:1366/1660 train_time:126182ms step_avg:92.37ms
step:1367/1660 train_time:126276ms step_avg:92.37ms
step:1368/1660 train_time:126371ms step_avg:92.38ms
step:1369/1660 train_time:126464ms step_avg:92.38ms
step:1370/1660 train_time:126557ms step_avg:92.38ms
step:1371/1660 train_time:126652ms step_avg:92.38ms
step:1372/1660 train_time:126746ms step_avg:92.38ms
step:1373/1660 train_time:126839ms step_avg:92.38ms
step:1374/1660 train_time:126933ms step_avg:92.38ms
step:1375/1660 train_time:127026ms step_avg:92.38ms
step:1375/1660 val_loss:3.3434 train_time:127120ms step_avg:92.45ms
step:1376/1660 train_time:127142ms step_avg:92.40ms
step:1377/1660 train_time:127217ms step_avg:92.39ms
step:1378/1660 train_time:127314ms step_avg:92.39ms
step:1379/1660 train_time:127407ms step_avg:92.39ms
step:1380/1660 train_time:127499ms step_avg:92.39ms
step:1381/1660 train_time:127592ms step_avg:92.39ms
step:1382/1660 train_time:127685ms step_avg:92.39ms
step:1383/1660 train_time:127777ms step_avg:92.39ms
step:1384/1660 train_time:127870ms step_avg:92.39ms
step:1385/1660 train_time:127963ms step_avg:92.39ms
step:1386/1660 train_time:128057ms step_avg:92.39ms
step:1387/1660 train_time:128155ms step_avg:92.40ms
step:1388/1660 train_time:128250ms step_avg:92.40ms
step:1389/1660 train_time:128343ms step_avg:92.40ms
step:1390/1660 train_time:128438ms step_avg:92.40ms
step:1391/1660 train_time:128531ms step_avg:92.40ms
step:1392/1660 train_time:128624ms step_avg:92.40ms
step:1393/1660 train_time:128716ms step_avg:92.40ms
step:1394/1660 train_time:128809ms step_avg:92.40ms
step:1395/1660 train_time:128901ms step_avg:92.40ms
step:1396/1660 train_time:128994ms step_avg:92.40ms
step:1397/1660 train_time:129088ms step_avg:92.40ms
step:1398/1660 train_time:129183ms step_avg:92.41ms
step:1399/1660 train_time:129277ms step_avg:92.41ms
step:1400/1660 train_time:129372ms step_avg:92.41ms
step:1401/1660 train_time:129465ms step_avg:92.41ms
step:1402/1660 train_time:129558ms step_avg:92.41ms
step:1403/1660 train_time:129652ms step_avg:92.41ms
step:1404/1660 train_time:129743ms step_avg:92.41ms
step:1405/1660 train_time:129836ms step_avg:92.41ms
step:1406/1660 train_time:129929ms step_avg:92.41ms
step:1407/1660 train_time:130022ms step_avg:92.41ms
step:1408/1660 train_time:130116ms step_avg:92.41ms
step:1409/1660 train_time:130210ms step_avg:92.41ms
step:1410/1660 train_time:130305ms step_avg:92.41ms
step:1411/1660 train_time:130400ms step_avg:92.42ms
step:1412/1660 train_time:130494ms step_avg:92.42ms
step:1413/1660 train_time:130588ms step_avg:92.42ms
step:1414/1660 train_time:130680ms step_avg:92.42ms
step:1415/1660 train_time:130773ms step_avg:92.42ms
step:1416/1660 train_time:130866ms step_avg:92.42ms
step:1417/1660 train_time:130959ms step_avg:92.42ms
step:1418/1660 train_time:131053ms step_avg:92.42ms
step:1419/1660 train_time:131146ms step_avg:92.42ms
step:1420/1660 train_time:131239ms step_avg:92.42ms
step:1421/1660 train_time:131334ms step_avg:92.42ms
step:1422/1660 train_time:131430ms step_avg:92.43ms
step:1423/1660 train_time:131523ms step_avg:92.43ms
step:1424/1660 train_time:131616ms step_avg:92.43ms
step:1425/1660 train_time:131708ms step_avg:92.43ms
step:1426/1660 train_time:131801ms step_avg:92.43ms
step:1427/1660 train_time:131895ms step_avg:92.43ms
step:1428/1660 train_time:131988ms step_avg:92.43ms
step:1429/1660 train_time:132081ms step_avg:92.43ms
step:1430/1660 train_time:132175ms step_avg:92.43ms
step:1431/1660 train_time:132268ms step_avg:92.43ms
step:1432/1660 train_time:132362ms step_avg:92.43ms
step:1433/1660 train_time:132457ms step_avg:92.43ms
step:1434/1660 train_time:132551ms step_avg:92.43ms
step:1435/1660 train_time:132644ms step_avg:92.43ms
step:1436/1660 train_time:132737ms step_avg:92.43ms
step:1437/1660 train_time:132830ms step_avg:92.44ms
step:1438/1660 train_time:132922ms step_avg:92.44ms
step:1439/1660 train_time:133016ms step_avg:92.44ms
step:1440/1660 train_time:133109ms step_avg:92.44ms
step:1441/1660 train_time:133202ms step_avg:92.44ms
step:1442/1660 train_time:133297ms step_avg:92.44ms
step:1443/1660 train_time:133391ms step_avg:92.44ms
step:1444/1660 train_time:133485ms step_avg:92.44ms
step:1445/1660 train_time:133581ms step_avg:92.44ms
step:1446/1660 train_time:133675ms step_avg:92.44ms
step:1447/1660 train_time:133767ms step_avg:92.44ms
step:1448/1660 train_time:133861ms step_avg:92.45ms
step:1449/1660 train_time:133954ms step_avg:92.45ms
step:1450/1660 train_time:134048ms step_avg:92.45ms
step:1451/1660 train_time:134141ms step_avg:92.45ms
step:1452/1660 train_time:134234ms step_avg:92.45ms
step:1453/1660 train_time:134327ms step_avg:92.45ms
step:1454/1660 train_time:134422ms step_avg:92.45ms
step:1455/1660 train_time:134516ms step_avg:92.45ms
step:1456/1660 train_time:134609ms step_avg:92.45ms
step:1457/1660 train_time:134703ms step_avg:92.45ms
step:1458/1660 train_time:134797ms step_avg:92.45ms
step:1459/1660 train_time:134890ms step_avg:92.45ms
step:1460/1660 train_time:134983ms step_avg:92.45ms
step:1461/1660 train_time:135077ms step_avg:92.46ms
step:1462/1660 train_time:135171ms step_avg:92.46ms
step:1463/1660 train_time:135264ms step_avg:92.46ms
step:1464/1660 train_time:135359ms step_avg:92.46ms
step:1465/1660 train_time:135453ms step_avg:92.46ms
step:1466/1660 train_time:135545ms step_avg:92.46ms
step:1467/1660 train_time:135638ms step_avg:92.46ms
step:1468/1660 train_time:135732ms step_avg:92.46ms
step:1469/1660 train_time:135826ms step_avg:92.46ms
step:1470/1660 train_time:135919ms step_avg:92.46ms
step:1471/1660 train_time:136012ms step_avg:92.46ms
step:1472/1660 train_time:136106ms step_avg:92.46ms
step:1473/1660 train_time:136201ms step_avg:92.46ms
step:1474/1660 train_time:136294ms step_avg:92.47ms
step:1475/1660 train_time:136388ms step_avg:92.47ms
step:1476/1660 train_time:136482ms step_avg:92.47ms
step:1477/1660 train_time:136576ms step_avg:92.47ms
step:1478/1660 train_time:136669ms step_avg:92.47ms
step:1479/1660 train_time:136763ms step_avg:92.47ms
step:1480/1660 train_time:136857ms step_avg:92.47ms
step:1481/1660 train_time:136952ms step_avg:92.47ms
step:1482/1660 train_time:137044ms step_avg:92.47ms
step:1483/1660 train_time:137138ms step_avg:92.47ms
step:1484/1660 train_time:137233ms step_avg:92.47ms
step:1485/1660 train_time:137326ms step_avg:92.48ms
step:1486/1660 train_time:137420ms step_avg:92.48ms
step:1487/1660 train_time:137513ms step_avg:92.48ms
step:1488/1660 train_time:137606ms step_avg:92.48ms
step:1489/1660 train_time:137700ms step_avg:92.48ms
step:1490/1660 train_time:137793ms step_avg:92.48ms
step:1491/1660 train_time:137887ms step_avg:92.48ms
step:1492/1660 train_time:137981ms step_avg:92.48ms
step:1493/1660 train_time:138074ms step_avg:92.48ms
step:1494/1660 train_time:138168ms step_avg:92.48ms
step:1495/1660 train_time:138262ms step_avg:92.48ms
step:1496/1660 train_time:138355ms step_avg:92.48ms
step:1497/1660 train_time:138448ms step_avg:92.48ms
step:1498/1660 train_time:138541ms step_avg:92.48ms
step:1499/1660 train_time:138636ms step_avg:92.49ms
step:1500/1660 train_time:138729ms step_avg:92.49ms
step:1500/1660 val_loss:3.3136 train_time:138824ms step_avg:92.55ms
step:1501/1660 train_time:138843ms step_avg:92.50ms
step:1502/1660 train_time:138920ms step_avg:92.49ms
step:1503/1660 train_time:139016ms step_avg:92.49ms
step:1504/1660 train_time:139109ms step_avg:92.49ms
step:1505/1660 train_time:139202ms step_avg:92.49ms
step:1506/1660 train_time:139294ms step_avg:92.49ms
step:1507/1660 train_time:139386ms step_avg:92.49ms
step:1508/1660 train_time:139479ms step_avg:92.49ms
step:1509/1660 train_time:139572ms step_avg:92.49ms
step:1510/1660 train_time:139664ms step_avg:92.49ms
step:1511/1660 train_time:139759ms step_avg:92.49ms
step:1512/1660 train_time:139855ms step_avg:92.50ms
step:1513/1660 train_time:139951ms step_avg:92.50ms
step:1514/1660 train_time:140045ms step_avg:92.50ms
step:1515/1660 train_time:140138ms step_avg:92.50ms
step:1516/1660 train_time:140230ms step_avg:92.50ms
step:1517/1660 train_time:140323ms step_avg:92.50ms
step:1518/1660 train_time:140417ms step_avg:92.50ms
step:1519/1660 train_time:140510ms step_avg:92.50ms
step:1520/1660 train_time:140602ms step_avg:92.50ms
step:1521/1660 train_time:140695ms step_avg:92.50ms
step:1522/1660 train_time:140789ms step_avg:92.50ms
step:1523/1660 train_time:140883ms step_avg:92.50ms
step:1524/1660 train_time:140978ms step_avg:92.51ms
step:1525/1660 train_time:141073ms step_avg:92.51ms
step:1526/1660 train_time:141166ms step_avg:92.51ms
step:1527/1660 train_time:141260ms step_avg:92.51ms
step:1528/1660 train_time:141353ms step_avg:92.51ms
step:1529/1660 train_time:141446ms step_avg:92.51ms
step:1530/1660 train_time:141539ms step_avg:92.51ms
step:1531/1660 train_time:141632ms step_avg:92.51ms
step:1532/1660 train_time:141726ms step_avg:92.51ms
step:1533/1660 train_time:141820ms step_avg:92.51ms
step:1534/1660 train_time:141915ms step_avg:92.51ms
step:1535/1660 train_time:142009ms step_avg:92.51ms
step:1536/1660 train_time:142103ms step_avg:92.52ms
step:1537/1660 train_time:142196ms step_avg:92.52ms
step:1538/1660 train_time:142289ms step_avg:92.52ms
step:1539/1660 train_time:142382ms step_avg:92.52ms
step:1540/1660 train_time:142475ms step_avg:92.52ms
step:1541/1660 train_time:142568ms step_avg:92.52ms
step:1542/1660 train_time:142661ms step_avg:92.52ms
step:1543/1660 train_time:142755ms step_avg:92.52ms
step:1544/1660 train_time:142849ms step_avg:92.52ms
step:1545/1660 train_time:142945ms step_avg:92.52ms
step:1546/1660 train_time:143038ms step_avg:92.52ms
step:1547/1660 train_time:143132ms step_avg:92.52ms
step:1548/1660 train_time:143225ms step_avg:92.52ms
step:1549/1660 train_time:143318ms step_avg:92.52ms
step:1550/1660 train_time:143411ms step_avg:92.52ms
step:1551/1660 train_time:143505ms step_avg:92.52ms
step:1552/1660 train_time:143598ms step_avg:92.52ms
step:1553/1660 train_time:143691ms step_avg:92.53ms
step:1554/1660 train_time:143784ms step_avg:92.53ms
step:1555/1660 train_time:143879ms step_avg:92.53ms
step:1556/1660 train_time:143973ms step_avg:92.53ms
step:1557/1660 train_time:144067ms step_avg:92.53ms
step:1558/1660 train_time:144161ms step_avg:92.53ms
step:1559/1660 train_time:144256ms step_avg:92.53ms
step:1560/1660 train_time:144348ms step_avg:92.53ms
step:1561/1660 train_time:144441ms step_avg:92.53ms
step:1562/1660 train_time:144534ms step_avg:92.53ms
step:1563/1660 train_time:144628ms step_avg:92.53ms
step:1564/1660 train_time:144722ms step_avg:92.53ms
step:1565/1660 train_time:144816ms step_avg:92.53ms
step:1566/1660 train_time:144909ms step_avg:92.53ms
step:1567/1660 train_time:145003ms step_avg:92.54ms
step:1568/1660 train_time:145096ms step_avg:92.54ms
step:1569/1660 train_time:145189ms step_avg:92.54ms
step:1570/1660 train_time:145283ms step_avg:92.54ms
step:1571/1660 train_time:145376ms step_avg:92.54ms
step:1572/1660 train_time:145469ms step_avg:92.54ms
step:1573/1660 train_time:145562ms step_avg:92.54ms
step:1574/1660 train_time:145656ms step_avg:92.54ms
step:1575/1660 train_time:145750ms step_avg:92.54ms
step:1576/1660 train_time:145844ms step_avg:92.54ms
step:1577/1660 train_time:145937ms step_avg:92.54ms
step:1578/1660 train_time:146030ms step_avg:92.54ms
step:1579/1660 train_time:146124ms step_avg:92.54ms
step:1580/1660 train_time:146219ms step_avg:92.54ms
step:1581/1660 train_time:146313ms step_avg:92.54ms
step:1582/1660 train_time:146406ms step_avg:92.54ms
step:1583/1660 train_time:146499ms step_avg:92.54ms
step:1584/1660 train_time:146591ms step_avg:92.55ms
step:1585/1660 train_time:146685ms step_avg:92.55ms
step:1586/1660 train_time:146779ms step_avg:92.55ms
step:1587/1660 train_time:146872ms step_avg:92.55ms
step:1588/1660 train_time:146965ms step_avg:92.55ms
step:1589/1660 train_time:147060ms step_avg:92.55ms
step:1590/1660 train_time:147154ms step_avg:92.55ms
step:1591/1660 train_time:147247ms step_avg:92.55ms
step:1592/1660 train_time:147340ms step_avg:92.55ms
step:1593/1660 train_time:147433ms step_avg:92.55ms
step:1594/1660 train_time:147527ms step_avg:92.55ms
step:1595/1660 train_time:147620ms step_avg:92.55ms
step:1596/1660 train_time:147713ms step_avg:92.55ms
step:1597/1660 train_time:147807ms step_avg:92.55ms
step:1598/1660 train_time:147901ms step_avg:92.55ms
step:1599/1660 train_time:147994ms step_avg:92.55ms
step:1600/1660 train_time:148087ms step_avg:92.55ms
step:1601/1660 train_time:148182ms step_avg:92.56ms
step:1602/1660 train_time:148275ms step_avg:92.56ms
step:1603/1660 train_time:148369ms step_avg:92.56ms
step:1604/1660 train_time:148462ms step_avg:92.56ms
step:1605/1660 train_time:148556ms step_avg:92.56ms
step:1606/1660 train_time:148648ms step_avg:92.56ms
step:1607/1660 train_time:148742ms step_avg:92.56ms
step:1608/1660 train_time:148835ms step_avg:92.56ms
step:1609/1660 train_time:148929ms step_avg:92.56ms
step:1610/1660 train_time:149022ms step_avg:92.56ms
step:1611/1660 train_time:149117ms step_avg:92.56ms
step:1612/1660 train_time:149211ms step_avg:92.56ms
step:1613/1660 train_time:149304ms step_avg:92.56ms
step:1614/1660 train_time:149398ms step_avg:92.56ms
step:1615/1660 train_time:149491ms step_avg:92.56ms
step:1616/1660 train_time:149584ms step_avg:92.56ms
step:1617/1660 train_time:149678ms step_avg:92.57ms
step:1618/1660 train_time:149771ms step_avg:92.57ms
step:1619/1660 train_time:149864ms step_avg:92.57ms
step:1620/1660 train_time:149957ms step_avg:92.57ms
step:1621/1660 train_time:150050ms step_avg:92.57ms
step:1622/1660 train_time:150145ms step_avg:92.57ms
step:1623/1660 train_time:150240ms step_avg:92.57ms
step:1624/1660 train_time:150334ms step_avg:92.57ms
step:1625/1660 train_time:150427ms step_avg:92.57ms
step:1625/1660 val_loss:3.2889 train_time:150522ms step_avg:92.63ms
step:1626/1660 train_time:150541ms step_avg:92.58ms
step:1627/1660 train_time:150621ms step_avg:92.58ms
step:1628/1660 train_time:150718ms step_avg:92.58ms
step:1629/1660 train_time:150814ms step_avg:92.58ms
step:1630/1660 train_time:150906ms step_avg:92.58ms
step:1631/1660 train_time:150999ms step_avg:92.58ms
step:1632/1660 train_time:151092ms step_avg:92.58ms
step:1633/1660 train_time:151184ms step_avg:92.58ms
step:1634/1660 train_time:151277ms step_avg:92.58ms
step:1635/1660 train_time:151369ms step_avg:92.58ms
step:1636/1660 train_time:151461ms step_avg:92.58ms
step:1637/1660 train_time:151558ms step_avg:92.58ms
step:1638/1660 train_time:151657ms step_avg:92.59ms
step:1639/1660 train_time:151754ms step_avg:92.59ms
step:1640/1660 train_time:151847ms step_avg:92.59ms
step:1641/1660 train_time:151940ms step_avg:92.59ms
step:1642/1660 train_time:152033ms step_avg:92.59ms
step:1643/1660 train_time:152126ms step_avg:92.59ms
step:1644/1660 train_time:152218ms step_avg:92.59ms
step:1645/1660 train_time:152310ms step_avg:92.59ms
step:1646/1660 train_time:152403ms step_avg:92.59ms
step:1647/1660 train_time:152497ms step_avg:92.59ms
step:1648/1660 train_time:152592ms step_avg:92.59ms
step:1649/1660 train_time:152687ms step_avg:92.59ms
step:1650/1660 train_time:152781ms step_avg:92.59ms
step:1651/1660 train_time:152874ms step_avg:92.59ms
step:1652/1660 train_time:152967ms step_avg:92.60ms
step:1653/1660 train_time:153059ms step_avg:92.59ms
step:1654/1660 train_time:153153ms step_avg:92.60ms
step:1655/1660 train_time:153246ms step_avg:92.60ms
step:1656/1660 train_time:153338ms step_avg:92.60ms
step:1657/1660 train_time:153431ms step_avg:92.60ms
step:1658/1660 train_time:153525ms step_avg:92.60ms
step:1659/1660 train_time:153619ms step_avg:92.60ms
step:1660/1660 train_time:153715ms step_avg:92.60ms
step:1660/1660 val_loss:3.2807 train_time:153811ms step_avg:92.66ms
peak memory allocated: 31587 MiB reserved: 47116 MiB

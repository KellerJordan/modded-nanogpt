import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            eff_lr_val = (
                group["lr"]
                * max(1, params[0].size(-2) / params[0].size(-1)) ** 0.5
                * getattr(params[0], "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(params[0], "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(params[0].grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.

            original_shape = batched_update_grads.shape
            if batched_update_grads.ndim > 3:
                assert batched_update_grads.ndim == 4
                batch = original_shape[0] * original_shape[1]
                # Flatten all but the first two dims after batch
                d1 = original_shape[2]
                d2 = original_shape[3]
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()
        
    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        assert hdim == dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkvo_w = nn.Parameter(torch.empty(4, hdim, dim))
        with torch.no_grad():
            self.qkvo_w[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make both matrices have the same shape because optimizer sorts params by shape
        # 2 matrices x 12 layers = 24 total, which is divisible by 8 GPU world size
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 7 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws: int, ws_final_layer: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = ws * args.block_size, (ws // 2) * args.block_size
        final_bm = ws_final_layer * args.block_size
        bm_sizes = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, final_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        for i in range(len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction="sum" if self.training else "mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload=False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            #t0 = time.perf_counter()
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            #t1 = time.perf_counter()
            #print(f'{t1-t0} slowload')
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    def __init__(self, file_iter, world_size):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()
    
    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()
    
    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()
    
    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
      #loading in a whole shard will be slow...
      #BosFinder tracks its self.i index. I can kickoff with a smaller set. then have the 
      finder = BOSFinder(tokens, world_size=world_size, quickload=True)
      preloader = DataPreloader(file_iter, world_size)
      preloader.start()
    else:
      pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                #tokens = _load_data_shard(next(file_iter))
                #finder = BOSFinder(tokens, world_size=world_size)
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main
@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1660 # number of iterations to run
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"data_threading_1660/{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws @classiclarryd
    ws_validate_final_layer: int = 20 # final layer is highly indifferent to length

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations:
        return args.ws_validate, args.ws_validate_final_layer
    x = step / (1 + args.num_iterations)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx], args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws=args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each
    if new_ws > ws:
        model.yarn.apply(ws, new_ws)
        ws = new_ws
    elif new_ws<ws:
        model.yarn.reset()
        ws = new_ws
    model(inputs, targets, cum_seqlens, ws, ws).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws, ws_final_layer = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    new_ws, ws_final_layer = get_ws(step)
    if new_ws != ws:
        model.yarn.apply(ws, new_ws)
        ws=new_ws

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws, ws_final_layer)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws, ws_final_layer).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.9.0.dev20250718+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Tue Sep 16 04:01:46 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   32C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   39C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   31C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   36C    P0            127W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          200415      C   /usr/bin/python3                       1510MiB |
|    0   N/A  N/A          200416      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          200417      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          200418      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          200419      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          200420      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          200421      C   /usr/bin/python3                        614MiB |
|    0   N/A  N/A          200422      C   /usr/bin/python3                        614MiB |
|    1   N/A  N/A          200416      C   /usr/bin/python3                       1510MiB |
|    2   N/A  N/A          200417      C   /usr/bin/python3                       1510MiB |
|    3   N/A  N/A          200418      C   /usr/bin/python3                       1510MiB |
|    4   N/A  N/A          200419      C   /usr/bin/python3                       1510MiB |
|    5   N/A  N/A          200420      C   /usr/bin/python3                       1510MiB |
|    6   N/A  N/A          200421      C   /usr/bin/python3                       1510MiB |
|    7   N/A  N/A          200422      C   /usr/bin/python3                       1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1660 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1660 train_time:146ms step_avg:145.85ms
step:2/1660 train_time:171ms step_avg:85.47ms
step:3/1660 train_time:234ms step_avg:77.90ms
step:4/1660 train_time:323ms step_avg:80.80ms
step:5/1660 train_time:414ms step_avg:82.73ms
step:6/1660 train_time:504ms step_avg:84.07ms
step:7/1660 train_time:595ms step_avg:84.98ms
step:8/1660 train_time:685ms step_avg:85.65ms
step:9/1660 train_time:776ms step_avg:86.22ms
step:10/1660 train_time:868ms step_avg:86.78ms
step:11/1660 train_time:959ms step_avg:87.16ms
step:12/1660 train_time:1054ms step_avg:87.79ms
step:13/1660 train_time:1149ms step_avg:88.40ms
step:14/1660 train_time:1242ms step_avg:88.72ms
step:15/1660 train_time:1333ms step_avg:88.87ms
step:16/1660 train_time:1424ms step_avg:88.99ms
step:17/1660 train_time:1515ms step_avg:89.12ms
step:18/1660 train_time:1607ms step_avg:89.27ms
step:19/1660 train_time:1698ms step_avg:89.36ms
step:20/1660 train_time:1789ms step_avg:89.44ms
step:21/1660 train_time:1880ms step_avg:89.50ms
step:22/1660 train_time:1972ms step_avg:89.65ms
step:23/1660 train_time:2066ms step_avg:89.83ms
step:24/1660 train_time:2160ms step_avg:89.99ms
step:25/1660 train_time:2253ms step_avg:90.11ms
step:26/1660 train_time:2346ms step_avg:90.23ms
step:27/1660 train_time:2437ms step_avg:90.27ms
step:28/1660 train_time:2530ms step_avg:90.35ms
step:29/1660 train_time:2621ms step_avg:90.38ms
step:30/1660 train_time:2713ms step_avg:90.42ms
step:31/1660 train_time:2804ms step_avg:90.44ms
step:32/1660 train_time:2896ms step_avg:90.50ms
step:33/1660 train_time:2988ms step_avg:90.55ms
step:34/1660 train_time:3081ms step_avg:90.61ms
step:35/1660 train_time:3173ms step_avg:90.66ms
step:36/1660 train_time:3264ms step_avg:90.68ms
step:37/1660 train_time:3356ms step_avg:90.71ms
step:38/1660 train_time:3448ms step_avg:90.73ms
step:39/1660 train_time:3539ms step_avg:90.74ms
step:40/1660 train_time:3630ms step_avg:90.76ms
step:41/1660 train_time:3722ms step_avg:90.77ms
step:42/1660 train_time:3814ms step_avg:90.81ms
step:43/1660 train_time:3906ms step_avg:90.84ms
step:44/1660 train_time:3997ms step_avg:90.84ms
step:45/1660 train_time:4089ms step_avg:90.87ms
step:46/1660 train_time:4182ms step_avg:90.91ms
step:47/1660 train_time:4274ms step_avg:90.94ms
step:48/1660 train_time:4366ms step_avg:90.96ms
step:49/1660 train_time:4458ms step_avg:90.97ms
step:50/1660 train_time:4549ms step_avg:90.98ms
step:51/1660 train_time:4640ms step_avg:90.99ms
step:52/1660 train_time:4732ms step_avg:91.01ms
step:53/1660 train_time:4824ms step_avg:91.02ms
step:54/1660 train_time:4916ms step_avg:91.03ms
step:55/1660 train_time:5007ms step_avg:91.04ms
step:56/1660 train_time:5099ms step_avg:91.06ms
step:57/1660 train_time:5193ms step_avg:91.10ms
step:58/1660 train_time:5285ms step_avg:91.12ms
step:59/1660 train_time:5376ms step_avg:91.12ms
step:60/1660 train_time:5467ms step_avg:91.12ms
step:61/1660 train_time:5559ms step_avg:91.13ms
step:62/1660 train_time:5650ms step_avg:91.13ms
step:63/1660 train_time:5742ms step_avg:91.14ms
step:64/1660 train_time:5833ms step_avg:91.15ms
step:65/1660 train_time:5925ms step_avg:91.15ms
step:66/1660 train_time:6016ms step_avg:91.15ms
step:67/1660 train_time:6108ms step_avg:91.17ms
step:68/1660 train_time:6201ms step_avg:91.19ms
step:69/1660 train_time:6294ms step_avg:91.22ms
step:70/1660 train_time:6386ms step_avg:91.23ms
step:71/1660 train_time:6478ms step_avg:91.24ms
step:72/1660 train_time:6570ms step_avg:91.25ms
step:73/1660 train_time:6662ms step_avg:91.26ms
step:74/1660 train_time:6754ms step_avg:91.27ms
step:75/1660 train_time:6847ms step_avg:91.29ms
step:76/1660 train_time:6938ms step_avg:91.29ms
step:77/1660 train_time:7029ms step_avg:91.29ms
step:78/1660 train_time:7121ms step_avg:91.29ms
step:79/1660 train_time:7214ms step_avg:91.31ms
step:80/1660 train_time:7307ms step_avg:91.33ms
step:81/1660 train_time:7398ms step_avg:91.33ms
step:82/1660 train_time:7490ms step_avg:91.35ms
step:83/1660 train_time:7582ms step_avg:91.35ms
step:84/1660 train_time:7673ms step_avg:91.35ms
step:85/1660 train_time:7764ms step_avg:91.35ms
step:86/1660 train_time:7856ms step_avg:91.35ms
step:87/1660 train_time:7948ms step_avg:91.35ms
step:88/1660 train_time:8039ms step_avg:91.35ms
step:89/1660 train_time:8130ms step_avg:91.35ms
step:90/1660 train_time:8221ms step_avg:91.35ms
step:91/1660 train_time:8313ms step_avg:91.36ms
step:92/1660 train_time:8405ms step_avg:91.36ms
step:93/1660 train_time:8496ms step_avg:91.36ms
step:94/1660 train_time:8589ms step_avg:91.37ms
step:95/1660 train_time:8680ms step_avg:91.37ms
step:96/1660 train_time:8772ms step_avg:91.38ms
step:97/1660 train_time:8864ms step_avg:91.38ms
step:98/1660 train_time:8956ms step_avg:91.38ms
step:99/1660 train_time:9047ms step_avg:91.39ms
step:100/1660 train_time:9138ms step_avg:91.38ms
step:101/1660 train_time:9229ms step_avg:91.38ms
step:102/1660 train_time:9320ms step_avg:91.38ms
step:103/1660 train_time:9413ms step_avg:91.39ms
step:104/1660 train_time:9506ms step_avg:91.40ms
step:105/1660 train_time:9597ms step_avg:91.40ms
step:106/1660 train_time:9688ms step_avg:91.40ms
step:107/1660 train_time:9779ms step_avg:91.40ms
step:108/1660 train_time:9871ms step_avg:91.40ms
step:109/1660 train_time:9962ms step_avg:91.40ms
step:110/1660 train_time:10054ms step_avg:91.40ms
step:111/1660 train_time:10147ms step_avg:91.42ms
step:112/1660 train_time:10238ms step_avg:91.41ms
step:113/1660 train_time:10329ms step_avg:91.41ms
step:114/1660 train_time:10421ms step_avg:91.41ms
step:115/1660 train_time:10515ms step_avg:91.43ms
step:116/1660 train_time:10606ms step_avg:91.43ms
step:117/1660 train_time:10697ms step_avg:91.43ms
step:118/1660 train_time:10789ms step_avg:91.43ms
step:119/1660 train_time:10881ms step_avg:91.43ms
step:120/1660 train_time:10972ms step_avg:91.44ms
step:121/1660 train_time:11064ms step_avg:91.44ms
step:122/1660 train_time:11155ms step_avg:91.44ms
step:123/1660 train_time:11247ms step_avg:91.44ms
step:124/1660 train_time:11338ms step_avg:91.44ms
step:125/1660 train_time:11430ms step_avg:91.44ms
step:125/1660 val_loss:4.3006 train_time:11523ms step_avg:92.18ms
step:126/1660 train_time:11546ms step_avg:91.63ms
step:127/1660 train_time:11617ms step_avg:91.47ms
step:128/1660 train_time:11718ms step_avg:91.55ms
step:129/1660 train_time:11814ms step_avg:91.58ms
step:130/1660 train_time:11905ms step_avg:91.58ms
step:131/1660 train_time:11997ms step_avg:91.58ms
step:132/1660 train_time:12087ms step_avg:91.57ms
step:133/1660 train_time:12178ms step_avg:91.56ms
step:134/1660 train_time:12268ms step_avg:91.55ms
step:135/1660 train_time:12359ms step_avg:91.55ms
step:136/1660 train_time:12449ms step_avg:91.54ms
step:137/1660 train_time:12540ms step_avg:91.53ms
step:138/1660 train_time:12633ms step_avg:91.55ms
step:139/1660 train_time:12728ms step_avg:91.57ms
step:140/1660 train_time:12824ms step_avg:91.60ms
step:141/1660 train_time:12916ms step_avg:91.60ms
step:142/1660 train_time:13008ms step_avg:91.60ms
step:143/1660 train_time:13100ms step_avg:91.60ms
step:144/1660 train_time:13190ms step_avg:91.60ms
step:145/1660 train_time:13280ms step_avg:91.59ms
step:146/1660 train_time:13370ms step_avg:91.58ms
step:147/1660 train_time:13461ms step_avg:91.57ms
step:148/1660 train_time:13552ms step_avg:91.57ms
step:149/1660 train_time:13645ms step_avg:91.58ms
step:150/1660 train_time:13737ms step_avg:91.58ms
step:151/1660 train_time:13830ms step_avg:91.59ms
step:152/1660 train_time:13923ms step_avg:91.60ms
step:153/1660 train_time:14014ms step_avg:91.60ms
step:154/1660 train_time:14105ms step_avg:91.59ms
step:155/1660 train_time:14196ms step_avg:91.59ms
step:156/1660 train_time:14286ms step_avg:91.58ms
step:157/1660 train_time:14377ms step_avg:91.57ms
step:158/1660 train_time:14468ms step_avg:91.57ms
step:159/1660 train_time:14559ms step_avg:91.57ms
step:160/1660 train_time:14653ms step_avg:91.58ms
step:161/1660 train_time:14744ms step_avg:91.58ms
step:162/1660 train_time:14836ms step_avg:91.58ms
step:163/1660 train_time:14929ms step_avg:91.59ms
step:164/1660 train_time:15021ms step_avg:91.59ms
step:165/1660 train_time:15112ms step_avg:91.59ms
step:166/1660 train_time:15203ms step_avg:91.58ms
step:167/1660 train_time:15294ms step_avg:91.58ms
step:168/1660 train_time:15384ms step_avg:91.57ms
step:169/1660 train_time:15475ms step_avg:91.57ms
step:170/1660 train_time:15566ms step_avg:91.56ms
step:171/1660 train_time:15659ms step_avg:91.57ms
step:172/1660 train_time:15751ms step_avg:91.58ms
step:173/1660 train_time:15843ms step_avg:91.58ms
step:174/1660 train_time:15936ms step_avg:91.58ms
step:175/1660 train_time:16027ms step_avg:91.58ms
step:176/1660 train_time:16119ms step_avg:91.58ms
step:177/1660 train_time:16210ms step_avg:91.58ms
step:178/1660 train_time:16301ms step_avg:91.58ms
step:179/1660 train_time:16393ms step_avg:91.58ms
step:180/1660 train_time:16484ms step_avg:91.58ms
step:181/1660 train_time:16575ms step_avg:91.57ms
step:182/1660 train_time:16667ms step_avg:91.57ms
step:183/1660 train_time:16760ms step_avg:91.58ms
step:184/1660 train_time:16853ms step_avg:91.59ms
step:185/1660 train_time:16945ms step_avg:91.59ms
step:186/1660 train_time:17037ms step_avg:91.60ms
step:187/1660 train_time:17128ms step_avg:91.59ms
step:188/1660 train_time:17220ms step_avg:91.59ms
step:189/1660 train_time:17312ms step_avg:91.60ms
step:190/1660 train_time:17403ms step_avg:91.59ms
step:191/1660 train_time:17494ms step_avg:91.59ms
step:192/1660 train_time:17585ms step_avg:91.59ms
step:193/1660 train_time:17676ms step_avg:91.59ms
step:194/1660 train_time:17768ms step_avg:91.59ms
step:195/1660 train_time:17861ms step_avg:91.60ms
step:196/1660 train_time:17953ms step_avg:91.60ms
step:197/1660 train_time:18044ms step_avg:91.60ms
step:198/1660 train_time:18135ms step_avg:91.59ms
step:199/1660 train_time:18227ms step_avg:91.60ms
step:200/1660 train_time:18319ms step_avg:91.60ms
step:201/1660 train_time:18410ms step_avg:91.59ms
step:202/1660 train_time:18501ms step_avg:91.59ms
step:203/1660 train_time:18593ms step_avg:91.59ms
step:204/1660 train_time:18685ms step_avg:91.59ms
step:205/1660 train_time:18777ms step_avg:91.59ms
step:206/1660 train_time:18868ms step_avg:91.59ms
step:207/1660 train_time:18961ms step_avg:91.60ms
step:208/1660 train_time:19052ms step_avg:91.60ms
step:209/1660 train_time:19144ms step_avg:91.60ms
step:210/1660 train_time:19235ms step_avg:91.60ms
step:211/1660 train_time:19326ms step_avg:91.59ms
step:212/1660 train_time:19418ms step_avg:91.60ms
step:213/1660 train_time:19510ms step_avg:91.60ms
step:214/1660 train_time:19602ms step_avg:91.60ms
step:215/1660 train_time:19693ms step_avg:91.60ms
step:216/1660 train_time:19785ms step_avg:91.60ms
step:217/1660 train_time:19876ms step_avg:91.60ms
step:218/1660 train_time:19968ms step_avg:91.60ms
step:219/1660 train_time:20060ms step_avg:91.60ms
step:220/1660 train_time:20152ms step_avg:91.60ms
step:221/1660 train_time:20244ms step_avg:91.60ms
step:222/1660 train_time:20335ms step_avg:91.60ms
step:223/1660 train_time:20426ms step_avg:91.59ms
step:224/1660 train_time:20517ms step_avg:91.59ms
step:225/1660 train_time:20609ms step_avg:91.59ms
step:226/1660 train_time:20701ms step_avg:91.60ms
step:227/1660 train_time:20792ms step_avg:91.60ms
step:228/1660 train_time:20884ms step_avg:91.60ms
step:229/1660 train_time:20976ms step_avg:91.60ms
step:230/1660 train_time:21068ms step_avg:91.60ms
step:231/1660 train_time:21160ms step_avg:91.60ms
step:232/1660 train_time:21251ms step_avg:91.60ms
step:233/1660 train_time:21343ms step_avg:91.60ms
step:234/1660 train_time:21434ms step_avg:91.60ms
step:235/1660 train_time:21525ms step_avg:91.59ms
step:236/1660 train_time:21616ms step_avg:91.59ms
step:237/1660 train_time:21707ms step_avg:91.59ms
step:238/1660 train_time:21799ms step_avg:91.59ms
step:239/1660 train_time:21891ms step_avg:91.59ms
step:240/1660 train_time:21983ms step_avg:91.60ms
step:241/1660 train_time:22075ms step_avg:91.60ms
step:242/1660 train_time:22166ms step_avg:91.60ms
step:243/1660 train_time:22258ms step_avg:91.60ms
step:244/1660 train_time:22349ms step_avg:91.59ms
step:245/1660 train_time:22441ms step_avg:91.59ms
step:246/1660 train_time:22533ms step_avg:91.60ms
step:247/1660 train_time:22623ms step_avg:91.59ms
step:248/1660 train_time:22715ms step_avg:91.59ms
step:249/1660 train_time:22807ms step_avg:91.59ms
step:250/1660 train_time:22899ms step_avg:91.60ms
step:250/1660 val_loss:3.9630 train_time:22992ms step_avg:91.97ms
step:251/1660 train_time:23013ms step_avg:91.69ms
step:252/1660 train_time:23085ms step_avg:91.61ms
step:253/1660 train_time:23181ms step_avg:91.62ms
step:254/1660 train_time:23275ms step_avg:91.63ms
step:255/1660 train_time:23367ms step_avg:91.63ms
step:256/1660 train_time:23457ms step_avg:91.63ms
step:257/1660 train_time:23547ms step_avg:91.62ms
step:258/1660 train_time:23637ms step_avg:91.62ms
step:259/1660 train_time:23728ms step_avg:91.61ms
step:260/1660 train_time:23818ms step_avg:91.61ms
step:261/1660 train_time:23910ms step_avg:91.61ms
step:262/1660 train_time:24002ms step_avg:91.61ms
step:263/1660 train_time:24096ms step_avg:91.62ms
step:264/1660 train_time:24190ms step_avg:91.63ms
step:265/1660 train_time:24283ms step_avg:91.63ms
step:266/1660 train_time:24375ms step_avg:91.63ms
step:267/1660 train_time:24466ms step_avg:91.63ms
step:268/1660 train_time:24556ms step_avg:91.63ms
step:269/1660 train_time:24648ms step_avg:91.63ms
step:270/1660 train_time:24738ms step_avg:91.62ms
step:271/1660 train_time:24829ms step_avg:91.62ms
step:272/1660 train_time:24920ms step_avg:91.62ms
step:273/1660 train_time:25011ms step_avg:91.62ms
step:274/1660 train_time:25104ms step_avg:91.62ms
step:275/1660 train_time:25196ms step_avg:91.62ms
step:276/1660 train_time:25289ms step_avg:91.63ms
step:277/1660 train_time:25381ms step_avg:91.63ms
step:278/1660 train_time:25473ms step_avg:91.63ms
step:279/1660 train_time:25563ms step_avg:91.62ms
step:280/1660 train_time:25653ms step_avg:91.62ms
step:281/1660 train_time:25744ms step_avg:91.61ms
step:282/1660 train_time:25835ms step_avg:91.61ms
step:283/1660 train_time:25926ms step_avg:91.61ms
step:284/1660 train_time:26017ms step_avg:91.61ms
step:285/1660 train_time:26110ms step_avg:91.61ms
step:286/1660 train_time:26202ms step_avg:91.62ms
step:287/1660 train_time:26294ms step_avg:91.62ms
step:288/1660 train_time:26386ms step_avg:91.62ms
step:289/1660 train_time:26478ms step_avg:91.62ms
step:290/1660 train_time:26569ms step_avg:91.62ms
step:291/1660 train_time:26660ms step_avg:91.62ms
step:292/1660 train_time:26751ms step_avg:91.61ms
step:293/1660 train_time:26842ms step_avg:91.61ms
step:294/1660 train_time:26933ms step_avg:91.61ms
step:295/1660 train_time:27024ms step_avg:91.61ms
step:296/1660 train_time:27116ms step_avg:91.61ms
step:297/1660 train_time:27211ms step_avg:91.62ms
step:298/1660 train_time:27304ms step_avg:91.62ms
step:299/1660 train_time:27395ms step_avg:91.62ms
step:300/1660 train_time:27486ms step_avg:91.62ms
step:301/1660 train_time:27577ms step_avg:91.62ms
step:302/1660 train_time:27669ms step_avg:91.62ms
step:303/1660 train_time:27759ms step_avg:91.62ms
step:304/1660 train_time:27851ms step_avg:91.61ms
step:305/1660 train_time:27942ms step_avg:91.61ms
step:306/1660 train_time:28034ms step_avg:91.61ms
step:307/1660 train_time:28126ms step_avg:91.62ms
step:308/1660 train_time:28219ms step_avg:91.62ms
step:309/1660 train_time:28311ms step_avg:91.62ms
step:310/1660 train_time:28403ms step_avg:91.62ms
step:311/1660 train_time:28495ms step_avg:91.62ms
step:312/1660 train_time:28586ms step_avg:91.62ms
step:313/1660 train_time:28677ms step_avg:91.62ms
step:314/1660 train_time:28769ms step_avg:91.62ms
step:315/1660 train_time:28860ms step_avg:91.62ms
step:316/1660 train_time:28951ms step_avg:91.62ms
step:317/1660 train_time:29043ms step_avg:91.62ms
step:318/1660 train_time:29135ms step_avg:91.62ms
step:319/1660 train_time:29227ms step_avg:91.62ms
step:320/1660 train_time:29319ms step_avg:91.62ms
step:321/1660 train_time:29412ms step_avg:91.63ms
step:322/1660 train_time:29504ms step_avg:91.63ms
step:323/1660 train_time:29595ms step_avg:91.63ms
step:324/1660 train_time:29686ms step_avg:91.62ms
step:325/1660 train_time:29777ms step_avg:91.62ms
step:326/1660 train_time:29869ms step_avg:91.62ms
step:327/1660 train_time:29960ms step_avg:91.62ms
step:328/1660 train_time:30052ms step_avg:91.62ms
step:329/1660 train_time:30143ms step_avg:91.62ms
step:330/1660 train_time:30235ms step_avg:91.62ms
step:331/1660 train_time:30327ms step_avg:91.62ms
step:332/1660 train_time:30419ms step_avg:91.62ms
step:333/1660 train_time:30511ms step_avg:91.62ms
step:334/1660 train_time:30602ms step_avg:91.62ms
step:335/1660 train_time:30694ms step_avg:91.62ms
step:336/1660 train_time:30785ms step_avg:91.62ms
step:337/1660 train_time:30876ms step_avg:91.62ms
step:338/1660 train_time:30967ms step_avg:91.62ms
step:339/1660 train_time:31058ms step_avg:91.62ms
step:340/1660 train_time:31150ms step_avg:91.62ms
step:341/1660 train_time:31242ms step_avg:91.62ms
step:342/1660 train_time:31334ms step_avg:91.62ms
step:343/1660 train_time:31425ms step_avg:91.62ms
step:344/1660 train_time:31517ms step_avg:91.62ms
step:345/1660 train_time:31610ms step_avg:91.62ms
step:346/1660 train_time:31701ms step_avg:91.62ms
step:347/1660 train_time:31794ms step_avg:91.62ms
step:348/1660 train_time:31885ms step_avg:91.62ms
step:349/1660 train_time:31977ms step_avg:91.62ms
step:350/1660 train_time:32068ms step_avg:91.62ms
step:351/1660 train_time:32158ms step_avg:91.62ms
step:352/1660 train_time:32250ms step_avg:91.62ms
step:353/1660 train_time:32342ms step_avg:91.62ms
step:354/1660 train_time:32435ms step_avg:91.62ms
step:355/1660 train_time:32526ms step_avg:91.62ms
step:356/1660 train_time:32617ms step_avg:91.62ms
step:357/1660 train_time:32709ms step_avg:91.62ms
step:358/1660 train_time:32800ms step_avg:91.62ms
step:359/1660 train_time:32891ms step_avg:91.62ms
step:360/1660 train_time:32982ms step_avg:91.62ms
step:361/1660 train_time:33074ms step_avg:91.62ms
step:362/1660 train_time:33165ms step_avg:91.62ms
step:363/1660 train_time:33256ms step_avg:91.61ms
step:364/1660 train_time:33347ms step_avg:91.61ms
step:365/1660 train_time:33439ms step_avg:91.61ms
step:366/1660 train_time:33531ms step_avg:91.61ms
step:367/1660 train_time:33622ms step_avg:91.61ms
step:368/1660 train_time:33713ms step_avg:91.61ms
step:369/1660 train_time:33805ms step_avg:91.61ms
step:370/1660 train_time:33897ms step_avg:91.61ms
step:371/1660 train_time:33989ms step_avg:91.61ms
step:372/1660 train_time:34080ms step_avg:91.61ms
step:373/1660 train_time:34172ms step_avg:91.61ms
step:374/1660 train_time:34263ms step_avg:91.61ms
step:375/1660 train_time:34357ms step_avg:91.62ms
step:375/1660 val_loss:3.8119 train_time:34450ms step_avg:91.87ms
step:376/1660 train_time:34472ms step_avg:91.68ms
step:377/1660 train_time:34544ms step_avg:91.63ms
step:378/1660 train_time:34638ms step_avg:91.64ms
step:379/1660 train_time:34729ms step_avg:91.63ms
step:380/1660 train_time:34820ms step_avg:91.63ms
step:381/1660 train_time:34911ms step_avg:91.63ms
step:382/1660 train_time:35001ms step_avg:91.63ms
step:383/1660 train_time:35092ms step_avg:91.62ms
step:384/1660 train_time:35183ms step_avg:91.62ms
step:385/1660 train_time:35274ms step_avg:91.62ms
step:386/1660 train_time:35365ms step_avg:91.62ms
step:387/1660 train_time:35458ms step_avg:91.62ms
step:388/1660 train_time:35551ms step_avg:91.63ms
step:389/1660 train_time:35644ms step_avg:91.63ms
step:390/1660 train_time:35736ms step_avg:91.63ms
step:391/1660 train_time:35826ms step_avg:91.63ms
step:392/1660 train_time:35917ms step_avg:91.63ms
step:393/1660 train_time:36008ms step_avg:91.62ms
step:394/1660 train_time:36099ms step_avg:91.62ms
step:395/1660 train_time:36189ms step_avg:91.62ms
step:396/1660 train_time:36280ms step_avg:91.62ms
step:397/1660 train_time:36371ms step_avg:91.62ms
step:398/1660 train_time:36464ms step_avg:91.62ms
step:399/1660 train_time:36556ms step_avg:91.62ms
step:400/1660 train_time:36648ms step_avg:91.62ms
step:401/1660 train_time:36739ms step_avg:91.62ms
step:402/1660 train_time:36831ms step_avg:91.62ms
step:403/1660 train_time:36922ms step_avg:91.62ms
step:404/1660 train_time:37013ms step_avg:91.62ms
step:405/1660 train_time:37104ms step_avg:91.61ms
step:406/1660 train_time:37195ms step_avg:91.61ms
step:407/1660 train_time:37285ms step_avg:91.61ms
step:408/1660 train_time:37376ms step_avg:91.61ms
step:409/1660 train_time:37467ms step_avg:91.61ms
step:410/1660 train_time:37560ms step_avg:91.61ms
step:411/1660 train_time:37652ms step_avg:91.61ms
step:412/1660 train_time:37744ms step_avg:91.61ms
step:413/1660 train_time:37836ms step_avg:91.61ms
step:414/1660 train_time:37927ms step_avg:91.61ms
step:415/1660 train_time:38018ms step_avg:91.61ms
step:416/1660 train_time:38109ms step_avg:91.61ms
step:417/1660 train_time:38200ms step_avg:91.61ms
step:418/1660 train_time:38291ms step_avg:91.60ms
step:419/1660 train_time:38382ms step_avg:91.60ms
step:420/1660 train_time:38473ms step_avg:91.60ms
step:421/1660 train_time:38564ms step_avg:91.60ms
step:422/1660 train_time:38656ms step_avg:91.60ms
step:423/1660 train_time:38748ms step_avg:91.60ms
step:424/1660 train_time:38840ms step_avg:91.60ms
step:425/1660 train_time:38932ms step_avg:91.61ms
step:426/1660 train_time:39023ms step_avg:91.60ms
step:427/1660 train_time:39114ms step_avg:91.60ms
step:428/1660 train_time:39205ms step_avg:91.60ms
step:429/1660 train_time:39296ms step_avg:91.60ms
step:430/1660 train_time:39387ms step_avg:91.60ms
step:431/1660 train_time:39478ms step_avg:91.60ms
step:432/1660 train_time:39570ms step_avg:91.60ms
step:433/1660 train_time:39662ms step_avg:91.60ms
step:434/1660 train_time:39753ms step_avg:91.60ms
step:435/1660 train_time:39844ms step_avg:91.60ms
step:436/1660 train_time:39936ms step_avg:91.60ms
step:437/1660 train_time:40026ms step_avg:91.59ms
step:438/1660 train_time:40117ms step_avg:91.59ms
step:439/1660 train_time:40210ms step_avg:91.59ms
step:440/1660 train_time:40301ms step_avg:91.59ms
step:441/1660 train_time:40391ms step_avg:91.59ms
step:442/1660 train_time:40482ms step_avg:91.59ms
step:443/1660 train_time:40574ms step_avg:91.59ms
step:444/1660 train_time:40666ms step_avg:91.59ms
step:445/1660 train_time:40757ms step_avg:91.59ms
step:446/1660 train_time:40849ms step_avg:91.59ms
step:447/1660 train_time:40941ms step_avg:91.59ms
step:448/1660 train_time:41032ms step_avg:91.59ms
step:449/1660 train_time:41123ms step_avg:91.59ms
step:450/1660 train_time:41214ms step_avg:91.59ms
step:451/1660 train_time:41305ms step_avg:91.58ms
step:452/1660 train_time:41396ms step_avg:91.58ms
step:453/1660 train_time:41487ms step_avg:91.58ms
step:454/1660 train_time:41578ms step_avg:91.58ms
step:455/1660 train_time:41670ms step_avg:91.58ms
step:456/1660 train_time:41761ms step_avg:91.58ms
step:457/1660 train_time:41854ms step_avg:91.58ms
step:458/1660 train_time:41945ms step_avg:91.58ms
step:459/1660 train_time:42036ms step_avg:91.58ms
step:460/1660 train_time:42127ms step_avg:91.58ms
step:461/1660 train_time:42219ms step_avg:91.58ms
step:462/1660 train_time:42310ms step_avg:91.58ms
step:463/1660 train_time:42401ms step_avg:91.58ms
step:464/1660 train_time:42493ms step_avg:91.58ms
step:465/1660 train_time:42584ms step_avg:91.58ms
step:466/1660 train_time:42675ms step_avg:91.58ms
step:467/1660 train_time:42767ms step_avg:91.58ms
step:468/1660 train_time:42859ms step_avg:91.58ms
step:469/1660 train_time:42950ms step_avg:91.58ms
step:470/1660 train_time:43041ms step_avg:91.58ms
step:471/1660 train_time:43132ms step_avg:91.57ms
step:472/1660 train_time:43223ms step_avg:91.57ms
step:473/1660 train_time:43314ms step_avg:91.57ms
step:474/1660 train_time:43405ms step_avg:91.57ms
step:475/1660 train_time:43496ms step_avg:91.57ms
step:476/1660 train_time:43588ms step_avg:91.57ms
step:477/1660 train_time:43679ms step_avg:91.57ms
step:478/1660 train_time:43770ms step_avg:91.57ms
step:479/1660 train_time:43861ms step_avg:91.57ms
step:480/1660 train_time:43953ms step_avg:91.57ms
step:481/1660 train_time:44045ms step_avg:91.57ms
step:482/1660 train_time:44136ms step_avg:91.57ms
step:483/1660 train_time:44227ms step_avg:91.57ms
step:484/1660 train_time:44318ms step_avg:91.57ms
step:485/1660 train_time:44409ms step_avg:91.57ms
step:486/1660 train_time:44501ms step_avg:91.57ms
step:487/1660 train_time:44593ms step_avg:91.57ms
step:488/1660 train_time:44685ms step_avg:91.57ms
step:489/1660 train_time:44776ms step_avg:91.57ms
step:490/1660 train_time:44868ms step_avg:91.57ms
step:491/1660 train_time:44960ms step_avg:91.57ms
step:492/1660 train_time:45052ms step_avg:91.57ms
step:493/1660 train_time:45143ms step_avg:91.57ms
step:494/1660 train_time:45236ms step_avg:91.57ms
step:495/1660 train_time:45326ms step_avg:91.57ms
step:496/1660 train_time:45418ms step_avg:91.57ms
step:497/1660 train_time:45509ms step_avg:91.57ms
step:498/1660 train_time:45601ms step_avg:91.57ms
step:499/1660 train_time:45692ms step_avg:91.57ms
step:500/1660 train_time:45783ms step_avg:91.57ms
step:500/1660 val_loss:3.7131 train_time:45875ms step_avg:91.75ms
step:501/1660 train_time:45897ms step_avg:91.61ms
step:502/1660 train_time:45972ms step_avg:91.58ms
step:503/1660 train_time:46072ms step_avg:91.59ms
step:504/1660 train_time:46165ms step_avg:91.60ms
step:505/1660 train_time:46255ms step_avg:91.59ms
step:506/1660 train_time:46346ms step_avg:91.59ms
step:507/1660 train_time:46436ms step_avg:91.59ms
step:508/1660 train_time:46527ms step_avg:91.59ms
step:509/1660 train_time:46617ms step_avg:91.58ms
step:510/1660 train_time:46707ms step_avg:91.58ms
step:511/1660 train_time:46798ms step_avg:91.58ms
step:512/1660 train_time:46891ms step_avg:91.58ms
step:513/1660 train_time:46986ms step_avg:91.59ms
step:514/1660 train_time:47081ms step_avg:91.60ms
step:515/1660 train_time:47173ms step_avg:91.60ms
step:516/1660 train_time:47264ms step_avg:91.60ms
step:517/1660 train_time:47354ms step_avg:91.59ms
step:518/1660 train_time:47445ms step_avg:91.59ms
step:519/1660 train_time:47535ms step_avg:91.59ms
step:520/1660 train_time:47626ms step_avg:91.59ms
step:521/1660 train_time:47716ms step_avg:91.59ms
step:522/1660 train_time:47808ms step_avg:91.59ms
step:523/1660 train_time:47900ms step_avg:91.59ms
step:524/1660 train_time:47992ms step_avg:91.59ms
step:525/1660 train_time:48086ms step_avg:91.59ms
step:526/1660 train_time:48177ms step_avg:91.59ms
step:527/1660 train_time:48269ms step_avg:91.59ms
step:528/1660 train_time:48359ms step_avg:91.59ms
step:529/1660 train_time:48451ms step_avg:91.59ms
step:530/1660 train_time:48541ms step_avg:91.59ms
step:531/1660 train_time:48631ms step_avg:91.58ms
step:532/1660 train_time:48722ms step_avg:91.58ms
step:533/1660 train_time:48813ms step_avg:91.58ms
step:534/1660 train_time:48905ms step_avg:91.58ms
step:535/1660 train_time:48998ms step_avg:91.58ms
step:536/1660 train_time:49090ms step_avg:91.59ms
step:537/1660 train_time:49182ms step_avg:91.59ms
step:538/1660 train_time:49273ms step_avg:91.59ms
step:539/1660 train_time:49365ms step_avg:91.59ms
step:540/1660 train_time:49456ms step_avg:91.59ms
step:541/1660 train_time:49547ms step_avg:91.58ms
step:542/1660 train_time:49638ms step_avg:91.58ms
step:543/1660 train_time:49729ms step_avg:91.58ms
step:544/1660 train_time:49820ms step_avg:91.58ms
step:545/1660 train_time:49911ms step_avg:91.58ms
step:546/1660 train_time:50004ms step_avg:91.58ms
step:547/1660 train_time:50096ms step_avg:91.58ms
step:548/1660 train_time:50188ms step_avg:91.58ms
step:549/1660 train_time:50279ms step_avg:91.58ms
step:550/1660 train_time:50370ms step_avg:91.58ms
step:551/1660 train_time:50462ms step_avg:91.58ms
step:552/1660 train_time:50553ms step_avg:91.58ms
step:553/1660 train_time:50643ms step_avg:91.58ms
step:554/1660 train_time:50734ms step_avg:91.58ms
step:555/1660 train_time:50825ms step_avg:91.58ms
step:556/1660 train_time:50917ms step_avg:91.58ms
step:557/1660 train_time:51011ms step_avg:91.58ms
step:558/1660 train_time:51103ms step_avg:91.58ms
step:559/1660 train_time:51196ms step_avg:91.58ms
step:560/1660 train_time:51289ms step_avg:91.59ms
step:561/1660 train_time:51381ms step_avg:91.59ms
step:562/1660 train_time:51473ms step_avg:91.59ms
step:563/1660 train_time:51565ms step_avg:91.59ms
step:564/1660 train_time:51657ms step_avg:91.59ms
step:565/1660 train_time:51749ms step_avg:91.59ms
step:566/1660 train_time:51842ms step_avg:91.59ms
step:567/1660 train_time:51935ms step_avg:91.60ms
step:568/1660 train_time:52028ms step_avg:91.60ms
step:569/1660 train_time:52121ms step_avg:91.60ms
step:570/1660 train_time:52213ms step_avg:91.60ms
step:571/1660 train_time:52307ms step_avg:91.61ms
step:572/1660 train_time:52400ms step_avg:91.61ms
step:573/1660 train_time:52492ms step_avg:91.61ms
step:574/1660 train_time:52585ms step_avg:91.61ms
step:575/1660 train_time:52677ms step_avg:91.61ms
step:576/1660 train_time:52769ms step_avg:91.61ms
step:577/1660 train_time:52862ms step_avg:91.61ms
step:578/1660 train_time:52954ms step_avg:91.62ms
step:579/1660 train_time:53048ms step_avg:91.62ms
step:580/1660 train_time:53140ms step_avg:91.62ms
step:581/1660 train_time:53233ms step_avg:91.62ms
step:582/1660 train_time:53327ms step_avg:91.63ms
step:583/1660 train_time:53420ms step_avg:91.63ms
step:584/1660 train_time:53512ms step_avg:91.63ms
step:585/1660 train_time:53606ms step_avg:91.63ms
step:586/1660 train_time:53698ms step_avg:91.64ms
step:587/1660 train_time:53790ms step_avg:91.64ms
step:588/1660 train_time:53882ms step_avg:91.64ms
step:589/1660 train_time:53974ms step_avg:91.64ms
step:590/1660 train_time:54067ms step_avg:91.64ms
step:591/1660 train_time:54160ms step_avg:91.64ms
step:592/1660 train_time:54253ms step_avg:91.64ms
step:593/1660 train_time:54346ms step_avg:91.65ms
step:594/1660 train_time:54438ms step_avg:91.65ms
step:595/1660 train_time:54531ms step_avg:91.65ms
step:596/1660 train_time:54624ms step_avg:91.65ms
step:597/1660 train_time:54716ms step_avg:91.65ms
step:598/1660 train_time:54809ms step_avg:91.65ms
step:599/1660 train_time:54902ms step_avg:91.66ms
step:600/1660 train_time:54995ms step_avg:91.66ms
step:601/1660 train_time:55088ms step_avg:91.66ms
step:602/1660 train_time:55181ms step_avg:91.66ms
step:603/1660 train_time:55273ms step_avg:91.66ms
step:604/1660 train_time:55366ms step_avg:91.67ms
step:605/1660 train_time:55458ms step_avg:91.67ms
step:606/1660 train_time:55551ms step_avg:91.67ms
step:607/1660 train_time:55644ms step_avg:91.67ms
step:608/1660 train_time:55736ms step_avg:91.67ms
step:609/1660 train_time:55829ms step_avg:91.67ms
step:610/1660 train_time:55921ms step_avg:91.67ms
step:611/1660 train_time:56013ms step_avg:91.67ms
step:612/1660 train_time:56107ms step_avg:91.68ms
step:613/1660 train_time:56199ms step_avg:91.68ms
step:614/1660 train_time:56292ms step_avg:91.68ms
step:615/1660 train_time:56385ms step_avg:91.68ms
step:616/1660 train_time:56478ms step_avg:91.68ms
step:617/1660 train_time:56570ms step_avg:91.69ms
step:618/1660 train_time:56663ms step_avg:91.69ms
step:619/1660 train_time:56755ms step_avg:91.69ms
step:620/1660 train_time:56847ms step_avg:91.69ms
step:621/1660 train_time:56939ms step_avg:91.69ms
step:622/1660 train_time:57032ms step_avg:91.69ms
step:623/1660 train_time:57125ms step_avg:91.69ms
step:624/1660 train_time:57218ms step_avg:91.70ms
step:625/1660 train_time:57310ms step_avg:91.70ms
step:625/1660 val_loss:3.6096 train_time:57404ms step_avg:91.85ms
step:626/1660 train_time:57425ms step_avg:91.73ms
step:627/1660 train_time:57500ms step_avg:91.71ms
step:628/1660 train_time:57600ms step_avg:91.72ms
step:629/1660 train_time:57695ms step_avg:91.73ms
step:630/1660 train_time:57789ms step_avg:91.73ms
step:631/1660 train_time:57881ms step_avg:91.73ms
step:632/1660 train_time:57971ms step_avg:91.73ms
step:633/1660 train_time:58063ms step_avg:91.73ms
step:634/1660 train_time:58154ms step_avg:91.73ms
step:635/1660 train_time:58246ms step_avg:91.73ms
step:636/1660 train_time:58338ms step_avg:91.73ms
step:637/1660 train_time:58431ms step_avg:91.73ms
step:638/1660 train_time:58527ms step_avg:91.73ms
step:639/1660 train_time:58622ms step_avg:91.74ms
step:640/1660 train_time:58716ms step_avg:91.74ms
step:641/1660 train_time:58809ms step_avg:91.75ms
step:642/1660 train_time:58902ms step_avg:91.75ms
step:643/1660 train_time:58994ms step_avg:91.75ms
step:644/1660 train_time:59085ms step_avg:91.75ms
step:645/1660 train_time:59177ms step_avg:91.75ms
step:646/1660 train_time:59269ms step_avg:91.75ms
step:647/1660 train_time:59361ms step_avg:91.75ms
step:648/1660 train_time:59454ms step_avg:91.75ms
step:649/1660 train_time:59549ms step_avg:91.76ms
step:650/1660 train_time:59643ms step_avg:91.76ms
step:651/1660 train_time:59736ms step_avg:91.76ms
step:652/1660 train_time:59830ms step_avg:91.76ms
step:653/1660 train_time:59923ms step_avg:91.77ms
step:654/1660 train_time:60016ms step_avg:91.77ms
step:655/1660 train_time:60107ms step_avg:91.77ms
step:656/1660 train_time:60199ms step_avg:91.77ms
step:657/1660 train_time:60292ms step_avg:91.77ms
step:658/1660 train_time:60384ms step_avg:91.77ms
step:659/1660 train_time:60477ms step_avg:91.77ms
step:660/1660 train_time:60571ms step_avg:91.77ms
step:661/1660 train_time:60664ms step_avg:91.78ms
step:662/1660 train_time:60757ms step_avg:91.78ms
step:663/1660 train_time:60851ms step_avg:91.78ms
step:664/1660 train_time:60944ms step_avg:91.78ms
step:665/1660 train_time:61036ms step_avg:91.78ms
step:666/1660 train_time:61129ms step_avg:91.78ms
step:667/1660 train_time:61221ms step_avg:91.79ms
step:668/1660 train_time:61313ms step_avg:91.79ms
step:669/1660 train_time:61405ms step_avg:91.79ms
step:670/1660 train_time:61498ms step_avg:91.79ms
step:671/1660 train_time:61592ms step_avg:91.79ms
step:672/1660 train_time:61685ms step_avg:91.79ms
step:673/1660 train_time:61777ms step_avg:91.79ms
step:674/1660 train_time:61870ms step_avg:91.80ms
step:675/1660 train_time:61962ms step_avg:91.80ms
step:676/1660 train_time:62055ms step_avg:91.80ms
step:677/1660 train_time:62148ms step_avg:91.80ms
step:678/1660 train_time:62239ms step_avg:91.80ms
step:679/1660 train_time:62331ms step_avg:91.80ms
step:680/1660 train_time:62423ms step_avg:91.80ms
step:681/1660 train_time:62516ms step_avg:91.80ms
step:682/1660 train_time:62609ms step_avg:91.80ms
step:683/1660 train_time:62702ms step_avg:91.80ms
step:684/1660 train_time:62794ms step_avg:91.80ms
step:685/1660 train_time:62887ms step_avg:91.81ms
step:686/1660 train_time:62980ms step_avg:91.81ms
step:687/1660 train_time:63072ms step_avg:91.81ms
step:688/1660 train_time:63163ms step_avg:91.81ms
step:689/1660 train_time:63255ms step_avg:91.81ms
step:690/1660 train_time:63348ms step_avg:91.81ms
step:691/1660 train_time:63441ms step_avg:91.81ms
step:692/1660 train_time:63533ms step_avg:91.81ms
step:693/1660 train_time:63626ms step_avg:91.81ms
step:694/1660 train_time:63719ms step_avg:91.81ms
step:695/1660 train_time:63812ms step_avg:91.82ms
step:696/1660 train_time:63905ms step_avg:91.82ms
step:697/1660 train_time:63998ms step_avg:91.82ms
step:698/1660 train_time:64091ms step_avg:91.82ms
step:699/1660 train_time:64184ms step_avg:91.82ms
step:700/1660 train_time:64276ms step_avg:91.82ms
step:701/1660 train_time:64370ms step_avg:91.83ms
step:702/1660 train_time:64463ms step_avg:91.83ms
step:703/1660 train_time:64555ms step_avg:91.83ms
step:704/1660 train_time:64648ms step_avg:91.83ms
step:705/1660 train_time:64740ms step_avg:91.83ms
step:706/1660 train_time:64834ms step_avg:91.83ms
step:707/1660 train_time:64927ms step_avg:91.83ms
step:708/1660 train_time:65018ms step_avg:91.83ms
step:709/1660 train_time:65111ms step_avg:91.84ms
step:710/1660 train_time:65203ms step_avg:91.84ms
step:711/1660 train_time:65295ms step_avg:91.84ms
step:712/1660 train_time:65388ms step_avg:91.84ms
step:713/1660 train_time:65481ms step_avg:91.84ms
step:714/1660 train_time:65573ms step_avg:91.84ms
step:715/1660 train_time:65665ms step_avg:91.84ms
step:716/1660 train_time:65758ms step_avg:91.84ms
step:717/1660 train_time:65851ms step_avg:91.84ms
step:718/1660 train_time:65943ms step_avg:91.84ms
step:719/1660 train_time:66036ms step_avg:91.84ms
step:720/1660 train_time:66129ms step_avg:91.85ms
step:721/1660 train_time:66221ms step_avg:91.85ms
step:722/1660 train_time:66314ms step_avg:91.85ms
step:723/1660 train_time:66406ms step_avg:91.85ms
step:724/1660 train_time:66498ms step_avg:91.85ms
step:725/1660 train_time:66591ms step_avg:91.85ms
step:726/1660 train_time:66684ms step_avg:91.85ms
step:727/1660 train_time:66777ms step_avg:91.85ms
step:728/1660 train_time:66870ms step_avg:91.85ms
step:729/1660 train_time:66963ms step_avg:91.86ms
step:730/1660 train_time:67056ms step_avg:91.86ms
step:731/1660 train_time:67148ms step_avg:91.86ms
step:732/1660 train_time:67241ms step_avg:91.86ms
step:733/1660 train_time:67333ms step_avg:91.86ms
step:734/1660 train_time:67426ms step_avg:91.86ms
step:735/1660 train_time:67518ms step_avg:91.86ms
step:736/1660 train_time:67611ms step_avg:91.86ms
step:737/1660 train_time:67703ms step_avg:91.86ms
step:738/1660 train_time:67796ms step_avg:91.86ms
step:739/1660 train_time:67890ms step_avg:91.87ms
step:740/1660 train_time:67983ms step_avg:91.87ms
step:741/1660 train_time:68075ms step_avg:91.87ms
step:742/1660 train_time:68167ms step_avg:91.87ms
step:743/1660 train_time:68260ms step_avg:91.87ms
step:744/1660 train_time:68352ms step_avg:91.87ms
step:745/1660 train_time:68445ms step_avg:91.87ms
step:746/1660 train_time:68538ms step_avg:91.87ms
step:747/1660 train_time:68630ms step_avg:91.87ms
step:748/1660 train_time:68723ms step_avg:91.88ms
step:749/1660 train_time:68816ms step_avg:91.88ms
step:750/1660 train_time:68909ms step_avg:91.88ms
step:750/1660 val_loss:3.5605 train_time:69003ms step_avg:92.00ms
step:751/1660 train_time:69024ms step_avg:91.91ms
step:752/1660 train_time:69100ms step_avg:91.89ms
step:753/1660 train_time:69200ms step_avg:91.90ms
step:754/1660 train_time:69292ms step_avg:91.90ms
step:755/1660 train_time:69383ms step_avg:91.90ms
step:756/1660 train_time:69475ms step_avg:91.90ms
step:757/1660 train_time:69566ms step_avg:91.90ms
step:758/1660 train_time:69658ms step_avg:91.90ms
step:759/1660 train_time:69749ms step_avg:91.90ms
step:760/1660 train_time:69840ms step_avg:91.89ms
step:761/1660 train_time:69932ms step_avg:91.89ms
step:762/1660 train_time:70027ms step_avg:91.90ms
step:763/1660 train_time:70122ms step_avg:91.90ms
step:764/1660 train_time:70217ms step_avg:91.91ms
step:765/1660 train_time:70310ms step_avg:91.91ms
step:766/1660 train_time:70403ms step_avg:91.91ms
step:767/1660 train_time:70495ms step_avg:91.91ms
step:768/1660 train_time:70587ms step_avg:91.91ms
step:769/1660 train_time:70679ms step_avg:91.91ms
step:770/1660 train_time:70770ms step_avg:91.91ms
step:771/1660 train_time:70862ms step_avg:91.91ms
step:772/1660 train_time:70954ms step_avg:91.91ms
step:773/1660 train_time:71049ms step_avg:91.91ms
step:774/1660 train_time:71144ms step_avg:91.92ms
step:775/1660 train_time:71238ms step_avg:91.92ms
step:776/1660 train_time:71330ms step_avg:91.92ms
step:777/1660 train_time:71422ms step_avg:91.92ms
step:778/1660 train_time:71514ms step_avg:91.92ms
step:779/1660 train_time:71607ms step_avg:91.92ms
step:780/1660 train_time:71699ms step_avg:91.92ms
step:781/1660 train_time:71791ms step_avg:91.92ms
step:782/1660 train_time:71883ms step_avg:91.92ms
step:783/1660 train_time:71976ms step_avg:91.92ms
step:784/1660 train_time:72071ms step_avg:91.93ms
step:785/1660 train_time:72165ms step_avg:91.93ms
step:786/1660 train_time:72258ms step_avg:91.93ms
step:787/1660 train_time:72351ms step_avg:91.93ms
step:788/1660 train_time:72443ms step_avg:91.93ms
step:789/1660 train_time:72535ms step_avg:91.93ms
step:790/1660 train_time:72629ms step_avg:91.93ms
step:791/1660 train_time:72720ms step_avg:91.93ms
step:792/1660 train_time:72812ms step_avg:91.93ms
step:793/1660 train_time:72904ms step_avg:91.93ms
step:794/1660 train_time:72996ms step_avg:91.93ms
step:795/1660 train_time:73089ms step_avg:91.94ms
step:796/1660 train_time:73183ms step_avg:91.94ms
step:797/1660 train_time:73276ms step_avg:91.94ms
step:798/1660 train_time:73368ms step_avg:91.94ms
step:799/1660 train_time:73460ms step_avg:91.94ms
step:800/1660 train_time:73552ms step_avg:91.94ms
step:801/1660 train_time:73646ms step_avg:91.94ms
step:802/1660 train_time:73738ms step_avg:91.94ms
step:803/1660 train_time:73830ms step_avg:91.94ms
step:804/1660 train_time:73923ms step_avg:91.94ms
step:805/1660 train_time:74017ms step_avg:91.95ms
step:806/1660 train_time:74111ms step_avg:91.95ms
step:807/1660 train_time:74204ms step_avg:91.95ms
step:808/1660 train_time:74296ms step_avg:91.95ms
step:809/1660 train_time:74389ms step_avg:91.95ms
step:810/1660 train_time:74481ms step_avg:91.95ms
step:811/1660 train_time:74575ms step_avg:91.95ms
step:812/1660 train_time:74667ms step_avg:91.95ms
step:813/1660 train_time:74760ms step_avg:91.96ms
step:814/1660 train_time:74852ms step_avg:91.96ms
step:815/1660 train_time:74945ms step_avg:91.96ms
step:816/1660 train_time:75038ms step_avg:91.96ms
step:817/1660 train_time:75131ms step_avg:91.96ms
step:818/1660 train_time:75225ms step_avg:91.96ms
step:819/1660 train_time:75317ms step_avg:91.96ms
step:820/1660 train_time:75409ms step_avg:91.96ms
step:821/1660 train_time:75501ms step_avg:91.96ms
step:822/1660 train_time:75594ms step_avg:91.96ms
step:823/1660 train_time:75687ms step_avg:91.96ms
step:824/1660 train_time:75779ms step_avg:91.97ms
step:825/1660 train_time:75872ms step_avg:91.97ms
step:826/1660 train_time:75964ms step_avg:91.97ms
step:827/1660 train_time:76057ms step_avg:91.97ms
step:828/1660 train_time:76150ms step_avg:91.97ms
step:829/1660 train_time:76243ms step_avg:91.97ms
step:830/1660 train_time:76335ms step_avg:91.97ms
step:831/1660 train_time:76428ms step_avg:91.97ms
step:832/1660 train_time:76522ms step_avg:91.97ms
step:833/1660 train_time:76614ms step_avg:91.97ms
step:834/1660 train_time:76707ms step_avg:91.97ms
step:835/1660 train_time:76798ms step_avg:91.97ms
step:836/1660 train_time:76891ms step_avg:91.97ms
step:837/1660 train_time:76983ms step_avg:91.97ms
step:838/1660 train_time:77075ms step_avg:91.98ms
step:839/1660 train_time:77169ms step_avg:91.98ms
step:840/1660 train_time:77262ms step_avg:91.98ms
step:841/1660 train_time:77354ms step_avg:91.98ms
step:842/1660 train_time:77447ms step_avg:91.98ms
step:843/1660 train_time:77540ms step_avg:91.98ms
step:844/1660 train_time:77632ms step_avg:91.98ms
step:845/1660 train_time:77725ms step_avg:91.98ms
step:846/1660 train_time:77817ms step_avg:91.98ms
step:847/1660 train_time:77910ms step_avg:91.98ms
step:848/1660 train_time:78002ms step_avg:91.98ms
step:849/1660 train_time:78095ms step_avg:91.98ms
step:850/1660 train_time:78188ms step_avg:91.99ms
step:851/1660 train_time:78281ms step_avg:91.99ms
step:852/1660 train_time:78374ms step_avg:91.99ms
step:853/1660 train_time:78466ms step_avg:91.99ms
step:854/1660 train_time:78559ms step_avg:91.99ms
step:855/1660 train_time:78652ms step_avg:91.99ms
step:856/1660 train_time:78744ms step_avg:91.99ms
step:857/1660 train_time:78836ms step_avg:91.99ms
step:858/1660 train_time:78931ms step_avg:91.99ms
step:859/1660 train_time:79024ms step_avg:91.99ms
step:860/1660 train_time:79115ms step_avg:91.99ms
step:861/1660 train_time:79209ms step_avg:92.00ms
step:862/1660 train_time:79302ms step_avg:92.00ms
step:863/1660 train_time:79395ms step_avg:92.00ms
step:864/1660 train_time:79488ms step_avg:92.00ms
step:865/1660 train_time:79580ms step_avg:92.00ms
step:866/1660 train_time:79672ms step_avg:92.00ms
step:867/1660 train_time:79765ms step_avg:92.00ms
step:868/1660 train_time:79857ms step_avg:92.00ms
step:869/1660 train_time:79950ms step_avg:92.00ms
step:870/1660 train_time:80042ms step_avg:92.00ms
step:871/1660 train_time:80135ms step_avg:92.00ms
step:872/1660 train_time:80228ms step_avg:92.00ms
step:873/1660 train_time:80321ms step_avg:92.01ms
step:874/1660 train_time:80413ms step_avg:92.01ms
step:875/1660 train_time:80505ms step_avg:92.01ms
step:875/1660 val_loss:3.5144 train_time:80599ms step_avg:92.11ms
step:876/1660 train_time:80620ms step_avg:92.03ms
step:877/1660 train_time:80695ms step_avg:92.01ms
step:878/1660 train_time:80791ms step_avg:92.02ms
step:879/1660 train_time:80885ms step_avg:92.02ms
step:880/1660 train_time:80977ms step_avg:92.02ms
step:881/1660 train_time:81069ms step_avg:92.02ms
step:882/1660 train_time:81160ms step_avg:92.02ms
step:883/1660 train_time:81251ms step_avg:92.02ms
step:884/1660 train_time:81343ms step_avg:92.02ms
step:885/1660 train_time:81434ms step_avg:92.02ms
step:886/1660 train_time:81528ms step_avg:92.02ms
step:887/1660 train_time:81621ms step_avg:92.02ms
step:888/1660 train_time:81716ms step_avg:92.02ms
step:889/1660 train_time:81810ms step_avg:92.02ms
step:890/1660 train_time:81904ms step_avg:92.03ms
step:891/1660 train_time:81996ms step_avg:92.03ms
step:892/1660 train_time:82089ms step_avg:92.03ms
step:893/1660 train_time:82181ms step_avg:92.03ms
step:894/1660 train_time:82273ms step_avg:92.03ms
step:895/1660 train_time:82365ms step_avg:92.03ms
step:896/1660 train_time:82457ms step_avg:92.03ms
step:897/1660 train_time:82550ms step_avg:92.03ms
step:898/1660 train_time:82644ms step_avg:92.03ms
step:899/1660 train_time:82737ms step_avg:92.03ms
step:900/1660 train_time:82831ms step_avg:92.03ms
step:901/1660 train_time:82924ms step_avg:92.04ms
step:902/1660 train_time:83017ms step_avg:92.04ms
step:903/1660 train_time:83110ms step_avg:92.04ms
step:904/1660 train_time:83202ms step_avg:92.04ms
step:905/1660 train_time:83294ms step_avg:92.04ms
step:906/1660 train_time:83386ms step_avg:92.04ms
step:907/1660 train_time:83477ms step_avg:92.04ms
step:908/1660 train_time:83570ms step_avg:92.04ms
step:909/1660 train_time:83663ms step_avg:92.04ms
step:910/1660 train_time:83757ms step_avg:92.04ms
step:911/1660 train_time:83851ms step_avg:92.04ms
step:912/1660 train_time:83944ms step_avg:92.04ms
step:913/1660 train_time:84036ms step_avg:92.04ms
step:914/1660 train_time:84129ms step_avg:92.04ms
step:915/1660 train_time:84221ms step_avg:92.05ms
step:916/1660 train_time:84313ms step_avg:92.04ms
step:917/1660 train_time:84405ms step_avg:92.04ms
step:918/1660 train_time:84496ms step_avg:92.04ms
step:919/1660 train_time:84589ms step_avg:92.04ms
step:920/1660 train_time:84681ms step_avg:92.05ms
step:921/1660 train_time:84774ms step_avg:92.05ms
step:922/1660 train_time:84868ms step_avg:92.05ms
step:923/1660 train_time:84961ms step_avg:92.05ms
step:924/1660 train_time:85053ms step_avg:92.05ms
step:925/1660 train_time:85146ms step_avg:92.05ms
step:926/1660 train_time:85238ms step_avg:92.05ms
step:927/1660 train_time:85331ms step_avg:92.05ms
step:928/1660 train_time:85423ms step_avg:92.05ms
step:929/1660 train_time:85515ms step_avg:92.05ms
step:930/1660 train_time:85608ms step_avg:92.05ms
step:931/1660 train_time:85701ms step_avg:92.05ms
step:932/1660 train_time:85793ms step_avg:92.05ms
step:933/1660 train_time:85887ms step_avg:92.05ms
step:934/1660 train_time:85979ms step_avg:92.05ms
step:935/1660 train_time:86072ms step_avg:92.06ms
step:936/1660 train_time:86165ms step_avg:92.06ms
step:937/1660 train_time:86258ms step_avg:92.06ms
step:938/1660 train_time:86351ms step_avg:92.06ms
step:939/1660 train_time:86443ms step_avg:92.06ms
step:940/1660 train_time:86535ms step_avg:92.06ms
step:941/1660 train_time:86628ms step_avg:92.06ms
step:942/1660 train_time:86721ms step_avg:92.06ms
step:943/1660 train_time:86814ms step_avg:92.06ms
step:944/1660 train_time:86906ms step_avg:92.06ms
step:945/1660 train_time:86998ms step_avg:92.06ms
step:946/1660 train_time:87091ms step_avg:92.06ms
step:947/1660 train_time:87184ms step_avg:92.06ms
step:948/1660 train_time:87276ms step_avg:92.06ms
step:949/1660 train_time:87369ms step_avg:92.06ms
step:950/1660 train_time:87462ms step_avg:92.07ms
step:951/1660 train_time:87555ms step_avg:92.07ms
step:952/1660 train_time:87647ms step_avg:92.07ms
step:953/1660 train_time:87741ms step_avg:92.07ms
step:954/1660 train_time:87833ms step_avg:92.07ms
step:955/1660 train_time:87925ms step_avg:92.07ms
step:956/1660 train_time:88019ms step_avg:92.07ms
step:957/1660 train_time:88112ms step_avg:92.07ms
step:958/1660 train_time:88204ms step_avg:92.07ms
step:959/1660 train_time:88297ms step_avg:92.07ms
step:960/1660 train_time:88390ms step_avg:92.07ms
step:961/1660 train_time:88482ms step_avg:92.07ms
step:962/1660 train_time:88575ms step_avg:92.07ms
step:963/1660 train_time:88668ms step_avg:92.07ms
step:964/1660 train_time:88761ms step_avg:92.08ms
step:965/1660 train_time:88854ms step_avg:92.08ms
step:966/1660 train_time:88947ms step_avg:92.08ms
step:967/1660 train_time:89040ms step_avg:92.08ms
step:968/1660 train_time:89133ms step_avg:92.08ms
step:969/1660 train_time:89226ms step_avg:92.08ms
step:970/1660 train_time:89319ms step_avg:92.08ms
step:971/1660 train_time:89412ms step_avg:92.08ms
step:972/1660 train_time:89504ms step_avg:92.08ms
step:973/1660 train_time:89596ms step_avg:92.08ms
step:974/1660 train_time:89689ms step_avg:92.08ms
step:975/1660 train_time:89780ms step_avg:92.08ms
step:976/1660 train_time:89872ms step_avg:92.08ms
step:977/1660 train_time:89965ms step_avg:92.08ms
step:978/1660 train_time:90058ms step_avg:92.08ms
step:979/1660 train_time:90151ms step_avg:92.08ms
step:980/1660 train_time:90244ms step_avg:92.09ms
step:981/1660 train_time:90336ms step_avg:92.09ms
step:982/1660 train_time:90430ms step_avg:92.09ms
step:983/1660 train_time:90522ms step_avg:92.09ms
step:984/1660 train_time:90615ms step_avg:92.09ms
step:985/1660 train_time:90708ms step_avg:92.09ms
step:986/1660 train_time:90801ms step_avg:92.09ms
step:987/1660 train_time:90893ms step_avg:92.09ms
step:988/1660 train_time:90985ms step_avg:92.09ms
step:989/1660 train_time:91078ms step_avg:92.09ms
step:990/1660 train_time:91171ms step_avg:92.09ms
step:991/1660 train_time:91264ms step_avg:92.09ms
step:992/1660 train_time:91356ms step_avg:92.09ms
step:993/1660 train_time:91449ms step_avg:92.09ms
step:994/1660 train_time:91542ms step_avg:92.09ms
step:995/1660 train_time:91634ms step_avg:92.09ms
step:996/1660 train_time:91727ms step_avg:92.09ms
step:997/1660 train_time:91819ms step_avg:92.09ms
step:998/1660 train_time:91911ms step_avg:92.10ms
step:999/1660 train_time:92004ms step_avg:92.10ms
step:1000/1660 train_time:92096ms step_avg:92.10ms
step:1000/1660 val_loss:3.4656 train_time:92191ms step_avg:92.19ms
step:1001/1660 train_time:92213ms step_avg:92.12ms
step:1002/1660 train_time:92291ms step_avg:92.11ms
step:1003/1660 train_time:92386ms step_avg:92.11ms
step:1004/1660 train_time:92480ms step_avg:92.11ms
step:1005/1660 train_time:92572ms step_avg:92.11ms
step:1006/1660 train_time:92663ms step_avg:92.11ms
step:1007/1660 train_time:92755ms step_avg:92.11ms
step:1008/1660 train_time:92846ms step_avg:92.11ms
step:1009/1660 train_time:92938ms step_avg:92.11ms
step:1010/1660 train_time:93029ms step_avg:92.11ms
step:1011/1660 train_time:93121ms step_avg:92.11ms
step:1012/1660 train_time:93217ms step_avg:92.11ms
step:1013/1660 train_time:93312ms step_avg:92.11ms
step:1014/1660 train_time:93407ms step_avg:92.12ms
step:1015/1660 train_time:93501ms step_avg:92.12ms
step:1016/1660 train_time:93592ms step_avg:92.12ms
step:1017/1660 train_time:93684ms step_avg:92.12ms
step:1018/1660 train_time:93775ms step_avg:92.12ms
step:1019/1660 train_time:93867ms step_avg:92.12ms
step:1020/1660 train_time:93958ms step_avg:92.12ms
step:1021/1660 train_time:94050ms step_avg:92.12ms
step:1022/1660 train_time:94143ms step_avg:92.12ms
step:1023/1660 train_time:94237ms step_avg:92.12ms
step:1024/1660 train_time:94331ms step_avg:92.12ms
step:1025/1660 train_time:94425ms step_avg:92.12ms
step:1026/1660 train_time:94517ms step_avg:92.12ms
step:1027/1660 train_time:94610ms step_avg:92.12ms
step:1028/1660 train_time:94703ms step_avg:92.12ms
step:1029/1660 train_time:94795ms step_avg:92.12ms
step:1030/1660 train_time:94887ms step_avg:92.12ms
step:1031/1660 train_time:94978ms step_avg:92.12ms
step:1032/1660 train_time:95070ms step_avg:92.12ms
step:1033/1660 train_time:95163ms step_avg:92.12ms
step:1034/1660 train_time:95257ms step_avg:92.12ms
step:1035/1660 train_time:95350ms step_avg:92.13ms
step:1036/1660 train_time:95444ms step_avg:92.13ms
step:1037/1660 train_time:95537ms step_avg:92.13ms
step:1038/1660 train_time:95631ms step_avg:92.13ms
step:1039/1660 train_time:95724ms step_avg:92.13ms
step:1040/1660 train_time:95815ms step_avg:92.13ms
step:1041/1660 train_time:95909ms step_avg:92.13ms
step:1042/1660 train_time:96001ms step_avg:92.13ms
step:1043/1660 train_time:96093ms step_avg:92.13ms
step:1044/1660 train_time:96185ms step_avg:92.13ms
step:1045/1660 train_time:96279ms step_avg:92.13ms
step:1046/1660 train_time:96372ms step_avg:92.13ms
step:1047/1660 train_time:96466ms step_avg:92.14ms
step:1048/1660 train_time:96559ms step_avg:92.14ms
step:1049/1660 train_time:96652ms step_avg:92.14ms
step:1050/1660 train_time:96745ms step_avg:92.14ms
step:1051/1660 train_time:96837ms step_avg:92.14ms
step:1052/1660 train_time:96930ms step_avg:92.14ms
step:1053/1660 train_time:97022ms step_avg:92.14ms
step:1054/1660 train_time:97114ms step_avg:92.14ms
step:1055/1660 train_time:97206ms step_avg:92.14ms
step:1056/1660 train_time:97299ms step_avg:92.14ms
step:1057/1660 train_time:97393ms step_avg:92.14ms
step:1058/1660 train_time:97485ms step_avg:92.14ms
step:1059/1660 train_time:97578ms step_avg:92.14ms
step:1060/1660 train_time:97670ms step_avg:92.14ms
step:1061/1660 train_time:97763ms step_avg:92.14ms
step:1062/1660 train_time:97856ms step_avg:92.14ms
step:1063/1660 train_time:97949ms step_avg:92.14ms
step:1064/1660 train_time:98041ms step_avg:92.14ms
step:1065/1660 train_time:98133ms step_avg:92.14ms
step:1066/1660 train_time:98225ms step_avg:92.14ms
step:1067/1660 train_time:98318ms step_avg:92.14ms
step:1068/1660 train_time:98412ms step_avg:92.15ms
step:1069/1660 train_time:98505ms step_avg:92.15ms
step:1070/1660 train_time:98597ms step_avg:92.15ms
step:1071/1660 train_time:98690ms step_avg:92.15ms
step:1072/1660 train_time:98783ms step_avg:92.15ms
step:1073/1660 train_time:98875ms step_avg:92.15ms
step:1074/1660 train_time:98968ms step_avg:92.15ms
step:1075/1660 train_time:99061ms step_avg:92.15ms
step:1076/1660 train_time:99153ms step_avg:92.15ms
step:1077/1660 train_time:99245ms step_avg:92.15ms
step:1078/1660 train_time:99338ms step_avg:92.15ms
step:1079/1660 train_time:99431ms step_avg:92.15ms
step:1080/1660 train_time:99524ms step_avg:92.15ms
step:1081/1660 train_time:99617ms step_avg:92.15ms
step:1082/1660 train_time:99711ms step_avg:92.15ms
step:1083/1660 train_time:99803ms step_avg:92.15ms
step:1084/1660 train_time:99895ms step_avg:92.15ms
step:1085/1660 train_time:99988ms step_avg:92.15ms
step:1086/1660 train_time:100080ms step_avg:92.15ms
step:1087/1660 train_time:100172ms step_avg:92.15ms
step:1088/1660 train_time:100265ms step_avg:92.16ms
step:1089/1660 train_time:100357ms step_avg:92.16ms
step:1090/1660 train_time:100451ms step_avg:92.16ms
step:1091/1660 train_time:100543ms step_avg:92.16ms
step:1092/1660 train_time:100636ms step_avg:92.16ms
step:1093/1660 train_time:100730ms step_avg:92.16ms
step:1094/1660 train_time:100822ms step_avg:92.16ms
step:1095/1660 train_time:100914ms step_avg:92.16ms
step:1096/1660 train_time:101008ms step_avg:92.16ms
step:1097/1660 train_time:101100ms step_avg:92.16ms
step:1098/1660 train_time:101192ms step_avg:92.16ms
step:1099/1660 train_time:101285ms step_avg:92.16ms
step:1100/1660 train_time:101376ms step_avg:92.16ms
step:1101/1660 train_time:101470ms step_avg:92.16ms
step:1102/1660 train_time:101563ms step_avg:92.16ms
step:1103/1660 train_time:101656ms step_avg:92.16ms
step:1104/1660 train_time:101749ms step_avg:92.16ms
step:1105/1660 train_time:101841ms step_avg:92.16ms
step:1106/1660 train_time:101934ms step_avg:92.16ms
step:1107/1660 train_time:102027ms step_avg:92.17ms
step:1108/1660 train_time:102119ms step_avg:92.17ms
step:1109/1660 train_time:102212ms step_avg:92.17ms
step:1110/1660 train_time:102305ms step_avg:92.17ms
step:1111/1660 train_time:102399ms step_avg:92.17ms
step:1112/1660 train_time:102492ms step_avg:92.17ms
step:1113/1660 train_time:102586ms step_avg:92.17ms
step:1114/1660 train_time:102679ms step_avg:92.17ms
step:1115/1660 train_time:102774ms step_avg:92.17ms
step:1116/1660 train_time:102868ms step_avg:92.18ms
step:1117/1660 train_time:102961ms step_avg:92.18ms
step:1118/1660 train_time:103055ms step_avg:92.18ms
step:1119/1660 train_time:103149ms step_avg:92.18ms
step:1120/1660 train_time:103241ms step_avg:92.18ms
step:1121/1660 train_time:103335ms step_avg:92.18ms
step:1122/1660 train_time:103428ms step_avg:92.18ms
step:1123/1660 train_time:103521ms step_avg:92.18ms
step:1124/1660 train_time:103614ms step_avg:92.18ms
step:1125/1660 train_time:103707ms step_avg:92.18ms
step:1125/1660 val_loss:3.4124 train_time:103803ms step_avg:92.27ms
step:1126/1660 train_time:103824ms step_avg:92.21ms
step:1127/1660 train_time:103899ms step_avg:92.19ms
step:1128/1660 train_time:103999ms step_avg:92.20ms
step:1129/1660 train_time:104097ms step_avg:92.20ms
step:1130/1660 train_time:104190ms step_avg:92.20ms
step:1131/1660 train_time:104282ms step_avg:92.20ms
step:1132/1660 train_time:104374ms step_avg:92.20ms
step:1133/1660 train_time:104466ms step_avg:92.20ms
step:1134/1660 train_time:104559ms step_avg:92.20ms
step:1135/1660 train_time:104650ms step_avg:92.20ms
step:1136/1660 train_time:104742ms step_avg:92.20ms
step:1137/1660 train_time:104837ms step_avg:92.20ms
step:1138/1660 train_time:104935ms step_avg:92.21ms
step:1139/1660 train_time:105030ms step_avg:92.21ms
step:1140/1660 train_time:105124ms step_avg:92.21ms
step:1141/1660 train_time:105218ms step_avg:92.22ms
step:1142/1660 train_time:105310ms step_avg:92.22ms
step:1143/1660 train_time:105403ms step_avg:92.22ms
step:1144/1660 train_time:105495ms step_avg:92.22ms
step:1145/1660 train_time:105587ms step_avg:92.22ms
step:1146/1660 train_time:105679ms step_avg:92.22ms
step:1147/1660 train_time:105771ms step_avg:92.22ms
step:1148/1660 train_time:105866ms step_avg:92.22ms
step:1149/1660 train_time:105962ms step_avg:92.22ms
step:1150/1660 train_time:106056ms step_avg:92.22ms
step:1151/1660 train_time:106150ms step_avg:92.22ms
step:1152/1660 train_time:106243ms step_avg:92.23ms
step:1153/1660 train_time:106336ms step_avg:92.23ms
step:1154/1660 train_time:106429ms step_avg:92.23ms
step:1155/1660 train_time:106523ms step_avg:92.23ms
step:1156/1660 train_time:106616ms step_avg:92.23ms
step:1157/1660 train_time:106708ms step_avg:92.23ms
step:1158/1660 train_time:106801ms step_avg:92.23ms
step:1159/1660 train_time:106895ms step_avg:92.23ms
step:1160/1660 train_time:106989ms step_avg:92.23ms
step:1161/1660 train_time:107085ms step_avg:92.24ms
step:1162/1660 train_time:107179ms step_avg:92.24ms
step:1163/1660 train_time:107272ms step_avg:92.24ms
step:1164/1660 train_time:107365ms step_avg:92.24ms
step:1165/1660 train_time:107459ms step_avg:92.24ms
step:1166/1660 train_time:107552ms step_avg:92.24ms
step:1167/1660 train_time:107644ms step_avg:92.24ms
step:1168/1660 train_time:107737ms step_avg:92.24ms
step:1169/1660 train_time:107830ms step_avg:92.24ms
step:1170/1660 train_time:107925ms step_avg:92.24ms
step:1171/1660 train_time:108019ms step_avg:92.24ms
step:1172/1660 train_time:108113ms step_avg:92.25ms
step:1173/1660 train_time:108207ms step_avg:92.25ms
step:1174/1660 train_time:108300ms step_avg:92.25ms
step:1175/1660 train_time:108393ms step_avg:92.25ms
step:1176/1660 train_time:108485ms step_avg:92.25ms
step:1177/1660 train_time:108578ms step_avg:92.25ms
step:1178/1660 train_time:108672ms step_avg:92.25ms
step:1179/1660 train_time:108765ms step_avg:92.25ms
step:1180/1660 train_time:108857ms step_avg:92.25ms
step:1181/1660 train_time:108950ms step_avg:92.25ms
step:1182/1660 train_time:109044ms step_avg:92.25ms
step:1183/1660 train_time:109138ms step_avg:92.26ms
step:1184/1660 train_time:109231ms step_avg:92.26ms
step:1185/1660 train_time:109325ms step_avg:92.26ms
step:1186/1660 train_time:109418ms step_avg:92.26ms
step:1187/1660 train_time:109510ms step_avg:92.26ms
step:1188/1660 train_time:109605ms step_avg:92.26ms
step:1189/1660 train_time:109700ms step_avg:92.26ms
step:1190/1660 train_time:109792ms step_avg:92.26ms
step:1191/1660 train_time:109886ms step_avg:92.26ms
step:1192/1660 train_time:109979ms step_avg:92.26ms
step:1193/1660 train_time:110072ms step_avg:92.26ms
step:1194/1660 train_time:110165ms step_avg:92.27ms
step:1195/1660 train_time:110258ms step_avg:92.27ms
step:1196/1660 train_time:110352ms step_avg:92.27ms
step:1197/1660 train_time:110445ms step_avg:92.27ms
step:1198/1660 train_time:110538ms step_avg:92.27ms
step:1199/1660 train_time:110631ms step_avg:92.27ms
step:1200/1660 train_time:110724ms step_avg:92.27ms
step:1201/1660 train_time:110817ms step_avg:92.27ms
step:1202/1660 train_time:110910ms step_avg:92.27ms
step:1203/1660 train_time:111005ms step_avg:92.27ms
step:1204/1660 train_time:111098ms step_avg:92.27ms
step:1205/1660 train_time:111191ms step_avg:92.28ms
step:1206/1660 train_time:111285ms step_avg:92.28ms
step:1207/1660 train_time:111378ms step_avg:92.28ms
step:1208/1660 train_time:111471ms step_avg:92.28ms
step:1209/1660 train_time:111564ms step_avg:92.28ms
step:1210/1660 train_time:111657ms step_avg:92.28ms
step:1211/1660 train_time:111749ms step_avg:92.28ms
step:1212/1660 train_time:111843ms step_avg:92.28ms
step:1213/1660 train_time:111936ms step_avg:92.28ms
step:1214/1660 train_time:112029ms step_avg:92.28ms
step:1215/1660 train_time:112122ms step_avg:92.28ms
step:1216/1660 train_time:112215ms step_avg:92.28ms
step:1217/1660 train_time:112308ms step_avg:92.28ms
step:1218/1660 train_time:112403ms step_avg:92.28ms
step:1219/1660 train_time:112498ms step_avg:92.29ms
step:1220/1660 train_time:112590ms step_avg:92.29ms
step:1221/1660 train_time:112684ms step_avg:92.29ms
step:1222/1660 train_time:112776ms step_avg:92.29ms
step:1223/1660 train_time:112870ms step_avg:92.29ms
step:1224/1660 train_time:112964ms step_avg:92.29ms
step:1225/1660 train_time:113058ms step_avg:92.29ms
step:1226/1660 train_time:113151ms step_avg:92.29ms
step:1227/1660 train_time:113244ms step_avg:92.29ms
step:1228/1660 train_time:113336ms step_avg:92.29ms
step:1229/1660 train_time:113429ms step_avg:92.29ms
step:1230/1660 train_time:113523ms step_avg:92.29ms
step:1231/1660 train_time:113616ms step_avg:92.30ms
step:1232/1660 train_time:113709ms step_avg:92.30ms
step:1233/1660 train_time:113803ms step_avg:92.30ms
step:1234/1660 train_time:113897ms step_avg:92.30ms
step:1235/1660 train_time:113990ms step_avg:92.30ms
step:1236/1660 train_time:114083ms step_avg:92.30ms
step:1237/1660 train_time:114177ms step_avg:92.30ms
step:1238/1660 train_time:114270ms step_avg:92.30ms
step:1239/1660 train_time:114365ms step_avg:92.30ms
step:1240/1660 train_time:114458ms step_avg:92.31ms
step:1241/1660 train_time:114552ms step_avg:92.31ms
step:1242/1660 train_time:114645ms step_avg:92.31ms
step:1243/1660 train_time:114738ms step_avg:92.31ms
step:1244/1660 train_time:114831ms step_avg:92.31ms
step:1245/1660 train_time:114926ms step_avg:92.31ms
step:1246/1660 train_time:115019ms step_avg:92.31ms
step:1247/1660 train_time:115112ms step_avg:92.31ms
step:1248/1660 train_time:115206ms step_avg:92.31ms
step:1249/1660 train_time:115299ms step_avg:92.31ms
step:1250/1660 train_time:115392ms step_avg:92.31ms
step:1250/1660 val_loss:3.3741 train_time:115487ms step_avg:92.39ms
step:1251/1660 train_time:115510ms step_avg:92.33ms
step:1252/1660 train_time:115586ms step_avg:92.32ms
step:1253/1660 train_time:115687ms step_avg:92.33ms
step:1254/1660 train_time:115781ms step_avg:92.33ms
step:1255/1660 train_time:115873ms step_avg:92.33ms
step:1256/1660 train_time:115965ms step_avg:92.33ms
step:1257/1660 train_time:116057ms step_avg:92.33ms
step:1258/1660 train_time:116149ms step_avg:92.33ms
step:1259/1660 train_time:116242ms step_avg:92.33ms
step:1260/1660 train_time:116334ms step_avg:92.33ms
step:1261/1660 train_time:116427ms step_avg:92.33ms
step:1262/1660 train_time:116522ms step_avg:92.33ms
step:1263/1660 train_time:116619ms step_avg:92.33ms
step:1264/1660 train_time:116716ms step_avg:92.34ms
step:1265/1660 train_time:116809ms step_avg:92.34ms
step:1266/1660 train_time:116902ms step_avg:92.34ms
step:1267/1660 train_time:116995ms step_avg:92.34ms
step:1268/1660 train_time:117087ms step_avg:92.34ms
step:1269/1660 train_time:117180ms step_avg:92.34ms
step:1270/1660 train_time:117273ms step_avg:92.34ms
step:1271/1660 train_time:117365ms step_avg:92.34ms
step:1272/1660 train_time:117458ms step_avg:92.34ms
step:1273/1660 train_time:117552ms step_avg:92.34ms
step:1274/1660 train_time:117649ms step_avg:92.35ms
step:1275/1660 train_time:117744ms step_avg:92.35ms
step:1276/1660 train_time:117838ms step_avg:92.35ms
step:1277/1660 train_time:117931ms step_avg:92.35ms
step:1278/1660 train_time:118025ms step_avg:92.35ms
step:1279/1660 train_time:118118ms step_avg:92.35ms
step:1280/1660 train_time:118211ms step_avg:92.35ms
step:1281/1660 train_time:118303ms step_avg:92.35ms
step:1282/1660 train_time:118396ms step_avg:92.35ms
step:1283/1660 train_time:118489ms step_avg:92.35ms
step:1284/1660 train_time:118585ms step_avg:92.36ms
step:1285/1660 train_time:118680ms step_avg:92.36ms
step:1286/1660 train_time:118774ms step_avg:92.36ms
step:1287/1660 train_time:118867ms step_avg:92.36ms
step:1288/1660 train_time:118960ms step_avg:92.36ms
step:1289/1660 train_time:119053ms step_avg:92.36ms
step:1290/1660 train_time:119147ms step_avg:92.36ms
step:1291/1660 train_time:119239ms step_avg:92.36ms
step:1292/1660 train_time:119332ms step_avg:92.36ms
step:1293/1660 train_time:119425ms step_avg:92.36ms
step:1294/1660 train_time:119520ms step_avg:92.36ms
step:1295/1660 train_time:119614ms step_avg:92.37ms
step:1296/1660 train_time:119708ms step_avg:92.37ms
step:1297/1660 train_time:119803ms step_avg:92.37ms
step:1298/1660 train_time:119897ms step_avg:92.37ms
step:1299/1660 train_time:119991ms step_avg:92.37ms
step:1300/1660 train_time:120084ms step_avg:92.37ms
step:1301/1660 train_time:120178ms step_avg:92.37ms
step:1302/1660 train_time:120271ms step_avg:92.37ms
step:1303/1660 train_time:120365ms step_avg:92.38ms
step:1304/1660 train_time:120458ms step_avg:92.38ms
step:1305/1660 train_time:120552ms step_avg:92.38ms
step:1306/1660 train_time:120645ms step_avg:92.38ms
step:1307/1660 train_time:120739ms step_avg:92.38ms
step:1308/1660 train_time:120832ms step_avg:92.38ms
step:1309/1660 train_time:120927ms step_avg:92.38ms
step:1310/1660 train_time:121020ms step_avg:92.38ms
step:1311/1660 train_time:121113ms step_avg:92.38ms
step:1312/1660 train_time:121206ms step_avg:92.38ms
step:1313/1660 train_time:121299ms step_avg:92.38ms
step:1314/1660 train_time:121392ms step_avg:92.38ms
step:1315/1660 train_time:121486ms step_avg:92.38ms
step:1316/1660 train_time:121579ms step_avg:92.38ms
step:1317/1660 train_time:121671ms step_avg:92.39ms
step:1318/1660 train_time:121766ms step_avg:92.39ms
step:1319/1660 train_time:121860ms step_avg:92.39ms
step:1320/1660 train_time:121954ms step_avg:92.39ms
step:1321/1660 train_time:122047ms step_avg:92.39ms
step:1322/1660 train_time:122141ms step_avg:92.39ms
step:1323/1660 train_time:122234ms step_avg:92.39ms
step:1324/1660 train_time:122328ms step_avg:92.39ms
step:1325/1660 train_time:122423ms step_avg:92.39ms
step:1326/1660 train_time:122517ms step_avg:92.40ms
step:1327/1660 train_time:122609ms step_avg:92.40ms
step:1328/1660 train_time:122703ms step_avg:92.40ms
step:1329/1660 train_time:122796ms step_avg:92.40ms
step:1330/1660 train_time:122889ms step_avg:92.40ms
step:1331/1660 train_time:122984ms step_avg:92.40ms
step:1332/1660 train_time:123078ms step_avg:92.40ms
step:1333/1660 train_time:123170ms step_avg:92.40ms
step:1334/1660 train_time:123264ms step_avg:92.40ms
step:1335/1660 train_time:123357ms step_avg:92.40ms
step:1336/1660 train_time:123451ms step_avg:92.40ms
step:1337/1660 train_time:123545ms step_avg:92.40ms
step:1338/1660 train_time:123638ms step_avg:92.41ms
step:1339/1660 train_time:123731ms step_avg:92.41ms
step:1340/1660 train_time:123826ms step_avg:92.41ms
step:1341/1660 train_time:123921ms step_avg:92.41ms
step:1342/1660 train_time:124014ms step_avg:92.41ms
step:1343/1660 train_time:124107ms step_avg:92.41ms
step:1344/1660 train_time:124200ms step_avg:92.41ms
step:1345/1660 train_time:124292ms step_avg:92.41ms
step:1346/1660 train_time:124387ms step_avg:92.41ms
step:1347/1660 train_time:124480ms step_avg:92.41ms
step:1348/1660 train_time:124573ms step_avg:92.41ms
step:1349/1660 train_time:124668ms step_avg:92.41ms
step:1350/1660 train_time:124761ms step_avg:92.42ms
step:1351/1660 train_time:124854ms step_avg:92.42ms
step:1352/1660 train_time:124950ms step_avg:92.42ms
step:1353/1660 train_time:125043ms step_avg:92.42ms
step:1354/1660 train_time:125136ms step_avg:92.42ms
step:1355/1660 train_time:125229ms step_avg:92.42ms
step:1356/1660 train_time:125323ms step_avg:92.42ms
step:1357/1660 train_time:125417ms step_avg:92.42ms
step:1358/1660 train_time:125510ms step_avg:92.42ms
step:1359/1660 train_time:125604ms step_avg:92.42ms
step:1360/1660 train_time:125697ms step_avg:92.42ms
step:1361/1660 train_time:125790ms step_avg:92.42ms
step:1362/1660 train_time:125883ms step_avg:92.43ms
step:1363/1660 train_time:125977ms step_avg:92.43ms
step:1364/1660 train_time:126070ms step_avg:92.43ms
step:1365/1660 train_time:126163ms step_avg:92.43ms
step:1366/1660 train_time:126258ms step_avg:92.43ms
step:1367/1660 train_time:126350ms step_avg:92.43ms
step:1368/1660 train_time:126444ms step_avg:92.43ms
step:1369/1660 train_time:126537ms step_avg:92.43ms
step:1370/1660 train_time:126631ms step_avg:92.43ms
step:1371/1660 train_time:126726ms step_avg:92.43ms
step:1372/1660 train_time:126820ms step_avg:92.43ms
step:1373/1660 train_time:126913ms step_avg:92.44ms
step:1374/1660 train_time:127007ms step_avg:92.44ms
step:1375/1660 train_time:127100ms step_avg:92.44ms
step:1375/1660 val_loss:3.3396 train_time:127195ms step_avg:92.51ms
step:1376/1660 train_time:127216ms step_avg:92.45ms
step:1377/1660 train_time:127293ms step_avg:92.44ms
step:1378/1660 train_time:127391ms step_avg:92.45ms
step:1379/1660 train_time:127484ms step_avg:92.45ms
step:1380/1660 train_time:127575ms step_avg:92.45ms
step:1381/1660 train_time:127668ms step_avg:92.45ms
step:1382/1660 train_time:127760ms step_avg:92.45ms
step:1383/1660 train_time:127853ms step_avg:92.45ms
step:1384/1660 train_time:127946ms step_avg:92.45ms
step:1385/1660 train_time:128038ms step_avg:92.45ms
step:1386/1660 train_time:128131ms step_avg:92.45ms
step:1387/1660 train_time:128226ms step_avg:92.45ms
step:1388/1660 train_time:128324ms step_avg:92.45ms
step:1389/1660 train_time:128417ms step_avg:92.45ms
step:1390/1660 train_time:128511ms step_avg:92.45ms
step:1391/1660 train_time:128603ms step_avg:92.45ms
step:1392/1660 train_time:128695ms step_avg:92.45ms
step:1393/1660 train_time:128788ms step_avg:92.45ms
step:1394/1660 train_time:128881ms step_avg:92.45ms
step:1395/1660 train_time:128974ms step_avg:92.45ms
step:1396/1660 train_time:129067ms step_avg:92.46ms
step:1397/1660 train_time:129161ms step_avg:92.46ms
step:1398/1660 train_time:129257ms step_avg:92.46ms
step:1399/1660 train_time:129353ms step_avg:92.46ms
step:1400/1660 train_time:129448ms step_avg:92.46ms
step:1401/1660 train_time:129541ms step_avg:92.46ms
step:1402/1660 train_time:129634ms step_avg:92.46ms
step:1403/1660 train_time:129727ms step_avg:92.46ms
step:1404/1660 train_time:129820ms step_avg:92.46ms
step:1405/1660 train_time:129913ms step_avg:92.46ms
step:1406/1660 train_time:130005ms step_avg:92.46ms
step:1407/1660 train_time:130098ms step_avg:92.47ms
step:1408/1660 train_time:130192ms step_avg:92.47ms
step:1409/1660 train_time:130287ms step_avg:92.47ms
step:1410/1660 train_time:130381ms step_avg:92.47ms
step:1411/1660 train_time:130475ms step_avg:92.47ms
step:1412/1660 train_time:130569ms step_avg:92.47ms
step:1413/1660 train_time:130662ms step_avg:92.47ms
step:1414/1660 train_time:130755ms step_avg:92.47ms
step:1415/1660 train_time:130848ms step_avg:92.47ms
step:1416/1660 train_time:130941ms step_avg:92.47ms
step:1417/1660 train_time:131034ms step_avg:92.47ms
step:1418/1660 train_time:131127ms step_avg:92.47ms
step:1419/1660 train_time:131220ms step_avg:92.47ms
step:1420/1660 train_time:131315ms step_avg:92.48ms
step:1421/1660 train_time:131410ms step_avg:92.48ms
step:1422/1660 train_time:131504ms step_avg:92.48ms
step:1423/1660 train_time:131597ms step_avg:92.48ms
step:1424/1660 train_time:131690ms step_avg:92.48ms
step:1425/1660 train_time:131784ms step_avg:92.48ms
step:1426/1660 train_time:131877ms step_avg:92.48ms
step:1427/1660 train_time:131970ms step_avg:92.48ms
step:1428/1660 train_time:132063ms step_avg:92.48ms
step:1429/1660 train_time:132157ms step_avg:92.48ms
step:1430/1660 train_time:132251ms step_avg:92.48ms
step:1431/1660 train_time:132345ms step_avg:92.48ms
step:1432/1660 train_time:132439ms step_avg:92.49ms
step:1433/1660 train_time:132533ms step_avg:92.49ms
step:1434/1660 train_time:132627ms step_avg:92.49ms
step:1435/1660 train_time:132721ms step_avg:92.49ms
step:1436/1660 train_time:132815ms step_avg:92.49ms
step:1437/1660 train_time:132909ms step_avg:92.49ms
step:1438/1660 train_time:133002ms step_avg:92.49ms
step:1439/1660 train_time:133095ms step_avg:92.49ms
step:1440/1660 train_time:133189ms step_avg:92.49ms
step:1441/1660 train_time:133282ms step_avg:92.49ms
step:1442/1660 train_time:133376ms step_avg:92.49ms
step:1443/1660 train_time:133470ms step_avg:92.49ms
step:1444/1660 train_time:133563ms step_avg:92.50ms
step:1445/1660 train_time:133657ms step_avg:92.50ms
step:1446/1660 train_time:133753ms step_avg:92.50ms
step:1447/1660 train_time:133848ms step_avg:92.50ms
step:1448/1660 train_time:133940ms step_avg:92.50ms
step:1449/1660 train_time:134034ms step_avg:92.50ms
step:1450/1660 train_time:134128ms step_avg:92.50ms
step:1451/1660 train_time:134221ms step_avg:92.50ms
step:1452/1660 train_time:134315ms step_avg:92.50ms
step:1453/1660 train_time:134408ms step_avg:92.50ms
step:1454/1660 train_time:134502ms step_avg:92.50ms
step:1455/1660 train_time:134596ms step_avg:92.51ms
step:1456/1660 train_time:134689ms step_avg:92.51ms
step:1457/1660 train_time:134782ms step_avg:92.51ms
step:1458/1660 train_time:134876ms step_avg:92.51ms
step:1459/1660 train_time:134969ms step_avg:92.51ms
step:1460/1660 train_time:135062ms step_avg:92.51ms
step:1461/1660 train_time:135157ms step_avg:92.51ms
step:1462/1660 train_time:135250ms step_avg:92.51ms
step:1463/1660 train_time:135343ms step_avg:92.51ms
step:1464/1660 train_time:135436ms step_avg:92.51ms
step:1465/1660 train_time:135529ms step_avg:92.51ms
step:1466/1660 train_time:135622ms step_avg:92.51ms
step:1467/1660 train_time:135716ms step_avg:92.51ms
step:1468/1660 train_time:135810ms step_avg:92.51ms
step:1469/1660 train_time:135904ms step_avg:92.51ms
step:1470/1660 train_time:135997ms step_avg:92.52ms
step:1471/1660 train_time:136091ms step_avg:92.52ms
step:1472/1660 train_time:136185ms step_avg:92.52ms
step:1473/1660 train_time:136279ms step_avg:92.52ms
step:1474/1660 train_time:136372ms step_avg:92.52ms
step:1475/1660 train_time:136466ms step_avg:92.52ms
step:1476/1660 train_time:136559ms step_avg:92.52ms
step:1477/1660 train_time:136653ms step_avg:92.52ms
step:1478/1660 train_time:136747ms step_avg:92.52ms
step:1479/1660 train_time:136841ms step_avg:92.52ms
step:1480/1660 train_time:136936ms step_avg:92.52ms
step:1481/1660 train_time:137030ms step_avg:92.53ms
step:1482/1660 train_time:137123ms step_avg:92.53ms
step:1483/1660 train_time:137216ms step_avg:92.53ms
step:1484/1660 train_time:137310ms step_avg:92.53ms
step:1485/1660 train_time:137404ms step_avg:92.53ms
step:1486/1660 train_time:137497ms step_avg:92.53ms
step:1487/1660 train_time:137590ms step_avg:92.53ms
step:1488/1660 train_time:137683ms step_avg:92.53ms
step:1489/1660 train_time:137776ms step_avg:92.53ms
step:1490/1660 train_time:137870ms step_avg:92.53ms
step:1491/1660 train_time:137965ms step_avg:92.53ms
step:1492/1660 train_time:138059ms step_avg:92.53ms
step:1493/1660 train_time:138154ms step_avg:92.53ms
step:1494/1660 train_time:138249ms step_avg:92.54ms
step:1495/1660 train_time:138341ms step_avg:92.54ms
step:1496/1660 train_time:138434ms step_avg:92.54ms
step:1497/1660 train_time:138528ms step_avg:92.54ms
step:1498/1660 train_time:138621ms step_avg:92.54ms
step:1499/1660 train_time:138715ms step_avg:92.54ms
step:1500/1660 train_time:138808ms step_avg:92.54ms
step:1500/1660 val_loss:3.3092 train_time:138903ms step_avg:92.60ms
step:1501/1660 train_time:138925ms step_avg:92.56ms
step:1502/1660 train_time:138998ms step_avg:92.54ms
step:1503/1660 train_time:139096ms step_avg:92.55ms
step:1504/1660 train_time:139189ms step_avg:92.55ms
step:1505/1660 train_time:139284ms step_avg:92.55ms
step:1506/1660 train_time:139376ms step_avg:92.55ms
step:1507/1660 train_time:139468ms step_avg:92.55ms
step:1508/1660 train_time:139561ms step_avg:92.55ms
step:1509/1660 train_time:139654ms step_avg:92.55ms
step:1510/1660 train_time:139746ms step_avg:92.55ms
step:1511/1660 train_time:139839ms step_avg:92.55ms
step:1512/1660 train_time:139934ms step_avg:92.55ms
step:1513/1660 train_time:140030ms step_avg:92.55ms
step:1514/1660 train_time:140124ms step_avg:92.55ms
step:1515/1660 train_time:140218ms step_avg:92.55ms
step:1516/1660 train_time:140312ms step_avg:92.55ms
step:1517/1660 train_time:140405ms step_avg:92.55ms
step:1518/1660 train_time:140498ms step_avg:92.55ms
step:1519/1660 train_time:140590ms step_avg:92.55ms
step:1520/1660 train_time:140682ms step_avg:92.55ms
step:1521/1660 train_time:140775ms step_avg:92.55ms
step:1522/1660 train_time:140869ms step_avg:92.56ms
step:1523/1660 train_time:140963ms step_avg:92.56ms
step:1524/1660 train_time:141056ms step_avg:92.56ms
step:1525/1660 train_time:141152ms step_avg:92.56ms
step:1526/1660 train_time:141245ms step_avg:92.56ms
step:1527/1660 train_time:141339ms step_avg:92.56ms
step:1528/1660 train_time:141432ms step_avg:92.56ms
step:1529/1660 train_time:141525ms step_avg:92.56ms
step:1530/1660 train_time:141618ms step_avg:92.56ms
step:1531/1660 train_time:141711ms step_avg:92.56ms
step:1532/1660 train_time:141805ms step_avg:92.56ms
step:1533/1660 train_time:141899ms step_avg:92.56ms
step:1534/1660 train_time:141992ms step_avg:92.56ms
step:1535/1660 train_time:142087ms step_avg:92.56ms
step:1536/1660 train_time:142181ms step_avg:92.57ms
step:1537/1660 train_time:142275ms step_avg:92.57ms
step:1538/1660 train_time:142369ms step_avg:92.57ms
step:1539/1660 train_time:142463ms step_avg:92.57ms
step:1540/1660 train_time:142556ms step_avg:92.57ms
step:1541/1660 train_time:142649ms step_avg:92.57ms
step:1542/1660 train_time:142743ms step_avg:92.57ms
step:1543/1660 train_time:142837ms step_avg:92.57ms
step:1544/1660 train_time:142930ms step_avg:92.57ms
step:1545/1660 train_time:143024ms step_avg:92.57ms
step:1546/1660 train_time:143118ms step_avg:92.57ms
step:1547/1660 train_time:143212ms step_avg:92.57ms
step:1548/1660 train_time:143305ms step_avg:92.57ms
step:1549/1660 train_time:143398ms step_avg:92.57ms
step:1550/1660 train_time:143491ms step_avg:92.57ms
step:1551/1660 train_time:143586ms step_avg:92.58ms
step:1552/1660 train_time:143679ms step_avg:92.58ms
step:1553/1660 train_time:143773ms step_avg:92.58ms
step:1554/1660 train_time:143866ms step_avg:92.58ms
step:1555/1660 train_time:143960ms step_avg:92.58ms
step:1556/1660 train_time:144055ms step_avg:92.58ms
step:1557/1660 train_time:144149ms step_avg:92.58ms
step:1558/1660 train_time:144243ms step_avg:92.58ms
step:1559/1660 train_time:144337ms step_avg:92.58ms
step:1560/1660 train_time:144431ms step_avg:92.58ms
step:1561/1660 train_time:144523ms step_avg:92.58ms
step:1562/1660 train_time:144616ms step_avg:92.58ms
step:1563/1660 train_time:144709ms step_avg:92.58ms
step:1564/1660 train_time:144803ms step_avg:92.59ms
step:1565/1660 train_time:144896ms step_avg:92.59ms
step:1566/1660 train_time:144990ms step_avg:92.59ms
step:1567/1660 train_time:145084ms step_avg:92.59ms
step:1568/1660 train_time:145179ms step_avg:92.59ms
step:1569/1660 train_time:145273ms step_avg:92.59ms
step:1570/1660 train_time:145367ms step_avg:92.59ms
step:1571/1660 train_time:145461ms step_avg:92.59ms
step:1572/1660 train_time:145553ms step_avg:92.59ms
step:1573/1660 train_time:145647ms step_avg:92.59ms
step:1574/1660 train_time:145740ms step_avg:92.59ms
step:1575/1660 train_time:145833ms step_avg:92.59ms
step:1576/1660 train_time:145926ms step_avg:92.59ms
step:1577/1660 train_time:146019ms step_avg:92.59ms
step:1578/1660 train_time:146113ms step_avg:92.59ms
step:1579/1660 train_time:146206ms step_avg:92.59ms
step:1580/1660 train_time:146300ms step_avg:92.60ms
step:1581/1660 train_time:146394ms step_avg:92.60ms
step:1582/1660 train_time:146487ms step_avg:92.60ms
step:1583/1660 train_time:146581ms step_avg:92.60ms
step:1584/1660 train_time:146675ms step_avg:92.60ms
step:1585/1660 train_time:146768ms step_avg:92.60ms
step:1586/1660 train_time:146862ms step_avg:92.60ms
step:1587/1660 train_time:146956ms step_avg:92.60ms
step:1588/1660 train_time:147050ms step_avg:92.60ms
step:1589/1660 train_time:147144ms step_avg:92.60ms
step:1590/1660 train_time:147237ms step_avg:92.60ms
step:1591/1660 train_time:147330ms step_avg:92.60ms
step:1592/1660 train_time:147423ms step_avg:92.60ms
step:1593/1660 train_time:147517ms step_avg:92.60ms
step:1594/1660 train_time:147610ms step_avg:92.60ms
step:1595/1660 train_time:147703ms step_avg:92.60ms
step:1596/1660 train_time:147796ms step_avg:92.60ms
step:1597/1660 train_time:147890ms step_avg:92.60ms
step:1598/1660 train_time:147983ms step_avg:92.61ms
step:1599/1660 train_time:148077ms step_avg:92.61ms
step:1600/1660 train_time:148170ms step_avg:92.61ms
step:1601/1660 train_time:148264ms step_avg:92.61ms
step:1602/1660 train_time:148357ms step_avg:92.61ms
step:1603/1660 train_time:148450ms step_avg:92.61ms
step:1604/1660 train_time:148544ms step_avg:92.61ms
step:1605/1660 train_time:148638ms step_avg:92.61ms
step:1606/1660 train_time:148730ms step_avg:92.61ms
step:1607/1660 train_time:148823ms step_avg:92.61ms
step:1608/1660 train_time:148917ms step_avg:92.61ms
step:1609/1660 train_time:149010ms step_avg:92.61ms
step:1610/1660 train_time:149105ms step_avg:92.61ms
step:1611/1660 train_time:149199ms step_avg:92.61ms
step:1612/1660 train_time:149292ms step_avg:92.61ms
step:1613/1660 train_time:149385ms step_avg:92.61ms
step:1614/1660 train_time:149479ms step_avg:92.61ms
step:1615/1660 train_time:149572ms step_avg:92.61ms
step:1616/1660 train_time:149665ms step_avg:92.61ms
step:1617/1660 train_time:149758ms step_avg:92.61ms
step:1618/1660 train_time:149852ms step_avg:92.62ms
step:1619/1660 train_time:149945ms step_avg:92.62ms
step:1620/1660 train_time:150039ms step_avg:92.62ms
step:1621/1660 train_time:150132ms step_avg:92.62ms
step:1622/1660 train_time:150226ms step_avg:92.62ms
step:1623/1660 train_time:150320ms step_avg:92.62ms
step:1624/1660 train_time:150414ms step_avg:92.62ms
step:1625/1660 train_time:150507ms step_avg:92.62ms
step:1625/1660 val_loss:3.2849 train_time:150602ms step_avg:92.68ms
step:1626/1660 train_time:150623ms step_avg:92.63ms
step:1627/1660 train_time:150701ms step_avg:92.63ms
step:1628/1660 train_time:150798ms step_avg:92.63ms
step:1629/1660 train_time:150892ms step_avg:92.63ms
step:1630/1660 train_time:150985ms step_avg:92.63ms
step:1631/1660 train_time:151078ms step_avg:92.63ms
step:1632/1660 train_time:151171ms step_avg:92.63ms
step:1633/1660 train_time:151263ms step_avg:92.63ms
step:1634/1660 train_time:151356ms step_avg:92.63ms
step:1635/1660 train_time:151450ms step_avg:92.63ms
step:1636/1660 train_time:151543ms step_avg:92.63ms
step:1637/1660 train_time:151639ms step_avg:92.63ms
step:1638/1660 train_time:151735ms step_avg:92.63ms
step:1639/1660 train_time:151830ms step_avg:92.64ms
step:1640/1660 train_time:151923ms step_avg:92.64ms
step:1641/1660 train_time:152016ms step_avg:92.64ms
step:1642/1660 train_time:152109ms step_avg:92.64ms
step:1643/1660 train_time:152202ms step_avg:92.64ms
step:1644/1660 train_time:152294ms step_avg:92.64ms
step:1645/1660 train_time:152387ms step_avg:92.64ms
step:1646/1660 train_time:152480ms step_avg:92.64ms
step:1647/1660 train_time:152574ms step_avg:92.64ms
step:1648/1660 train_time:152669ms step_avg:92.64ms
step:1649/1660 train_time:152763ms step_avg:92.64ms
step:1650/1660 train_time:152857ms step_avg:92.64ms
step:1651/1660 train_time:152951ms step_avg:92.64ms
step:1652/1660 train_time:153045ms step_avg:92.64ms
step:1653/1660 train_time:153137ms step_avg:92.64ms
step:1654/1660 train_time:153230ms step_avg:92.64ms
step:1655/1660 train_time:153322ms step_avg:92.64ms
step:1656/1660 train_time:153415ms step_avg:92.64ms
step:1657/1660 train_time:153510ms step_avg:92.64ms
step:1658/1660 train_time:153603ms step_avg:92.64ms
step:1659/1660 train_time:153697ms step_avg:92.64ms
step:1660/1660 train_time:153791ms step_avg:92.65ms
step:1660/1660 val_loss:3.2767 train_time:153887ms step_avg:92.70ms
peak memory allocated: 31587 MiB reserved: 47056 MiB

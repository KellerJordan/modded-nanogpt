import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # weight decay
                if wd != 0:
                    mask = (update * p_slice) >= 0
                    # lr as weight decay  schedule
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)

                    update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)
                
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2050  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 18 12:37:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2090 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2090 train_time:85ms step_avg:84.89ms
step:2/2090 train_time:109ms step_avg:54.40ms
step:3/2090 train_time:132ms step_avg:43.90ms
step:4/2090 train_time:164ms step_avg:41.01ms
step:5/2090 train_time:197ms step_avg:39.30ms
step:6/2090 train_time:293ms step_avg:48.81ms
step:7/2090 train_time:312ms step_avg:44.51ms
step:8/2090 train_time:341ms step_avg:42.66ms
step:9/2090 train_time:374ms step_avg:41.58ms
step:10/2090 train_time:407ms step_avg:40.70ms
step:11/2090 train_time:440ms step_avg:40.01ms
step:12/2090 train_time:473ms step_avg:39.42ms
step:13/2090 train_time:506ms step_avg:38.94ms
step:14/2090 train_time:539ms step_avg:38.50ms
step:15/2090 train_time:573ms step_avg:38.17ms
step:16/2090 train_time:605ms step_avg:37.84ms
step:17/2090 train_time:639ms step_avg:37.56ms
step:18/2090 train_time:671ms step_avg:37.30ms
step:19/2090 train_time:705ms step_avg:37.10ms
step:20/2090 train_time:738ms step_avg:36.88ms
step:21/2090 train_time:772ms step_avg:36.74ms
step:22/2090 train_time:805ms step_avg:36.57ms
step:23/2090 train_time:838ms step_avg:36.44ms
step:24/2090 train_time:871ms step_avg:36.29ms
step:25/2090 train_time:904ms step_avg:36.18ms
step:26/2090 train_time:938ms step_avg:36.06ms
step:27/2090 train_time:971ms step_avg:35.97ms
step:28/2090 train_time:1004ms step_avg:35.86ms
step:29/2090 train_time:1037ms step_avg:35.77ms
step:30/2090 train_time:1071ms step_avg:35.69ms
step:31/2090 train_time:1104ms step_avg:35.60ms
step:32/2090 train_time:1137ms step_avg:35.53ms
step:33/2090 train_time:1171ms step_avg:35.47ms
step:34/2090 train_time:1203ms step_avg:35.39ms
step:35/2090 train_time:1238ms step_avg:35.37ms
step:36/2090 train_time:1271ms step_avg:35.30ms
step:37/2090 train_time:1305ms step_avg:35.28ms
step:38/2090 train_time:1338ms step_avg:35.22ms
step:39/2090 train_time:1373ms step_avg:35.20ms
step:40/2090 train_time:1406ms step_avg:35.14ms
step:41/2090 train_time:1439ms step_avg:35.11ms
step:42/2090 train_time:1473ms step_avg:35.06ms
step:43/2090 train_time:1506ms step_avg:35.03ms
step:44/2090 train_time:1539ms step_avg:34.98ms
step:45/2090 train_time:1573ms step_avg:34.95ms
step:46/2090 train_time:1606ms step_avg:34.90ms
step:47/2090 train_time:1639ms step_avg:34.87ms
step:48/2090 train_time:1672ms step_avg:34.83ms
step:49/2090 train_time:1705ms step_avg:34.80ms
step:50/2090 train_time:1738ms step_avg:34.76ms
step:51/2090 train_time:1772ms step_avg:34.75ms
step:52/2090 train_time:1805ms step_avg:34.71ms
step:53/2090 train_time:1838ms step_avg:34.68ms
step:54/2090 train_time:1871ms step_avg:34.65ms
step:55/2090 train_time:1904ms step_avg:34.62ms
step:56/2090 train_time:1937ms step_avg:34.59ms
step:57/2090 train_time:1970ms step_avg:34.56ms
step:58/2090 train_time:2003ms step_avg:34.54ms
step:59/2090 train_time:2036ms step_avg:34.51ms
step:60/2090 train_time:2069ms step_avg:34.48ms
step:61/2090 train_time:2102ms step_avg:34.46ms
step:62/2090 train_time:2135ms step_avg:34.43ms
step:63/2090 train_time:2169ms step_avg:34.43ms
step:64/2090 train_time:2202ms step_avg:34.40ms
step:65/2090 train_time:2235ms step_avg:34.39ms
step:66/2090 train_time:2268ms step_avg:34.37ms
step:67/2090 train_time:2302ms step_avg:34.35ms
step:68/2090 train_time:2334ms step_avg:34.33ms
step:69/2090 train_time:2368ms step_avg:34.32ms
step:70/2090 train_time:2401ms step_avg:34.30ms
step:71/2090 train_time:2435ms step_avg:34.29ms
step:72/2090 train_time:2468ms step_avg:34.27ms
step:73/2090 train_time:2501ms step_avg:34.26ms
step:74/2090 train_time:2534ms step_avg:34.25ms
step:75/2090 train_time:2568ms step_avg:34.24ms
step:76/2090 train_time:2601ms step_avg:34.22ms
step:77/2090 train_time:2634ms step_avg:34.21ms
step:78/2090 train_time:2667ms step_avg:34.19ms
step:79/2090 train_time:2700ms step_avg:34.18ms
step:80/2090 train_time:2733ms step_avg:34.17ms
step:81/2090 train_time:2767ms step_avg:34.16ms
step:82/2090 train_time:2799ms step_avg:34.14ms
step:83/2090 train_time:2833ms step_avg:34.13ms
step:84/2090 train_time:2866ms step_avg:34.12ms
step:85/2090 train_time:2899ms step_avg:34.10ms
step:86/2090 train_time:2932ms step_avg:34.09ms
step:87/2090 train_time:2965ms step_avg:34.08ms
step:88/2090 train_time:2998ms step_avg:34.07ms
step:89/2090 train_time:3031ms step_avg:34.06ms
step:90/2090 train_time:3064ms step_avg:34.05ms
step:91/2090 train_time:3097ms step_avg:34.04ms
step:92/2090 train_time:3130ms step_avg:34.02ms
step:93/2090 train_time:3163ms step_avg:34.01ms
step:94/2090 train_time:3196ms step_avg:34.00ms
step:95/2090 train_time:3230ms step_avg:34.00ms
step:96/2090 train_time:3263ms step_avg:33.98ms
step:97/2090 train_time:3296ms step_avg:33.98ms
step:98/2090 train_time:3329ms step_avg:33.97ms
step:99/2090 train_time:3362ms step_avg:33.96ms
step:100/2090 train_time:3395ms step_avg:33.95ms
step:101/2090 train_time:3428ms step_avg:33.94ms
step:102/2090 train_time:3461ms step_avg:33.93ms
step:103/2090 train_time:3495ms step_avg:33.93ms
step:104/2090 train_time:3528ms step_avg:33.92ms
step:105/2090 train_time:3561ms step_avg:33.91ms
step:106/2090 train_time:3594ms step_avg:33.90ms
step:107/2090 train_time:3627ms step_avg:33.90ms
step:108/2090 train_time:3660ms step_avg:33.89ms
step:109/2090 train_time:3694ms step_avg:33.89ms
step:110/2090 train_time:3727ms step_avg:33.88ms
step:111/2090 train_time:3760ms step_avg:33.87ms
step:112/2090 train_time:3793ms step_avg:33.87ms
step:113/2090 train_time:3826ms step_avg:33.86ms
step:114/2090 train_time:3859ms step_avg:33.85ms
step:115/2090 train_time:3892ms step_avg:33.85ms
step:116/2090 train_time:3925ms step_avg:33.84ms
step:117/2090 train_time:3958ms step_avg:33.83ms
step:118/2090 train_time:3991ms step_avg:33.82ms
step:119/2090 train_time:4024ms step_avg:33.82ms
step:120/2090 train_time:4057ms step_avg:33.81ms
step:121/2090 train_time:4090ms step_avg:33.81ms
step:122/2090 train_time:4124ms step_avg:33.80ms
step:123/2090 train_time:4156ms step_avg:33.79ms
step:124/2090 train_time:4189ms step_avg:33.79ms
step:125/2090 train_time:4222ms step_avg:33.78ms
step:126/2090 train_time:4255ms step_avg:33.77ms
step:127/2090 train_time:4288ms step_avg:33.76ms
step:128/2090 train_time:4321ms step_avg:33.76ms
step:129/2090 train_time:4354ms step_avg:33.75ms
step:130/2090 train_time:4387ms step_avg:33.75ms
step:131/2090 train_time:4421ms step_avg:33.75ms
step:132/2090 train_time:4454ms step_avg:33.74ms
step:133/2090 train_time:4487ms step_avg:33.74ms
step:134/2090 train_time:4520ms step_avg:33.73ms
step:135/2090 train_time:4553ms step_avg:33.73ms
step:136/2090 train_time:4586ms step_avg:33.72ms
step:137/2090 train_time:4620ms step_avg:33.72ms
step:138/2090 train_time:4652ms step_avg:33.71ms
step:139/2090 train_time:4686ms step_avg:33.71ms
step:140/2090 train_time:4718ms step_avg:33.70ms
step:141/2090 train_time:4752ms step_avg:33.70ms
step:142/2090 train_time:4784ms step_avg:33.69ms
step:143/2090 train_time:4818ms step_avg:33.69ms
step:144/2090 train_time:4851ms step_avg:33.69ms
step:145/2090 train_time:4884ms step_avg:33.68ms
step:146/2090 train_time:4917ms step_avg:33.68ms
step:147/2090 train_time:4950ms step_avg:33.68ms
step:148/2090 train_time:4983ms step_avg:33.67ms
step:149/2090 train_time:5016ms step_avg:33.67ms
step:150/2090 train_time:5049ms step_avg:33.66ms
step:151/2090 train_time:5083ms step_avg:33.66ms
step:152/2090 train_time:5115ms step_avg:33.65ms
step:153/2090 train_time:5149ms step_avg:33.65ms
step:154/2090 train_time:5181ms step_avg:33.65ms
step:155/2090 train_time:5215ms step_avg:33.64ms
step:156/2090 train_time:5247ms step_avg:33.64ms
step:157/2090 train_time:5281ms step_avg:33.63ms
step:158/2090 train_time:5313ms step_avg:33.63ms
step:159/2090 train_time:5347ms step_avg:33.63ms
step:160/2090 train_time:5379ms step_avg:33.62ms
step:161/2090 train_time:5412ms step_avg:33.62ms
step:162/2090 train_time:5445ms step_avg:33.61ms
step:163/2090 train_time:5478ms step_avg:33.61ms
step:164/2090 train_time:5511ms step_avg:33.60ms
step:165/2090 train_time:5544ms step_avg:33.60ms
step:166/2090 train_time:5577ms step_avg:33.60ms
step:167/2090 train_time:5610ms step_avg:33.60ms
step:168/2090 train_time:5643ms step_avg:33.59ms
step:169/2090 train_time:5677ms step_avg:33.59ms
step:170/2090 train_time:5709ms step_avg:33.59ms
step:171/2090 train_time:5743ms step_avg:33.58ms
step:172/2090 train_time:5776ms step_avg:33.58ms
step:173/2090 train_time:5809ms step_avg:33.58ms
step:174/2090 train_time:5842ms step_avg:33.58ms
step:175/2090 train_time:5876ms step_avg:33.58ms
step:176/2090 train_time:5909ms step_avg:33.57ms
step:177/2090 train_time:5943ms step_avg:33.57ms
step:178/2090 train_time:5976ms step_avg:33.57ms
step:179/2090 train_time:6009ms step_avg:33.57ms
step:180/2090 train_time:6042ms step_avg:33.56ms
step:181/2090 train_time:6075ms step_avg:33.56ms
step:182/2090 train_time:6108ms step_avg:33.56ms
step:183/2090 train_time:6141ms step_avg:33.56ms
step:184/2090 train_time:6174ms step_avg:33.55ms
step:185/2090 train_time:6207ms step_avg:33.55ms
step:186/2090 train_time:6240ms step_avg:33.55ms
step:187/2090 train_time:6273ms step_avg:33.55ms
step:188/2090 train_time:6306ms step_avg:33.54ms
step:189/2090 train_time:6339ms step_avg:33.54ms
step:190/2090 train_time:6372ms step_avg:33.54ms
step:191/2090 train_time:6405ms step_avg:33.53ms
step:192/2090 train_time:6437ms step_avg:33.53ms
step:193/2090 train_time:6471ms step_avg:33.53ms
step:194/2090 train_time:6503ms step_avg:33.52ms
step:195/2090 train_time:6536ms step_avg:33.52ms
step:196/2090 train_time:6569ms step_avg:33.52ms
step:197/2090 train_time:6602ms step_avg:33.51ms
step:198/2090 train_time:6635ms step_avg:33.51ms
step:199/2090 train_time:6668ms step_avg:33.51ms
step:200/2090 train_time:6701ms step_avg:33.51ms
step:201/2090 train_time:6735ms step_avg:33.51ms
step:202/2090 train_time:6767ms step_avg:33.50ms
step:203/2090 train_time:6801ms step_avg:33.50ms
step:204/2090 train_time:6834ms step_avg:33.50ms
step:205/2090 train_time:6867ms step_avg:33.50ms
step:206/2090 train_time:6900ms step_avg:33.49ms
step:207/2090 train_time:6933ms step_avg:33.49ms
step:208/2090 train_time:6966ms step_avg:33.49ms
step:209/2090 train_time:6999ms step_avg:33.49ms
step:210/2090 train_time:7032ms step_avg:33.48ms
step:211/2090 train_time:7065ms step_avg:33.48ms
step:212/2090 train_time:7098ms step_avg:33.48ms
step:213/2090 train_time:7131ms step_avg:33.48ms
step:214/2090 train_time:7164ms step_avg:33.47ms
step:215/2090 train_time:7197ms step_avg:33.47ms
step:216/2090 train_time:7230ms step_avg:33.47ms
step:217/2090 train_time:7263ms step_avg:33.47ms
step:218/2090 train_time:7296ms step_avg:33.47ms
step:219/2090 train_time:7329ms step_avg:33.47ms
step:220/2090 train_time:7362ms step_avg:33.46ms
step:221/2090 train_time:7395ms step_avg:33.46ms
step:222/2090 train_time:7428ms step_avg:33.46ms
step:223/2090 train_time:7461ms step_avg:33.46ms
step:224/2090 train_time:7494ms step_avg:33.45ms
step:225/2090 train_time:7527ms step_avg:33.45ms
step:226/2090 train_time:7560ms step_avg:33.45ms
step:227/2090 train_time:7593ms step_avg:33.45ms
step:228/2090 train_time:7626ms step_avg:33.45ms
step:229/2090 train_time:7658ms step_avg:33.44ms
step:230/2090 train_time:7691ms step_avg:33.44ms
step:231/2090 train_time:7724ms step_avg:33.44ms
step:232/2090 train_time:7757ms step_avg:33.44ms
step:233/2090 train_time:7790ms step_avg:33.43ms
step:234/2090 train_time:7823ms step_avg:33.43ms
step:235/2090 train_time:7856ms step_avg:33.43ms
step:236/2090 train_time:7889ms step_avg:33.43ms
step:237/2090 train_time:7922ms step_avg:33.43ms
step:238/2090 train_time:7955ms step_avg:33.42ms
step:239/2090 train_time:7989ms step_avg:33.42ms
step:240/2090 train_time:8021ms step_avg:33.42ms
step:241/2090 train_time:8055ms step_avg:33.42ms
step:242/2090 train_time:8088ms step_avg:33.42ms
step:243/2090 train_time:8121ms step_avg:33.42ms
step:244/2090 train_time:8154ms step_avg:33.42ms
step:245/2090 train_time:8187ms step_avg:33.42ms
step:246/2090 train_time:8220ms step_avg:33.41ms
step:247/2090 train_time:8253ms step_avg:33.41ms
step:248/2090 train_time:8286ms step_avg:33.41ms
step:249/2090 train_time:8319ms step_avg:33.41ms
step:250/2090 train_time:8352ms step_avg:33.41ms
step:250/2090 val_loss:4.2699 train_time:8387ms step_avg:33.55ms
step:251/2090 train_time:8408ms step_avg:33.50ms
step:252/2090 train_time:8427ms step_avg:33.44ms
step:253/2090 train_time:8454ms step_avg:33.41ms
step:254/2090 train_time:8487ms step_avg:33.41ms
step:255/2090 train_time:8521ms step_avg:33.42ms
step:256/2090 train_time:8554ms step_avg:33.41ms
step:257/2090 train_time:8587ms step_avg:33.41ms
step:258/2090 train_time:8620ms step_avg:33.41ms
step:259/2090 train_time:8653ms step_avg:33.41ms
step:260/2090 train_time:8686ms step_avg:33.41ms
step:261/2090 train_time:8719ms step_avg:33.41ms
step:262/2090 train_time:8752ms step_avg:33.40ms
step:263/2090 train_time:8785ms step_avg:33.40ms
step:264/2090 train_time:8818ms step_avg:33.40ms
step:265/2090 train_time:8851ms step_avg:33.40ms
step:266/2090 train_time:8883ms step_avg:33.40ms
step:267/2090 train_time:8916ms step_avg:33.39ms
step:268/2090 train_time:8949ms step_avg:33.39ms
step:269/2090 train_time:8982ms step_avg:33.39ms
step:270/2090 train_time:9014ms step_avg:33.39ms
step:271/2090 train_time:9047ms step_avg:33.38ms
step:272/2090 train_time:9080ms step_avg:33.38ms
step:273/2090 train_time:9113ms step_avg:33.38ms
step:274/2090 train_time:9146ms step_avg:33.38ms
step:275/2090 train_time:9178ms step_avg:33.38ms
step:276/2090 train_time:9211ms step_avg:33.37ms
step:277/2090 train_time:9244ms step_avg:33.37ms
step:278/2090 train_time:9277ms step_avg:33.37ms
step:279/2090 train_time:9310ms step_avg:33.37ms
step:280/2090 train_time:9343ms step_avg:33.37ms
step:281/2090 train_time:9376ms step_avg:33.37ms
step:282/2090 train_time:9409ms step_avg:33.37ms
step:283/2090 train_time:9443ms step_avg:33.37ms
step:284/2090 train_time:9476ms step_avg:33.37ms
step:285/2090 train_time:9510ms step_avg:33.37ms
step:286/2090 train_time:9543ms step_avg:33.37ms
step:287/2090 train_time:9576ms step_avg:33.37ms
step:288/2090 train_time:9610ms step_avg:33.37ms
step:289/2090 train_time:9643ms step_avg:33.37ms
step:290/2090 train_time:9676ms step_avg:33.36ms
step:291/2090 train_time:9709ms step_avg:33.37ms
step:292/2090 train_time:9742ms step_avg:33.36ms
step:293/2090 train_time:9775ms step_avg:33.36ms
step:294/2090 train_time:9808ms step_avg:33.36ms
step:295/2090 train_time:9841ms step_avg:33.36ms
step:296/2090 train_time:9874ms step_avg:33.36ms
step:297/2090 train_time:9907ms step_avg:33.36ms
step:298/2090 train_time:9940ms step_avg:33.36ms
step:299/2090 train_time:9973ms step_avg:33.35ms
step:300/2090 train_time:10006ms step_avg:33.35ms
step:301/2090 train_time:10039ms step_avg:33.35ms
step:302/2090 train_time:10071ms step_avg:33.35ms
step:303/2090 train_time:10104ms step_avg:33.35ms
step:304/2090 train_time:10137ms step_avg:33.35ms
step:305/2090 train_time:10170ms step_avg:33.35ms
step:306/2090 train_time:10203ms step_avg:33.34ms
step:307/2090 train_time:10236ms step_avg:33.34ms
step:308/2090 train_time:10269ms step_avg:33.34ms
step:309/2090 train_time:10302ms step_avg:33.34ms
step:310/2090 train_time:10335ms step_avg:33.34ms
step:311/2090 train_time:10368ms step_avg:33.34ms
step:312/2090 train_time:10401ms step_avg:33.34ms
step:313/2090 train_time:10434ms step_avg:33.34ms
step:314/2090 train_time:10467ms step_avg:33.33ms
step:315/2090 train_time:10500ms step_avg:33.33ms
step:316/2090 train_time:10533ms step_avg:33.33ms
step:317/2090 train_time:10567ms step_avg:33.33ms
step:318/2090 train_time:10600ms step_avg:33.33ms
step:319/2090 train_time:10633ms step_avg:33.33ms
step:320/2090 train_time:10666ms step_avg:33.33ms
step:321/2090 train_time:10699ms step_avg:33.33ms
step:322/2090 train_time:10732ms step_avg:33.33ms
step:323/2090 train_time:10765ms step_avg:33.33ms
step:324/2090 train_time:10798ms step_avg:33.33ms
step:325/2090 train_time:10832ms step_avg:33.33ms
step:326/2090 train_time:10865ms step_avg:33.33ms
step:327/2090 train_time:10898ms step_avg:33.33ms
step:328/2090 train_time:10931ms step_avg:33.32ms
step:329/2090 train_time:10964ms step_avg:33.32ms
step:330/2090 train_time:10997ms step_avg:33.32ms
step:331/2090 train_time:11030ms step_avg:33.32ms
step:332/2090 train_time:11062ms step_avg:33.32ms
step:333/2090 train_time:11095ms step_avg:33.32ms
step:334/2090 train_time:11128ms step_avg:33.32ms
step:335/2090 train_time:11161ms step_avg:33.32ms
step:336/2090 train_time:11194ms step_avg:33.31ms
step:337/2090 train_time:11227ms step_avg:33.31ms
step:338/2090 train_time:11260ms step_avg:33.31ms
step:339/2090 train_time:11293ms step_avg:33.31ms
step:340/2090 train_time:11326ms step_avg:33.31ms
step:341/2090 train_time:11359ms step_avg:33.31ms
step:342/2090 train_time:11391ms step_avg:33.31ms
step:343/2090 train_time:11425ms step_avg:33.31ms
step:344/2090 train_time:11458ms step_avg:33.31ms
step:345/2090 train_time:11492ms step_avg:33.31ms
step:346/2090 train_time:11524ms step_avg:33.31ms
step:347/2090 train_time:11557ms step_avg:33.31ms
step:348/2090 train_time:11590ms step_avg:33.31ms
step:349/2090 train_time:11624ms step_avg:33.31ms
step:350/2090 train_time:11656ms step_avg:33.30ms
step:351/2090 train_time:11690ms step_avg:33.31ms
step:352/2090 train_time:11723ms step_avg:33.30ms
step:353/2090 train_time:11756ms step_avg:33.30ms
step:354/2090 train_time:11789ms step_avg:33.30ms
step:355/2090 train_time:11822ms step_avg:33.30ms
step:356/2090 train_time:11855ms step_avg:33.30ms
step:357/2090 train_time:11889ms step_avg:33.30ms
step:358/2090 train_time:11921ms step_avg:33.30ms
step:359/2090 train_time:11955ms step_avg:33.30ms
step:360/2090 train_time:11987ms step_avg:33.30ms
step:361/2090 train_time:12020ms step_avg:33.30ms
step:362/2090 train_time:12053ms step_avg:33.30ms
step:363/2090 train_time:12086ms step_avg:33.30ms
step:364/2090 train_time:12119ms step_avg:33.29ms
step:365/2090 train_time:12152ms step_avg:33.29ms
step:366/2090 train_time:12185ms step_avg:33.29ms
step:367/2090 train_time:12218ms step_avg:33.29ms
step:368/2090 train_time:12251ms step_avg:33.29ms
step:369/2090 train_time:12284ms step_avg:33.29ms
step:370/2090 train_time:12317ms step_avg:33.29ms
step:371/2090 train_time:12350ms step_avg:33.29ms
step:372/2090 train_time:12382ms step_avg:33.29ms
step:373/2090 train_time:12415ms step_avg:33.29ms
step:374/2090 train_time:12448ms step_avg:33.28ms
step:375/2090 train_time:12481ms step_avg:33.28ms
step:376/2090 train_time:12514ms step_avg:33.28ms
step:377/2090 train_time:12548ms step_avg:33.28ms
step:378/2090 train_time:12580ms step_avg:33.28ms
step:379/2090 train_time:12613ms step_avg:33.28ms
step:380/2090 train_time:12646ms step_avg:33.28ms
step:381/2090 train_time:12680ms step_avg:33.28ms
step:382/2090 train_time:12713ms step_avg:33.28ms
step:383/2090 train_time:12746ms step_avg:33.28ms
step:384/2090 train_time:12779ms step_avg:33.28ms
step:385/2090 train_time:12812ms step_avg:33.28ms
step:386/2090 train_time:12844ms step_avg:33.28ms
step:387/2090 train_time:12878ms step_avg:33.28ms
step:388/2090 train_time:12910ms step_avg:33.27ms
step:389/2090 train_time:12944ms step_avg:33.28ms
step:390/2090 train_time:12977ms step_avg:33.27ms
step:391/2090 train_time:13010ms step_avg:33.27ms
step:392/2090 train_time:13043ms step_avg:33.27ms
step:393/2090 train_time:13076ms step_avg:33.27ms
step:394/2090 train_time:13109ms step_avg:33.27ms
step:395/2090 train_time:13142ms step_avg:33.27ms
step:396/2090 train_time:13175ms step_avg:33.27ms
step:397/2090 train_time:13208ms step_avg:33.27ms
step:398/2090 train_time:13241ms step_avg:33.27ms
step:399/2090 train_time:13273ms step_avg:33.27ms
step:400/2090 train_time:13306ms step_avg:33.27ms
step:401/2090 train_time:13339ms step_avg:33.26ms
step:402/2090 train_time:13372ms step_avg:33.26ms
step:403/2090 train_time:13405ms step_avg:33.26ms
step:404/2090 train_time:13438ms step_avg:33.26ms
step:405/2090 train_time:13471ms step_avg:33.26ms
step:406/2090 train_time:13504ms step_avg:33.26ms
step:407/2090 train_time:13537ms step_avg:33.26ms
step:408/2090 train_time:13570ms step_avg:33.26ms
step:409/2090 train_time:13603ms step_avg:33.26ms
step:410/2090 train_time:13636ms step_avg:33.26ms
step:411/2090 train_time:13669ms step_avg:33.26ms
step:412/2090 train_time:13702ms step_avg:33.26ms
step:413/2090 train_time:13735ms step_avg:33.26ms
step:414/2090 train_time:13768ms step_avg:33.26ms
step:415/2090 train_time:13801ms step_avg:33.26ms
step:416/2090 train_time:13834ms step_avg:33.25ms
step:417/2090 train_time:13867ms step_avg:33.26ms
step:418/2090 train_time:13900ms step_avg:33.25ms
step:419/2090 train_time:13934ms step_avg:33.25ms
step:420/2090 train_time:13967ms step_avg:33.25ms
step:421/2090 train_time:14000ms step_avg:33.25ms
step:422/2090 train_time:14033ms step_avg:33.25ms
step:423/2090 train_time:14066ms step_avg:33.25ms
step:424/2090 train_time:14099ms step_avg:33.25ms
step:425/2090 train_time:14132ms step_avg:33.25ms
step:426/2090 train_time:14165ms step_avg:33.25ms
step:427/2090 train_time:14198ms step_avg:33.25ms
step:428/2090 train_time:14231ms step_avg:33.25ms
step:429/2090 train_time:14264ms step_avg:33.25ms
step:430/2090 train_time:14297ms step_avg:33.25ms
step:431/2090 train_time:14330ms step_avg:33.25ms
step:432/2090 train_time:14363ms step_avg:33.25ms
step:433/2090 train_time:14396ms step_avg:33.25ms
step:434/2090 train_time:14429ms step_avg:33.25ms
step:435/2090 train_time:14462ms step_avg:33.25ms
step:436/2090 train_time:14494ms step_avg:33.24ms
step:437/2090 train_time:14527ms step_avg:33.24ms
step:438/2090 train_time:14560ms step_avg:33.24ms
step:439/2090 train_time:14593ms step_avg:33.24ms
step:440/2090 train_time:14626ms step_avg:33.24ms
step:441/2090 train_time:14659ms step_avg:33.24ms
step:442/2090 train_time:14692ms step_avg:33.24ms
step:443/2090 train_time:14725ms step_avg:33.24ms
step:444/2090 train_time:14758ms step_avg:33.24ms
step:445/2090 train_time:14791ms step_avg:33.24ms
step:446/2090 train_time:14824ms step_avg:33.24ms
step:447/2090 train_time:14857ms step_avg:33.24ms
step:448/2090 train_time:14890ms step_avg:33.24ms
step:449/2090 train_time:14924ms step_avg:33.24ms
step:450/2090 train_time:14956ms step_avg:33.24ms
step:451/2090 train_time:14991ms step_avg:33.24ms
step:452/2090 train_time:15024ms step_avg:33.24ms
step:453/2090 train_time:15056ms step_avg:33.24ms
step:454/2090 train_time:15089ms step_avg:33.24ms
step:455/2090 train_time:15122ms step_avg:33.24ms
step:456/2090 train_time:15155ms step_avg:33.23ms
step:457/2090 train_time:15188ms step_avg:33.23ms
step:458/2090 train_time:15221ms step_avg:33.23ms
step:459/2090 train_time:15254ms step_avg:33.23ms
step:460/2090 train_time:15287ms step_avg:33.23ms
step:461/2090 train_time:15320ms step_avg:33.23ms
step:462/2090 train_time:15353ms step_avg:33.23ms
step:463/2090 train_time:15386ms step_avg:33.23ms
step:464/2090 train_time:15419ms step_avg:33.23ms
step:465/2090 train_time:15452ms step_avg:33.23ms
step:466/2090 train_time:15485ms step_avg:33.23ms
step:467/2090 train_time:15517ms step_avg:33.23ms
step:468/2090 train_time:15551ms step_avg:33.23ms
step:469/2090 train_time:15583ms step_avg:33.23ms
step:470/2090 train_time:15616ms step_avg:33.23ms
step:471/2090 train_time:15650ms step_avg:33.23ms
step:472/2090 train_time:15682ms step_avg:33.23ms
step:473/2090 train_time:15716ms step_avg:33.23ms
step:474/2090 train_time:15748ms step_avg:33.22ms
step:475/2090 train_time:15781ms step_avg:33.22ms
step:476/2090 train_time:15814ms step_avg:33.22ms
step:477/2090 train_time:15847ms step_avg:33.22ms
step:478/2090 train_time:15880ms step_avg:33.22ms
step:479/2090 train_time:15913ms step_avg:33.22ms
step:480/2090 train_time:15946ms step_avg:33.22ms
step:481/2090 train_time:15979ms step_avg:33.22ms
step:482/2090 train_time:16012ms step_avg:33.22ms
step:483/2090 train_time:16045ms step_avg:33.22ms
step:484/2090 train_time:16078ms step_avg:33.22ms
step:485/2090 train_time:16112ms step_avg:33.22ms
step:486/2090 train_time:16144ms step_avg:33.22ms
step:487/2090 train_time:16178ms step_avg:33.22ms
step:488/2090 train_time:16211ms step_avg:33.22ms
step:489/2090 train_time:16244ms step_avg:33.22ms
step:490/2090 train_time:16277ms step_avg:33.22ms
step:491/2090 train_time:16310ms step_avg:33.22ms
step:492/2090 train_time:16343ms step_avg:33.22ms
step:493/2090 train_time:16376ms step_avg:33.22ms
step:494/2090 train_time:16409ms step_avg:33.22ms
step:495/2090 train_time:16442ms step_avg:33.22ms
step:496/2090 train_time:16475ms step_avg:33.22ms
step:497/2090 train_time:16508ms step_avg:33.21ms
step:498/2090 train_time:16540ms step_avg:33.21ms
step:499/2090 train_time:16574ms step_avg:33.21ms
step:500/2090 train_time:16607ms step_avg:33.21ms
step:500/2090 val_loss:4.0091 train_time:16642ms step_avg:33.28ms
step:501/2090 train_time:16661ms step_avg:33.26ms
step:502/2090 train_time:16681ms step_avg:33.23ms
step:503/2090 train_time:16711ms step_avg:33.22ms
step:504/2090 train_time:16745ms step_avg:33.22ms
step:505/2090 train_time:16779ms step_avg:33.23ms
step:506/2090 train_time:16813ms step_avg:33.23ms
step:507/2090 train_time:16847ms step_avg:33.23ms
step:508/2090 train_time:16880ms step_avg:33.23ms
step:509/2090 train_time:16913ms step_avg:33.23ms
step:510/2090 train_time:16946ms step_avg:33.23ms
step:511/2090 train_time:16979ms step_avg:33.23ms
step:512/2090 train_time:17011ms step_avg:33.23ms
step:513/2090 train_time:17044ms step_avg:33.22ms
step:514/2090 train_time:17077ms step_avg:33.22ms
step:515/2090 train_time:17110ms step_avg:33.22ms
step:516/2090 train_time:17142ms step_avg:33.22ms
step:517/2090 train_time:17175ms step_avg:33.22ms
step:518/2090 train_time:17208ms step_avg:33.22ms
step:519/2090 train_time:17241ms step_avg:33.22ms
step:520/2090 train_time:17273ms step_avg:33.22ms
step:521/2090 train_time:17306ms step_avg:33.22ms
step:522/2090 train_time:17339ms step_avg:33.22ms
step:523/2090 train_time:17372ms step_avg:33.22ms
step:524/2090 train_time:17404ms step_avg:33.21ms
step:525/2090 train_time:17437ms step_avg:33.21ms
step:526/2090 train_time:17470ms step_avg:33.21ms
step:527/2090 train_time:17503ms step_avg:33.21ms
step:528/2090 train_time:17536ms step_avg:33.21ms
step:529/2090 train_time:17569ms step_avg:33.21ms
step:530/2090 train_time:17602ms step_avg:33.21ms
step:531/2090 train_time:17635ms step_avg:33.21ms
step:532/2090 train_time:17668ms step_avg:33.21ms
step:533/2090 train_time:17701ms step_avg:33.21ms
step:534/2090 train_time:17734ms step_avg:33.21ms
step:535/2090 train_time:17767ms step_avg:33.21ms
step:536/2090 train_time:17800ms step_avg:33.21ms
step:537/2090 train_time:17834ms step_avg:33.21ms
step:538/2090 train_time:17867ms step_avg:33.21ms
step:539/2090 train_time:17900ms step_avg:33.21ms
step:540/2090 train_time:17932ms step_avg:33.21ms
step:541/2090 train_time:17966ms step_avg:33.21ms
step:542/2090 train_time:17999ms step_avg:33.21ms
step:543/2090 train_time:18032ms step_avg:33.21ms
step:544/2090 train_time:18064ms step_avg:33.21ms
step:545/2090 train_time:18097ms step_avg:33.21ms
step:546/2090 train_time:18130ms step_avg:33.21ms
step:547/2090 train_time:18163ms step_avg:33.20ms
step:548/2090 train_time:18196ms step_avg:33.20ms
step:549/2090 train_time:18228ms step_avg:33.20ms
step:550/2090 train_time:18261ms step_avg:33.20ms
step:551/2090 train_time:18294ms step_avg:33.20ms
step:552/2090 train_time:18326ms step_avg:33.20ms
step:553/2090 train_time:18359ms step_avg:33.20ms
step:554/2090 train_time:18392ms step_avg:33.20ms
step:555/2090 train_time:18425ms step_avg:33.20ms
step:556/2090 train_time:18458ms step_avg:33.20ms
step:557/2090 train_time:18491ms step_avg:33.20ms
step:558/2090 train_time:18523ms step_avg:33.20ms
step:559/2090 train_time:18556ms step_avg:33.20ms
step:560/2090 train_time:18589ms step_avg:33.19ms
step:561/2090 train_time:18622ms step_avg:33.19ms
step:562/2090 train_time:18654ms step_avg:33.19ms
step:563/2090 train_time:18688ms step_avg:33.19ms
step:564/2090 train_time:18721ms step_avg:33.19ms
step:565/2090 train_time:18754ms step_avg:33.19ms
step:566/2090 train_time:18786ms step_avg:33.19ms
step:567/2090 train_time:18820ms step_avg:33.19ms
step:568/2090 train_time:18853ms step_avg:33.19ms
step:569/2090 train_time:18886ms step_avg:33.19ms
step:570/2090 train_time:18919ms step_avg:33.19ms
step:571/2090 train_time:18952ms step_avg:33.19ms
step:572/2090 train_time:18985ms step_avg:33.19ms
step:573/2090 train_time:19018ms step_avg:33.19ms
step:574/2090 train_time:19051ms step_avg:33.19ms
step:575/2090 train_time:19085ms step_avg:33.19ms
step:576/2090 train_time:19117ms step_avg:33.19ms
step:577/2090 train_time:19151ms step_avg:33.19ms
step:578/2090 train_time:19184ms step_avg:33.19ms
step:579/2090 train_time:19217ms step_avg:33.19ms
step:580/2090 train_time:19250ms step_avg:33.19ms
step:581/2090 train_time:19282ms step_avg:33.19ms
step:582/2090 train_time:19315ms step_avg:33.19ms
step:583/2090 train_time:19348ms step_avg:33.19ms
step:584/2090 train_time:19382ms step_avg:33.19ms
step:585/2090 train_time:19414ms step_avg:33.19ms
step:586/2090 train_time:19447ms step_avg:33.19ms
step:587/2090 train_time:19480ms step_avg:33.19ms
step:588/2090 train_time:19513ms step_avg:33.19ms
step:589/2090 train_time:19546ms step_avg:33.19ms
step:590/2090 train_time:19579ms step_avg:33.18ms
step:591/2090 train_time:19612ms step_avg:33.18ms
step:592/2090 train_time:19645ms step_avg:33.18ms
step:593/2090 train_time:19678ms step_avg:33.18ms
step:594/2090 train_time:19711ms step_avg:33.18ms
step:595/2090 train_time:19744ms step_avg:33.18ms
step:596/2090 train_time:19777ms step_avg:33.18ms
step:597/2090 train_time:19810ms step_avg:33.18ms
step:598/2090 train_time:19843ms step_avg:33.18ms
step:599/2090 train_time:19876ms step_avg:33.18ms
step:600/2090 train_time:19908ms step_avg:33.18ms
step:601/2090 train_time:19942ms step_avg:33.18ms
step:602/2090 train_time:19975ms step_avg:33.18ms
step:603/2090 train_time:20008ms step_avg:33.18ms
step:604/2090 train_time:20041ms step_avg:33.18ms
step:605/2090 train_time:20074ms step_avg:33.18ms
step:606/2090 train_time:20107ms step_avg:33.18ms
step:607/2090 train_time:20140ms step_avg:33.18ms
step:608/2090 train_time:20173ms step_avg:33.18ms
step:609/2090 train_time:20206ms step_avg:33.18ms
step:610/2090 train_time:20239ms step_avg:33.18ms
step:611/2090 train_time:20272ms step_avg:33.18ms
step:612/2090 train_time:20305ms step_avg:33.18ms
step:613/2090 train_time:20338ms step_avg:33.18ms
step:614/2090 train_time:20371ms step_avg:33.18ms
step:615/2090 train_time:20404ms step_avg:33.18ms
step:616/2090 train_time:20437ms step_avg:33.18ms
step:617/2090 train_time:20470ms step_avg:33.18ms
step:618/2090 train_time:20502ms step_avg:33.18ms
step:619/2090 train_time:20535ms step_avg:33.18ms
step:620/2090 train_time:20568ms step_avg:33.17ms
step:621/2090 train_time:20601ms step_avg:33.17ms
step:622/2090 train_time:20634ms step_avg:33.17ms
step:623/2090 train_time:20667ms step_avg:33.17ms
step:624/2090 train_time:20700ms step_avg:33.17ms
step:625/2090 train_time:20733ms step_avg:33.17ms
step:626/2090 train_time:20766ms step_avg:33.17ms
step:627/2090 train_time:20799ms step_avg:33.17ms
step:628/2090 train_time:20832ms step_avg:33.17ms
step:629/2090 train_time:20864ms step_avg:33.17ms
step:630/2090 train_time:20898ms step_avg:33.17ms
step:631/2090 train_time:20931ms step_avg:33.17ms
step:632/2090 train_time:20963ms step_avg:33.17ms
step:633/2090 train_time:20996ms step_avg:33.17ms
step:634/2090 train_time:21029ms step_avg:33.17ms
step:635/2090 train_time:21062ms step_avg:33.17ms
step:636/2090 train_time:21095ms step_avg:33.17ms
step:637/2090 train_time:21129ms step_avg:33.17ms
step:638/2090 train_time:21162ms step_avg:33.17ms
step:639/2090 train_time:21195ms step_avg:33.17ms
step:640/2090 train_time:21228ms step_avg:33.17ms
step:641/2090 train_time:21260ms step_avg:33.17ms
step:642/2090 train_time:21294ms step_avg:33.17ms
step:643/2090 train_time:21327ms step_avg:33.17ms
step:644/2090 train_time:21359ms step_avg:33.17ms
step:645/2090 train_time:21392ms step_avg:33.17ms
step:646/2090 train_time:21425ms step_avg:33.17ms
step:647/2090 train_time:21458ms step_avg:33.17ms
step:648/2090 train_time:21491ms step_avg:33.16ms
step:649/2090 train_time:21524ms step_avg:33.16ms
step:650/2090 train_time:21557ms step_avg:33.16ms
step:651/2090 train_time:21590ms step_avg:33.16ms
step:652/2090 train_time:21623ms step_avg:33.16ms
step:653/2090 train_time:21656ms step_avg:33.16ms
step:654/2090 train_time:21689ms step_avg:33.16ms
step:655/2090 train_time:21722ms step_avg:33.16ms
step:656/2090 train_time:21755ms step_avg:33.16ms
step:657/2090 train_time:21788ms step_avg:33.16ms
step:658/2090 train_time:21821ms step_avg:33.16ms
step:659/2090 train_time:21854ms step_avg:33.16ms
step:660/2090 train_time:21887ms step_avg:33.16ms
step:661/2090 train_time:21920ms step_avg:33.16ms
step:662/2090 train_time:21952ms step_avg:33.16ms
step:663/2090 train_time:21986ms step_avg:33.16ms
step:664/2090 train_time:22019ms step_avg:33.16ms
step:665/2090 train_time:22052ms step_avg:33.16ms
step:666/2090 train_time:22085ms step_avg:33.16ms
step:667/2090 train_time:22118ms step_avg:33.16ms
step:668/2090 train_time:22151ms step_avg:33.16ms
step:669/2090 train_time:22185ms step_avg:33.16ms
step:670/2090 train_time:22218ms step_avg:33.16ms
step:671/2090 train_time:22250ms step_avg:33.16ms
step:672/2090 train_time:22283ms step_avg:33.16ms
step:673/2090 train_time:22316ms step_avg:33.16ms
step:674/2090 train_time:22350ms step_avg:33.16ms
step:675/2090 train_time:22383ms step_avg:33.16ms
step:676/2090 train_time:22415ms step_avg:33.16ms
step:677/2090 train_time:22448ms step_avg:33.16ms
step:678/2090 train_time:22481ms step_avg:33.16ms
step:679/2090 train_time:22514ms step_avg:33.16ms
step:680/2090 train_time:22547ms step_avg:33.16ms
step:681/2090 train_time:22580ms step_avg:33.16ms
step:682/2090 train_time:22613ms step_avg:33.16ms
step:683/2090 train_time:22646ms step_avg:33.16ms
step:684/2090 train_time:22678ms step_avg:33.16ms
step:685/2090 train_time:22712ms step_avg:33.16ms
step:686/2090 train_time:22770ms step_avg:33.19ms
step:687/2090 train_time:22830ms step_avg:33.23ms
step:688/2090 train_time:22889ms step_avg:33.27ms
step:689/2090 train_time:22949ms step_avg:33.31ms
step:690/2090 train_time:23009ms step_avg:33.35ms
step:691/2090 train_time:23069ms step_avg:33.39ms
step:692/2090 train_time:23128ms step_avg:33.42ms
step:693/2090 train_time:23189ms step_avg:33.46ms
step:694/2090 train_time:23248ms step_avg:33.50ms
step:695/2090 train_time:23308ms step_avg:33.54ms
step:696/2090 train_time:23368ms step_avg:33.57ms
step:697/2090 train_time:23429ms step_avg:33.61ms
step:698/2090 train_time:23488ms step_avg:33.65ms
step:699/2090 train_time:23548ms step_avg:33.69ms
step:700/2090 train_time:23608ms step_avg:33.73ms
step:701/2090 train_time:23668ms step_avg:33.76ms
step:702/2090 train_time:23726ms step_avg:33.80ms
step:703/2090 train_time:23786ms step_avg:33.84ms
step:704/2090 train_time:23845ms step_avg:33.87ms
step:705/2090 train_time:23905ms step_avg:33.91ms
step:706/2090 train_time:23964ms step_avg:33.94ms
step:707/2090 train_time:24024ms step_avg:33.98ms
step:708/2090 train_time:24083ms step_avg:34.02ms
step:709/2090 train_time:24143ms step_avg:34.05ms
step:710/2090 train_time:24202ms step_avg:34.09ms
step:711/2090 train_time:24262ms step_avg:34.12ms
step:712/2090 train_time:24321ms step_avg:34.16ms
step:713/2090 train_time:24382ms step_avg:34.20ms
step:714/2090 train_time:24441ms step_avg:34.23ms
step:715/2090 train_time:24501ms step_avg:34.27ms
step:716/2090 train_time:24560ms step_avg:34.30ms
step:717/2090 train_time:24621ms step_avg:34.34ms
step:718/2090 train_time:24680ms step_avg:34.37ms
step:719/2090 train_time:24741ms step_avg:34.41ms
step:720/2090 train_time:24800ms step_avg:34.44ms
step:721/2090 train_time:24861ms step_avg:34.48ms
step:722/2090 train_time:24919ms step_avg:34.51ms
step:723/2090 train_time:24980ms step_avg:34.55ms
step:724/2090 train_time:25039ms step_avg:34.58ms
step:725/2090 train_time:25099ms step_avg:34.62ms
step:726/2090 train_time:25159ms step_avg:34.65ms
step:727/2090 train_time:25219ms step_avg:34.69ms
step:728/2090 train_time:25279ms step_avg:34.72ms
step:729/2090 train_time:25340ms step_avg:34.76ms
step:730/2090 train_time:25400ms step_avg:34.79ms
step:731/2090 train_time:25460ms step_avg:34.83ms
step:732/2090 train_time:25519ms step_avg:34.86ms
step:733/2090 train_time:25580ms step_avg:34.90ms
step:734/2090 train_time:25640ms step_avg:34.93ms
step:735/2090 train_time:25701ms step_avg:34.97ms
step:736/2090 train_time:25760ms step_avg:35.00ms
step:737/2090 train_time:25821ms step_avg:35.04ms
step:738/2090 train_time:25880ms step_avg:35.07ms
step:739/2090 train_time:25940ms step_avg:35.10ms
step:740/2090 train_time:25999ms step_avg:35.13ms
step:741/2090 train_time:26060ms step_avg:35.17ms
step:742/2090 train_time:26119ms step_avg:35.20ms
step:743/2090 train_time:26178ms step_avg:35.23ms
step:744/2090 train_time:26238ms step_avg:35.27ms
step:745/2090 train_time:26299ms step_avg:35.30ms
step:746/2090 train_time:26358ms step_avg:35.33ms
step:747/2090 train_time:26418ms step_avg:35.37ms
step:748/2090 train_time:26479ms step_avg:35.40ms
step:749/2090 train_time:26539ms step_avg:35.43ms
step:750/2090 train_time:26599ms step_avg:35.47ms
step:750/2090 val_loss:3.8540 train_time:26662ms step_avg:35.55ms
step:751/2090 train_time:26682ms step_avg:35.53ms
step:752/2090 train_time:26722ms step_avg:35.53ms
step:753/2090 train_time:26785ms step_avg:35.57ms
step:754/2090 train_time:26846ms step_avg:35.60ms
step:755/2090 train_time:26906ms step_avg:35.64ms
step:756/2090 train_time:26966ms step_avg:35.67ms
step:757/2090 train_time:27026ms step_avg:35.70ms
step:758/2090 train_time:27085ms step_avg:35.73ms
step:759/2090 train_time:27145ms step_avg:35.76ms
step:760/2090 train_time:27204ms step_avg:35.79ms
step:761/2090 train_time:27264ms step_avg:35.83ms
step:762/2090 train_time:27323ms step_avg:35.86ms
step:763/2090 train_time:27383ms step_avg:35.89ms
step:764/2090 train_time:27442ms step_avg:35.92ms
step:765/2090 train_time:27502ms step_avg:35.95ms
step:766/2090 train_time:27561ms step_avg:35.98ms
step:767/2090 train_time:27623ms step_avg:36.01ms
step:768/2090 train_time:27684ms step_avg:36.05ms
step:769/2090 train_time:27746ms step_avg:36.08ms
step:770/2090 train_time:27807ms step_avg:36.11ms
step:771/2090 train_time:27868ms step_avg:36.14ms
step:772/2090 train_time:27928ms step_avg:36.18ms
step:773/2090 train_time:27988ms step_avg:36.21ms
step:774/2090 train_time:28047ms step_avg:36.24ms
step:775/2090 train_time:28107ms step_avg:36.27ms
step:776/2090 train_time:28167ms step_avg:36.30ms
step:777/2090 train_time:28228ms step_avg:36.33ms
step:778/2090 train_time:28286ms step_avg:36.36ms
step:779/2090 train_time:28347ms step_avg:36.39ms
step:780/2090 train_time:28405ms step_avg:36.42ms
step:781/2090 train_time:28466ms step_avg:36.45ms
step:782/2090 train_time:28525ms step_avg:36.48ms
step:783/2090 train_time:28586ms step_avg:36.51ms
step:784/2090 train_time:28647ms step_avg:36.54ms
step:785/2090 train_time:28708ms step_avg:36.57ms
step:786/2090 train_time:28768ms step_avg:36.60ms
step:787/2090 train_time:28829ms step_avg:36.63ms
step:788/2090 train_time:28890ms step_avg:36.66ms
step:789/2090 train_time:28950ms step_avg:36.69ms
step:790/2090 train_time:29009ms step_avg:36.72ms
step:791/2090 train_time:29069ms step_avg:36.75ms
step:792/2090 train_time:29129ms step_avg:36.78ms
step:793/2090 train_time:29189ms step_avg:36.81ms
step:794/2090 train_time:29249ms step_avg:36.84ms
step:795/2090 train_time:29309ms step_avg:36.87ms
step:796/2090 train_time:29369ms step_avg:36.90ms
step:797/2090 train_time:29429ms step_avg:36.93ms
step:798/2090 train_time:29489ms step_avg:36.95ms
step:799/2090 train_time:29550ms step_avg:36.98ms
step:800/2090 train_time:29609ms step_avg:37.01ms
step:801/2090 train_time:29669ms step_avg:37.04ms
step:802/2090 train_time:29730ms step_avg:37.07ms
step:803/2090 train_time:29790ms step_avg:37.10ms
step:804/2090 train_time:29850ms step_avg:37.13ms
step:805/2090 train_time:29910ms step_avg:37.16ms
step:806/2090 train_time:29969ms step_avg:37.18ms
step:807/2090 train_time:30030ms step_avg:37.21ms
step:808/2090 train_time:30090ms step_avg:37.24ms
step:809/2090 train_time:30150ms step_avg:37.27ms
step:810/2090 train_time:30209ms step_avg:37.29ms
step:811/2090 train_time:30269ms step_avg:37.32ms
step:812/2090 train_time:30327ms step_avg:37.35ms
step:813/2090 train_time:30388ms step_avg:37.38ms
step:814/2090 train_time:30447ms step_avg:37.40ms
step:815/2090 train_time:30508ms step_avg:37.43ms
step:816/2090 train_time:30567ms step_avg:37.46ms
step:817/2090 train_time:30628ms step_avg:37.49ms
step:818/2090 train_time:30688ms step_avg:37.52ms
step:819/2090 train_time:30749ms step_avg:37.54ms
step:820/2090 train_time:30809ms step_avg:37.57ms
step:821/2090 train_time:30869ms step_avg:37.60ms
step:822/2090 train_time:30929ms step_avg:37.63ms
step:823/2090 train_time:30989ms step_avg:37.65ms
step:824/2090 train_time:31049ms step_avg:37.68ms
step:825/2090 train_time:31109ms step_avg:37.71ms
step:826/2090 train_time:31168ms step_avg:37.73ms
step:827/2090 train_time:31229ms step_avg:37.76ms
step:828/2090 train_time:31289ms step_avg:37.79ms
step:829/2090 train_time:31350ms step_avg:37.82ms
step:830/2090 train_time:31409ms step_avg:37.84ms
step:831/2090 train_time:31469ms step_avg:37.87ms
step:832/2090 train_time:31529ms step_avg:37.90ms
step:833/2090 train_time:31590ms step_avg:37.92ms
step:834/2090 train_time:31649ms step_avg:37.95ms
step:835/2090 train_time:31710ms step_avg:37.98ms
step:836/2090 train_time:31770ms step_avg:38.00ms
step:837/2090 train_time:31830ms step_avg:38.03ms
step:838/2090 train_time:31890ms step_avg:38.05ms
step:839/2090 train_time:31950ms step_avg:38.08ms
step:840/2090 train_time:32009ms step_avg:38.11ms
step:841/2090 train_time:32070ms step_avg:38.13ms
step:842/2090 train_time:32130ms step_avg:38.16ms
step:843/2090 train_time:32190ms step_avg:38.18ms
step:844/2090 train_time:32249ms step_avg:38.21ms
step:845/2090 train_time:32309ms step_avg:38.24ms
step:846/2090 train_time:32369ms step_avg:38.26ms
step:847/2090 train_time:32429ms step_avg:38.29ms
step:848/2090 train_time:32489ms step_avg:38.31ms
step:849/2090 train_time:32549ms step_avg:38.34ms
step:850/2090 train_time:32608ms step_avg:38.36ms
step:851/2090 train_time:32669ms step_avg:38.39ms
step:852/2090 train_time:32728ms step_avg:38.41ms
step:853/2090 train_time:32789ms step_avg:38.44ms
step:854/2090 train_time:32849ms step_avg:38.47ms
step:855/2090 train_time:32910ms step_avg:38.49ms
step:856/2090 train_time:32969ms step_avg:38.52ms
step:857/2090 train_time:33030ms step_avg:38.54ms
step:858/2090 train_time:33089ms step_avg:38.57ms
step:859/2090 train_time:33150ms step_avg:38.59ms
step:860/2090 train_time:33209ms step_avg:38.61ms
step:861/2090 train_time:33269ms step_avg:38.64ms
step:862/2090 train_time:33329ms step_avg:38.66ms
step:863/2090 train_time:33390ms step_avg:38.69ms
step:864/2090 train_time:33449ms step_avg:38.71ms
step:865/2090 train_time:33509ms step_avg:38.74ms
step:866/2090 train_time:33569ms step_avg:38.76ms
step:867/2090 train_time:33630ms step_avg:38.79ms
step:868/2090 train_time:33690ms step_avg:38.81ms
step:869/2090 train_time:33751ms step_avg:38.84ms
step:870/2090 train_time:33810ms step_avg:38.86ms
step:871/2090 train_time:33870ms step_avg:38.89ms
step:872/2090 train_time:33929ms step_avg:38.91ms
step:873/2090 train_time:33991ms step_avg:38.94ms
step:874/2090 train_time:34050ms step_avg:38.96ms
step:875/2090 train_time:34111ms step_avg:38.98ms
step:876/2090 train_time:34170ms step_avg:39.01ms
step:877/2090 train_time:34231ms step_avg:39.03ms
step:878/2090 train_time:34291ms step_avg:39.06ms
step:879/2090 train_time:34351ms step_avg:39.08ms
step:880/2090 train_time:34410ms step_avg:39.10ms
step:881/2090 train_time:34471ms step_avg:39.13ms
step:882/2090 train_time:34530ms step_avg:39.15ms
step:883/2090 train_time:34591ms step_avg:39.17ms
step:884/2090 train_time:34650ms step_avg:39.20ms
step:885/2090 train_time:34711ms step_avg:39.22ms
step:886/2090 train_time:34770ms step_avg:39.24ms
step:887/2090 train_time:34830ms step_avg:39.27ms
step:888/2090 train_time:34889ms step_avg:39.29ms
step:889/2090 train_time:34950ms step_avg:39.31ms
step:890/2090 train_time:35009ms step_avg:39.34ms
step:891/2090 train_time:35070ms step_avg:39.36ms
step:892/2090 train_time:35129ms step_avg:39.38ms
step:893/2090 train_time:35189ms step_avg:39.41ms
step:894/2090 train_time:35249ms step_avg:39.43ms
step:895/2090 train_time:35309ms step_avg:39.45ms
step:896/2090 train_time:35369ms step_avg:39.47ms
step:897/2090 train_time:35429ms step_avg:39.50ms
step:898/2090 train_time:35489ms step_avg:39.52ms
step:899/2090 train_time:35550ms step_avg:39.54ms
step:900/2090 train_time:35609ms step_avg:39.57ms
step:901/2090 train_time:35670ms step_avg:39.59ms
step:902/2090 train_time:35730ms step_avg:39.61ms
step:903/2090 train_time:35791ms step_avg:39.64ms
step:904/2090 train_time:35850ms step_avg:39.66ms
step:905/2090 train_time:35910ms step_avg:39.68ms
step:906/2090 train_time:35970ms step_avg:39.70ms
step:907/2090 train_time:36030ms step_avg:39.72ms
step:908/2090 train_time:36090ms step_avg:39.75ms
step:909/2090 train_time:36151ms step_avg:39.77ms
step:910/2090 train_time:36210ms step_avg:39.79ms
step:911/2090 train_time:36270ms step_avg:39.81ms
step:912/2090 train_time:36329ms step_avg:39.83ms
step:913/2090 train_time:36389ms step_avg:39.86ms
step:914/2090 train_time:36449ms step_avg:39.88ms
step:915/2090 train_time:36509ms step_avg:39.90ms
step:916/2090 train_time:36569ms step_avg:39.92ms
step:917/2090 train_time:36630ms step_avg:39.95ms
step:918/2090 train_time:36690ms step_avg:39.97ms
step:919/2090 train_time:36750ms step_avg:39.99ms
step:920/2090 train_time:36809ms step_avg:40.01ms
step:921/2090 train_time:36869ms step_avg:40.03ms
step:922/2090 train_time:36929ms step_avg:40.05ms
step:923/2090 train_time:36989ms step_avg:40.08ms
step:924/2090 train_time:37049ms step_avg:40.10ms
step:925/2090 train_time:37109ms step_avg:40.12ms
step:926/2090 train_time:37168ms step_avg:40.14ms
step:927/2090 train_time:37228ms step_avg:40.16ms
step:928/2090 train_time:37288ms step_avg:40.18ms
step:929/2090 train_time:37348ms step_avg:40.20ms
step:930/2090 train_time:37408ms step_avg:40.22ms
step:931/2090 train_time:37468ms step_avg:40.25ms
step:932/2090 train_time:37528ms step_avg:40.27ms
step:933/2090 train_time:37589ms step_avg:40.29ms
step:934/2090 train_time:37648ms step_avg:40.31ms
step:935/2090 train_time:37709ms step_avg:40.33ms
step:936/2090 train_time:37768ms step_avg:40.35ms
step:937/2090 train_time:37829ms step_avg:40.37ms
step:938/2090 train_time:37888ms step_avg:40.39ms
step:939/2090 train_time:37949ms step_avg:40.41ms
step:940/2090 train_time:38008ms step_avg:40.43ms
step:941/2090 train_time:38068ms step_avg:40.46ms
step:942/2090 train_time:38127ms step_avg:40.47ms
step:943/2090 train_time:38188ms step_avg:40.50ms
step:944/2090 train_time:38248ms step_avg:40.52ms
step:945/2090 train_time:38308ms step_avg:40.54ms
step:946/2090 train_time:38367ms step_avg:40.56ms
step:947/2090 train_time:38428ms step_avg:40.58ms
step:948/2090 train_time:38488ms step_avg:40.60ms
step:949/2090 train_time:38549ms step_avg:40.62ms
step:950/2090 train_time:38608ms step_avg:40.64ms
step:951/2090 train_time:38669ms step_avg:40.66ms
step:952/2090 train_time:38729ms step_avg:40.68ms
step:953/2090 train_time:38790ms step_avg:40.70ms
step:954/2090 train_time:38850ms step_avg:40.72ms
step:955/2090 train_time:38911ms step_avg:40.74ms
step:956/2090 train_time:38970ms step_avg:40.76ms
step:957/2090 train_time:39031ms step_avg:40.78ms
step:958/2090 train_time:39090ms step_avg:40.80ms
step:959/2090 train_time:39150ms step_avg:40.82ms
step:960/2090 train_time:39210ms step_avg:40.84ms
step:961/2090 train_time:39270ms step_avg:40.86ms
step:962/2090 train_time:39330ms step_avg:40.88ms
step:963/2090 train_time:39390ms step_avg:40.90ms
step:964/2090 train_time:39450ms step_avg:40.92ms
step:965/2090 train_time:39510ms step_avg:40.94ms
step:966/2090 train_time:39569ms step_avg:40.96ms
step:967/2090 train_time:39630ms step_avg:40.98ms
step:968/2090 train_time:39690ms step_avg:41.00ms
step:969/2090 train_time:39750ms step_avg:41.02ms
step:970/2090 train_time:39809ms step_avg:41.04ms
step:971/2090 train_time:39870ms step_avg:41.06ms
step:972/2090 train_time:39930ms step_avg:41.08ms
step:973/2090 train_time:39990ms step_avg:41.10ms
step:974/2090 train_time:40050ms step_avg:41.12ms
step:975/2090 train_time:40110ms step_avg:41.14ms
step:976/2090 train_time:40169ms step_avg:41.16ms
step:977/2090 train_time:40229ms step_avg:41.18ms
step:978/2090 train_time:40289ms step_avg:41.20ms
step:979/2090 train_time:40349ms step_avg:41.21ms
step:980/2090 train_time:40408ms step_avg:41.23ms
step:981/2090 train_time:40468ms step_avg:41.25ms
step:982/2090 train_time:40528ms step_avg:41.27ms
step:983/2090 train_time:40588ms step_avg:41.29ms
step:984/2090 train_time:40647ms step_avg:41.31ms
step:985/2090 train_time:40708ms step_avg:41.33ms
step:986/2090 train_time:40767ms step_avg:41.35ms
step:987/2090 train_time:40828ms step_avg:41.37ms
step:988/2090 train_time:40888ms step_avg:41.38ms
step:989/2090 train_time:40948ms step_avg:41.40ms
step:990/2090 train_time:41008ms step_avg:41.42ms
step:991/2090 train_time:41068ms step_avg:41.44ms
step:992/2090 train_time:41128ms step_avg:41.46ms
step:993/2090 train_time:41188ms step_avg:41.48ms
step:994/2090 train_time:41248ms step_avg:41.50ms
step:995/2090 train_time:41309ms step_avg:41.52ms
step:996/2090 train_time:41368ms step_avg:41.53ms
step:997/2090 train_time:41428ms step_avg:41.55ms
step:998/2090 train_time:41488ms step_avg:41.57ms
step:999/2090 train_time:41549ms step_avg:41.59ms
step:1000/2090 train_time:41608ms step_avg:41.61ms
step:1000/2090 val_loss:3.7020 train_time:41671ms step_avg:41.67ms
step:1001/2090 train_time:41690ms step_avg:41.65ms
step:1002/2090 train_time:41730ms step_avg:41.65ms
step:1003/2090 train_time:41792ms step_avg:41.67ms
step:1004/2090 train_time:41853ms step_avg:41.69ms
step:1005/2090 train_time:41913ms step_avg:41.70ms
step:1006/2090 train_time:41972ms step_avg:41.72ms
step:1007/2090 train_time:42032ms step_avg:41.74ms
step:1008/2090 train_time:42091ms step_avg:41.76ms
step:1009/2090 train_time:42152ms step_avg:41.78ms
step:1010/2090 train_time:42210ms step_avg:41.79ms
step:1011/2090 train_time:42270ms step_avg:41.81ms
step:1012/2090 train_time:42328ms step_avg:41.83ms
step:1013/2090 train_time:42388ms step_avg:41.84ms
step:1014/2090 train_time:42446ms step_avg:41.86ms
step:1015/2090 train_time:42505ms step_avg:41.88ms
step:1016/2090 train_time:42564ms step_avg:41.89ms
step:1017/2090 train_time:42625ms step_avg:41.91ms
step:1018/2090 train_time:42684ms step_avg:41.93ms
step:1019/2090 train_time:42745ms step_avg:41.95ms
step:1020/2090 train_time:42805ms step_avg:41.97ms
step:1021/2090 train_time:42865ms step_avg:41.98ms
step:1022/2090 train_time:42925ms step_avg:42.00ms
step:1023/2090 train_time:42986ms step_avg:42.02ms
step:1024/2090 train_time:43044ms step_avg:42.04ms
step:1025/2090 train_time:43104ms step_avg:42.05ms
step:1026/2090 train_time:43162ms step_avg:42.07ms
step:1027/2090 train_time:43222ms step_avg:42.09ms
step:1028/2090 train_time:43281ms step_avg:42.10ms
step:1029/2090 train_time:43341ms step_avg:42.12ms
step:1030/2090 train_time:43400ms step_avg:42.14ms
step:1031/2090 train_time:43459ms step_avg:42.15ms
step:1032/2090 train_time:43519ms step_avg:42.17ms
step:1033/2090 train_time:43580ms step_avg:42.19ms
step:1034/2090 train_time:43639ms step_avg:42.20ms
step:1035/2090 train_time:43701ms step_avg:42.22ms
step:1036/2090 train_time:43761ms step_avg:42.24ms
step:1037/2090 train_time:43822ms step_avg:42.26ms
step:1038/2090 train_time:43881ms step_avg:42.27ms
step:1039/2090 train_time:43942ms step_avg:42.29ms
step:1040/2090 train_time:44002ms step_avg:42.31ms
step:1041/2090 train_time:44062ms step_avg:42.33ms
step:1042/2090 train_time:44121ms step_avg:42.34ms
step:1043/2090 train_time:44181ms step_avg:42.36ms
step:1044/2090 train_time:44241ms step_avg:42.38ms
step:1045/2090 train_time:44301ms step_avg:42.39ms
step:1046/2090 train_time:44360ms step_avg:42.41ms
step:1047/2090 train_time:44419ms step_avg:42.43ms
step:1048/2090 train_time:44478ms step_avg:42.44ms
step:1049/2090 train_time:44539ms step_avg:42.46ms
step:1050/2090 train_time:44599ms step_avg:42.48ms
step:1051/2090 train_time:44660ms step_avg:42.49ms
step:1052/2090 train_time:44720ms step_avg:42.51ms
step:1053/2090 train_time:44780ms step_avg:42.53ms
step:1054/2090 train_time:44840ms step_avg:42.54ms
step:1055/2090 train_time:44901ms step_avg:42.56ms
step:1056/2090 train_time:44960ms step_avg:42.58ms
step:1057/2090 train_time:45020ms step_avg:42.59ms
step:1058/2090 train_time:45079ms step_avg:42.61ms
step:1059/2090 train_time:45139ms step_avg:42.62ms
step:1060/2090 train_time:45199ms step_avg:42.64ms
step:1061/2090 train_time:45259ms step_avg:42.66ms
step:1062/2090 train_time:45318ms step_avg:42.67ms
step:1063/2090 train_time:45378ms step_avg:42.69ms
step:1064/2090 train_time:45437ms step_avg:42.70ms
step:1065/2090 train_time:45497ms step_avg:42.72ms
step:1066/2090 train_time:45557ms step_avg:42.74ms
step:1067/2090 train_time:45617ms step_avg:42.75ms
step:1068/2090 train_time:45677ms step_avg:42.77ms
step:1069/2090 train_time:45738ms step_avg:42.79ms
step:1070/2090 train_time:45798ms step_avg:42.80ms
step:1071/2090 train_time:45858ms step_avg:42.82ms
step:1072/2090 train_time:45918ms step_avg:42.83ms
step:1073/2090 train_time:45978ms step_avg:42.85ms
step:1074/2090 train_time:46038ms step_avg:42.87ms
step:1075/2090 train_time:46099ms step_avg:42.88ms
step:1076/2090 train_time:46158ms step_avg:42.90ms
step:1077/2090 train_time:46218ms step_avg:42.91ms
step:1078/2090 train_time:46278ms step_avg:42.93ms
step:1079/2090 train_time:46338ms step_avg:42.95ms
step:1080/2090 train_time:46398ms step_avg:42.96ms
step:1081/2090 train_time:46458ms step_avg:42.98ms
step:1082/2090 train_time:46518ms step_avg:42.99ms
step:1083/2090 train_time:46578ms step_avg:43.01ms
step:1084/2090 train_time:46639ms step_avg:43.02ms
step:1085/2090 train_time:46700ms step_avg:43.04ms
step:1086/2090 train_time:46761ms step_avg:43.06ms
step:1087/2090 train_time:46820ms step_avg:43.07ms
step:1088/2090 train_time:46880ms step_avg:43.09ms
step:1089/2090 train_time:46940ms step_avg:43.10ms
step:1090/2090 train_time:46999ms step_avg:43.12ms
step:1091/2090 train_time:47060ms step_avg:43.13ms
step:1092/2090 train_time:47119ms step_avg:43.15ms
step:1093/2090 train_time:47179ms step_avg:43.16ms
step:1094/2090 train_time:47239ms step_avg:43.18ms
step:1095/2090 train_time:47298ms step_avg:43.19ms
step:1096/2090 train_time:47357ms step_avg:43.21ms
step:1097/2090 train_time:47417ms step_avg:43.22ms
step:1098/2090 train_time:47477ms step_avg:43.24ms
step:1099/2090 train_time:47537ms step_avg:43.26ms
step:1100/2090 train_time:47597ms step_avg:43.27ms
step:1101/2090 train_time:47658ms step_avg:43.29ms
step:1102/2090 train_time:47718ms step_avg:43.30ms
step:1103/2090 train_time:47778ms step_avg:43.32ms
step:1104/2090 train_time:47839ms step_avg:43.33ms
step:1105/2090 train_time:47899ms step_avg:43.35ms
step:1106/2090 train_time:47959ms step_avg:43.36ms
step:1107/2090 train_time:48019ms step_avg:43.38ms
step:1108/2090 train_time:48078ms step_avg:43.39ms
step:1109/2090 train_time:48139ms step_avg:43.41ms
step:1110/2090 train_time:48198ms step_avg:43.42ms
step:1111/2090 train_time:48258ms step_avg:43.44ms
step:1112/2090 train_time:48317ms step_avg:43.45ms
step:1113/2090 train_time:48377ms step_avg:43.47ms
step:1114/2090 train_time:48437ms step_avg:43.48ms
step:1115/2090 train_time:48498ms step_avg:43.50ms
step:1116/2090 train_time:48557ms step_avg:43.51ms
step:1117/2090 train_time:48617ms step_avg:43.52ms
step:1118/2090 train_time:48678ms step_avg:43.54ms
step:1119/2090 train_time:48739ms step_avg:43.56ms
step:1120/2090 train_time:48798ms step_avg:43.57ms
step:1121/2090 train_time:48859ms step_avg:43.59ms
step:1122/2090 train_time:48919ms step_avg:43.60ms
step:1123/2090 train_time:48979ms step_avg:43.61ms
step:1124/2090 train_time:49039ms step_avg:43.63ms
step:1125/2090 train_time:49099ms step_avg:43.64ms
step:1126/2090 train_time:49159ms step_avg:43.66ms
step:1127/2090 train_time:49219ms step_avg:43.67ms
step:1128/2090 train_time:49278ms step_avg:43.69ms
step:1129/2090 train_time:49338ms step_avg:43.70ms
step:1130/2090 train_time:49397ms step_avg:43.71ms
step:1131/2090 train_time:49458ms step_avg:43.73ms
step:1132/2090 train_time:49518ms step_avg:43.74ms
step:1133/2090 train_time:49578ms step_avg:43.76ms
step:1134/2090 train_time:49637ms step_avg:43.77ms
step:1135/2090 train_time:49699ms step_avg:43.79ms
step:1136/2090 train_time:49758ms step_avg:43.80ms
step:1137/2090 train_time:49818ms step_avg:43.82ms
step:1138/2090 train_time:49879ms step_avg:43.83ms
step:1139/2090 train_time:49939ms step_avg:43.84ms
step:1140/2090 train_time:50000ms step_avg:43.86ms
step:1141/2090 train_time:50060ms step_avg:43.87ms
step:1142/2090 train_time:50120ms step_avg:43.89ms
step:1143/2090 train_time:50180ms step_avg:43.90ms
step:1144/2090 train_time:50240ms step_avg:43.92ms
step:1145/2090 train_time:50300ms step_avg:43.93ms
step:1146/2090 train_time:50359ms step_avg:43.94ms
step:1147/2090 train_time:50420ms step_avg:43.96ms
step:1148/2090 train_time:50480ms step_avg:43.97ms
step:1149/2090 train_time:50541ms step_avg:43.99ms
step:1150/2090 train_time:50600ms step_avg:44.00ms
step:1151/2090 train_time:50661ms step_avg:44.01ms
step:1152/2090 train_time:50720ms step_avg:44.03ms
step:1153/2090 train_time:50780ms step_avg:44.04ms
step:1154/2090 train_time:50840ms step_avg:44.06ms
step:1155/2090 train_time:50900ms step_avg:44.07ms
step:1156/2090 train_time:50960ms step_avg:44.08ms
step:1157/2090 train_time:51020ms step_avg:44.10ms
step:1158/2090 train_time:51080ms step_avg:44.11ms
step:1159/2090 train_time:51140ms step_avg:44.12ms
step:1160/2090 train_time:51200ms step_avg:44.14ms
step:1161/2090 train_time:51261ms step_avg:44.15ms
step:1162/2090 train_time:51320ms step_avg:44.16ms
step:1163/2090 train_time:51380ms step_avg:44.18ms
step:1164/2090 train_time:51440ms step_avg:44.19ms
step:1165/2090 train_time:51500ms step_avg:44.21ms
step:1166/2090 train_time:51559ms step_avg:44.22ms
step:1167/2090 train_time:51619ms step_avg:44.23ms
step:1168/2090 train_time:51679ms step_avg:44.25ms
step:1169/2090 train_time:51739ms step_avg:44.26ms
step:1170/2090 train_time:51799ms step_avg:44.27ms
step:1171/2090 train_time:51860ms step_avg:44.29ms
step:1172/2090 train_time:51919ms step_avg:44.30ms
step:1173/2090 train_time:51980ms step_avg:44.31ms
step:1174/2090 train_time:52041ms step_avg:44.33ms
step:1175/2090 train_time:52101ms step_avg:44.34ms
step:1176/2090 train_time:52160ms step_avg:44.35ms
step:1177/2090 train_time:52220ms step_avg:44.37ms
step:1178/2090 train_time:52280ms step_avg:44.38ms
step:1179/2090 train_time:52340ms step_avg:44.39ms
step:1180/2090 train_time:52400ms step_avg:44.41ms
step:1181/2090 train_time:52460ms step_avg:44.42ms
step:1182/2090 train_time:52519ms step_avg:44.43ms
step:1183/2090 train_time:52580ms step_avg:44.45ms
step:1184/2090 train_time:52640ms step_avg:44.46ms
step:1185/2090 train_time:52700ms step_avg:44.47ms
step:1186/2090 train_time:52760ms step_avg:44.49ms
step:1187/2090 train_time:52821ms step_avg:44.50ms
step:1188/2090 train_time:52881ms step_avg:44.51ms
step:1189/2090 train_time:52941ms step_avg:44.53ms
step:1190/2090 train_time:53001ms step_avg:44.54ms
step:1191/2090 train_time:53061ms step_avg:44.55ms
step:1192/2090 train_time:53120ms step_avg:44.56ms
step:1193/2090 train_time:53181ms step_avg:44.58ms
step:1194/2090 train_time:53240ms step_avg:44.59ms
step:1195/2090 train_time:53300ms step_avg:44.60ms
step:1196/2090 train_time:53360ms step_avg:44.62ms
step:1197/2090 train_time:53420ms step_avg:44.63ms
step:1198/2090 train_time:53479ms step_avg:44.64ms
step:1199/2090 train_time:53540ms step_avg:44.65ms
step:1200/2090 train_time:53599ms step_avg:44.67ms
step:1201/2090 train_time:53659ms step_avg:44.68ms
step:1202/2090 train_time:53719ms step_avg:44.69ms
step:1203/2090 train_time:53781ms step_avg:44.71ms
step:1204/2090 train_time:53840ms step_avg:44.72ms
step:1205/2090 train_time:53900ms step_avg:44.73ms
step:1206/2090 train_time:53959ms step_avg:44.74ms
step:1207/2090 train_time:54019ms step_avg:44.76ms
step:1208/2090 train_time:54079ms step_avg:44.77ms
step:1209/2090 train_time:54140ms step_avg:44.78ms
step:1210/2090 train_time:54200ms step_avg:44.79ms
step:1211/2090 train_time:54260ms step_avg:44.81ms
step:1212/2090 train_time:54319ms step_avg:44.82ms
step:1213/2090 train_time:54380ms step_avg:44.83ms
step:1214/2090 train_time:54440ms step_avg:44.84ms
step:1215/2090 train_time:54500ms step_avg:44.86ms
step:1216/2090 train_time:54559ms step_avg:44.87ms
step:1217/2090 train_time:54620ms step_avg:44.88ms
step:1218/2090 train_time:54679ms step_avg:44.89ms
step:1219/2090 train_time:54740ms step_avg:44.91ms
step:1220/2090 train_time:54800ms step_avg:44.92ms
step:1221/2090 train_time:54861ms step_avg:44.93ms
step:1222/2090 train_time:54920ms step_avg:44.94ms
step:1223/2090 train_time:54981ms step_avg:44.96ms
step:1224/2090 train_time:55040ms step_avg:44.97ms
step:1225/2090 train_time:55101ms step_avg:44.98ms
step:1226/2090 train_time:55160ms step_avg:44.99ms
step:1227/2090 train_time:55221ms step_avg:45.00ms
step:1228/2090 train_time:55280ms step_avg:45.02ms
step:1229/2090 train_time:55342ms step_avg:45.03ms
step:1230/2090 train_time:55401ms step_avg:45.04ms
step:1231/2090 train_time:55462ms step_avg:45.05ms
step:1232/2090 train_time:55521ms step_avg:45.07ms
step:1233/2090 train_time:55581ms step_avg:45.08ms
step:1234/2090 train_time:55641ms step_avg:45.09ms
step:1235/2090 train_time:55701ms step_avg:45.10ms
step:1236/2090 train_time:55761ms step_avg:45.11ms
step:1237/2090 train_time:55821ms step_avg:45.13ms
step:1238/2090 train_time:55881ms step_avg:45.14ms
step:1239/2090 train_time:55941ms step_avg:45.15ms
step:1240/2090 train_time:56000ms step_avg:45.16ms
step:1241/2090 train_time:56061ms step_avg:45.17ms
step:1242/2090 train_time:56120ms step_avg:45.19ms
step:1243/2090 train_time:56180ms step_avg:45.20ms
step:1244/2090 train_time:56240ms step_avg:45.21ms
step:1245/2090 train_time:56301ms step_avg:45.22ms
step:1246/2090 train_time:56361ms step_avg:45.23ms
step:1247/2090 train_time:56421ms step_avg:45.25ms
step:1248/2090 train_time:56481ms step_avg:45.26ms
step:1249/2090 train_time:56541ms step_avg:45.27ms
step:1250/2090 train_time:56601ms step_avg:45.28ms
step:1250/2090 val_loss:3.5863 train_time:56663ms step_avg:45.33ms
step:1251/2090 train_time:56683ms step_avg:45.31ms
step:1252/2090 train_time:56722ms step_avg:45.31ms
step:1253/2090 train_time:56784ms step_avg:45.32ms
step:1254/2090 train_time:56850ms step_avg:45.33ms
step:1255/2090 train_time:56911ms step_avg:45.35ms
step:1256/2090 train_time:56971ms step_avg:45.36ms
step:1257/2090 train_time:57031ms step_avg:45.37ms
step:1258/2090 train_time:57089ms step_avg:45.38ms
step:1259/2090 train_time:57150ms step_avg:45.39ms
step:1260/2090 train_time:57208ms step_avg:45.40ms
step:1261/2090 train_time:57268ms step_avg:45.41ms
step:1262/2090 train_time:57327ms step_avg:45.43ms
step:1263/2090 train_time:57386ms step_avg:45.44ms
step:1264/2090 train_time:57444ms step_avg:45.45ms
step:1265/2090 train_time:57504ms step_avg:45.46ms
step:1266/2090 train_time:57562ms step_avg:45.47ms
step:1267/2090 train_time:57623ms step_avg:45.48ms
step:1268/2090 train_time:57682ms step_avg:45.49ms
step:1269/2090 train_time:57743ms step_avg:45.50ms
step:1270/2090 train_time:57803ms step_avg:45.51ms
step:1271/2090 train_time:57864ms step_avg:45.53ms
step:1272/2090 train_time:57923ms step_avg:45.54ms
step:1273/2090 train_time:57983ms step_avg:45.55ms
step:1274/2090 train_time:58042ms step_avg:45.56ms
step:1275/2090 train_time:58102ms step_avg:45.57ms
step:1276/2090 train_time:58160ms step_avg:45.58ms
step:1277/2090 train_time:58220ms step_avg:45.59ms
step:1278/2090 train_time:58279ms step_avg:45.60ms
step:1279/2090 train_time:58339ms step_avg:45.61ms
step:1280/2090 train_time:58398ms step_avg:45.62ms
step:1281/2090 train_time:58458ms step_avg:45.63ms
step:1282/2090 train_time:58517ms step_avg:45.64ms
step:1283/2090 train_time:58576ms step_avg:45.66ms
step:1284/2090 train_time:58636ms step_avg:45.67ms
step:1285/2090 train_time:58697ms step_avg:45.68ms
step:1286/2090 train_time:58758ms step_avg:45.69ms
step:1287/2090 train_time:58818ms step_avg:45.70ms
step:1288/2090 train_time:58879ms step_avg:45.71ms
step:1289/2090 train_time:58940ms step_avg:45.73ms
step:1290/2090 train_time:58999ms step_avg:45.74ms
step:1291/2090 train_time:59059ms step_avg:45.75ms
step:1292/2090 train_time:59118ms step_avg:45.76ms
step:1293/2090 train_time:59178ms step_avg:45.77ms
step:1294/2090 train_time:59237ms step_avg:45.78ms
step:1295/2090 train_time:59297ms step_avg:45.79ms
step:1296/2090 train_time:59356ms step_avg:45.80ms
step:1297/2090 train_time:59416ms step_avg:45.81ms
step:1298/2090 train_time:59475ms step_avg:45.82ms
step:1299/2090 train_time:59535ms step_avg:45.83ms
step:1300/2090 train_time:59594ms step_avg:45.84ms
step:1301/2090 train_time:59654ms step_avg:45.85ms
step:1302/2090 train_time:59714ms step_avg:45.86ms
step:1303/2090 train_time:59775ms step_avg:45.87ms
step:1304/2090 train_time:59835ms step_avg:45.89ms
step:1305/2090 train_time:59896ms step_avg:45.90ms
step:1306/2090 train_time:59957ms step_avg:45.91ms
step:1307/2090 train_time:60017ms step_avg:45.92ms
step:1308/2090 train_time:60077ms step_avg:45.93ms
step:1309/2090 train_time:60137ms step_avg:45.94ms
step:1310/2090 train_time:60197ms step_avg:45.95ms
step:1311/2090 train_time:60257ms step_avg:45.96ms
step:1312/2090 train_time:60316ms step_avg:45.97ms
step:1313/2090 train_time:60376ms step_avg:45.98ms
step:1314/2090 train_time:60435ms step_avg:45.99ms
step:1315/2090 train_time:60495ms step_avg:46.00ms
step:1316/2090 train_time:60555ms step_avg:46.01ms
step:1317/2090 train_time:60615ms step_avg:46.03ms
step:1318/2090 train_time:60675ms step_avg:46.04ms
step:1319/2090 train_time:60736ms step_avg:46.05ms
step:1320/2090 train_time:60796ms step_avg:46.06ms
step:1321/2090 train_time:60857ms step_avg:46.07ms
step:1322/2090 train_time:60917ms step_avg:46.08ms
step:1323/2090 train_time:60978ms step_avg:46.09ms
step:1324/2090 train_time:61037ms step_avg:46.10ms
step:1325/2090 train_time:61097ms step_avg:46.11ms
step:1326/2090 train_time:61157ms step_avg:46.12ms
step:1327/2090 train_time:61218ms step_avg:46.13ms
step:1328/2090 train_time:61276ms step_avg:46.14ms
step:1329/2090 train_time:61337ms step_avg:46.15ms
step:1330/2090 train_time:61396ms step_avg:46.16ms
step:1331/2090 train_time:61456ms step_avg:46.17ms
step:1332/2090 train_time:61515ms step_avg:46.18ms
step:1333/2090 train_time:61575ms step_avg:46.19ms
step:1334/2090 train_time:61634ms step_avg:46.20ms
step:1335/2090 train_time:61695ms step_avg:46.21ms
step:1336/2090 train_time:61755ms step_avg:46.22ms
step:1337/2090 train_time:61816ms step_avg:46.23ms
step:1338/2090 train_time:61876ms step_avg:46.25ms
step:1339/2090 train_time:61937ms step_avg:46.26ms
step:1340/2090 train_time:61996ms step_avg:46.27ms
step:1341/2090 train_time:62057ms step_avg:46.28ms
step:1342/2090 train_time:62117ms step_avg:46.29ms
step:1343/2090 train_time:62178ms step_avg:46.30ms
step:1344/2090 train_time:62237ms step_avg:46.31ms
step:1345/2090 train_time:62298ms step_avg:46.32ms
step:1346/2090 train_time:62357ms step_avg:46.33ms
step:1347/2090 train_time:62417ms step_avg:46.34ms
step:1348/2090 train_time:62476ms step_avg:46.35ms
step:1349/2090 train_time:62537ms step_avg:46.36ms
step:1350/2090 train_time:62596ms step_avg:46.37ms
step:1351/2090 train_time:62656ms step_avg:46.38ms
step:1352/2090 train_time:62716ms step_avg:46.39ms
step:1353/2090 train_time:62776ms step_avg:46.40ms
step:1354/2090 train_time:62837ms step_avg:46.41ms
step:1355/2090 train_time:62897ms step_avg:46.42ms
step:1356/2090 train_time:62957ms step_avg:46.43ms
step:1357/2090 train_time:63017ms step_avg:46.44ms
step:1358/2090 train_time:63076ms step_avg:46.45ms
step:1359/2090 train_time:63137ms step_avg:46.46ms
step:1360/2090 train_time:63197ms step_avg:46.47ms
step:1361/2090 train_time:63258ms step_avg:46.48ms
step:1362/2090 train_time:63317ms step_avg:46.49ms
step:1363/2090 train_time:63377ms step_avg:46.50ms
step:1364/2090 train_time:63437ms step_avg:46.51ms
step:1365/2090 train_time:63497ms step_avg:46.52ms
step:1366/2090 train_time:63556ms step_avg:46.53ms
step:1367/2090 train_time:63617ms step_avg:46.54ms
step:1368/2090 train_time:63677ms step_avg:46.55ms
step:1369/2090 train_time:63766ms step_avg:46.58ms
step:1370/2090 train_time:63852ms step_avg:46.61ms
step:1371/2090 train_time:63940ms step_avg:46.64ms
step:1372/2090 train_time:64027ms step_avg:46.67ms
step:1373/2090 train_time:64115ms step_avg:46.70ms
step:1374/2090 train_time:64202ms step_avg:46.73ms
step:1375/2090 train_time:64291ms step_avg:46.76ms
step:1376/2090 train_time:64377ms step_avg:46.79ms
step:1377/2090 train_time:64465ms step_avg:46.82ms
step:1378/2090 train_time:64552ms step_avg:46.84ms
step:1379/2090 train_time:64639ms step_avg:46.87ms
step:1380/2090 train_time:64726ms step_avg:46.90ms
step:1381/2090 train_time:64814ms step_avg:46.93ms
step:1382/2090 train_time:64900ms step_avg:46.96ms
step:1383/2090 train_time:64989ms step_avg:46.99ms
step:1384/2090 train_time:65076ms step_avg:47.02ms
step:1385/2090 train_time:65164ms step_avg:47.05ms
step:1386/2090 train_time:65251ms step_avg:47.08ms
step:1387/2090 train_time:65338ms step_avg:47.11ms
step:1388/2090 train_time:65426ms step_avg:47.14ms
step:1389/2090 train_time:65514ms step_avg:47.17ms
step:1390/2090 train_time:65602ms step_avg:47.20ms
step:1391/2090 train_time:65689ms step_avg:47.22ms
step:1392/2090 train_time:65775ms step_avg:47.25ms
step:1393/2090 train_time:65863ms step_avg:47.28ms
step:1394/2090 train_time:65950ms step_avg:47.31ms
step:1395/2090 train_time:66037ms step_avg:47.34ms
step:1396/2090 train_time:66125ms step_avg:47.37ms
step:1397/2090 train_time:66213ms step_avg:47.40ms
step:1398/2090 train_time:66300ms step_avg:47.42ms
step:1399/2090 train_time:66389ms step_avg:47.45ms
step:1400/2090 train_time:66475ms step_avg:47.48ms
step:1401/2090 train_time:66563ms step_avg:47.51ms
step:1402/2090 train_time:66651ms step_avg:47.54ms
step:1403/2090 train_time:66739ms step_avg:47.57ms
step:1404/2090 train_time:66825ms step_avg:47.60ms
step:1405/2090 train_time:66914ms step_avg:47.63ms
step:1406/2090 train_time:67001ms step_avg:47.65ms
step:1407/2090 train_time:67090ms step_avg:47.68ms
step:1408/2090 train_time:67176ms step_avg:47.71ms
step:1409/2090 train_time:67265ms step_avg:47.74ms
step:1410/2090 train_time:67351ms step_avg:47.77ms
step:1411/2090 train_time:67439ms step_avg:47.80ms
step:1412/2090 train_time:67526ms step_avg:47.82ms
step:1413/2090 train_time:67614ms step_avg:47.85ms
step:1414/2090 train_time:67700ms step_avg:47.88ms
step:1415/2090 train_time:67790ms step_avg:47.91ms
step:1416/2090 train_time:67876ms step_avg:47.93ms
step:1417/2090 train_time:67964ms step_avg:47.96ms
step:1418/2090 train_time:68050ms step_avg:47.99ms
step:1419/2090 train_time:68138ms step_avg:48.02ms
step:1420/2090 train_time:68226ms step_avg:48.05ms
step:1421/2090 train_time:68314ms step_avg:48.07ms
step:1422/2090 train_time:68401ms step_avg:48.10ms
step:1423/2090 train_time:68490ms step_avg:48.13ms
step:1424/2090 train_time:68576ms step_avg:48.16ms
step:1425/2090 train_time:68665ms step_avg:48.19ms
step:1426/2090 train_time:68751ms step_avg:48.21ms
step:1427/2090 train_time:68839ms step_avg:48.24ms
step:1428/2090 train_time:68927ms step_avg:48.27ms
step:1429/2090 train_time:69014ms step_avg:48.30ms
step:1430/2090 train_time:69101ms step_avg:48.32ms
step:1431/2090 train_time:69189ms step_avg:48.35ms
step:1432/2090 train_time:69275ms step_avg:48.38ms
step:1433/2090 train_time:69363ms step_avg:48.40ms
step:1434/2090 train_time:69450ms step_avg:48.43ms
step:1435/2090 train_time:69538ms step_avg:48.46ms
step:1436/2090 train_time:69624ms step_avg:48.48ms
step:1437/2090 train_time:69713ms step_avg:48.51ms
step:1438/2090 train_time:69800ms step_avg:48.54ms
step:1439/2090 train_time:69889ms step_avg:48.57ms
step:1440/2090 train_time:69976ms step_avg:48.59ms
step:1441/2090 train_time:70064ms step_avg:48.62ms
step:1442/2090 train_time:70151ms step_avg:48.65ms
step:1443/2090 train_time:70238ms step_avg:48.67ms
step:1444/2090 train_time:70325ms step_avg:48.70ms
step:1445/2090 train_time:70413ms step_avg:48.73ms
step:1446/2090 train_time:70499ms step_avg:48.75ms
step:1447/2090 train_time:70588ms step_avg:48.78ms
step:1448/2090 train_time:70674ms step_avg:48.81ms
step:1449/2090 train_time:70763ms step_avg:48.84ms
step:1450/2090 train_time:70850ms step_avg:48.86ms
step:1451/2090 train_time:70937ms step_avg:48.89ms
step:1452/2090 train_time:71024ms step_avg:48.91ms
step:1453/2090 train_time:71112ms step_avg:48.94ms
step:1454/2090 train_time:71199ms step_avg:48.97ms
step:1455/2090 train_time:71287ms step_avg:48.99ms
step:1456/2090 train_time:71374ms step_avg:49.02ms
step:1457/2090 train_time:71463ms step_avg:49.05ms
step:1458/2090 train_time:71550ms step_avg:49.07ms
step:1459/2090 train_time:71637ms step_avg:49.10ms
step:1460/2090 train_time:71724ms step_avg:49.13ms
step:1461/2090 train_time:71812ms step_avg:49.15ms
step:1462/2090 train_time:71899ms step_avg:49.18ms
step:1463/2090 train_time:71987ms step_avg:49.21ms
step:1464/2090 train_time:72073ms step_avg:49.23ms
step:1465/2090 train_time:72161ms step_avg:49.26ms
step:1466/2090 train_time:72248ms step_avg:49.28ms
step:1467/2090 train_time:72336ms step_avg:49.31ms
step:1468/2090 train_time:72423ms step_avg:49.33ms
step:1469/2090 train_time:72512ms step_avg:49.36ms
step:1470/2090 train_time:72599ms step_avg:49.39ms
step:1471/2090 train_time:72687ms step_avg:49.41ms
step:1472/2090 train_time:72773ms step_avg:49.44ms
step:1473/2090 train_time:72861ms step_avg:49.46ms
step:1474/2090 train_time:72948ms step_avg:49.49ms
step:1475/2090 train_time:73036ms step_avg:49.52ms
step:1476/2090 train_time:73123ms step_avg:49.54ms
step:1477/2090 train_time:73212ms step_avg:49.57ms
step:1478/2090 train_time:73298ms step_avg:49.59ms
step:1479/2090 train_time:73387ms step_avg:49.62ms
step:1480/2090 train_time:73473ms step_avg:49.64ms
step:1481/2090 train_time:73562ms step_avg:49.67ms
step:1482/2090 train_time:73650ms step_avg:49.70ms
step:1483/2090 train_time:73737ms step_avg:49.72ms
step:1484/2090 train_time:73823ms step_avg:49.75ms
step:1485/2090 train_time:73911ms step_avg:49.77ms
step:1486/2090 train_time:73998ms step_avg:49.80ms
step:1487/2090 train_time:74086ms step_avg:49.82ms
step:1488/2090 train_time:74172ms step_avg:49.85ms
step:1489/2090 train_time:74260ms step_avg:49.87ms
step:1490/2090 train_time:74348ms step_avg:49.90ms
step:1491/2090 train_time:74435ms step_avg:49.92ms
step:1492/2090 train_time:74522ms step_avg:49.95ms
step:1493/2090 train_time:74610ms step_avg:49.97ms
step:1494/2090 train_time:74697ms step_avg:50.00ms
step:1495/2090 train_time:74784ms step_avg:50.02ms
step:1496/2090 train_time:74871ms step_avg:50.05ms
step:1497/2090 train_time:74959ms step_avg:50.07ms
step:1498/2090 train_time:75046ms step_avg:50.10ms
step:1499/2090 train_time:75135ms step_avg:50.12ms
step:1500/2090 train_time:75222ms step_avg:50.15ms
step:1500/2090 val_loss:3.4756 train_time:75312ms step_avg:50.21ms
step:1501/2090 train_time:75332ms step_avg:50.19ms
step:1502/2090 train_time:75401ms step_avg:50.20ms
step:1503/2090 train_time:75492ms step_avg:50.23ms
step:1504/2090 train_time:75579ms step_avg:50.25ms
step:1505/2090 train_time:75667ms step_avg:50.28ms
step:1506/2090 train_time:75752ms step_avg:50.30ms
step:1507/2090 train_time:75839ms step_avg:50.32ms
step:1508/2090 train_time:75925ms step_avg:50.35ms
step:1509/2090 train_time:76012ms step_avg:50.37ms
step:1510/2090 train_time:76098ms step_avg:50.40ms
step:1511/2090 train_time:76186ms step_avg:50.42ms
step:1512/2090 train_time:76274ms step_avg:50.45ms
step:1513/2090 train_time:76364ms step_avg:50.47ms
step:1514/2090 train_time:76453ms step_avg:50.50ms
step:1515/2090 train_time:76543ms step_avg:50.52ms
step:1516/2090 train_time:76631ms step_avg:50.55ms
step:1517/2090 train_time:76719ms step_avg:50.57ms
step:1518/2090 train_time:76805ms step_avg:50.60ms
step:1519/2090 train_time:76892ms step_avg:50.62ms
step:1520/2090 train_time:76977ms step_avg:50.64ms
step:1521/2090 train_time:77064ms step_avg:50.67ms
step:1522/2090 train_time:77151ms step_avg:50.69ms
step:1523/2090 train_time:77238ms step_avg:50.71ms
step:1524/2090 train_time:77328ms step_avg:50.74ms
step:1525/2090 train_time:77417ms step_avg:50.77ms
step:1526/2090 train_time:77505ms step_avg:50.79ms
step:1527/2090 train_time:77595ms step_avg:50.82ms
step:1528/2090 train_time:77682ms step_avg:50.84ms
step:1529/2090 train_time:77771ms step_avg:50.86ms
step:1530/2090 train_time:77856ms step_avg:50.89ms
step:1531/2090 train_time:77944ms step_avg:50.91ms
step:1532/2090 train_time:78030ms step_avg:50.93ms
step:1533/2090 train_time:78117ms step_avg:50.96ms
step:1534/2090 train_time:78204ms step_avg:50.98ms
step:1535/2090 train_time:78293ms step_avg:51.01ms
step:1536/2090 train_time:78381ms step_avg:51.03ms
step:1537/2090 train_time:78471ms step_avg:51.05ms
step:1538/2090 train_time:78559ms step_avg:51.08ms
step:1539/2090 train_time:78647ms step_avg:51.10ms
step:1540/2090 train_time:78733ms step_avg:51.13ms
step:1541/2090 train_time:78821ms step_avg:51.15ms
step:1542/2090 train_time:78908ms step_avg:51.17ms
step:1543/2090 train_time:78995ms step_avg:51.20ms
step:1544/2090 train_time:79081ms step_avg:51.22ms
step:1545/2090 train_time:79168ms step_avg:51.24ms
step:1546/2090 train_time:79255ms step_avg:51.26ms
step:1547/2090 train_time:79344ms step_avg:51.29ms
step:1548/2090 train_time:79432ms step_avg:51.31ms
step:1549/2090 train_time:79521ms step_avg:51.34ms
step:1550/2090 train_time:79609ms step_avg:51.36ms
step:1551/2090 train_time:79697ms step_avg:51.38ms
step:1552/2090 train_time:79784ms step_avg:51.41ms
step:1553/2090 train_time:79871ms step_avg:51.43ms
step:1554/2090 train_time:79957ms step_avg:51.45ms
step:1555/2090 train_time:80044ms step_avg:51.48ms
step:1556/2090 train_time:80131ms step_avg:51.50ms
step:1557/2090 train_time:80218ms step_avg:51.52ms
step:1558/2090 train_time:80305ms step_avg:51.54ms
step:1559/2090 train_time:80394ms step_avg:51.57ms
step:1560/2090 train_time:80481ms step_avg:51.59ms
step:1561/2090 train_time:80571ms step_avg:51.61ms
step:1562/2090 train_time:80658ms step_avg:51.64ms
step:1563/2090 train_time:80747ms step_avg:51.66ms
step:1564/2090 train_time:80833ms step_avg:51.68ms
step:1565/2090 train_time:80920ms step_avg:51.71ms
step:1566/2090 train_time:81007ms step_avg:51.73ms
step:1567/2090 train_time:81095ms step_avg:51.75ms
step:1568/2090 train_time:81182ms step_avg:51.77ms
step:1569/2090 train_time:81271ms step_avg:51.80ms
step:1570/2090 train_time:81357ms step_avg:51.82ms
step:1571/2090 train_time:81446ms step_avg:51.84ms
step:1572/2090 train_time:81533ms step_avg:51.87ms
step:1573/2090 train_time:81621ms step_avg:51.89ms
step:1574/2090 train_time:81709ms step_avg:51.91ms
step:1575/2090 train_time:81797ms step_avg:51.93ms
step:1576/2090 train_time:81884ms step_avg:51.96ms
step:1577/2090 train_time:81972ms step_avg:51.98ms
step:1578/2090 train_time:82058ms step_avg:52.00ms
step:1579/2090 train_time:82146ms step_avg:52.02ms
step:1580/2090 train_time:82233ms step_avg:52.05ms
step:1581/2090 train_time:82320ms step_avg:52.07ms
step:1582/2090 train_time:82408ms step_avg:52.09ms
step:1583/2090 train_time:82495ms step_avg:52.11ms
step:1584/2090 train_time:82583ms step_avg:52.14ms
step:1585/2090 train_time:82671ms step_avg:52.16ms
step:1586/2090 train_time:82757ms step_avg:52.18ms
step:1587/2090 train_time:82846ms step_avg:52.20ms
step:1588/2090 train_time:82933ms step_avg:52.22ms
step:1589/2090 train_time:83020ms step_avg:52.25ms
step:1590/2090 train_time:83107ms step_avg:52.27ms
step:1591/2090 train_time:83195ms step_avg:52.29ms
step:1592/2090 train_time:83282ms step_avg:52.31ms
step:1593/2090 train_time:83371ms step_avg:52.34ms
step:1594/2090 train_time:83457ms step_avg:52.36ms
step:1595/2090 train_time:83546ms step_avg:52.38ms
step:1596/2090 train_time:83634ms step_avg:52.40ms
step:1597/2090 train_time:83722ms step_avg:52.42ms
step:1598/2090 train_time:83809ms step_avg:52.45ms
step:1599/2090 train_time:83898ms step_avg:52.47ms
step:1600/2090 train_time:83984ms step_avg:52.49ms
step:1601/2090 train_time:84072ms step_avg:52.51ms
step:1602/2090 train_time:84158ms step_avg:52.53ms
step:1603/2090 train_time:84247ms step_avg:52.56ms
step:1604/2090 train_time:84333ms step_avg:52.58ms
step:1605/2090 train_time:84421ms step_avg:52.60ms
step:1606/2090 train_time:84510ms step_avg:52.62ms
step:1607/2090 train_time:84597ms step_avg:52.64ms
step:1608/2090 train_time:84684ms step_avg:52.66ms
step:1609/2090 train_time:84772ms step_avg:52.69ms
step:1610/2090 train_time:84858ms step_avg:52.71ms
step:1611/2090 train_time:84947ms step_avg:52.73ms
step:1612/2090 train_time:85033ms step_avg:52.75ms
step:1613/2090 train_time:85120ms step_avg:52.77ms
step:1614/2090 train_time:85208ms step_avg:52.79ms
step:1615/2090 train_time:85295ms step_avg:52.81ms
step:1616/2090 train_time:85382ms step_avg:52.84ms
step:1617/2090 train_time:85472ms step_avg:52.86ms
step:1618/2090 train_time:85558ms step_avg:52.88ms
step:1619/2090 train_time:85646ms step_avg:52.90ms
step:1620/2090 train_time:85732ms step_avg:52.92ms
step:1621/2090 train_time:85820ms step_avg:52.94ms
step:1622/2090 train_time:85907ms step_avg:52.96ms
step:1623/2090 train_time:85995ms step_avg:52.99ms
step:1624/2090 train_time:86081ms step_avg:53.01ms
step:1625/2090 train_time:86171ms step_avg:53.03ms
step:1626/2090 train_time:86257ms step_avg:53.05ms
step:1627/2090 train_time:86346ms step_avg:53.07ms
step:1628/2090 train_time:86433ms step_avg:53.09ms
step:1629/2090 train_time:86521ms step_avg:53.11ms
step:1630/2090 train_time:86609ms step_avg:53.13ms
step:1631/2090 train_time:86696ms step_avg:53.16ms
step:1632/2090 train_time:86783ms step_avg:53.18ms
step:1633/2090 train_time:86871ms step_avg:53.20ms
step:1634/2090 train_time:86958ms step_avg:53.22ms
step:1635/2090 train_time:87046ms step_avg:53.24ms
step:1636/2090 train_time:87133ms step_avg:53.26ms
step:1637/2090 train_time:87220ms step_avg:53.28ms
step:1638/2090 train_time:87307ms step_avg:53.30ms
step:1639/2090 train_time:87395ms step_avg:53.32ms
step:1640/2090 train_time:87482ms step_avg:53.34ms
step:1641/2090 train_time:87571ms step_avg:53.36ms
step:1642/2090 train_time:87657ms step_avg:53.38ms
step:1643/2090 train_time:87746ms step_avg:53.41ms
step:1644/2090 train_time:87833ms step_avg:53.43ms
step:1645/2090 train_time:87920ms step_avg:53.45ms
step:1646/2090 train_time:88007ms step_avg:53.47ms
step:1647/2090 train_time:88095ms step_avg:53.49ms
step:1648/2090 train_time:88183ms step_avg:53.51ms
step:1649/2090 train_time:88271ms step_avg:53.53ms
step:1650/2090 train_time:88357ms step_avg:53.55ms
step:1651/2090 train_time:88445ms step_avg:53.57ms
step:1652/2090 train_time:88533ms step_avg:53.59ms
step:1653/2090 train_time:88620ms step_avg:53.61ms
step:1654/2090 train_time:88708ms step_avg:53.63ms
step:1655/2090 train_time:88796ms step_avg:53.65ms
step:1656/2090 train_time:88883ms step_avg:53.67ms
step:1657/2090 train_time:88972ms step_avg:53.69ms
step:1658/2090 train_time:89059ms step_avg:53.71ms
step:1659/2090 train_time:89147ms step_avg:53.74ms
step:1660/2090 train_time:89233ms step_avg:53.75ms
step:1661/2090 train_time:89320ms step_avg:53.78ms
step:1662/2090 train_time:89408ms step_avg:53.80ms
step:1663/2090 train_time:89495ms step_avg:53.82ms
step:1664/2090 train_time:89582ms step_avg:53.84ms
step:1665/2090 train_time:89671ms step_avg:53.86ms
step:1666/2090 train_time:89757ms step_avg:53.88ms
step:1667/2090 train_time:89845ms step_avg:53.90ms
step:1668/2090 train_time:89932ms step_avg:53.92ms
step:1669/2090 train_time:90019ms step_avg:53.94ms
step:1670/2090 train_time:90106ms step_avg:53.96ms
step:1671/2090 train_time:90194ms step_avg:53.98ms
step:1672/2090 train_time:90281ms step_avg:54.00ms
step:1673/2090 train_time:90369ms step_avg:54.02ms
step:1674/2090 train_time:90456ms step_avg:54.04ms
step:1675/2090 train_time:90544ms step_avg:54.06ms
step:1676/2090 train_time:90631ms step_avg:54.08ms
step:1677/2090 train_time:90719ms step_avg:54.10ms
step:1678/2090 train_time:90807ms step_avg:54.12ms
step:1679/2090 train_time:90895ms step_avg:54.14ms
step:1680/2090 train_time:90982ms step_avg:54.16ms
step:1681/2090 train_time:91070ms step_avg:54.18ms
step:1682/2090 train_time:91156ms step_avg:54.19ms
step:1683/2090 train_time:91244ms step_avg:54.22ms
step:1684/2090 train_time:91331ms step_avg:54.23ms
step:1685/2090 train_time:91419ms step_avg:54.25ms
step:1686/2090 train_time:91506ms step_avg:54.27ms
step:1687/2090 train_time:91594ms step_avg:54.29ms
step:1688/2090 train_time:91681ms step_avg:54.31ms
step:1689/2090 train_time:91769ms step_avg:54.33ms
step:1690/2090 train_time:91855ms step_avg:54.35ms
step:1691/2090 train_time:91943ms step_avg:54.37ms
step:1692/2090 train_time:92031ms step_avg:54.39ms
step:1693/2090 train_time:92119ms step_avg:54.41ms
step:1694/2090 train_time:92206ms step_avg:54.43ms
step:1695/2090 train_time:92294ms step_avg:54.45ms
step:1696/2090 train_time:92381ms step_avg:54.47ms
step:1697/2090 train_time:92469ms step_avg:54.49ms
step:1698/2090 train_time:92556ms step_avg:54.51ms
step:1699/2090 train_time:92645ms step_avg:54.53ms
step:1700/2090 train_time:92731ms step_avg:54.55ms
step:1701/2090 train_time:92818ms step_avg:54.57ms
step:1702/2090 train_time:92905ms step_avg:54.59ms
step:1703/2090 train_time:92993ms step_avg:54.61ms
step:1704/2090 train_time:93080ms step_avg:54.62ms
step:1705/2090 train_time:93168ms step_avg:54.64ms
step:1706/2090 train_time:93255ms step_avg:54.66ms
step:1707/2090 train_time:93343ms step_avg:54.68ms
step:1708/2090 train_time:93430ms step_avg:54.70ms
step:1709/2090 train_time:93517ms step_avg:54.72ms
step:1710/2090 train_time:93605ms step_avg:54.74ms
step:1711/2090 train_time:93693ms step_avg:54.76ms
step:1712/2090 train_time:93779ms step_avg:54.78ms
step:1713/2090 train_time:93867ms step_avg:54.80ms
step:1714/2090 train_time:93954ms step_avg:54.82ms
step:1715/2090 train_time:94041ms step_avg:54.83ms
step:1716/2090 train_time:94129ms step_avg:54.85ms
step:1717/2090 train_time:94216ms step_avg:54.87ms
step:1718/2090 train_time:94304ms step_avg:54.89ms
step:1719/2090 train_time:94392ms step_avg:54.91ms
step:1720/2090 train_time:94478ms step_avg:54.93ms
step:1721/2090 train_time:94566ms step_avg:54.95ms
step:1722/2090 train_time:94653ms step_avg:54.97ms
step:1723/2090 train_time:94741ms step_avg:54.99ms
step:1724/2090 train_time:94827ms step_avg:55.00ms
step:1725/2090 train_time:94915ms step_avg:55.02ms
step:1726/2090 train_time:95002ms step_avg:55.04ms
step:1727/2090 train_time:95091ms step_avg:55.06ms
step:1728/2090 train_time:95177ms step_avg:55.08ms
step:1729/2090 train_time:95265ms step_avg:55.10ms
step:1730/2090 train_time:95351ms step_avg:55.12ms
step:1731/2090 train_time:95439ms step_avg:55.14ms
step:1732/2090 train_time:95526ms step_avg:55.15ms
step:1733/2090 train_time:95614ms step_avg:55.17ms
step:1734/2090 train_time:95703ms step_avg:55.19ms
step:1735/2090 train_time:95791ms step_avg:55.21ms
step:1736/2090 train_time:95878ms step_avg:55.23ms
step:1737/2090 train_time:95966ms step_avg:55.25ms
step:1738/2090 train_time:96053ms step_avg:55.27ms
step:1739/2090 train_time:96141ms step_avg:55.29ms
step:1740/2090 train_time:96228ms step_avg:55.30ms
step:1741/2090 train_time:96315ms step_avg:55.32ms
step:1742/2090 train_time:96402ms step_avg:55.34ms
step:1743/2090 train_time:96491ms step_avg:55.36ms
step:1744/2090 train_time:96577ms step_avg:55.38ms
step:1745/2090 train_time:96665ms step_avg:55.40ms
step:1746/2090 train_time:96751ms step_avg:55.41ms
step:1747/2090 train_time:96838ms step_avg:55.43ms
step:1748/2090 train_time:96926ms step_avg:55.45ms
step:1749/2090 train_time:97015ms step_avg:55.47ms
step:1750/2090 train_time:97102ms step_avg:55.49ms
step:1750/2090 val_loss:3.3749 train_time:97191ms step_avg:55.54ms
step:1751/2090 train_time:97212ms step_avg:55.52ms
step:1752/2090 train_time:97280ms step_avg:55.53ms
step:1753/2090 train_time:97372ms step_avg:55.55ms
step:1754/2090 train_time:97458ms step_avg:55.56ms
step:1755/2090 train_time:97546ms step_avg:55.58ms
step:1756/2090 train_time:97632ms step_avg:55.60ms
step:1757/2090 train_time:97718ms step_avg:55.62ms
step:1758/2090 train_time:97804ms step_avg:55.63ms
step:1759/2090 train_time:97891ms step_avg:55.65ms
step:1760/2090 train_time:97977ms step_avg:55.67ms
step:1761/2090 train_time:98064ms step_avg:55.69ms
step:1762/2090 train_time:98153ms step_avg:55.71ms
step:1763/2090 train_time:98241ms step_avg:55.72ms
step:1764/2090 train_time:98332ms step_avg:55.74ms
step:1765/2090 train_time:98420ms step_avg:55.76ms
step:1766/2090 train_time:98508ms step_avg:55.78ms
step:1767/2090 train_time:98596ms step_avg:55.80ms
step:1768/2090 train_time:98683ms step_avg:55.82ms
step:1769/2090 train_time:98770ms step_avg:55.83ms
step:1770/2090 train_time:98856ms step_avg:55.85ms
step:1771/2090 train_time:98943ms step_avg:55.87ms
step:1772/2090 train_time:99029ms step_avg:55.89ms
step:1773/2090 train_time:99117ms step_avg:55.90ms
step:1774/2090 train_time:99204ms step_avg:55.92ms
step:1775/2090 train_time:99294ms step_avg:55.94ms
step:1776/2090 train_time:99382ms step_avg:55.96ms
step:1777/2090 train_time:99470ms step_avg:55.98ms
step:1778/2090 train_time:99557ms step_avg:55.99ms
step:1779/2090 train_time:99645ms step_avg:56.01ms
step:1780/2090 train_time:99731ms step_avg:56.03ms
step:1781/2090 train_time:99818ms step_avg:56.05ms
step:1782/2090 train_time:99905ms step_avg:56.06ms
step:1783/2090 train_time:99992ms step_avg:56.08ms
step:1784/2090 train_time:100078ms step_avg:56.10ms
step:1785/2090 train_time:100166ms step_avg:56.12ms
step:1786/2090 train_time:100254ms step_avg:56.13ms
step:1787/2090 train_time:100344ms step_avg:56.15ms
step:1788/2090 train_time:100433ms step_avg:56.17ms
step:1789/2090 train_time:100521ms step_avg:56.19ms
step:1790/2090 train_time:100608ms step_avg:56.21ms
step:1791/2090 train_time:100696ms step_avg:56.22ms
step:1792/2090 train_time:100782ms step_avg:56.24ms
step:1793/2090 train_time:100869ms step_avg:56.26ms
step:1794/2090 train_time:100955ms step_avg:56.27ms
step:1795/2090 train_time:101041ms step_avg:56.29ms
step:1796/2090 train_time:101128ms step_avg:56.31ms
step:1797/2090 train_time:101217ms step_avg:56.33ms
step:1798/2090 train_time:101304ms step_avg:56.34ms
step:1799/2090 train_time:101393ms step_avg:56.36ms
step:1800/2090 train_time:101480ms step_avg:56.38ms
step:1801/2090 train_time:101569ms step_avg:56.40ms
step:1802/2090 train_time:101656ms step_avg:56.41ms
step:1803/2090 train_time:101743ms step_avg:56.43ms
step:1804/2090 train_time:101830ms step_avg:56.45ms
step:1805/2090 train_time:101918ms step_avg:56.46ms
step:1806/2090 train_time:102005ms step_avg:56.48ms
step:1807/2090 train_time:102093ms step_avg:56.50ms
step:1808/2090 train_time:102180ms step_avg:56.52ms
step:1809/2090 train_time:102268ms step_avg:56.53ms
step:1810/2090 train_time:102356ms step_avg:56.55ms
step:1811/2090 train_time:102443ms step_avg:56.57ms
step:1812/2090 train_time:102531ms step_avg:56.58ms
step:1813/2090 train_time:102618ms step_avg:56.60ms
step:1814/2090 train_time:102705ms step_avg:56.62ms
step:1815/2090 train_time:102793ms step_avg:56.64ms
step:1816/2090 train_time:102879ms step_avg:56.65ms
step:1817/2090 train_time:102967ms step_avg:56.67ms
step:1818/2090 train_time:103054ms step_avg:56.69ms
step:1819/2090 train_time:103141ms step_avg:56.70ms
step:1820/2090 train_time:103228ms step_avg:56.72ms
step:1821/2090 train_time:103317ms step_avg:56.74ms
step:1822/2090 train_time:103404ms step_avg:56.75ms
step:1823/2090 train_time:103493ms step_avg:56.77ms
step:1824/2090 train_time:103580ms step_avg:56.79ms
step:1825/2090 train_time:103668ms step_avg:56.80ms
step:1826/2090 train_time:103755ms step_avg:56.82ms
step:1827/2090 train_time:103842ms step_avg:56.84ms
step:1828/2090 train_time:103929ms step_avg:56.85ms
step:1829/2090 train_time:104017ms step_avg:56.87ms
step:1830/2090 train_time:104103ms step_avg:56.89ms
step:1831/2090 train_time:104192ms step_avg:56.90ms
step:1832/2090 train_time:104279ms step_avg:56.92ms
step:1833/2090 train_time:104367ms step_avg:56.94ms
step:1834/2090 train_time:104455ms step_avg:56.95ms
step:1835/2090 train_time:104543ms step_avg:56.97ms
step:1836/2090 train_time:104630ms step_avg:56.99ms
step:1837/2090 train_time:104718ms step_avg:57.00ms
step:1838/2090 train_time:104805ms step_avg:57.02ms
step:1839/2090 train_time:104893ms step_avg:57.04ms
step:1840/2090 train_time:104979ms step_avg:57.05ms
step:1841/2090 train_time:105066ms step_avg:57.07ms
step:1842/2090 train_time:105153ms step_avg:57.09ms
step:1843/2090 train_time:105241ms step_avg:57.10ms
step:1844/2090 train_time:105328ms step_avg:57.12ms
step:1845/2090 train_time:105416ms step_avg:57.14ms
step:1846/2090 train_time:105503ms step_avg:57.15ms
step:1847/2090 train_time:105592ms step_avg:57.17ms
step:1848/2090 train_time:105678ms step_avg:57.19ms
step:1849/2090 train_time:105767ms step_avg:57.20ms
step:1850/2090 train_time:105854ms step_avg:57.22ms
step:1851/2090 train_time:105941ms step_avg:57.23ms
step:1852/2090 train_time:106028ms step_avg:57.25ms
step:1853/2090 train_time:106116ms step_avg:57.27ms
step:1854/2090 train_time:106202ms step_avg:57.28ms
step:1855/2090 train_time:106291ms step_avg:57.30ms
step:1856/2090 train_time:106378ms step_avg:57.32ms
step:1857/2090 train_time:106467ms step_avg:57.33ms
step:1858/2090 train_time:106554ms step_avg:57.35ms
step:1859/2090 train_time:106641ms step_avg:57.36ms
step:1860/2090 train_time:106729ms step_avg:57.38ms
step:1861/2090 train_time:106816ms step_avg:57.40ms
step:1862/2090 train_time:106903ms step_avg:57.41ms
step:1863/2090 train_time:106991ms step_avg:57.43ms
step:1864/2090 train_time:107077ms step_avg:57.44ms
step:1865/2090 train_time:107165ms step_avg:57.46ms
step:1866/2090 train_time:107252ms step_avg:57.48ms
step:1867/2090 train_time:107340ms step_avg:57.49ms
step:1868/2090 train_time:107428ms step_avg:57.51ms
step:1869/2090 train_time:107517ms step_avg:57.53ms
step:1870/2090 train_time:107603ms step_avg:57.54ms
step:1871/2090 train_time:107691ms step_avg:57.56ms
step:1872/2090 train_time:107778ms step_avg:57.57ms
step:1873/2090 train_time:107865ms step_avg:57.59ms
step:1874/2090 train_time:107953ms step_avg:57.61ms
step:1875/2090 train_time:108041ms step_avg:57.62ms
step:1876/2090 train_time:108127ms step_avg:57.64ms
step:1877/2090 train_time:108215ms step_avg:57.65ms
step:1878/2090 train_time:108302ms step_avg:57.67ms
step:1879/2090 train_time:108390ms step_avg:57.68ms
step:1880/2090 train_time:108477ms step_avg:57.70ms
step:1881/2090 train_time:108565ms step_avg:57.72ms
step:1882/2090 train_time:108653ms step_avg:57.73ms
step:1883/2090 train_time:108741ms step_avg:57.75ms
step:1884/2090 train_time:108828ms step_avg:57.76ms
step:1885/2090 train_time:108916ms step_avg:57.78ms
step:1886/2090 train_time:109003ms step_avg:57.80ms
step:1887/2090 train_time:109090ms step_avg:57.81ms
step:1888/2090 train_time:109177ms step_avg:57.83ms
step:1889/2090 train_time:109265ms step_avg:57.84ms
step:1890/2090 train_time:109352ms step_avg:57.86ms
step:1891/2090 train_time:109439ms step_avg:57.87ms
step:1892/2090 train_time:109526ms step_avg:57.89ms
step:1893/2090 train_time:109616ms step_avg:57.91ms
step:1894/2090 train_time:109702ms step_avg:57.92ms
step:1895/2090 train_time:109791ms step_avg:57.94ms
step:1896/2090 train_time:109877ms step_avg:57.95ms
step:1897/2090 train_time:109964ms step_avg:57.97ms
step:1898/2090 train_time:110053ms step_avg:57.98ms
step:1899/2090 train_time:110140ms step_avg:58.00ms
step:1900/2090 train_time:110227ms step_avg:58.01ms
step:1901/2090 train_time:110316ms step_avg:58.03ms
step:1902/2090 train_time:110402ms step_avg:58.05ms
step:1903/2090 train_time:110490ms step_avg:58.06ms
step:1904/2090 train_time:110577ms step_avg:58.08ms
step:1905/2090 train_time:110665ms step_avg:58.09ms
step:1906/2090 train_time:110753ms step_avg:58.11ms
step:1907/2090 train_time:110841ms step_avg:58.12ms
step:1908/2090 train_time:110928ms step_avg:58.14ms
step:1909/2090 train_time:111016ms step_avg:58.15ms
step:1910/2090 train_time:111103ms step_avg:58.17ms
step:1911/2090 train_time:111191ms step_avg:58.18ms
step:1912/2090 train_time:111277ms step_avg:58.20ms
step:1913/2090 train_time:111365ms step_avg:58.21ms
step:1914/2090 train_time:111453ms step_avg:58.23ms
step:1915/2090 train_time:111541ms step_avg:58.25ms
step:1916/2090 train_time:111628ms step_avg:58.26ms
step:1917/2090 train_time:111716ms step_avg:58.28ms
step:1918/2090 train_time:111803ms step_avg:58.29ms
step:1919/2090 train_time:111891ms step_avg:58.31ms
step:1920/2090 train_time:111977ms step_avg:58.32ms
step:1921/2090 train_time:112065ms step_avg:58.34ms
step:1922/2090 train_time:112152ms step_avg:58.35ms
step:1923/2090 train_time:112240ms step_avg:58.37ms
step:1924/2090 train_time:112327ms step_avg:58.38ms
step:1925/2090 train_time:112415ms step_avg:58.40ms
step:1926/2090 train_time:112503ms step_avg:58.41ms
step:1927/2090 train_time:112590ms step_avg:58.43ms
step:1928/2090 train_time:112677ms step_avg:58.44ms
step:1929/2090 train_time:112764ms step_avg:58.46ms
step:1930/2090 train_time:112852ms step_avg:58.47ms
step:1931/2090 train_time:112939ms step_avg:58.49ms
step:1932/2090 train_time:113026ms step_avg:58.50ms
step:1933/2090 train_time:113115ms step_avg:58.52ms
step:1934/2090 train_time:113201ms step_avg:58.53ms
step:1935/2090 train_time:113289ms step_avg:58.55ms
step:1936/2090 train_time:113375ms step_avg:58.56ms
step:1937/2090 train_time:113463ms step_avg:58.58ms
step:1938/2090 train_time:113550ms step_avg:58.59ms
step:1939/2090 train_time:113638ms step_avg:58.61ms
step:1940/2090 train_time:113725ms step_avg:58.62ms
step:1941/2090 train_time:113814ms step_avg:58.64ms
step:1942/2090 train_time:113901ms step_avg:58.65ms
step:1943/2090 train_time:113990ms step_avg:58.67ms
step:1944/2090 train_time:114076ms step_avg:58.68ms
step:1945/2090 train_time:114164ms step_avg:58.70ms
step:1946/2090 train_time:114251ms step_avg:58.71ms
step:1947/2090 train_time:114339ms step_avg:58.73ms
step:1948/2090 train_time:114425ms step_avg:58.74ms
step:1949/2090 train_time:114514ms step_avg:58.76ms
step:1950/2090 train_time:114601ms step_avg:58.77ms
step:1951/2090 train_time:114689ms step_avg:58.78ms
step:1952/2090 train_time:114776ms step_avg:58.80ms
step:1953/2090 train_time:114863ms step_avg:58.81ms
step:1954/2090 train_time:114950ms step_avg:58.83ms
step:1955/2090 train_time:115038ms step_avg:58.84ms
step:1956/2090 train_time:115125ms step_avg:58.86ms
step:1957/2090 train_time:115213ms step_avg:58.87ms
step:1958/2090 train_time:115299ms step_avg:58.89ms
step:1959/2090 train_time:115387ms step_avg:58.90ms
step:1960/2090 train_time:115475ms step_avg:58.92ms
step:1961/2090 train_time:115562ms step_avg:58.93ms
step:1962/2090 train_time:115649ms step_avg:58.94ms
step:1963/2090 train_time:115737ms step_avg:58.96ms
step:1964/2090 train_time:115824ms step_avg:58.97ms
step:1965/2090 train_time:115913ms step_avg:58.99ms
step:1966/2090 train_time:116000ms step_avg:59.00ms
step:1967/2090 train_time:116087ms step_avg:59.02ms
step:1968/2090 train_time:116175ms step_avg:59.03ms
step:1969/2090 train_time:116262ms step_avg:59.05ms
step:1970/2090 train_time:116349ms step_avg:59.06ms
step:1971/2090 train_time:116438ms step_avg:59.08ms
step:1972/2090 train_time:116524ms step_avg:59.09ms
step:1973/2090 train_time:116613ms step_avg:59.10ms
step:1974/2090 train_time:116699ms step_avg:59.12ms
step:1975/2090 train_time:116789ms step_avg:59.13ms
step:1976/2090 train_time:116875ms step_avg:59.15ms
step:1977/2090 train_time:116962ms step_avg:59.16ms
step:1978/2090 train_time:117049ms step_avg:59.18ms
step:1979/2090 train_time:117137ms step_avg:59.19ms
step:1980/2090 train_time:117224ms step_avg:59.20ms
step:1981/2090 train_time:117312ms step_avg:59.22ms
step:1982/2090 train_time:117399ms step_avg:59.23ms
step:1983/2090 train_time:117487ms step_avg:59.25ms
step:1984/2090 train_time:117573ms step_avg:59.26ms
step:1985/2090 train_time:117661ms step_avg:59.27ms
step:1986/2090 train_time:117748ms step_avg:59.29ms
step:1987/2090 train_time:117836ms step_avg:59.30ms
step:1988/2090 train_time:117923ms step_avg:59.32ms
step:1989/2090 train_time:118010ms step_avg:59.33ms
step:1990/2090 train_time:118097ms step_avg:59.35ms
step:1991/2090 train_time:118185ms step_avg:59.36ms
step:1992/2090 train_time:118272ms step_avg:59.37ms
step:1993/2090 train_time:118359ms step_avg:59.39ms
step:1994/2090 train_time:118446ms step_avg:59.40ms
step:1995/2090 train_time:118535ms step_avg:59.42ms
step:1996/2090 train_time:118623ms step_avg:59.43ms
step:1997/2090 train_time:118710ms step_avg:59.44ms
step:1998/2090 train_time:118797ms step_avg:59.46ms
step:1999/2090 train_time:118885ms step_avg:59.47ms
step:2000/2090 train_time:118971ms step_avg:59.49ms
step:2000/2090 val_loss:3.2974 train_time:119061ms step_avg:59.53ms
step:2001/2090 train_time:119081ms step_avg:59.51ms
step:2002/2090 train_time:119149ms step_avg:59.51ms
step:2003/2090 train_time:119244ms step_avg:59.53ms
step:2004/2090 train_time:119332ms step_avg:59.55ms
step:2005/2090 train_time:119419ms step_avg:59.56ms
step:2006/2090 train_time:119505ms step_avg:59.57ms
step:2007/2090 train_time:119592ms step_avg:59.59ms
step:2008/2090 train_time:119678ms step_avg:59.60ms
step:2009/2090 train_time:119764ms step_avg:59.61ms
step:2010/2090 train_time:119851ms step_avg:59.63ms
step:2011/2090 train_time:119938ms step_avg:59.64ms
step:2012/2090 train_time:120025ms step_avg:59.65ms
step:2013/2090 train_time:120116ms step_avg:59.67ms
step:2014/2090 train_time:120205ms step_avg:59.68ms
step:2015/2090 train_time:120295ms step_avg:59.70ms
step:2016/2090 train_time:120381ms step_avg:59.71ms
step:2017/2090 train_time:120468ms step_avg:59.73ms
step:2018/2090 train_time:120555ms step_avg:59.74ms
step:2019/2090 train_time:120642ms step_avg:59.75ms
step:2020/2090 train_time:120728ms step_avg:59.77ms
step:2021/2090 train_time:120815ms step_avg:59.78ms
step:2022/2090 train_time:120902ms step_avg:59.79ms
step:2023/2090 train_time:120989ms step_avg:59.81ms
step:2024/2090 train_time:121077ms step_avg:59.82ms
step:2025/2090 train_time:121166ms step_avg:59.84ms
step:2026/2090 train_time:121254ms step_avg:59.85ms
step:2027/2090 train_time:121343ms step_avg:59.86ms
step:2028/2090 train_time:121429ms step_avg:59.88ms
step:2029/2090 train_time:121517ms step_avg:59.89ms
step:2030/2090 train_time:121604ms step_avg:59.90ms
step:2031/2090 train_time:121691ms step_avg:59.92ms
step:2032/2090 train_time:121778ms step_avg:59.93ms
step:2033/2090 train_time:121864ms step_avg:59.94ms
step:2034/2090 train_time:121951ms step_avg:59.96ms
step:2035/2090 train_time:122041ms step_avg:59.97ms
step:2036/2090 train_time:122128ms step_avg:59.98ms
step:2037/2090 train_time:122217ms step_avg:60.00ms
step:2038/2090 train_time:122304ms step_avg:60.01ms
step:2039/2090 train_time:122393ms step_avg:60.03ms
step:2040/2090 train_time:122479ms step_avg:60.04ms
step:2041/2090 train_time:122567ms step_avg:60.05ms
step:2042/2090 train_time:122653ms step_avg:60.07ms
step:2043/2090 train_time:122741ms step_avg:60.08ms
step:2044/2090 train_time:122828ms step_avg:60.09ms
step:2045/2090 train_time:122915ms step_avg:60.11ms
step:2046/2090 train_time:123002ms step_avg:60.12ms
step:2047/2090 train_time:123091ms step_avg:60.13ms
step:2048/2090 train_time:123179ms step_avg:60.15ms
step:2049/2090 train_time:123267ms step_avg:60.16ms
step:2050/2090 train_time:123355ms step_avg:60.17ms
step:2051/2090 train_time:123444ms step_avg:60.19ms
step:2052/2090 train_time:123532ms step_avg:60.20ms
step:2053/2090 train_time:123620ms step_avg:60.21ms
step:2054/2090 train_time:123707ms step_avg:60.23ms
step:2055/2090 train_time:123796ms step_avg:60.24ms
step:2056/2090 train_time:123882ms step_avg:60.25ms
step:2057/2090 train_time:123970ms step_avg:60.27ms
step:2058/2090 train_time:124058ms step_avg:60.28ms
step:2059/2090 train_time:124146ms step_avg:60.29ms
step:2060/2090 train_time:124233ms step_avg:60.31ms
step:2061/2090 train_time:124322ms step_avg:60.32ms
step:2062/2090 train_time:124409ms step_avg:60.33ms
step:2063/2090 train_time:124499ms step_avg:60.35ms
step:2064/2090 train_time:124586ms step_avg:60.36ms
step:2065/2090 train_time:124675ms step_avg:60.38ms
step:2066/2090 train_time:124761ms step_avg:60.39ms
step:2067/2090 train_time:124849ms step_avg:60.40ms
step:2068/2090 train_time:124936ms step_avg:60.41ms
step:2069/2090 train_time:125024ms step_avg:60.43ms
step:2070/2090 train_time:125111ms step_avg:60.44ms
step:2071/2090 train_time:125200ms step_avg:60.45ms
step:2072/2090 train_time:125288ms step_avg:60.47ms
step:2073/2090 train_time:125376ms step_avg:60.48ms
step:2074/2090 train_time:125463ms step_avg:60.49ms
step:2075/2090 train_time:125552ms step_avg:60.51ms
step:2076/2090 train_time:125639ms step_avg:60.52ms
step:2077/2090 train_time:125727ms step_avg:60.53ms
step:2078/2090 train_time:125814ms step_avg:60.55ms
step:2079/2090 train_time:125902ms step_avg:60.56ms
step:2080/2090 train_time:125989ms step_avg:60.57ms
step:2081/2090 train_time:126076ms step_avg:60.58ms
step:2082/2090 train_time:126163ms step_avg:60.60ms
step:2083/2090 train_time:126251ms step_avg:60.61ms
step:2084/2090 train_time:126340ms step_avg:60.62ms
step:2085/2090 train_time:126428ms step_avg:60.64ms
step:2086/2090 train_time:126516ms step_avg:60.65ms
step:2087/2090 train_time:126604ms step_avg:60.66ms
step:2088/2090 train_time:126692ms step_avg:60.68ms
step:2089/2090 train_time:126780ms step_avg:60.69ms
step:2090/2090 train_time:126867ms step_avg:60.70ms
step:2090/2090 val_loss:3.2765 train_time:126957ms step_avg:60.74ms
peak memory allocated: 29892 MiB reserved: 44076 MiB

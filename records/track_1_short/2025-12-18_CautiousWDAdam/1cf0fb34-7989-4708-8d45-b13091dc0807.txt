import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # weight decay
                if wd != 0:
                    mask = (update * p_slice) >= 0
                    # lr as weight decay  schedule
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)

                    update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)
                
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2050  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 18 12:18:26 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2090 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2090 train_time:88ms step_avg:87.77ms
step:2/2090 train_time:114ms step_avg:56.91ms
step:3/2090 train_time:136ms step_avg:45.25ms
step:4/2090 train_time:159ms step_avg:39.86ms
step:5/2090 train_time:192ms step_avg:38.36ms
step:6/2090 train_time:296ms step_avg:49.32ms
step:7/2090 train_time:315ms step_avg:45.01ms
step:8/2090 train_time:341ms step_avg:42.62ms
step:9/2090 train_time:374ms step_avg:41.52ms
step:10/2090 train_time:407ms step_avg:40.68ms
step:11/2090 train_time:440ms step_avg:40.00ms
step:12/2090 train_time:473ms step_avg:39.41ms
step:13/2090 train_time:506ms step_avg:38.94ms
step:14/2090 train_time:539ms step_avg:38.51ms
step:15/2090 train_time:572ms step_avg:38.17ms
step:16/2090 train_time:605ms step_avg:37.84ms
step:17/2090 train_time:639ms step_avg:37.58ms
step:18/2090 train_time:672ms step_avg:37.31ms
step:19/2090 train_time:705ms step_avg:37.12ms
step:20/2090 train_time:738ms step_avg:36.92ms
step:21/2090 train_time:772ms step_avg:36.75ms
step:22/2090 train_time:805ms step_avg:36.58ms
step:23/2090 train_time:838ms step_avg:36.46ms
step:24/2090 train_time:871ms step_avg:36.31ms
step:25/2090 train_time:905ms step_avg:36.19ms
step:26/2090 train_time:938ms step_avg:36.07ms
step:27/2090 train_time:971ms step_avg:35.98ms
step:28/2090 train_time:1004ms step_avg:35.87ms
step:29/2090 train_time:1038ms step_avg:35.80ms
step:30/2090 train_time:1071ms step_avg:35.71ms
step:31/2090 train_time:1104ms step_avg:35.63ms
step:32/2090 train_time:1137ms step_avg:35.54ms
step:33/2090 train_time:1171ms step_avg:35.49ms
step:34/2090 train_time:1204ms step_avg:35.42ms
step:35/2090 train_time:1239ms step_avg:35.40ms
step:36/2090 train_time:1272ms step_avg:35.34ms
step:37/2090 train_time:1307ms step_avg:35.32ms
step:38/2090 train_time:1340ms step_avg:35.26ms
step:39/2090 train_time:1374ms step_avg:35.23ms
step:40/2090 train_time:1407ms step_avg:35.18ms
step:41/2090 train_time:1441ms step_avg:35.14ms
step:42/2090 train_time:1473ms step_avg:35.08ms
step:43/2090 train_time:1507ms step_avg:35.05ms
step:44/2090 train_time:1540ms step_avg:35.00ms
step:45/2090 train_time:1573ms step_avg:34.97ms
step:46/2090 train_time:1606ms step_avg:34.92ms
step:47/2090 train_time:1640ms step_avg:34.89ms
step:48/2090 train_time:1673ms step_avg:34.85ms
step:49/2090 train_time:1706ms step_avg:34.81ms
step:50/2090 train_time:1739ms step_avg:34.78ms
step:51/2090 train_time:1772ms step_avg:34.75ms
step:52/2090 train_time:1805ms step_avg:34.72ms
step:53/2090 train_time:1839ms step_avg:34.69ms
step:54/2090 train_time:1872ms step_avg:34.66ms
step:55/2090 train_time:1905ms step_avg:34.64ms
step:56/2090 train_time:1938ms step_avg:34.61ms
step:57/2090 train_time:1972ms step_avg:34.59ms
step:58/2090 train_time:2005ms step_avg:34.56ms
step:59/2090 train_time:2038ms step_avg:34.54ms
step:60/2090 train_time:2071ms step_avg:34.51ms
step:61/2090 train_time:2104ms step_avg:34.50ms
step:62/2090 train_time:2137ms step_avg:34.47ms
step:63/2090 train_time:2171ms step_avg:34.46ms
step:64/2090 train_time:2204ms step_avg:34.43ms
step:65/2090 train_time:2237ms step_avg:34.42ms
step:66/2090 train_time:2270ms step_avg:34.40ms
step:67/2090 train_time:2304ms step_avg:34.39ms
step:68/2090 train_time:2337ms step_avg:34.36ms
step:69/2090 train_time:2370ms step_avg:34.35ms
step:70/2090 train_time:2404ms step_avg:34.34ms
step:71/2090 train_time:2437ms step_avg:34.33ms
step:72/2090 train_time:2470ms step_avg:34.31ms
step:73/2090 train_time:2503ms step_avg:34.29ms
step:74/2090 train_time:2536ms step_avg:34.28ms
step:75/2090 train_time:2570ms step_avg:34.27ms
step:76/2090 train_time:2603ms step_avg:34.25ms
step:77/2090 train_time:2637ms step_avg:34.24ms
step:78/2090 train_time:2670ms step_avg:34.23ms
step:79/2090 train_time:2703ms step_avg:34.21ms
step:80/2090 train_time:2736ms step_avg:34.20ms
step:81/2090 train_time:2769ms step_avg:34.19ms
step:82/2090 train_time:2802ms step_avg:34.17ms
step:83/2090 train_time:2836ms step_avg:34.17ms
step:84/2090 train_time:2869ms step_avg:34.15ms
step:85/2090 train_time:2902ms step_avg:34.14ms
step:86/2090 train_time:2935ms step_avg:34.13ms
step:87/2090 train_time:2968ms step_avg:34.12ms
step:88/2090 train_time:3001ms step_avg:34.10ms
step:89/2090 train_time:3034ms step_avg:34.09ms
step:90/2090 train_time:3067ms step_avg:34.08ms
step:91/2090 train_time:3100ms step_avg:34.07ms
step:92/2090 train_time:3133ms step_avg:34.06ms
step:93/2090 train_time:3167ms step_avg:34.05ms
step:94/2090 train_time:3199ms step_avg:34.04ms
step:95/2090 train_time:3233ms step_avg:34.03ms
step:96/2090 train_time:3265ms step_avg:34.02ms
step:97/2090 train_time:3299ms step_avg:34.01ms
step:98/2090 train_time:3332ms step_avg:34.00ms
step:99/2090 train_time:3365ms step_avg:33.99ms
step:100/2090 train_time:3398ms step_avg:33.98ms
step:101/2090 train_time:3431ms step_avg:33.97ms
step:102/2090 train_time:3464ms step_avg:33.96ms
step:103/2090 train_time:3498ms step_avg:33.96ms
step:104/2090 train_time:3531ms step_avg:33.95ms
step:105/2090 train_time:3564ms step_avg:33.95ms
step:106/2090 train_time:3597ms step_avg:33.94ms
step:107/2090 train_time:3631ms step_avg:33.93ms
step:108/2090 train_time:3663ms step_avg:33.92ms
step:109/2090 train_time:3697ms step_avg:33.92ms
step:110/2090 train_time:3730ms step_avg:33.91ms
step:111/2090 train_time:3763ms step_avg:33.90ms
step:112/2090 train_time:3796ms step_avg:33.89ms
step:113/2090 train_time:3829ms step_avg:33.89ms
step:114/2090 train_time:3862ms step_avg:33.88ms
step:115/2090 train_time:3895ms step_avg:33.87ms
step:116/2090 train_time:3928ms step_avg:33.86ms
step:117/2090 train_time:3961ms step_avg:33.86ms
step:118/2090 train_time:3994ms step_avg:33.85ms
step:119/2090 train_time:4027ms step_avg:33.84ms
step:120/2090 train_time:4060ms step_avg:33.83ms
step:121/2090 train_time:4094ms step_avg:33.83ms
step:122/2090 train_time:4127ms step_avg:33.82ms
step:123/2090 train_time:4160ms step_avg:33.82ms
step:124/2090 train_time:4193ms step_avg:33.81ms
step:125/2090 train_time:4226ms step_avg:33.81ms
step:126/2090 train_time:4259ms step_avg:33.80ms
step:127/2090 train_time:4292ms step_avg:33.80ms
step:128/2090 train_time:4325ms step_avg:33.79ms
step:129/2090 train_time:4358ms step_avg:33.78ms
step:130/2090 train_time:4391ms step_avg:33.78ms
step:131/2090 train_time:4424ms step_avg:33.77ms
step:132/2090 train_time:4457ms step_avg:33.77ms
step:133/2090 train_time:4491ms step_avg:33.77ms
step:134/2090 train_time:4524ms step_avg:33.76ms
step:135/2090 train_time:4557ms step_avg:33.76ms
step:136/2090 train_time:4590ms step_avg:33.75ms
step:137/2090 train_time:4623ms step_avg:33.75ms
step:138/2090 train_time:4656ms step_avg:33.74ms
step:139/2090 train_time:4690ms step_avg:33.74ms
step:140/2090 train_time:4723ms step_avg:33.74ms
step:141/2090 train_time:4756ms step_avg:33.73ms
step:142/2090 train_time:4789ms step_avg:33.72ms
step:143/2090 train_time:4822ms step_avg:33.72ms
step:144/2090 train_time:4855ms step_avg:33.71ms
step:145/2090 train_time:4888ms step_avg:33.71ms
step:146/2090 train_time:4920ms step_avg:33.70ms
step:147/2090 train_time:4954ms step_avg:33.70ms
step:148/2090 train_time:4987ms step_avg:33.70ms
step:149/2090 train_time:5020ms step_avg:33.69ms
step:150/2090 train_time:5053ms step_avg:33.69ms
step:151/2090 train_time:5086ms step_avg:33.68ms
step:152/2090 train_time:5118ms step_avg:33.67ms
step:153/2090 train_time:5152ms step_avg:33.68ms
step:154/2090 train_time:5185ms step_avg:33.67ms
step:155/2090 train_time:5219ms step_avg:33.67ms
step:156/2090 train_time:5252ms step_avg:33.66ms
step:157/2090 train_time:5284ms step_avg:33.66ms
step:158/2090 train_time:5317ms step_avg:33.65ms
step:159/2090 train_time:5350ms step_avg:33.65ms
step:160/2090 train_time:5383ms step_avg:33.65ms
step:161/2090 train_time:5417ms step_avg:33.64ms
step:162/2090 train_time:5449ms step_avg:33.64ms
step:163/2090 train_time:5483ms step_avg:33.64ms
step:164/2090 train_time:5516ms step_avg:33.63ms
step:165/2090 train_time:5549ms step_avg:33.63ms
step:166/2090 train_time:5582ms step_avg:33.62ms
step:167/2090 train_time:5615ms step_avg:33.62ms
step:168/2090 train_time:5648ms step_avg:33.62ms
step:169/2090 train_time:5681ms step_avg:33.62ms
step:170/2090 train_time:5714ms step_avg:33.61ms
step:171/2090 train_time:5747ms step_avg:33.61ms
step:172/2090 train_time:5780ms step_avg:33.61ms
step:173/2090 train_time:5813ms step_avg:33.60ms
step:174/2090 train_time:5846ms step_avg:33.60ms
step:175/2090 train_time:5879ms step_avg:33.60ms
step:176/2090 train_time:5912ms step_avg:33.59ms
step:177/2090 train_time:5946ms step_avg:33.59ms
step:178/2090 train_time:5978ms step_avg:33.59ms
step:179/2090 train_time:6012ms step_avg:33.59ms
step:180/2090 train_time:6045ms step_avg:33.58ms
step:181/2090 train_time:6078ms step_avg:33.58ms
step:182/2090 train_time:6111ms step_avg:33.58ms
step:183/2090 train_time:6144ms step_avg:33.57ms
step:184/2090 train_time:6177ms step_avg:33.57ms
step:185/2090 train_time:6210ms step_avg:33.57ms
step:186/2090 train_time:6243ms step_avg:33.56ms
step:187/2090 train_time:6276ms step_avg:33.56ms
step:188/2090 train_time:6309ms step_avg:33.56ms
step:189/2090 train_time:6342ms step_avg:33.56ms
step:190/2090 train_time:6375ms step_avg:33.55ms
step:191/2090 train_time:6408ms step_avg:33.55ms
step:192/2090 train_time:6441ms step_avg:33.55ms
step:193/2090 train_time:6474ms step_avg:33.55ms
step:194/2090 train_time:6507ms step_avg:33.54ms
step:195/2090 train_time:6540ms step_avg:33.54ms
step:196/2090 train_time:6573ms step_avg:33.54ms
step:197/2090 train_time:6606ms step_avg:33.53ms
step:198/2090 train_time:6639ms step_avg:33.53ms
step:199/2090 train_time:6672ms step_avg:33.53ms
step:200/2090 train_time:6705ms step_avg:33.53ms
step:201/2090 train_time:6739ms step_avg:33.52ms
step:202/2090 train_time:6771ms step_avg:33.52ms
step:203/2090 train_time:6805ms step_avg:33.52ms
step:204/2090 train_time:6837ms step_avg:33.52ms
step:205/2090 train_time:6871ms step_avg:33.52ms
step:206/2090 train_time:6904ms step_avg:33.51ms
step:207/2090 train_time:6938ms step_avg:33.51ms
step:208/2090 train_time:6970ms step_avg:33.51ms
step:209/2090 train_time:7004ms step_avg:33.51ms
step:210/2090 train_time:7037ms step_avg:33.51ms
step:211/2090 train_time:7070ms step_avg:33.51ms
step:212/2090 train_time:7103ms step_avg:33.51ms
step:213/2090 train_time:7137ms step_avg:33.51ms
step:214/2090 train_time:7170ms step_avg:33.51ms
step:215/2090 train_time:7203ms step_avg:33.50ms
step:216/2090 train_time:7236ms step_avg:33.50ms
step:217/2090 train_time:7270ms step_avg:33.50ms
step:218/2090 train_time:7302ms step_avg:33.50ms
step:219/2090 train_time:7336ms step_avg:33.50ms
step:220/2090 train_time:7368ms step_avg:33.49ms
step:221/2090 train_time:7401ms step_avg:33.49ms
step:222/2090 train_time:7434ms step_avg:33.49ms
step:223/2090 train_time:7467ms step_avg:33.49ms
step:224/2090 train_time:7500ms step_avg:33.48ms
step:225/2090 train_time:7534ms step_avg:33.48ms
step:226/2090 train_time:7566ms step_avg:33.48ms
step:227/2090 train_time:7600ms step_avg:33.48ms
step:228/2090 train_time:7632ms step_avg:33.47ms
step:229/2090 train_time:7666ms step_avg:33.47ms
step:230/2090 train_time:7698ms step_avg:33.47ms
step:231/2090 train_time:7731ms step_avg:33.47ms
step:232/2090 train_time:7764ms step_avg:33.47ms
step:233/2090 train_time:7797ms step_avg:33.47ms
step:234/2090 train_time:7830ms step_avg:33.46ms
step:235/2090 train_time:7863ms step_avg:33.46ms
step:236/2090 train_time:7897ms step_avg:33.46ms
step:237/2090 train_time:7929ms step_avg:33.46ms
step:238/2090 train_time:7962ms step_avg:33.45ms
step:239/2090 train_time:7996ms step_avg:33.46ms
step:240/2090 train_time:8029ms step_avg:33.45ms
step:241/2090 train_time:8062ms step_avg:33.45ms
step:242/2090 train_time:8095ms step_avg:33.45ms
step:243/2090 train_time:8128ms step_avg:33.45ms
step:244/2090 train_time:8161ms step_avg:33.45ms
step:245/2090 train_time:8194ms step_avg:33.45ms
step:246/2090 train_time:8227ms step_avg:33.44ms
step:247/2090 train_time:8260ms step_avg:33.44ms
step:248/2090 train_time:8293ms step_avg:33.44ms
step:249/2090 train_time:8326ms step_avg:33.44ms
step:250/2090 train_time:8359ms step_avg:33.44ms
step:250/2090 val_loss:4.2797 train_time:8395ms step_avg:33.58ms
step:251/2090 train_time:8414ms step_avg:33.52ms
step:252/2090 train_time:8434ms step_avg:33.47ms
step:253/2090 train_time:8462ms step_avg:33.45ms
step:254/2090 train_time:8495ms step_avg:33.44ms
step:255/2090 train_time:8530ms step_avg:33.45ms
step:256/2090 train_time:8563ms step_avg:33.45ms
step:257/2090 train_time:8597ms step_avg:33.45ms
step:258/2090 train_time:8630ms step_avg:33.45ms
step:259/2090 train_time:8663ms step_avg:33.45ms
step:260/2090 train_time:8696ms step_avg:33.45ms
step:261/2090 train_time:8729ms step_avg:33.44ms
step:262/2090 train_time:8762ms step_avg:33.44ms
step:263/2090 train_time:8794ms step_avg:33.44ms
step:264/2090 train_time:8827ms step_avg:33.44ms
step:265/2090 train_time:8860ms step_avg:33.43ms
step:266/2090 train_time:8893ms step_avg:33.43ms
step:267/2090 train_time:8926ms step_avg:33.43ms
step:268/2090 train_time:8959ms step_avg:33.43ms
step:269/2090 train_time:8992ms step_avg:33.43ms
step:270/2090 train_time:9024ms step_avg:33.42ms
step:271/2090 train_time:9057ms step_avg:33.42ms
step:272/2090 train_time:9090ms step_avg:33.42ms
step:273/2090 train_time:9123ms step_avg:33.42ms
step:274/2090 train_time:9156ms step_avg:33.41ms
step:275/2090 train_time:9188ms step_avg:33.41ms
step:276/2090 train_time:9221ms step_avg:33.41ms
step:277/2090 train_time:9254ms step_avg:33.41ms
step:278/2090 train_time:9287ms step_avg:33.41ms
step:279/2090 train_time:9320ms step_avg:33.40ms
step:280/2090 train_time:9353ms step_avg:33.40ms
step:281/2090 train_time:9386ms step_avg:33.40ms
step:282/2090 train_time:9419ms step_avg:33.40ms
step:283/2090 train_time:9452ms step_avg:33.40ms
step:284/2090 train_time:9485ms step_avg:33.40ms
step:285/2090 train_time:9519ms step_avg:33.40ms
step:286/2090 train_time:9551ms step_avg:33.40ms
step:287/2090 train_time:9585ms step_avg:33.40ms
step:288/2090 train_time:9618ms step_avg:33.40ms
step:289/2090 train_time:9651ms step_avg:33.40ms
step:290/2090 train_time:9684ms step_avg:33.39ms
step:291/2090 train_time:9717ms step_avg:33.39ms
step:292/2090 train_time:9751ms step_avg:33.39ms
step:293/2090 train_time:9783ms step_avg:33.39ms
step:294/2090 train_time:9816ms step_avg:33.39ms
step:295/2090 train_time:9849ms step_avg:33.39ms
step:296/2090 train_time:9882ms step_avg:33.38ms
step:297/2090 train_time:9915ms step_avg:33.38ms
step:298/2090 train_time:9948ms step_avg:33.38ms
step:299/2090 train_time:9981ms step_avg:33.38ms
step:300/2090 train_time:10014ms step_avg:33.38ms
step:301/2090 train_time:10047ms step_avg:33.38ms
step:302/2090 train_time:10080ms step_avg:33.38ms
step:303/2090 train_time:10113ms step_avg:33.38ms
step:304/2090 train_time:10145ms step_avg:33.37ms
step:305/2090 train_time:10179ms step_avg:33.37ms
step:306/2090 train_time:10212ms step_avg:33.37ms
step:307/2090 train_time:10245ms step_avg:33.37ms
step:308/2090 train_time:10278ms step_avg:33.37ms
step:309/2090 train_time:10310ms step_avg:33.37ms
step:310/2090 train_time:10343ms step_avg:33.36ms
step:311/2090 train_time:10377ms step_avg:33.37ms
step:312/2090 train_time:10409ms step_avg:33.36ms
step:313/2090 train_time:10443ms step_avg:33.36ms
step:314/2090 train_time:10476ms step_avg:33.36ms
step:315/2090 train_time:10509ms step_avg:33.36ms
step:316/2090 train_time:10542ms step_avg:33.36ms
step:317/2090 train_time:10575ms step_avg:33.36ms
step:318/2090 train_time:10608ms step_avg:33.36ms
step:319/2090 train_time:10641ms step_avg:33.36ms
step:320/2090 train_time:10674ms step_avg:33.36ms
step:321/2090 train_time:10707ms step_avg:33.36ms
step:322/2090 train_time:10740ms step_avg:33.35ms
step:323/2090 train_time:10773ms step_avg:33.35ms
step:324/2090 train_time:10806ms step_avg:33.35ms
step:325/2090 train_time:10839ms step_avg:33.35ms
step:326/2090 train_time:10872ms step_avg:33.35ms
step:327/2090 train_time:10905ms step_avg:33.35ms
step:328/2090 train_time:10938ms step_avg:33.35ms
step:329/2090 train_time:10971ms step_avg:33.35ms
step:330/2090 train_time:11004ms step_avg:33.34ms
step:331/2090 train_time:11037ms step_avg:33.34ms
step:332/2090 train_time:11070ms step_avg:33.34ms
step:333/2090 train_time:11102ms step_avg:33.34ms
step:334/2090 train_time:11135ms step_avg:33.34ms
step:335/2090 train_time:11168ms step_avg:33.34ms
step:336/2090 train_time:11201ms step_avg:33.34ms
step:337/2090 train_time:11234ms step_avg:33.33ms
step:338/2090 train_time:11266ms step_avg:33.33ms
step:339/2090 train_time:11300ms step_avg:33.33ms
step:340/2090 train_time:11333ms step_avg:33.33ms
step:341/2090 train_time:11366ms step_avg:33.33ms
step:342/2090 train_time:11399ms step_avg:33.33ms
step:343/2090 train_time:11432ms step_avg:33.33ms
step:344/2090 train_time:11465ms step_avg:33.33ms
step:345/2090 train_time:11498ms step_avg:33.33ms
step:346/2090 train_time:11531ms step_avg:33.33ms
step:347/2090 train_time:11564ms step_avg:33.33ms
step:348/2090 train_time:11597ms step_avg:33.32ms
step:349/2090 train_time:11629ms step_avg:33.32ms
step:350/2090 train_time:11662ms step_avg:33.32ms
step:351/2090 train_time:11695ms step_avg:33.32ms
step:352/2090 train_time:11728ms step_avg:33.32ms
step:353/2090 train_time:11761ms step_avg:33.32ms
step:354/2090 train_time:11794ms step_avg:33.32ms
step:355/2090 train_time:11827ms step_avg:33.32ms
step:356/2090 train_time:11860ms step_avg:33.31ms
step:357/2090 train_time:11893ms step_avg:33.31ms
step:358/2090 train_time:11925ms step_avg:33.31ms
step:359/2090 train_time:11958ms step_avg:33.31ms
step:360/2090 train_time:11991ms step_avg:33.31ms
step:361/2090 train_time:12024ms step_avg:33.31ms
step:362/2090 train_time:12057ms step_avg:33.31ms
step:363/2090 train_time:12090ms step_avg:33.31ms
step:364/2090 train_time:12122ms step_avg:33.30ms
step:365/2090 train_time:12155ms step_avg:33.30ms
step:366/2090 train_time:12188ms step_avg:33.30ms
step:367/2090 train_time:12221ms step_avg:33.30ms
step:368/2090 train_time:12254ms step_avg:33.30ms
step:369/2090 train_time:12287ms step_avg:33.30ms
step:370/2090 train_time:12320ms step_avg:33.30ms
step:371/2090 train_time:12353ms step_avg:33.30ms
step:372/2090 train_time:12386ms step_avg:33.30ms
step:373/2090 train_time:12419ms step_avg:33.30ms
step:374/2090 train_time:12452ms step_avg:33.29ms
step:375/2090 train_time:12486ms step_avg:33.29ms
step:376/2090 train_time:12518ms step_avg:33.29ms
step:377/2090 train_time:12551ms step_avg:33.29ms
step:378/2090 train_time:12584ms step_avg:33.29ms
step:379/2090 train_time:12617ms step_avg:33.29ms
step:380/2090 train_time:12650ms step_avg:33.29ms
step:381/2090 train_time:12683ms step_avg:33.29ms
step:382/2090 train_time:12716ms step_avg:33.29ms
step:383/2090 train_time:12749ms step_avg:33.29ms
step:384/2090 train_time:12782ms step_avg:33.29ms
step:385/2090 train_time:12815ms step_avg:33.29ms
step:386/2090 train_time:12848ms step_avg:33.28ms
step:387/2090 train_time:12881ms step_avg:33.29ms
step:388/2090 train_time:12914ms step_avg:33.28ms
step:389/2090 train_time:12948ms step_avg:33.28ms
step:390/2090 train_time:12981ms step_avg:33.28ms
step:391/2090 train_time:13014ms step_avg:33.28ms
step:392/2090 train_time:13046ms step_avg:33.28ms
step:393/2090 train_time:13080ms step_avg:33.28ms
step:394/2090 train_time:13113ms step_avg:33.28ms
step:395/2090 train_time:13145ms step_avg:33.28ms
step:396/2090 train_time:13179ms step_avg:33.28ms
step:397/2090 train_time:13211ms step_avg:33.28ms
step:398/2090 train_time:13244ms step_avg:33.28ms
step:399/2090 train_time:13277ms step_avg:33.28ms
step:400/2090 train_time:13310ms step_avg:33.28ms
step:401/2090 train_time:13343ms step_avg:33.28ms
step:402/2090 train_time:13376ms step_avg:33.27ms
step:403/2090 train_time:13409ms step_avg:33.27ms
step:404/2090 train_time:13442ms step_avg:33.27ms
step:405/2090 train_time:13475ms step_avg:33.27ms
step:406/2090 train_time:13508ms step_avg:33.27ms
step:407/2090 train_time:13541ms step_avg:33.27ms
step:408/2090 train_time:13574ms step_avg:33.27ms
step:409/2090 train_time:13607ms step_avg:33.27ms
step:410/2090 train_time:13640ms step_avg:33.27ms
step:411/2090 train_time:13673ms step_avg:33.27ms
step:412/2090 train_time:13706ms step_avg:33.27ms
step:413/2090 train_time:13739ms step_avg:33.27ms
step:414/2090 train_time:13772ms step_avg:33.27ms
step:415/2090 train_time:13805ms step_avg:33.26ms
step:416/2090 train_time:13838ms step_avg:33.26ms
step:417/2090 train_time:13871ms step_avg:33.26ms
step:418/2090 train_time:13903ms step_avg:33.26ms
step:419/2090 train_time:13937ms step_avg:33.26ms
step:420/2090 train_time:13970ms step_avg:33.26ms
step:421/2090 train_time:14002ms step_avg:33.26ms
step:422/2090 train_time:14035ms step_avg:33.26ms
step:423/2090 train_time:14069ms step_avg:33.26ms
step:424/2090 train_time:14101ms step_avg:33.26ms
step:425/2090 train_time:14134ms step_avg:33.26ms
step:426/2090 train_time:14167ms step_avg:33.26ms
step:427/2090 train_time:14200ms step_avg:33.26ms
step:428/2090 train_time:14234ms step_avg:33.26ms
step:429/2090 train_time:14266ms step_avg:33.26ms
step:430/2090 train_time:14299ms step_avg:33.25ms
step:431/2090 train_time:14332ms step_avg:33.25ms
step:432/2090 train_time:14365ms step_avg:33.25ms
step:433/2090 train_time:14398ms step_avg:33.25ms
step:434/2090 train_time:14431ms step_avg:33.25ms
step:435/2090 train_time:14464ms step_avg:33.25ms
step:436/2090 train_time:14497ms step_avg:33.25ms
step:437/2090 train_time:14530ms step_avg:33.25ms
step:438/2090 train_time:14563ms step_avg:33.25ms
step:439/2090 train_time:14596ms step_avg:33.25ms
step:440/2090 train_time:14629ms step_avg:33.25ms
step:441/2090 train_time:14662ms step_avg:33.25ms
step:442/2090 train_time:14695ms step_avg:33.25ms
step:443/2090 train_time:14727ms step_avg:33.24ms
step:444/2090 train_time:14760ms step_avg:33.24ms
step:445/2090 train_time:14793ms step_avg:33.24ms
step:446/2090 train_time:14826ms step_avg:33.24ms
step:447/2090 train_time:14859ms step_avg:33.24ms
step:448/2090 train_time:14892ms step_avg:33.24ms
step:449/2090 train_time:14926ms step_avg:33.24ms
step:450/2090 train_time:14958ms step_avg:33.24ms
step:451/2090 train_time:14991ms step_avg:33.24ms
step:452/2090 train_time:15024ms step_avg:33.24ms
step:453/2090 train_time:15057ms step_avg:33.24ms
step:454/2090 train_time:15090ms step_avg:33.24ms
step:455/2090 train_time:15123ms step_avg:33.24ms
step:456/2090 train_time:15156ms step_avg:33.24ms
step:457/2090 train_time:15189ms step_avg:33.24ms
step:458/2090 train_time:15222ms step_avg:33.24ms
step:459/2090 train_time:15255ms step_avg:33.23ms
step:460/2090 train_time:15287ms step_avg:33.23ms
step:461/2090 train_time:15320ms step_avg:33.23ms
step:462/2090 train_time:15353ms step_avg:33.23ms
step:463/2090 train_time:15386ms step_avg:33.23ms
step:464/2090 train_time:15419ms step_avg:33.23ms
step:465/2090 train_time:15452ms step_avg:33.23ms
step:466/2090 train_time:15485ms step_avg:33.23ms
step:467/2090 train_time:15518ms step_avg:33.23ms
step:468/2090 train_time:15551ms step_avg:33.23ms
step:469/2090 train_time:15584ms step_avg:33.23ms
step:470/2090 train_time:15617ms step_avg:33.23ms
step:471/2090 train_time:15650ms step_avg:33.23ms
step:472/2090 train_time:15683ms step_avg:33.23ms
step:473/2090 train_time:15716ms step_avg:33.23ms
step:474/2090 train_time:15748ms step_avg:33.22ms
step:475/2090 train_time:15782ms step_avg:33.22ms
step:476/2090 train_time:15815ms step_avg:33.22ms
step:477/2090 train_time:15848ms step_avg:33.22ms
step:478/2090 train_time:15880ms step_avg:33.22ms
step:479/2090 train_time:15913ms step_avg:33.22ms
step:480/2090 train_time:15946ms step_avg:33.22ms
step:481/2090 train_time:15979ms step_avg:33.22ms
step:482/2090 train_time:16012ms step_avg:33.22ms
step:483/2090 train_time:16045ms step_avg:33.22ms
step:484/2090 train_time:16078ms step_avg:33.22ms
step:485/2090 train_time:16111ms step_avg:33.22ms
step:486/2090 train_time:16144ms step_avg:33.22ms
step:487/2090 train_time:16177ms step_avg:33.22ms
step:488/2090 train_time:16210ms step_avg:33.22ms
step:489/2090 train_time:16243ms step_avg:33.22ms
step:490/2090 train_time:16275ms step_avg:33.21ms
step:491/2090 train_time:16309ms step_avg:33.22ms
step:492/2090 train_time:16341ms step_avg:33.21ms
step:493/2090 train_time:16375ms step_avg:33.21ms
step:494/2090 train_time:16407ms step_avg:33.21ms
step:495/2090 train_time:16440ms step_avg:33.21ms
step:496/2090 train_time:16473ms step_avg:33.21ms
step:497/2090 train_time:16506ms step_avg:33.21ms
step:498/2090 train_time:16539ms step_avg:33.21ms
step:499/2090 train_time:16572ms step_avg:33.21ms
step:500/2090 train_time:16605ms step_avg:33.21ms
step:500/2090 val_loss:4.0112 train_time:16641ms step_avg:33.28ms
step:501/2090 train_time:16660ms step_avg:33.25ms
step:502/2090 train_time:16680ms step_avg:33.23ms
step:503/2090 train_time:16708ms step_avg:33.22ms
step:504/2090 train_time:16741ms step_avg:33.22ms
step:505/2090 train_time:16775ms step_avg:33.22ms
step:506/2090 train_time:16809ms step_avg:33.22ms
step:507/2090 train_time:16842ms step_avg:33.22ms
step:508/2090 train_time:16875ms step_avg:33.22ms
step:509/2090 train_time:16909ms step_avg:33.22ms
step:510/2090 train_time:16941ms step_avg:33.22ms
step:511/2090 train_time:16974ms step_avg:33.22ms
step:512/2090 train_time:17007ms step_avg:33.22ms
step:513/2090 train_time:17040ms step_avg:33.22ms
step:514/2090 train_time:17073ms step_avg:33.22ms
step:515/2090 train_time:17106ms step_avg:33.21ms
step:516/2090 train_time:17138ms step_avg:33.21ms
step:517/2090 train_time:17172ms step_avg:33.21ms
step:518/2090 train_time:17204ms step_avg:33.21ms
step:519/2090 train_time:17237ms step_avg:33.21ms
step:520/2090 train_time:17270ms step_avg:33.21ms
step:521/2090 train_time:17303ms step_avg:33.21ms
step:522/2090 train_time:17336ms step_avg:33.21ms
step:523/2090 train_time:17368ms step_avg:33.21ms
step:524/2090 train_time:17401ms step_avg:33.21ms
step:525/2090 train_time:17434ms step_avg:33.21ms
step:526/2090 train_time:17467ms step_avg:33.21ms
step:527/2090 train_time:17500ms step_avg:33.21ms
step:528/2090 train_time:17533ms step_avg:33.21ms
step:529/2090 train_time:17566ms step_avg:33.21ms
step:530/2090 train_time:17598ms step_avg:33.20ms
step:531/2090 train_time:17632ms step_avg:33.20ms
step:532/2090 train_time:17665ms step_avg:33.20ms
step:533/2090 train_time:17698ms step_avg:33.20ms
step:534/2090 train_time:17731ms step_avg:33.20ms
step:535/2090 train_time:17765ms step_avg:33.21ms
step:536/2090 train_time:17798ms step_avg:33.20ms
step:537/2090 train_time:17831ms step_avg:33.20ms
step:538/2090 train_time:17864ms step_avg:33.20ms
step:539/2090 train_time:17898ms step_avg:33.21ms
step:540/2090 train_time:17930ms step_avg:33.20ms
step:541/2090 train_time:17964ms step_avg:33.20ms
step:542/2090 train_time:17996ms step_avg:33.20ms
step:543/2090 train_time:18029ms step_avg:33.20ms
step:544/2090 train_time:18062ms step_avg:33.20ms
step:545/2090 train_time:18095ms step_avg:33.20ms
step:546/2090 train_time:18128ms step_avg:33.20ms
step:547/2090 train_time:18161ms step_avg:33.20ms
step:548/2090 train_time:18194ms step_avg:33.20ms
step:549/2090 train_time:18227ms step_avg:33.20ms
step:550/2090 train_time:18260ms step_avg:33.20ms
step:551/2090 train_time:18293ms step_avg:33.20ms
step:552/2090 train_time:18326ms step_avg:33.20ms
step:553/2090 train_time:18359ms step_avg:33.20ms
step:554/2090 train_time:18392ms step_avg:33.20ms
step:555/2090 train_time:18425ms step_avg:33.20ms
step:556/2090 train_time:18458ms step_avg:33.20ms
step:557/2090 train_time:18491ms step_avg:33.20ms
step:558/2090 train_time:18523ms step_avg:33.20ms
step:559/2090 train_time:18556ms step_avg:33.19ms
step:560/2090 train_time:18589ms step_avg:33.19ms
step:561/2090 train_time:18622ms step_avg:33.19ms
step:562/2090 train_time:18654ms step_avg:33.19ms
step:563/2090 train_time:18688ms step_avg:33.19ms
step:564/2090 train_time:18720ms step_avg:33.19ms
step:565/2090 train_time:18754ms step_avg:33.19ms
step:566/2090 train_time:18786ms step_avg:33.19ms
step:567/2090 train_time:18820ms step_avg:33.19ms
step:568/2090 train_time:18853ms step_avg:33.19ms
step:569/2090 train_time:18886ms step_avg:33.19ms
step:570/2090 train_time:18919ms step_avg:33.19ms
step:571/2090 train_time:18952ms step_avg:33.19ms
step:572/2090 train_time:18985ms step_avg:33.19ms
step:573/2090 train_time:19018ms step_avg:33.19ms
step:574/2090 train_time:19051ms step_avg:33.19ms
step:575/2090 train_time:19085ms step_avg:33.19ms
step:576/2090 train_time:19117ms step_avg:33.19ms
step:577/2090 train_time:19150ms step_avg:33.19ms
step:578/2090 train_time:19183ms step_avg:33.19ms
step:579/2090 train_time:19216ms step_avg:33.19ms
step:580/2090 train_time:19249ms step_avg:33.19ms
step:581/2090 train_time:19282ms step_avg:33.19ms
step:582/2090 train_time:19315ms step_avg:33.19ms
step:583/2090 train_time:19348ms step_avg:33.19ms
step:584/2090 train_time:19381ms step_avg:33.19ms
step:585/2090 train_time:19413ms step_avg:33.19ms
step:586/2090 train_time:19446ms step_avg:33.18ms
step:587/2090 train_time:19479ms step_avg:33.18ms
step:588/2090 train_time:19512ms step_avg:33.18ms
step:589/2090 train_time:19545ms step_avg:33.18ms
step:590/2090 train_time:19578ms step_avg:33.18ms
step:591/2090 train_time:19611ms step_avg:33.18ms
step:592/2090 train_time:19644ms step_avg:33.18ms
step:593/2090 train_time:19677ms step_avg:33.18ms
step:594/2090 train_time:19710ms step_avg:33.18ms
step:595/2090 train_time:19743ms step_avg:33.18ms
step:596/2090 train_time:19776ms step_avg:33.18ms
step:597/2090 train_time:19809ms step_avg:33.18ms
step:598/2090 train_time:19842ms step_avg:33.18ms
step:599/2090 train_time:19875ms step_avg:33.18ms
step:600/2090 train_time:19908ms step_avg:33.18ms
step:601/2090 train_time:19941ms step_avg:33.18ms
step:602/2090 train_time:19974ms step_avg:33.18ms
step:603/2090 train_time:20007ms step_avg:33.18ms
step:604/2090 train_time:20040ms step_avg:33.18ms
step:605/2090 train_time:20072ms step_avg:33.18ms
step:606/2090 train_time:20105ms step_avg:33.18ms
step:607/2090 train_time:20138ms step_avg:33.18ms
step:608/2090 train_time:20171ms step_avg:33.18ms
step:609/2090 train_time:20204ms step_avg:33.18ms
step:610/2090 train_time:20237ms step_avg:33.18ms
step:611/2090 train_time:20270ms step_avg:33.18ms
step:612/2090 train_time:20303ms step_avg:33.18ms
step:613/2090 train_time:20336ms step_avg:33.17ms
step:614/2090 train_time:20369ms step_avg:33.17ms
step:615/2090 train_time:20402ms step_avg:33.17ms
step:616/2090 train_time:20435ms step_avg:33.17ms
step:617/2090 train_time:20468ms step_avg:33.17ms
step:618/2090 train_time:20501ms step_avg:33.17ms
step:619/2090 train_time:20533ms step_avg:33.17ms
step:620/2090 train_time:20566ms step_avg:33.17ms
step:621/2090 train_time:20599ms step_avg:33.17ms
step:622/2090 train_time:20632ms step_avg:33.17ms
step:623/2090 train_time:20665ms step_avg:33.17ms
step:624/2090 train_time:20698ms step_avg:33.17ms
step:625/2090 train_time:20730ms step_avg:33.17ms
step:626/2090 train_time:20763ms step_avg:33.17ms
step:627/2090 train_time:20797ms step_avg:33.17ms
step:628/2090 train_time:20830ms step_avg:33.17ms
step:629/2090 train_time:20863ms step_avg:33.17ms
step:630/2090 train_time:20896ms step_avg:33.17ms
step:631/2090 train_time:20929ms step_avg:33.17ms
step:632/2090 train_time:20962ms step_avg:33.17ms
step:633/2090 train_time:20995ms step_avg:33.17ms
step:634/2090 train_time:21028ms step_avg:33.17ms
step:635/2090 train_time:21060ms step_avg:33.17ms
step:636/2090 train_time:21093ms step_avg:33.17ms
step:637/2090 train_time:21127ms step_avg:33.17ms
step:638/2090 train_time:21160ms step_avg:33.17ms
step:639/2090 train_time:21192ms step_avg:33.16ms
step:640/2090 train_time:21225ms step_avg:33.16ms
step:641/2090 train_time:21259ms step_avg:33.16ms
step:642/2090 train_time:21291ms step_avg:33.16ms
step:643/2090 train_time:21325ms step_avg:33.16ms
step:644/2090 train_time:21357ms step_avg:33.16ms
step:645/2090 train_time:21391ms step_avg:33.16ms
step:646/2090 train_time:21424ms step_avg:33.16ms
step:647/2090 train_time:21457ms step_avg:33.16ms
step:648/2090 train_time:21490ms step_avg:33.16ms
step:649/2090 train_time:21523ms step_avg:33.16ms
step:650/2090 train_time:21556ms step_avg:33.16ms
step:651/2090 train_time:21589ms step_avg:33.16ms
step:652/2090 train_time:21622ms step_avg:33.16ms
step:653/2090 train_time:21655ms step_avg:33.16ms
step:654/2090 train_time:21688ms step_avg:33.16ms
step:655/2090 train_time:21721ms step_avg:33.16ms
step:656/2090 train_time:21754ms step_avg:33.16ms
step:657/2090 train_time:21786ms step_avg:33.16ms
step:658/2090 train_time:21819ms step_avg:33.16ms
step:659/2090 train_time:21852ms step_avg:33.16ms
step:660/2090 train_time:21885ms step_avg:33.16ms
step:661/2090 train_time:21918ms step_avg:33.16ms
step:662/2090 train_time:21951ms step_avg:33.16ms
step:663/2090 train_time:21984ms step_avg:33.16ms
step:664/2090 train_time:22017ms step_avg:33.16ms
step:665/2090 train_time:22050ms step_avg:33.16ms
step:666/2090 train_time:22083ms step_avg:33.16ms
step:667/2090 train_time:22117ms step_avg:33.16ms
step:668/2090 train_time:22149ms step_avg:33.16ms
step:669/2090 train_time:22183ms step_avg:33.16ms
step:670/2090 train_time:22216ms step_avg:33.16ms
step:671/2090 train_time:22249ms step_avg:33.16ms
step:672/2090 train_time:22282ms step_avg:33.16ms
step:673/2090 train_time:22315ms step_avg:33.16ms
step:674/2090 train_time:22348ms step_avg:33.16ms
step:675/2090 train_time:22381ms step_avg:33.16ms
step:676/2090 train_time:22414ms step_avg:33.16ms
step:677/2090 train_time:22447ms step_avg:33.16ms
step:678/2090 train_time:22480ms step_avg:33.16ms
step:679/2090 train_time:22513ms step_avg:33.16ms
step:680/2090 train_time:22546ms step_avg:33.16ms
step:681/2090 train_time:22579ms step_avg:33.16ms
step:682/2090 train_time:22612ms step_avg:33.16ms
step:683/2090 train_time:22645ms step_avg:33.16ms
step:684/2090 train_time:22678ms step_avg:33.16ms
step:685/2090 train_time:22712ms step_avg:33.16ms
step:686/2090 train_time:22770ms step_avg:33.19ms
step:687/2090 train_time:22831ms step_avg:33.23ms
step:688/2090 train_time:22890ms step_avg:33.27ms
step:689/2090 train_time:22950ms step_avg:33.31ms
step:690/2090 train_time:23010ms step_avg:33.35ms
step:691/2090 train_time:23070ms step_avg:33.39ms
step:692/2090 train_time:23130ms step_avg:33.42ms
step:693/2090 train_time:23190ms step_avg:33.46ms
step:694/2090 train_time:23250ms step_avg:33.50ms
step:695/2090 train_time:23310ms step_avg:33.54ms
step:696/2090 train_time:23369ms step_avg:33.58ms
step:697/2090 train_time:23430ms step_avg:33.62ms
step:698/2090 train_time:23489ms step_avg:33.65ms
step:699/2090 train_time:23550ms step_avg:33.69ms
step:700/2090 train_time:23610ms step_avg:33.73ms
step:701/2090 train_time:23671ms step_avg:33.77ms
step:702/2090 train_time:23730ms step_avg:33.80ms
step:703/2090 train_time:23790ms step_avg:33.84ms
step:704/2090 train_time:23849ms step_avg:33.88ms
step:705/2090 train_time:23909ms step_avg:33.91ms
step:706/2090 train_time:23969ms step_avg:33.95ms
step:707/2090 train_time:24030ms step_avg:33.99ms
step:708/2090 train_time:24088ms step_avg:34.02ms
step:709/2090 train_time:24149ms step_avg:34.06ms
step:710/2090 train_time:24209ms step_avg:34.10ms
step:711/2090 train_time:24270ms step_avg:34.13ms
step:712/2090 train_time:24329ms step_avg:34.17ms
step:713/2090 train_time:24390ms step_avg:34.21ms
step:714/2090 train_time:24449ms step_avg:34.24ms
step:715/2090 train_time:24509ms step_avg:34.28ms
step:716/2090 train_time:24568ms step_avg:34.31ms
step:717/2090 train_time:24627ms step_avg:34.35ms
step:718/2090 train_time:24687ms step_avg:34.38ms
step:719/2090 train_time:24748ms step_avg:34.42ms
step:720/2090 train_time:24807ms step_avg:34.45ms
step:721/2090 train_time:24867ms step_avg:34.49ms
step:722/2090 train_time:24927ms step_avg:34.52ms
step:723/2090 train_time:24987ms step_avg:34.56ms
step:724/2090 train_time:25047ms step_avg:34.59ms
step:725/2090 train_time:25107ms step_avg:34.63ms
step:726/2090 train_time:25167ms step_avg:34.67ms
step:727/2090 train_time:25228ms step_avg:34.70ms
step:728/2090 train_time:25287ms step_avg:34.73ms
step:729/2090 train_time:25347ms step_avg:34.77ms
step:730/2090 train_time:25406ms step_avg:34.80ms
step:731/2090 train_time:25466ms step_avg:34.84ms
step:732/2090 train_time:25526ms step_avg:34.87ms
step:733/2090 train_time:25585ms step_avg:34.91ms
step:734/2090 train_time:25645ms step_avg:34.94ms
step:735/2090 train_time:25705ms step_avg:34.97ms
step:736/2090 train_time:25764ms step_avg:35.01ms
step:737/2090 train_time:25825ms step_avg:35.04ms
step:738/2090 train_time:25885ms step_avg:35.07ms
step:739/2090 train_time:25946ms step_avg:35.11ms
step:740/2090 train_time:26005ms step_avg:35.14ms
step:741/2090 train_time:26065ms step_avg:35.18ms
step:742/2090 train_time:26125ms step_avg:35.21ms
step:743/2090 train_time:26186ms step_avg:35.24ms
step:744/2090 train_time:26246ms step_avg:35.28ms
step:745/2090 train_time:26306ms step_avg:35.31ms
step:746/2090 train_time:26366ms step_avg:35.34ms
step:747/2090 train_time:26426ms step_avg:35.38ms
step:748/2090 train_time:26485ms step_avg:35.41ms
step:749/2090 train_time:26546ms step_avg:35.44ms
step:750/2090 train_time:26606ms step_avg:35.47ms
step:750/2090 val_loss:3.8639 train_time:26668ms step_avg:35.56ms
step:751/2090 train_time:26688ms step_avg:35.54ms
step:752/2090 train_time:26727ms step_avg:35.54ms
step:753/2090 train_time:26790ms step_avg:35.58ms
step:754/2090 train_time:26854ms step_avg:35.61ms
step:755/2090 train_time:26914ms step_avg:35.65ms
step:756/2090 train_time:26973ms step_avg:35.68ms
step:757/2090 train_time:27033ms step_avg:35.71ms
step:758/2090 train_time:27091ms step_avg:35.74ms
step:759/2090 train_time:27151ms step_avg:35.77ms
step:760/2090 train_time:27209ms step_avg:35.80ms
step:761/2090 train_time:27268ms step_avg:35.83ms
step:762/2090 train_time:27327ms step_avg:35.86ms
step:763/2090 train_time:27386ms step_avg:35.89ms
step:764/2090 train_time:27445ms step_avg:35.92ms
step:765/2090 train_time:27505ms step_avg:35.95ms
step:766/2090 train_time:27564ms step_avg:35.98ms
step:767/2090 train_time:27624ms step_avg:36.02ms
step:768/2090 train_time:27685ms step_avg:36.05ms
step:769/2090 train_time:27746ms step_avg:36.08ms
step:770/2090 train_time:27807ms step_avg:36.11ms
step:771/2090 train_time:27869ms step_avg:36.15ms
step:772/2090 train_time:27929ms step_avg:36.18ms
step:773/2090 train_time:27990ms step_avg:36.21ms
step:774/2090 train_time:28049ms step_avg:36.24ms
step:775/2090 train_time:28109ms step_avg:36.27ms
step:776/2090 train_time:28167ms step_avg:36.30ms
step:777/2090 train_time:28227ms step_avg:36.33ms
step:778/2090 train_time:28286ms step_avg:36.36ms
step:779/2090 train_time:28346ms step_avg:36.39ms
step:780/2090 train_time:28404ms step_avg:36.42ms
step:781/2090 train_time:28464ms step_avg:36.45ms
step:782/2090 train_time:28523ms step_avg:36.47ms
step:783/2090 train_time:28582ms step_avg:36.50ms
step:784/2090 train_time:28642ms step_avg:36.53ms
step:785/2090 train_time:28703ms step_avg:36.56ms
step:786/2090 train_time:28763ms step_avg:36.59ms
step:787/2090 train_time:28824ms step_avg:36.62ms
step:788/2090 train_time:28884ms step_avg:36.65ms
step:789/2090 train_time:28945ms step_avg:36.69ms
step:790/2090 train_time:29004ms step_avg:36.71ms
step:791/2090 train_time:29064ms step_avg:36.74ms
step:792/2090 train_time:29125ms step_avg:36.77ms
step:793/2090 train_time:29185ms step_avg:36.80ms
step:794/2090 train_time:29244ms step_avg:36.83ms
step:795/2090 train_time:29304ms step_avg:36.86ms
step:796/2090 train_time:29363ms step_avg:36.89ms
step:797/2090 train_time:29423ms step_avg:36.92ms
step:798/2090 train_time:29482ms step_avg:36.94ms
step:799/2090 train_time:29542ms step_avg:36.97ms
step:800/2090 train_time:29601ms step_avg:37.00ms
step:801/2090 train_time:29662ms step_avg:37.03ms
step:802/2090 train_time:29721ms step_avg:37.06ms
step:803/2090 train_time:29783ms step_avg:37.09ms
step:804/2090 train_time:29842ms step_avg:37.12ms
step:805/2090 train_time:29904ms step_avg:37.15ms
step:806/2090 train_time:29964ms step_avg:37.18ms
step:807/2090 train_time:30025ms step_avg:37.21ms
step:808/2090 train_time:30085ms step_avg:37.23ms
step:809/2090 train_time:30145ms step_avg:37.26ms
step:810/2090 train_time:30204ms step_avg:37.29ms
step:811/2090 train_time:30264ms step_avg:37.32ms
step:812/2090 train_time:30323ms step_avg:37.34ms
step:813/2090 train_time:30383ms step_avg:37.37ms
step:814/2090 train_time:30441ms step_avg:37.40ms
step:815/2090 train_time:30501ms step_avg:37.43ms
step:816/2090 train_time:30561ms step_avg:37.45ms
step:817/2090 train_time:30621ms step_avg:37.48ms
step:818/2090 train_time:30680ms step_avg:37.51ms
step:819/2090 train_time:30741ms step_avg:37.53ms
step:820/2090 train_time:30800ms step_avg:37.56ms
step:821/2090 train_time:30862ms step_avg:37.59ms
step:822/2090 train_time:30922ms step_avg:37.62ms
step:823/2090 train_time:30983ms step_avg:37.65ms
step:824/2090 train_time:31043ms step_avg:37.67ms
step:825/2090 train_time:31103ms step_avg:37.70ms
step:826/2090 train_time:31163ms step_avg:37.73ms
step:827/2090 train_time:31224ms step_avg:37.76ms
step:828/2090 train_time:31283ms step_avg:37.78ms
step:829/2090 train_time:31343ms step_avg:37.81ms
step:830/2090 train_time:31402ms step_avg:37.83ms
step:831/2090 train_time:31462ms step_avg:37.86ms
step:832/2090 train_time:31521ms step_avg:37.89ms
step:833/2090 train_time:31582ms step_avg:37.91ms
step:834/2090 train_time:31641ms step_avg:37.94ms
step:835/2090 train_time:31701ms step_avg:37.97ms
step:836/2090 train_time:31761ms step_avg:37.99ms
step:837/2090 train_time:31822ms step_avg:38.02ms
step:838/2090 train_time:31881ms step_avg:38.04ms
step:839/2090 train_time:31942ms step_avg:38.07ms
step:840/2090 train_time:32001ms step_avg:38.10ms
step:841/2090 train_time:32062ms step_avg:38.12ms
step:842/2090 train_time:32121ms step_avg:38.15ms
step:843/2090 train_time:32182ms step_avg:38.18ms
step:844/2090 train_time:32241ms step_avg:38.20ms
step:845/2090 train_time:32302ms step_avg:38.23ms
step:846/2090 train_time:32361ms step_avg:38.25ms
step:847/2090 train_time:32421ms step_avg:38.28ms
step:848/2090 train_time:32480ms step_avg:38.30ms
step:849/2090 train_time:32541ms step_avg:38.33ms
step:850/2090 train_time:32600ms step_avg:38.35ms
step:851/2090 train_time:32660ms step_avg:38.38ms
step:852/2090 train_time:32720ms step_avg:38.40ms
step:853/2090 train_time:32781ms step_avg:38.43ms
step:854/2090 train_time:32840ms step_avg:38.45ms
step:855/2090 train_time:32901ms step_avg:38.48ms
step:856/2090 train_time:32961ms step_avg:38.51ms
step:857/2090 train_time:33022ms step_avg:38.53ms
step:858/2090 train_time:33081ms step_avg:38.56ms
step:859/2090 train_time:33141ms step_avg:38.58ms
step:860/2090 train_time:33200ms step_avg:38.61ms
step:861/2090 train_time:33261ms step_avg:38.63ms
step:862/2090 train_time:33320ms step_avg:38.65ms
step:863/2090 train_time:33381ms step_avg:38.68ms
step:864/2090 train_time:33439ms step_avg:38.70ms
step:865/2090 train_time:33499ms step_avg:38.73ms
step:866/2090 train_time:33558ms step_avg:38.75ms
step:867/2090 train_time:33618ms step_avg:38.78ms
step:868/2090 train_time:33678ms step_avg:38.80ms
step:869/2090 train_time:33739ms step_avg:38.83ms
step:870/2090 train_time:33798ms step_avg:38.85ms
step:871/2090 train_time:33859ms step_avg:38.87ms
step:872/2090 train_time:33919ms step_avg:38.90ms
step:873/2090 train_time:33979ms step_avg:38.92ms
step:874/2090 train_time:34039ms step_avg:38.95ms
step:875/2090 train_time:34099ms step_avg:38.97ms
step:876/2090 train_time:34159ms step_avg:38.99ms
step:877/2090 train_time:34219ms step_avg:39.02ms
step:878/2090 train_time:34279ms step_avg:39.04ms
step:879/2090 train_time:34339ms step_avg:39.07ms
step:880/2090 train_time:34398ms step_avg:39.09ms
step:881/2090 train_time:34458ms step_avg:39.11ms
step:882/2090 train_time:34518ms step_avg:39.14ms
step:883/2090 train_time:34578ms step_avg:39.16ms
step:884/2090 train_time:34637ms step_avg:39.18ms
step:885/2090 train_time:34697ms step_avg:39.21ms
step:886/2090 train_time:34756ms step_avg:39.23ms
step:887/2090 train_time:34816ms step_avg:39.25ms
step:888/2090 train_time:34875ms step_avg:39.27ms
step:889/2090 train_time:34936ms step_avg:39.30ms
step:890/2090 train_time:34996ms step_avg:39.32ms
step:891/2090 train_time:35056ms step_avg:39.34ms
step:892/2090 train_time:35117ms step_avg:39.37ms
step:893/2090 train_time:35176ms step_avg:39.39ms
step:894/2090 train_time:35236ms step_avg:39.41ms
step:895/2090 train_time:35296ms step_avg:39.44ms
step:896/2090 train_time:35356ms step_avg:39.46ms
step:897/2090 train_time:35416ms step_avg:39.48ms
step:898/2090 train_time:35475ms step_avg:39.50ms
step:899/2090 train_time:35535ms step_avg:39.53ms
step:900/2090 train_time:35594ms step_avg:39.55ms
step:901/2090 train_time:35655ms step_avg:39.57ms
step:902/2090 train_time:35715ms step_avg:39.60ms
step:903/2090 train_time:35774ms step_avg:39.62ms
step:904/2090 train_time:35834ms step_avg:39.64ms
step:905/2090 train_time:35894ms step_avg:39.66ms
step:906/2090 train_time:35954ms step_avg:39.68ms
step:907/2090 train_time:36014ms step_avg:39.71ms
step:908/2090 train_time:36074ms step_avg:39.73ms
step:909/2090 train_time:36134ms step_avg:39.75ms
step:910/2090 train_time:36193ms step_avg:39.77ms
step:911/2090 train_time:36254ms step_avg:39.80ms
step:912/2090 train_time:36314ms step_avg:39.82ms
step:913/2090 train_time:36374ms step_avg:39.84ms
step:914/2090 train_time:36434ms step_avg:39.86ms
step:915/2090 train_time:36494ms step_avg:39.88ms
step:916/2090 train_time:36553ms step_avg:39.91ms
step:917/2090 train_time:36613ms step_avg:39.93ms
step:918/2090 train_time:36672ms step_avg:39.95ms
step:919/2090 train_time:36733ms step_avg:39.97ms
step:920/2090 train_time:36792ms step_avg:39.99ms
step:921/2090 train_time:36852ms step_avg:40.01ms
step:922/2090 train_time:36911ms step_avg:40.03ms
step:923/2090 train_time:36971ms step_avg:40.05ms
step:924/2090 train_time:37031ms step_avg:40.08ms
step:925/2090 train_time:37090ms step_avg:40.10ms
step:926/2090 train_time:37150ms step_avg:40.12ms
step:927/2090 train_time:37210ms step_avg:40.14ms
step:928/2090 train_time:37269ms step_avg:40.16ms
step:929/2090 train_time:37329ms step_avg:40.18ms
step:930/2090 train_time:37389ms step_avg:40.20ms
step:931/2090 train_time:37450ms step_avg:40.23ms
step:932/2090 train_time:37509ms step_avg:40.25ms
step:933/2090 train_time:37569ms step_avg:40.27ms
step:934/2090 train_time:37628ms step_avg:40.29ms
step:935/2090 train_time:37689ms step_avg:40.31ms
step:936/2090 train_time:37748ms step_avg:40.33ms
step:937/2090 train_time:37809ms step_avg:40.35ms
step:938/2090 train_time:37868ms step_avg:40.37ms
step:939/2090 train_time:37928ms step_avg:40.39ms
step:940/2090 train_time:37987ms step_avg:40.41ms
step:941/2090 train_time:38048ms step_avg:40.43ms
step:942/2090 train_time:38107ms step_avg:40.45ms
step:943/2090 train_time:38168ms step_avg:40.47ms
step:944/2090 train_time:38227ms step_avg:40.49ms
step:945/2090 train_time:38288ms step_avg:40.52ms
step:946/2090 train_time:38347ms step_avg:40.54ms
step:947/2090 train_time:38408ms step_avg:40.56ms
step:948/2090 train_time:38468ms step_avg:40.58ms
step:949/2090 train_time:38527ms step_avg:40.60ms
step:950/2090 train_time:38587ms step_avg:40.62ms
step:951/2090 train_time:38647ms step_avg:40.64ms
step:952/2090 train_time:38707ms step_avg:40.66ms
step:953/2090 train_time:38767ms step_avg:40.68ms
step:954/2090 train_time:38827ms step_avg:40.70ms
step:955/2090 train_time:38887ms step_avg:40.72ms
step:956/2090 train_time:38946ms step_avg:40.74ms
step:957/2090 train_time:39007ms step_avg:40.76ms
step:958/2090 train_time:39066ms step_avg:40.78ms
step:959/2090 train_time:39127ms step_avg:40.80ms
step:960/2090 train_time:39186ms step_avg:40.82ms
step:961/2090 train_time:39247ms step_avg:40.84ms
step:962/2090 train_time:39306ms step_avg:40.86ms
step:963/2090 train_time:39367ms step_avg:40.88ms
step:964/2090 train_time:39426ms step_avg:40.90ms
step:965/2090 train_time:39487ms step_avg:40.92ms
step:966/2090 train_time:39546ms step_avg:40.94ms
step:967/2090 train_time:39607ms step_avg:40.96ms
step:968/2090 train_time:39666ms step_avg:40.98ms
step:969/2090 train_time:39726ms step_avg:41.00ms
step:970/2090 train_time:39785ms step_avg:41.02ms
step:971/2090 train_time:39845ms step_avg:41.04ms
step:972/2090 train_time:39904ms step_avg:41.05ms
step:973/2090 train_time:39965ms step_avg:41.07ms
step:974/2090 train_time:40024ms step_avg:41.09ms
step:975/2090 train_time:40085ms step_avg:41.11ms
step:976/2090 train_time:40144ms step_avg:41.13ms
step:977/2090 train_time:40204ms step_avg:41.15ms
step:978/2090 train_time:40264ms step_avg:41.17ms
step:979/2090 train_time:40324ms step_avg:41.19ms
step:980/2090 train_time:40384ms step_avg:41.21ms
step:981/2090 train_time:40445ms step_avg:41.23ms
step:982/2090 train_time:40504ms step_avg:41.25ms
step:983/2090 train_time:40565ms step_avg:41.27ms
step:984/2090 train_time:40625ms step_avg:41.29ms
step:985/2090 train_time:40686ms step_avg:41.31ms
step:986/2090 train_time:40745ms step_avg:41.32ms
step:987/2090 train_time:40806ms step_avg:41.34ms
step:988/2090 train_time:40866ms step_avg:41.36ms
step:989/2090 train_time:40926ms step_avg:41.38ms
step:990/2090 train_time:40986ms step_avg:41.40ms
step:991/2090 train_time:41047ms step_avg:41.42ms
step:992/2090 train_time:41106ms step_avg:41.44ms
step:993/2090 train_time:41167ms step_avg:41.46ms
step:994/2090 train_time:41226ms step_avg:41.47ms
step:995/2090 train_time:41286ms step_avg:41.49ms
step:996/2090 train_time:41345ms step_avg:41.51ms
step:997/2090 train_time:41406ms step_avg:41.53ms
step:998/2090 train_time:41465ms step_avg:41.55ms
step:999/2090 train_time:41526ms step_avg:41.57ms
step:1000/2090 train_time:41585ms step_avg:41.59ms
step:1000/2090 val_loss:3.7068 train_time:41648ms step_avg:41.65ms
step:1001/2090 train_time:41669ms step_avg:41.63ms
step:1002/2090 train_time:41707ms step_avg:41.62ms
step:1003/2090 train_time:41770ms step_avg:41.65ms
step:1004/2090 train_time:41832ms step_avg:41.66ms
step:1005/2090 train_time:41893ms step_avg:41.68ms
step:1006/2090 train_time:41952ms step_avg:41.70ms
step:1007/2090 train_time:42011ms step_avg:41.72ms
step:1008/2090 train_time:42070ms step_avg:41.74ms
step:1009/2090 train_time:42130ms step_avg:41.75ms
step:1010/2090 train_time:42189ms step_avg:41.77ms
step:1011/2090 train_time:42249ms step_avg:41.79ms
step:1012/2090 train_time:42308ms step_avg:41.81ms
step:1013/2090 train_time:42367ms step_avg:41.82ms
step:1014/2090 train_time:42427ms step_avg:41.84ms
step:1015/2090 train_time:42487ms step_avg:41.86ms
step:1016/2090 train_time:42546ms step_avg:41.88ms
step:1017/2090 train_time:42607ms step_avg:41.89ms
step:1018/2090 train_time:42668ms step_avg:41.91ms
step:1019/2090 train_time:42730ms step_avg:41.93ms
step:1020/2090 train_time:42790ms step_avg:41.95ms
step:1021/2090 train_time:42851ms step_avg:41.97ms
step:1022/2090 train_time:42910ms step_avg:41.99ms
step:1023/2090 train_time:42971ms step_avg:42.00ms
step:1024/2090 train_time:43030ms step_avg:42.02ms
step:1025/2090 train_time:43090ms step_avg:42.04ms
step:1026/2090 train_time:43149ms step_avg:42.06ms
step:1027/2090 train_time:43208ms step_avg:42.07ms
step:1028/2090 train_time:43267ms step_avg:42.09ms
step:1029/2090 train_time:43326ms step_avg:42.10ms
step:1030/2090 train_time:43385ms step_avg:42.12ms
step:1031/2090 train_time:43445ms step_avg:42.14ms
step:1032/2090 train_time:43504ms step_avg:42.15ms
step:1033/2090 train_time:43563ms step_avg:42.17ms
step:1034/2090 train_time:43623ms step_avg:42.19ms
step:1035/2090 train_time:43684ms step_avg:42.21ms
step:1036/2090 train_time:43744ms step_avg:42.22ms
step:1037/2090 train_time:43805ms step_avg:42.24ms
step:1038/2090 train_time:43865ms step_avg:42.26ms
step:1039/2090 train_time:43926ms step_avg:42.28ms
step:1040/2090 train_time:43985ms step_avg:42.29ms
step:1041/2090 train_time:44045ms step_avg:42.31ms
step:1042/2090 train_time:44105ms step_avg:42.33ms
step:1043/2090 train_time:44165ms step_avg:42.34ms
step:1044/2090 train_time:44224ms step_avg:42.36ms
step:1045/2090 train_time:44284ms step_avg:42.38ms
step:1046/2090 train_time:44343ms step_avg:42.39ms
step:1047/2090 train_time:44403ms step_avg:42.41ms
step:1048/2090 train_time:44461ms step_avg:42.43ms
step:1049/2090 train_time:44522ms step_avg:42.44ms
step:1050/2090 train_time:44582ms step_avg:42.46ms
step:1051/2090 train_time:44642ms step_avg:42.48ms
step:1052/2090 train_time:44701ms step_avg:42.49ms
step:1053/2090 train_time:44762ms step_avg:42.51ms
step:1054/2090 train_time:44822ms step_avg:42.53ms
step:1055/2090 train_time:44883ms step_avg:42.54ms
step:1056/2090 train_time:44941ms step_avg:42.56ms
step:1057/2090 train_time:45002ms step_avg:42.58ms
step:1058/2090 train_time:45061ms step_avg:42.59ms
step:1059/2090 train_time:45122ms step_avg:42.61ms
step:1060/2090 train_time:45182ms step_avg:42.62ms
step:1061/2090 train_time:45242ms step_avg:42.64ms
step:1062/2090 train_time:45301ms step_avg:42.66ms
step:1063/2090 train_time:45362ms step_avg:42.67ms
step:1064/2090 train_time:45420ms step_avg:42.69ms
step:1065/2090 train_time:45480ms step_avg:42.70ms
step:1066/2090 train_time:45539ms step_avg:42.72ms
step:1067/2090 train_time:45600ms step_avg:42.74ms
step:1068/2090 train_time:45659ms step_avg:42.75ms
step:1069/2090 train_time:45719ms step_avg:42.77ms
step:1070/2090 train_time:45779ms step_avg:42.78ms
step:1071/2090 train_time:45841ms step_avg:42.80ms
step:1072/2090 train_time:45900ms step_avg:42.82ms
step:1073/2090 train_time:45961ms step_avg:42.83ms
step:1074/2090 train_time:46020ms step_avg:42.85ms
step:1075/2090 train_time:46081ms step_avg:42.87ms
step:1076/2090 train_time:46141ms step_avg:42.88ms
step:1077/2090 train_time:46202ms step_avg:42.90ms
step:1078/2090 train_time:46261ms step_avg:42.91ms
step:1079/2090 train_time:46321ms step_avg:42.93ms
step:1080/2090 train_time:46380ms step_avg:42.94ms
step:1081/2090 train_time:46439ms step_avg:42.96ms
step:1082/2090 train_time:46499ms step_avg:42.97ms
step:1083/2090 train_time:46559ms step_avg:42.99ms
step:1084/2090 train_time:46618ms step_avg:43.01ms
step:1085/2090 train_time:46679ms step_avg:43.02ms
step:1086/2090 train_time:46738ms step_avg:43.04ms
step:1087/2090 train_time:46799ms step_avg:43.05ms
step:1088/2090 train_time:46858ms step_avg:43.07ms
step:1089/2090 train_time:46919ms step_avg:43.08ms
step:1090/2090 train_time:46979ms step_avg:43.10ms
step:1091/2090 train_time:47040ms step_avg:43.12ms
step:1092/2090 train_time:47099ms step_avg:43.13ms
step:1093/2090 train_time:47160ms step_avg:43.15ms
step:1094/2090 train_time:47219ms step_avg:43.16ms
step:1095/2090 train_time:47279ms step_avg:43.18ms
step:1096/2090 train_time:47339ms step_avg:43.19ms
step:1097/2090 train_time:47399ms step_avg:43.21ms
step:1098/2090 train_time:47458ms step_avg:43.22ms
step:1099/2090 train_time:47518ms step_avg:43.24ms
step:1100/2090 train_time:47578ms step_avg:43.25ms
step:1101/2090 train_time:47638ms step_avg:43.27ms
step:1102/2090 train_time:47698ms step_avg:43.28ms
step:1103/2090 train_time:47759ms step_avg:43.30ms
step:1104/2090 train_time:47818ms step_avg:43.31ms
step:1105/2090 train_time:47878ms step_avg:43.33ms
step:1106/2090 train_time:47938ms step_avg:43.34ms
step:1107/2090 train_time:47998ms step_avg:43.36ms
step:1108/2090 train_time:48058ms step_avg:43.37ms
step:1109/2090 train_time:48119ms step_avg:43.39ms
step:1110/2090 train_time:48177ms step_avg:43.40ms
step:1111/2090 train_time:48238ms step_avg:43.42ms
step:1112/2090 train_time:48297ms step_avg:43.43ms
step:1113/2090 train_time:48357ms step_avg:43.45ms
step:1114/2090 train_time:48416ms step_avg:43.46ms
step:1115/2090 train_time:48476ms step_avg:43.48ms
step:1116/2090 train_time:48535ms step_avg:43.49ms
step:1117/2090 train_time:48595ms step_avg:43.51ms
step:1118/2090 train_time:48655ms step_avg:43.52ms
step:1119/2090 train_time:48715ms step_avg:43.53ms
step:1120/2090 train_time:48775ms step_avg:43.55ms
step:1121/2090 train_time:48836ms step_avg:43.56ms
step:1122/2090 train_time:48895ms step_avg:43.58ms
step:1123/2090 train_time:48956ms step_avg:43.59ms
step:1124/2090 train_time:49016ms step_avg:43.61ms
step:1125/2090 train_time:49077ms step_avg:43.62ms
step:1126/2090 train_time:49137ms step_avg:43.64ms
step:1127/2090 train_time:49197ms step_avg:43.65ms
step:1128/2090 train_time:49256ms step_avg:43.67ms
step:1129/2090 train_time:49317ms step_avg:43.68ms
step:1130/2090 train_time:49376ms step_avg:43.70ms
step:1131/2090 train_time:49437ms step_avg:43.71ms
step:1132/2090 train_time:49496ms step_avg:43.72ms
step:1133/2090 train_time:49556ms step_avg:43.74ms
step:1134/2090 train_time:49616ms step_avg:43.75ms
step:1135/2090 train_time:49677ms step_avg:43.77ms
step:1136/2090 train_time:49736ms step_avg:43.78ms
step:1137/2090 train_time:49797ms step_avg:43.80ms
step:1138/2090 train_time:49856ms step_avg:43.81ms
step:1139/2090 train_time:49917ms step_avg:43.82ms
step:1140/2090 train_time:49976ms step_avg:43.84ms
step:1141/2090 train_time:50037ms step_avg:43.85ms
step:1142/2090 train_time:50096ms step_avg:43.87ms
step:1143/2090 train_time:50156ms step_avg:43.88ms
step:1144/2090 train_time:50216ms step_avg:43.89ms
step:1145/2090 train_time:50276ms step_avg:43.91ms
step:1146/2090 train_time:50335ms step_avg:43.92ms
step:1147/2090 train_time:50396ms step_avg:43.94ms
step:1148/2090 train_time:50455ms step_avg:43.95ms
step:1149/2090 train_time:50517ms step_avg:43.97ms
step:1150/2090 train_time:50576ms step_avg:43.98ms
step:1151/2090 train_time:50636ms step_avg:43.99ms
step:1152/2090 train_time:50696ms step_avg:44.01ms
step:1153/2090 train_time:50756ms step_avg:44.02ms
step:1154/2090 train_time:50816ms step_avg:44.03ms
step:1155/2090 train_time:50876ms step_avg:44.05ms
step:1156/2090 train_time:50936ms step_avg:44.06ms
step:1157/2090 train_time:50997ms step_avg:44.08ms
step:1158/2090 train_time:51056ms step_avg:44.09ms
step:1159/2090 train_time:51117ms step_avg:44.10ms
step:1160/2090 train_time:51176ms step_avg:44.12ms
step:1161/2090 train_time:51237ms step_avg:44.13ms
step:1162/2090 train_time:51296ms step_avg:44.14ms
step:1163/2090 train_time:51357ms step_avg:44.16ms
step:1164/2090 train_time:51417ms step_avg:44.17ms
step:1165/2090 train_time:51477ms step_avg:44.19ms
step:1166/2090 train_time:51537ms step_avg:44.20ms
step:1167/2090 train_time:51597ms step_avg:44.21ms
step:1168/2090 train_time:51656ms step_avg:44.23ms
step:1169/2090 train_time:51717ms step_avg:44.24ms
step:1170/2090 train_time:51777ms step_avg:44.25ms
step:1171/2090 train_time:51838ms step_avg:44.27ms
step:1172/2090 train_time:51897ms step_avg:44.28ms
step:1173/2090 train_time:51958ms step_avg:44.29ms
step:1174/2090 train_time:52018ms step_avg:44.31ms
step:1175/2090 train_time:52078ms step_avg:44.32ms
step:1176/2090 train_time:52138ms step_avg:44.33ms
step:1177/2090 train_time:52198ms step_avg:44.35ms
step:1178/2090 train_time:52257ms step_avg:44.36ms
step:1179/2090 train_time:52318ms step_avg:44.37ms
step:1180/2090 train_time:52377ms step_avg:44.39ms
step:1181/2090 train_time:52438ms step_avg:44.40ms
step:1182/2090 train_time:52498ms step_avg:44.41ms
step:1183/2090 train_time:52559ms step_avg:44.43ms
step:1184/2090 train_time:52618ms step_avg:44.44ms
step:1185/2090 train_time:52678ms step_avg:44.45ms
step:1186/2090 train_time:52737ms step_avg:44.47ms
step:1187/2090 train_time:52798ms step_avg:44.48ms
step:1188/2090 train_time:52857ms step_avg:44.49ms
step:1189/2090 train_time:52917ms step_avg:44.51ms
step:1190/2090 train_time:52977ms step_avg:44.52ms
step:1191/2090 train_time:53037ms step_avg:44.53ms
step:1192/2090 train_time:53096ms step_avg:44.54ms
step:1193/2090 train_time:53157ms step_avg:44.56ms
step:1194/2090 train_time:53216ms step_avg:44.57ms
step:1195/2090 train_time:53276ms step_avg:44.58ms
step:1196/2090 train_time:53336ms step_avg:44.60ms
step:1197/2090 train_time:53396ms step_avg:44.61ms
step:1198/2090 train_time:53455ms step_avg:44.62ms
step:1199/2090 train_time:53517ms step_avg:44.63ms
step:1200/2090 train_time:53576ms step_avg:44.65ms
step:1201/2090 train_time:53636ms step_avg:44.66ms
step:1202/2090 train_time:53696ms step_avg:44.67ms
step:1203/2090 train_time:53756ms step_avg:44.69ms
step:1204/2090 train_time:53816ms step_avg:44.70ms
step:1205/2090 train_time:53877ms step_avg:44.71ms
step:1206/2090 train_time:53936ms step_avg:44.72ms
step:1207/2090 train_time:53996ms step_avg:44.74ms
step:1208/2090 train_time:54056ms step_avg:44.75ms
step:1209/2090 train_time:54117ms step_avg:44.76ms
step:1210/2090 train_time:54176ms step_avg:44.77ms
step:1211/2090 train_time:54236ms step_avg:44.79ms
step:1212/2090 train_time:54295ms step_avg:44.80ms
step:1213/2090 train_time:54356ms step_avg:44.81ms
step:1214/2090 train_time:54415ms step_avg:44.82ms
step:1215/2090 train_time:54476ms step_avg:44.84ms
step:1216/2090 train_time:54535ms step_avg:44.85ms
step:1217/2090 train_time:54596ms step_avg:44.86ms
step:1218/2090 train_time:54656ms step_avg:44.87ms
step:1219/2090 train_time:54716ms step_avg:44.89ms
step:1220/2090 train_time:54776ms step_avg:44.90ms
step:1221/2090 train_time:54836ms step_avg:44.91ms
step:1222/2090 train_time:54895ms step_avg:44.92ms
step:1223/2090 train_time:54956ms step_avg:44.94ms
step:1224/2090 train_time:55016ms step_avg:44.95ms
step:1225/2090 train_time:55076ms step_avg:44.96ms
step:1226/2090 train_time:55136ms step_avg:44.97ms
step:1227/2090 train_time:55197ms step_avg:44.99ms
step:1228/2090 train_time:55256ms step_avg:45.00ms
step:1229/2090 train_time:55317ms step_avg:45.01ms
step:1230/2090 train_time:55376ms step_avg:45.02ms
step:1231/2090 train_time:55437ms step_avg:45.03ms
step:1232/2090 train_time:55496ms step_avg:45.05ms
step:1233/2090 train_time:55556ms step_avg:45.06ms
step:1234/2090 train_time:55616ms step_avg:45.07ms
step:1235/2090 train_time:55677ms step_avg:45.08ms
step:1236/2090 train_time:55736ms step_avg:45.09ms
step:1237/2090 train_time:55797ms step_avg:45.11ms
step:1238/2090 train_time:55856ms step_avg:45.12ms
step:1239/2090 train_time:55916ms step_avg:45.13ms
step:1240/2090 train_time:55975ms step_avg:45.14ms
step:1241/2090 train_time:56036ms step_avg:45.15ms
step:1242/2090 train_time:56095ms step_avg:45.17ms
step:1243/2090 train_time:56155ms step_avg:45.18ms
step:1244/2090 train_time:56215ms step_avg:45.19ms
step:1245/2090 train_time:56275ms step_avg:45.20ms
step:1246/2090 train_time:56335ms step_avg:45.21ms
step:1247/2090 train_time:56396ms step_avg:45.23ms
step:1248/2090 train_time:56455ms step_avg:45.24ms
step:1249/2090 train_time:56515ms step_avg:45.25ms
step:1250/2090 train_time:56574ms step_avg:45.26ms
step:1250/2090 val_loss:3.5848 train_time:56637ms step_avg:45.31ms
step:1251/2090 train_time:56657ms step_avg:45.29ms
step:1252/2090 train_time:56698ms step_avg:45.29ms
step:1253/2090 train_time:56760ms step_avg:45.30ms
step:1254/2090 train_time:56823ms step_avg:45.31ms
step:1255/2090 train_time:56884ms step_avg:45.33ms
step:1256/2090 train_time:56943ms step_avg:45.34ms
step:1257/2090 train_time:57003ms step_avg:45.35ms
step:1258/2090 train_time:57062ms step_avg:45.36ms
step:1259/2090 train_time:57122ms step_avg:45.37ms
step:1260/2090 train_time:57180ms step_avg:45.38ms
step:1261/2090 train_time:57240ms step_avg:45.39ms
step:1262/2090 train_time:57299ms step_avg:45.40ms
step:1263/2090 train_time:57358ms step_avg:45.41ms
step:1264/2090 train_time:57418ms step_avg:45.43ms
step:1265/2090 train_time:57477ms step_avg:45.44ms
step:1266/2090 train_time:57536ms step_avg:45.45ms
step:1267/2090 train_time:57597ms step_avg:45.46ms
step:1268/2090 train_time:57657ms step_avg:45.47ms
step:1269/2090 train_time:57719ms step_avg:45.48ms
step:1270/2090 train_time:57780ms step_avg:45.50ms
step:1271/2090 train_time:57841ms step_avg:45.51ms
step:1272/2090 train_time:57902ms step_avg:45.52ms
step:1273/2090 train_time:57961ms step_avg:45.53ms
step:1274/2090 train_time:58021ms step_avg:45.54ms
step:1275/2090 train_time:58080ms step_avg:45.55ms
step:1276/2090 train_time:58139ms step_avg:45.56ms
step:1277/2090 train_time:58198ms step_avg:45.57ms
step:1278/2090 train_time:58257ms step_avg:45.58ms
step:1279/2090 train_time:58317ms step_avg:45.60ms
step:1280/2090 train_time:58376ms step_avg:45.61ms
step:1281/2090 train_time:58435ms step_avg:45.62ms
step:1282/2090 train_time:58494ms step_avg:45.63ms
step:1283/2090 train_time:58554ms step_avg:45.64ms
step:1284/2090 train_time:58613ms step_avg:45.65ms
step:1285/2090 train_time:58674ms step_avg:45.66ms
step:1286/2090 train_time:58734ms step_avg:45.67ms
step:1287/2090 train_time:58795ms step_avg:45.68ms
step:1288/2090 train_time:58856ms step_avg:45.70ms
step:1289/2090 train_time:58918ms step_avg:45.71ms
step:1290/2090 train_time:58978ms step_avg:45.72ms
step:1291/2090 train_time:59038ms step_avg:45.73ms
step:1292/2090 train_time:59097ms step_avg:45.74ms
step:1293/2090 train_time:59157ms step_avg:45.75ms
step:1294/2090 train_time:59216ms step_avg:45.76ms
step:1295/2090 train_time:59276ms step_avg:45.77ms
step:1296/2090 train_time:59335ms step_avg:45.78ms
step:1297/2090 train_time:59394ms step_avg:45.79ms
step:1298/2090 train_time:59453ms step_avg:45.80ms
step:1299/2090 train_time:59513ms step_avg:45.81ms
step:1300/2090 train_time:59573ms step_avg:45.83ms
step:1301/2090 train_time:59633ms step_avg:45.84ms
step:1302/2090 train_time:59693ms step_avg:45.85ms
step:1303/2090 train_time:59754ms step_avg:45.86ms
step:1304/2090 train_time:59814ms step_avg:45.87ms
step:1305/2090 train_time:59874ms step_avg:45.88ms
step:1306/2090 train_time:59934ms step_avg:45.89ms
step:1307/2090 train_time:59995ms step_avg:45.90ms
step:1308/2090 train_time:60055ms step_avg:45.91ms
step:1309/2090 train_time:60114ms step_avg:45.92ms
step:1310/2090 train_time:60174ms step_avg:45.93ms
step:1311/2090 train_time:60233ms step_avg:45.94ms
step:1312/2090 train_time:60293ms step_avg:45.95ms
step:1313/2090 train_time:60352ms step_avg:45.97ms
step:1314/2090 train_time:60411ms step_avg:45.98ms
step:1315/2090 train_time:60471ms step_avg:45.99ms
step:1316/2090 train_time:60530ms step_avg:46.00ms
step:1317/2090 train_time:60590ms step_avg:46.01ms
step:1318/2090 train_time:60649ms step_avg:46.02ms
step:1319/2090 train_time:60710ms step_avg:46.03ms
step:1320/2090 train_time:60770ms step_avg:46.04ms
step:1321/2090 train_time:60830ms step_avg:46.05ms
step:1322/2090 train_time:60890ms step_avg:46.06ms
step:1323/2090 train_time:60951ms step_avg:46.07ms
step:1324/2090 train_time:61010ms step_avg:46.08ms
step:1325/2090 train_time:61071ms step_avg:46.09ms
step:1326/2090 train_time:61131ms step_avg:46.10ms
step:1327/2090 train_time:61191ms step_avg:46.11ms
step:1328/2090 train_time:61251ms step_avg:46.12ms
step:1329/2090 train_time:61311ms step_avg:46.13ms
step:1330/2090 train_time:61370ms step_avg:46.14ms
step:1331/2090 train_time:61430ms step_avg:46.15ms
step:1332/2090 train_time:61490ms step_avg:46.16ms
step:1333/2090 train_time:61549ms step_avg:46.17ms
step:1334/2090 train_time:61608ms step_avg:46.18ms
step:1335/2090 train_time:61669ms step_avg:46.19ms
step:1336/2090 train_time:61728ms step_avg:46.20ms
step:1337/2090 train_time:61789ms step_avg:46.21ms
step:1338/2090 train_time:61849ms step_avg:46.23ms
step:1339/2090 train_time:61911ms step_avg:46.24ms
step:1340/2090 train_time:61970ms step_avg:46.25ms
step:1341/2090 train_time:62031ms step_avg:46.26ms
step:1342/2090 train_time:62092ms step_avg:46.27ms
step:1343/2090 train_time:62152ms step_avg:46.28ms
step:1344/2090 train_time:62211ms step_avg:46.29ms
step:1345/2090 train_time:62271ms step_avg:46.30ms
step:1346/2090 train_time:62331ms step_avg:46.31ms
step:1347/2090 train_time:62391ms step_avg:46.32ms
step:1348/2090 train_time:62450ms step_avg:46.33ms
step:1349/2090 train_time:62510ms step_avg:46.34ms
step:1350/2090 train_time:62569ms step_avg:46.35ms
step:1351/2090 train_time:62630ms step_avg:46.36ms
step:1352/2090 train_time:62689ms step_avg:46.37ms
step:1353/2090 train_time:62749ms step_avg:46.38ms
step:1354/2090 train_time:62809ms step_avg:46.39ms
step:1355/2090 train_time:62869ms step_avg:46.40ms
step:1356/2090 train_time:62929ms step_avg:46.41ms
step:1357/2090 train_time:62990ms step_avg:46.42ms
step:1358/2090 train_time:63049ms step_avg:46.43ms
step:1359/2090 train_time:63110ms step_avg:46.44ms
step:1360/2090 train_time:63169ms step_avg:46.45ms
step:1361/2090 train_time:63229ms step_avg:46.46ms
step:1362/2090 train_time:63289ms step_avg:46.47ms
step:1363/2090 train_time:63349ms step_avg:46.48ms
step:1364/2090 train_time:63408ms step_avg:46.49ms
step:1365/2090 train_time:63469ms step_avg:46.50ms
step:1366/2090 train_time:63528ms step_avg:46.51ms
step:1367/2090 train_time:63588ms step_avg:46.52ms
step:1368/2090 train_time:63648ms step_avg:46.53ms
step:1369/2090 train_time:63736ms step_avg:46.56ms
step:1370/2090 train_time:63823ms step_avg:46.59ms
step:1371/2090 train_time:63911ms step_avg:46.62ms
step:1372/2090 train_time:63998ms step_avg:46.65ms
step:1373/2090 train_time:64086ms step_avg:46.68ms
step:1374/2090 train_time:64174ms step_avg:46.71ms
step:1375/2090 train_time:64262ms step_avg:46.74ms
step:1376/2090 train_time:64349ms step_avg:46.77ms
step:1377/2090 train_time:64437ms step_avg:46.79ms
step:1378/2090 train_time:64523ms step_avg:46.82ms
step:1379/2090 train_time:64610ms step_avg:46.85ms
step:1380/2090 train_time:64698ms step_avg:46.88ms
step:1381/2090 train_time:64786ms step_avg:46.91ms
step:1382/2090 train_time:64873ms step_avg:46.94ms
step:1383/2090 train_time:64961ms step_avg:46.97ms
step:1384/2090 train_time:65048ms step_avg:47.00ms
step:1385/2090 train_time:65137ms step_avg:47.03ms
step:1386/2090 train_time:65224ms step_avg:47.06ms
step:1387/2090 train_time:65311ms step_avg:47.09ms
step:1388/2090 train_time:65399ms step_avg:47.12ms
step:1389/2090 train_time:65486ms step_avg:47.15ms
step:1390/2090 train_time:65573ms step_avg:47.17ms
step:1391/2090 train_time:65660ms step_avg:47.20ms
step:1392/2090 train_time:65747ms step_avg:47.23ms
step:1393/2090 train_time:65835ms step_avg:47.26ms
step:1394/2090 train_time:65923ms step_avg:47.29ms
step:1395/2090 train_time:66011ms step_avg:47.32ms
step:1396/2090 train_time:66099ms step_avg:47.35ms
step:1397/2090 train_time:66187ms step_avg:47.38ms
step:1398/2090 train_time:66275ms step_avg:47.41ms
step:1399/2090 train_time:66363ms step_avg:47.44ms
step:1400/2090 train_time:66449ms step_avg:47.46ms
step:1401/2090 train_time:66537ms step_avg:47.49ms
step:1402/2090 train_time:66624ms step_avg:47.52ms
step:1403/2090 train_time:66711ms step_avg:47.55ms
step:1404/2090 train_time:66799ms step_avg:47.58ms
step:1405/2090 train_time:66886ms step_avg:47.61ms
step:1406/2090 train_time:66974ms step_avg:47.63ms
step:1407/2090 train_time:67062ms step_avg:47.66ms
step:1408/2090 train_time:67149ms step_avg:47.69ms
step:1409/2090 train_time:67237ms step_avg:47.72ms
step:1410/2090 train_time:67325ms step_avg:47.75ms
step:1411/2090 train_time:67412ms step_avg:47.78ms
step:1412/2090 train_time:67499ms step_avg:47.80ms
step:1413/2090 train_time:67588ms step_avg:47.83ms
step:1414/2090 train_time:67674ms step_avg:47.86ms
step:1415/2090 train_time:67762ms step_avg:47.89ms
step:1416/2090 train_time:67849ms step_avg:47.92ms
step:1417/2090 train_time:67938ms step_avg:47.94ms
step:1418/2090 train_time:68024ms step_avg:47.97ms
step:1419/2090 train_time:68113ms step_avg:48.00ms
step:1420/2090 train_time:68200ms step_avg:48.03ms
step:1421/2090 train_time:68288ms step_avg:48.06ms
step:1422/2090 train_time:68375ms step_avg:48.08ms
step:1423/2090 train_time:68463ms step_avg:48.11ms
step:1424/2090 train_time:68551ms step_avg:48.14ms
step:1425/2090 train_time:68639ms step_avg:48.17ms
step:1426/2090 train_time:68725ms step_avg:48.19ms
step:1427/2090 train_time:68813ms step_avg:48.22ms
step:1428/2090 train_time:68900ms step_avg:48.25ms
step:1429/2090 train_time:68989ms step_avg:48.28ms
step:1430/2090 train_time:69076ms step_avg:48.31ms
step:1431/2090 train_time:69164ms step_avg:48.33ms
step:1432/2090 train_time:69251ms step_avg:48.36ms
step:1433/2090 train_time:69338ms step_avg:48.39ms
step:1434/2090 train_time:69426ms step_avg:48.41ms
step:1435/2090 train_time:69513ms step_avg:48.44ms
step:1436/2090 train_time:69600ms step_avg:48.47ms
step:1437/2090 train_time:69687ms step_avg:48.50ms
step:1438/2090 train_time:69775ms step_avg:48.52ms
step:1439/2090 train_time:69863ms step_avg:48.55ms
step:1440/2090 train_time:69950ms step_avg:48.58ms
step:1441/2090 train_time:70037ms step_avg:48.60ms
step:1442/2090 train_time:70125ms step_avg:48.63ms
step:1443/2090 train_time:70213ms step_avg:48.66ms
step:1444/2090 train_time:70300ms step_avg:48.68ms
step:1445/2090 train_time:70388ms step_avg:48.71ms
step:1446/2090 train_time:70475ms step_avg:48.74ms
step:1447/2090 train_time:70563ms step_avg:48.77ms
step:1448/2090 train_time:70650ms step_avg:48.79ms
step:1449/2090 train_time:70739ms step_avg:48.82ms
step:1450/2090 train_time:70826ms step_avg:48.85ms
step:1451/2090 train_time:70914ms step_avg:48.87ms
step:1452/2090 train_time:71001ms step_avg:48.90ms
step:1453/2090 train_time:71089ms step_avg:48.93ms
step:1454/2090 train_time:71176ms step_avg:48.95ms
step:1455/2090 train_time:71264ms step_avg:48.98ms
step:1456/2090 train_time:71351ms step_avg:49.00ms
step:1457/2090 train_time:71439ms step_avg:49.03ms
step:1458/2090 train_time:71527ms step_avg:49.06ms
step:1459/2090 train_time:71614ms step_avg:49.08ms
step:1460/2090 train_time:71701ms step_avg:49.11ms
step:1461/2090 train_time:71789ms step_avg:49.14ms
step:1462/2090 train_time:71876ms step_avg:49.16ms
step:1463/2090 train_time:71964ms step_avg:49.19ms
step:1464/2090 train_time:72052ms step_avg:49.22ms
step:1465/2090 train_time:72139ms step_avg:49.24ms
step:1466/2090 train_time:72226ms step_avg:49.27ms
step:1467/2090 train_time:72314ms step_avg:49.29ms
step:1468/2090 train_time:72401ms step_avg:49.32ms
step:1469/2090 train_time:72488ms step_avg:49.35ms
step:1470/2090 train_time:72576ms step_avg:49.37ms
step:1471/2090 train_time:72664ms step_avg:49.40ms
step:1472/2090 train_time:72751ms step_avg:49.42ms
step:1473/2090 train_time:72839ms step_avg:49.45ms
step:1474/2090 train_time:72926ms step_avg:49.47ms
step:1475/2090 train_time:73015ms step_avg:49.50ms
step:1476/2090 train_time:73101ms step_avg:49.53ms
step:1477/2090 train_time:73189ms step_avg:49.55ms
step:1478/2090 train_time:73275ms step_avg:49.58ms
step:1479/2090 train_time:73364ms step_avg:49.60ms
step:1480/2090 train_time:73451ms step_avg:49.63ms
step:1481/2090 train_time:73538ms step_avg:49.65ms
step:1482/2090 train_time:73625ms step_avg:49.68ms
step:1483/2090 train_time:73713ms step_avg:49.71ms
step:1484/2090 train_time:73800ms step_avg:49.73ms
step:1485/2090 train_time:73890ms step_avg:49.76ms
step:1486/2090 train_time:73977ms step_avg:49.78ms
step:1487/2090 train_time:74064ms step_avg:49.81ms
step:1488/2090 train_time:74152ms step_avg:49.83ms
step:1489/2090 train_time:74240ms step_avg:49.86ms
step:1490/2090 train_time:74327ms step_avg:49.88ms
step:1491/2090 train_time:74415ms step_avg:49.91ms
step:1492/2090 train_time:74502ms step_avg:49.93ms
step:1493/2090 train_time:74591ms step_avg:49.96ms
step:1494/2090 train_time:74677ms step_avg:49.98ms
step:1495/2090 train_time:74765ms step_avg:50.01ms
step:1496/2090 train_time:74852ms step_avg:50.04ms
step:1497/2090 train_time:74940ms step_avg:50.06ms
step:1498/2090 train_time:75027ms step_avg:50.08ms
step:1499/2090 train_time:75115ms step_avg:50.11ms
step:1500/2090 train_time:75202ms step_avg:50.13ms
step:1500/2090 val_loss:3.4756 train_time:75291ms step_avg:50.19ms
step:1501/2090 train_time:75312ms step_avg:50.17ms
step:1502/2090 train_time:75382ms step_avg:50.19ms
step:1503/2090 train_time:75472ms step_avg:50.21ms
step:1504/2090 train_time:75561ms step_avg:50.24ms
step:1505/2090 train_time:75649ms step_avg:50.27ms
step:1506/2090 train_time:75735ms step_avg:50.29ms
step:1507/2090 train_time:75822ms step_avg:50.31ms
step:1508/2090 train_time:75908ms step_avg:50.34ms
step:1509/2090 train_time:75995ms step_avg:50.36ms
step:1510/2090 train_time:76081ms step_avg:50.38ms
step:1511/2090 train_time:76167ms step_avg:50.41ms
step:1512/2090 train_time:76255ms step_avg:50.43ms
step:1513/2090 train_time:76345ms step_avg:50.46ms
step:1514/2090 train_time:76434ms step_avg:50.49ms
step:1515/2090 train_time:76524ms step_avg:50.51ms
step:1516/2090 train_time:76612ms step_avg:50.54ms
step:1517/2090 train_time:76701ms step_avg:50.56ms
step:1518/2090 train_time:76787ms step_avg:50.58ms
step:1519/2090 train_time:76874ms step_avg:50.61ms
step:1520/2090 train_time:76960ms step_avg:50.63ms
step:1521/2090 train_time:77047ms step_avg:50.66ms
step:1522/2090 train_time:77134ms step_avg:50.68ms
step:1523/2090 train_time:77221ms step_avg:50.70ms
step:1524/2090 train_time:77311ms step_avg:50.73ms
step:1525/2090 train_time:77399ms step_avg:50.75ms
step:1526/2090 train_time:77487ms step_avg:50.78ms
step:1527/2090 train_time:77576ms step_avg:50.80ms
step:1528/2090 train_time:77664ms step_avg:50.83ms
step:1529/2090 train_time:77752ms step_avg:50.85ms
step:1530/2090 train_time:77839ms step_avg:50.88ms
step:1531/2090 train_time:77926ms step_avg:50.90ms
step:1532/2090 train_time:78012ms step_avg:50.92ms
step:1533/2090 train_time:78101ms step_avg:50.95ms
step:1534/2090 train_time:78187ms step_avg:50.97ms
step:1535/2090 train_time:78275ms step_avg:50.99ms
step:1536/2090 train_time:78364ms step_avg:51.02ms
step:1537/2090 train_time:78453ms step_avg:51.04ms
step:1538/2090 train_time:78541ms step_avg:51.07ms
step:1539/2090 train_time:78629ms step_avg:51.09ms
step:1540/2090 train_time:78716ms step_avg:51.11ms
step:1541/2090 train_time:78804ms step_avg:51.14ms
step:1542/2090 train_time:78891ms step_avg:51.16ms
step:1543/2090 train_time:78979ms step_avg:51.19ms
step:1544/2090 train_time:79065ms step_avg:51.21ms
step:1545/2090 train_time:79153ms step_avg:51.23ms
step:1546/2090 train_time:79240ms step_avg:51.25ms
step:1547/2090 train_time:79327ms step_avg:51.28ms
step:1548/2090 train_time:79415ms step_avg:51.30ms
step:1549/2090 train_time:79504ms step_avg:51.33ms
step:1550/2090 train_time:79591ms step_avg:51.35ms
step:1551/2090 train_time:79681ms step_avg:51.37ms
step:1552/2090 train_time:79768ms step_avg:51.40ms
step:1553/2090 train_time:79856ms step_avg:51.42ms
step:1554/2090 train_time:79942ms step_avg:51.44ms
step:1555/2090 train_time:80030ms step_avg:51.47ms
step:1556/2090 train_time:80117ms step_avg:51.49ms
step:1557/2090 train_time:80205ms step_avg:51.51ms
step:1558/2090 train_time:80291ms step_avg:51.53ms
step:1559/2090 train_time:80379ms step_avg:51.56ms
step:1560/2090 train_time:80466ms step_avg:51.58ms
step:1561/2090 train_time:80554ms step_avg:51.60ms
step:1562/2090 train_time:80641ms step_avg:51.63ms
step:1563/2090 train_time:80730ms step_avg:51.65ms
step:1564/2090 train_time:80817ms step_avg:51.67ms
step:1565/2090 train_time:80905ms step_avg:51.70ms
step:1566/2090 train_time:80993ms step_avg:51.72ms
step:1567/2090 train_time:81081ms step_avg:51.74ms
step:1568/2090 train_time:81168ms step_avg:51.77ms
step:1569/2090 train_time:81255ms step_avg:51.79ms
step:1570/2090 train_time:81342ms step_avg:51.81ms
step:1571/2090 train_time:81430ms step_avg:51.83ms
step:1572/2090 train_time:81518ms step_avg:51.86ms
step:1573/2090 train_time:81606ms step_avg:51.88ms
step:1574/2090 train_time:81693ms step_avg:51.90ms
step:1575/2090 train_time:81782ms step_avg:51.92ms
step:1576/2090 train_time:81869ms step_avg:51.95ms
step:1577/2090 train_time:81957ms step_avg:51.97ms
step:1578/2090 train_time:82043ms step_avg:51.99ms
step:1579/2090 train_time:82131ms step_avg:52.01ms
step:1580/2090 train_time:82218ms step_avg:52.04ms
step:1581/2090 train_time:82306ms step_avg:52.06ms
step:1582/2090 train_time:82393ms step_avg:52.08ms
step:1583/2090 train_time:82482ms step_avg:52.11ms
step:1584/2090 train_time:82570ms step_avg:52.13ms
step:1585/2090 train_time:82658ms step_avg:52.15ms
step:1586/2090 train_time:82744ms step_avg:52.17ms
step:1587/2090 train_time:82833ms step_avg:52.19ms
step:1588/2090 train_time:82920ms step_avg:52.22ms
step:1589/2090 train_time:83008ms step_avg:52.24ms
step:1590/2090 train_time:83094ms step_avg:52.26ms
step:1591/2090 train_time:83183ms step_avg:52.28ms
step:1592/2090 train_time:83269ms step_avg:52.30ms
step:1593/2090 train_time:83358ms step_avg:52.33ms
step:1594/2090 train_time:83445ms step_avg:52.35ms
step:1595/2090 train_time:83533ms step_avg:52.37ms
step:1596/2090 train_time:83620ms step_avg:52.39ms
step:1597/2090 train_time:83708ms step_avg:52.42ms
step:1598/2090 train_time:83796ms step_avg:52.44ms
step:1599/2090 train_time:83885ms step_avg:52.46ms
step:1600/2090 train_time:83972ms step_avg:52.48ms
step:1601/2090 train_time:84060ms step_avg:52.50ms
step:1602/2090 train_time:84146ms step_avg:52.53ms
step:1603/2090 train_time:84235ms step_avg:52.55ms
step:1604/2090 train_time:84322ms step_avg:52.57ms
step:1605/2090 train_time:84410ms step_avg:52.59ms
step:1606/2090 train_time:84497ms step_avg:52.61ms
step:1607/2090 train_time:84585ms step_avg:52.64ms
step:1608/2090 train_time:84672ms step_avg:52.66ms
step:1609/2090 train_time:84760ms step_avg:52.68ms
step:1610/2090 train_time:84848ms step_avg:52.70ms
step:1611/2090 train_time:84936ms step_avg:52.72ms
step:1612/2090 train_time:85022ms step_avg:52.74ms
step:1613/2090 train_time:85110ms step_avg:52.77ms
step:1614/2090 train_time:85198ms step_avg:52.79ms
step:1615/2090 train_time:85286ms step_avg:52.81ms
step:1616/2090 train_time:85373ms step_avg:52.83ms
step:1617/2090 train_time:85460ms step_avg:52.85ms
step:1618/2090 train_time:85547ms step_avg:52.87ms
step:1619/2090 train_time:85636ms step_avg:52.89ms
step:1620/2090 train_time:85722ms step_avg:52.92ms
step:1621/2090 train_time:85811ms step_avg:52.94ms
step:1622/2090 train_time:85898ms step_avg:52.96ms
step:1623/2090 train_time:85986ms step_avg:52.98ms
step:1624/2090 train_time:86072ms step_avg:53.00ms
step:1625/2090 train_time:86161ms step_avg:53.02ms
step:1626/2090 train_time:86247ms step_avg:53.04ms
step:1627/2090 train_time:86335ms step_avg:53.06ms
step:1628/2090 train_time:86423ms step_avg:53.09ms
step:1629/2090 train_time:86510ms step_avg:53.11ms
step:1630/2090 train_time:86597ms step_avg:53.13ms
step:1631/2090 train_time:86685ms step_avg:53.15ms
step:1632/2090 train_time:86773ms step_avg:53.17ms
step:1633/2090 train_time:86861ms step_avg:53.19ms
step:1634/2090 train_time:86948ms step_avg:53.21ms
step:1635/2090 train_time:87037ms step_avg:53.23ms
step:1636/2090 train_time:87125ms step_avg:53.25ms
step:1637/2090 train_time:87212ms step_avg:53.28ms
step:1638/2090 train_time:87299ms step_avg:53.30ms
step:1639/2090 train_time:87387ms step_avg:53.32ms
step:1640/2090 train_time:87474ms step_avg:53.34ms
step:1641/2090 train_time:87563ms step_avg:53.36ms
step:1642/2090 train_time:87649ms step_avg:53.38ms
step:1643/2090 train_time:87738ms step_avg:53.40ms
step:1644/2090 train_time:87824ms step_avg:53.42ms
step:1645/2090 train_time:87912ms step_avg:53.44ms
step:1646/2090 train_time:87999ms step_avg:53.46ms
step:1647/2090 train_time:88087ms step_avg:53.48ms
step:1648/2090 train_time:88174ms step_avg:53.50ms
step:1649/2090 train_time:88262ms step_avg:53.52ms
step:1650/2090 train_time:88350ms step_avg:53.55ms
step:1651/2090 train_time:88438ms step_avg:53.57ms
step:1652/2090 train_time:88524ms step_avg:53.59ms
step:1653/2090 train_time:88613ms step_avg:53.61ms
step:1654/2090 train_time:88700ms step_avg:53.63ms
step:1655/2090 train_time:88788ms step_avg:53.65ms
step:1656/2090 train_time:88875ms step_avg:53.67ms
step:1657/2090 train_time:88964ms step_avg:53.69ms
step:1658/2090 train_time:89051ms step_avg:53.71ms
step:1659/2090 train_time:89139ms step_avg:53.73ms
step:1660/2090 train_time:89226ms step_avg:53.75ms
step:1661/2090 train_time:89314ms step_avg:53.77ms
step:1662/2090 train_time:89401ms step_avg:53.79ms
step:1663/2090 train_time:89489ms step_avg:53.81ms
step:1664/2090 train_time:89576ms step_avg:53.83ms
step:1665/2090 train_time:89664ms step_avg:53.85ms
step:1666/2090 train_time:89752ms step_avg:53.87ms
step:1667/2090 train_time:89839ms step_avg:53.89ms
step:1668/2090 train_time:89926ms step_avg:53.91ms
step:1669/2090 train_time:90015ms step_avg:53.93ms
step:1670/2090 train_time:90102ms step_avg:53.95ms
step:1671/2090 train_time:90190ms step_avg:53.97ms
step:1672/2090 train_time:90277ms step_avg:53.99ms
step:1673/2090 train_time:90366ms step_avg:54.01ms
step:1674/2090 train_time:90453ms step_avg:54.03ms
step:1675/2090 train_time:90541ms step_avg:54.05ms
step:1676/2090 train_time:90628ms step_avg:54.07ms
step:1677/2090 train_time:90716ms step_avg:54.09ms
step:1678/2090 train_time:90804ms step_avg:54.11ms
step:1679/2090 train_time:90892ms step_avg:54.13ms
step:1680/2090 train_time:90979ms step_avg:54.15ms
step:1681/2090 train_time:91067ms step_avg:54.17ms
step:1682/2090 train_time:91154ms step_avg:54.19ms
step:1683/2090 train_time:91242ms step_avg:54.21ms
step:1684/2090 train_time:91329ms step_avg:54.23ms
step:1685/2090 train_time:91417ms step_avg:54.25ms
step:1686/2090 train_time:91505ms step_avg:54.27ms
step:1687/2090 train_time:91593ms step_avg:54.29ms
step:1688/2090 train_time:91679ms step_avg:54.31ms
step:1689/2090 train_time:91769ms step_avg:54.33ms
step:1690/2090 train_time:91855ms step_avg:54.35ms
step:1691/2090 train_time:91943ms step_avg:54.37ms
step:1692/2090 train_time:92030ms step_avg:54.39ms
step:1693/2090 train_time:92118ms step_avg:54.41ms
step:1694/2090 train_time:92205ms step_avg:54.43ms
step:1695/2090 train_time:92293ms step_avg:54.45ms
step:1696/2090 train_time:92381ms step_avg:54.47ms
step:1697/2090 train_time:92468ms step_avg:54.49ms
step:1698/2090 train_time:92555ms step_avg:54.51ms
step:1699/2090 train_time:92644ms step_avg:54.53ms
step:1700/2090 train_time:92731ms step_avg:54.55ms
step:1701/2090 train_time:92818ms step_avg:54.57ms
step:1702/2090 train_time:92906ms step_avg:54.59ms
step:1703/2090 train_time:92993ms step_avg:54.61ms
step:1704/2090 train_time:93080ms step_avg:54.62ms
step:1705/2090 train_time:93168ms step_avg:54.64ms
step:1706/2090 train_time:93255ms step_avg:54.66ms
step:1707/2090 train_time:93343ms step_avg:54.68ms
step:1708/2090 train_time:93430ms step_avg:54.70ms
step:1709/2090 train_time:93517ms step_avg:54.72ms
step:1710/2090 train_time:93605ms step_avg:54.74ms
step:1711/2090 train_time:93693ms step_avg:54.76ms
step:1712/2090 train_time:93781ms step_avg:54.78ms
step:1713/2090 train_time:93869ms step_avg:54.80ms
step:1714/2090 train_time:93956ms step_avg:54.82ms
step:1715/2090 train_time:94044ms step_avg:54.84ms
step:1716/2090 train_time:94131ms step_avg:54.85ms
step:1717/2090 train_time:94219ms step_avg:54.87ms
step:1718/2090 train_time:94306ms step_avg:54.89ms
step:1719/2090 train_time:94394ms step_avg:54.91ms
step:1720/2090 train_time:94481ms step_avg:54.93ms
step:1721/2090 train_time:94569ms step_avg:54.95ms
step:1722/2090 train_time:94656ms step_avg:54.97ms
step:1723/2090 train_time:94744ms step_avg:54.99ms
step:1724/2090 train_time:94832ms step_avg:55.01ms
step:1725/2090 train_time:94919ms step_avg:55.03ms
step:1726/2090 train_time:95006ms step_avg:55.04ms
step:1727/2090 train_time:95094ms step_avg:55.06ms
step:1728/2090 train_time:95181ms step_avg:55.08ms
step:1729/2090 train_time:95269ms step_avg:55.10ms
step:1730/2090 train_time:95357ms step_avg:55.12ms
step:1731/2090 train_time:95444ms step_avg:55.14ms
step:1732/2090 train_time:95532ms step_avg:55.16ms
step:1733/2090 train_time:95619ms step_avg:55.18ms
step:1734/2090 train_time:95706ms step_avg:55.19ms
step:1735/2090 train_time:95794ms step_avg:55.21ms
step:1736/2090 train_time:95881ms step_avg:55.23ms
step:1737/2090 train_time:95970ms step_avg:55.25ms
step:1738/2090 train_time:96057ms step_avg:55.27ms
step:1739/2090 train_time:96144ms step_avg:55.29ms
step:1740/2090 train_time:96232ms step_avg:55.31ms
step:1741/2090 train_time:96319ms step_avg:55.32ms
step:1742/2090 train_time:96406ms step_avg:55.34ms
step:1743/2090 train_time:96495ms step_avg:55.36ms
step:1744/2090 train_time:96581ms step_avg:55.38ms
step:1745/2090 train_time:96670ms step_avg:55.40ms
step:1746/2090 train_time:96757ms step_avg:55.42ms
step:1747/2090 train_time:96845ms step_avg:55.43ms
step:1748/2090 train_time:96932ms step_avg:55.45ms
step:1749/2090 train_time:97021ms step_avg:55.47ms
step:1750/2090 train_time:97107ms step_avg:55.49ms
step:1750/2090 val_loss:3.3755 train_time:97197ms step_avg:55.54ms
step:1751/2090 train_time:97218ms step_avg:55.52ms
step:1752/2090 train_time:97288ms step_avg:55.53ms
step:1753/2090 train_time:97381ms step_avg:55.55ms
step:1754/2090 train_time:97468ms step_avg:55.57ms
step:1755/2090 train_time:97555ms step_avg:55.59ms
step:1756/2090 train_time:97640ms step_avg:55.60ms
step:1757/2090 train_time:97727ms step_avg:55.62ms
step:1758/2090 train_time:97813ms step_avg:55.64ms
step:1759/2090 train_time:97900ms step_avg:55.66ms
step:1760/2090 train_time:97987ms step_avg:55.67ms
step:1761/2090 train_time:98074ms step_avg:55.69ms
step:1762/2090 train_time:98162ms step_avg:55.71ms
step:1763/2090 train_time:98252ms step_avg:55.73ms
step:1764/2090 train_time:98341ms step_avg:55.75ms
step:1765/2090 train_time:98431ms step_avg:55.77ms
step:1766/2090 train_time:98518ms step_avg:55.79ms
step:1767/2090 train_time:98605ms step_avg:55.80ms
step:1768/2090 train_time:98692ms step_avg:55.82ms
step:1769/2090 train_time:98779ms step_avg:55.84ms
step:1770/2090 train_time:98866ms step_avg:55.86ms
step:1771/2090 train_time:98954ms step_avg:55.87ms
step:1772/2090 train_time:99040ms step_avg:55.89ms
step:1773/2090 train_time:99128ms step_avg:55.91ms
step:1774/2090 train_time:99215ms step_avg:55.93ms
step:1775/2090 train_time:99305ms step_avg:55.95ms
step:1776/2090 train_time:99394ms step_avg:55.97ms
step:1777/2090 train_time:99483ms step_avg:55.98ms
step:1778/2090 train_time:99569ms step_avg:56.00ms
step:1779/2090 train_time:99656ms step_avg:56.02ms
step:1780/2090 train_time:99742ms step_avg:56.04ms
step:1781/2090 train_time:99829ms step_avg:56.05ms
step:1782/2090 train_time:99916ms step_avg:56.07ms
step:1783/2090 train_time:100003ms step_avg:56.09ms
step:1784/2090 train_time:100091ms step_avg:56.10ms
step:1785/2090 train_time:100179ms step_avg:56.12ms
step:1786/2090 train_time:100268ms step_avg:56.14ms
step:1787/2090 train_time:100357ms step_avg:56.16ms
step:1788/2090 train_time:100445ms step_avg:56.18ms
step:1789/2090 train_time:100533ms step_avg:56.19ms
step:1790/2090 train_time:100620ms step_avg:56.21ms
step:1791/2090 train_time:100707ms step_avg:56.23ms
step:1792/2090 train_time:100793ms step_avg:56.25ms
step:1793/2090 train_time:100882ms step_avg:56.26ms
step:1794/2090 train_time:100968ms step_avg:56.28ms
step:1795/2090 train_time:101056ms step_avg:56.30ms
step:1796/2090 train_time:101143ms step_avg:56.32ms
step:1797/2090 train_time:101231ms step_avg:56.33ms
step:1798/2090 train_time:101320ms step_avg:56.35ms
step:1799/2090 train_time:101408ms step_avg:56.37ms
step:1800/2090 train_time:101496ms step_avg:56.39ms
step:1801/2090 train_time:101585ms step_avg:56.40ms
step:1802/2090 train_time:101671ms step_avg:56.42ms
step:1803/2090 train_time:101759ms step_avg:56.44ms
step:1804/2090 train_time:101845ms step_avg:56.45ms
step:1805/2090 train_time:101933ms step_avg:56.47ms
step:1806/2090 train_time:102021ms step_avg:56.49ms
step:1807/2090 train_time:102109ms step_avg:56.51ms
step:1808/2090 train_time:102195ms step_avg:56.52ms
step:1809/2090 train_time:102284ms step_avg:56.54ms
step:1810/2090 train_time:102372ms step_avg:56.56ms
step:1811/2090 train_time:102461ms step_avg:56.58ms
step:1812/2090 train_time:102548ms step_avg:56.59ms
step:1813/2090 train_time:102635ms step_avg:56.61ms
step:1814/2090 train_time:102722ms step_avg:56.63ms
step:1815/2090 train_time:102809ms step_avg:56.64ms
step:1816/2090 train_time:102896ms step_avg:56.66ms
step:1817/2090 train_time:102984ms step_avg:56.68ms
step:1818/2090 train_time:103070ms step_avg:56.69ms
step:1819/2090 train_time:103159ms step_avg:56.71ms
step:1820/2090 train_time:103246ms step_avg:56.73ms
step:1821/2090 train_time:103334ms step_avg:56.75ms
step:1822/2090 train_time:103422ms step_avg:56.76ms
step:1823/2090 train_time:103510ms step_avg:56.78ms
step:1824/2090 train_time:103597ms step_avg:56.80ms
step:1825/2090 train_time:103685ms step_avg:56.81ms
step:1826/2090 train_time:103772ms step_avg:56.83ms
step:1827/2090 train_time:103861ms step_avg:56.85ms
step:1828/2090 train_time:103947ms step_avg:56.86ms
step:1829/2090 train_time:104035ms step_avg:56.88ms
step:1830/2090 train_time:104122ms step_avg:56.90ms
step:1831/2090 train_time:104210ms step_avg:56.91ms
step:1832/2090 train_time:104297ms step_avg:56.93ms
step:1833/2090 train_time:104386ms step_avg:56.95ms
step:1834/2090 train_time:104473ms step_avg:56.96ms
step:1835/2090 train_time:104561ms step_avg:56.98ms
step:1836/2090 train_time:104648ms step_avg:57.00ms
step:1837/2090 train_time:104735ms step_avg:57.01ms
step:1838/2090 train_time:104822ms step_avg:57.03ms
step:1839/2090 train_time:104909ms step_avg:57.05ms
step:1840/2090 train_time:104997ms step_avg:57.06ms
step:1841/2090 train_time:105084ms step_avg:57.08ms
step:1842/2090 train_time:105171ms step_avg:57.10ms
step:1843/2090 train_time:105258ms step_avg:57.11ms
step:1844/2090 train_time:105346ms step_avg:57.13ms
step:1845/2090 train_time:105434ms step_avg:57.15ms
step:1846/2090 train_time:105521ms step_avg:57.16ms
step:1847/2090 train_time:105609ms step_avg:57.18ms
step:1848/2090 train_time:105696ms step_avg:57.19ms
step:1849/2090 train_time:105784ms step_avg:57.21ms
step:1850/2090 train_time:105872ms step_avg:57.23ms
step:1851/2090 train_time:105960ms step_avg:57.24ms
step:1852/2090 train_time:106047ms step_avg:57.26ms
step:1853/2090 train_time:106135ms step_avg:57.28ms
step:1854/2090 train_time:106221ms step_avg:57.29ms
step:1855/2090 train_time:106309ms step_avg:57.31ms
step:1856/2090 train_time:106397ms step_avg:57.33ms
step:1857/2090 train_time:106484ms step_avg:57.34ms
step:1858/2090 train_time:106571ms step_avg:57.36ms
step:1859/2090 train_time:106659ms step_avg:57.37ms
step:1860/2090 train_time:106746ms step_avg:57.39ms
step:1861/2090 train_time:106834ms step_avg:57.41ms
step:1862/2090 train_time:106921ms step_avg:57.42ms
step:1863/2090 train_time:107009ms step_avg:57.44ms
step:1864/2090 train_time:107096ms step_avg:57.46ms
step:1865/2090 train_time:107184ms step_avg:57.47ms
step:1866/2090 train_time:107272ms step_avg:57.49ms
step:1867/2090 train_time:107360ms step_avg:57.50ms
step:1868/2090 train_time:107447ms step_avg:57.52ms
step:1869/2090 train_time:107535ms step_avg:57.54ms
step:1870/2090 train_time:107622ms step_avg:57.55ms
step:1871/2090 train_time:107710ms step_avg:57.57ms
step:1872/2090 train_time:107796ms step_avg:57.58ms
step:1873/2090 train_time:107885ms step_avg:57.60ms
step:1874/2090 train_time:107972ms step_avg:57.62ms
step:1875/2090 train_time:108060ms step_avg:57.63ms
step:1876/2090 train_time:108147ms step_avg:57.65ms
step:1877/2090 train_time:108235ms step_avg:57.66ms
step:1878/2090 train_time:108321ms step_avg:57.68ms
step:1879/2090 train_time:108409ms step_avg:57.70ms
step:1880/2090 train_time:108497ms step_avg:57.71ms
step:1881/2090 train_time:108585ms step_avg:57.73ms
step:1882/2090 train_time:108673ms step_avg:57.74ms
step:1883/2090 train_time:108760ms step_avg:57.76ms
step:1884/2090 train_time:108847ms step_avg:57.77ms
step:1885/2090 train_time:108935ms step_avg:57.79ms
step:1886/2090 train_time:109021ms step_avg:57.81ms
step:1887/2090 train_time:109109ms step_avg:57.82ms
step:1888/2090 train_time:109196ms step_avg:57.84ms
step:1889/2090 train_time:109284ms step_avg:57.85ms
step:1890/2090 train_time:109371ms step_avg:57.87ms
step:1891/2090 train_time:109458ms step_avg:57.88ms
step:1892/2090 train_time:109546ms step_avg:57.90ms
step:1893/2090 train_time:109634ms step_avg:57.92ms
step:1894/2090 train_time:109721ms step_avg:57.93ms
step:1895/2090 train_time:109809ms step_avg:57.95ms
step:1896/2090 train_time:109895ms step_avg:57.96ms
step:1897/2090 train_time:109983ms step_avg:57.98ms
step:1898/2090 train_time:110070ms step_avg:57.99ms
step:1899/2090 train_time:110159ms step_avg:58.01ms
step:1900/2090 train_time:110246ms step_avg:58.02ms
step:1901/2090 train_time:110334ms step_avg:58.04ms
step:1902/2090 train_time:110421ms step_avg:58.06ms
step:1903/2090 train_time:110509ms step_avg:58.07ms
step:1904/2090 train_time:110596ms step_avg:58.09ms
step:1905/2090 train_time:110684ms step_avg:58.10ms
step:1906/2090 train_time:110771ms step_avg:58.12ms
step:1907/2090 train_time:110860ms step_avg:58.13ms
step:1908/2090 train_time:110946ms step_avg:58.15ms
step:1909/2090 train_time:111034ms step_avg:58.16ms
step:1910/2090 train_time:111121ms step_avg:58.18ms
step:1911/2090 train_time:111209ms step_avg:58.19ms
step:1912/2090 train_time:111297ms step_avg:58.21ms
step:1913/2090 train_time:111385ms step_avg:58.23ms
step:1914/2090 train_time:111471ms step_avg:58.24ms
step:1915/2090 train_time:111560ms step_avg:58.26ms
step:1916/2090 train_time:111646ms step_avg:58.27ms
step:1917/2090 train_time:111734ms step_avg:58.29ms
step:1918/2090 train_time:111821ms step_avg:58.30ms
step:1919/2090 train_time:111909ms step_avg:58.32ms
step:1920/2090 train_time:111997ms step_avg:58.33ms
step:1921/2090 train_time:112085ms step_avg:58.35ms
step:1922/2090 train_time:112172ms step_avg:58.36ms
step:1923/2090 train_time:112260ms step_avg:58.38ms
step:1924/2090 train_time:112347ms step_avg:58.39ms
step:1925/2090 train_time:112435ms step_avg:58.41ms
step:1926/2090 train_time:112521ms step_avg:58.42ms
step:1927/2090 train_time:112610ms step_avg:58.44ms
step:1928/2090 train_time:112699ms step_avg:58.45ms
step:1929/2090 train_time:112786ms step_avg:58.47ms
step:1930/2090 train_time:112874ms step_avg:58.48ms
step:1931/2090 train_time:112961ms step_avg:58.50ms
step:1932/2090 train_time:113048ms step_avg:58.51ms
step:1933/2090 train_time:113135ms step_avg:58.53ms
step:1934/2090 train_time:113222ms step_avg:58.54ms
step:1935/2090 train_time:113311ms step_avg:58.56ms
step:1936/2090 train_time:113398ms step_avg:58.57ms
step:1937/2090 train_time:113486ms step_avg:58.59ms
step:1938/2090 train_time:113573ms step_avg:58.60ms
step:1939/2090 train_time:113661ms step_avg:58.62ms
step:1940/2090 train_time:113748ms step_avg:58.63ms
step:1941/2090 train_time:113836ms step_avg:58.65ms
step:1942/2090 train_time:113923ms step_avg:58.66ms
step:1943/2090 train_time:114011ms step_avg:58.68ms
step:1944/2090 train_time:114098ms step_avg:58.69ms
step:1945/2090 train_time:114185ms step_avg:58.71ms
step:1946/2090 train_time:114272ms step_avg:58.72ms
step:1947/2090 train_time:114360ms step_avg:58.74ms
step:1948/2090 train_time:114447ms step_avg:58.75ms
step:1949/2090 train_time:114535ms step_avg:58.77ms
step:1950/2090 train_time:114622ms step_avg:58.78ms
step:1951/2090 train_time:114710ms step_avg:58.80ms
step:1952/2090 train_time:114797ms step_avg:58.81ms
step:1953/2090 train_time:114884ms step_avg:58.82ms
step:1954/2090 train_time:114971ms step_avg:58.84ms
step:1955/2090 train_time:115060ms step_avg:58.85ms
step:1956/2090 train_time:115146ms step_avg:58.87ms
step:1957/2090 train_time:115234ms step_avg:58.88ms
step:1958/2090 train_time:115321ms step_avg:58.90ms
step:1959/2090 train_time:115409ms step_avg:58.91ms
step:1960/2090 train_time:115496ms step_avg:58.93ms
step:1961/2090 train_time:115584ms step_avg:58.94ms
step:1962/2090 train_time:115671ms step_avg:58.96ms
step:1963/2090 train_time:115758ms step_avg:58.97ms
step:1964/2090 train_time:115845ms step_avg:58.98ms
step:1965/2090 train_time:115933ms step_avg:59.00ms
step:1966/2090 train_time:116021ms step_avg:59.01ms
step:1967/2090 train_time:116109ms step_avg:59.03ms
step:1968/2090 train_time:116195ms step_avg:59.04ms
step:1969/2090 train_time:116284ms step_avg:59.06ms
step:1970/2090 train_time:116371ms step_avg:59.07ms
step:1971/2090 train_time:116458ms step_avg:59.09ms
step:1972/2090 train_time:116545ms step_avg:59.10ms
step:1973/2090 train_time:116633ms step_avg:59.11ms
step:1974/2090 train_time:116719ms step_avg:59.13ms
step:1975/2090 train_time:116808ms step_avg:59.14ms
step:1976/2090 train_time:116895ms step_avg:59.16ms
step:1977/2090 train_time:116983ms step_avg:59.17ms
step:1978/2090 train_time:117070ms step_avg:59.19ms
step:1979/2090 train_time:117158ms step_avg:59.20ms
step:1980/2090 train_time:117245ms step_avg:59.21ms
step:1981/2090 train_time:117333ms step_avg:59.23ms
step:1982/2090 train_time:117421ms step_avg:59.24ms
step:1983/2090 train_time:117508ms step_avg:59.26ms
step:1984/2090 train_time:117595ms step_avg:59.27ms
step:1985/2090 train_time:117683ms step_avg:59.29ms
step:1986/2090 train_time:117770ms step_avg:59.30ms
step:1987/2090 train_time:117859ms step_avg:59.31ms
step:1988/2090 train_time:117946ms step_avg:59.33ms
step:1989/2090 train_time:118035ms step_avg:59.34ms
step:1990/2090 train_time:118121ms step_avg:59.36ms
step:1991/2090 train_time:118209ms step_avg:59.37ms
step:1992/2090 train_time:118296ms step_avg:59.39ms
step:1993/2090 train_time:118385ms step_avg:59.40ms
step:1994/2090 train_time:118472ms step_avg:59.41ms
step:1995/2090 train_time:118560ms step_avg:59.43ms
step:1996/2090 train_time:118646ms step_avg:59.44ms
step:1997/2090 train_time:118734ms step_avg:59.46ms
step:1998/2090 train_time:118822ms step_avg:59.47ms
step:1999/2090 train_time:118910ms step_avg:59.48ms
step:2000/2090 train_time:118998ms step_avg:59.50ms
step:2000/2090 val_loss:3.2979 train_time:119087ms step_avg:59.54ms
step:2001/2090 train_time:119108ms step_avg:59.52ms
step:2002/2090 train_time:119176ms step_avg:59.53ms
step:2003/2090 train_time:119268ms step_avg:59.54ms
step:2004/2090 train_time:119355ms step_avg:59.56ms
step:2005/2090 train_time:119442ms step_avg:59.57ms
step:2006/2090 train_time:119529ms step_avg:59.59ms
step:2007/2090 train_time:119616ms step_avg:59.60ms
step:2008/2090 train_time:119702ms step_avg:59.61ms
step:2009/2090 train_time:119788ms step_avg:59.63ms
step:2010/2090 train_time:119875ms step_avg:59.64ms
step:2011/2090 train_time:119963ms step_avg:59.65ms
step:2012/2090 train_time:120053ms step_avg:59.67ms
step:2013/2090 train_time:120143ms step_avg:59.68ms
step:2014/2090 train_time:120232ms step_avg:59.70ms
step:2015/2090 train_time:120321ms step_avg:59.71ms
step:2016/2090 train_time:120408ms step_avg:59.73ms
step:2017/2090 train_time:120496ms step_avg:59.74ms
step:2018/2090 train_time:120581ms step_avg:59.75ms
step:2019/2090 train_time:120668ms step_avg:59.77ms
step:2020/2090 train_time:120754ms step_avg:59.78ms
step:2021/2090 train_time:120843ms step_avg:59.79ms
step:2022/2090 train_time:120929ms step_avg:59.81ms
step:2023/2090 train_time:121018ms step_avg:59.82ms
step:2024/2090 train_time:121107ms step_avg:59.84ms
step:2025/2090 train_time:121196ms step_avg:59.85ms
step:2026/2090 train_time:121285ms step_avg:59.86ms
step:2027/2090 train_time:121372ms step_avg:59.88ms
step:2028/2090 train_time:121459ms step_avg:59.89ms
step:2029/2090 train_time:121546ms step_avg:59.90ms
step:2030/2090 train_time:121633ms step_avg:59.92ms
step:2031/2090 train_time:121720ms step_avg:59.93ms
step:2032/2090 train_time:121806ms step_avg:59.94ms
step:2033/2090 train_time:121893ms step_avg:59.96ms
step:2034/2090 train_time:121981ms step_avg:59.97ms
step:2035/2090 train_time:122070ms step_avg:59.99ms
step:2036/2090 train_time:122157ms step_avg:60.00ms
step:2037/2090 train_time:122246ms step_avg:60.01ms
step:2038/2090 train_time:122333ms step_avg:60.03ms
step:2039/2090 train_time:122422ms step_avg:60.04ms
step:2040/2090 train_time:122509ms step_avg:60.05ms
step:2041/2090 train_time:122596ms step_avg:60.07ms
step:2042/2090 train_time:122683ms step_avg:60.08ms
step:2043/2090 train_time:122770ms step_avg:60.09ms
step:2044/2090 train_time:122856ms step_avg:60.11ms
step:2045/2090 train_time:122945ms step_avg:60.12ms
step:2046/2090 train_time:123032ms step_avg:60.13ms
step:2047/2090 train_time:123121ms step_avg:60.15ms
step:2048/2090 train_time:123209ms step_avg:60.16ms
step:2049/2090 train_time:123298ms step_avg:60.17ms
step:2050/2090 train_time:123385ms step_avg:60.19ms
step:2051/2090 train_time:123473ms step_avg:60.20ms
step:2052/2090 train_time:123560ms step_avg:60.21ms
step:2053/2090 train_time:123647ms step_avg:60.23ms
step:2054/2090 train_time:123734ms step_avg:60.24ms
step:2055/2090 train_time:123821ms step_avg:60.25ms
step:2056/2090 train_time:123909ms step_avg:60.27ms
step:2057/2090 train_time:123997ms step_avg:60.28ms
step:2058/2090 train_time:124086ms step_avg:60.29ms
step:2059/2090 train_time:124175ms step_avg:60.31ms
step:2060/2090 train_time:124262ms step_avg:60.32ms
step:2061/2090 train_time:124351ms step_avg:60.34ms
step:2062/2090 train_time:124438ms step_avg:60.35ms
step:2063/2090 train_time:124527ms step_avg:60.36ms
step:2064/2090 train_time:124613ms step_avg:60.37ms
step:2065/2090 train_time:124701ms step_avg:60.39ms
step:2066/2090 train_time:124788ms step_avg:60.40ms
step:2067/2090 train_time:124876ms step_avg:60.41ms
step:2068/2090 train_time:124963ms step_avg:60.43ms
step:2069/2090 train_time:125052ms step_avg:60.44ms
step:2070/2090 train_time:125139ms step_avg:60.45ms
step:2071/2090 train_time:125228ms step_avg:60.47ms
step:2072/2090 train_time:125316ms step_avg:60.48ms
step:2073/2090 train_time:125403ms step_avg:60.49ms
step:2074/2090 train_time:125491ms step_avg:60.51ms
step:2075/2090 train_time:125578ms step_avg:60.52ms
step:2076/2090 train_time:125667ms step_avg:60.53ms
step:2077/2090 train_time:125754ms step_avg:60.55ms
step:2078/2090 train_time:125841ms step_avg:60.56ms
step:2079/2090 train_time:125929ms step_avg:60.57ms
step:2080/2090 train_time:126017ms step_avg:60.58ms
step:2081/2090 train_time:126105ms step_avg:60.60ms
step:2082/2090 train_time:126192ms step_avg:60.61ms
step:2083/2090 train_time:126281ms step_avg:60.62ms
step:2084/2090 train_time:126370ms step_avg:60.64ms
step:2085/2090 train_time:126457ms step_avg:60.65ms
step:2086/2090 train_time:126544ms step_avg:60.66ms
step:2087/2090 train_time:126632ms step_avg:60.68ms
step:2088/2090 train_time:126719ms step_avg:60.69ms
step:2089/2090 train_time:126806ms step_avg:60.70ms
step:2090/2090 train_time:126894ms step_avg:60.71ms
step:2090/2090 val_loss:3.2769 train_time:126984ms step_avg:60.76ms
peak memory allocated: 29862 MiB reserved: 45156 MiB

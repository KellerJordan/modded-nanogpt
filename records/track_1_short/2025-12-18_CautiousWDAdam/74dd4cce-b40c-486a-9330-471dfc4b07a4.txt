import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # weight decay
                if wd != 0:
                    mask = (update * p_slice) >= 0
                    # lr as weight decay  schedule
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)

                    update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)
                
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2050  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 18 12:14:41 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2090 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2090 train_time:84ms step_avg:83.87ms
step:2/2090 train_time:108ms step_avg:53.78ms
step:3/2090 train_time:129ms step_avg:42.90ms
step:4/2090 train_time:160ms step_avg:39.90ms
step:5/2090 train_time:192ms step_avg:38.44ms
step:6/2090 train_time:280ms step_avg:46.64ms
step:7/2090 train_time:308ms step_avg:43.99ms
step:8/2090 train_time:341ms step_avg:42.59ms
step:9/2090 train_time:374ms step_avg:41.53ms
step:10/2090 train_time:406ms step_avg:40.64ms
step:11/2090 train_time:440ms step_avg:40.00ms
step:12/2090 train_time:473ms step_avg:39.39ms
step:13/2090 train_time:507ms step_avg:38.97ms
step:14/2090 train_time:539ms step_avg:38.53ms
step:15/2090 train_time:573ms step_avg:38.22ms
step:16/2090 train_time:606ms step_avg:37.87ms
step:17/2090 train_time:640ms step_avg:37.63ms
step:18/2090 train_time:672ms step_avg:37.36ms
step:19/2090 train_time:706ms step_avg:37.17ms
step:20/2090 train_time:739ms step_avg:36.95ms
step:21/2090 train_time:773ms step_avg:36.81ms
step:22/2090 train_time:806ms step_avg:36.63ms
step:23/2090 train_time:840ms step_avg:36.51ms
step:24/2090 train_time:873ms step_avg:36.37ms
step:25/2090 train_time:906ms step_avg:36.26ms
step:26/2090 train_time:939ms step_avg:36.13ms
step:27/2090 train_time:974ms step_avg:36.06ms
step:28/2090 train_time:1006ms step_avg:35.94ms
step:29/2090 train_time:1040ms step_avg:35.88ms
step:30/2090 train_time:1073ms step_avg:35.78ms
step:31/2090 train_time:1107ms step_avg:35.71ms
step:32/2090 train_time:1140ms step_avg:35.62ms
step:33/2090 train_time:1174ms step_avg:35.57ms
step:34/2090 train_time:1207ms step_avg:35.51ms
step:35/2090 train_time:1243ms step_avg:35.50ms
step:36/2090 train_time:1276ms step_avg:35.44ms
step:37/2090 train_time:1311ms step_avg:35.42ms
step:38/2090 train_time:1343ms step_avg:35.35ms
step:39/2090 train_time:1378ms step_avg:35.33ms
step:40/2090 train_time:1411ms step_avg:35.26ms
step:41/2090 train_time:1445ms step_avg:35.24ms
step:42/2090 train_time:1478ms step_avg:35.18ms
step:43/2090 train_time:1512ms step_avg:35.15ms
step:44/2090 train_time:1544ms step_avg:35.10ms
step:45/2090 train_time:1579ms step_avg:35.08ms
step:46/2090 train_time:1611ms step_avg:35.03ms
step:47/2090 train_time:1645ms step_avg:35.00ms
step:48/2090 train_time:1678ms step_avg:34.96ms
step:49/2090 train_time:1712ms step_avg:34.93ms
step:50/2090 train_time:1744ms step_avg:34.89ms
step:51/2090 train_time:1778ms step_avg:34.86ms
step:52/2090 train_time:1811ms step_avg:34.83ms
step:53/2090 train_time:1846ms step_avg:34.82ms
step:54/2090 train_time:1877ms step_avg:34.77ms
step:55/2090 train_time:1911ms step_avg:34.74ms
step:56/2090 train_time:1944ms step_avg:34.71ms
step:57/2090 train_time:1977ms step_avg:34.69ms
step:58/2090 train_time:2010ms step_avg:34.66ms
step:59/2090 train_time:2044ms step_avg:34.64ms
step:60/2090 train_time:2076ms step_avg:34.60ms
step:61/2090 train_time:2110ms step_avg:34.59ms
step:62/2090 train_time:2143ms step_avg:34.56ms
step:63/2090 train_time:2177ms step_avg:34.55ms
step:64/2090 train_time:2210ms step_avg:34.53ms
step:65/2090 train_time:2244ms step_avg:34.52ms
step:66/2090 train_time:2277ms step_avg:34.50ms
step:67/2090 train_time:2311ms step_avg:34.49ms
step:68/2090 train_time:2344ms step_avg:34.47ms
step:69/2090 train_time:2378ms step_avg:34.46ms
step:70/2090 train_time:2410ms step_avg:34.44ms
step:71/2090 train_time:2444ms step_avg:34.42ms
step:72/2090 train_time:2477ms step_avg:34.40ms
step:73/2090 train_time:2511ms step_avg:34.39ms
step:74/2090 train_time:2543ms step_avg:34.37ms
step:75/2090 train_time:2577ms step_avg:34.36ms
step:76/2090 train_time:2610ms step_avg:34.34ms
step:77/2090 train_time:2644ms step_avg:34.33ms
step:78/2090 train_time:2676ms step_avg:34.31ms
step:79/2090 train_time:2710ms step_avg:34.30ms
step:80/2090 train_time:2743ms step_avg:34.28ms
step:81/2090 train_time:2776ms step_avg:34.27ms
step:82/2090 train_time:2809ms step_avg:34.25ms
step:83/2090 train_time:2842ms step_avg:34.25ms
step:84/2090 train_time:2875ms step_avg:34.23ms
step:85/2090 train_time:2908ms step_avg:34.22ms
step:86/2090 train_time:2941ms step_avg:34.20ms
step:87/2090 train_time:2974ms step_avg:34.19ms
step:88/2090 train_time:3007ms step_avg:34.17ms
step:89/2090 train_time:3041ms step_avg:34.16ms
step:90/2090 train_time:3074ms step_avg:34.15ms
step:91/2090 train_time:3107ms step_avg:34.14ms
step:92/2090 train_time:3140ms step_avg:34.13ms
step:93/2090 train_time:3173ms step_avg:34.12ms
step:94/2090 train_time:3206ms step_avg:34.10ms
step:95/2090 train_time:3240ms step_avg:34.10ms
step:96/2090 train_time:3272ms step_avg:34.09ms
step:97/2090 train_time:3306ms step_avg:34.09ms
step:98/2090 train_time:3339ms step_avg:34.07ms
step:99/2090 train_time:3373ms step_avg:34.07ms
step:100/2090 train_time:3406ms step_avg:34.06ms
step:101/2090 train_time:3439ms step_avg:34.05ms
step:102/2090 train_time:3472ms step_avg:34.04ms
step:103/2090 train_time:3506ms step_avg:34.04ms
step:104/2090 train_time:3538ms step_avg:34.02ms
step:105/2090 train_time:3572ms step_avg:34.02ms
step:106/2090 train_time:3605ms step_avg:34.01ms
step:107/2090 train_time:3639ms step_avg:34.01ms
step:108/2090 train_time:3672ms step_avg:34.00ms
step:109/2090 train_time:3705ms step_avg:33.99ms
step:110/2090 train_time:3738ms step_avg:33.99ms
step:111/2090 train_time:3772ms step_avg:33.98ms
step:112/2090 train_time:3805ms step_avg:33.97ms
step:113/2090 train_time:3838ms step_avg:33.97ms
step:114/2090 train_time:3871ms step_avg:33.96ms
step:115/2090 train_time:3905ms step_avg:33.95ms
step:116/2090 train_time:3938ms step_avg:33.95ms
step:117/2090 train_time:3971ms step_avg:33.94ms
step:118/2090 train_time:4004ms step_avg:33.93ms
step:119/2090 train_time:4038ms step_avg:33.93ms
step:120/2090 train_time:4071ms step_avg:33.92ms
step:121/2090 train_time:4104ms step_avg:33.92ms
step:122/2090 train_time:4137ms step_avg:33.91ms
step:123/2090 train_time:4170ms step_avg:33.90ms
step:124/2090 train_time:4203ms step_avg:33.89ms
step:125/2090 train_time:4237ms step_avg:33.89ms
step:126/2090 train_time:4270ms step_avg:33.89ms
step:127/2090 train_time:4304ms step_avg:33.89ms
step:128/2090 train_time:4337ms step_avg:33.88ms
step:129/2090 train_time:4370ms step_avg:33.88ms
step:130/2090 train_time:4403ms step_avg:33.87ms
step:131/2090 train_time:4436ms step_avg:33.86ms
step:132/2090 train_time:4468ms step_avg:33.85ms
step:133/2090 train_time:4502ms step_avg:33.85ms
step:134/2090 train_time:4535ms step_avg:33.84ms
step:135/2090 train_time:4569ms step_avg:33.84ms
step:136/2090 train_time:4601ms step_avg:33.83ms
step:137/2090 train_time:4635ms step_avg:33.83ms
step:138/2090 train_time:4668ms step_avg:33.82ms
step:139/2090 train_time:4701ms step_avg:33.82ms
step:140/2090 train_time:4734ms step_avg:33.81ms
step:141/2090 train_time:4767ms step_avg:33.81ms
step:142/2090 train_time:4800ms step_avg:33.80ms
step:143/2090 train_time:4834ms step_avg:33.80ms
step:144/2090 train_time:4866ms step_avg:33.79ms
step:145/2090 train_time:4900ms step_avg:33.79ms
step:146/2090 train_time:4932ms step_avg:33.78ms
step:147/2090 train_time:4966ms step_avg:33.78ms
step:148/2090 train_time:4999ms step_avg:33.77ms
step:149/2090 train_time:5032ms step_avg:33.77ms
step:150/2090 train_time:5065ms step_avg:33.76ms
step:151/2090 train_time:5098ms step_avg:33.76ms
step:152/2090 train_time:5131ms step_avg:33.75ms
step:153/2090 train_time:5164ms step_avg:33.75ms
step:154/2090 train_time:5197ms step_avg:33.75ms
step:155/2090 train_time:5230ms step_avg:33.74ms
step:156/2090 train_time:5263ms step_avg:33.74ms
step:157/2090 train_time:5296ms step_avg:33.73ms
step:158/2090 train_time:5329ms step_avg:33.73ms
step:159/2090 train_time:5363ms step_avg:33.73ms
step:160/2090 train_time:5396ms step_avg:33.72ms
step:161/2090 train_time:5429ms step_avg:33.72ms
step:162/2090 train_time:5462ms step_avg:33.71ms
step:163/2090 train_time:5495ms step_avg:33.71ms
step:164/2090 train_time:5527ms step_avg:33.70ms
step:165/2090 train_time:5561ms step_avg:33.71ms
step:166/2090 train_time:5594ms step_avg:33.70ms
step:167/2090 train_time:5628ms step_avg:33.70ms
step:168/2090 train_time:5660ms step_avg:33.69ms
step:169/2090 train_time:5694ms step_avg:33.69ms
step:170/2090 train_time:5727ms step_avg:33.69ms
step:171/2090 train_time:5760ms step_avg:33.69ms
step:172/2090 train_time:5793ms step_avg:33.68ms
step:173/2090 train_time:5827ms step_avg:33.68ms
step:174/2090 train_time:5860ms step_avg:33.68ms
step:175/2090 train_time:5893ms step_avg:33.67ms
step:176/2090 train_time:5925ms step_avg:33.67ms
step:177/2090 train_time:5959ms step_avg:33.67ms
step:178/2090 train_time:5992ms step_avg:33.66ms
step:179/2090 train_time:6026ms step_avg:33.66ms
step:180/2090 train_time:6058ms step_avg:33.66ms
step:181/2090 train_time:6092ms step_avg:33.66ms
step:182/2090 train_time:6125ms step_avg:33.65ms
step:183/2090 train_time:6158ms step_avg:33.65ms
step:184/2090 train_time:6191ms step_avg:33.64ms
step:185/2090 train_time:6224ms step_avg:33.64ms
step:186/2090 train_time:6257ms step_avg:33.64ms
step:187/2090 train_time:6290ms step_avg:33.64ms
step:188/2090 train_time:6323ms step_avg:33.63ms
step:189/2090 train_time:6356ms step_avg:33.63ms
step:190/2090 train_time:6389ms step_avg:33.63ms
step:191/2090 train_time:6423ms step_avg:33.63ms
step:192/2090 train_time:6455ms step_avg:33.62ms
step:193/2090 train_time:6489ms step_avg:33.62ms
step:194/2090 train_time:6522ms step_avg:33.62ms
step:195/2090 train_time:6555ms step_avg:33.62ms
step:196/2090 train_time:6588ms step_avg:33.61ms
step:197/2090 train_time:6622ms step_avg:33.61ms
step:198/2090 train_time:6654ms step_avg:33.61ms
step:199/2090 train_time:6688ms step_avg:33.61ms
step:200/2090 train_time:6720ms step_avg:33.60ms
step:201/2090 train_time:6754ms step_avg:33.60ms
step:202/2090 train_time:6787ms step_avg:33.60ms
step:203/2090 train_time:6820ms step_avg:33.60ms
step:204/2090 train_time:6853ms step_avg:33.59ms
step:205/2090 train_time:6886ms step_avg:33.59ms
step:206/2090 train_time:6919ms step_avg:33.59ms
step:207/2090 train_time:6952ms step_avg:33.59ms
step:208/2090 train_time:6985ms step_avg:33.58ms
step:209/2090 train_time:7018ms step_avg:33.58ms
step:210/2090 train_time:7051ms step_avg:33.58ms
step:211/2090 train_time:7084ms step_avg:33.58ms
step:212/2090 train_time:7117ms step_avg:33.57ms
step:213/2090 train_time:7151ms step_avg:33.57ms
step:214/2090 train_time:7183ms step_avg:33.57ms
step:215/2090 train_time:7217ms step_avg:33.57ms
step:216/2090 train_time:7250ms step_avg:33.56ms
step:217/2090 train_time:7283ms step_avg:33.56ms
step:218/2090 train_time:7316ms step_avg:33.56ms
step:219/2090 train_time:7349ms step_avg:33.56ms
step:220/2090 train_time:7382ms step_avg:33.55ms
step:221/2090 train_time:7415ms step_avg:33.55ms
step:222/2090 train_time:7448ms step_avg:33.55ms
step:223/2090 train_time:7481ms step_avg:33.55ms
step:224/2090 train_time:7513ms step_avg:33.54ms
step:225/2090 train_time:7547ms step_avg:33.54ms
step:226/2090 train_time:7580ms step_avg:33.54ms
step:227/2090 train_time:7613ms step_avg:33.54ms
step:228/2090 train_time:7646ms step_avg:33.53ms
step:229/2090 train_time:7679ms step_avg:33.53ms
step:230/2090 train_time:7712ms step_avg:33.53ms
step:231/2090 train_time:7746ms step_avg:33.53ms
step:232/2090 train_time:7778ms step_avg:33.53ms
step:233/2090 train_time:7812ms step_avg:33.53ms
step:234/2090 train_time:7845ms step_avg:33.52ms
step:235/2090 train_time:7878ms step_avg:33.53ms
step:236/2090 train_time:7911ms step_avg:33.52ms
step:237/2090 train_time:7945ms step_avg:33.52ms
step:238/2090 train_time:7978ms step_avg:33.52ms
step:239/2090 train_time:8011ms step_avg:33.52ms
step:240/2090 train_time:8044ms step_avg:33.52ms
step:241/2090 train_time:8078ms step_avg:33.52ms
step:242/2090 train_time:8110ms step_avg:33.51ms
step:243/2090 train_time:8144ms step_avg:33.51ms
step:244/2090 train_time:8176ms step_avg:33.51ms
step:245/2090 train_time:8210ms step_avg:33.51ms
step:246/2090 train_time:8242ms step_avg:33.51ms
step:247/2090 train_time:8276ms step_avg:33.51ms
step:248/2090 train_time:8308ms step_avg:33.50ms
step:249/2090 train_time:8342ms step_avg:33.50ms
step:250/2090 train_time:8375ms step_avg:33.50ms
step:250/2090 val_loss:4.2691 train_time:8410ms step_avg:33.64ms
step:251/2090 train_time:8430ms step_avg:33.58ms
step:252/2090 train_time:8449ms step_avg:33.53ms
step:253/2090 train_time:8478ms step_avg:33.51ms
step:254/2090 train_time:8511ms step_avg:33.51ms
step:255/2090 train_time:8547ms step_avg:33.52ms
step:256/2090 train_time:8581ms step_avg:33.52ms
step:257/2090 train_time:8615ms step_avg:33.52ms
step:258/2090 train_time:8648ms step_avg:33.52ms
step:259/2090 train_time:8681ms step_avg:33.52ms
step:260/2090 train_time:8714ms step_avg:33.52ms
step:261/2090 train_time:8748ms step_avg:33.52ms
step:262/2090 train_time:8780ms step_avg:33.51ms
step:263/2090 train_time:8813ms step_avg:33.51ms
step:264/2090 train_time:8846ms step_avg:33.51ms
step:265/2090 train_time:8880ms step_avg:33.51ms
step:266/2090 train_time:8912ms step_avg:33.51ms
step:267/2090 train_time:8946ms step_avg:33.50ms
step:268/2090 train_time:8978ms step_avg:33.50ms
step:269/2090 train_time:9011ms step_avg:33.50ms
step:270/2090 train_time:9044ms step_avg:33.50ms
step:271/2090 train_time:9077ms step_avg:33.49ms
step:272/2090 train_time:9109ms step_avg:33.49ms
step:273/2090 train_time:9143ms step_avg:33.49ms
step:274/2090 train_time:9175ms step_avg:33.49ms
step:275/2090 train_time:9208ms step_avg:33.49ms
step:276/2090 train_time:9241ms step_avg:33.48ms
step:277/2090 train_time:9274ms step_avg:33.48ms
step:278/2090 train_time:9307ms step_avg:33.48ms
step:279/2090 train_time:9340ms step_avg:33.48ms
step:280/2090 train_time:9373ms step_avg:33.47ms
step:281/2090 train_time:9406ms step_avg:33.47ms
step:282/2090 train_time:9438ms step_avg:33.47ms
step:283/2090 train_time:9472ms step_avg:33.47ms
step:284/2090 train_time:9505ms step_avg:33.47ms
step:285/2090 train_time:9540ms step_avg:33.47ms
step:286/2090 train_time:9572ms step_avg:33.47ms
step:287/2090 train_time:9606ms step_avg:33.47ms
step:288/2090 train_time:9639ms step_avg:33.47ms
step:289/2090 train_time:9673ms step_avg:33.47ms
step:290/2090 train_time:9706ms step_avg:33.47ms
step:291/2090 train_time:9740ms step_avg:33.47ms
step:292/2090 train_time:9772ms step_avg:33.47ms
step:293/2090 train_time:9806ms step_avg:33.47ms
step:294/2090 train_time:9839ms step_avg:33.46ms
step:295/2090 train_time:9872ms step_avg:33.46ms
step:296/2090 train_time:9905ms step_avg:33.46ms
step:297/2090 train_time:9939ms step_avg:33.46ms
step:298/2090 train_time:9971ms step_avg:33.46ms
step:299/2090 train_time:10004ms step_avg:33.46ms
step:300/2090 train_time:10037ms step_avg:33.46ms
step:301/2090 train_time:10070ms step_avg:33.46ms
step:302/2090 train_time:10103ms step_avg:33.45ms
step:303/2090 train_time:10136ms step_avg:33.45ms
step:304/2090 train_time:10169ms step_avg:33.45ms
step:305/2090 train_time:10202ms step_avg:33.45ms
step:306/2090 train_time:10235ms step_avg:33.45ms
step:307/2090 train_time:10268ms step_avg:33.45ms
step:308/2090 train_time:10300ms step_avg:33.44ms
step:309/2090 train_time:10333ms step_avg:33.44ms
step:310/2090 train_time:10366ms step_avg:33.44ms
step:311/2090 train_time:10400ms step_avg:33.44ms
step:312/2090 train_time:10432ms step_avg:33.44ms
step:313/2090 train_time:10466ms step_avg:33.44ms
step:314/2090 train_time:10499ms step_avg:33.44ms
step:315/2090 train_time:10532ms step_avg:33.44ms
step:316/2090 train_time:10565ms step_avg:33.43ms
step:317/2090 train_time:10599ms step_avg:33.43ms
step:318/2090 train_time:10631ms step_avg:33.43ms
step:319/2090 train_time:10665ms step_avg:33.43ms
step:320/2090 train_time:10698ms step_avg:33.43ms
step:321/2090 train_time:10732ms step_avg:33.43ms
step:322/2090 train_time:10765ms step_avg:33.43ms
step:323/2090 train_time:10798ms step_avg:33.43ms
step:324/2090 train_time:10831ms step_avg:33.43ms
step:325/2090 train_time:10864ms step_avg:33.43ms
step:326/2090 train_time:10897ms step_avg:33.43ms
step:327/2090 train_time:10931ms step_avg:33.43ms
step:328/2090 train_time:10963ms step_avg:33.43ms
step:329/2090 train_time:10997ms step_avg:33.42ms
step:330/2090 train_time:11029ms step_avg:33.42ms
step:331/2090 train_time:11063ms step_avg:33.42ms
step:332/2090 train_time:11096ms step_avg:33.42ms
step:333/2090 train_time:11129ms step_avg:33.42ms
step:334/2090 train_time:11162ms step_avg:33.42ms
step:335/2090 train_time:11195ms step_avg:33.42ms
step:336/2090 train_time:11228ms step_avg:33.42ms
step:337/2090 train_time:11261ms step_avg:33.42ms
step:338/2090 train_time:11294ms step_avg:33.41ms
step:339/2090 train_time:11327ms step_avg:33.41ms
step:340/2090 train_time:11360ms step_avg:33.41ms
step:341/2090 train_time:11393ms step_avg:33.41ms
step:342/2090 train_time:11426ms step_avg:33.41ms
step:343/2090 train_time:11460ms step_avg:33.41ms
step:344/2090 train_time:11493ms step_avg:33.41ms
step:345/2090 train_time:11526ms step_avg:33.41ms
step:346/2090 train_time:11559ms step_avg:33.41ms
step:347/2090 train_time:11593ms step_avg:33.41ms
step:348/2090 train_time:11626ms step_avg:33.41ms
step:349/2090 train_time:11660ms step_avg:33.41ms
step:350/2090 train_time:11692ms step_avg:33.41ms
step:351/2090 train_time:11726ms step_avg:33.41ms
step:352/2090 train_time:11759ms step_avg:33.40ms
step:353/2090 train_time:11792ms step_avg:33.41ms
step:354/2090 train_time:11825ms step_avg:33.40ms
step:355/2090 train_time:11859ms step_avg:33.40ms
step:356/2090 train_time:11891ms step_avg:33.40ms
step:357/2090 train_time:11925ms step_avg:33.40ms
step:358/2090 train_time:11958ms step_avg:33.40ms
step:359/2090 train_time:11991ms step_avg:33.40ms
step:360/2090 train_time:12024ms step_avg:33.40ms
step:361/2090 train_time:12058ms step_avg:33.40ms
step:362/2090 train_time:12091ms step_avg:33.40ms
step:363/2090 train_time:12124ms step_avg:33.40ms
step:364/2090 train_time:12157ms step_avg:33.40ms
step:365/2090 train_time:12190ms step_avg:33.40ms
step:366/2090 train_time:12222ms step_avg:33.39ms
step:367/2090 train_time:12255ms step_avg:33.39ms
step:368/2090 train_time:12288ms step_avg:33.39ms
step:369/2090 train_time:12321ms step_avg:33.39ms
step:370/2090 train_time:12354ms step_avg:33.39ms
step:371/2090 train_time:12387ms step_avg:33.39ms
step:372/2090 train_time:12420ms step_avg:33.39ms
step:373/2090 train_time:12453ms step_avg:33.39ms
step:374/2090 train_time:12486ms step_avg:33.38ms
step:375/2090 train_time:12520ms step_avg:33.39ms
step:376/2090 train_time:12553ms step_avg:33.38ms
step:377/2090 train_time:12586ms step_avg:33.39ms
step:378/2090 train_time:12619ms step_avg:33.38ms
step:379/2090 train_time:12652ms step_avg:33.38ms
step:380/2090 train_time:12685ms step_avg:33.38ms
step:381/2090 train_time:12719ms step_avg:33.38ms
step:382/2090 train_time:12751ms step_avg:33.38ms
step:383/2090 train_time:12785ms step_avg:33.38ms
step:384/2090 train_time:12818ms step_avg:33.38ms
step:385/2090 train_time:12851ms step_avg:33.38ms
step:386/2090 train_time:12884ms step_avg:33.38ms
step:387/2090 train_time:12918ms step_avg:33.38ms
step:388/2090 train_time:12950ms step_avg:33.38ms
step:389/2090 train_time:12984ms step_avg:33.38ms
step:390/2090 train_time:13017ms step_avg:33.38ms
step:391/2090 train_time:13050ms step_avg:33.38ms
step:392/2090 train_time:13083ms step_avg:33.37ms
step:393/2090 train_time:13116ms step_avg:33.37ms
step:394/2090 train_time:13149ms step_avg:33.37ms
step:395/2090 train_time:13183ms step_avg:33.37ms
step:396/2090 train_time:13215ms step_avg:33.37ms
step:397/2090 train_time:13249ms step_avg:33.37ms
step:398/2090 train_time:13281ms step_avg:33.37ms
step:399/2090 train_time:13314ms step_avg:33.37ms
step:400/2090 train_time:13347ms step_avg:33.37ms
step:401/2090 train_time:13381ms step_avg:33.37ms
step:402/2090 train_time:13413ms step_avg:33.37ms
step:403/2090 train_time:13446ms step_avg:33.37ms
step:404/2090 train_time:13479ms step_avg:33.36ms
step:405/2090 train_time:13512ms step_avg:33.36ms
step:406/2090 train_time:13545ms step_avg:33.36ms
step:407/2090 train_time:13579ms step_avg:33.36ms
step:408/2090 train_time:13612ms step_avg:33.36ms
step:409/2090 train_time:13645ms step_avg:33.36ms
step:410/2090 train_time:13678ms step_avg:33.36ms
step:411/2090 train_time:13711ms step_avg:33.36ms
step:412/2090 train_time:13744ms step_avg:33.36ms
step:413/2090 train_time:13777ms step_avg:33.36ms
step:414/2090 train_time:13810ms step_avg:33.36ms
step:415/2090 train_time:13843ms step_avg:33.36ms
step:416/2090 train_time:13876ms step_avg:33.35ms
step:417/2090 train_time:13909ms step_avg:33.36ms
step:418/2090 train_time:13942ms step_avg:33.35ms
step:419/2090 train_time:13975ms step_avg:33.35ms
step:420/2090 train_time:14008ms step_avg:33.35ms
step:421/2090 train_time:14041ms step_avg:33.35ms
step:422/2090 train_time:14074ms step_avg:33.35ms
step:423/2090 train_time:14108ms step_avg:33.35ms
step:424/2090 train_time:14140ms step_avg:33.35ms
step:425/2090 train_time:14173ms step_avg:33.35ms
step:426/2090 train_time:14206ms step_avg:33.35ms
step:427/2090 train_time:14240ms step_avg:33.35ms
step:428/2090 train_time:14272ms step_avg:33.35ms
step:429/2090 train_time:14306ms step_avg:33.35ms
step:430/2090 train_time:14339ms step_avg:33.35ms
step:431/2090 train_time:14372ms step_avg:33.35ms
step:432/2090 train_time:14404ms step_avg:33.34ms
step:433/2090 train_time:14438ms step_avg:33.34ms
step:434/2090 train_time:14470ms step_avg:33.34ms
step:435/2090 train_time:14504ms step_avg:33.34ms
step:436/2090 train_time:14536ms step_avg:33.34ms
step:437/2090 train_time:14570ms step_avg:33.34ms
step:438/2090 train_time:14602ms step_avg:33.34ms
step:439/2090 train_time:14636ms step_avg:33.34ms
step:440/2090 train_time:14668ms step_avg:33.34ms
step:441/2090 train_time:14702ms step_avg:33.34ms
step:442/2090 train_time:14734ms step_avg:33.34ms
step:443/2090 train_time:14768ms step_avg:33.34ms
step:444/2090 train_time:14800ms step_avg:33.33ms
step:445/2090 train_time:14834ms step_avg:33.33ms
step:446/2090 train_time:14867ms step_avg:33.33ms
step:447/2090 train_time:14901ms step_avg:33.33ms
step:448/2090 train_time:14933ms step_avg:33.33ms
step:449/2090 train_time:14967ms step_avg:33.33ms
step:450/2090 train_time:15000ms step_avg:33.33ms
step:451/2090 train_time:15034ms step_avg:33.33ms
step:452/2090 train_time:15066ms step_avg:33.33ms
step:453/2090 train_time:15099ms step_avg:33.33ms
step:454/2090 train_time:15132ms step_avg:33.33ms
step:455/2090 train_time:15166ms step_avg:33.33ms
step:456/2090 train_time:15198ms step_avg:33.33ms
step:457/2090 train_time:15231ms step_avg:33.33ms
step:458/2090 train_time:15264ms step_avg:33.33ms
step:459/2090 train_time:15297ms step_avg:33.33ms
step:460/2090 train_time:15329ms step_avg:33.32ms
step:461/2090 train_time:15363ms step_avg:33.33ms
step:462/2090 train_time:15396ms step_avg:33.32ms
step:463/2090 train_time:15429ms step_avg:33.32ms
step:464/2090 train_time:15462ms step_avg:33.32ms
step:465/2090 train_time:15495ms step_avg:33.32ms
step:466/2090 train_time:15528ms step_avg:33.32ms
step:467/2090 train_time:15561ms step_avg:33.32ms
step:468/2090 train_time:15594ms step_avg:33.32ms
step:469/2090 train_time:15628ms step_avg:33.32ms
step:470/2090 train_time:15661ms step_avg:33.32ms
step:471/2090 train_time:15694ms step_avg:33.32ms
step:472/2090 train_time:15727ms step_avg:33.32ms
step:473/2090 train_time:15760ms step_avg:33.32ms
step:474/2090 train_time:15793ms step_avg:33.32ms
step:475/2090 train_time:15826ms step_avg:33.32ms
step:476/2090 train_time:15859ms step_avg:33.32ms
step:477/2090 train_time:15892ms step_avg:33.32ms
step:478/2090 train_time:15925ms step_avg:33.32ms
step:479/2090 train_time:15959ms step_avg:33.32ms
step:480/2090 train_time:15992ms step_avg:33.32ms
step:481/2090 train_time:16025ms step_avg:33.32ms
step:482/2090 train_time:16058ms step_avg:33.32ms
step:483/2090 train_time:16092ms step_avg:33.32ms
step:484/2090 train_time:16124ms step_avg:33.31ms
step:485/2090 train_time:16158ms step_avg:33.32ms
step:486/2090 train_time:16191ms step_avg:33.31ms
step:487/2090 train_time:16225ms step_avg:33.32ms
step:488/2090 train_time:16257ms step_avg:33.31ms
step:489/2090 train_time:16291ms step_avg:33.31ms
step:490/2090 train_time:16323ms step_avg:33.31ms
step:491/2090 train_time:16357ms step_avg:33.31ms
step:492/2090 train_time:16390ms step_avg:33.31ms
step:493/2090 train_time:16423ms step_avg:33.31ms
step:494/2090 train_time:16456ms step_avg:33.31ms
step:495/2090 train_time:16489ms step_avg:33.31ms
step:496/2090 train_time:16522ms step_avg:33.31ms
step:497/2090 train_time:16555ms step_avg:33.31ms
step:498/2090 train_time:16588ms step_avg:33.31ms
step:499/2090 train_time:16621ms step_avg:33.31ms
step:500/2090 train_time:16654ms step_avg:33.31ms
step:500/2090 val_loss:4.0103 train_time:16690ms step_avg:33.38ms
step:501/2090 train_time:16709ms step_avg:33.35ms
step:502/2090 train_time:16729ms step_avg:33.32ms
step:503/2090 train_time:16757ms step_avg:33.31ms
step:504/2090 train_time:16790ms step_avg:33.31ms
step:505/2090 train_time:16825ms step_avg:33.32ms
step:506/2090 train_time:16858ms step_avg:33.32ms
step:507/2090 train_time:16893ms step_avg:33.32ms
step:508/2090 train_time:16926ms step_avg:33.32ms
step:509/2090 train_time:16959ms step_avg:33.32ms
step:510/2090 train_time:16992ms step_avg:33.32ms
step:511/2090 train_time:17025ms step_avg:33.32ms
step:512/2090 train_time:17058ms step_avg:33.32ms
step:513/2090 train_time:17091ms step_avg:33.32ms
step:514/2090 train_time:17123ms step_avg:33.31ms
step:515/2090 train_time:17157ms step_avg:33.31ms
step:516/2090 train_time:17189ms step_avg:33.31ms
step:517/2090 train_time:17223ms step_avg:33.31ms
step:518/2090 train_time:17256ms step_avg:33.31ms
step:519/2090 train_time:17289ms step_avg:33.31ms
step:520/2090 train_time:17321ms step_avg:33.31ms
step:521/2090 train_time:17354ms step_avg:33.31ms
step:522/2090 train_time:17387ms step_avg:33.31ms
step:523/2090 train_time:17420ms step_avg:33.31ms
step:524/2090 train_time:17452ms step_avg:33.31ms
step:525/2090 train_time:17486ms step_avg:33.31ms
step:526/2090 train_time:17519ms step_avg:33.31ms
step:527/2090 train_time:17552ms step_avg:33.30ms
step:528/2090 train_time:17584ms step_avg:33.30ms
step:529/2090 train_time:17618ms step_avg:33.30ms
step:530/2090 train_time:17650ms step_avg:33.30ms
step:531/2090 train_time:17684ms step_avg:33.30ms
step:532/2090 train_time:17717ms step_avg:33.30ms
step:533/2090 train_time:17750ms step_avg:33.30ms
step:534/2090 train_time:17783ms step_avg:33.30ms
step:535/2090 train_time:17817ms step_avg:33.30ms
step:536/2090 train_time:17850ms step_avg:33.30ms
step:537/2090 train_time:17884ms step_avg:33.30ms
step:538/2090 train_time:17917ms step_avg:33.30ms
step:539/2090 train_time:17950ms step_avg:33.30ms
step:540/2090 train_time:17983ms step_avg:33.30ms
step:541/2090 train_time:18017ms step_avg:33.30ms
step:542/2090 train_time:18049ms step_avg:33.30ms
step:543/2090 train_time:18083ms step_avg:33.30ms
step:544/2090 train_time:18116ms step_avg:33.30ms
step:545/2090 train_time:18149ms step_avg:33.30ms
step:546/2090 train_time:18182ms step_avg:33.30ms
step:547/2090 train_time:18216ms step_avg:33.30ms
step:548/2090 train_time:18248ms step_avg:33.30ms
step:549/2090 train_time:18282ms step_avg:33.30ms
step:550/2090 train_time:18314ms step_avg:33.30ms
step:551/2090 train_time:18348ms step_avg:33.30ms
step:552/2090 train_time:18380ms step_avg:33.30ms
step:553/2090 train_time:18414ms step_avg:33.30ms
step:554/2090 train_time:18447ms step_avg:33.30ms
step:555/2090 train_time:18480ms step_avg:33.30ms
step:556/2090 train_time:18512ms step_avg:33.30ms
step:557/2090 train_time:18545ms step_avg:33.29ms
step:558/2090 train_time:18578ms step_avg:33.29ms
step:559/2090 train_time:18611ms step_avg:33.29ms
step:560/2090 train_time:18644ms step_avg:33.29ms
step:561/2090 train_time:18677ms step_avg:33.29ms
step:562/2090 train_time:18710ms step_avg:33.29ms
step:563/2090 train_time:18743ms step_avg:33.29ms
step:564/2090 train_time:18776ms step_avg:33.29ms
step:565/2090 train_time:18810ms step_avg:33.29ms
step:566/2090 train_time:18843ms step_avg:33.29ms
step:567/2090 train_time:18876ms step_avg:33.29ms
step:568/2090 train_time:18909ms step_avg:33.29ms
step:569/2090 train_time:18942ms step_avg:33.29ms
step:570/2090 train_time:18975ms step_avg:33.29ms
step:571/2090 train_time:19009ms step_avg:33.29ms
step:572/2090 train_time:19041ms step_avg:33.29ms
step:573/2090 train_time:19075ms step_avg:33.29ms
step:574/2090 train_time:19107ms step_avg:33.29ms
step:575/2090 train_time:19141ms step_avg:33.29ms
step:576/2090 train_time:19174ms step_avg:33.29ms
step:577/2090 train_time:19207ms step_avg:33.29ms
step:578/2090 train_time:19240ms step_avg:33.29ms
step:579/2090 train_time:19273ms step_avg:33.29ms
step:580/2090 train_time:19305ms step_avg:33.28ms
step:581/2090 train_time:19339ms step_avg:33.29ms
step:582/2090 train_time:19372ms step_avg:33.28ms
step:583/2090 train_time:19404ms step_avg:33.28ms
step:584/2090 train_time:19437ms step_avg:33.28ms
step:585/2090 train_time:19470ms step_avg:33.28ms
step:586/2090 train_time:19503ms step_avg:33.28ms
step:587/2090 train_time:19536ms step_avg:33.28ms
step:588/2090 train_time:19569ms step_avg:33.28ms
step:589/2090 train_time:19602ms step_avg:33.28ms
step:590/2090 train_time:19635ms step_avg:33.28ms
step:591/2090 train_time:19668ms step_avg:33.28ms
step:592/2090 train_time:19701ms step_avg:33.28ms
step:593/2090 train_time:19734ms step_avg:33.28ms
step:594/2090 train_time:19766ms step_avg:33.28ms
step:595/2090 train_time:19800ms step_avg:33.28ms
step:596/2090 train_time:19833ms step_avg:33.28ms
step:597/2090 train_time:19866ms step_avg:33.28ms
step:598/2090 train_time:19899ms step_avg:33.28ms
step:599/2090 train_time:19932ms step_avg:33.28ms
step:600/2090 train_time:19965ms step_avg:33.27ms
step:601/2090 train_time:19998ms step_avg:33.28ms
step:602/2090 train_time:20031ms step_avg:33.27ms
step:603/2090 train_time:20064ms step_avg:33.27ms
step:604/2090 train_time:20097ms step_avg:33.27ms
step:605/2090 train_time:20130ms step_avg:33.27ms
step:606/2090 train_time:20163ms step_avg:33.27ms
step:607/2090 train_time:20197ms step_avg:33.27ms
step:608/2090 train_time:20229ms step_avg:33.27ms
step:609/2090 train_time:20263ms step_avg:33.27ms
step:610/2090 train_time:20296ms step_avg:33.27ms
step:611/2090 train_time:20329ms step_avg:33.27ms
step:612/2090 train_time:20362ms step_avg:33.27ms
step:613/2090 train_time:20395ms step_avg:33.27ms
step:614/2090 train_time:20428ms step_avg:33.27ms
step:615/2090 train_time:20461ms step_avg:33.27ms
step:616/2090 train_time:20494ms step_avg:33.27ms
step:617/2090 train_time:20527ms step_avg:33.27ms
step:618/2090 train_time:20560ms step_avg:33.27ms
step:619/2090 train_time:20594ms step_avg:33.27ms
step:620/2090 train_time:20626ms step_avg:33.27ms
step:621/2090 train_time:20660ms step_avg:33.27ms
step:622/2090 train_time:20692ms step_avg:33.27ms
step:623/2090 train_time:20726ms step_avg:33.27ms
step:624/2090 train_time:20759ms step_avg:33.27ms
step:625/2090 train_time:20792ms step_avg:33.27ms
step:626/2090 train_time:20825ms step_avg:33.27ms
step:627/2090 train_time:20858ms step_avg:33.27ms
step:628/2090 train_time:20891ms step_avg:33.27ms
step:629/2090 train_time:20924ms step_avg:33.27ms
step:630/2090 train_time:20957ms step_avg:33.27ms
step:631/2090 train_time:20990ms step_avg:33.27ms
step:632/2090 train_time:21023ms step_avg:33.26ms
step:633/2090 train_time:21056ms step_avg:33.26ms
step:634/2090 train_time:21089ms step_avg:33.26ms
step:635/2090 train_time:21123ms step_avg:33.26ms
step:636/2090 train_time:21155ms step_avg:33.26ms
step:637/2090 train_time:21189ms step_avg:33.26ms
step:638/2090 train_time:21221ms step_avg:33.26ms
step:639/2090 train_time:21255ms step_avg:33.26ms
step:640/2090 train_time:21288ms step_avg:33.26ms
step:641/2090 train_time:21321ms step_avg:33.26ms
step:642/2090 train_time:21354ms step_avg:33.26ms
step:643/2090 train_time:21387ms step_avg:33.26ms
step:644/2090 train_time:21420ms step_avg:33.26ms
step:645/2090 train_time:21453ms step_avg:33.26ms
step:646/2090 train_time:21486ms step_avg:33.26ms
step:647/2090 train_time:21520ms step_avg:33.26ms
step:648/2090 train_time:21553ms step_avg:33.26ms
step:649/2090 train_time:21586ms step_avg:33.26ms
step:650/2090 train_time:21619ms step_avg:33.26ms
step:651/2090 train_time:21652ms step_avg:33.26ms
step:652/2090 train_time:21684ms step_avg:33.26ms
step:653/2090 train_time:21718ms step_avg:33.26ms
step:654/2090 train_time:21751ms step_avg:33.26ms
step:655/2090 train_time:21784ms step_avg:33.26ms
step:656/2090 train_time:21817ms step_avg:33.26ms
step:657/2090 train_time:21850ms step_avg:33.26ms
step:658/2090 train_time:21883ms step_avg:33.26ms
step:659/2090 train_time:21916ms step_avg:33.26ms
step:660/2090 train_time:21949ms step_avg:33.26ms
step:661/2090 train_time:21983ms step_avg:33.26ms
step:662/2090 train_time:22016ms step_avg:33.26ms
step:663/2090 train_time:22049ms step_avg:33.26ms
step:664/2090 train_time:22082ms step_avg:33.26ms
step:665/2090 train_time:22115ms step_avg:33.26ms
step:666/2090 train_time:22148ms step_avg:33.25ms
step:667/2090 train_time:22181ms step_avg:33.26ms
step:668/2090 train_time:22214ms step_avg:33.25ms
step:669/2090 train_time:22247ms step_avg:33.25ms
step:670/2090 train_time:22280ms step_avg:33.25ms
step:671/2090 train_time:22314ms step_avg:33.25ms
step:672/2090 train_time:22347ms step_avg:33.25ms
step:673/2090 train_time:22380ms step_avg:33.25ms
step:674/2090 train_time:22413ms step_avg:33.25ms
step:675/2090 train_time:22446ms step_avg:33.25ms
step:676/2090 train_time:22479ms step_avg:33.25ms
step:677/2090 train_time:22512ms step_avg:33.25ms
step:678/2090 train_time:22545ms step_avg:33.25ms
step:679/2090 train_time:22579ms step_avg:33.25ms
step:680/2090 train_time:22611ms step_avg:33.25ms
step:681/2090 train_time:22645ms step_avg:33.25ms
step:682/2090 train_time:22678ms step_avg:33.25ms
step:683/2090 train_time:22711ms step_avg:33.25ms
step:684/2090 train_time:22744ms step_avg:33.25ms
step:685/2090 train_time:22778ms step_avg:33.25ms
step:686/2090 train_time:22836ms step_avg:33.29ms
step:687/2090 train_time:22896ms step_avg:33.33ms
step:688/2090 train_time:22956ms step_avg:33.37ms
step:689/2090 train_time:23016ms step_avg:33.41ms
step:690/2090 train_time:23076ms step_avg:33.44ms
step:691/2090 train_time:23137ms step_avg:33.48ms
step:692/2090 train_time:23196ms step_avg:33.52ms
step:693/2090 train_time:23256ms step_avg:33.56ms
step:694/2090 train_time:23315ms step_avg:33.59ms
step:695/2090 train_time:23375ms step_avg:33.63ms
step:696/2090 train_time:23435ms step_avg:33.67ms
step:697/2090 train_time:23496ms step_avg:33.71ms
step:698/2090 train_time:23555ms step_avg:33.75ms
step:699/2090 train_time:23615ms step_avg:33.78ms
step:700/2090 train_time:23675ms step_avg:33.82ms
step:701/2090 train_time:23735ms step_avg:33.86ms
step:702/2090 train_time:23794ms step_avg:33.89ms
step:703/2090 train_time:23854ms step_avg:33.93ms
step:704/2090 train_time:23914ms step_avg:33.97ms
step:705/2090 train_time:23974ms step_avg:34.01ms
step:706/2090 train_time:24034ms step_avg:34.04ms
step:707/2090 train_time:24095ms step_avg:34.08ms
step:708/2090 train_time:24155ms step_avg:34.12ms
step:709/2090 train_time:24214ms step_avg:34.15ms
step:710/2090 train_time:24274ms step_avg:34.19ms
step:711/2090 train_time:24334ms step_avg:34.23ms
step:712/2090 train_time:24394ms step_avg:34.26ms
step:713/2090 train_time:24455ms step_avg:34.30ms
step:714/2090 train_time:24514ms step_avg:34.33ms
step:715/2090 train_time:24575ms step_avg:34.37ms
step:716/2090 train_time:24634ms step_avg:34.40ms
step:717/2090 train_time:24693ms step_avg:34.44ms
step:718/2090 train_time:24753ms step_avg:34.47ms
step:719/2090 train_time:24813ms step_avg:34.51ms
step:720/2090 train_time:24872ms step_avg:34.54ms
step:721/2090 train_time:24933ms step_avg:34.58ms
step:722/2090 train_time:24992ms step_avg:34.62ms
step:723/2090 train_time:25053ms step_avg:34.65ms
step:724/2090 train_time:25112ms step_avg:34.69ms
step:725/2090 train_time:25172ms step_avg:34.72ms
step:726/2090 train_time:25232ms step_avg:34.75ms
step:727/2090 train_time:25292ms step_avg:34.79ms
step:728/2090 train_time:25351ms step_avg:34.82ms
step:729/2090 train_time:25412ms step_avg:34.86ms
step:730/2090 train_time:25472ms step_avg:34.89ms
step:731/2090 train_time:25533ms step_avg:34.93ms
step:732/2090 train_time:25593ms step_avg:34.96ms
step:733/2090 train_time:25653ms step_avg:35.00ms
step:734/2090 train_time:25712ms step_avg:35.03ms
step:735/2090 train_time:25772ms step_avg:35.06ms
step:736/2090 train_time:25832ms step_avg:35.10ms
step:737/2090 train_time:25892ms step_avg:35.13ms
step:738/2090 train_time:25951ms step_avg:35.16ms
step:739/2090 train_time:26012ms step_avg:35.20ms
step:740/2090 train_time:26072ms step_avg:35.23ms
step:741/2090 train_time:26133ms step_avg:35.27ms
step:742/2090 train_time:26193ms step_avg:35.30ms
step:743/2090 train_time:26253ms step_avg:35.33ms
step:744/2090 train_time:26312ms step_avg:35.37ms
step:745/2090 train_time:26373ms step_avg:35.40ms
step:746/2090 train_time:26432ms step_avg:35.43ms
step:747/2090 train_time:26493ms step_avg:35.47ms
step:748/2090 train_time:26552ms step_avg:35.50ms
step:749/2090 train_time:26613ms step_avg:35.53ms
step:750/2090 train_time:26672ms step_avg:35.56ms
step:750/2090 val_loss:3.8493 train_time:26735ms step_avg:35.65ms
step:751/2090 train_time:26755ms step_avg:35.63ms
step:752/2090 train_time:26795ms step_avg:35.63ms
step:753/2090 train_time:26858ms step_avg:35.67ms
step:754/2090 train_time:26920ms step_avg:35.70ms
step:755/2090 train_time:26981ms step_avg:35.74ms
step:756/2090 train_time:27040ms step_avg:35.77ms
step:757/2090 train_time:27100ms step_avg:35.80ms
step:758/2090 train_time:27159ms step_avg:35.83ms
step:759/2090 train_time:27218ms step_avg:35.86ms
step:760/2090 train_time:27277ms step_avg:35.89ms
step:761/2090 train_time:27336ms step_avg:35.92ms
step:762/2090 train_time:27395ms step_avg:35.95ms
step:763/2090 train_time:27454ms step_avg:35.98ms
step:764/2090 train_time:27513ms step_avg:36.01ms
step:765/2090 train_time:27572ms step_avg:36.04ms
step:766/2090 train_time:27631ms step_avg:36.07ms
step:767/2090 train_time:27692ms step_avg:36.10ms
step:768/2090 train_time:27753ms step_avg:36.14ms
step:769/2090 train_time:27815ms step_avg:36.17ms
step:770/2090 train_time:27877ms step_avg:36.20ms
step:771/2090 train_time:27939ms step_avg:36.24ms
step:772/2090 train_time:27999ms step_avg:36.27ms
step:773/2090 train_time:28059ms step_avg:36.30ms
step:774/2090 train_time:28118ms step_avg:36.33ms
step:775/2090 train_time:28178ms step_avg:36.36ms
step:776/2090 train_time:28237ms step_avg:36.39ms
step:777/2090 train_time:28296ms step_avg:36.42ms
step:778/2090 train_time:28354ms step_avg:36.45ms
step:779/2090 train_time:28414ms step_avg:36.48ms
step:780/2090 train_time:28473ms step_avg:36.50ms
step:781/2090 train_time:28532ms step_avg:36.53ms
step:782/2090 train_time:28591ms step_avg:36.56ms
step:783/2090 train_time:28651ms step_avg:36.59ms
step:784/2090 train_time:28711ms step_avg:36.62ms
step:785/2090 train_time:28773ms step_avg:36.65ms
step:786/2090 train_time:28833ms step_avg:36.68ms
step:787/2090 train_time:28894ms step_avg:36.71ms
step:788/2090 train_time:28954ms step_avg:36.74ms
step:789/2090 train_time:29015ms step_avg:36.77ms
step:790/2090 train_time:29075ms step_avg:36.80ms
step:791/2090 train_time:29135ms step_avg:36.83ms
step:792/2090 train_time:29194ms step_avg:36.86ms
step:793/2090 train_time:29254ms step_avg:36.89ms
step:794/2090 train_time:29313ms step_avg:36.92ms
step:795/2090 train_time:29372ms step_avg:36.95ms
step:796/2090 train_time:29431ms step_avg:36.97ms
step:797/2090 train_time:29491ms step_avg:37.00ms
step:798/2090 train_time:29550ms step_avg:37.03ms
step:799/2090 train_time:29610ms step_avg:37.06ms
step:800/2090 train_time:29669ms step_avg:37.09ms
step:801/2090 train_time:29730ms step_avg:37.12ms
step:802/2090 train_time:29790ms step_avg:37.15ms
step:803/2090 train_time:29852ms step_avg:37.18ms
step:804/2090 train_time:29912ms step_avg:37.20ms
step:805/2090 train_time:29973ms step_avg:37.23ms
step:806/2090 train_time:30033ms step_avg:37.26ms
step:807/2090 train_time:30094ms step_avg:37.29ms
step:808/2090 train_time:30153ms step_avg:37.32ms
step:809/2090 train_time:30213ms step_avg:37.35ms
step:810/2090 train_time:30272ms step_avg:37.37ms
step:811/2090 train_time:30333ms step_avg:37.40ms
step:812/2090 train_time:30391ms step_avg:37.43ms
step:813/2090 train_time:30452ms step_avg:37.46ms
step:814/2090 train_time:30510ms step_avg:37.48ms
step:815/2090 train_time:30570ms step_avg:37.51ms
step:816/2090 train_time:30629ms step_avg:37.54ms
step:817/2090 train_time:30689ms step_avg:37.56ms
step:818/2090 train_time:30749ms step_avg:37.59ms
step:819/2090 train_time:30809ms step_avg:37.62ms
step:820/2090 train_time:30869ms step_avg:37.65ms
step:821/2090 train_time:30931ms step_avg:37.67ms
step:822/2090 train_time:30991ms step_avg:37.70ms
step:823/2090 train_time:31052ms step_avg:37.73ms
step:824/2090 train_time:31112ms step_avg:37.76ms
step:825/2090 train_time:31172ms step_avg:37.78ms
step:826/2090 train_time:31231ms step_avg:37.81ms
step:827/2090 train_time:31292ms step_avg:37.84ms
step:828/2090 train_time:31351ms step_avg:37.86ms
step:829/2090 train_time:31411ms step_avg:37.89ms
step:830/2090 train_time:31470ms step_avg:37.92ms
step:831/2090 train_time:31530ms step_avg:37.94ms
step:832/2090 train_time:31589ms step_avg:37.97ms
step:833/2090 train_time:31650ms step_avg:37.99ms
step:834/2090 train_time:31709ms step_avg:38.02ms
step:835/2090 train_time:31769ms step_avg:38.05ms
step:836/2090 train_time:31829ms step_avg:38.07ms
step:837/2090 train_time:31890ms step_avg:38.10ms
step:838/2090 train_time:31949ms step_avg:38.13ms
step:839/2090 train_time:32010ms step_avg:38.15ms
step:840/2090 train_time:32070ms step_avg:38.18ms
step:841/2090 train_time:32131ms step_avg:38.21ms
step:842/2090 train_time:32190ms step_avg:38.23ms
step:843/2090 train_time:32252ms step_avg:38.26ms
step:844/2090 train_time:32311ms step_avg:38.28ms
step:845/2090 train_time:32372ms step_avg:38.31ms
step:846/2090 train_time:32431ms step_avg:38.33ms
step:847/2090 train_time:32491ms step_avg:38.36ms
step:848/2090 train_time:32550ms step_avg:38.38ms
step:849/2090 train_time:32611ms step_avg:38.41ms
step:850/2090 train_time:32670ms step_avg:38.44ms
step:851/2090 train_time:32731ms step_avg:38.46ms
step:852/2090 train_time:32790ms step_avg:38.49ms
step:853/2090 train_time:32851ms step_avg:38.51ms
step:854/2090 train_time:32911ms step_avg:38.54ms
step:855/2090 train_time:32972ms step_avg:38.56ms
step:856/2090 train_time:33031ms step_avg:38.59ms
step:857/2090 train_time:33092ms step_avg:38.61ms
step:858/2090 train_time:33151ms step_avg:38.64ms
step:859/2090 train_time:33212ms step_avg:38.66ms
step:860/2090 train_time:33272ms step_avg:38.69ms
step:861/2090 train_time:33332ms step_avg:38.71ms
step:862/2090 train_time:33391ms step_avg:38.74ms
step:863/2090 train_time:33451ms step_avg:38.76ms
step:864/2090 train_time:33509ms step_avg:38.78ms
step:865/2090 train_time:33569ms step_avg:38.81ms
step:866/2090 train_time:33629ms step_avg:38.83ms
step:867/2090 train_time:33689ms step_avg:38.86ms
step:868/2090 train_time:33749ms step_avg:38.88ms
step:869/2090 train_time:33811ms step_avg:38.91ms
step:870/2090 train_time:33871ms step_avg:38.93ms
step:871/2090 train_time:33932ms step_avg:38.96ms
step:872/2090 train_time:33991ms step_avg:38.98ms
step:873/2090 train_time:34053ms step_avg:39.01ms
step:874/2090 train_time:34112ms step_avg:39.03ms
step:875/2090 train_time:34173ms step_avg:39.05ms
step:876/2090 train_time:34232ms step_avg:39.08ms
step:877/2090 train_time:34293ms step_avg:39.10ms
step:878/2090 train_time:34352ms step_avg:39.13ms
step:879/2090 train_time:34412ms step_avg:39.15ms
step:880/2090 train_time:34472ms step_avg:39.17ms
step:881/2090 train_time:34532ms step_avg:39.20ms
step:882/2090 train_time:34591ms step_avg:39.22ms
step:883/2090 train_time:34652ms step_avg:39.24ms
step:884/2090 train_time:34711ms step_avg:39.27ms
step:885/2090 train_time:34772ms step_avg:39.29ms
step:886/2090 train_time:34832ms step_avg:39.31ms
step:887/2090 train_time:34893ms step_avg:39.34ms
step:888/2090 train_time:34953ms step_avg:39.36ms
step:889/2090 train_time:35014ms step_avg:39.39ms
step:890/2090 train_time:35073ms step_avg:39.41ms
step:891/2090 train_time:35133ms step_avg:39.43ms
step:892/2090 train_time:35192ms step_avg:39.45ms
step:893/2090 train_time:35253ms step_avg:39.48ms
step:894/2090 train_time:35312ms step_avg:39.50ms
step:895/2090 train_time:35372ms step_avg:39.52ms
step:896/2090 train_time:35431ms step_avg:39.54ms
step:897/2090 train_time:35491ms step_avg:39.57ms
step:898/2090 train_time:35551ms step_avg:39.59ms
step:899/2090 train_time:35611ms step_avg:39.61ms
step:900/2090 train_time:35670ms step_avg:39.63ms
step:901/2090 train_time:35731ms step_avg:39.66ms
step:902/2090 train_time:35791ms step_avg:39.68ms
step:903/2090 train_time:35852ms step_avg:39.70ms
step:904/2090 train_time:35911ms step_avg:39.72ms
step:905/2090 train_time:35972ms step_avg:39.75ms
step:906/2090 train_time:36032ms step_avg:39.77ms
step:907/2090 train_time:36093ms step_avg:39.79ms
step:908/2090 train_time:36152ms step_avg:39.82ms
step:909/2090 train_time:36213ms step_avg:39.84ms
step:910/2090 train_time:36273ms step_avg:39.86ms
step:911/2090 train_time:36333ms step_avg:39.88ms
step:912/2090 train_time:36392ms step_avg:39.90ms
step:913/2090 train_time:36453ms step_avg:39.93ms
step:914/2090 train_time:36512ms step_avg:39.95ms
step:915/2090 train_time:36572ms step_avg:39.97ms
step:916/2090 train_time:36631ms step_avg:39.99ms
step:917/2090 train_time:36692ms step_avg:40.01ms
step:918/2090 train_time:36751ms step_avg:40.03ms
step:919/2090 train_time:36811ms step_avg:40.06ms
step:920/2090 train_time:36871ms step_avg:40.08ms
step:921/2090 train_time:36931ms step_avg:40.10ms
step:922/2090 train_time:36991ms step_avg:40.12ms
step:923/2090 train_time:37052ms step_avg:40.14ms
step:924/2090 train_time:37112ms step_avg:40.16ms
step:925/2090 train_time:37172ms step_avg:40.19ms
step:926/2090 train_time:37232ms step_avg:40.21ms
step:927/2090 train_time:37293ms step_avg:40.23ms
step:928/2090 train_time:37351ms step_avg:40.25ms
step:929/2090 train_time:37412ms step_avg:40.27ms
step:930/2090 train_time:37471ms step_avg:40.29ms
step:931/2090 train_time:37532ms step_avg:40.31ms
step:932/2090 train_time:37591ms step_avg:40.33ms
step:933/2090 train_time:37651ms step_avg:40.36ms
step:934/2090 train_time:37710ms step_avg:40.38ms
step:935/2090 train_time:37771ms step_avg:40.40ms
step:936/2090 train_time:37830ms step_avg:40.42ms
step:937/2090 train_time:37891ms step_avg:40.44ms
step:938/2090 train_time:37951ms step_avg:40.46ms
step:939/2090 train_time:38011ms step_avg:40.48ms
step:940/2090 train_time:38070ms step_avg:40.50ms
step:941/2090 train_time:38131ms step_avg:40.52ms
step:942/2090 train_time:38190ms step_avg:40.54ms
step:943/2090 train_time:38251ms step_avg:40.56ms
step:944/2090 train_time:38311ms step_avg:40.58ms
step:945/2090 train_time:38371ms step_avg:40.60ms
step:946/2090 train_time:38430ms step_avg:40.62ms
step:947/2090 train_time:38491ms step_avg:40.65ms
step:948/2090 train_time:38550ms step_avg:40.66ms
step:949/2090 train_time:38611ms step_avg:40.69ms
step:950/2090 train_time:38670ms step_avg:40.71ms
step:951/2090 train_time:38731ms step_avg:40.73ms
step:952/2090 train_time:38790ms step_avg:40.75ms
step:953/2090 train_time:38852ms step_avg:40.77ms
step:954/2090 train_time:38911ms step_avg:40.79ms
step:955/2090 train_time:38972ms step_avg:40.81ms
step:956/2090 train_time:39031ms step_avg:40.83ms
step:957/2090 train_time:39092ms step_avg:40.85ms
step:958/2090 train_time:39151ms step_avg:40.87ms
step:959/2090 train_time:39212ms step_avg:40.89ms
step:960/2090 train_time:39272ms step_avg:40.91ms
step:961/2090 train_time:39332ms step_avg:40.93ms
step:962/2090 train_time:39392ms step_avg:40.95ms
step:963/2090 train_time:39453ms step_avg:40.97ms
step:964/2090 train_time:39511ms step_avg:40.99ms
step:965/2090 train_time:39572ms step_avg:41.01ms
step:966/2090 train_time:39632ms step_avg:41.03ms
step:967/2090 train_time:39692ms step_avg:41.05ms
step:968/2090 train_time:39752ms step_avg:41.07ms
step:969/2090 train_time:39813ms step_avg:41.09ms
step:970/2090 train_time:39873ms step_avg:41.11ms
step:971/2090 train_time:39933ms step_avg:41.13ms
step:972/2090 train_time:39993ms step_avg:41.15ms
step:973/2090 train_time:40053ms step_avg:41.16ms
step:974/2090 train_time:40112ms step_avg:41.18ms
step:975/2090 train_time:40173ms step_avg:41.20ms
step:976/2090 train_time:40232ms step_avg:41.22ms
step:977/2090 train_time:40292ms step_avg:41.24ms
step:978/2090 train_time:40352ms step_avg:41.26ms
step:979/2090 train_time:40412ms step_avg:41.28ms
step:980/2090 train_time:40472ms step_avg:41.30ms
step:981/2090 train_time:40533ms step_avg:41.32ms
step:982/2090 train_time:40591ms step_avg:41.34ms
step:983/2090 train_time:40651ms step_avg:41.35ms
step:984/2090 train_time:40710ms step_avg:41.37ms
step:985/2090 train_time:40771ms step_avg:41.39ms
step:986/2090 train_time:40830ms step_avg:41.41ms
step:987/2090 train_time:40890ms step_avg:41.43ms
step:988/2090 train_time:40950ms step_avg:41.45ms
step:989/2090 train_time:41011ms step_avg:41.47ms
step:990/2090 train_time:41071ms step_avg:41.49ms
step:991/2090 train_time:41131ms step_avg:41.50ms
step:992/2090 train_time:41191ms step_avg:41.52ms
step:993/2090 train_time:41252ms step_avg:41.54ms
step:994/2090 train_time:41312ms step_avg:41.56ms
step:995/2090 train_time:41373ms step_avg:41.58ms
step:996/2090 train_time:41432ms step_avg:41.60ms
step:997/2090 train_time:41492ms step_avg:41.62ms
step:998/2090 train_time:41551ms step_avg:41.63ms
step:999/2090 train_time:41612ms step_avg:41.65ms
step:1000/2090 train_time:41672ms step_avg:41.67ms
step:1000/2090 val_loss:3.7053 train_time:41734ms step_avg:41.73ms
step:1001/2090 train_time:41753ms step_avg:41.71ms
step:1002/2090 train_time:41794ms step_avg:41.71ms
step:1003/2090 train_time:41856ms step_avg:41.73ms
step:1004/2090 train_time:41918ms step_avg:41.75ms
step:1005/2090 train_time:41980ms step_avg:41.77ms
step:1006/2090 train_time:42039ms step_avg:41.79ms
step:1007/2090 train_time:42099ms step_avg:41.81ms
step:1008/2090 train_time:42158ms step_avg:41.82ms
step:1009/2090 train_time:42218ms step_avg:41.84ms
step:1010/2090 train_time:42277ms step_avg:41.86ms
step:1011/2090 train_time:42336ms step_avg:41.88ms
step:1012/2090 train_time:42395ms step_avg:41.89ms
step:1013/2090 train_time:42455ms step_avg:41.91ms
step:1014/2090 train_time:42513ms step_avg:41.93ms
step:1015/2090 train_time:42573ms step_avg:41.94ms
step:1016/2090 train_time:42632ms step_avg:41.96ms
step:1017/2090 train_time:42692ms step_avg:41.98ms
step:1018/2090 train_time:42752ms step_avg:42.00ms
step:1019/2090 train_time:42813ms step_avg:42.01ms
step:1020/2090 train_time:42874ms step_avg:42.03ms
step:1021/2090 train_time:42935ms step_avg:42.05ms
step:1022/2090 train_time:42996ms step_avg:42.07ms
step:1023/2090 train_time:43056ms step_avg:42.09ms
step:1024/2090 train_time:43115ms step_avg:42.10ms
step:1025/2090 train_time:43176ms step_avg:42.12ms
step:1026/2090 train_time:43235ms step_avg:42.14ms
step:1027/2090 train_time:43295ms step_avg:42.16ms
step:1028/2090 train_time:43354ms step_avg:42.17ms
step:1029/2090 train_time:43413ms step_avg:42.19ms
step:1030/2090 train_time:43472ms step_avg:42.21ms
step:1031/2090 train_time:43532ms step_avg:42.22ms
step:1032/2090 train_time:43591ms step_avg:42.24ms
step:1033/2090 train_time:43651ms step_avg:42.26ms
step:1034/2090 train_time:43710ms step_avg:42.27ms
step:1035/2090 train_time:43771ms step_avg:42.29ms
step:1036/2090 train_time:43830ms step_avg:42.31ms
step:1037/2090 train_time:43891ms step_avg:42.33ms
step:1038/2090 train_time:43952ms step_avg:42.34ms
step:1039/2090 train_time:44013ms step_avg:42.36ms
step:1040/2090 train_time:44073ms step_avg:42.38ms
step:1041/2090 train_time:44133ms step_avg:42.40ms
step:1042/2090 train_time:44193ms step_avg:42.41ms
step:1043/2090 train_time:44253ms step_avg:42.43ms
step:1044/2090 train_time:44312ms step_avg:42.44ms
step:1045/2090 train_time:44372ms step_avg:42.46ms
step:1046/2090 train_time:44431ms step_avg:42.48ms
step:1047/2090 train_time:44490ms step_avg:42.49ms
step:1048/2090 train_time:44549ms step_avg:42.51ms
step:1049/2090 train_time:44609ms step_avg:42.53ms
step:1050/2090 train_time:44669ms step_avg:42.54ms
step:1051/2090 train_time:44728ms step_avg:42.56ms
step:1052/2090 train_time:44788ms step_avg:42.57ms
step:1053/2090 train_time:44848ms step_avg:42.59ms
step:1054/2090 train_time:44908ms step_avg:42.61ms
step:1055/2090 train_time:44968ms step_avg:42.62ms
step:1056/2090 train_time:45028ms step_avg:42.64ms
step:1057/2090 train_time:45088ms step_avg:42.66ms
step:1058/2090 train_time:45148ms step_avg:42.67ms
step:1059/2090 train_time:45209ms step_avg:42.69ms
step:1060/2090 train_time:45268ms step_avg:42.71ms
step:1061/2090 train_time:45328ms step_avg:42.72ms
step:1062/2090 train_time:45387ms step_avg:42.74ms
step:1063/2090 train_time:45447ms step_avg:42.75ms
step:1064/2090 train_time:45506ms step_avg:42.77ms
step:1065/2090 train_time:45566ms step_avg:42.78ms
step:1066/2090 train_time:45625ms step_avg:42.80ms
step:1067/2090 train_time:45686ms step_avg:42.82ms
step:1068/2090 train_time:45745ms step_avg:42.83ms
step:1069/2090 train_time:45806ms step_avg:42.85ms
step:1070/2090 train_time:45866ms step_avg:42.87ms
step:1071/2090 train_time:45927ms step_avg:42.88ms
step:1072/2090 train_time:45987ms step_avg:42.90ms
step:1073/2090 train_time:46047ms step_avg:42.91ms
step:1074/2090 train_time:46107ms step_avg:42.93ms
step:1075/2090 train_time:46167ms step_avg:42.95ms
step:1076/2090 train_time:46227ms step_avg:42.96ms
step:1077/2090 train_time:46287ms step_avg:42.98ms
step:1078/2090 train_time:46347ms step_avg:42.99ms
step:1079/2090 train_time:46407ms step_avg:43.01ms
step:1080/2090 train_time:46466ms step_avg:43.02ms
step:1081/2090 train_time:46526ms step_avg:43.04ms
step:1082/2090 train_time:46585ms step_avg:43.05ms
step:1083/2090 train_time:46645ms step_avg:43.07ms
step:1084/2090 train_time:46705ms step_avg:43.09ms
step:1085/2090 train_time:46765ms step_avg:43.10ms
step:1086/2090 train_time:46825ms step_avg:43.12ms
step:1087/2090 train_time:46885ms step_avg:43.13ms
step:1088/2090 train_time:46945ms step_avg:43.15ms
step:1089/2090 train_time:47006ms step_avg:43.16ms
step:1090/2090 train_time:47066ms step_avg:43.18ms
step:1091/2090 train_time:47126ms step_avg:43.20ms
step:1092/2090 train_time:47186ms step_avg:43.21ms
step:1093/2090 train_time:47247ms step_avg:43.23ms
step:1094/2090 train_time:47306ms step_avg:43.24ms
step:1095/2090 train_time:47366ms step_avg:43.26ms
step:1096/2090 train_time:47425ms step_avg:43.27ms
step:1097/2090 train_time:47485ms step_avg:43.29ms
step:1098/2090 train_time:47544ms step_avg:43.30ms
step:1099/2090 train_time:47604ms step_avg:43.32ms
step:1100/2090 train_time:47663ms step_avg:43.33ms
step:1101/2090 train_time:47724ms step_avg:43.35ms
step:1102/2090 train_time:47784ms step_avg:43.36ms
step:1103/2090 train_time:47845ms step_avg:43.38ms
step:1104/2090 train_time:47905ms step_avg:43.39ms
step:1105/2090 train_time:47966ms step_avg:43.41ms
step:1106/2090 train_time:48026ms step_avg:43.42ms
step:1107/2090 train_time:48086ms step_avg:43.44ms
step:1108/2090 train_time:48146ms step_avg:43.45ms
step:1109/2090 train_time:48206ms step_avg:43.47ms
step:1110/2090 train_time:48266ms step_avg:43.48ms
step:1111/2090 train_time:48328ms step_avg:43.50ms
step:1112/2090 train_time:48387ms step_avg:43.51ms
step:1113/2090 train_time:48447ms step_avg:43.53ms
step:1114/2090 train_time:48506ms step_avg:43.54ms
step:1115/2090 train_time:48566ms step_avg:43.56ms
step:1116/2090 train_time:48625ms step_avg:43.57ms
step:1117/2090 train_time:48685ms step_avg:43.59ms
step:1118/2090 train_time:48745ms step_avg:43.60ms
step:1119/2090 train_time:48805ms step_avg:43.61ms
step:1120/2090 train_time:48865ms step_avg:43.63ms
step:1121/2090 train_time:48925ms step_avg:43.64ms
step:1122/2090 train_time:48985ms step_avg:43.66ms
step:1123/2090 train_time:49045ms step_avg:43.67ms
step:1124/2090 train_time:49105ms step_avg:43.69ms
step:1125/2090 train_time:49166ms step_avg:43.70ms
step:1126/2090 train_time:49225ms step_avg:43.72ms
step:1127/2090 train_time:49286ms step_avg:43.73ms
step:1128/2090 train_time:49345ms step_avg:43.75ms
step:1129/2090 train_time:49405ms step_avg:43.76ms
step:1130/2090 train_time:49465ms step_avg:43.77ms
step:1131/2090 train_time:49524ms step_avg:43.79ms
step:1132/2090 train_time:49583ms step_avg:43.80ms
step:1133/2090 train_time:49643ms step_avg:43.82ms
step:1134/2090 train_time:49703ms step_avg:43.83ms
step:1135/2090 train_time:49763ms step_avg:43.84ms
step:1136/2090 train_time:49823ms step_avg:43.86ms
step:1137/2090 train_time:49884ms step_avg:43.87ms
step:1138/2090 train_time:49943ms step_avg:43.89ms
step:1139/2090 train_time:50005ms step_avg:43.90ms
step:1140/2090 train_time:50064ms step_avg:43.92ms
step:1141/2090 train_time:50125ms step_avg:43.93ms
step:1142/2090 train_time:50184ms step_avg:43.94ms
step:1143/2090 train_time:50245ms step_avg:43.96ms
step:1144/2090 train_time:50304ms step_avg:43.97ms
step:1145/2090 train_time:50365ms step_avg:43.99ms
step:1146/2090 train_time:50425ms step_avg:44.00ms
step:1147/2090 train_time:50485ms step_avg:44.01ms
step:1148/2090 train_time:50544ms step_avg:44.03ms
step:1149/2090 train_time:50604ms step_avg:44.04ms
step:1150/2090 train_time:50663ms step_avg:44.06ms
step:1151/2090 train_time:50724ms step_avg:44.07ms
step:1152/2090 train_time:50783ms step_avg:44.08ms
step:1153/2090 train_time:50843ms step_avg:44.10ms
step:1154/2090 train_time:50903ms step_avg:44.11ms
step:1155/2090 train_time:50963ms step_avg:44.12ms
step:1156/2090 train_time:51023ms step_avg:44.14ms
step:1157/2090 train_time:51084ms step_avg:44.15ms
step:1158/2090 train_time:51143ms step_avg:44.17ms
step:1159/2090 train_time:51204ms step_avg:44.18ms
step:1160/2090 train_time:51263ms step_avg:44.19ms
step:1161/2090 train_time:51324ms step_avg:44.21ms
step:1162/2090 train_time:51383ms step_avg:44.22ms
step:1163/2090 train_time:51444ms step_avg:44.23ms
step:1164/2090 train_time:51503ms step_avg:44.25ms
step:1165/2090 train_time:51564ms step_avg:44.26ms
step:1166/2090 train_time:51623ms step_avg:44.27ms
step:1167/2090 train_time:51683ms step_avg:44.29ms
step:1168/2090 train_time:51742ms step_avg:44.30ms
step:1169/2090 train_time:51802ms step_avg:44.31ms
step:1170/2090 train_time:51861ms step_avg:44.33ms
step:1171/2090 train_time:51922ms step_avg:44.34ms
step:1172/2090 train_time:51981ms step_avg:44.35ms
step:1173/2090 train_time:52042ms step_avg:44.37ms
step:1174/2090 train_time:52102ms step_avg:44.38ms
step:1175/2090 train_time:52163ms step_avg:44.39ms
step:1176/2090 train_time:52223ms step_avg:44.41ms
step:1177/2090 train_time:52283ms step_avg:44.42ms
step:1178/2090 train_time:52343ms step_avg:44.43ms
step:1179/2090 train_time:52403ms step_avg:44.45ms
step:1180/2090 train_time:52462ms step_avg:44.46ms
step:1181/2090 train_time:52523ms step_avg:44.47ms
step:1182/2090 train_time:52583ms step_avg:44.49ms
step:1183/2090 train_time:52643ms step_avg:44.50ms
step:1184/2090 train_time:52703ms step_avg:44.51ms
step:1185/2090 train_time:52763ms step_avg:44.53ms
step:1186/2090 train_time:52822ms step_avg:44.54ms
step:1187/2090 train_time:52884ms step_avg:44.55ms
step:1188/2090 train_time:52943ms step_avg:44.56ms
step:1189/2090 train_time:53003ms step_avg:44.58ms
step:1190/2090 train_time:53062ms step_avg:44.59ms
step:1191/2090 train_time:53123ms step_avg:44.60ms
step:1192/2090 train_time:53182ms step_avg:44.62ms
step:1193/2090 train_time:53243ms step_avg:44.63ms
step:1194/2090 train_time:53303ms step_avg:44.64ms
step:1195/2090 train_time:53364ms step_avg:44.66ms
step:1196/2090 train_time:53423ms step_avg:44.67ms
step:1197/2090 train_time:53484ms step_avg:44.68ms
step:1198/2090 train_time:53543ms step_avg:44.69ms
step:1199/2090 train_time:53603ms step_avg:44.71ms
step:1200/2090 train_time:53662ms step_avg:44.72ms
step:1201/2090 train_time:53722ms step_avg:44.73ms
step:1202/2090 train_time:53782ms step_avg:44.74ms
step:1203/2090 train_time:53842ms step_avg:44.76ms
step:1204/2090 train_time:53902ms step_avg:44.77ms
step:1205/2090 train_time:53962ms step_avg:44.78ms
step:1206/2090 train_time:54022ms step_avg:44.79ms
step:1207/2090 train_time:54082ms step_avg:44.81ms
step:1208/2090 train_time:54141ms step_avg:44.82ms
step:1209/2090 train_time:54202ms step_avg:44.83ms
step:1210/2090 train_time:54262ms step_avg:44.84ms
step:1211/2090 train_time:54323ms step_avg:44.86ms
step:1212/2090 train_time:54382ms step_avg:44.87ms
step:1213/2090 train_time:54443ms step_avg:44.88ms
step:1214/2090 train_time:54502ms step_avg:44.89ms
step:1215/2090 train_time:54563ms step_avg:44.91ms
step:1216/2090 train_time:54622ms step_avg:44.92ms
step:1217/2090 train_time:54683ms step_avg:44.93ms
step:1218/2090 train_time:54742ms step_avg:44.94ms
step:1219/2090 train_time:54802ms step_avg:44.96ms
step:1220/2090 train_time:54861ms step_avg:44.97ms
step:1221/2090 train_time:54922ms step_avg:44.98ms
step:1222/2090 train_time:54981ms step_avg:44.99ms
step:1223/2090 train_time:55042ms step_avg:45.01ms
step:1224/2090 train_time:55102ms step_avg:45.02ms
step:1225/2090 train_time:55162ms step_avg:45.03ms
step:1226/2090 train_time:55222ms step_avg:45.04ms
step:1227/2090 train_time:55283ms step_avg:45.06ms
step:1228/2090 train_time:55342ms step_avg:45.07ms
step:1229/2090 train_time:55403ms step_avg:45.08ms
step:1230/2090 train_time:55462ms step_avg:45.09ms
step:1231/2090 train_time:55523ms step_avg:45.10ms
step:1232/2090 train_time:55583ms step_avg:45.12ms
step:1233/2090 train_time:55643ms step_avg:45.13ms
step:1234/2090 train_time:55702ms step_avg:45.14ms
step:1235/2090 train_time:55762ms step_avg:45.15ms
step:1236/2090 train_time:55822ms step_avg:45.16ms
step:1237/2090 train_time:55882ms step_avg:45.18ms
step:1238/2090 train_time:55941ms step_avg:45.19ms
step:1239/2090 train_time:56002ms step_avg:45.20ms
step:1240/2090 train_time:56061ms step_avg:45.21ms
step:1241/2090 train_time:56122ms step_avg:45.22ms
step:1242/2090 train_time:56181ms step_avg:45.23ms
step:1243/2090 train_time:56242ms step_avg:45.25ms
step:1244/2090 train_time:56302ms step_avg:45.26ms
step:1245/2090 train_time:56362ms step_avg:45.27ms
step:1246/2090 train_time:56422ms step_avg:45.28ms
step:1247/2090 train_time:56482ms step_avg:45.29ms
step:1248/2090 train_time:56542ms step_avg:45.31ms
step:1249/2090 train_time:56603ms step_avg:45.32ms
step:1250/2090 train_time:56662ms step_avg:45.33ms
step:1250/2090 val_loss:3.5869 train_time:56725ms step_avg:45.38ms
step:1251/2090 train_time:56745ms step_avg:45.36ms
step:1252/2090 train_time:56783ms step_avg:45.35ms
step:1253/2090 train_time:56846ms step_avg:45.37ms
step:1254/2090 train_time:56908ms step_avg:45.38ms
step:1255/2090 train_time:56969ms step_avg:45.39ms
step:1256/2090 train_time:57029ms step_avg:45.41ms
step:1257/2090 train_time:57090ms step_avg:45.42ms
step:1258/2090 train_time:57148ms step_avg:45.43ms
step:1259/2090 train_time:57209ms step_avg:45.44ms
step:1260/2090 train_time:57267ms step_avg:45.45ms
step:1261/2090 train_time:57327ms step_avg:45.46ms
step:1262/2090 train_time:57387ms step_avg:45.47ms
step:1263/2090 train_time:57447ms step_avg:45.48ms
step:1264/2090 train_time:57506ms step_avg:45.50ms
step:1265/2090 train_time:57567ms step_avg:45.51ms
step:1266/2090 train_time:57625ms step_avg:45.52ms
step:1267/2090 train_time:57687ms step_avg:45.53ms
step:1268/2090 train_time:57746ms step_avg:45.54ms
step:1269/2090 train_time:57807ms step_avg:45.55ms
step:1270/2090 train_time:57867ms step_avg:45.56ms
step:1271/2090 train_time:57928ms step_avg:45.58ms
step:1272/2090 train_time:57988ms step_avg:45.59ms
step:1273/2090 train_time:58050ms step_avg:45.60ms
step:1274/2090 train_time:58110ms step_avg:45.61ms
step:1275/2090 train_time:58170ms step_avg:45.62ms
step:1276/2090 train_time:58230ms step_avg:45.63ms
step:1277/2090 train_time:58290ms step_avg:45.65ms
step:1278/2090 train_time:58349ms step_avg:45.66ms
step:1279/2090 train_time:58410ms step_avg:45.67ms
step:1280/2090 train_time:58469ms step_avg:45.68ms
step:1281/2090 train_time:58529ms step_avg:45.69ms
step:1282/2090 train_time:58588ms step_avg:45.70ms
step:1283/2090 train_time:58649ms step_avg:45.71ms
step:1284/2090 train_time:58708ms step_avg:45.72ms
step:1285/2090 train_time:58769ms step_avg:45.73ms
step:1286/2090 train_time:58829ms step_avg:45.75ms
step:1287/2090 train_time:58890ms step_avg:45.76ms
step:1288/2090 train_time:58950ms step_avg:45.77ms
step:1289/2090 train_time:59011ms step_avg:45.78ms
step:1290/2090 train_time:59071ms step_avg:45.79ms
step:1291/2090 train_time:59131ms step_avg:45.80ms
step:1292/2090 train_time:59191ms step_avg:45.81ms
step:1293/2090 train_time:59251ms step_avg:45.82ms
step:1294/2090 train_time:59310ms step_avg:45.83ms
step:1295/2090 train_time:59370ms step_avg:45.85ms
step:1296/2090 train_time:59430ms step_avg:45.86ms
step:1297/2090 train_time:59490ms step_avg:45.87ms
step:1298/2090 train_time:59549ms step_avg:45.88ms
step:1299/2090 train_time:59610ms step_avg:45.89ms
step:1300/2090 train_time:59670ms step_avg:45.90ms
step:1301/2090 train_time:59731ms step_avg:45.91ms
step:1302/2090 train_time:59790ms step_avg:45.92ms
step:1303/2090 train_time:59851ms step_avg:45.93ms
step:1304/2090 train_time:59910ms step_avg:45.94ms
step:1305/2090 train_time:59971ms step_avg:45.95ms
step:1306/2090 train_time:60031ms step_avg:45.97ms
step:1307/2090 train_time:60092ms step_avg:45.98ms
step:1308/2090 train_time:60152ms step_avg:45.99ms
step:1309/2090 train_time:60212ms step_avg:46.00ms
step:1310/2090 train_time:60271ms step_avg:46.01ms
step:1311/2090 train_time:60331ms step_avg:46.02ms
step:1312/2090 train_time:60390ms step_avg:46.03ms
step:1313/2090 train_time:60451ms step_avg:46.04ms
step:1314/2090 train_time:60510ms step_avg:46.05ms
step:1315/2090 train_time:60571ms step_avg:46.06ms
step:1316/2090 train_time:60631ms step_avg:46.07ms
step:1317/2090 train_time:60692ms step_avg:46.08ms
step:1318/2090 train_time:60751ms step_avg:46.09ms
step:1319/2090 train_time:60812ms step_avg:46.10ms
step:1320/2090 train_time:60872ms step_avg:46.11ms
step:1321/2090 train_time:60932ms step_avg:46.13ms
step:1322/2090 train_time:60992ms step_avg:46.14ms
step:1323/2090 train_time:61053ms step_avg:46.15ms
step:1324/2090 train_time:61113ms step_avg:46.16ms
step:1325/2090 train_time:61173ms step_avg:46.17ms
step:1326/2090 train_time:61232ms step_avg:46.18ms
step:1327/2090 train_time:61292ms step_avg:46.19ms
step:1328/2090 train_time:61352ms step_avg:46.20ms
step:1329/2090 train_time:61412ms step_avg:46.21ms
step:1330/2090 train_time:61472ms step_avg:46.22ms
step:1331/2090 train_time:61533ms step_avg:46.23ms
step:1332/2090 train_time:61592ms step_avg:46.24ms
step:1333/2090 train_time:61653ms step_avg:46.25ms
step:1334/2090 train_time:61712ms step_avg:46.26ms
step:1335/2090 train_time:61774ms step_avg:46.27ms
step:1336/2090 train_time:61833ms step_avg:46.28ms
step:1337/2090 train_time:61895ms step_avg:46.29ms
step:1338/2090 train_time:61954ms step_avg:46.30ms
step:1339/2090 train_time:62015ms step_avg:46.31ms
step:1340/2090 train_time:62075ms step_avg:46.32ms
step:1341/2090 train_time:62135ms step_avg:46.34ms
step:1342/2090 train_time:62195ms step_avg:46.35ms
step:1343/2090 train_time:62255ms step_avg:46.36ms
step:1344/2090 train_time:62314ms step_avg:46.36ms
step:1345/2090 train_time:62374ms step_avg:46.37ms
step:1346/2090 train_time:62434ms step_avg:46.38ms
step:1347/2090 train_time:62494ms step_avg:46.39ms
step:1348/2090 train_time:62553ms step_avg:46.40ms
step:1349/2090 train_time:62613ms step_avg:46.41ms
step:1350/2090 train_time:62672ms step_avg:46.42ms
step:1351/2090 train_time:62732ms step_avg:46.43ms
step:1352/2090 train_time:62792ms step_avg:46.44ms
step:1353/2090 train_time:62853ms step_avg:46.45ms
step:1354/2090 train_time:62912ms step_avg:46.46ms
step:1355/2090 train_time:62973ms step_avg:46.47ms
step:1356/2090 train_time:63033ms step_avg:46.48ms
step:1357/2090 train_time:63093ms step_avg:46.49ms
step:1358/2090 train_time:63153ms step_avg:46.50ms
step:1359/2090 train_time:63213ms step_avg:46.51ms
step:1360/2090 train_time:63272ms step_avg:46.52ms
step:1361/2090 train_time:63332ms step_avg:46.53ms
step:1362/2090 train_time:63391ms step_avg:46.54ms
step:1363/2090 train_time:63452ms step_avg:46.55ms
step:1364/2090 train_time:63511ms step_avg:46.56ms
step:1365/2090 train_time:63572ms step_avg:46.57ms
step:1366/2090 train_time:63631ms step_avg:46.58ms
step:1367/2090 train_time:63692ms step_avg:46.59ms
step:1368/2090 train_time:63752ms step_avg:46.60ms
step:1369/2090 train_time:63841ms step_avg:46.63ms
step:1370/2090 train_time:63927ms step_avg:46.66ms
step:1371/2090 train_time:64018ms step_avg:46.69ms
step:1372/2090 train_time:64105ms step_avg:46.72ms
step:1373/2090 train_time:64193ms step_avg:46.75ms
step:1374/2090 train_time:64280ms step_avg:46.78ms
step:1375/2090 train_time:64368ms step_avg:46.81ms
step:1376/2090 train_time:64455ms step_avg:46.84ms
step:1377/2090 train_time:64542ms step_avg:46.87ms
step:1378/2090 train_time:64629ms step_avg:46.90ms
step:1379/2090 train_time:64716ms step_avg:46.93ms
step:1380/2090 train_time:64803ms step_avg:46.96ms
step:1381/2090 train_time:64891ms step_avg:46.99ms
step:1382/2090 train_time:64978ms step_avg:47.02ms
step:1383/2090 train_time:65067ms step_avg:47.05ms
step:1384/2090 train_time:65154ms step_avg:47.08ms
step:1385/2090 train_time:65243ms step_avg:47.11ms
step:1386/2090 train_time:65330ms step_avg:47.14ms
step:1387/2090 train_time:65417ms step_avg:47.16ms
step:1388/2090 train_time:65504ms step_avg:47.19ms
step:1389/2090 train_time:65591ms step_avg:47.22ms
step:1390/2090 train_time:65678ms step_avg:47.25ms
step:1391/2090 train_time:65766ms step_avg:47.28ms
step:1392/2090 train_time:65853ms step_avg:47.31ms
step:1393/2090 train_time:65941ms step_avg:47.34ms
step:1394/2090 train_time:66028ms step_avg:47.37ms
step:1395/2090 train_time:66117ms step_avg:47.40ms
step:1396/2090 train_time:66204ms step_avg:47.42ms
step:1397/2090 train_time:66293ms step_avg:47.45ms
step:1398/2090 train_time:66379ms step_avg:47.48ms
step:1399/2090 train_time:66467ms step_avg:47.51ms
step:1400/2090 train_time:66553ms step_avg:47.54ms
step:1401/2090 train_time:66642ms step_avg:47.57ms
step:1402/2090 train_time:66728ms step_avg:47.60ms
step:1403/2090 train_time:66816ms step_avg:47.62ms
step:1404/2090 train_time:66903ms step_avg:47.65ms
step:1405/2090 train_time:66992ms step_avg:47.68ms
step:1406/2090 train_time:67079ms step_avg:47.71ms
step:1407/2090 train_time:67167ms step_avg:47.74ms
step:1408/2090 train_time:67255ms step_avg:47.77ms
step:1409/2090 train_time:67343ms step_avg:47.79ms
step:1410/2090 train_time:67430ms step_avg:47.82ms
step:1411/2090 train_time:67517ms step_avg:47.85ms
step:1412/2090 train_time:67605ms step_avg:47.88ms
step:1413/2090 train_time:67692ms step_avg:47.91ms
step:1414/2090 train_time:67779ms step_avg:47.93ms
step:1415/2090 train_time:67867ms step_avg:47.96ms
step:1416/2090 train_time:67955ms step_avg:47.99ms
step:1417/2090 train_time:68044ms step_avg:48.02ms
step:1418/2090 train_time:68130ms step_avg:48.05ms
step:1419/2090 train_time:68219ms step_avg:48.08ms
step:1420/2090 train_time:68306ms step_avg:48.10ms
step:1421/2090 train_time:68394ms step_avg:48.13ms
step:1422/2090 train_time:68482ms step_avg:48.16ms
step:1423/2090 train_time:68569ms step_avg:48.19ms
step:1424/2090 train_time:68656ms step_avg:48.21ms
step:1425/2090 train_time:68744ms step_avg:48.24ms
step:1426/2090 train_time:68830ms step_avg:48.27ms
step:1427/2090 train_time:68918ms step_avg:48.30ms
step:1428/2090 train_time:69006ms step_avg:48.32ms
step:1429/2090 train_time:69093ms step_avg:48.35ms
step:1430/2090 train_time:69181ms step_avg:48.38ms
step:1431/2090 train_time:69269ms step_avg:48.41ms
step:1432/2090 train_time:69357ms step_avg:48.43ms
step:1433/2090 train_time:69446ms step_avg:48.46ms
step:1434/2090 train_time:69533ms step_avg:48.49ms
step:1435/2090 train_time:69622ms step_avg:48.52ms
step:1436/2090 train_time:69708ms step_avg:48.54ms
step:1437/2090 train_time:69796ms step_avg:48.57ms
step:1438/2090 train_time:69883ms step_avg:48.60ms
step:1439/2090 train_time:69971ms step_avg:48.62ms
step:1440/2090 train_time:70058ms step_avg:48.65ms
step:1441/2090 train_time:70146ms step_avg:48.68ms
step:1442/2090 train_time:70233ms step_avg:48.71ms
step:1443/2090 train_time:70321ms step_avg:48.73ms
step:1444/2090 train_time:70408ms step_avg:48.76ms
step:1445/2090 train_time:70496ms step_avg:48.79ms
step:1446/2090 train_time:70583ms step_avg:48.81ms
step:1447/2090 train_time:70671ms step_avg:48.84ms
step:1448/2090 train_time:70757ms step_avg:48.87ms
step:1449/2090 train_time:70845ms step_avg:48.89ms
step:1450/2090 train_time:70932ms step_avg:48.92ms
step:1451/2090 train_time:71019ms step_avg:48.95ms
step:1452/2090 train_time:71106ms step_avg:48.97ms
step:1453/2090 train_time:71195ms step_avg:49.00ms
step:1454/2090 train_time:71283ms step_avg:49.03ms
step:1455/2090 train_time:71370ms step_avg:49.05ms
step:1456/2090 train_time:71458ms step_avg:49.08ms
step:1457/2090 train_time:71547ms step_avg:49.11ms
step:1458/2090 train_time:71633ms step_avg:49.13ms
step:1459/2090 train_time:71722ms step_avg:49.16ms
step:1460/2090 train_time:71808ms step_avg:49.18ms
step:1461/2090 train_time:71897ms step_avg:49.21ms
step:1462/2090 train_time:71983ms step_avg:49.24ms
step:1463/2090 train_time:72071ms step_avg:49.26ms
step:1464/2090 train_time:72158ms step_avg:49.29ms
step:1465/2090 train_time:72247ms step_avg:49.32ms
step:1466/2090 train_time:72334ms step_avg:49.34ms
step:1467/2090 train_time:72421ms step_avg:49.37ms
step:1468/2090 train_time:72508ms step_avg:49.39ms
step:1469/2090 train_time:72595ms step_avg:49.42ms
step:1470/2090 train_time:72683ms step_avg:49.44ms
step:1471/2090 train_time:72771ms step_avg:49.47ms
step:1472/2090 train_time:72859ms step_avg:49.50ms
step:1473/2090 train_time:72947ms step_avg:49.52ms
step:1474/2090 train_time:73033ms step_avg:49.55ms
step:1475/2090 train_time:73121ms step_avg:49.57ms
step:1476/2090 train_time:73207ms step_avg:49.60ms
step:1477/2090 train_time:73296ms step_avg:49.62ms
step:1478/2090 train_time:73383ms step_avg:49.65ms
step:1479/2090 train_time:73471ms step_avg:49.68ms
step:1480/2090 train_time:73558ms step_avg:49.70ms
step:1481/2090 train_time:73646ms step_avg:49.73ms
step:1482/2090 train_time:73735ms step_avg:49.75ms
step:1483/2090 train_time:73822ms step_avg:49.78ms
step:1484/2090 train_time:73908ms step_avg:49.80ms
step:1485/2090 train_time:73996ms step_avg:49.83ms
step:1486/2090 train_time:74083ms step_avg:49.85ms
step:1487/2090 train_time:74171ms step_avg:49.88ms
step:1488/2090 train_time:74258ms step_avg:49.90ms
step:1489/2090 train_time:74346ms step_avg:49.93ms
step:1490/2090 train_time:74433ms step_avg:49.96ms
step:1491/2090 train_time:74523ms step_avg:49.98ms
step:1492/2090 train_time:74610ms step_avg:50.01ms
step:1493/2090 train_time:74698ms step_avg:50.03ms
step:1494/2090 train_time:74785ms step_avg:50.06ms
step:1495/2090 train_time:74872ms step_avg:50.08ms
step:1496/2090 train_time:74959ms step_avg:50.11ms
step:1497/2090 train_time:75047ms step_avg:50.13ms
step:1498/2090 train_time:75134ms step_avg:50.16ms
step:1499/2090 train_time:75222ms step_avg:50.18ms
step:1500/2090 train_time:75308ms step_avg:50.21ms
step:1500/2090 val_loss:3.4767 train_time:75397ms step_avg:50.26ms
step:1501/2090 train_time:75418ms step_avg:50.24ms
step:1502/2090 train_time:75487ms step_avg:50.26ms
step:1503/2090 train_time:75580ms step_avg:50.29ms
step:1504/2090 train_time:75666ms step_avg:50.31ms
step:1505/2090 train_time:75755ms step_avg:50.34ms
step:1506/2090 train_time:75841ms step_avg:50.36ms
step:1507/2090 train_time:75928ms step_avg:50.38ms
step:1508/2090 train_time:76013ms step_avg:50.41ms
step:1509/2090 train_time:76100ms step_avg:50.43ms
step:1510/2090 train_time:76187ms step_avg:50.45ms
step:1511/2090 train_time:76274ms step_avg:50.48ms
step:1512/2090 train_time:76363ms step_avg:50.50ms
step:1513/2090 train_time:76454ms step_avg:50.53ms
step:1514/2090 train_time:76542ms step_avg:50.56ms
step:1515/2090 train_time:76631ms step_avg:50.58ms
step:1516/2090 train_time:76719ms step_avg:50.61ms
step:1517/2090 train_time:76807ms step_avg:50.63ms
step:1518/2090 train_time:76894ms step_avg:50.65ms
step:1519/2090 train_time:76981ms step_avg:50.68ms
step:1520/2090 train_time:77067ms step_avg:50.70ms
step:1521/2090 train_time:77154ms step_avg:50.73ms
step:1522/2090 train_time:77240ms step_avg:50.75ms
step:1523/2090 train_time:77328ms step_avg:50.77ms
step:1524/2090 train_time:77416ms step_avg:50.80ms
step:1525/2090 train_time:77506ms step_avg:50.82ms
step:1526/2090 train_time:77595ms step_avg:50.85ms
step:1527/2090 train_time:77684ms step_avg:50.87ms
step:1528/2090 train_time:77770ms step_avg:50.90ms
step:1529/2090 train_time:77859ms step_avg:50.92ms
step:1530/2090 train_time:77944ms step_avg:50.94ms
step:1531/2090 train_time:78031ms step_avg:50.97ms
step:1532/2090 train_time:78118ms step_avg:50.99ms
step:1533/2090 train_time:78206ms step_avg:51.01ms
step:1534/2090 train_time:78293ms step_avg:51.04ms
step:1535/2090 train_time:78382ms step_avg:51.06ms
step:1536/2090 train_time:78470ms step_avg:51.09ms
step:1537/2090 train_time:78558ms step_avg:51.11ms
step:1538/2090 train_time:78645ms step_avg:51.13ms
step:1539/2090 train_time:78734ms step_avg:51.16ms
step:1540/2090 train_time:78821ms step_avg:51.18ms
step:1541/2090 train_time:78909ms step_avg:51.21ms
step:1542/2090 train_time:78995ms step_avg:51.23ms
step:1543/2090 train_time:79082ms step_avg:51.25ms
step:1544/2090 train_time:79169ms step_avg:51.28ms
step:1545/2090 train_time:79256ms step_avg:51.30ms
step:1546/2090 train_time:79343ms step_avg:51.32ms
step:1547/2090 train_time:79431ms step_avg:51.35ms
step:1548/2090 train_time:79519ms step_avg:51.37ms
step:1549/2090 train_time:79607ms step_avg:51.39ms
step:1550/2090 train_time:79694ms step_avg:51.42ms
step:1551/2090 train_time:79783ms step_avg:51.44ms
step:1552/2090 train_time:79869ms step_avg:51.46ms
step:1553/2090 train_time:79957ms step_avg:51.49ms
step:1554/2090 train_time:80044ms step_avg:51.51ms
step:1555/2090 train_time:80132ms step_avg:51.53ms
step:1556/2090 train_time:80220ms step_avg:51.55ms
step:1557/2090 train_time:80308ms step_avg:51.58ms
step:1558/2090 train_time:80395ms step_avg:51.60ms
step:1559/2090 train_time:80483ms step_avg:51.62ms
step:1560/2090 train_time:80571ms step_avg:51.65ms
step:1561/2090 train_time:80660ms step_avg:51.67ms
step:1562/2090 train_time:80747ms step_avg:51.69ms
step:1563/2090 train_time:80835ms step_avg:51.72ms
step:1564/2090 train_time:80922ms step_avg:51.74ms
step:1565/2090 train_time:81010ms step_avg:51.76ms
step:1566/2090 train_time:81096ms step_avg:51.79ms
step:1567/2090 train_time:81184ms step_avg:51.81ms
step:1568/2090 train_time:81271ms step_avg:51.83ms
step:1569/2090 train_time:81359ms step_avg:51.85ms
step:1570/2090 train_time:81447ms step_avg:51.88ms
step:1571/2090 train_time:81535ms step_avg:51.90ms
step:1572/2090 train_time:81623ms step_avg:51.92ms
step:1573/2090 train_time:81710ms step_avg:51.95ms
step:1574/2090 train_time:81797ms step_avg:51.97ms
step:1575/2090 train_time:81885ms step_avg:51.99ms
step:1576/2090 train_time:81972ms step_avg:52.01ms
step:1577/2090 train_time:82060ms step_avg:52.04ms
step:1578/2090 train_time:82146ms step_avg:52.06ms
step:1579/2090 train_time:82234ms step_avg:52.08ms
step:1580/2090 train_time:82321ms step_avg:52.10ms
step:1581/2090 train_time:82410ms step_avg:52.12ms
step:1582/2090 train_time:82497ms step_avg:52.15ms
step:1583/2090 train_time:82586ms step_avg:52.17ms
step:1584/2090 train_time:82673ms step_avg:52.19ms
step:1585/2090 train_time:82760ms step_avg:52.21ms
step:1586/2090 train_time:82848ms step_avg:52.24ms
step:1587/2090 train_time:82936ms step_avg:52.26ms
step:1588/2090 train_time:83023ms step_avg:52.28ms
step:1589/2090 train_time:83111ms step_avg:52.30ms
step:1590/2090 train_time:83198ms step_avg:52.33ms
step:1591/2090 train_time:83286ms step_avg:52.35ms
step:1592/2090 train_time:83373ms step_avg:52.37ms
step:1593/2090 train_time:83461ms step_avg:52.39ms
step:1594/2090 train_time:83548ms step_avg:52.41ms
step:1595/2090 train_time:83636ms step_avg:52.44ms
step:1596/2090 train_time:83723ms step_avg:52.46ms
step:1597/2090 train_time:83812ms step_avg:52.48ms
step:1598/2090 train_time:83899ms step_avg:52.50ms
step:1599/2090 train_time:83987ms step_avg:52.52ms
step:1600/2090 train_time:84074ms step_avg:52.55ms
step:1601/2090 train_time:84163ms step_avg:52.57ms
step:1602/2090 train_time:84249ms step_avg:52.59ms
step:1603/2090 train_time:84337ms step_avg:52.61ms
step:1604/2090 train_time:84424ms step_avg:52.63ms
step:1605/2090 train_time:84513ms step_avg:52.66ms
step:1606/2090 train_time:84601ms step_avg:52.68ms
step:1607/2090 train_time:84689ms step_avg:52.70ms
step:1608/2090 train_time:84776ms step_avg:52.72ms
step:1609/2090 train_time:84865ms step_avg:52.74ms
step:1610/2090 train_time:84951ms step_avg:52.76ms
step:1611/2090 train_time:85038ms step_avg:52.79ms
step:1612/2090 train_time:85125ms step_avg:52.81ms
step:1613/2090 train_time:85213ms step_avg:52.83ms
step:1614/2090 train_time:85300ms step_avg:52.85ms
step:1615/2090 train_time:85388ms step_avg:52.87ms
step:1616/2090 train_time:85475ms step_avg:52.89ms
step:1617/2090 train_time:85564ms step_avg:52.92ms
step:1618/2090 train_time:85652ms step_avg:52.94ms
step:1619/2090 train_time:85741ms step_avg:52.96ms
step:1620/2090 train_time:85827ms step_avg:52.98ms
step:1621/2090 train_time:85915ms step_avg:53.00ms
step:1622/2090 train_time:86002ms step_avg:53.02ms
step:1623/2090 train_time:86090ms step_avg:53.04ms
step:1624/2090 train_time:86177ms step_avg:53.06ms
step:1625/2090 train_time:86265ms step_avg:53.09ms
step:1626/2090 train_time:86352ms step_avg:53.11ms
step:1627/2090 train_time:86440ms step_avg:53.13ms
step:1628/2090 train_time:86527ms step_avg:53.15ms
step:1629/2090 train_time:86615ms step_avg:53.17ms
step:1630/2090 train_time:86702ms step_avg:53.19ms
step:1631/2090 train_time:86790ms step_avg:53.21ms
step:1632/2090 train_time:86877ms step_avg:53.23ms
step:1633/2090 train_time:86966ms step_avg:53.26ms
step:1634/2090 train_time:87052ms step_avg:53.28ms
step:1635/2090 train_time:87140ms step_avg:53.30ms
step:1636/2090 train_time:87227ms step_avg:53.32ms
step:1637/2090 train_time:87315ms step_avg:53.34ms
step:1638/2090 train_time:87402ms step_avg:53.36ms
step:1639/2090 train_time:87490ms step_avg:53.38ms
step:1640/2090 train_time:87577ms step_avg:53.40ms
step:1641/2090 train_time:87665ms step_avg:53.42ms
step:1642/2090 train_time:87752ms step_avg:53.44ms
step:1643/2090 train_time:87840ms step_avg:53.46ms
step:1644/2090 train_time:87927ms step_avg:53.48ms
step:1645/2090 train_time:88015ms step_avg:53.50ms
step:1646/2090 train_time:88101ms step_avg:53.52ms
step:1647/2090 train_time:88189ms step_avg:53.54ms
step:1648/2090 train_time:88277ms step_avg:53.57ms
step:1649/2090 train_time:88365ms step_avg:53.59ms
step:1650/2090 train_time:88452ms step_avg:53.61ms
step:1651/2090 train_time:88540ms step_avg:53.63ms
step:1652/2090 train_time:88626ms step_avg:53.65ms
step:1653/2090 train_time:88715ms step_avg:53.67ms
step:1654/2090 train_time:88802ms step_avg:53.69ms
step:1655/2090 train_time:88889ms step_avg:53.71ms
step:1656/2090 train_time:88976ms step_avg:53.73ms
step:1657/2090 train_time:89065ms step_avg:53.75ms
step:1658/2090 train_time:89151ms step_avg:53.77ms
step:1659/2090 train_time:89239ms step_avg:53.79ms
step:1660/2090 train_time:89325ms step_avg:53.81ms
step:1661/2090 train_time:89413ms step_avg:53.83ms
step:1662/2090 train_time:89500ms step_avg:53.85ms
step:1663/2090 train_time:89589ms step_avg:53.87ms
step:1664/2090 train_time:89676ms step_avg:53.89ms
step:1665/2090 train_time:89765ms step_avg:53.91ms
step:1666/2090 train_time:89852ms step_avg:53.93ms
step:1667/2090 train_time:89941ms step_avg:53.95ms
step:1668/2090 train_time:90027ms step_avg:53.97ms
step:1669/2090 train_time:90115ms step_avg:53.99ms
step:1670/2090 train_time:90203ms step_avg:54.01ms
step:1671/2090 train_time:90290ms step_avg:54.03ms
step:1672/2090 train_time:90377ms step_avg:54.05ms
step:1673/2090 train_time:90465ms step_avg:54.07ms
step:1674/2090 train_time:90552ms step_avg:54.09ms
step:1675/2090 train_time:90641ms step_avg:54.11ms
step:1676/2090 train_time:90727ms step_avg:54.13ms
step:1677/2090 train_time:90815ms step_avg:54.15ms
step:1678/2090 train_time:90903ms step_avg:54.17ms
step:1679/2090 train_time:90991ms step_avg:54.19ms
step:1680/2090 train_time:91078ms step_avg:54.21ms
step:1681/2090 train_time:91165ms step_avg:54.23ms
step:1682/2090 train_time:91252ms step_avg:54.25ms
step:1683/2090 train_time:91340ms step_avg:54.27ms
step:1684/2090 train_time:91427ms step_avg:54.29ms
step:1685/2090 train_time:91515ms step_avg:54.31ms
step:1686/2090 train_time:91602ms step_avg:54.33ms
step:1687/2090 train_time:91690ms step_avg:54.35ms
step:1688/2090 train_time:91777ms step_avg:54.37ms
step:1689/2090 train_time:91865ms step_avg:54.39ms
step:1690/2090 train_time:91952ms step_avg:54.41ms
step:1691/2090 train_time:92040ms step_avg:54.43ms
step:1692/2090 train_time:92126ms step_avg:54.45ms
step:1693/2090 train_time:92215ms step_avg:54.47ms
step:1694/2090 train_time:92302ms step_avg:54.49ms
step:1695/2090 train_time:92390ms step_avg:54.51ms
step:1696/2090 train_time:92476ms step_avg:54.53ms
step:1697/2090 train_time:92564ms step_avg:54.55ms
step:1698/2090 train_time:92650ms step_avg:54.56ms
step:1699/2090 train_time:92740ms step_avg:54.59ms
step:1700/2090 train_time:92827ms step_avg:54.60ms
step:1701/2090 train_time:92915ms step_avg:54.62ms
step:1702/2090 train_time:93002ms step_avg:54.64ms
step:1703/2090 train_time:93090ms step_avg:54.66ms
step:1704/2090 train_time:93178ms step_avg:54.68ms
step:1705/2090 train_time:93265ms step_avg:54.70ms
step:1706/2090 train_time:93352ms step_avg:54.72ms
step:1707/2090 train_time:93440ms step_avg:54.74ms
step:1708/2090 train_time:93526ms step_avg:54.76ms
step:1709/2090 train_time:93614ms step_avg:54.78ms
step:1710/2090 train_time:93701ms step_avg:54.80ms
step:1711/2090 train_time:93789ms step_avg:54.82ms
step:1712/2090 train_time:93876ms step_avg:54.83ms
step:1713/2090 train_time:93965ms step_avg:54.85ms
step:1714/2090 train_time:94052ms step_avg:54.87ms
step:1715/2090 train_time:94141ms step_avg:54.89ms
step:1716/2090 train_time:94227ms step_avg:54.91ms
step:1717/2090 train_time:94315ms step_avg:54.93ms
step:1718/2090 train_time:94402ms step_avg:54.95ms
step:1719/2090 train_time:94491ms step_avg:54.97ms
step:1720/2090 train_time:94578ms step_avg:54.99ms
step:1721/2090 train_time:94666ms step_avg:55.01ms
step:1722/2090 train_time:94753ms step_avg:55.02ms
step:1723/2090 train_time:94842ms step_avg:55.04ms
step:1724/2090 train_time:94929ms step_avg:55.06ms
step:1725/2090 train_time:95017ms step_avg:55.08ms
step:1726/2090 train_time:95104ms step_avg:55.10ms
step:1727/2090 train_time:95192ms step_avg:55.12ms
step:1728/2090 train_time:95279ms step_avg:55.14ms
step:1729/2090 train_time:95368ms step_avg:55.16ms
step:1730/2090 train_time:95456ms step_avg:55.18ms
step:1731/2090 train_time:95544ms step_avg:55.20ms
step:1732/2090 train_time:95631ms step_avg:55.21ms
step:1733/2090 train_time:95719ms step_avg:55.23ms
step:1734/2090 train_time:95806ms step_avg:55.25ms
step:1735/2090 train_time:95895ms step_avg:55.27ms
step:1736/2090 train_time:95981ms step_avg:55.29ms
step:1737/2090 train_time:96070ms step_avg:55.31ms
step:1738/2090 train_time:96157ms step_avg:55.33ms
step:1739/2090 train_time:96244ms step_avg:55.34ms
step:1740/2090 train_time:96331ms step_avg:55.36ms
step:1741/2090 train_time:96419ms step_avg:55.38ms
step:1742/2090 train_time:96506ms step_avg:55.40ms
step:1743/2090 train_time:96595ms step_avg:55.42ms
step:1744/2090 train_time:96682ms step_avg:55.44ms
step:1745/2090 train_time:96769ms step_avg:55.46ms
step:1746/2090 train_time:96857ms step_avg:55.47ms
step:1747/2090 train_time:96946ms step_avg:55.49ms
step:1748/2090 train_time:97034ms step_avg:55.51ms
step:1749/2090 train_time:97123ms step_avg:55.53ms
step:1750/2090 train_time:97209ms step_avg:55.55ms
step:1750/2090 val_loss:3.3761 train_time:97299ms step_avg:55.60ms
step:1751/2090 train_time:97319ms step_avg:55.58ms
step:1752/2090 train_time:97389ms step_avg:55.59ms
step:1753/2090 train_time:97480ms step_avg:55.61ms
step:1754/2090 train_time:97567ms step_avg:55.63ms
step:1755/2090 train_time:97655ms step_avg:55.64ms
step:1756/2090 train_time:97741ms step_avg:55.66ms
step:1757/2090 train_time:97827ms step_avg:55.68ms
step:1758/2090 train_time:97914ms step_avg:55.70ms
step:1759/2090 train_time:98000ms step_avg:55.71ms
step:1760/2090 train_time:98086ms step_avg:55.73ms
step:1761/2090 train_time:98173ms step_avg:55.75ms
step:1762/2090 train_time:98261ms step_avg:55.77ms
step:1763/2090 train_time:98353ms step_avg:55.79ms
step:1764/2090 train_time:98442ms step_avg:55.81ms
step:1765/2090 train_time:98531ms step_avg:55.82ms
step:1766/2090 train_time:98618ms step_avg:55.84ms
step:1767/2090 train_time:98706ms step_avg:55.86ms
step:1768/2090 train_time:98793ms step_avg:55.88ms
step:1769/2090 train_time:98880ms step_avg:55.90ms
step:1770/2090 train_time:98966ms step_avg:55.91ms
step:1771/2090 train_time:99053ms step_avg:55.93ms
step:1772/2090 train_time:99139ms step_avg:55.95ms
step:1773/2090 train_time:99227ms step_avg:55.97ms
step:1774/2090 train_time:99314ms step_avg:55.98ms
step:1775/2090 train_time:99404ms step_avg:56.00ms
step:1776/2090 train_time:99493ms step_avg:56.02ms
step:1777/2090 train_time:99581ms step_avg:56.04ms
step:1778/2090 train_time:99669ms step_avg:56.06ms
step:1779/2090 train_time:99758ms step_avg:56.08ms
step:1780/2090 train_time:99843ms step_avg:56.09ms
step:1781/2090 train_time:99931ms step_avg:56.11ms
step:1782/2090 train_time:100017ms step_avg:56.13ms
step:1783/2090 train_time:100104ms step_avg:56.14ms
step:1784/2090 train_time:100190ms step_avg:56.16ms
step:1785/2090 train_time:100278ms step_avg:56.18ms
step:1786/2090 train_time:100366ms step_avg:56.20ms
step:1787/2090 train_time:100455ms step_avg:56.21ms
step:1788/2090 train_time:100544ms step_avg:56.23ms
step:1789/2090 train_time:100631ms step_avg:56.25ms
step:1790/2090 train_time:100719ms step_avg:56.27ms
step:1791/2090 train_time:100806ms step_avg:56.29ms
step:1792/2090 train_time:100893ms step_avg:56.30ms
step:1793/2090 train_time:100982ms step_avg:56.32ms
step:1794/2090 train_time:101068ms step_avg:56.34ms
step:1795/2090 train_time:101157ms step_avg:56.35ms
step:1796/2090 train_time:101243ms step_avg:56.37ms
step:1797/2090 train_time:101331ms step_avg:56.39ms
step:1798/2090 train_time:101419ms step_avg:56.41ms
step:1799/2090 train_time:101507ms step_avg:56.42ms
step:1800/2090 train_time:101595ms step_avg:56.44ms
step:1801/2090 train_time:101684ms step_avg:56.46ms
step:1802/2090 train_time:101771ms step_avg:56.48ms
step:1803/2090 train_time:101859ms step_avg:56.49ms
step:1804/2090 train_time:101945ms step_avg:56.51ms
step:1805/2090 train_time:102033ms step_avg:56.53ms
step:1806/2090 train_time:102119ms step_avg:56.54ms
step:1807/2090 train_time:102207ms step_avg:56.56ms
step:1808/2090 train_time:102293ms step_avg:56.58ms
step:1809/2090 train_time:102382ms step_avg:56.60ms
step:1810/2090 train_time:102469ms step_avg:56.61ms
step:1811/2090 train_time:102558ms step_avg:56.63ms
step:1812/2090 train_time:102646ms step_avg:56.65ms
step:1813/2090 train_time:102733ms step_avg:56.66ms
step:1814/2090 train_time:102820ms step_avg:56.68ms
step:1815/2090 train_time:102908ms step_avg:56.70ms
step:1816/2090 train_time:102996ms step_avg:56.72ms
step:1817/2090 train_time:103084ms step_avg:56.73ms
step:1818/2090 train_time:103170ms step_avg:56.75ms
step:1819/2090 train_time:103258ms step_avg:56.77ms
step:1820/2090 train_time:103345ms step_avg:56.78ms
step:1821/2090 train_time:103435ms step_avg:56.80ms
step:1822/2090 train_time:103522ms step_avg:56.82ms
step:1823/2090 train_time:103610ms step_avg:56.84ms
step:1824/2090 train_time:103699ms step_avg:56.85ms
step:1825/2090 train_time:103787ms step_avg:56.87ms
step:1826/2090 train_time:103874ms step_avg:56.89ms
step:1827/2090 train_time:103962ms step_avg:56.90ms
step:1828/2090 train_time:104049ms step_avg:56.92ms
step:1829/2090 train_time:104136ms step_avg:56.94ms
step:1830/2090 train_time:104223ms step_avg:56.95ms
step:1831/2090 train_time:104310ms step_avg:56.97ms
step:1832/2090 train_time:104397ms step_avg:56.99ms
step:1833/2090 train_time:104486ms step_avg:57.00ms
step:1834/2090 train_time:104573ms step_avg:57.02ms
step:1835/2090 train_time:104662ms step_avg:57.04ms
step:1836/2090 train_time:104749ms step_avg:57.05ms
step:1837/2090 train_time:104836ms step_avg:57.07ms
step:1838/2090 train_time:104924ms step_avg:57.09ms
step:1839/2090 train_time:105012ms step_avg:57.10ms
step:1840/2090 train_time:105099ms step_avg:57.12ms
step:1841/2090 train_time:105187ms step_avg:57.14ms
step:1842/2090 train_time:105273ms step_avg:57.15ms
step:1843/2090 train_time:105362ms step_avg:57.17ms
step:1844/2090 train_time:105449ms step_avg:57.19ms
step:1845/2090 train_time:105537ms step_avg:57.20ms
step:1846/2090 train_time:105624ms step_avg:57.22ms
step:1847/2090 train_time:105712ms step_avg:57.23ms
step:1848/2090 train_time:105799ms step_avg:57.25ms
step:1849/2090 train_time:105887ms step_avg:57.27ms
step:1850/2090 train_time:105974ms step_avg:57.28ms
step:1851/2090 train_time:106063ms step_avg:57.30ms
step:1852/2090 train_time:106149ms step_avg:57.32ms
step:1853/2090 train_time:106237ms step_avg:57.33ms
step:1854/2090 train_time:106324ms step_avg:57.35ms
step:1855/2090 train_time:106412ms step_avg:57.36ms
step:1856/2090 train_time:106498ms step_avg:57.38ms
step:1857/2090 train_time:106587ms step_avg:57.40ms
step:1858/2090 train_time:106674ms step_avg:57.41ms
step:1859/2090 train_time:106762ms step_avg:57.43ms
step:1860/2090 train_time:106849ms step_avg:57.45ms
step:1861/2090 train_time:106937ms step_avg:57.46ms
step:1862/2090 train_time:107023ms step_avg:57.48ms
step:1863/2090 train_time:107111ms step_avg:57.49ms
step:1864/2090 train_time:107198ms step_avg:57.51ms
step:1865/2090 train_time:107286ms step_avg:57.53ms
step:1866/2090 train_time:107374ms step_avg:57.54ms
step:1867/2090 train_time:107463ms step_avg:57.56ms
step:1868/2090 train_time:107549ms step_avg:57.57ms
step:1869/2090 train_time:107638ms step_avg:57.59ms
step:1870/2090 train_time:107725ms step_avg:57.61ms
step:1871/2090 train_time:107813ms step_avg:57.62ms
step:1872/2090 train_time:107899ms step_avg:57.64ms
step:1873/2090 train_time:107987ms step_avg:57.65ms
step:1874/2090 train_time:108074ms step_avg:57.67ms
step:1875/2090 train_time:108162ms step_avg:57.69ms
step:1876/2090 train_time:108249ms step_avg:57.70ms
step:1877/2090 train_time:108336ms step_avg:57.72ms
step:1878/2090 train_time:108424ms step_avg:57.73ms
step:1879/2090 train_time:108511ms step_avg:57.75ms
step:1880/2090 train_time:108598ms step_avg:57.77ms
step:1881/2090 train_time:108687ms step_avg:57.78ms
step:1882/2090 train_time:108774ms step_avg:57.80ms
step:1883/2090 train_time:108862ms step_avg:57.81ms
step:1884/2090 train_time:108950ms step_avg:57.83ms
step:1885/2090 train_time:109038ms step_avg:57.84ms
step:1886/2090 train_time:109124ms step_avg:57.86ms
step:1887/2090 train_time:109213ms step_avg:57.88ms
step:1888/2090 train_time:109300ms step_avg:57.89ms
step:1889/2090 train_time:109387ms step_avg:57.91ms
step:1890/2090 train_time:109474ms step_avg:57.92ms
step:1891/2090 train_time:109563ms step_avg:57.94ms
step:1892/2090 train_time:109649ms step_avg:57.95ms
step:1893/2090 train_time:109737ms step_avg:57.97ms
step:1894/2090 train_time:109824ms step_avg:57.99ms
step:1895/2090 train_time:109913ms step_avg:58.00ms
step:1896/2090 train_time:110000ms step_avg:58.02ms
step:1897/2090 train_time:110087ms step_avg:58.03ms
step:1898/2090 train_time:110175ms step_avg:58.05ms
step:1899/2090 train_time:110263ms step_avg:58.06ms
step:1900/2090 train_time:110349ms step_avg:58.08ms
step:1901/2090 train_time:110437ms step_avg:58.09ms
step:1902/2090 train_time:110524ms step_avg:58.11ms
step:1903/2090 train_time:110612ms step_avg:58.12ms
step:1904/2090 train_time:110699ms step_avg:58.14ms
step:1905/2090 train_time:110787ms step_avg:58.16ms
step:1906/2090 train_time:110874ms step_avg:58.17ms
step:1907/2090 train_time:110962ms step_avg:58.19ms
step:1908/2090 train_time:111048ms step_avg:58.20ms
step:1909/2090 train_time:111136ms step_avg:58.22ms
step:1910/2090 train_time:111224ms step_avg:58.23ms
step:1911/2090 train_time:111311ms step_avg:58.25ms
step:1912/2090 train_time:111398ms step_avg:58.26ms
step:1913/2090 train_time:111486ms step_avg:58.28ms
step:1914/2090 train_time:111573ms step_avg:58.29ms
step:1915/2090 train_time:111661ms step_avg:58.31ms
step:1916/2090 train_time:111748ms step_avg:58.32ms
step:1917/2090 train_time:111837ms step_avg:58.34ms
step:1918/2090 train_time:111923ms step_avg:58.35ms
step:1919/2090 train_time:112011ms step_avg:58.37ms
step:1920/2090 train_time:112098ms step_avg:58.38ms
step:1921/2090 train_time:112186ms step_avg:58.40ms
step:1922/2090 train_time:112274ms step_avg:58.41ms
step:1923/2090 train_time:112362ms step_avg:58.43ms
step:1924/2090 train_time:112449ms step_avg:58.45ms
step:1925/2090 train_time:112537ms step_avg:58.46ms
step:1926/2090 train_time:112624ms step_avg:58.48ms
step:1927/2090 train_time:112713ms step_avg:58.49ms
step:1928/2090 train_time:112800ms step_avg:58.51ms
step:1929/2090 train_time:112888ms step_avg:58.52ms
step:1930/2090 train_time:112976ms step_avg:58.54ms
step:1931/2090 train_time:113065ms step_avg:58.55ms
step:1932/2090 train_time:113151ms step_avg:58.57ms
step:1933/2090 train_time:113239ms step_avg:58.58ms
step:1934/2090 train_time:113325ms step_avg:58.60ms
step:1935/2090 train_time:113413ms step_avg:58.61ms
step:1936/2090 train_time:113501ms step_avg:58.63ms
step:1937/2090 train_time:113590ms step_avg:58.64ms
step:1938/2090 train_time:113677ms step_avg:58.66ms
step:1939/2090 train_time:113764ms step_avg:58.67ms
step:1940/2090 train_time:113852ms step_avg:58.69ms
step:1941/2090 train_time:113940ms step_avg:58.70ms
step:1942/2090 train_time:114027ms step_avg:58.72ms
step:1943/2090 train_time:114115ms step_avg:58.73ms
step:1944/2090 train_time:114202ms step_avg:58.75ms
step:1945/2090 train_time:114289ms step_avg:58.76ms
step:1946/2090 train_time:114376ms step_avg:58.77ms
step:1947/2090 train_time:114464ms step_avg:58.79ms
step:1948/2090 train_time:114551ms step_avg:58.80ms
step:1949/2090 train_time:114640ms step_avg:58.82ms
step:1950/2090 train_time:114728ms step_avg:58.83ms
step:1951/2090 train_time:114816ms step_avg:58.85ms
step:1952/2090 train_time:114904ms step_avg:58.86ms
step:1953/2090 train_time:114991ms step_avg:58.88ms
step:1954/2090 train_time:115078ms step_avg:58.89ms
step:1955/2090 train_time:115166ms step_avg:58.91ms
step:1956/2090 train_time:115254ms step_avg:58.92ms
step:1957/2090 train_time:115341ms step_avg:58.94ms
step:1958/2090 train_time:115429ms step_avg:58.95ms
step:1959/2090 train_time:115516ms step_avg:58.97ms
step:1960/2090 train_time:115603ms step_avg:58.98ms
step:1961/2090 train_time:115691ms step_avg:59.00ms
step:1962/2090 train_time:115778ms step_avg:59.01ms
step:1963/2090 train_time:115866ms step_avg:59.02ms
step:1964/2090 train_time:115954ms step_avg:59.04ms
step:1965/2090 train_time:116042ms step_avg:59.05ms
step:1966/2090 train_time:116129ms step_avg:59.07ms
step:1967/2090 train_time:116217ms step_avg:59.08ms
step:1968/2090 train_time:116305ms step_avg:59.10ms
step:1969/2090 train_time:116392ms step_avg:59.11ms
step:1970/2090 train_time:116479ms step_avg:59.13ms
step:1971/2090 train_time:116568ms step_avg:59.14ms
step:1972/2090 train_time:116654ms step_avg:59.16ms
step:1973/2090 train_time:116743ms step_avg:59.17ms
step:1974/2090 train_time:116829ms step_avg:59.18ms
step:1975/2090 train_time:116917ms step_avg:59.20ms
step:1976/2090 train_time:117005ms step_avg:59.21ms
step:1977/2090 train_time:117092ms step_avg:59.23ms
step:1978/2090 train_time:117179ms step_avg:59.24ms
step:1979/2090 train_time:117267ms step_avg:59.26ms
step:1980/2090 train_time:117354ms step_avg:59.27ms
step:1981/2090 train_time:117443ms step_avg:59.28ms
step:1982/2090 train_time:117530ms step_avg:59.30ms
step:1983/2090 train_time:117618ms step_avg:59.31ms
step:1984/2090 train_time:117705ms step_avg:59.33ms
step:1985/2090 train_time:117792ms step_avg:59.34ms
step:1986/2090 train_time:117879ms step_avg:59.36ms
step:1987/2090 train_time:117966ms step_avg:59.37ms
step:1988/2090 train_time:118054ms step_avg:59.38ms
step:1989/2090 train_time:118142ms step_avg:59.40ms
step:1990/2090 train_time:118230ms step_avg:59.41ms
step:1991/2090 train_time:118319ms step_avg:59.43ms
step:1992/2090 train_time:118406ms step_avg:59.44ms
step:1993/2090 train_time:118494ms step_avg:59.46ms
step:1994/2090 train_time:118581ms step_avg:59.47ms
step:1995/2090 train_time:118668ms step_avg:59.48ms
step:1996/2090 train_time:118756ms step_avg:59.50ms
step:1997/2090 train_time:118844ms step_avg:59.51ms
step:1998/2090 train_time:118931ms step_avg:59.53ms
step:1999/2090 train_time:119020ms step_avg:59.54ms
step:2000/2090 train_time:119107ms step_avg:59.55ms
step:2000/2090 val_loss:3.2992 train_time:119196ms step_avg:59.60ms
step:2001/2090 train_time:119216ms step_avg:59.58ms
step:2002/2090 train_time:119285ms step_avg:59.58ms
step:2003/2090 train_time:119377ms step_avg:59.60ms
step:2004/2090 train_time:119464ms step_avg:59.61ms
step:2005/2090 train_time:119553ms step_avg:59.63ms
step:2006/2090 train_time:119639ms step_avg:59.64ms
step:2007/2090 train_time:119725ms step_avg:59.65ms
step:2008/2090 train_time:119811ms step_avg:59.67ms
step:2009/2090 train_time:119897ms step_avg:59.68ms
step:2010/2090 train_time:119983ms step_avg:59.69ms
step:2011/2090 train_time:120070ms step_avg:59.71ms
step:2012/2090 train_time:120158ms step_avg:59.72ms
step:2013/2090 train_time:120249ms step_avg:59.74ms
step:2014/2090 train_time:120339ms step_avg:59.75ms
step:2015/2090 train_time:120427ms step_avg:59.77ms
step:2016/2090 train_time:120515ms step_avg:59.78ms
step:2017/2090 train_time:120603ms step_avg:59.79ms
step:2018/2090 train_time:120689ms step_avg:59.81ms
step:2019/2090 train_time:120776ms step_avg:59.82ms
step:2020/2090 train_time:120862ms step_avg:59.83ms
step:2021/2090 train_time:120949ms step_avg:59.85ms
step:2022/2090 train_time:121035ms step_avg:59.86ms
step:2023/2090 train_time:121123ms step_avg:59.87ms
step:2024/2090 train_time:121210ms step_avg:59.89ms
step:2025/2090 train_time:121300ms step_avg:59.90ms
step:2026/2090 train_time:121389ms step_avg:59.92ms
step:2027/2090 train_time:121477ms step_avg:59.93ms
step:2028/2090 train_time:121566ms step_avg:59.94ms
step:2029/2090 train_time:121653ms step_avg:59.96ms
step:2030/2090 train_time:121738ms step_avg:59.97ms
step:2031/2090 train_time:121826ms step_avg:59.98ms
step:2032/2090 train_time:121912ms step_avg:60.00ms
step:2033/2090 train_time:122000ms step_avg:60.01ms
step:2034/2090 train_time:122087ms step_avg:60.02ms
step:2035/2090 train_time:122176ms step_avg:60.04ms
step:2036/2090 train_time:122264ms step_avg:60.05ms
step:2037/2090 train_time:122352ms step_avg:60.06ms
step:2038/2090 train_time:122440ms step_avg:60.08ms
step:2039/2090 train_time:122529ms step_avg:60.09ms
step:2040/2090 train_time:122616ms step_avg:60.11ms
step:2041/2090 train_time:122704ms step_avg:60.12ms
step:2042/2090 train_time:122789ms step_avg:60.13ms
step:2043/2090 train_time:122877ms step_avg:60.15ms
step:2044/2090 train_time:122963ms step_avg:60.16ms
step:2045/2090 train_time:123051ms step_avg:60.17ms
step:2046/2090 train_time:123138ms step_avg:60.18ms
step:2047/2090 train_time:123226ms step_avg:60.20ms
step:2048/2090 train_time:123314ms step_avg:60.21ms
step:2049/2090 train_time:123403ms step_avg:60.23ms
step:2050/2090 train_time:123491ms step_avg:60.24ms
step:2051/2090 train_time:123579ms step_avg:60.25ms
step:2052/2090 train_time:123666ms step_avg:60.27ms
step:2053/2090 train_time:123754ms step_avg:60.28ms
step:2054/2090 train_time:123841ms step_avg:60.29ms
step:2055/2090 train_time:123928ms step_avg:60.31ms
step:2056/2090 train_time:124015ms step_avg:60.32ms
step:2057/2090 train_time:124104ms step_avg:60.33ms
step:2058/2090 train_time:124191ms step_avg:60.35ms
step:2059/2090 train_time:124279ms step_avg:60.36ms
step:2060/2090 train_time:124367ms step_avg:60.37ms
step:2061/2090 train_time:124456ms step_avg:60.39ms
step:2062/2090 train_time:124544ms step_avg:60.40ms
step:2063/2090 train_time:124632ms step_avg:60.41ms
step:2064/2090 train_time:124719ms step_avg:60.43ms
step:2065/2090 train_time:124807ms step_avg:60.44ms
step:2066/2090 train_time:124893ms step_avg:60.45ms
step:2067/2090 train_time:124982ms step_avg:60.47ms
step:2068/2090 train_time:125069ms step_avg:60.48ms
step:2069/2090 train_time:125156ms step_avg:60.49ms
step:2070/2090 train_time:125244ms step_avg:60.50ms
step:2071/2090 train_time:125332ms step_avg:60.52ms
step:2072/2090 train_time:125420ms step_avg:60.53ms
step:2073/2090 train_time:125509ms step_avg:60.54ms
step:2074/2090 train_time:125597ms step_avg:60.56ms
step:2075/2090 train_time:125685ms step_avg:60.57ms
step:2076/2090 train_time:125772ms step_avg:60.58ms
step:2077/2090 train_time:125860ms step_avg:60.60ms
step:2078/2090 train_time:125947ms step_avg:60.61ms
step:2079/2090 train_time:126036ms step_avg:60.62ms
step:2080/2090 train_time:126123ms step_avg:60.64ms
step:2081/2090 train_time:126211ms step_avg:60.65ms
step:2082/2090 train_time:126299ms step_avg:60.66ms
step:2083/2090 train_time:126387ms step_avg:60.68ms
step:2084/2090 train_time:126474ms step_avg:60.69ms
step:2085/2090 train_time:126564ms step_avg:60.70ms
step:2086/2090 train_time:126651ms step_avg:60.71ms
step:2087/2090 train_time:126739ms step_avg:60.73ms
step:2088/2090 train_time:126827ms step_avg:60.74ms
step:2089/2090 train_time:126914ms step_avg:60.75ms
step:2090/2090 train_time:127002ms step_avg:60.77ms
step:2090/2090 val_loss:3.2779 train_time:127092ms step_avg:60.81ms
peak memory allocated: 29862 MiB reserved: 44036 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # weight decay
                if wd != 0:
                    mask = (update * p_slice) >= 0
                    # lr as weight decay  schedule
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)

                    update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)
                
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2050  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 18 12:07:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   25C    P0            114W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   23C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   25C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2090 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2090 train_time:76ms step_avg:76.00ms
step:2/2090 train_time:100ms step_avg:49.76ms
step:3/2090 train_time:124ms step_avg:41.49ms
step:4/2090 train_time:157ms step_avg:39.15ms
step:5/2090 train_time:189ms step_avg:37.85ms
step:6/2090 train_time:285ms step_avg:47.53ms
step:7/2090 train_time:304ms step_avg:43.41ms
step:8/2090 train_time:330ms step_avg:41.27ms
step:9/2090 train_time:363ms step_avg:40.33ms
step:10/2090 train_time:396ms step_avg:39.57ms
step:11/2090 train_time:429ms step_avg:39.01ms
step:12/2090 train_time:462ms step_avg:38.51ms
step:13/2090 train_time:496ms step_avg:38.13ms
step:14/2090 train_time:529ms step_avg:37.77ms
step:15/2090 train_time:562ms step_avg:37.47ms
step:16/2090 train_time:595ms step_avg:37.18ms
step:17/2090 train_time:628ms step_avg:36.95ms
step:18/2090 train_time:661ms step_avg:36.72ms
step:19/2090 train_time:695ms step_avg:36.57ms
step:20/2090 train_time:728ms step_avg:36.38ms
step:21/2090 train_time:761ms step_avg:36.24ms
step:22/2090 train_time:794ms step_avg:36.08ms
step:23/2090 train_time:827ms step_avg:35.97ms
step:24/2090 train_time:860ms step_avg:35.84ms
step:25/2090 train_time:894ms step_avg:35.75ms
step:26/2090 train_time:926ms step_avg:35.63ms
step:27/2090 train_time:960ms step_avg:35.56ms
step:28/2090 train_time:993ms step_avg:35.46ms
step:29/2090 train_time:1026ms step_avg:35.39ms
step:30/2090 train_time:1059ms step_avg:35.30ms
step:31/2090 train_time:1093ms step_avg:35.25ms
step:32/2090 train_time:1126ms step_avg:35.17ms
step:33/2090 train_time:1159ms step_avg:35.13ms
step:34/2090 train_time:1193ms step_avg:35.08ms
step:35/2090 train_time:1227ms step_avg:35.07ms
step:36/2090 train_time:1261ms step_avg:35.02ms
step:37/2090 train_time:1295ms step_avg:34.99ms
step:38/2090 train_time:1328ms step_avg:34.94ms
step:39/2090 train_time:1362ms step_avg:34.91ms
step:40/2090 train_time:1395ms step_avg:34.86ms
step:41/2090 train_time:1428ms step_avg:34.83ms
step:42/2090 train_time:1461ms step_avg:34.79ms
step:43/2090 train_time:1495ms step_avg:34.76ms
step:44/2090 train_time:1528ms step_avg:34.72ms
step:45/2090 train_time:1561ms step_avg:34.69ms
step:46/2090 train_time:1595ms step_avg:34.66ms
step:47/2090 train_time:1628ms step_avg:34.64ms
step:48/2090 train_time:1661ms step_avg:34.61ms
step:49/2090 train_time:1695ms step_avg:34.58ms
step:50/2090 train_time:1727ms step_avg:34.55ms
step:51/2090 train_time:1761ms step_avg:34.52ms
step:52/2090 train_time:1793ms step_avg:34.49ms
step:53/2090 train_time:1827ms step_avg:34.47ms
step:54/2090 train_time:1860ms step_avg:34.44ms
step:55/2090 train_time:1894ms step_avg:34.43ms
step:56/2090 train_time:1927ms step_avg:34.41ms
step:57/2090 train_time:1960ms step_avg:34.39ms
step:58/2090 train_time:1993ms step_avg:34.36ms
step:59/2090 train_time:2027ms step_avg:34.35ms
step:60/2090 train_time:2059ms step_avg:34.32ms
step:61/2090 train_time:2093ms step_avg:34.31ms
step:62/2090 train_time:2126ms step_avg:34.29ms
step:63/2090 train_time:2160ms step_avg:34.28ms
step:64/2090 train_time:2192ms step_avg:34.26ms
step:65/2090 train_time:2226ms step_avg:34.25ms
step:66/2090 train_time:2259ms step_avg:34.23ms
step:67/2090 train_time:2293ms step_avg:34.22ms
step:68/2090 train_time:2326ms step_avg:34.20ms
step:69/2090 train_time:2360ms step_avg:34.20ms
step:70/2090 train_time:2393ms step_avg:34.18ms
step:71/2090 train_time:2426ms step_avg:34.17ms
step:72/2090 train_time:2459ms step_avg:34.16ms
step:73/2090 train_time:2493ms step_avg:34.14ms
step:74/2090 train_time:2526ms step_avg:34.13ms
step:75/2090 train_time:2559ms step_avg:34.12ms
step:76/2090 train_time:2592ms step_avg:34.11ms
step:77/2090 train_time:2626ms step_avg:34.10ms
step:78/2090 train_time:2658ms step_avg:34.08ms
step:79/2090 train_time:2692ms step_avg:34.08ms
step:80/2090 train_time:2725ms step_avg:34.06ms
step:81/2090 train_time:2758ms step_avg:34.06ms
step:82/2090 train_time:2791ms step_avg:34.04ms
step:83/2090 train_time:2825ms step_avg:34.03ms
step:84/2090 train_time:2857ms step_avg:34.02ms
step:85/2090 train_time:2891ms step_avg:34.01ms
step:86/2090 train_time:2923ms step_avg:33.99ms
step:87/2090 train_time:2957ms step_avg:33.99ms
step:88/2090 train_time:2990ms step_avg:33.97ms
step:89/2090 train_time:3023ms step_avg:33.97ms
step:90/2090 train_time:3056ms step_avg:33.95ms
step:91/2090 train_time:3089ms step_avg:33.95ms
step:92/2090 train_time:3122ms step_avg:33.94ms
step:93/2090 train_time:3156ms step_avg:33.93ms
step:94/2090 train_time:3188ms step_avg:33.92ms
step:95/2090 train_time:3222ms step_avg:33.91ms
step:96/2090 train_time:3254ms step_avg:33.90ms
step:97/2090 train_time:3288ms step_avg:33.90ms
step:98/2090 train_time:3321ms step_avg:33.88ms
step:99/2090 train_time:3355ms step_avg:33.89ms
step:100/2090 train_time:3388ms step_avg:33.88ms
step:101/2090 train_time:3421ms step_avg:33.87ms
step:102/2090 train_time:3454ms step_avg:33.86ms
step:103/2090 train_time:3487ms step_avg:33.86ms
step:104/2090 train_time:3520ms step_avg:33.85ms
step:105/2090 train_time:3553ms step_avg:33.84ms
step:106/2090 train_time:3586ms step_avg:33.83ms
step:107/2090 train_time:3620ms step_avg:33.83ms
step:108/2090 train_time:3653ms step_avg:33.82ms
step:109/2090 train_time:3686ms step_avg:33.81ms
step:110/2090 train_time:3719ms step_avg:33.81ms
step:111/2090 train_time:3752ms step_avg:33.80ms
step:112/2090 train_time:3785ms step_avg:33.79ms
step:113/2090 train_time:3818ms step_avg:33.79ms
step:114/2090 train_time:3851ms step_avg:33.78ms
step:115/2090 train_time:3884ms step_avg:33.78ms
step:116/2090 train_time:3917ms step_avg:33.77ms
step:117/2090 train_time:3951ms step_avg:33.77ms
step:118/2090 train_time:3984ms step_avg:33.76ms
step:119/2090 train_time:4017ms step_avg:33.75ms
step:120/2090 train_time:4050ms step_avg:33.75ms
step:121/2090 train_time:4083ms step_avg:33.74ms
step:122/2090 train_time:4116ms step_avg:33.74ms
step:123/2090 train_time:4149ms step_avg:33.73ms
step:124/2090 train_time:4182ms step_avg:33.72ms
step:125/2090 train_time:4215ms step_avg:33.72ms
step:126/2090 train_time:4248ms step_avg:33.72ms
step:127/2090 train_time:4281ms step_avg:33.71ms
step:128/2090 train_time:4314ms step_avg:33.70ms
step:129/2090 train_time:4347ms step_avg:33.70ms
step:130/2090 train_time:4380ms step_avg:33.69ms
step:131/2090 train_time:4414ms step_avg:33.69ms
step:132/2090 train_time:4447ms step_avg:33.69ms
step:133/2090 train_time:4480ms step_avg:33.69ms
step:134/2090 train_time:4513ms step_avg:33.68ms
step:135/2090 train_time:4546ms step_avg:33.68ms
step:136/2090 train_time:4579ms step_avg:33.67ms
step:137/2090 train_time:4613ms step_avg:33.67ms
step:138/2090 train_time:4646ms step_avg:33.66ms
step:139/2090 train_time:4679ms step_avg:33.66ms
step:140/2090 train_time:4712ms step_avg:33.65ms
step:141/2090 train_time:4745ms step_avg:33.65ms
step:142/2090 train_time:4777ms step_avg:33.64ms
step:143/2090 train_time:4811ms step_avg:33.64ms
step:144/2090 train_time:4844ms step_avg:33.64ms
step:145/2090 train_time:4877ms step_avg:33.64ms
step:146/2090 train_time:4910ms step_avg:33.63ms
step:147/2090 train_time:4943ms step_avg:33.63ms
step:148/2090 train_time:4976ms step_avg:33.62ms
step:149/2090 train_time:5009ms step_avg:33.62ms
step:150/2090 train_time:5042ms step_avg:33.61ms
step:151/2090 train_time:5075ms step_avg:33.61ms
step:152/2090 train_time:5108ms step_avg:33.60ms
step:153/2090 train_time:5141ms step_avg:33.60ms
step:154/2090 train_time:5174ms step_avg:33.60ms
step:155/2090 train_time:5207ms step_avg:33.60ms
step:156/2090 train_time:5240ms step_avg:33.59ms
step:157/2090 train_time:5274ms step_avg:33.59ms
step:158/2090 train_time:5306ms step_avg:33.58ms
step:159/2090 train_time:5340ms step_avg:33.58ms
step:160/2090 train_time:5373ms step_avg:33.58ms
step:161/2090 train_time:5406ms step_avg:33.58ms
step:162/2090 train_time:5439ms step_avg:33.57ms
step:163/2090 train_time:5473ms step_avg:33.57ms
step:164/2090 train_time:5505ms step_avg:33.57ms
step:165/2090 train_time:5538ms step_avg:33.57ms
step:166/2090 train_time:5571ms step_avg:33.56ms
step:167/2090 train_time:5604ms step_avg:33.56ms
step:168/2090 train_time:5637ms step_avg:33.55ms
step:169/2090 train_time:5670ms step_avg:33.55ms
step:170/2090 train_time:5703ms step_avg:33.55ms
step:171/2090 train_time:5736ms step_avg:33.54ms
step:172/2090 train_time:5769ms step_avg:33.54ms
step:173/2090 train_time:5802ms step_avg:33.54ms
step:174/2090 train_time:5835ms step_avg:33.53ms
step:175/2090 train_time:5868ms step_avg:33.53ms
step:176/2090 train_time:5901ms step_avg:33.53ms
step:177/2090 train_time:5934ms step_avg:33.53ms
step:178/2090 train_time:5967ms step_avg:33.52ms
step:179/2090 train_time:6000ms step_avg:33.52ms
step:180/2090 train_time:6033ms step_avg:33.52ms
step:181/2090 train_time:6066ms step_avg:33.51ms
step:182/2090 train_time:6099ms step_avg:33.51ms
step:183/2090 train_time:6132ms step_avg:33.51ms
step:184/2090 train_time:6165ms step_avg:33.51ms
step:185/2090 train_time:6198ms step_avg:33.50ms
step:186/2090 train_time:6231ms step_avg:33.50ms
step:187/2090 train_time:6264ms step_avg:33.50ms
step:188/2090 train_time:6297ms step_avg:33.49ms
step:189/2090 train_time:6330ms step_avg:33.49ms
step:190/2090 train_time:6363ms step_avg:33.49ms
step:191/2090 train_time:6396ms step_avg:33.49ms
step:192/2090 train_time:6429ms step_avg:33.48ms
step:193/2090 train_time:6462ms step_avg:33.48ms
step:194/2090 train_time:6495ms step_avg:33.48ms
step:195/2090 train_time:6528ms step_avg:33.48ms
step:196/2090 train_time:6561ms step_avg:33.47ms
step:197/2090 train_time:6594ms step_avg:33.47ms
step:198/2090 train_time:6627ms step_avg:33.47ms
step:199/2090 train_time:6660ms step_avg:33.47ms
step:200/2090 train_time:6693ms step_avg:33.47ms
step:201/2090 train_time:6726ms step_avg:33.46ms
step:202/2090 train_time:6759ms step_avg:33.46ms
step:203/2090 train_time:6792ms step_avg:33.46ms
step:204/2090 train_time:6825ms step_avg:33.46ms
step:205/2090 train_time:6858ms step_avg:33.46ms
step:206/2090 train_time:6891ms step_avg:33.45ms
step:207/2090 train_time:6925ms step_avg:33.45ms
step:208/2090 train_time:6957ms step_avg:33.45ms
step:209/2090 train_time:6991ms step_avg:33.45ms
step:210/2090 train_time:7023ms step_avg:33.44ms
step:211/2090 train_time:7057ms step_avg:33.44ms
step:212/2090 train_time:7089ms step_avg:33.44ms
step:213/2090 train_time:7122ms step_avg:33.44ms
step:214/2090 train_time:7155ms step_avg:33.43ms
step:215/2090 train_time:7188ms step_avg:33.43ms
step:216/2090 train_time:7221ms step_avg:33.43ms
step:217/2090 train_time:7254ms step_avg:33.43ms
step:218/2090 train_time:7287ms step_avg:33.43ms
step:219/2090 train_time:7320ms step_avg:33.42ms
step:220/2090 train_time:7353ms step_avg:33.42ms
step:221/2090 train_time:7386ms step_avg:33.42ms
step:222/2090 train_time:7419ms step_avg:33.42ms
step:223/2090 train_time:7452ms step_avg:33.42ms
step:224/2090 train_time:7485ms step_avg:33.42ms
step:225/2090 train_time:7519ms step_avg:33.42ms
step:226/2090 train_time:7552ms step_avg:33.41ms
step:227/2090 train_time:7585ms step_avg:33.41ms
step:228/2090 train_time:7617ms step_avg:33.41ms
step:229/2090 train_time:7651ms step_avg:33.41ms
step:230/2090 train_time:7684ms step_avg:33.41ms
step:231/2090 train_time:7717ms step_avg:33.41ms
step:232/2090 train_time:7750ms step_avg:33.40ms
step:233/2090 train_time:7783ms step_avg:33.40ms
step:234/2090 train_time:7816ms step_avg:33.40ms
step:235/2090 train_time:7849ms step_avg:33.40ms
step:236/2090 train_time:7882ms step_avg:33.40ms
step:237/2090 train_time:7915ms step_avg:33.40ms
step:238/2090 train_time:7948ms step_avg:33.39ms
step:239/2090 train_time:7981ms step_avg:33.39ms
step:240/2090 train_time:8014ms step_avg:33.39ms
step:241/2090 train_time:8047ms step_avg:33.39ms
step:242/2090 train_time:8080ms step_avg:33.39ms
step:243/2090 train_time:8113ms step_avg:33.39ms
step:244/2090 train_time:8145ms step_avg:33.38ms
step:245/2090 train_time:8179ms step_avg:33.38ms
step:246/2090 train_time:8211ms step_avg:33.38ms
step:247/2090 train_time:8245ms step_avg:33.38ms
step:248/2090 train_time:8277ms step_avg:33.38ms
step:249/2090 train_time:8310ms step_avg:33.37ms
step:250/2090 train_time:8343ms step_avg:33.37ms
step:250/2090 val_loss:4.2759 train_time:8379ms step_avg:33.51ms
step:251/2090 train_time:8399ms step_avg:33.46ms
step:252/2090 train_time:8419ms step_avg:33.41ms
step:253/2090 train_time:8445ms step_avg:33.38ms
step:254/2090 train_time:8479ms step_avg:33.38ms
step:255/2090 train_time:8515ms step_avg:33.39ms
step:256/2090 train_time:8549ms step_avg:33.39ms
step:257/2090 train_time:8583ms step_avg:33.40ms
step:258/2090 train_time:8616ms step_avg:33.39ms
step:259/2090 train_time:8649ms step_avg:33.39ms
step:260/2090 train_time:8682ms step_avg:33.39ms
step:261/2090 train_time:8715ms step_avg:33.39ms
step:262/2090 train_time:8748ms step_avg:33.39ms
step:263/2090 train_time:8780ms step_avg:33.39ms
step:264/2090 train_time:8813ms step_avg:33.38ms
step:265/2090 train_time:8846ms step_avg:33.38ms
step:266/2090 train_time:8879ms step_avg:33.38ms
step:267/2090 train_time:8912ms step_avg:33.38ms
step:268/2090 train_time:8944ms step_avg:33.37ms
step:269/2090 train_time:8977ms step_avg:33.37ms
step:270/2090 train_time:9010ms step_avg:33.37ms
step:271/2090 train_time:9043ms step_avg:33.37ms
step:272/2090 train_time:9076ms step_avg:33.37ms
step:273/2090 train_time:9109ms step_avg:33.37ms
step:274/2090 train_time:9142ms step_avg:33.36ms
step:275/2090 train_time:9175ms step_avg:33.36ms
step:276/2090 train_time:9207ms step_avg:33.36ms
step:277/2090 train_time:9240ms step_avg:33.36ms
step:278/2090 train_time:9273ms step_avg:33.36ms
step:279/2090 train_time:9306ms step_avg:33.35ms
step:280/2090 train_time:9339ms step_avg:33.35ms
step:281/2090 train_time:9371ms step_avg:33.35ms
step:282/2090 train_time:9404ms step_avg:33.35ms
step:283/2090 train_time:9438ms step_avg:33.35ms
step:284/2090 train_time:9470ms step_avg:33.35ms
step:285/2090 train_time:9504ms step_avg:33.35ms
step:286/2090 train_time:9537ms step_avg:33.35ms
step:287/2090 train_time:9571ms step_avg:33.35ms
step:288/2090 train_time:9603ms step_avg:33.34ms
step:289/2090 train_time:9637ms step_avg:33.35ms
step:290/2090 train_time:9670ms step_avg:33.34ms
step:291/2090 train_time:9703ms step_avg:33.34ms
step:292/2090 train_time:9735ms step_avg:33.34ms
step:293/2090 train_time:9769ms step_avg:33.34ms
step:294/2090 train_time:9802ms step_avg:33.34ms
step:295/2090 train_time:9835ms step_avg:33.34ms
step:296/2090 train_time:9867ms step_avg:33.34ms
step:297/2090 train_time:9901ms step_avg:33.34ms
step:298/2090 train_time:9933ms step_avg:33.33ms
step:299/2090 train_time:9966ms step_avg:33.33ms
step:300/2090 train_time:9999ms step_avg:33.33ms
step:301/2090 train_time:10032ms step_avg:33.33ms
step:302/2090 train_time:10065ms step_avg:33.33ms
step:303/2090 train_time:10098ms step_avg:33.33ms
step:304/2090 train_time:10131ms step_avg:33.33ms
step:305/2090 train_time:10164ms step_avg:33.32ms
step:306/2090 train_time:10197ms step_avg:33.32ms
step:307/2090 train_time:10230ms step_avg:33.32ms
step:308/2090 train_time:10262ms step_avg:33.32ms
step:309/2090 train_time:10295ms step_avg:33.32ms
step:310/2090 train_time:10328ms step_avg:33.32ms
step:311/2090 train_time:10361ms step_avg:33.31ms
step:312/2090 train_time:10393ms step_avg:33.31ms
step:313/2090 train_time:10426ms step_avg:33.31ms
step:314/2090 train_time:10459ms step_avg:33.31ms
step:315/2090 train_time:10492ms step_avg:33.31ms
step:316/2090 train_time:10525ms step_avg:33.31ms
step:317/2090 train_time:10559ms step_avg:33.31ms
step:318/2090 train_time:10591ms step_avg:33.31ms
step:319/2090 train_time:10625ms step_avg:33.31ms
step:320/2090 train_time:10658ms step_avg:33.31ms
step:321/2090 train_time:10691ms step_avg:33.30ms
step:322/2090 train_time:10724ms step_avg:33.30ms
step:323/2090 train_time:10757ms step_avg:33.30ms
step:324/2090 train_time:10789ms step_avg:33.30ms
step:325/2090 train_time:10823ms step_avg:33.30ms
step:326/2090 train_time:10855ms step_avg:33.30ms
step:327/2090 train_time:10889ms step_avg:33.30ms
step:328/2090 train_time:10921ms step_avg:33.30ms
step:329/2090 train_time:10955ms step_avg:33.30ms
step:330/2090 train_time:10988ms step_avg:33.30ms
step:331/2090 train_time:11021ms step_avg:33.30ms
step:332/2090 train_time:11053ms step_avg:33.29ms
step:333/2090 train_time:11086ms step_avg:33.29ms
step:334/2090 train_time:11119ms step_avg:33.29ms
step:335/2090 train_time:11152ms step_avg:33.29ms
step:336/2090 train_time:11185ms step_avg:33.29ms
step:337/2090 train_time:11218ms step_avg:33.29ms
step:338/2090 train_time:11250ms step_avg:33.29ms
step:339/2090 train_time:11284ms step_avg:33.28ms
step:340/2090 train_time:11316ms step_avg:33.28ms
step:341/2090 train_time:11349ms step_avg:33.28ms
step:342/2090 train_time:11382ms step_avg:33.28ms
step:343/2090 train_time:11415ms step_avg:33.28ms
step:344/2090 train_time:11448ms step_avg:33.28ms
step:345/2090 train_time:11481ms step_avg:33.28ms
step:346/2090 train_time:11514ms step_avg:33.28ms
step:347/2090 train_time:11547ms step_avg:33.28ms
step:348/2090 train_time:11580ms step_avg:33.28ms
step:349/2090 train_time:11613ms step_avg:33.28ms
step:350/2090 train_time:11646ms step_avg:33.27ms
step:351/2090 train_time:11679ms step_avg:33.27ms
step:352/2090 train_time:11712ms step_avg:33.27ms
step:353/2090 train_time:11745ms step_avg:33.27ms
step:354/2090 train_time:11778ms step_avg:33.27ms
step:355/2090 train_time:11811ms step_avg:33.27ms
step:356/2090 train_time:11844ms step_avg:33.27ms
step:357/2090 train_time:11877ms step_avg:33.27ms
step:358/2090 train_time:11910ms step_avg:33.27ms
step:359/2090 train_time:11943ms step_avg:33.27ms
step:360/2090 train_time:11975ms step_avg:33.26ms
step:361/2090 train_time:12009ms step_avg:33.26ms
step:362/2090 train_time:12042ms step_avg:33.26ms
step:363/2090 train_time:12075ms step_avg:33.26ms
step:364/2090 train_time:12107ms step_avg:33.26ms
step:365/2090 train_time:12140ms step_avg:33.26ms
step:366/2090 train_time:12173ms step_avg:33.26ms
step:367/2090 train_time:12206ms step_avg:33.26ms
step:368/2090 train_time:12239ms step_avg:33.26ms
step:369/2090 train_time:12272ms step_avg:33.26ms
step:370/2090 train_time:12304ms step_avg:33.25ms
step:371/2090 train_time:12337ms step_avg:33.25ms
step:372/2090 train_time:12370ms step_avg:33.25ms
step:373/2090 train_time:12403ms step_avg:33.25ms
step:374/2090 train_time:12436ms step_avg:33.25ms
step:375/2090 train_time:12469ms step_avg:33.25ms
step:376/2090 train_time:12501ms step_avg:33.25ms
step:377/2090 train_time:12534ms step_avg:33.25ms
step:378/2090 train_time:12567ms step_avg:33.25ms
step:379/2090 train_time:12600ms step_avg:33.25ms
step:380/2090 train_time:12633ms step_avg:33.24ms
step:381/2090 train_time:12666ms step_avg:33.24ms
step:382/2090 train_time:12699ms step_avg:33.24ms
step:383/2090 train_time:12732ms step_avg:33.24ms
step:384/2090 train_time:12765ms step_avg:33.24ms
step:385/2090 train_time:12798ms step_avg:33.24ms
step:386/2090 train_time:12830ms step_avg:33.24ms
step:387/2090 train_time:12863ms step_avg:33.24ms
step:388/2090 train_time:12896ms step_avg:33.24ms
step:389/2090 train_time:12930ms step_avg:33.24ms
step:390/2090 train_time:12963ms step_avg:33.24ms
step:391/2090 train_time:12996ms step_avg:33.24ms
step:392/2090 train_time:13029ms step_avg:33.24ms
step:393/2090 train_time:13062ms step_avg:33.24ms
step:394/2090 train_time:13094ms step_avg:33.23ms
step:395/2090 train_time:13127ms step_avg:33.23ms
step:396/2090 train_time:13160ms step_avg:33.23ms
step:397/2090 train_time:13194ms step_avg:33.23ms
step:398/2090 train_time:13226ms step_avg:33.23ms
step:399/2090 train_time:13259ms step_avg:33.23ms
step:400/2090 train_time:13292ms step_avg:33.23ms
step:401/2090 train_time:13325ms step_avg:33.23ms
step:402/2090 train_time:13358ms step_avg:33.23ms
step:403/2090 train_time:13391ms step_avg:33.23ms
step:404/2090 train_time:13423ms step_avg:33.23ms
step:405/2090 train_time:13456ms step_avg:33.23ms
step:406/2090 train_time:13489ms step_avg:33.22ms
step:407/2090 train_time:13522ms step_avg:33.22ms
step:408/2090 train_time:13555ms step_avg:33.22ms
step:409/2090 train_time:13588ms step_avg:33.22ms
step:410/2090 train_time:13621ms step_avg:33.22ms
step:411/2090 train_time:13654ms step_avg:33.22ms
step:412/2090 train_time:13687ms step_avg:33.22ms
step:413/2090 train_time:13720ms step_avg:33.22ms
step:414/2090 train_time:13753ms step_avg:33.22ms
step:415/2090 train_time:13785ms step_avg:33.22ms
step:416/2090 train_time:13818ms step_avg:33.22ms
step:417/2090 train_time:13851ms step_avg:33.22ms
step:418/2090 train_time:13884ms step_avg:33.22ms
step:419/2090 train_time:13918ms step_avg:33.22ms
step:420/2090 train_time:13951ms step_avg:33.22ms
step:421/2090 train_time:13983ms step_avg:33.21ms
step:422/2090 train_time:14016ms step_avg:33.21ms
step:423/2090 train_time:14050ms step_avg:33.21ms
step:424/2090 train_time:14082ms step_avg:33.21ms
step:425/2090 train_time:14116ms step_avg:33.21ms
step:426/2090 train_time:14148ms step_avg:33.21ms
step:427/2090 train_time:14182ms step_avg:33.21ms
step:428/2090 train_time:14215ms step_avg:33.21ms
step:429/2090 train_time:14248ms step_avg:33.21ms
step:430/2090 train_time:14281ms step_avg:33.21ms
step:431/2090 train_time:14314ms step_avg:33.21ms
step:432/2090 train_time:14347ms step_avg:33.21ms
step:433/2090 train_time:14380ms step_avg:33.21ms
step:434/2090 train_time:14412ms step_avg:33.21ms
step:435/2090 train_time:14445ms step_avg:33.21ms
step:436/2090 train_time:14478ms step_avg:33.21ms
step:437/2090 train_time:14511ms step_avg:33.21ms
step:438/2090 train_time:14544ms step_avg:33.21ms
step:439/2090 train_time:14577ms step_avg:33.20ms
step:440/2090 train_time:14610ms step_avg:33.20ms
step:441/2090 train_time:14643ms step_avg:33.20ms
step:442/2090 train_time:14676ms step_avg:33.20ms
step:443/2090 train_time:14709ms step_avg:33.20ms
step:444/2090 train_time:14742ms step_avg:33.20ms
step:445/2090 train_time:14775ms step_avg:33.20ms
step:446/2090 train_time:14808ms step_avg:33.20ms
step:447/2090 train_time:14841ms step_avg:33.20ms
step:448/2090 train_time:14874ms step_avg:33.20ms
step:449/2090 train_time:14907ms step_avg:33.20ms
step:450/2090 train_time:14940ms step_avg:33.20ms
step:451/2090 train_time:14972ms step_avg:33.20ms
step:452/2090 train_time:15005ms step_avg:33.20ms
step:453/2090 train_time:15038ms step_avg:33.20ms
step:454/2090 train_time:15071ms step_avg:33.20ms
step:455/2090 train_time:15104ms step_avg:33.20ms
step:456/2090 train_time:15137ms step_avg:33.20ms
step:457/2090 train_time:15170ms step_avg:33.20ms
step:458/2090 train_time:15203ms step_avg:33.19ms
step:459/2090 train_time:15236ms step_avg:33.19ms
step:460/2090 train_time:15268ms step_avg:33.19ms
step:461/2090 train_time:15302ms step_avg:33.19ms
step:462/2090 train_time:15334ms step_avg:33.19ms
step:463/2090 train_time:15368ms step_avg:33.19ms
step:464/2090 train_time:15400ms step_avg:33.19ms
step:465/2090 train_time:15434ms step_avg:33.19ms
step:466/2090 train_time:15466ms step_avg:33.19ms
step:467/2090 train_time:15499ms step_avg:33.19ms
step:468/2090 train_time:15532ms step_avg:33.19ms
step:469/2090 train_time:15566ms step_avg:33.19ms
step:470/2090 train_time:15598ms step_avg:33.19ms
step:471/2090 train_time:15631ms step_avg:33.19ms
step:472/2090 train_time:15664ms step_avg:33.19ms
step:473/2090 train_time:15697ms step_avg:33.19ms
step:474/2090 train_time:15730ms step_avg:33.19ms
step:475/2090 train_time:15763ms step_avg:33.19ms
step:476/2090 train_time:15796ms step_avg:33.18ms
step:477/2090 train_time:15829ms step_avg:33.18ms
step:478/2090 train_time:15862ms step_avg:33.18ms
step:479/2090 train_time:15895ms step_avg:33.18ms
step:480/2090 train_time:15928ms step_avg:33.18ms
step:481/2090 train_time:15961ms step_avg:33.18ms
step:482/2090 train_time:15994ms step_avg:33.18ms
step:483/2090 train_time:16027ms step_avg:33.18ms
step:484/2090 train_time:16060ms step_avg:33.18ms
step:485/2090 train_time:16093ms step_avg:33.18ms
step:486/2090 train_time:16125ms step_avg:33.18ms
step:487/2090 train_time:16158ms step_avg:33.18ms
step:488/2090 train_time:16191ms step_avg:33.18ms
step:489/2090 train_time:16224ms step_avg:33.18ms
step:490/2090 train_time:16257ms step_avg:33.18ms
step:491/2090 train_time:16290ms step_avg:33.18ms
step:492/2090 train_time:16323ms step_avg:33.18ms
step:493/2090 train_time:16356ms step_avg:33.18ms
step:494/2090 train_time:16388ms step_avg:33.18ms
step:495/2090 train_time:16422ms step_avg:33.17ms
step:496/2090 train_time:16454ms step_avg:33.17ms
step:497/2090 train_time:16487ms step_avg:33.17ms
step:498/2090 train_time:16521ms step_avg:33.17ms
step:499/2090 train_time:16554ms step_avg:33.17ms
step:500/2090 train_time:16586ms step_avg:33.17ms
step:500/2090 val_loss:4.0097 train_time:16622ms step_avg:33.24ms
step:501/2090 train_time:16641ms step_avg:33.22ms
step:502/2090 train_time:16660ms step_avg:33.19ms
step:503/2090 train_time:16691ms step_avg:33.18ms
step:504/2090 train_time:16724ms step_avg:33.18ms
step:505/2090 train_time:16760ms step_avg:33.19ms
step:506/2090 train_time:16794ms step_avg:33.19ms
step:507/2090 train_time:16829ms step_avg:33.19ms
step:508/2090 train_time:16861ms step_avg:33.19ms
step:509/2090 train_time:16895ms step_avg:33.19ms
step:510/2090 train_time:16928ms step_avg:33.19ms
step:511/2090 train_time:16961ms step_avg:33.19ms
step:512/2090 train_time:16994ms step_avg:33.19ms
step:513/2090 train_time:17027ms step_avg:33.19ms
step:514/2090 train_time:17060ms step_avg:33.19ms
step:515/2090 train_time:17092ms step_avg:33.19ms
step:516/2090 train_time:17125ms step_avg:33.19ms
step:517/2090 train_time:17158ms step_avg:33.19ms
step:518/2090 train_time:17190ms step_avg:33.19ms
step:519/2090 train_time:17223ms step_avg:33.19ms
step:520/2090 train_time:17256ms step_avg:33.18ms
step:521/2090 train_time:17289ms step_avg:33.18ms
step:522/2090 train_time:17322ms step_avg:33.18ms
step:523/2090 train_time:17355ms step_avg:33.18ms
step:524/2090 train_time:17387ms step_avg:33.18ms
step:525/2090 train_time:17420ms step_avg:33.18ms
step:526/2090 train_time:17453ms step_avg:33.18ms
step:527/2090 train_time:17486ms step_avg:33.18ms
step:528/2090 train_time:17519ms step_avg:33.18ms
step:529/2090 train_time:17552ms step_avg:33.18ms
step:530/2090 train_time:17584ms step_avg:33.18ms
step:531/2090 train_time:17617ms step_avg:33.18ms
step:532/2090 train_time:17650ms step_avg:33.18ms
step:533/2090 train_time:17683ms step_avg:33.18ms
step:534/2090 train_time:17717ms step_avg:33.18ms
step:535/2090 train_time:17750ms step_avg:33.18ms
step:536/2090 train_time:17783ms step_avg:33.18ms
step:537/2090 train_time:17816ms step_avg:33.18ms
step:538/2090 train_time:17849ms step_avg:33.18ms
step:539/2090 train_time:17882ms step_avg:33.18ms
step:540/2090 train_time:17915ms step_avg:33.18ms
step:541/2090 train_time:17948ms step_avg:33.18ms
step:542/2090 train_time:17981ms step_avg:33.18ms
step:543/2090 train_time:18014ms step_avg:33.18ms
step:544/2090 train_time:18047ms step_avg:33.17ms
step:545/2090 train_time:18080ms step_avg:33.17ms
step:546/2090 train_time:18113ms step_avg:33.17ms
step:547/2090 train_time:18146ms step_avg:33.17ms
step:548/2090 train_time:18179ms step_avg:33.17ms
step:549/2090 train_time:18212ms step_avg:33.17ms
step:550/2090 train_time:18245ms step_avg:33.17ms
step:551/2090 train_time:18277ms step_avg:33.17ms
step:552/2090 train_time:18310ms step_avg:33.17ms
step:553/2090 train_time:18343ms step_avg:33.17ms
step:554/2090 train_time:18376ms step_avg:33.17ms
step:555/2090 train_time:18410ms step_avg:33.17ms
step:556/2090 train_time:18442ms step_avg:33.17ms
step:557/2090 train_time:18475ms step_avg:33.17ms
step:558/2090 train_time:18508ms step_avg:33.17ms
step:559/2090 train_time:18541ms step_avg:33.17ms
step:560/2090 train_time:18573ms step_avg:33.17ms
step:561/2090 train_time:18606ms step_avg:33.17ms
step:562/2090 train_time:18639ms step_avg:33.17ms
step:563/2090 train_time:18672ms step_avg:33.17ms
step:564/2090 train_time:18705ms step_avg:33.17ms
step:565/2090 train_time:18738ms step_avg:33.16ms
step:566/2090 train_time:18771ms step_avg:33.16ms
step:567/2090 train_time:18804ms step_avg:33.16ms
step:568/2090 train_time:18837ms step_avg:33.16ms
step:569/2090 train_time:18870ms step_avg:33.16ms
step:570/2090 train_time:18903ms step_avg:33.16ms
step:571/2090 train_time:18936ms step_avg:33.16ms
step:572/2090 train_time:18969ms step_avg:33.16ms
step:573/2090 train_time:19003ms step_avg:33.16ms
step:574/2090 train_time:19036ms step_avg:33.16ms
step:575/2090 train_time:19069ms step_avg:33.16ms
step:576/2090 train_time:19102ms step_avg:33.16ms
step:577/2090 train_time:19135ms step_avg:33.16ms
step:578/2090 train_time:19167ms step_avg:33.16ms
step:579/2090 train_time:19200ms step_avg:33.16ms
step:580/2090 train_time:19233ms step_avg:33.16ms
step:581/2090 train_time:19266ms step_avg:33.16ms
step:582/2090 train_time:19299ms step_avg:33.16ms
step:583/2090 train_time:19332ms step_avg:33.16ms
step:584/2090 train_time:19365ms step_avg:33.16ms
step:585/2090 train_time:19398ms step_avg:33.16ms
step:586/2090 train_time:19430ms step_avg:33.16ms
step:587/2090 train_time:19463ms step_avg:33.16ms
step:588/2090 train_time:19496ms step_avg:33.16ms
step:589/2090 train_time:19529ms step_avg:33.16ms
step:590/2090 train_time:19562ms step_avg:33.16ms
step:591/2090 train_time:19595ms step_avg:33.16ms
step:592/2090 train_time:19627ms step_avg:33.15ms
step:593/2090 train_time:19660ms step_avg:33.15ms
step:594/2090 train_time:19693ms step_avg:33.15ms
step:595/2090 train_time:19727ms step_avg:33.15ms
step:596/2090 train_time:19759ms step_avg:33.15ms
step:597/2090 train_time:19793ms step_avg:33.15ms
step:598/2090 train_time:19825ms step_avg:33.15ms
step:599/2090 train_time:19859ms step_avg:33.15ms
step:600/2090 train_time:19892ms step_avg:33.15ms
step:601/2090 train_time:19925ms step_avg:33.15ms
step:602/2090 train_time:19957ms step_avg:33.15ms
step:603/2090 train_time:19990ms step_avg:33.15ms
step:604/2090 train_time:20023ms step_avg:33.15ms
step:605/2090 train_time:20056ms step_avg:33.15ms
step:606/2090 train_time:20089ms step_avg:33.15ms
step:607/2090 train_time:20122ms step_avg:33.15ms
step:608/2090 train_time:20155ms step_avg:33.15ms
step:609/2090 train_time:20188ms step_avg:33.15ms
step:610/2090 train_time:20221ms step_avg:33.15ms
step:611/2090 train_time:20254ms step_avg:33.15ms
step:612/2090 train_time:20287ms step_avg:33.15ms
step:613/2090 train_time:20320ms step_avg:33.15ms
step:614/2090 train_time:20353ms step_avg:33.15ms
step:615/2090 train_time:20386ms step_avg:33.15ms
step:616/2090 train_time:20419ms step_avg:33.15ms
step:617/2090 train_time:20452ms step_avg:33.15ms
step:618/2090 train_time:20485ms step_avg:33.15ms
step:619/2090 train_time:20518ms step_avg:33.15ms
step:620/2090 train_time:20550ms step_avg:33.15ms
step:621/2090 train_time:20584ms step_avg:33.15ms
step:622/2090 train_time:20616ms step_avg:33.14ms
step:623/2090 train_time:20649ms step_avg:33.15ms
step:624/2090 train_time:20683ms step_avg:33.15ms
step:625/2090 train_time:20715ms step_avg:33.14ms
step:626/2090 train_time:20748ms step_avg:33.14ms
step:627/2090 train_time:20781ms step_avg:33.14ms
step:628/2090 train_time:20814ms step_avg:33.14ms
step:629/2090 train_time:20847ms step_avg:33.14ms
step:630/2090 train_time:20880ms step_avg:33.14ms
step:631/2090 train_time:20913ms step_avg:33.14ms
step:632/2090 train_time:20945ms step_avg:33.14ms
step:633/2090 train_time:20978ms step_avg:33.14ms
step:634/2090 train_time:21011ms step_avg:33.14ms
step:635/2090 train_time:21044ms step_avg:33.14ms
step:636/2090 train_time:21077ms step_avg:33.14ms
step:637/2090 train_time:21110ms step_avg:33.14ms
step:638/2090 train_time:21143ms step_avg:33.14ms
step:639/2090 train_time:21176ms step_avg:33.14ms
step:640/2090 train_time:21209ms step_avg:33.14ms
step:641/2090 train_time:21242ms step_avg:33.14ms
step:642/2090 train_time:21275ms step_avg:33.14ms
step:643/2090 train_time:21308ms step_avg:33.14ms
step:644/2090 train_time:21341ms step_avg:33.14ms
step:645/2090 train_time:21374ms step_avg:33.14ms
step:646/2090 train_time:21407ms step_avg:33.14ms
step:647/2090 train_time:21440ms step_avg:33.14ms
step:648/2090 train_time:21473ms step_avg:33.14ms
step:649/2090 train_time:21506ms step_avg:33.14ms
step:650/2090 train_time:21539ms step_avg:33.14ms
step:651/2090 train_time:21572ms step_avg:33.14ms
step:652/2090 train_time:21605ms step_avg:33.14ms
step:653/2090 train_time:21637ms step_avg:33.14ms
step:654/2090 train_time:21670ms step_avg:33.13ms
step:655/2090 train_time:21704ms step_avg:33.14ms
step:656/2090 train_time:21736ms step_avg:33.13ms
step:657/2090 train_time:21770ms step_avg:33.14ms
step:658/2090 train_time:21803ms step_avg:33.13ms
step:659/2090 train_time:21836ms step_avg:33.13ms
step:660/2090 train_time:21869ms step_avg:33.13ms
step:661/2090 train_time:21902ms step_avg:33.13ms
step:662/2090 train_time:21935ms step_avg:33.13ms
step:663/2090 train_time:21968ms step_avg:33.13ms
step:664/2090 train_time:22001ms step_avg:33.13ms
step:665/2090 train_time:22034ms step_avg:33.13ms
step:666/2090 train_time:22067ms step_avg:33.13ms
step:667/2090 train_time:22100ms step_avg:33.13ms
step:668/2090 train_time:22133ms step_avg:33.13ms
step:669/2090 train_time:22166ms step_avg:33.13ms
step:670/2090 train_time:22198ms step_avg:33.13ms
step:671/2090 train_time:22232ms step_avg:33.13ms
step:672/2090 train_time:22264ms step_avg:33.13ms
step:673/2090 train_time:22297ms step_avg:33.13ms
step:674/2090 train_time:22330ms step_avg:33.13ms
step:675/2090 train_time:22363ms step_avg:33.13ms
step:676/2090 train_time:22396ms step_avg:33.13ms
step:677/2090 train_time:22429ms step_avg:33.13ms
step:678/2090 train_time:22462ms step_avg:33.13ms
step:679/2090 train_time:22495ms step_avg:33.13ms
step:680/2090 train_time:22528ms step_avg:33.13ms
step:681/2090 train_time:22561ms step_avg:33.13ms
step:682/2090 train_time:22593ms step_avg:33.13ms
step:683/2090 train_time:22627ms step_avg:33.13ms
step:684/2090 train_time:22659ms step_avg:33.13ms
step:685/2090 train_time:22693ms step_avg:33.13ms
step:686/2090 train_time:22751ms step_avg:33.17ms
step:687/2090 train_time:22811ms step_avg:33.20ms
step:688/2090 train_time:22871ms step_avg:33.24ms
step:689/2090 train_time:22931ms step_avg:33.28ms
step:690/2090 train_time:22990ms step_avg:33.32ms
step:691/2090 train_time:23051ms step_avg:33.36ms
step:692/2090 train_time:23110ms step_avg:33.40ms
step:693/2090 train_time:23171ms step_avg:33.44ms
step:694/2090 train_time:23230ms step_avg:33.47ms
step:695/2090 train_time:23290ms step_avg:33.51ms
step:696/2090 train_time:23350ms step_avg:33.55ms
step:697/2090 train_time:23410ms step_avg:33.59ms
step:698/2090 train_time:23470ms step_avg:33.62ms
step:699/2090 train_time:23530ms step_avg:33.66ms
step:700/2090 train_time:23590ms step_avg:33.70ms
step:701/2090 train_time:23650ms step_avg:33.74ms
step:702/2090 train_time:23709ms step_avg:33.77ms
step:703/2090 train_time:23769ms step_avg:33.81ms
step:704/2090 train_time:23828ms step_avg:33.85ms
step:705/2090 train_time:23889ms step_avg:33.89ms
step:706/2090 train_time:23949ms step_avg:33.92ms
step:707/2090 train_time:24009ms step_avg:33.96ms
step:708/2090 train_time:24068ms step_avg:33.99ms
step:709/2090 train_time:24128ms step_avg:34.03ms
step:710/2090 train_time:24187ms step_avg:34.07ms
step:711/2090 train_time:24247ms step_avg:34.10ms
step:712/2090 train_time:24307ms step_avg:34.14ms
step:713/2090 train_time:24367ms step_avg:34.17ms
step:714/2090 train_time:24426ms step_avg:34.21ms
step:715/2090 train_time:24486ms step_avg:34.25ms
step:716/2090 train_time:24545ms step_avg:34.28ms
step:717/2090 train_time:24606ms step_avg:34.32ms
step:718/2090 train_time:24665ms step_avg:34.35ms
step:719/2090 train_time:24725ms step_avg:34.39ms
step:720/2090 train_time:24784ms step_avg:34.42ms
step:721/2090 train_time:24845ms step_avg:34.46ms
step:722/2090 train_time:24905ms step_avg:34.49ms
step:723/2090 train_time:24966ms step_avg:34.53ms
step:724/2090 train_time:25025ms step_avg:34.57ms
step:725/2090 train_time:25085ms step_avg:34.60ms
step:726/2090 train_time:25144ms step_avg:34.63ms
step:727/2090 train_time:25205ms step_avg:34.67ms
step:728/2090 train_time:25264ms step_avg:34.70ms
step:729/2090 train_time:25325ms step_avg:34.74ms
step:730/2090 train_time:25384ms step_avg:34.77ms
step:731/2090 train_time:25444ms step_avg:34.81ms
step:732/2090 train_time:25503ms step_avg:34.84ms
step:733/2090 train_time:25563ms step_avg:34.87ms
step:734/2090 train_time:25622ms step_avg:34.91ms
step:735/2090 train_time:25682ms step_avg:34.94ms
step:736/2090 train_time:25741ms step_avg:34.97ms
step:737/2090 train_time:25802ms step_avg:35.01ms
step:738/2090 train_time:25861ms step_avg:35.04ms
step:739/2090 train_time:25922ms step_avg:35.08ms
step:740/2090 train_time:25981ms step_avg:35.11ms
step:741/2090 train_time:26042ms step_avg:35.14ms
step:742/2090 train_time:26101ms step_avg:35.18ms
step:743/2090 train_time:26162ms step_avg:35.21ms
step:744/2090 train_time:26222ms step_avg:35.24ms
step:745/2090 train_time:26282ms step_avg:35.28ms
step:746/2090 train_time:26341ms step_avg:35.31ms
step:747/2090 train_time:26402ms step_avg:35.34ms
step:748/2090 train_time:26461ms step_avg:35.38ms
step:749/2090 train_time:26522ms step_avg:35.41ms
step:750/2090 train_time:26581ms step_avg:35.44ms
step:750/2090 val_loss:3.8562 train_time:26643ms step_avg:35.52ms
step:751/2090 train_time:26663ms step_avg:35.50ms
step:752/2090 train_time:26703ms step_avg:35.51ms
step:753/2090 train_time:26766ms step_avg:35.55ms
step:754/2090 train_time:26829ms step_avg:35.58ms
step:755/2090 train_time:26890ms step_avg:35.62ms
step:756/2090 train_time:26950ms step_avg:35.65ms
step:757/2090 train_time:27010ms step_avg:35.68ms
step:758/2090 train_time:27069ms step_avg:35.71ms
step:759/2090 train_time:27130ms step_avg:35.74ms
step:760/2090 train_time:27189ms step_avg:35.78ms
step:761/2090 train_time:27250ms step_avg:35.81ms
step:762/2090 train_time:27309ms step_avg:35.84ms
step:763/2090 train_time:27368ms step_avg:35.87ms
step:764/2090 train_time:27427ms step_avg:35.90ms
step:765/2090 train_time:27486ms step_avg:35.93ms
step:766/2090 train_time:27545ms step_avg:35.96ms
step:767/2090 train_time:27605ms step_avg:35.99ms
step:768/2090 train_time:27666ms step_avg:36.02ms
step:769/2090 train_time:27727ms step_avg:36.06ms
step:770/2090 train_time:27788ms step_avg:36.09ms
step:771/2090 train_time:27850ms step_avg:36.12ms
step:772/2090 train_time:27909ms step_avg:36.15ms
step:773/2090 train_time:27970ms step_avg:36.18ms
step:774/2090 train_time:28029ms step_avg:36.21ms
step:775/2090 train_time:28089ms step_avg:36.24ms
step:776/2090 train_time:28149ms step_avg:36.27ms
step:777/2090 train_time:28209ms step_avg:36.31ms
step:778/2090 train_time:28269ms step_avg:36.33ms
step:779/2090 train_time:28328ms step_avg:36.36ms
step:780/2090 train_time:28386ms step_avg:36.39ms
step:781/2090 train_time:28446ms step_avg:36.42ms
step:782/2090 train_time:28505ms step_avg:36.45ms
step:783/2090 train_time:28565ms step_avg:36.48ms
step:784/2090 train_time:28624ms step_avg:36.51ms
step:785/2090 train_time:28685ms step_avg:36.54ms
step:786/2090 train_time:28745ms step_avg:36.57ms
step:787/2090 train_time:28807ms step_avg:36.60ms
step:788/2090 train_time:28867ms step_avg:36.63ms
step:789/2090 train_time:28927ms step_avg:36.66ms
step:790/2090 train_time:28987ms step_avg:36.69ms
step:791/2090 train_time:29046ms step_avg:36.72ms
step:792/2090 train_time:29105ms step_avg:36.75ms
step:793/2090 train_time:29166ms step_avg:36.78ms
step:794/2090 train_time:29226ms step_avg:36.81ms
step:795/2090 train_time:29286ms step_avg:36.84ms
step:796/2090 train_time:29345ms step_avg:36.87ms
step:797/2090 train_time:29404ms step_avg:36.89ms
step:798/2090 train_time:29463ms step_avg:36.92ms
step:799/2090 train_time:29523ms step_avg:36.95ms
step:800/2090 train_time:29583ms step_avg:36.98ms
step:801/2090 train_time:29643ms step_avg:37.01ms
step:802/2090 train_time:29703ms step_avg:37.04ms
step:803/2090 train_time:29764ms step_avg:37.07ms
step:804/2090 train_time:29824ms step_avg:37.09ms
step:805/2090 train_time:29884ms step_avg:37.12ms
step:806/2090 train_time:29943ms step_avg:37.15ms
step:807/2090 train_time:30004ms step_avg:37.18ms
step:808/2090 train_time:30063ms step_avg:37.21ms
step:809/2090 train_time:30123ms step_avg:37.24ms
step:810/2090 train_time:30183ms step_avg:37.26ms
step:811/2090 train_time:30244ms step_avg:37.29ms
step:812/2090 train_time:30303ms step_avg:37.32ms
step:813/2090 train_time:30363ms step_avg:37.35ms
step:814/2090 train_time:30422ms step_avg:37.37ms
step:815/2090 train_time:30483ms step_avg:37.40ms
step:816/2090 train_time:30542ms step_avg:37.43ms
step:817/2090 train_time:30603ms step_avg:37.46ms
step:818/2090 train_time:30663ms step_avg:37.48ms
step:819/2090 train_time:30723ms step_avg:37.51ms
step:820/2090 train_time:30783ms step_avg:37.54ms
step:821/2090 train_time:30843ms step_avg:37.57ms
step:822/2090 train_time:30902ms step_avg:37.59ms
step:823/2090 train_time:30963ms step_avg:37.62ms
step:824/2090 train_time:31022ms step_avg:37.65ms
step:825/2090 train_time:31083ms step_avg:37.68ms
step:826/2090 train_time:31142ms step_avg:37.70ms
step:827/2090 train_time:31202ms step_avg:37.73ms
step:828/2090 train_time:31261ms step_avg:37.76ms
step:829/2090 train_time:31321ms step_avg:37.78ms
step:830/2090 train_time:31380ms step_avg:37.81ms
step:831/2090 train_time:31439ms step_avg:37.83ms
step:832/2090 train_time:31498ms step_avg:37.86ms
step:833/2090 train_time:31559ms step_avg:37.89ms
step:834/2090 train_time:31618ms step_avg:37.91ms
step:835/2090 train_time:31679ms step_avg:37.94ms
step:836/2090 train_time:31739ms step_avg:37.96ms
step:837/2090 train_time:31799ms step_avg:37.99ms
step:838/2090 train_time:31859ms step_avg:38.02ms
step:839/2090 train_time:31919ms step_avg:38.04ms
step:840/2090 train_time:31978ms step_avg:38.07ms
step:841/2090 train_time:32039ms step_avg:38.10ms
step:842/2090 train_time:32098ms step_avg:38.12ms
step:843/2090 train_time:32158ms step_avg:38.15ms
step:844/2090 train_time:32218ms step_avg:38.17ms
step:845/2090 train_time:32279ms step_avg:38.20ms
step:846/2090 train_time:32338ms step_avg:38.22ms
step:847/2090 train_time:32398ms step_avg:38.25ms
step:848/2090 train_time:32457ms step_avg:38.27ms
step:849/2090 train_time:32517ms step_avg:38.30ms
step:850/2090 train_time:32575ms step_avg:38.32ms
step:851/2090 train_time:32636ms step_avg:38.35ms
step:852/2090 train_time:32696ms step_avg:38.38ms
step:853/2090 train_time:32756ms step_avg:38.40ms
step:854/2090 train_time:32816ms step_avg:38.43ms
step:855/2090 train_time:32876ms step_avg:38.45ms
step:856/2090 train_time:32935ms step_avg:38.48ms
step:857/2090 train_time:32995ms step_avg:38.50ms
step:858/2090 train_time:33055ms step_avg:38.53ms
step:859/2090 train_time:33115ms step_avg:38.55ms
step:860/2090 train_time:33174ms step_avg:38.57ms
step:861/2090 train_time:33234ms step_avg:38.60ms
step:862/2090 train_time:33294ms step_avg:38.62ms
step:863/2090 train_time:33354ms step_avg:38.65ms
step:864/2090 train_time:33412ms step_avg:38.67ms
step:865/2090 train_time:33472ms step_avg:38.70ms
step:866/2090 train_time:33531ms step_avg:38.72ms
step:867/2090 train_time:33590ms step_avg:38.74ms
step:868/2090 train_time:33650ms step_avg:38.77ms
step:869/2090 train_time:33710ms step_avg:38.79ms
step:870/2090 train_time:33770ms step_avg:38.82ms
step:871/2090 train_time:33830ms step_avg:38.84ms
step:872/2090 train_time:33889ms step_avg:38.86ms
step:873/2090 train_time:33950ms step_avg:38.89ms
step:874/2090 train_time:34010ms step_avg:38.91ms
step:875/2090 train_time:34071ms step_avg:38.94ms
step:876/2090 train_time:34130ms step_avg:38.96ms
step:877/2090 train_time:34191ms step_avg:38.99ms
step:878/2090 train_time:34250ms step_avg:39.01ms
step:879/2090 train_time:34310ms step_avg:39.03ms
step:880/2090 train_time:34369ms step_avg:39.06ms
step:881/2090 train_time:34429ms step_avg:39.08ms
step:882/2090 train_time:34488ms step_avg:39.10ms
step:883/2090 train_time:34548ms step_avg:39.13ms
step:884/2090 train_time:34607ms step_avg:39.15ms
step:885/2090 train_time:34667ms step_avg:39.17ms
step:886/2090 train_time:34727ms step_avg:39.20ms
step:887/2090 train_time:34788ms step_avg:39.22ms
step:888/2090 train_time:34848ms step_avg:39.24ms
step:889/2090 train_time:34907ms step_avg:39.27ms
step:890/2090 train_time:34967ms step_avg:39.29ms
step:891/2090 train_time:35027ms step_avg:39.31ms
step:892/2090 train_time:35087ms step_avg:39.34ms
step:893/2090 train_time:35148ms step_avg:39.36ms
step:894/2090 train_time:35208ms step_avg:39.38ms
step:895/2090 train_time:35268ms step_avg:39.41ms
step:896/2090 train_time:35327ms step_avg:39.43ms
step:897/2090 train_time:35387ms step_avg:39.45ms
step:898/2090 train_time:35445ms step_avg:39.47ms
step:899/2090 train_time:35505ms step_avg:39.49ms
step:900/2090 train_time:35564ms step_avg:39.52ms
step:901/2090 train_time:35625ms step_avg:39.54ms
step:902/2090 train_time:35685ms step_avg:39.56ms
step:903/2090 train_time:35745ms step_avg:39.58ms
step:904/2090 train_time:35805ms step_avg:39.61ms
step:905/2090 train_time:35865ms step_avg:39.63ms
step:906/2090 train_time:35925ms step_avg:39.65ms
step:907/2090 train_time:35986ms step_avg:39.68ms
step:908/2090 train_time:36045ms step_avg:39.70ms
step:909/2090 train_time:36106ms step_avg:39.72ms
step:910/2090 train_time:36166ms step_avg:39.74ms
step:911/2090 train_time:36225ms step_avg:39.76ms
step:912/2090 train_time:36284ms step_avg:39.79ms
step:913/2090 train_time:36345ms step_avg:39.81ms
step:914/2090 train_time:36404ms step_avg:39.83ms
step:915/2090 train_time:36464ms step_avg:39.85ms
step:916/2090 train_time:36524ms step_avg:39.87ms
step:917/2090 train_time:36583ms step_avg:39.89ms
step:918/2090 train_time:36643ms step_avg:39.92ms
step:919/2090 train_time:36704ms step_avg:39.94ms
step:920/2090 train_time:36763ms step_avg:39.96ms
step:921/2090 train_time:36824ms step_avg:39.98ms
step:922/2090 train_time:36884ms step_avg:40.00ms
step:923/2090 train_time:36944ms step_avg:40.03ms
step:924/2090 train_time:37003ms step_avg:40.05ms
step:925/2090 train_time:37065ms step_avg:40.07ms
step:926/2090 train_time:37124ms step_avg:40.09ms
step:927/2090 train_time:37184ms step_avg:40.11ms
step:928/2090 train_time:37244ms step_avg:40.13ms
step:929/2090 train_time:37304ms step_avg:40.16ms
step:930/2090 train_time:37363ms step_avg:40.18ms
step:931/2090 train_time:37423ms step_avg:40.20ms
step:932/2090 train_time:37483ms step_avg:40.22ms
step:933/2090 train_time:37543ms step_avg:40.24ms
step:934/2090 train_time:37602ms step_avg:40.26ms
step:935/2090 train_time:37662ms step_avg:40.28ms
step:936/2090 train_time:37722ms step_avg:40.30ms
step:937/2090 train_time:37783ms step_avg:40.32ms
step:938/2090 train_time:37842ms step_avg:40.34ms
step:939/2090 train_time:37903ms step_avg:40.36ms
step:940/2090 train_time:37962ms step_avg:40.39ms
step:941/2090 train_time:38022ms step_avg:40.41ms
step:942/2090 train_time:38082ms step_avg:40.43ms
step:943/2090 train_time:38142ms step_avg:40.45ms
step:944/2090 train_time:38202ms step_avg:40.47ms
step:945/2090 train_time:38263ms step_avg:40.49ms
step:946/2090 train_time:38322ms step_avg:40.51ms
step:947/2090 train_time:38382ms step_avg:40.53ms
step:948/2090 train_time:38441ms step_avg:40.55ms
step:949/2090 train_time:38501ms step_avg:40.57ms
step:950/2090 train_time:38561ms step_avg:40.59ms
step:951/2090 train_time:38621ms step_avg:40.61ms
step:952/2090 train_time:38681ms step_avg:40.63ms
step:953/2090 train_time:38741ms step_avg:40.65ms
step:954/2090 train_time:38801ms step_avg:40.67ms
step:955/2090 train_time:38862ms step_avg:40.69ms
step:956/2090 train_time:38921ms step_avg:40.71ms
step:957/2090 train_time:38982ms step_avg:40.73ms
step:958/2090 train_time:39042ms step_avg:40.75ms
step:959/2090 train_time:39103ms step_avg:40.77ms
step:960/2090 train_time:39162ms step_avg:40.79ms
step:961/2090 train_time:39223ms step_avg:40.81ms
step:962/2090 train_time:39282ms step_avg:40.83ms
step:963/2090 train_time:39343ms step_avg:40.85ms
step:964/2090 train_time:39402ms step_avg:40.87ms
step:965/2090 train_time:39462ms step_avg:40.89ms
step:966/2090 train_time:39522ms step_avg:40.91ms
step:967/2090 train_time:39582ms step_avg:40.93ms
step:968/2090 train_time:39641ms step_avg:40.95ms
step:969/2090 train_time:39701ms step_avg:40.97ms
step:970/2090 train_time:39761ms step_avg:40.99ms
step:971/2090 train_time:39821ms step_avg:41.01ms
step:972/2090 train_time:39881ms step_avg:41.03ms
step:973/2090 train_time:39941ms step_avg:41.05ms
step:974/2090 train_time:40001ms step_avg:41.07ms
step:975/2090 train_time:40061ms step_avg:41.09ms
step:976/2090 train_time:40121ms step_avg:41.11ms
step:977/2090 train_time:40182ms step_avg:41.13ms
step:978/2090 train_time:40241ms step_avg:41.15ms
step:979/2090 train_time:40302ms step_avg:41.17ms
step:980/2090 train_time:40361ms step_avg:41.18ms
step:981/2090 train_time:40422ms step_avg:41.21ms
step:982/2090 train_time:40482ms step_avg:41.22ms
step:983/2090 train_time:40542ms step_avg:41.24ms
step:984/2090 train_time:40601ms step_avg:41.26ms
step:985/2090 train_time:40661ms step_avg:41.28ms
step:986/2090 train_time:40720ms step_avg:41.30ms
step:987/2090 train_time:40781ms step_avg:41.32ms
step:988/2090 train_time:40840ms step_avg:41.34ms
step:989/2090 train_time:40900ms step_avg:41.36ms
step:990/2090 train_time:40960ms step_avg:41.37ms
step:991/2090 train_time:41021ms step_avg:41.39ms
step:992/2090 train_time:41080ms step_avg:41.41ms
step:993/2090 train_time:41141ms step_avg:41.43ms
step:994/2090 train_time:41201ms step_avg:41.45ms
step:995/2090 train_time:41261ms step_avg:41.47ms
step:996/2090 train_time:41320ms step_avg:41.49ms
step:997/2090 train_time:41380ms step_avg:41.50ms
step:998/2090 train_time:41440ms step_avg:41.52ms
step:999/2090 train_time:41500ms step_avg:41.54ms
step:1000/2090 train_time:41559ms step_avg:41.56ms
step:1000/2090 val_loss:3.7045 train_time:41621ms step_avg:41.62ms
step:1001/2090 train_time:41641ms step_avg:41.60ms
step:1002/2090 train_time:41680ms step_avg:41.60ms
step:1003/2090 train_time:41743ms step_avg:41.62ms
step:1004/2090 train_time:41805ms step_avg:41.64ms
step:1005/2090 train_time:41865ms step_avg:41.66ms
step:1006/2090 train_time:41926ms step_avg:41.68ms
step:1007/2090 train_time:41985ms step_avg:41.69ms
step:1008/2090 train_time:42044ms step_avg:41.71ms
step:1009/2090 train_time:42104ms step_avg:41.73ms
step:1010/2090 train_time:42162ms step_avg:41.74ms
step:1011/2090 train_time:42222ms step_avg:41.76ms
step:1012/2090 train_time:42281ms step_avg:41.78ms
step:1013/2090 train_time:42341ms step_avg:41.80ms
step:1014/2090 train_time:42400ms step_avg:41.81ms
step:1015/2090 train_time:42459ms step_avg:41.83ms
step:1016/2090 train_time:42518ms step_avg:41.85ms
step:1017/2090 train_time:42581ms step_avg:41.87ms
step:1018/2090 train_time:42642ms step_avg:41.89ms
step:1019/2090 train_time:42704ms step_avg:41.91ms
step:1020/2090 train_time:42765ms step_avg:41.93ms
step:1021/2090 train_time:42827ms step_avg:41.95ms
step:1022/2090 train_time:42887ms step_avg:41.96ms
step:1023/2090 train_time:42948ms step_avg:41.98ms
step:1024/2090 train_time:43008ms step_avg:42.00ms
step:1025/2090 train_time:43067ms step_avg:42.02ms
step:1026/2090 train_time:43125ms step_avg:42.03ms
step:1027/2090 train_time:43185ms step_avg:42.05ms
step:1028/2090 train_time:43243ms step_avg:42.07ms
step:1029/2090 train_time:43303ms step_avg:42.08ms
step:1030/2090 train_time:43361ms step_avg:42.10ms
step:1031/2090 train_time:43421ms step_avg:42.12ms
step:1032/2090 train_time:43480ms step_avg:42.13ms
step:1033/2090 train_time:43541ms step_avg:42.15ms
step:1034/2090 train_time:43601ms step_avg:42.17ms
step:1035/2090 train_time:43663ms step_avg:42.19ms
step:1036/2090 train_time:43723ms step_avg:42.20ms
step:1037/2090 train_time:43784ms step_avg:42.22ms
step:1038/2090 train_time:43845ms step_avg:42.24ms
step:1039/2090 train_time:43906ms step_avg:42.26ms
step:1040/2090 train_time:43966ms step_avg:42.27ms
step:1041/2090 train_time:44025ms step_avg:42.29ms
step:1042/2090 train_time:44084ms step_avg:42.31ms
step:1043/2090 train_time:44144ms step_avg:42.32ms
step:1044/2090 train_time:44203ms step_avg:42.34ms
step:1045/2090 train_time:44263ms step_avg:42.36ms
step:1046/2090 train_time:44321ms step_avg:42.37ms
step:1047/2090 train_time:44381ms step_avg:42.39ms
step:1048/2090 train_time:44439ms step_avg:42.40ms
step:1049/2090 train_time:44500ms step_avg:42.42ms
step:1050/2090 train_time:44559ms step_avg:42.44ms
step:1051/2090 train_time:44619ms step_avg:42.45ms
step:1052/2090 train_time:44680ms step_avg:42.47ms
step:1053/2090 train_time:44741ms step_avg:42.49ms
step:1054/2090 train_time:44801ms step_avg:42.51ms
step:1055/2090 train_time:44862ms step_avg:42.52ms
step:1056/2090 train_time:44922ms step_avg:42.54ms
step:1057/2090 train_time:44983ms step_avg:42.56ms
step:1058/2090 train_time:45042ms step_avg:42.57ms
step:1059/2090 train_time:45103ms step_avg:42.59ms
step:1060/2090 train_time:45162ms step_avg:42.61ms
step:1061/2090 train_time:45222ms step_avg:42.62ms
step:1062/2090 train_time:45281ms step_avg:42.64ms
step:1063/2090 train_time:45341ms step_avg:42.65ms
step:1064/2090 train_time:45400ms step_avg:42.67ms
step:1065/2090 train_time:45460ms step_avg:42.69ms
step:1066/2090 train_time:45519ms step_avg:42.70ms
step:1067/2090 train_time:45579ms step_avg:42.72ms
step:1068/2090 train_time:45639ms step_avg:42.73ms
step:1069/2090 train_time:45700ms step_avg:42.75ms
step:1070/2090 train_time:45760ms step_avg:42.77ms
step:1071/2090 train_time:45821ms step_avg:42.78ms
step:1072/2090 train_time:45881ms step_avg:42.80ms
step:1073/2090 train_time:45942ms step_avg:42.82ms
step:1074/2090 train_time:46002ms step_avg:42.83ms
step:1075/2090 train_time:46062ms step_avg:42.85ms
step:1076/2090 train_time:46121ms step_avg:42.86ms
step:1077/2090 train_time:46181ms step_avg:42.88ms
step:1078/2090 train_time:46240ms step_avg:42.89ms
step:1079/2090 train_time:46300ms step_avg:42.91ms
step:1080/2090 train_time:46359ms step_avg:42.92ms
step:1081/2090 train_time:46419ms step_avg:42.94ms
step:1082/2090 train_time:46478ms step_avg:42.96ms
step:1083/2090 train_time:46538ms step_avg:42.97ms
step:1084/2090 train_time:46597ms step_avg:42.99ms
step:1085/2090 train_time:46658ms step_avg:43.00ms
step:1086/2090 train_time:46717ms step_avg:43.02ms
step:1087/2090 train_time:46778ms step_avg:43.03ms
step:1088/2090 train_time:46838ms step_avg:43.05ms
step:1089/2090 train_time:46899ms step_avg:43.07ms
step:1090/2090 train_time:46959ms step_avg:43.08ms
step:1091/2090 train_time:47021ms step_avg:43.10ms
step:1092/2090 train_time:47080ms step_avg:43.11ms
step:1093/2090 train_time:47141ms step_avg:43.13ms
step:1094/2090 train_time:47200ms step_avg:43.14ms
step:1095/2090 train_time:47259ms step_avg:43.16ms
step:1096/2090 train_time:47318ms step_avg:43.17ms
step:1097/2090 train_time:47378ms step_avg:43.19ms
step:1098/2090 train_time:47437ms step_avg:43.20ms
step:1099/2090 train_time:47497ms step_avg:43.22ms
step:1100/2090 train_time:47556ms step_avg:43.23ms
step:1101/2090 train_time:47617ms step_avg:43.25ms
step:1102/2090 train_time:47676ms step_avg:43.26ms
step:1103/2090 train_time:47737ms step_avg:43.28ms
step:1104/2090 train_time:47797ms step_avg:43.29ms
step:1105/2090 train_time:47857ms step_avg:43.31ms
step:1106/2090 train_time:47917ms step_avg:43.32ms
step:1107/2090 train_time:47978ms step_avg:43.34ms
step:1108/2090 train_time:48039ms step_avg:43.36ms
step:1109/2090 train_time:48099ms step_avg:43.37ms
step:1110/2090 train_time:48158ms step_avg:43.39ms
step:1111/2090 train_time:48218ms step_avg:43.40ms
step:1112/2090 train_time:48278ms step_avg:43.42ms
step:1113/2090 train_time:48337ms step_avg:43.43ms
step:1114/2090 train_time:48397ms step_avg:43.44ms
step:1115/2090 train_time:48457ms step_avg:43.46ms
step:1116/2090 train_time:48516ms step_avg:43.47ms
step:1117/2090 train_time:48576ms step_avg:43.49ms
step:1118/2090 train_time:48636ms step_avg:43.50ms
step:1119/2090 train_time:48696ms step_avg:43.52ms
step:1120/2090 train_time:48756ms step_avg:43.53ms
step:1121/2090 train_time:48816ms step_avg:43.55ms
step:1122/2090 train_time:48875ms step_avg:43.56ms
step:1123/2090 train_time:48936ms step_avg:43.58ms
step:1124/2090 train_time:48996ms step_avg:43.59ms
step:1125/2090 train_time:49056ms step_avg:43.61ms
step:1126/2090 train_time:49117ms step_avg:43.62ms
step:1127/2090 train_time:49177ms step_avg:43.64ms
step:1128/2090 train_time:49236ms step_avg:43.65ms
step:1129/2090 train_time:49297ms step_avg:43.66ms
step:1130/2090 train_time:49356ms step_avg:43.68ms
step:1131/2090 train_time:49416ms step_avg:43.69ms
step:1132/2090 train_time:49475ms step_avg:43.71ms
step:1133/2090 train_time:49536ms step_avg:43.72ms
step:1134/2090 train_time:49595ms step_avg:43.73ms
step:1135/2090 train_time:49656ms step_avg:43.75ms
step:1136/2090 train_time:49715ms step_avg:43.76ms
step:1137/2090 train_time:49775ms step_avg:43.78ms
step:1138/2090 train_time:49835ms step_avg:43.79ms
step:1139/2090 train_time:49896ms step_avg:43.81ms
step:1140/2090 train_time:49956ms step_avg:43.82ms
step:1141/2090 train_time:50016ms step_avg:43.84ms
step:1142/2090 train_time:50075ms step_avg:43.85ms
step:1143/2090 train_time:50136ms step_avg:43.86ms
step:1144/2090 train_time:50196ms step_avg:43.88ms
step:1145/2090 train_time:50256ms step_avg:43.89ms
step:1146/2090 train_time:50315ms step_avg:43.90ms
step:1147/2090 train_time:50376ms step_avg:43.92ms
step:1148/2090 train_time:50435ms step_avg:43.93ms
step:1149/2090 train_time:50495ms step_avg:43.95ms
step:1150/2090 train_time:50554ms step_avg:43.96ms
step:1151/2090 train_time:50614ms step_avg:43.97ms
step:1152/2090 train_time:50673ms step_avg:43.99ms
step:1153/2090 train_time:50733ms step_avg:44.00ms
step:1154/2090 train_time:50793ms step_avg:44.01ms
step:1155/2090 train_time:50854ms step_avg:44.03ms
step:1156/2090 train_time:50913ms step_avg:44.04ms
step:1157/2090 train_time:50974ms step_avg:44.06ms
step:1158/2090 train_time:51033ms step_avg:44.07ms
step:1159/2090 train_time:51094ms step_avg:44.08ms
step:1160/2090 train_time:51154ms step_avg:44.10ms
step:1161/2090 train_time:51215ms step_avg:44.11ms
step:1162/2090 train_time:51274ms step_avg:44.13ms
step:1163/2090 train_time:51335ms step_avg:44.14ms
step:1164/2090 train_time:51394ms step_avg:44.15ms
step:1165/2090 train_time:51453ms step_avg:44.17ms
step:1166/2090 train_time:51512ms step_avg:44.18ms
step:1167/2090 train_time:51572ms step_avg:44.19ms
step:1168/2090 train_time:51631ms step_avg:44.20ms
step:1169/2090 train_time:51691ms step_avg:44.22ms
step:1170/2090 train_time:51750ms step_avg:44.23ms
step:1171/2090 train_time:51811ms step_avg:44.24ms
step:1172/2090 train_time:51870ms step_avg:44.26ms
step:1173/2090 train_time:51930ms step_avg:44.27ms
step:1174/2090 train_time:51989ms step_avg:44.28ms
step:1175/2090 train_time:52049ms step_avg:44.30ms
step:1176/2090 train_time:52109ms step_avg:44.31ms
step:1177/2090 train_time:52170ms step_avg:44.32ms
step:1178/2090 train_time:52229ms step_avg:44.34ms
step:1179/2090 train_time:52289ms step_avg:44.35ms
step:1180/2090 train_time:52349ms step_avg:44.36ms
step:1181/2090 train_time:52409ms step_avg:44.38ms
step:1182/2090 train_time:52468ms step_avg:44.39ms
step:1183/2090 train_time:52529ms step_avg:44.40ms
step:1184/2090 train_time:52588ms step_avg:44.42ms
step:1185/2090 train_time:52648ms step_avg:44.43ms
step:1186/2090 train_time:52708ms step_avg:44.44ms
step:1187/2090 train_time:52768ms step_avg:44.45ms
step:1188/2090 train_time:52827ms step_avg:44.47ms
step:1189/2090 train_time:52887ms step_avg:44.48ms
step:1190/2090 train_time:52946ms step_avg:44.49ms
step:1191/2090 train_time:53007ms step_avg:44.51ms
step:1192/2090 train_time:53067ms step_avg:44.52ms
step:1193/2090 train_time:53127ms step_avg:44.53ms
step:1194/2090 train_time:53187ms step_avg:44.55ms
step:1195/2090 train_time:53248ms step_avg:44.56ms
step:1196/2090 train_time:53307ms step_avg:44.57ms
step:1197/2090 train_time:53368ms step_avg:44.58ms
step:1198/2090 train_time:53427ms step_avg:44.60ms
step:1199/2090 train_time:53488ms step_avg:44.61ms
step:1200/2090 train_time:53547ms step_avg:44.62ms
step:1201/2090 train_time:53607ms step_avg:44.63ms
step:1202/2090 train_time:53666ms step_avg:44.65ms
step:1203/2090 train_time:53726ms step_avg:44.66ms
step:1204/2090 train_time:53785ms step_avg:44.67ms
step:1205/2090 train_time:53845ms step_avg:44.68ms
step:1206/2090 train_time:53905ms step_avg:44.70ms
step:1207/2090 train_time:53964ms step_avg:44.71ms
step:1208/2090 train_time:54024ms step_avg:44.72ms
step:1209/2090 train_time:54084ms step_avg:44.73ms
step:1210/2090 train_time:54144ms step_avg:44.75ms
step:1211/2090 train_time:54205ms step_avg:44.76ms
step:1212/2090 train_time:54264ms step_avg:44.77ms
step:1213/2090 train_time:54324ms step_avg:44.79ms
step:1214/2090 train_time:54384ms step_avg:44.80ms
step:1215/2090 train_time:54444ms step_avg:44.81ms
step:1216/2090 train_time:54504ms step_avg:44.82ms
step:1217/2090 train_time:54564ms step_avg:44.83ms
step:1218/2090 train_time:54623ms step_avg:44.85ms
step:1219/2090 train_time:54683ms step_avg:44.86ms
step:1220/2090 train_time:54743ms step_avg:44.87ms
step:1221/2090 train_time:54803ms step_avg:44.88ms
step:1222/2090 train_time:54862ms step_avg:44.89ms
step:1223/2090 train_time:54921ms step_avg:44.91ms
step:1224/2090 train_time:54981ms step_avg:44.92ms
step:1225/2090 train_time:55041ms step_avg:44.93ms
step:1226/2090 train_time:55100ms step_avg:44.94ms
step:1227/2090 train_time:55162ms step_avg:44.96ms
step:1228/2090 train_time:55221ms step_avg:44.97ms
step:1229/2090 train_time:55282ms step_avg:44.98ms
step:1230/2090 train_time:55341ms step_avg:44.99ms
step:1231/2090 train_time:55402ms step_avg:45.01ms
step:1232/2090 train_time:55461ms step_avg:45.02ms
step:1233/2090 train_time:55521ms step_avg:45.03ms
step:1234/2090 train_time:55580ms step_avg:45.04ms
step:1235/2090 train_time:55640ms step_avg:45.05ms
step:1236/2090 train_time:55700ms step_avg:45.06ms
step:1237/2090 train_time:55761ms step_avg:45.08ms
step:1238/2090 train_time:55820ms step_avg:45.09ms
step:1239/2090 train_time:55881ms step_avg:45.10ms
step:1240/2090 train_time:55939ms step_avg:45.11ms
step:1241/2090 train_time:56000ms step_avg:45.12ms
step:1242/2090 train_time:56060ms step_avg:45.14ms
step:1243/2090 train_time:56120ms step_avg:45.15ms
step:1244/2090 train_time:56180ms step_avg:45.16ms
step:1245/2090 train_time:56240ms step_avg:45.17ms
step:1246/2090 train_time:56300ms step_avg:45.18ms
step:1247/2090 train_time:56361ms step_avg:45.20ms
step:1248/2090 train_time:56420ms step_avg:45.21ms
step:1249/2090 train_time:56480ms step_avg:45.22ms
step:1250/2090 train_time:56539ms step_avg:45.23ms
step:1250/2090 val_loss:3.5861 train_time:56602ms step_avg:45.28ms
step:1251/2090 train_time:56621ms step_avg:45.26ms
step:1252/2090 train_time:56662ms step_avg:45.26ms
step:1253/2090 train_time:56726ms step_avg:45.27ms
step:1254/2090 train_time:56788ms step_avg:45.29ms
step:1255/2090 train_time:56849ms step_avg:45.30ms
step:1256/2090 train_time:56908ms step_avg:45.31ms
step:1257/2090 train_time:56967ms step_avg:45.32ms
step:1258/2090 train_time:57026ms step_avg:45.33ms
step:1259/2090 train_time:57085ms step_avg:45.34ms
step:1260/2090 train_time:57143ms step_avg:45.35ms
step:1261/2090 train_time:57203ms step_avg:45.36ms
step:1262/2090 train_time:57261ms step_avg:45.37ms
step:1263/2090 train_time:57320ms step_avg:45.38ms
step:1264/2090 train_time:57379ms step_avg:45.39ms
step:1265/2090 train_time:57439ms step_avg:45.41ms
step:1266/2090 train_time:57498ms step_avg:45.42ms
step:1267/2090 train_time:57559ms step_avg:45.43ms
step:1268/2090 train_time:57619ms step_avg:45.44ms
step:1269/2090 train_time:57682ms step_avg:45.45ms
step:1270/2090 train_time:57742ms step_avg:45.47ms
step:1271/2090 train_time:57804ms step_avg:45.48ms
step:1272/2090 train_time:57863ms step_avg:45.49ms
step:1273/2090 train_time:57924ms step_avg:45.50ms
step:1274/2090 train_time:57983ms step_avg:45.51ms
step:1275/2090 train_time:58043ms step_avg:45.52ms
step:1276/2090 train_time:58102ms step_avg:45.53ms
step:1277/2090 train_time:58161ms step_avg:45.55ms
step:1278/2090 train_time:58220ms step_avg:45.56ms
step:1279/2090 train_time:58280ms step_avg:45.57ms
step:1280/2090 train_time:58339ms step_avg:45.58ms
step:1281/2090 train_time:58398ms step_avg:45.59ms
step:1282/2090 train_time:58458ms step_avg:45.60ms
step:1283/2090 train_time:58518ms step_avg:45.61ms
step:1284/2090 train_time:58578ms step_avg:45.62ms
step:1285/2090 train_time:58640ms step_avg:45.63ms
step:1286/2090 train_time:58700ms step_avg:45.65ms
step:1287/2090 train_time:58761ms step_avg:45.66ms
step:1288/2090 train_time:58821ms step_avg:45.67ms
step:1289/2090 train_time:58884ms step_avg:45.68ms
step:1290/2090 train_time:58942ms step_avg:45.69ms
step:1291/2090 train_time:59003ms step_avg:45.70ms
step:1292/2090 train_time:59062ms step_avg:45.71ms
step:1293/2090 train_time:59121ms step_avg:45.72ms
step:1294/2090 train_time:59180ms step_avg:45.73ms
step:1295/2090 train_time:59240ms step_avg:45.75ms
step:1296/2090 train_time:59299ms step_avg:45.76ms
step:1297/2090 train_time:59359ms step_avg:45.77ms
step:1298/2090 train_time:59418ms step_avg:45.78ms
step:1299/2090 train_time:59479ms step_avg:45.79ms
step:1300/2090 train_time:59538ms step_avg:45.80ms
step:1301/2090 train_time:59599ms step_avg:45.81ms
step:1302/2090 train_time:59659ms step_avg:45.82ms
step:1303/2090 train_time:59720ms step_avg:45.83ms
step:1304/2090 train_time:59780ms step_avg:45.84ms
step:1305/2090 train_time:59842ms step_avg:45.86ms
step:1306/2090 train_time:59902ms step_avg:45.87ms
step:1307/2090 train_time:59963ms step_avg:45.88ms
step:1308/2090 train_time:60022ms step_avg:45.89ms
step:1309/2090 train_time:60082ms step_avg:45.90ms
step:1310/2090 train_time:60140ms step_avg:45.91ms
step:1311/2090 train_time:60201ms step_avg:45.92ms
step:1312/2090 train_time:60260ms step_avg:45.93ms
step:1313/2090 train_time:60319ms step_avg:45.94ms
step:1314/2090 train_time:60379ms step_avg:45.95ms
step:1315/2090 train_time:60439ms step_avg:45.96ms
step:1316/2090 train_time:60499ms step_avg:45.97ms
step:1317/2090 train_time:60560ms step_avg:45.98ms
step:1318/2090 train_time:60619ms step_avg:45.99ms
step:1319/2090 train_time:60680ms step_avg:46.00ms
step:1320/2090 train_time:60740ms step_avg:46.02ms
step:1321/2090 train_time:60802ms step_avg:46.03ms
step:1322/2090 train_time:60862ms step_avg:46.04ms
step:1323/2090 train_time:60922ms step_avg:46.05ms
step:1324/2090 train_time:60982ms step_avg:46.06ms
step:1325/2090 train_time:61043ms step_avg:46.07ms
step:1326/2090 train_time:61102ms step_avg:46.08ms
step:1327/2090 train_time:61162ms step_avg:46.09ms
step:1328/2090 train_time:61221ms step_avg:46.10ms
step:1329/2090 train_time:61281ms step_avg:46.11ms
step:1330/2090 train_time:61340ms step_avg:46.12ms
step:1331/2090 train_time:61400ms step_avg:46.13ms
step:1332/2090 train_time:61459ms step_avg:46.14ms
step:1333/2090 train_time:61519ms step_avg:46.15ms
step:1334/2090 train_time:61578ms step_avg:46.16ms
step:1335/2090 train_time:61639ms step_avg:46.17ms
step:1336/2090 train_time:61700ms step_avg:46.18ms
step:1337/2090 train_time:61761ms step_avg:46.19ms
step:1338/2090 train_time:61821ms step_avg:46.20ms
step:1339/2090 train_time:61882ms step_avg:46.21ms
step:1340/2090 train_time:61941ms step_avg:46.22ms
step:1341/2090 train_time:62001ms step_avg:46.24ms
step:1342/2090 train_time:62061ms step_avg:46.25ms
step:1343/2090 train_time:62122ms step_avg:46.26ms
step:1344/2090 train_time:62181ms step_avg:46.27ms
step:1345/2090 train_time:62241ms step_avg:46.28ms
step:1346/2090 train_time:62300ms step_avg:46.29ms
step:1347/2090 train_time:62360ms step_avg:46.30ms
step:1348/2090 train_time:62418ms step_avg:46.30ms
step:1349/2090 train_time:62479ms step_avg:46.31ms
step:1350/2090 train_time:62538ms step_avg:46.32ms
step:1351/2090 train_time:62599ms step_avg:46.34ms
step:1352/2090 train_time:62659ms step_avg:46.35ms
step:1353/2090 train_time:62720ms step_avg:46.36ms
step:1354/2090 train_time:62779ms step_avg:46.37ms
step:1355/2090 train_time:62840ms step_avg:46.38ms
step:1356/2090 train_time:62901ms step_avg:46.39ms
step:1357/2090 train_time:62961ms step_avg:46.40ms
step:1358/2090 train_time:63020ms step_avg:46.41ms
step:1359/2090 train_time:63080ms step_avg:46.42ms
step:1360/2090 train_time:63139ms step_avg:46.43ms
step:1361/2090 train_time:63199ms step_avg:46.44ms
step:1362/2090 train_time:63258ms step_avg:46.44ms
step:1363/2090 train_time:63318ms step_avg:46.45ms
step:1364/2090 train_time:63377ms step_avg:46.46ms
step:1365/2090 train_time:63437ms step_avg:46.47ms
step:1366/2090 train_time:63496ms step_avg:46.48ms
step:1367/2090 train_time:63556ms step_avg:46.49ms
step:1368/2090 train_time:63616ms step_avg:46.50ms
step:1369/2090 train_time:63704ms step_avg:46.53ms
step:1370/2090 train_time:63793ms step_avg:46.56ms
step:1371/2090 train_time:63881ms step_avg:46.59ms
step:1372/2090 train_time:63968ms step_avg:46.62ms
step:1373/2090 train_time:64057ms step_avg:46.65ms
step:1374/2090 train_time:64144ms step_avg:46.68ms
step:1375/2090 train_time:64231ms step_avg:46.71ms
step:1376/2090 train_time:64318ms step_avg:46.74ms
step:1377/2090 train_time:64406ms step_avg:46.77ms
step:1378/2090 train_time:64492ms step_avg:46.80ms
step:1379/2090 train_time:64580ms step_avg:46.83ms
step:1380/2090 train_time:64666ms step_avg:46.86ms
step:1381/2090 train_time:64756ms step_avg:46.89ms
step:1382/2090 train_time:64843ms step_avg:46.92ms
step:1383/2090 train_time:64931ms step_avg:46.95ms
step:1384/2090 train_time:65019ms step_avg:46.98ms
step:1385/2090 train_time:65107ms step_avg:47.01ms
step:1386/2090 train_time:65194ms step_avg:47.04ms
step:1387/2090 train_time:65281ms step_avg:47.07ms
step:1388/2090 train_time:65369ms step_avg:47.10ms
step:1389/2090 train_time:65456ms step_avg:47.12ms
step:1390/2090 train_time:65543ms step_avg:47.15ms
step:1391/2090 train_time:65631ms step_avg:47.18ms
step:1392/2090 train_time:65718ms step_avg:47.21ms
step:1393/2090 train_time:65806ms step_avg:47.24ms
step:1394/2090 train_time:65893ms step_avg:47.27ms
step:1395/2090 train_time:65981ms step_avg:47.30ms
step:1396/2090 train_time:66068ms step_avg:47.33ms
step:1397/2090 train_time:66157ms step_avg:47.36ms
step:1398/2090 train_time:66243ms step_avg:47.38ms
step:1399/2090 train_time:66331ms step_avg:47.41ms
step:1400/2090 train_time:66418ms step_avg:47.44ms
step:1401/2090 train_time:66505ms step_avg:47.47ms
step:1402/2090 train_time:66592ms step_avg:47.50ms
step:1403/2090 train_time:66680ms step_avg:47.53ms
step:1404/2090 train_time:66768ms step_avg:47.56ms
step:1405/2090 train_time:66856ms step_avg:47.58ms
step:1406/2090 train_time:66944ms step_avg:47.61ms
step:1407/2090 train_time:67032ms step_avg:47.64ms
step:1408/2090 train_time:67119ms step_avg:47.67ms
step:1409/2090 train_time:67206ms step_avg:47.70ms
step:1410/2090 train_time:67294ms step_avg:47.73ms
step:1411/2090 train_time:67382ms step_avg:47.75ms
step:1412/2090 train_time:67469ms step_avg:47.78ms
step:1413/2090 train_time:67557ms step_avg:47.81ms
step:1414/2090 train_time:67643ms step_avg:47.84ms
step:1415/2090 train_time:67731ms step_avg:47.87ms
step:1416/2090 train_time:67817ms step_avg:47.89ms
step:1417/2090 train_time:67906ms step_avg:47.92ms
step:1418/2090 train_time:67993ms step_avg:47.95ms
step:1419/2090 train_time:68081ms step_avg:47.98ms
step:1420/2090 train_time:68168ms step_avg:48.01ms
step:1421/2090 train_time:68257ms step_avg:48.03ms
step:1422/2090 train_time:68344ms step_avg:48.06ms
step:1423/2090 train_time:68432ms step_avg:48.09ms
step:1424/2090 train_time:68519ms step_avg:48.12ms
step:1425/2090 train_time:68607ms step_avg:48.15ms
step:1426/2090 train_time:68694ms step_avg:48.17ms
step:1427/2090 train_time:68783ms step_avg:48.20ms
step:1428/2090 train_time:68870ms step_avg:48.23ms
step:1429/2090 train_time:68958ms step_avg:48.26ms
step:1430/2090 train_time:69045ms step_avg:48.28ms
step:1431/2090 train_time:69133ms step_avg:48.31ms
step:1432/2090 train_time:69220ms step_avg:48.34ms
step:1433/2090 train_time:69309ms step_avg:48.37ms
step:1434/2090 train_time:69395ms step_avg:48.39ms
step:1435/2090 train_time:69483ms step_avg:48.42ms
step:1436/2090 train_time:69570ms step_avg:48.45ms
step:1437/2090 train_time:69658ms step_avg:48.47ms
step:1438/2090 train_time:69744ms step_avg:48.50ms
step:1439/2090 train_time:69833ms step_avg:48.53ms
step:1440/2090 train_time:69920ms step_avg:48.56ms
step:1441/2090 train_time:70008ms step_avg:48.58ms
step:1442/2090 train_time:70094ms step_avg:48.61ms
step:1443/2090 train_time:70183ms step_avg:48.64ms
step:1444/2090 train_time:70271ms step_avg:48.66ms
step:1445/2090 train_time:70359ms step_avg:48.69ms
step:1446/2090 train_time:70446ms step_avg:48.72ms
step:1447/2090 train_time:70535ms step_avg:48.75ms
step:1448/2090 train_time:70621ms step_avg:48.77ms
step:1449/2090 train_time:70708ms step_avg:48.80ms
step:1450/2090 train_time:70795ms step_avg:48.82ms
step:1451/2090 train_time:70884ms step_avg:48.85ms
step:1452/2090 train_time:70971ms step_avg:48.88ms
step:1453/2090 train_time:71059ms step_avg:48.90ms
step:1454/2090 train_time:71146ms step_avg:48.93ms
step:1455/2090 train_time:71235ms step_avg:48.96ms
step:1456/2090 train_time:71322ms step_avg:48.99ms
step:1457/2090 train_time:71411ms step_avg:49.01ms
step:1458/2090 train_time:71498ms step_avg:49.04ms
step:1459/2090 train_time:71586ms step_avg:49.07ms
step:1460/2090 train_time:71672ms step_avg:49.09ms
step:1461/2090 train_time:71760ms step_avg:49.12ms
step:1462/2090 train_time:71847ms step_avg:49.14ms
step:1463/2090 train_time:71936ms step_avg:49.17ms
step:1464/2090 train_time:72024ms step_avg:49.20ms
step:1465/2090 train_time:72111ms step_avg:49.22ms
step:1466/2090 train_time:72198ms step_avg:49.25ms
step:1467/2090 train_time:72286ms step_avg:49.27ms
step:1468/2090 train_time:72373ms step_avg:49.30ms
step:1469/2090 train_time:72462ms step_avg:49.33ms
step:1470/2090 train_time:72549ms step_avg:49.35ms
step:1471/2090 train_time:72636ms step_avg:49.38ms
step:1472/2090 train_time:72724ms step_avg:49.40ms
step:1473/2090 train_time:72812ms step_avg:49.43ms
step:1474/2090 train_time:72899ms step_avg:49.46ms
step:1475/2090 train_time:72988ms step_avg:49.48ms
step:1476/2090 train_time:73074ms step_avg:49.51ms
step:1477/2090 train_time:73163ms step_avg:49.53ms
step:1478/2090 train_time:73250ms step_avg:49.56ms
step:1479/2090 train_time:73338ms step_avg:49.59ms
step:1480/2090 train_time:73425ms step_avg:49.61ms
step:1481/2090 train_time:73514ms step_avg:49.64ms
step:1482/2090 train_time:73601ms step_avg:49.66ms
step:1483/2090 train_time:73689ms step_avg:49.69ms
step:1484/2090 train_time:73776ms step_avg:49.71ms
step:1485/2090 train_time:73864ms step_avg:49.74ms
step:1486/2090 train_time:73951ms step_avg:49.77ms
step:1487/2090 train_time:74039ms step_avg:49.79ms
step:1488/2090 train_time:74126ms step_avg:49.82ms
step:1489/2090 train_time:74213ms step_avg:49.84ms
step:1490/2090 train_time:74302ms step_avg:49.87ms
step:1491/2090 train_time:74389ms step_avg:49.89ms
step:1492/2090 train_time:74476ms step_avg:49.92ms
step:1493/2090 train_time:74565ms step_avg:49.94ms
step:1494/2090 train_time:74652ms step_avg:49.97ms
step:1495/2090 train_time:74739ms step_avg:49.99ms
step:1496/2090 train_time:74826ms step_avg:50.02ms
step:1497/2090 train_time:74914ms step_avg:50.04ms
step:1498/2090 train_time:75000ms step_avg:50.07ms
step:1499/2090 train_time:75088ms step_avg:50.09ms
step:1500/2090 train_time:75176ms step_avg:50.12ms
step:1500/2090 val_loss:3.4756 train_time:75266ms step_avg:50.18ms
step:1501/2090 train_time:75286ms step_avg:50.16ms
step:1502/2090 train_time:75359ms step_avg:50.17ms
step:1503/2090 train_time:75453ms step_avg:50.20ms
step:1504/2090 train_time:75541ms step_avg:50.23ms
step:1505/2090 train_time:75630ms step_avg:50.25ms
step:1506/2090 train_time:75716ms step_avg:50.28ms
step:1507/2090 train_time:75803ms step_avg:50.30ms
step:1508/2090 train_time:75888ms step_avg:50.32ms
step:1509/2090 train_time:75975ms step_avg:50.35ms
step:1510/2090 train_time:76061ms step_avg:50.37ms
step:1511/2090 train_time:76147ms step_avg:50.40ms
step:1512/2090 train_time:76235ms step_avg:50.42ms
step:1513/2090 train_time:76325ms step_avg:50.45ms
step:1514/2090 train_time:76416ms step_avg:50.47ms
step:1515/2090 train_time:76506ms step_avg:50.50ms
step:1516/2090 train_time:76593ms step_avg:50.52ms
step:1517/2090 train_time:76681ms step_avg:50.55ms
step:1518/2090 train_time:76767ms step_avg:50.57ms
step:1519/2090 train_time:76855ms step_avg:50.60ms
step:1520/2090 train_time:76941ms step_avg:50.62ms
step:1521/2090 train_time:77028ms step_avg:50.64ms
step:1522/2090 train_time:77114ms step_avg:50.67ms
step:1523/2090 train_time:77202ms step_avg:50.69ms
step:1524/2090 train_time:77290ms step_avg:50.72ms
step:1525/2090 train_time:77380ms step_avg:50.74ms
step:1526/2090 train_time:77469ms step_avg:50.77ms
step:1527/2090 train_time:77558ms step_avg:50.79ms
step:1528/2090 train_time:77645ms step_avg:50.81ms
step:1529/2090 train_time:77733ms step_avg:50.84ms
step:1530/2090 train_time:77820ms step_avg:50.86ms
step:1531/2090 train_time:77907ms step_avg:50.89ms
step:1532/2090 train_time:77993ms step_avg:50.91ms
step:1533/2090 train_time:78080ms step_avg:50.93ms
step:1534/2090 train_time:78166ms step_avg:50.96ms
step:1535/2090 train_time:78255ms step_avg:50.98ms
step:1536/2090 train_time:78343ms step_avg:51.00ms
step:1537/2090 train_time:78434ms step_avg:51.03ms
step:1538/2090 train_time:78521ms step_avg:51.05ms
step:1539/2090 train_time:78610ms step_avg:51.08ms
step:1540/2090 train_time:78697ms step_avg:51.10ms
step:1541/2090 train_time:78784ms step_avg:51.13ms
step:1542/2090 train_time:78870ms step_avg:51.15ms
step:1543/2090 train_time:78958ms step_avg:51.17ms
step:1544/2090 train_time:79045ms step_avg:51.19ms
step:1545/2090 train_time:79132ms step_avg:51.22ms
step:1546/2090 train_time:79220ms step_avg:51.24ms
step:1547/2090 train_time:79308ms step_avg:51.27ms
step:1548/2090 train_time:79396ms step_avg:51.29ms
step:1549/2090 train_time:79484ms step_avg:51.31ms
step:1550/2090 train_time:79572ms step_avg:51.34ms
step:1551/2090 train_time:79660ms step_avg:51.36ms
step:1552/2090 train_time:79747ms step_avg:51.38ms
step:1553/2090 train_time:79836ms step_avg:51.41ms
step:1554/2090 train_time:79922ms step_avg:51.43ms
step:1555/2090 train_time:80010ms step_avg:51.45ms
step:1556/2090 train_time:80097ms step_avg:51.48ms
step:1557/2090 train_time:80183ms step_avg:51.50ms
step:1558/2090 train_time:80270ms step_avg:51.52ms
step:1559/2090 train_time:80360ms step_avg:51.55ms
step:1560/2090 train_time:80447ms step_avg:51.57ms
step:1561/2090 train_time:80535ms step_avg:51.59ms
step:1562/2090 train_time:80623ms step_avg:51.62ms
step:1563/2090 train_time:80711ms step_avg:51.64ms
step:1564/2090 train_time:80798ms step_avg:51.66ms
step:1565/2090 train_time:80887ms step_avg:51.68ms
step:1566/2090 train_time:80973ms step_avg:51.71ms
step:1567/2090 train_time:81061ms step_avg:51.73ms
step:1568/2090 train_time:81147ms step_avg:51.75ms
step:1569/2090 train_time:81235ms step_avg:51.77ms
step:1570/2090 train_time:81322ms step_avg:51.80ms
step:1571/2090 train_time:81409ms step_avg:51.82ms
step:1572/2090 train_time:81497ms step_avg:51.84ms
step:1573/2090 train_time:81585ms step_avg:51.87ms
step:1574/2090 train_time:81672ms step_avg:51.89ms
step:1575/2090 train_time:81760ms step_avg:51.91ms
step:1576/2090 train_time:81847ms step_avg:51.93ms
step:1577/2090 train_time:81935ms step_avg:51.96ms
step:1578/2090 train_time:82021ms step_avg:51.98ms
step:1579/2090 train_time:82109ms step_avg:52.00ms
step:1580/2090 train_time:82196ms step_avg:52.02ms
step:1581/2090 train_time:82284ms step_avg:52.05ms
step:1582/2090 train_time:82371ms step_avg:52.07ms
step:1583/2090 train_time:82460ms step_avg:52.09ms
step:1584/2090 train_time:82547ms step_avg:52.11ms
step:1585/2090 train_time:82635ms step_avg:52.14ms
step:1586/2090 train_time:82722ms step_avg:52.16ms
step:1587/2090 train_time:82810ms step_avg:52.18ms
step:1588/2090 train_time:82898ms step_avg:52.20ms
step:1589/2090 train_time:82986ms step_avg:52.22ms
step:1590/2090 train_time:83072ms step_avg:52.25ms
step:1591/2090 train_time:83160ms step_avg:52.27ms
step:1592/2090 train_time:83246ms step_avg:52.29ms
step:1593/2090 train_time:83335ms step_avg:52.31ms
step:1594/2090 train_time:83422ms step_avg:52.33ms
step:1595/2090 train_time:83510ms step_avg:52.36ms
step:1596/2090 train_time:83596ms step_avg:52.38ms
step:1597/2090 train_time:83685ms step_avg:52.40ms
step:1598/2090 train_time:83772ms step_avg:52.42ms
step:1599/2090 train_time:83861ms step_avg:52.45ms
step:1600/2090 train_time:83947ms step_avg:52.47ms
step:1601/2090 train_time:84036ms step_avg:52.49ms
step:1602/2090 train_time:84122ms step_avg:52.51ms
step:1603/2090 train_time:84210ms step_avg:52.53ms
step:1604/2090 train_time:84298ms step_avg:52.55ms
step:1605/2090 train_time:84385ms step_avg:52.58ms
step:1606/2090 train_time:84472ms step_avg:52.60ms
step:1607/2090 train_time:84560ms step_avg:52.62ms
step:1608/2090 train_time:84648ms step_avg:52.64ms
step:1609/2090 train_time:84737ms step_avg:52.66ms
step:1610/2090 train_time:84824ms step_avg:52.69ms
step:1611/2090 train_time:84913ms step_avg:52.71ms
step:1612/2090 train_time:85001ms step_avg:52.73ms
step:1613/2090 train_time:85089ms step_avg:52.75ms
step:1614/2090 train_time:85176ms step_avg:52.77ms
step:1615/2090 train_time:85264ms step_avg:52.80ms
step:1616/2090 train_time:85351ms step_avg:52.82ms
step:1617/2090 train_time:85439ms step_avg:52.84ms
step:1618/2090 train_time:85525ms step_avg:52.86ms
step:1619/2090 train_time:85614ms step_avg:52.88ms
step:1620/2090 train_time:85700ms step_avg:52.90ms
step:1621/2090 train_time:85788ms step_avg:52.92ms
step:1622/2090 train_time:85875ms step_avg:52.94ms
step:1623/2090 train_time:85963ms step_avg:52.97ms
step:1624/2090 train_time:86050ms step_avg:52.99ms
step:1625/2090 train_time:86138ms step_avg:53.01ms
step:1626/2090 train_time:86225ms step_avg:53.03ms
step:1627/2090 train_time:86313ms step_avg:53.05ms
step:1628/2090 train_time:86400ms step_avg:53.07ms
step:1629/2090 train_time:86488ms step_avg:53.09ms
step:1630/2090 train_time:86574ms step_avg:53.11ms
step:1631/2090 train_time:86662ms step_avg:53.13ms
step:1632/2090 train_time:86749ms step_avg:53.15ms
step:1633/2090 train_time:86838ms step_avg:53.18ms
step:1634/2090 train_time:86924ms step_avg:53.20ms
step:1635/2090 train_time:87012ms step_avg:53.22ms
step:1636/2090 train_time:87099ms step_avg:53.24ms
step:1637/2090 train_time:87187ms step_avg:53.26ms
step:1638/2090 train_time:87274ms step_avg:53.28ms
step:1639/2090 train_time:87362ms step_avg:53.30ms
step:1640/2090 train_time:87450ms step_avg:53.32ms
step:1641/2090 train_time:87539ms step_avg:53.35ms
step:1642/2090 train_time:87625ms step_avg:53.36ms
step:1643/2090 train_time:87714ms step_avg:53.39ms
step:1644/2090 train_time:87802ms step_avg:53.41ms
step:1645/2090 train_time:87891ms step_avg:53.43ms
step:1646/2090 train_time:87977ms step_avg:53.45ms
step:1647/2090 train_time:88065ms step_avg:53.47ms
step:1648/2090 train_time:88153ms step_avg:53.49ms
step:1649/2090 train_time:88241ms step_avg:53.51ms
step:1650/2090 train_time:88329ms step_avg:53.53ms
step:1651/2090 train_time:88416ms step_avg:53.55ms
step:1652/2090 train_time:88503ms step_avg:53.57ms
step:1653/2090 train_time:88591ms step_avg:53.59ms
step:1654/2090 train_time:88678ms step_avg:53.61ms
step:1655/2090 train_time:88766ms step_avg:53.64ms
step:1656/2090 train_time:88853ms step_avg:53.66ms
step:1657/2090 train_time:88941ms step_avg:53.68ms
step:1658/2090 train_time:89029ms step_avg:53.70ms
step:1659/2090 train_time:89116ms step_avg:53.72ms
step:1660/2090 train_time:89204ms step_avg:53.74ms
step:1661/2090 train_time:89292ms step_avg:53.76ms
step:1662/2090 train_time:89378ms step_avg:53.78ms
step:1663/2090 train_time:89466ms step_avg:53.80ms
step:1664/2090 train_time:89553ms step_avg:53.82ms
step:1665/2090 train_time:89642ms step_avg:53.84ms
step:1666/2090 train_time:89729ms step_avg:53.86ms
step:1667/2090 train_time:89817ms step_avg:53.88ms
step:1668/2090 train_time:89903ms step_avg:53.90ms
step:1669/2090 train_time:89992ms step_avg:53.92ms
step:1670/2090 train_time:90078ms step_avg:53.94ms
step:1671/2090 train_time:90166ms step_avg:53.96ms
step:1672/2090 train_time:90253ms step_avg:53.98ms
step:1673/2090 train_time:90341ms step_avg:54.00ms
step:1674/2090 train_time:90428ms step_avg:54.02ms
step:1675/2090 train_time:90516ms step_avg:54.04ms
step:1676/2090 train_time:90603ms step_avg:54.06ms
step:1677/2090 train_time:90691ms step_avg:54.08ms
step:1678/2090 train_time:90778ms step_avg:54.10ms
step:1679/2090 train_time:90866ms step_avg:54.12ms
step:1680/2090 train_time:90953ms step_avg:54.14ms
step:1681/2090 train_time:91041ms step_avg:54.16ms
step:1682/2090 train_time:91127ms step_avg:54.18ms
step:1683/2090 train_time:91215ms step_avg:54.20ms
step:1684/2090 train_time:91302ms step_avg:54.22ms
step:1685/2090 train_time:91391ms step_avg:54.24ms
step:1686/2090 train_time:91478ms step_avg:54.26ms
step:1687/2090 train_time:91566ms step_avg:54.28ms
step:1688/2090 train_time:91653ms step_avg:54.30ms
step:1689/2090 train_time:91742ms step_avg:54.32ms
step:1690/2090 train_time:91829ms step_avg:54.34ms
step:1691/2090 train_time:91917ms step_avg:54.36ms
step:1692/2090 train_time:92004ms step_avg:54.38ms
step:1693/2090 train_time:92092ms step_avg:54.40ms
step:1694/2090 train_time:92179ms step_avg:54.41ms
step:1695/2090 train_time:92267ms step_avg:54.43ms
step:1696/2090 train_time:92354ms step_avg:54.45ms
step:1697/2090 train_time:92442ms step_avg:54.47ms
step:1698/2090 train_time:92529ms step_avg:54.49ms
step:1699/2090 train_time:92617ms step_avg:54.51ms
step:1700/2090 train_time:92705ms step_avg:54.53ms
step:1701/2090 train_time:92794ms step_avg:54.55ms
step:1702/2090 train_time:92880ms step_avg:54.57ms
step:1703/2090 train_time:92968ms step_avg:54.59ms
step:1704/2090 train_time:93055ms step_avg:54.61ms
step:1705/2090 train_time:93143ms step_avg:54.63ms
step:1706/2090 train_time:93230ms step_avg:54.65ms
step:1707/2090 train_time:93318ms step_avg:54.67ms
step:1708/2090 train_time:93404ms step_avg:54.69ms
step:1709/2090 train_time:93493ms step_avg:54.71ms
step:1710/2090 train_time:93579ms step_avg:54.72ms
step:1711/2090 train_time:93667ms step_avg:54.74ms
step:1712/2090 train_time:93754ms step_avg:54.76ms
step:1713/2090 train_time:93842ms step_avg:54.78ms
step:1714/2090 train_time:93930ms step_avg:54.80ms
step:1715/2090 train_time:94018ms step_avg:54.82ms
step:1716/2090 train_time:94105ms step_avg:54.84ms
step:1717/2090 train_time:94193ms step_avg:54.86ms
step:1718/2090 train_time:94280ms step_avg:54.88ms
step:1719/2090 train_time:94368ms step_avg:54.90ms
step:1720/2090 train_time:94455ms step_avg:54.92ms
step:1721/2090 train_time:94542ms step_avg:54.93ms
step:1722/2090 train_time:94630ms step_avg:54.95ms
step:1723/2090 train_time:94718ms step_avg:54.97ms
step:1724/2090 train_time:94804ms step_avg:54.99ms
step:1725/2090 train_time:94893ms step_avg:55.01ms
step:1726/2090 train_time:94980ms step_avg:55.03ms
step:1727/2090 train_time:95067ms step_avg:55.05ms
step:1728/2090 train_time:95155ms step_avg:55.07ms
step:1729/2090 train_time:95242ms step_avg:55.09ms
step:1730/2090 train_time:95329ms step_avg:55.10ms
step:1731/2090 train_time:95417ms step_avg:55.12ms
step:1732/2090 train_time:95504ms step_avg:55.14ms
step:1733/2090 train_time:95593ms step_avg:55.16ms
step:1734/2090 train_time:95681ms step_avg:55.18ms
step:1735/2090 train_time:95769ms step_avg:55.20ms
step:1736/2090 train_time:95857ms step_avg:55.22ms
step:1737/2090 train_time:95944ms step_avg:55.24ms
step:1738/2090 train_time:96032ms step_avg:55.25ms
step:1739/2090 train_time:96120ms step_avg:55.27ms
step:1740/2090 train_time:96206ms step_avg:55.29ms
step:1741/2090 train_time:96294ms step_avg:55.31ms
step:1742/2090 train_time:96381ms step_avg:55.33ms
step:1743/2090 train_time:96469ms step_avg:55.35ms
step:1744/2090 train_time:96557ms step_avg:55.36ms
step:1745/2090 train_time:96644ms step_avg:55.38ms
step:1746/2090 train_time:96731ms step_avg:55.40ms
step:1747/2090 train_time:96820ms step_avg:55.42ms
step:1748/2090 train_time:96906ms step_avg:55.44ms
step:1749/2090 train_time:96994ms step_avg:55.46ms
step:1750/2090 train_time:97081ms step_avg:55.47ms
step:1750/2090 val_loss:3.3751 train_time:97171ms step_avg:55.53ms
step:1751/2090 train_time:97191ms step_avg:55.51ms
step:1752/2090 train_time:97261ms step_avg:55.51ms
step:1753/2090 train_time:97356ms step_avg:55.54ms
step:1754/2090 train_time:97444ms step_avg:55.56ms
step:1755/2090 train_time:97531ms step_avg:55.57ms
step:1756/2090 train_time:97617ms step_avg:55.59ms
step:1757/2090 train_time:97704ms step_avg:55.61ms
step:1758/2090 train_time:97790ms step_avg:55.63ms
step:1759/2090 train_time:97876ms step_avg:55.64ms
step:1760/2090 train_time:97962ms step_avg:55.66ms
step:1761/2090 train_time:98049ms step_avg:55.68ms
step:1762/2090 train_time:98137ms step_avg:55.70ms
step:1763/2090 train_time:98226ms step_avg:55.72ms
step:1764/2090 train_time:98316ms step_avg:55.73ms
step:1765/2090 train_time:98407ms step_avg:55.75ms
step:1766/2090 train_time:98494ms step_avg:55.77ms
step:1767/2090 train_time:98582ms step_avg:55.79ms
step:1768/2090 train_time:98669ms step_avg:55.81ms
step:1769/2090 train_time:98755ms step_avg:55.83ms
step:1770/2090 train_time:98842ms step_avg:55.84ms
step:1771/2090 train_time:98929ms step_avg:55.86ms
step:1772/2090 train_time:99015ms step_avg:55.88ms
step:1773/2090 train_time:99102ms step_avg:55.90ms
step:1774/2090 train_time:99189ms step_avg:55.91ms
step:1775/2090 train_time:99280ms step_avg:55.93ms
step:1776/2090 train_time:99368ms step_avg:55.95ms
step:1777/2090 train_time:99457ms step_avg:55.97ms
step:1778/2090 train_time:99545ms step_avg:55.99ms
step:1779/2090 train_time:99633ms step_avg:56.00ms
step:1780/2090 train_time:99719ms step_avg:56.02ms
step:1781/2090 train_time:99806ms step_avg:56.04ms
step:1782/2090 train_time:99892ms step_avg:56.06ms
step:1783/2090 train_time:99979ms step_avg:56.07ms
step:1784/2090 train_time:100065ms step_avg:56.09ms
step:1785/2090 train_time:100154ms step_avg:56.11ms
step:1786/2090 train_time:100241ms step_avg:56.13ms
step:1787/2090 train_time:100330ms step_avg:56.14ms
step:1788/2090 train_time:100418ms step_avg:56.16ms
step:1789/2090 train_time:100507ms step_avg:56.18ms
step:1790/2090 train_time:100594ms step_avg:56.20ms
step:1791/2090 train_time:100682ms step_avg:56.22ms
step:1792/2090 train_time:100768ms step_avg:56.23ms
step:1793/2090 train_time:100855ms step_avg:56.25ms
step:1794/2090 train_time:100941ms step_avg:56.27ms
step:1795/2090 train_time:101028ms step_avg:56.28ms
step:1796/2090 train_time:101115ms step_avg:56.30ms
step:1797/2090 train_time:101203ms step_avg:56.32ms
step:1798/2090 train_time:101292ms step_avg:56.34ms
step:1799/2090 train_time:101381ms step_avg:56.35ms
step:1800/2090 train_time:101469ms step_avg:56.37ms
step:1801/2090 train_time:101557ms step_avg:56.39ms
step:1802/2090 train_time:101644ms step_avg:56.41ms
step:1803/2090 train_time:101733ms step_avg:56.42ms
step:1804/2090 train_time:101819ms step_avg:56.44ms
step:1805/2090 train_time:101907ms step_avg:56.46ms
step:1806/2090 train_time:101993ms step_avg:56.47ms
step:1807/2090 train_time:102080ms step_avg:56.49ms
step:1808/2090 train_time:102168ms step_avg:56.51ms
step:1809/2090 train_time:102255ms step_avg:56.53ms
step:1810/2090 train_time:102342ms step_avg:56.54ms
step:1811/2090 train_time:102432ms step_avg:56.56ms
step:1812/2090 train_time:102520ms step_avg:56.58ms
step:1813/2090 train_time:102607ms step_avg:56.60ms
step:1814/2090 train_time:102694ms step_avg:56.61ms
step:1815/2090 train_time:102782ms step_avg:56.63ms
step:1816/2090 train_time:102869ms step_avg:56.65ms
step:1817/2090 train_time:102956ms step_avg:56.66ms
step:1818/2090 train_time:103043ms step_avg:56.68ms
step:1819/2090 train_time:103131ms step_avg:56.70ms
step:1820/2090 train_time:103218ms step_avg:56.71ms
step:1821/2090 train_time:103306ms step_avg:56.73ms
step:1822/2090 train_time:103393ms step_avg:56.75ms
step:1823/2090 train_time:103482ms step_avg:56.76ms
step:1824/2090 train_time:103569ms step_avg:56.78ms
step:1825/2090 train_time:103656ms step_avg:56.80ms
step:1826/2090 train_time:103744ms step_avg:56.81ms
step:1827/2090 train_time:103832ms step_avg:56.83ms
step:1828/2090 train_time:103919ms step_avg:56.85ms
step:1829/2090 train_time:104006ms step_avg:56.86ms
step:1830/2090 train_time:104093ms step_avg:56.88ms
step:1831/2090 train_time:104181ms step_avg:56.90ms
step:1832/2090 train_time:104268ms step_avg:56.91ms
step:1833/2090 train_time:104357ms step_avg:56.93ms
step:1834/2090 train_time:104445ms step_avg:56.95ms
step:1835/2090 train_time:104533ms step_avg:56.97ms
step:1836/2090 train_time:104620ms step_avg:56.98ms
step:1837/2090 train_time:104708ms step_avg:57.00ms
step:1838/2090 train_time:104794ms step_avg:57.02ms
step:1839/2090 train_time:104882ms step_avg:57.03ms
step:1840/2090 train_time:104968ms step_avg:57.05ms
step:1841/2090 train_time:105056ms step_avg:57.06ms
step:1842/2090 train_time:105142ms step_avg:57.08ms
step:1843/2090 train_time:105231ms step_avg:57.10ms
step:1844/2090 train_time:105319ms step_avg:57.11ms
step:1845/2090 train_time:105407ms step_avg:57.13ms
step:1846/2090 train_time:105494ms step_avg:57.15ms
step:1847/2090 train_time:105582ms step_avg:57.16ms
step:1848/2090 train_time:105669ms step_avg:57.18ms
step:1849/2090 train_time:105757ms step_avg:57.20ms
step:1850/2090 train_time:105844ms step_avg:57.21ms
step:1851/2090 train_time:105932ms step_avg:57.23ms
step:1852/2090 train_time:106018ms step_avg:57.25ms
step:1853/2090 train_time:106106ms step_avg:57.26ms
step:1854/2090 train_time:106193ms step_avg:57.28ms
step:1855/2090 train_time:106281ms step_avg:57.29ms
step:1856/2090 train_time:106368ms step_avg:57.31ms
step:1857/2090 train_time:106457ms step_avg:57.33ms
step:1858/2090 train_time:106545ms step_avg:57.34ms
step:1859/2090 train_time:106633ms step_avg:57.36ms
step:1860/2090 train_time:106720ms step_avg:57.38ms
step:1861/2090 train_time:106807ms step_avg:57.39ms
step:1862/2090 train_time:106894ms step_avg:57.41ms
step:1863/2090 train_time:106982ms step_avg:57.42ms
step:1864/2090 train_time:107068ms step_avg:57.44ms
step:1865/2090 train_time:107156ms step_avg:57.46ms
step:1866/2090 train_time:107243ms step_avg:57.47ms
step:1867/2090 train_time:107331ms step_avg:57.49ms
step:1868/2090 train_time:107418ms step_avg:57.50ms
step:1869/2090 train_time:107506ms step_avg:57.52ms
step:1870/2090 train_time:107593ms step_avg:57.54ms
step:1871/2090 train_time:107681ms step_avg:57.55ms
step:1872/2090 train_time:107768ms step_avg:57.57ms
step:1873/2090 train_time:107856ms step_avg:57.58ms
step:1874/2090 train_time:107943ms step_avg:57.60ms
step:1875/2090 train_time:108031ms step_avg:57.62ms
step:1876/2090 train_time:108118ms step_avg:57.63ms
step:1877/2090 train_time:108205ms step_avg:57.65ms
step:1878/2090 train_time:108292ms step_avg:57.66ms
step:1879/2090 train_time:108380ms step_avg:57.68ms
step:1880/2090 train_time:108468ms step_avg:57.70ms
step:1881/2090 train_time:108556ms step_avg:57.71ms
step:1882/2090 train_time:108644ms step_avg:57.73ms
step:1883/2090 train_time:108733ms step_avg:57.74ms
step:1884/2090 train_time:108821ms step_avg:57.76ms
step:1885/2090 train_time:108909ms step_avg:57.78ms
step:1886/2090 train_time:108996ms step_avg:57.79ms
step:1887/2090 train_time:109085ms step_avg:57.81ms
step:1888/2090 train_time:109171ms step_avg:57.82ms
step:1889/2090 train_time:109259ms step_avg:57.84ms
step:1890/2090 train_time:109346ms step_avg:57.85ms
step:1891/2090 train_time:109434ms step_avg:57.87ms
step:1892/2090 train_time:109521ms step_avg:57.89ms
step:1893/2090 train_time:109609ms step_avg:57.90ms
step:1894/2090 train_time:109697ms step_avg:57.92ms
step:1895/2090 train_time:109785ms step_avg:57.93ms
step:1896/2090 train_time:109871ms step_avg:57.95ms
step:1897/2090 train_time:109960ms step_avg:57.97ms
step:1898/2090 train_time:110047ms step_avg:57.98ms
step:1899/2090 train_time:110135ms step_avg:58.00ms
step:1900/2090 train_time:110221ms step_avg:58.01ms
step:1901/2090 train_time:110309ms step_avg:58.03ms
step:1902/2090 train_time:110396ms step_avg:58.04ms
step:1903/2090 train_time:110484ms step_avg:58.06ms
step:1904/2090 train_time:110571ms step_avg:58.07ms
step:1905/2090 train_time:110660ms step_avg:58.09ms
step:1906/2090 train_time:110747ms step_avg:58.10ms
step:1907/2090 train_time:110835ms step_avg:58.12ms
step:1908/2090 train_time:110923ms step_avg:58.14ms
step:1909/2090 train_time:111012ms step_avg:58.15ms
step:1910/2090 train_time:111099ms step_avg:58.17ms
step:1911/2090 train_time:111186ms step_avg:58.18ms
step:1912/2090 train_time:111272ms step_avg:58.20ms
step:1913/2090 train_time:111360ms step_avg:58.21ms
step:1914/2090 train_time:111447ms step_avg:58.23ms
step:1915/2090 train_time:111535ms step_avg:58.24ms
step:1916/2090 train_time:111623ms step_avg:58.26ms
step:1917/2090 train_time:111712ms step_avg:58.27ms
step:1918/2090 train_time:111798ms step_avg:58.29ms
step:1919/2090 train_time:111886ms step_avg:58.30ms
step:1920/2090 train_time:111972ms step_avg:58.32ms
step:1921/2090 train_time:112061ms step_avg:58.33ms
step:1922/2090 train_time:112148ms step_avg:58.35ms
step:1923/2090 train_time:112236ms step_avg:58.36ms
step:1924/2090 train_time:112322ms step_avg:58.38ms
step:1925/2090 train_time:112410ms step_avg:58.39ms
step:1926/2090 train_time:112497ms step_avg:58.41ms
step:1927/2090 train_time:112585ms step_avg:58.42ms
step:1928/2090 train_time:112672ms step_avg:58.44ms
step:1929/2090 train_time:112760ms step_avg:58.46ms
step:1930/2090 train_time:112847ms step_avg:58.47ms
step:1931/2090 train_time:112934ms step_avg:58.48ms
step:1932/2090 train_time:113021ms step_avg:58.50ms
step:1933/2090 train_time:113109ms step_avg:58.51ms
step:1934/2090 train_time:113196ms step_avg:58.53ms
step:1935/2090 train_time:113283ms step_avg:58.54ms
step:1936/2090 train_time:113371ms step_avg:58.56ms
step:1937/2090 train_time:113459ms step_avg:58.57ms
step:1938/2090 train_time:113545ms step_avg:58.59ms
step:1939/2090 train_time:113635ms step_avg:58.60ms
step:1940/2090 train_time:113721ms step_avg:58.62ms
step:1941/2090 train_time:113809ms step_avg:58.63ms
step:1942/2090 train_time:113896ms step_avg:58.65ms
step:1943/2090 train_time:113983ms step_avg:58.66ms
step:1944/2090 train_time:114071ms step_avg:58.68ms
step:1945/2090 train_time:114159ms step_avg:58.69ms
step:1946/2090 train_time:114245ms step_avg:58.71ms
step:1947/2090 train_time:114333ms step_avg:58.72ms
step:1948/2090 train_time:114420ms step_avg:58.74ms
step:1949/2090 train_time:114509ms step_avg:58.75ms
step:1950/2090 train_time:114596ms step_avg:58.77ms
step:1951/2090 train_time:114686ms step_avg:58.78ms
step:1952/2090 train_time:114772ms step_avg:58.80ms
step:1953/2090 train_time:114860ms step_avg:58.81ms
step:1954/2090 train_time:114947ms step_avg:58.83ms
step:1955/2090 train_time:115035ms step_avg:58.84ms
step:1956/2090 train_time:115122ms step_avg:58.86ms
step:1957/2090 train_time:115210ms step_avg:58.87ms
step:1958/2090 train_time:115298ms step_avg:58.89ms
step:1959/2090 train_time:115385ms step_avg:58.90ms
step:1960/2090 train_time:115472ms step_avg:58.91ms
step:1961/2090 train_time:115560ms step_avg:58.93ms
step:1962/2090 train_time:115648ms step_avg:58.94ms
step:1963/2090 train_time:115736ms step_avg:58.96ms
step:1964/2090 train_time:115823ms step_avg:58.97ms
step:1965/2090 train_time:115911ms step_avg:58.99ms
step:1966/2090 train_time:115998ms step_avg:59.00ms
step:1967/2090 train_time:116085ms step_avg:59.02ms
step:1968/2090 train_time:116172ms step_avg:59.03ms
step:1969/2090 train_time:116260ms step_avg:59.05ms
step:1970/2090 train_time:116348ms step_avg:59.06ms
step:1971/2090 train_time:116436ms step_avg:59.07ms
step:1972/2090 train_time:116523ms step_avg:59.09ms
step:1973/2090 train_time:116612ms step_avg:59.10ms
step:1974/2090 train_time:116698ms step_avg:59.12ms
step:1975/2090 train_time:116787ms step_avg:59.13ms
step:1976/2090 train_time:116874ms step_avg:59.15ms
step:1977/2090 train_time:116962ms step_avg:59.16ms
step:1978/2090 train_time:117049ms step_avg:59.18ms
step:1979/2090 train_time:117138ms step_avg:59.19ms
step:1980/2090 train_time:117225ms step_avg:59.20ms
step:1981/2090 train_time:117313ms step_avg:59.22ms
step:1982/2090 train_time:117400ms step_avg:59.23ms
step:1983/2090 train_time:117487ms step_avg:59.25ms
step:1984/2090 train_time:117575ms step_avg:59.26ms
step:1985/2090 train_time:117662ms step_avg:59.28ms
step:1986/2090 train_time:117750ms step_avg:59.29ms
step:1987/2090 train_time:117837ms step_avg:59.30ms
step:1988/2090 train_time:117924ms step_avg:59.32ms
step:1989/2090 train_time:118012ms step_avg:59.33ms
step:1990/2090 train_time:118098ms step_avg:59.35ms
step:1991/2090 train_time:118187ms step_avg:59.36ms
step:1992/2090 train_time:118273ms step_avg:59.37ms
step:1993/2090 train_time:118361ms step_avg:59.39ms
step:1994/2090 train_time:118448ms step_avg:59.40ms
step:1995/2090 train_time:118536ms step_avg:59.42ms
step:1996/2090 train_time:118623ms step_avg:59.43ms
step:1997/2090 train_time:118711ms step_avg:59.44ms
step:1998/2090 train_time:118798ms step_avg:59.46ms
step:1999/2090 train_time:118886ms step_avg:59.47ms
step:2000/2090 train_time:118973ms step_avg:59.49ms
step:2000/2090 val_loss:3.2981 train_time:119063ms step_avg:59.53ms
step:2001/2090 train_time:119083ms step_avg:59.51ms
step:2002/2090 train_time:119155ms step_avg:59.52ms
step:2003/2090 train_time:119248ms step_avg:59.53ms
step:2004/2090 train_time:119336ms step_avg:59.55ms
step:2005/2090 train_time:119423ms step_avg:59.56ms
step:2006/2090 train_time:119509ms step_avg:59.58ms
step:2007/2090 train_time:119595ms step_avg:59.59ms
step:2008/2090 train_time:119682ms step_avg:59.60ms
step:2009/2090 train_time:119769ms step_avg:59.62ms
step:2010/2090 train_time:119855ms step_avg:59.63ms
step:2011/2090 train_time:119941ms step_avg:59.64ms
step:2012/2090 train_time:120029ms step_avg:59.66ms
step:2013/2090 train_time:120120ms step_avg:59.67ms
step:2014/2090 train_time:120211ms step_avg:59.69ms
step:2015/2090 train_time:120301ms step_avg:59.70ms
step:2016/2090 train_time:120388ms step_avg:59.72ms
step:2017/2090 train_time:120476ms step_avg:59.73ms
step:2018/2090 train_time:120562ms step_avg:59.74ms
step:2019/2090 train_time:120649ms step_avg:59.76ms
step:2020/2090 train_time:120735ms step_avg:59.77ms
step:2021/2090 train_time:120822ms step_avg:59.78ms
step:2022/2090 train_time:120908ms step_avg:59.80ms
step:2023/2090 train_time:120996ms step_avg:59.81ms
step:2024/2090 train_time:121085ms step_avg:59.82ms
step:2025/2090 train_time:121175ms step_avg:59.84ms
step:2026/2090 train_time:121262ms step_avg:59.85ms
step:2027/2090 train_time:121351ms step_avg:59.87ms
step:2028/2090 train_time:121439ms step_avg:59.88ms
step:2029/2090 train_time:121526ms step_avg:59.89ms
step:2030/2090 train_time:121613ms step_avg:59.91ms
step:2031/2090 train_time:121700ms step_avg:59.92ms
step:2032/2090 train_time:121786ms step_avg:59.93ms
step:2033/2090 train_time:121874ms step_avg:59.95ms
step:2034/2090 train_time:121960ms step_avg:59.96ms
step:2035/2090 train_time:122050ms step_avg:59.98ms
step:2036/2090 train_time:122138ms step_avg:59.99ms
step:2037/2090 train_time:122228ms step_avg:60.00ms
step:2038/2090 train_time:122315ms step_avg:60.02ms
step:2039/2090 train_time:122403ms step_avg:60.03ms
step:2040/2090 train_time:122490ms step_avg:60.04ms
step:2041/2090 train_time:122578ms step_avg:60.06ms
step:2042/2090 train_time:122664ms step_avg:60.07ms
step:2043/2090 train_time:122751ms step_avg:60.08ms
step:2044/2090 train_time:122838ms step_avg:60.10ms
step:2045/2090 train_time:122926ms step_avg:60.11ms
step:2046/2090 train_time:123013ms step_avg:60.12ms
step:2047/2090 train_time:123101ms step_avg:60.14ms
step:2048/2090 train_time:123190ms step_avg:60.15ms
step:2049/2090 train_time:123279ms step_avg:60.17ms
step:2050/2090 train_time:123366ms step_avg:60.18ms
step:2051/2090 train_time:123454ms step_avg:60.19ms
step:2052/2090 train_time:123541ms step_avg:60.21ms
step:2053/2090 train_time:123629ms step_avg:60.22ms
step:2054/2090 train_time:123716ms step_avg:60.23ms
step:2055/2090 train_time:123804ms step_avg:60.25ms
step:2056/2090 train_time:123891ms step_avg:60.26ms
step:2057/2090 train_time:123980ms step_avg:60.27ms
step:2058/2090 train_time:124066ms step_avg:60.28ms
step:2059/2090 train_time:124155ms step_avg:60.30ms
step:2060/2090 train_time:124242ms step_avg:60.31ms
step:2061/2090 train_time:124331ms step_avg:60.33ms
step:2062/2090 train_time:124419ms step_avg:60.34ms
step:2063/2090 train_time:124507ms step_avg:60.35ms
step:2064/2090 train_time:124595ms step_avg:60.37ms
step:2065/2090 train_time:124683ms step_avg:60.38ms
step:2066/2090 train_time:124770ms step_avg:60.39ms
step:2067/2090 train_time:124858ms step_avg:60.41ms
step:2068/2090 train_time:124946ms step_avg:60.42ms
step:2069/2090 train_time:125035ms step_avg:60.43ms
step:2070/2090 train_time:125122ms step_avg:60.45ms
step:2071/2090 train_time:125211ms step_avg:60.46ms
step:2072/2090 train_time:125299ms step_avg:60.47ms
step:2073/2090 train_time:125387ms step_avg:60.49ms
step:2074/2090 train_time:125475ms step_avg:60.50ms
step:2075/2090 train_time:125564ms step_avg:60.51ms
step:2076/2090 train_time:125651ms step_avg:60.53ms
step:2077/2090 train_time:125738ms step_avg:60.54ms
step:2078/2090 train_time:125826ms step_avg:60.55ms
step:2079/2090 train_time:125914ms step_avg:60.56ms
step:2080/2090 train_time:126001ms step_avg:60.58ms
step:2081/2090 train_time:126089ms step_avg:60.59ms
step:2082/2090 train_time:126177ms step_avg:60.60ms
step:2083/2090 train_time:126266ms step_avg:60.62ms
step:2084/2090 train_time:126354ms step_avg:60.63ms
step:2085/2090 train_time:126442ms step_avg:60.64ms
step:2086/2090 train_time:126530ms step_avg:60.66ms
step:2087/2090 train_time:126618ms step_avg:60.67ms
step:2088/2090 train_time:126705ms step_avg:60.68ms
step:2089/2090 train_time:126793ms step_avg:60.70ms
step:2090/2090 train_time:126880ms step_avg:60.71ms
step:2090/2090 val_loss:3.2773 train_time:126971ms step_avg:60.75ms
peak memory allocated: 29892 MiB reserved: 44496 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # weight decay
                if wd != 0:
                    mask = (update * p_slice) >= 0
                    # lr as weight decay  schedule
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)

                    update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)
                
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2050  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 18 12:29:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2090 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2090 train_time:76ms step_avg:75.79ms
step:2/2090 train_time:99ms step_avg:49.31ms
step:3/2090 train_time:122ms step_avg:40.80ms
step:4/2090 train_time:155ms step_avg:38.65ms
step:5/2090 train_time:187ms step_avg:37.42ms
step:6/2090 train_time:264ms step_avg:44.05ms
step:7/2090 train_time:297ms step_avg:42.42ms
step:8/2090 train_time:330ms step_avg:41.23ms
step:9/2090 train_time:363ms step_avg:40.31ms
step:10/2090 train_time:396ms step_avg:39.55ms
step:11/2090 train_time:429ms step_avg:39.04ms
step:12/2090 train_time:462ms step_avg:38.51ms
step:13/2090 train_time:496ms step_avg:38.13ms
step:14/2090 train_time:529ms step_avg:37.77ms
step:15/2090 train_time:562ms step_avg:37.48ms
step:16/2090 train_time:595ms step_avg:37.19ms
step:17/2090 train_time:629ms step_avg:37.00ms
step:18/2090 train_time:662ms step_avg:36.77ms
step:19/2090 train_time:696ms step_avg:36.61ms
step:20/2090 train_time:729ms step_avg:36.43ms
step:21/2090 train_time:762ms step_avg:36.30ms
step:22/2090 train_time:795ms step_avg:36.15ms
step:23/2090 train_time:829ms step_avg:36.05ms
step:24/2090 train_time:862ms step_avg:35.92ms
step:25/2090 train_time:896ms step_avg:35.82ms
step:26/2090 train_time:928ms step_avg:35.71ms
step:27/2090 train_time:962ms step_avg:35.63ms
step:28/2090 train_time:995ms step_avg:35.53ms
step:29/2090 train_time:1029ms step_avg:35.47ms
step:30/2090 train_time:1061ms step_avg:35.38ms
step:31/2090 train_time:1095ms step_avg:35.32ms
step:32/2090 train_time:1128ms step_avg:35.24ms
step:33/2090 train_time:1162ms step_avg:35.20ms
step:34/2090 train_time:1195ms step_avg:35.15ms
step:35/2090 train_time:1230ms step_avg:35.15ms
step:36/2090 train_time:1263ms step_avg:35.09ms
step:37/2090 train_time:1298ms step_avg:35.07ms
step:38/2090 train_time:1331ms step_avg:35.02ms
step:39/2090 train_time:1365ms step_avg:35.00ms
step:40/2090 train_time:1398ms step_avg:34.95ms
step:41/2090 train_time:1432ms step_avg:34.92ms
step:42/2090 train_time:1465ms step_avg:34.87ms
step:43/2090 train_time:1498ms step_avg:34.84ms
step:44/2090 train_time:1531ms step_avg:34.79ms
step:45/2090 train_time:1565ms step_avg:34.77ms
step:46/2090 train_time:1597ms step_avg:34.73ms
step:47/2090 train_time:1631ms step_avg:34.70ms
step:48/2090 train_time:1664ms step_avg:34.66ms
step:49/2090 train_time:1698ms step_avg:34.64ms
step:50/2090 train_time:1730ms step_avg:34.61ms
step:51/2090 train_time:1764ms step_avg:34.59ms
step:52/2090 train_time:1797ms step_avg:34.56ms
step:53/2090 train_time:1831ms step_avg:34.54ms
step:54/2090 train_time:1864ms step_avg:34.51ms
step:55/2090 train_time:1898ms step_avg:34.50ms
step:56/2090 train_time:1930ms step_avg:34.47ms
step:57/2090 train_time:1964ms step_avg:34.46ms
step:58/2090 train_time:1997ms step_avg:34.43ms
step:59/2090 train_time:2031ms step_avg:34.42ms
step:60/2090 train_time:2063ms step_avg:34.39ms
step:61/2090 train_time:2097ms step_avg:34.37ms
step:62/2090 train_time:2130ms step_avg:34.35ms
step:63/2090 train_time:2163ms step_avg:34.34ms
step:64/2090 train_time:2196ms step_avg:34.32ms
step:65/2090 train_time:2230ms step_avg:34.31ms
step:66/2090 train_time:2263ms step_avg:34.28ms
step:67/2090 train_time:2297ms step_avg:34.28ms
step:68/2090 train_time:2330ms step_avg:34.26ms
step:69/2090 train_time:2363ms step_avg:34.25ms
step:70/2090 train_time:2396ms step_avg:34.23ms
step:71/2090 train_time:2430ms step_avg:34.23ms
step:72/2090 train_time:2463ms step_avg:34.21ms
step:73/2090 train_time:2497ms step_avg:34.20ms
step:74/2090 train_time:2529ms step_avg:34.18ms
step:75/2090 train_time:2563ms step_avg:34.18ms
step:76/2090 train_time:2596ms step_avg:34.16ms
step:77/2090 train_time:2630ms step_avg:34.16ms
step:78/2090 train_time:2663ms step_avg:34.14ms
step:79/2090 train_time:2696ms step_avg:34.13ms
step:80/2090 train_time:2729ms step_avg:34.12ms
step:81/2090 train_time:2762ms step_avg:34.10ms
step:82/2090 train_time:2795ms step_avg:34.09ms
step:83/2090 train_time:2829ms step_avg:34.09ms
step:84/2090 train_time:2862ms step_avg:34.07ms
step:85/2090 train_time:2895ms step_avg:34.06ms
step:86/2090 train_time:2928ms step_avg:34.05ms
step:87/2090 train_time:2962ms step_avg:34.05ms
step:88/2090 train_time:2995ms step_avg:34.03ms
step:89/2090 train_time:3029ms step_avg:34.03ms
step:90/2090 train_time:3062ms step_avg:34.02ms
step:91/2090 train_time:3095ms step_avg:34.01ms
step:92/2090 train_time:3128ms step_avg:34.00ms
step:93/2090 train_time:3161ms step_avg:33.99ms
step:94/2090 train_time:3194ms step_avg:33.98ms
step:95/2090 train_time:3228ms step_avg:33.98ms
step:96/2090 train_time:3261ms step_avg:33.97ms
step:97/2090 train_time:3295ms step_avg:33.97ms
step:98/2090 train_time:3328ms step_avg:33.95ms
step:99/2090 train_time:3361ms step_avg:33.95ms
step:100/2090 train_time:3394ms step_avg:33.94ms
step:101/2090 train_time:3428ms step_avg:33.94ms
step:102/2090 train_time:3461ms step_avg:33.93ms
step:103/2090 train_time:3494ms step_avg:33.92ms
step:104/2090 train_time:3527ms step_avg:33.91ms
step:105/2090 train_time:3561ms step_avg:33.91ms
step:106/2090 train_time:3594ms step_avg:33.90ms
step:107/2090 train_time:3628ms step_avg:33.91ms
step:108/2090 train_time:3661ms step_avg:33.90ms
step:109/2090 train_time:3695ms step_avg:33.90ms
step:110/2090 train_time:3728ms step_avg:33.89ms
step:111/2090 train_time:3761ms step_avg:33.88ms
step:112/2090 train_time:3794ms step_avg:33.87ms
step:113/2090 train_time:3827ms step_avg:33.87ms
step:114/2090 train_time:3860ms step_avg:33.86ms
step:115/2090 train_time:3894ms step_avg:33.86ms
step:116/2090 train_time:3927ms step_avg:33.85ms
step:117/2090 train_time:3961ms step_avg:33.85ms
step:118/2090 train_time:3993ms step_avg:33.84ms
step:119/2090 train_time:4027ms step_avg:33.84ms
step:120/2090 train_time:4060ms step_avg:33.83ms
step:121/2090 train_time:4093ms step_avg:33.83ms
step:122/2090 train_time:4126ms step_avg:33.82ms
step:123/2090 train_time:4159ms step_avg:33.81ms
step:124/2090 train_time:4192ms step_avg:33.80ms
step:125/2090 train_time:4226ms step_avg:33.80ms
step:126/2090 train_time:4258ms step_avg:33.79ms
step:127/2090 train_time:4292ms step_avg:33.79ms
step:128/2090 train_time:4325ms step_avg:33.79ms
step:129/2090 train_time:4358ms step_avg:33.78ms
step:130/2090 train_time:4391ms step_avg:33.78ms
step:131/2090 train_time:4425ms step_avg:33.78ms
step:132/2090 train_time:4457ms step_avg:33.77ms
step:133/2090 train_time:4491ms step_avg:33.77ms
step:134/2090 train_time:4524ms step_avg:33.76ms
step:135/2090 train_time:4557ms step_avg:33.76ms
step:136/2090 train_time:4590ms step_avg:33.75ms
step:137/2090 train_time:4623ms step_avg:33.75ms
step:138/2090 train_time:4656ms step_avg:33.74ms
step:139/2090 train_time:4690ms step_avg:33.74ms
step:140/2090 train_time:4722ms step_avg:33.73ms
step:141/2090 train_time:4756ms step_avg:33.73ms
step:142/2090 train_time:4789ms step_avg:33.72ms
step:143/2090 train_time:4823ms step_avg:33.72ms
step:144/2090 train_time:4855ms step_avg:33.72ms
step:145/2090 train_time:4889ms step_avg:33.71ms
step:146/2090 train_time:4922ms step_avg:33.71ms
step:147/2090 train_time:4955ms step_avg:33.71ms
step:148/2090 train_time:4988ms step_avg:33.70ms
step:149/2090 train_time:5021ms step_avg:33.70ms
step:150/2090 train_time:5054ms step_avg:33.69ms
step:151/2090 train_time:5087ms step_avg:33.69ms
step:152/2090 train_time:5120ms step_avg:33.68ms
step:153/2090 train_time:5153ms step_avg:33.68ms
step:154/2090 train_time:5186ms step_avg:33.68ms
step:155/2090 train_time:5220ms step_avg:33.67ms
step:156/2090 train_time:5252ms step_avg:33.67ms
step:157/2090 train_time:5286ms step_avg:33.67ms
step:158/2090 train_time:5318ms step_avg:33.66ms
step:159/2090 train_time:5352ms step_avg:33.66ms
step:160/2090 train_time:5385ms step_avg:33.66ms
step:161/2090 train_time:5418ms step_avg:33.65ms
step:162/2090 train_time:5451ms step_avg:33.65ms
step:163/2090 train_time:5485ms step_avg:33.65ms
step:164/2090 train_time:5517ms step_avg:33.64ms
step:165/2090 train_time:5551ms step_avg:33.64ms
step:166/2090 train_time:5584ms step_avg:33.64ms
step:167/2090 train_time:5618ms step_avg:33.64ms
step:168/2090 train_time:5650ms step_avg:33.63ms
step:169/2090 train_time:5684ms step_avg:33.63ms
step:170/2090 train_time:5717ms step_avg:33.63ms
step:171/2090 train_time:5750ms step_avg:33.63ms
step:172/2090 train_time:5783ms step_avg:33.62ms
step:173/2090 train_time:5817ms step_avg:33.62ms
step:174/2090 train_time:5849ms step_avg:33.62ms
step:175/2090 train_time:5883ms step_avg:33.62ms
step:176/2090 train_time:5916ms step_avg:33.61ms
step:177/2090 train_time:5950ms step_avg:33.61ms
step:178/2090 train_time:5982ms step_avg:33.61ms
step:179/2090 train_time:6016ms step_avg:33.61ms
step:180/2090 train_time:6049ms step_avg:33.60ms
step:181/2090 train_time:6082ms step_avg:33.60ms
step:182/2090 train_time:6115ms step_avg:33.60ms
step:183/2090 train_time:6148ms step_avg:33.60ms
step:184/2090 train_time:6181ms step_avg:33.59ms
step:185/2090 train_time:6214ms step_avg:33.59ms
step:186/2090 train_time:6247ms step_avg:33.59ms
step:187/2090 train_time:6280ms step_avg:33.58ms
step:188/2090 train_time:6313ms step_avg:33.58ms
step:189/2090 train_time:6346ms step_avg:33.58ms
step:190/2090 train_time:6379ms step_avg:33.57ms
step:191/2090 train_time:6413ms step_avg:33.57ms
step:192/2090 train_time:6446ms step_avg:33.57ms
step:193/2090 train_time:6479ms step_avg:33.57ms
step:194/2090 train_time:6512ms step_avg:33.57ms
step:195/2090 train_time:6545ms step_avg:33.57ms
step:196/2090 train_time:6578ms step_avg:33.56ms
step:197/2090 train_time:6612ms step_avg:33.56ms
step:198/2090 train_time:6645ms step_avg:33.56ms
step:199/2090 train_time:6678ms step_avg:33.56ms
step:200/2090 train_time:6711ms step_avg:33.55ms
step:201/2090 train_time:6745ms step_avg:33.56ms
step:202/2090 train_time:6778ms step_avg:33.55ms
step:203/2090 train_time:6811ms step_avg:33.55ms
step:204/2090 train_time:6844ms step_avg:33.55ms
step:205/2090 train_time:6878ms step_avg:33.55ms
step:206/2090 train_time:6910ms step_avg:33.55ms
step:207/2090 train_time:6944ms step_avg:33.55ms
step:208/2090 train_time:6977ms step_avg:33.54ms
step:209/2090 train_time:7011ms step_avg:33.54ms
step:210/2090 train_time:7043ms step_avg:33.54ms
step:211/2090 train_time:7077ms step_avg:33.54ms
step:212/2090 train_time:7110ms step_avg:33.54ms
step:213/2090 train_time:7143ms step_avg:33.54ms
step:214/2090 train_time:7176ms step_avg:33.53ms
step:215/2090 train_time:7209ms step_avg:33.53ms
step:216/2090 train_time:7242ms step_avg:33.53ms
step:217/2090 train_time:7275ms step_avg:33.53ms
step:218/2090 train_time:7308ms step_avg:33.52ms
step:219/2090 train_time:7341ms step_avg:33.52ms
step:220/2090 train_time:7374ms step_avg:33.52ms
step:221/2090 train_time:7407ms step_avg:33.52ms
step:222/2090 train_time:7440ms step_avg:33.51ms
step:223/2090 train_time:7473ms step_avg:33.51ms
step:224/2090 train_time:7505ms step_avg:33.51ms
step:225/2090 train_time:7539ms step_avg:33.51ms
step:226/2090 train_time:7572ms step_avg:33.50ms
step:227/2090 train_time:7605ms step_avg:33.50ms
step:228/2090 train_time:7638ms step_avg:33.50ms
step:229/2090 train_time:7672ms step_avg:33.50ms
step:230/2090 train_time:7705ms step_avg:33.50ms
step:231/2090 train_time:7738ms step_avg:33.50ms
step:232/2090 train_time:7771ms step_avg:33.50ms
step:233/2090 train_time:7805ms step_avg:33.50ms
step:234/2090 train_time:7837ms step_avg:33.49ms
step:235/2090 train_time:7871ms step_avg:33.49ms
step:236/2090 train_time:7904ms step_avg:33.49ms
step:237/2090 train_time:7937ms step_avg:33.49ms
step:238/2090 train_time:7970ms step_avg:33.49ms
step:239/2090 train_time:8003ms step_avg:33.49ms
step:240/2090 train_time:8036ms step_avg:33.48ms
step:241/2090 train_time:8070ms step_avg:33.48ms
step:242/2090 train_time:8102ms step_avg:33.48ms
step:243/2090 train_time:8136ms step_avg:33.48ms
step:244/2090 train_time:8169ms step_avg:33.48ms
step:245/2090 train_time:8202ms step_avg:33.48ms
step:246/2090 train_time:8234ms step_avg:33.47ms
step:247/2090 train_time:8268ms step_avg:33.47ms
step:248/2090 train_time:8301ms step_avg:33.47ms
step:249/2090 train_time:8334ms step_avg:33.47ms
step:250/2090 train_time:8367ms step_avg:33.47ms
step:250/2090 val_loss:4.2776 train_time:8402ms step_avg:33.61ms
step:251/2090 train_time:8421ms step_avg:33.55ms
step:252/2090 train_time:8440ms step_avg:33.49ms
step:253/2090 train_time:8470ms step_avg:33.48ms
step:254/2090 train_time:8503ms step_avg:33.48ms
step:255/2090 train_time:8538ms step_avg:33.48ms
step:256/2090 train_time:8572ms step_avg:33.48ms
step:257/2090 train_time:8607ms step_avg:33.49ms
step:258/2090 train_time:8639ms step_avg:33.49ms
step:259/2090 train_time:8673ms step_avg:33.49ms
step:260/2090 train_time:8706ms step_avg:33.48ms
step:261/2090 train_time:8739ms step_avg:33.48ms
step:262/2090 train_time:8772ms step_avg:33.48ms
step:263/2090 train_time:8805ms step_avg:33.48ms
step:264/2090 train_time:8837ms step_avg:33.48ms
step:265/2090 train_time:8871ms step_avg:33.48ms
step:266/2090 train_time:8904ms step_avg:33.47ms
step:267/2090 train_time:8937ms step_avg:33.47ms
step:268/2090 train_time:8970ms step_avg:33.47ms
step:269/2090 train_time:9002ms step_avg:33.47ms
step:270/2090 train_time:9035ms step_avg:33.46ms
step:271/2090 train_time:9068ms step_avg:33.46ms
step:272/2090 train_time:9101ms step_avg:33.46ms
step:273/2090 train_time:9134ms step_avg:33.46ms
step:274/2090 train_time:9167ms step_avg:33.46ms
step:275/2090 train_time:9200ms step_avg:33.45ms
step:276/2090 train_time:9233ms step_avg:33.45ms
step:277/2090 train_time:9266ms step_avg:33.45ms
step:278/2090 train_time:9298ms step_avg:33.45ms
step:279/2090 train_time:9331ms step_avg:33.45ms
step:280/2090 train_time:9364ms step_avg:33.44ms
step:281/2090 train_time:9397ms step_avg:33.44ms
step:282/2090 train_time:9430ms step_avg:33.44ms
step:283/2090 train_time:9463ms step_avg:33.44ms
step:284/2090 train_time:9496ms step_avg:33.44ms
step:285/2090 train_time:9530ms step_avg:33.44ms
step:286/2090 train_time:9563ms step_avg:33.44ms
step:287/2090 train_time:9597ms step_avg:33.44ms
step:288/2090 train_time:9630ms step_avg:33.44ms
step:289/2090 train_time:9663ms step_avg:33.44ms
step:290/2090 train_time:9696ms step_avg:33.43ms
step:291/2090 train_time:9730ms step_avg:33.44ms
step:292/2090 train_time:9762ms step_avg:33.43ms
step:293/2090 train_time:9796ms step_avg:33.43ms
step:294/2090 train_time:9829ms step_avg:33.43ms
step:295/2090 train_time:9862ms step_avg:33.43ms
step:296/2090 train_time:9895ms step_avg:33.43ms
step:297/2090 train_time:9928ms step_avg:33.43ms
step:298/2090 train_time:9961ms step_avg:33.42ms
step:299/2090 train_time:9994ms step_avg:33.42ms
step:300/2090 train_time:10027ms step_avg:33.42ms
step:301/2090 train_time:10060ms step_avg:33.42ms
step:302/2090 train_time:10093ms step_avg:33.42ms
step:303/2090 train_time:10126ms step_avg:33.42ms
step:304/2090 train_time:10158ms step_avg:33.42ms
step:305/2090 train_time:10192ms step_avg:33.41ms
step:306/2090 train_time:10224ms step_avg:33.41ms
step:307/2090 train_time:10257ms step_avg:33.41ms
step:308/2090 train_time:10290ms step_avg:33.41ms
step:309/2090 train_time:10323ms step_avg:33.41ms
step:310/2090 train_time:10356ms step_avg:33.41ms
step:311/2090 train_time:10389ms step_avg:33.41ms
step:312/2090 train_time:10422ms step_avg:33.40ms
step:313/2090 train_time:10455ms step_avg:33.40ms
step:314/2090 train_time:10487ms step_avg:33.40ms
step:315/2090 train_time:10521ms step_avg:33.40ms
step:316/2090 train_time:10554ms step_avg:33.40ms
step:317/2090 train_time:10588ms step_avg:33.40ms
step:318/2090 train_time:10620ms step_avg:33.40ms
step:319/2090 train_time:10654ms step_avg:33.40ms
step:320/2090 train_time:10687ms step_avg:33.40ms
step:321/2090 train_time:10720ms step_avg:33.40ms
step:322/2090 train_time:10753ms step_avg:33.39ms
step:323/2090 train_time:10787ms step_avg:33.39ms
step:324/2090 train_time:10819ms step_avg:33.39ms
step:325/2090 train_time:10853ms step_avg:33.39ms
step:326/2090 train_time:10886ms step_avg:33.39ms
step:327/2090 train_time:10919ms step_avg:33.39ms
step:328/2090 train_time:10952ms step_avg:33.39ms
step:329/2090 train_time:10985ms step_avg:33.39ms
step:330/2090 train_time:11018ms step_avg:33.39ms
step:331/2090 train_time:11052ms step_avg:33.39ms
step:332/2090 train_time:11085ms step_avg:33.39ms
step:333/2090 train_time:11117ms step_avg:33.39ms
step:334/2090 train_time:11150ms step_avg:33.38ms
step:335/2090 train_time:11183ms step_avg:33.38ms
step:336/2090 train_time:11216ms step_avg:33.38ms
step:337/2090 train_time:11249ms step_avg:33.38ms
step:338/2090 train_time:11281ms step_avg:33.38ms
step:339/2090 train_time:11315ms step_avg:33.38ms
step:340/2090 train_time:11347ms step_avg:33.37ms
step:341/2090 train_time:11380ms step_avg:33.37ms
step:342/2090 train_time:11413ms step_avg:33.37ms
step:343/2090 train_time:11446ms step_avg:33.37ms
step:344/2090 train_time:11479ms step_avg:33.37ms
step:345/2090 train_time:11512ms step_avg:33.37ms
step:346/2090 train_time:11545ms step_avg:33.37ms
step:347/2090 train_time:11578ms step_avg:33.37ms
step:348/2090 train_time:11612ms step_avg:33.37ms
step:349/2090 train_time:11645ms step_avg:33.37ms
step:350/2090 train_time:11677ms step_avg:33.36ms
step:351/2090 train_time:11711ms step_avg:33.36ms
step:352/2090 train_time:11743ms step_avg:33.36ms
step:353/2090 train_time:11776ms step_avg:33.36ms
step:354/2090 train_time:11809ms step_avg:33.36ms
step:355/2090 train_time:11842ms step_avg:33.36ms
step:356/2090 train_time:11875ms step_avg:33.36ms
step:357/2090 train_time:11909ms step_avg:33.36ms
step:358/2090 train_time:11941ms step_avg:33.36ms
step:359/2090 train_time:11975ms step_avg:33.36ms
step:360/2090 train_time:12008ms step_avg:33.36ms
step:361/2090 train_time:12041ms step_avg:33.35ms
step:362/2090 train_time:12074ms step_avg:33.35ms
step:363/2090 train_time:12107ms step_avg:33.35ms
step:364/2090 train_time:12140ms step_avg:33.35ms
step:365/2090 train_time:12173ms step_avg:33.35ms
step:366/2090 train_time:12206ms step_avg:33.35ms
step:367/2090 train_time:12239ms step_avg:33.35ms
step:368/2090 train_time:12271ms step_avg:33.35ms
step:369/2090 train_time:12304ms step_avg:33.35ms
step:370/2090 train_time:12337ms step_avg:33.34ms
step:371/2090 train_time:12370ms step_avg:33.34ms
step:372/2090 train_time:12403ms step_avg:33.34ms
step:373/2090 train_time:12436ms step_avg:33.34ms
step:374/2090 train_time:12469ms step_avg:33.34ms
step:375/2090 train_time:12502ms step_avg:33.34ms
step:376/2090 train_time:12535ms step_avg:33.34ms
step:377/2090 train_time:12568ms step_avg:33.34ms
step:378/2090 train_time:12601ms step_avg:33.34ms
step:379/2090 train_time:12634ms step_avg:33.34ms
step:380/2090 train_time:12667ms step_avg:33.33ms
step:381/2090 train_time:12700ms step_avg:33.33ms
step:382/2090 train_time:12733ms step_avg:33.33ms
step:383/2090 train_time:12766ms step_avg:33.33ms
step:384/2090 train_time:12799ms step_avg:33.33ms
step:385/2090 train_time:12832ms step_avg:33.33ms
step:386/2090 train_time:12865ms step_avg:33.33ms
step:387/2090 train_time:12898ms step_avg:33.33ms
step:388/2090 train_time:12931ms step_avg:33.33ms
step:389/2090 train_time:12965ms step_avg:33.33ms
step:390/2090 train_time:12997ms step_avg:33.33ms
step:391/2090 train_time:13031ms step_avg:33.33ms
step:392/2090 train_time:13063ms step_avg:33.32ms
step:393/2090 train_time:13097ms step_avg:33.32ms
step:394/2090 train_time:13129ms step_avg:33.32ms
step:395/2090 train_time:13163ms step_avg:33.32ms
step:396/2090 train_time:13195ms step_avg:33.32ms
step:397/2090 train_time:13229ms step_avg:33.32ms
step:398/2090 train_time:13261ms step_avg:33.32ms
step:399/2090 train_time:13294ms step_avg:33.32ms
step:400/2090 train_time:13327ms step_avg:33.32ms
step:401/2090 train_time:13360ms step_avg:33.32ms
step:402/2090 train_time:13393ms step_avg:33.32ms
step:403/2090 train_time:13426ms step_avg:33.32ms
step:404/2090 train_time:13459ms step_avg:33.31ms
step:405/2090 train_time:13492ms step_avg:33.31ms
step:406/2090 train_time:13525ms step_avg:33.31ms
step:407/2090 train_time:13558ms step_avg:33.31ms
step:408/2090 train_time:13591ms step_avg:33.31ms
step:409/2090 train_time:13624ms step_avg:33.31ms
step:410/2090 train_time:13657ms step_avg:33.31ms
step:411/2090 train_time:13691ms step_avg:33.31ms
step:412/2090 train_time:13723ms step_avg:33.31ms
step:413/2090 train_time:13757ms step_avg:33.31ms
step:414/2090 train_time:13789ms step_avg:33.31ms
step:415/2090 train_time:13823ms step_avg:33.31ms
step:416/2090 train_time:13855ms step_avg:33.31ms
step:417/2090 train_time:13889ms step_avg:33.31ms
step:418/2090 train_time:13922ms step_avg:33.31ms
step:419/2090 train_time:13954ms step_avg:33.30ms
step:420/2090 train_time:13988ms step_avg:33.30ms
step:421/2090 train_time:14021ms step_avg:33.30ms
step:422/2090 train_time:14054ms step_avg:33.30ms
step:423/2090 train_time:14087ms step_avg:33.30ms
step:424/2090 train_time:14119ms step_avg:33.30ms
step:425/2090 train_time:14153ms step_avg:33.30ms
step:426/2090 train_time:14186ms step_avg:33.30ms
step:427/2090 train_time:14219ms step_avg:33.30ms
step:428/2090 train_time:14252ms step_avg:33.30ms
step:429/2090 train_time:14285ms step_avg:33.30ms
step:430/2090 train_time:14318ms step_avg:33.30ms
step:431/2090 train_time:14351ms step_avg:33.30ms
step:432/2090 train_time:14384ms step_avg:33.30ms
step:433/2090 train_time:14417ms step_avg:33.29ms
step:434/2090 train_time:14449ms step_avg:33.29ms
step:435/2090 train_time:14483ms step_avg:33.29ms
step:436/2090 train_time:14515ms step_avg:33.29ms
step:437/2090 train_time:14549ms step_avg:33.29ms
step:438/2090 train_time:14582ms step_avg:33.29ms
step:439/2090 train_time:14615ms step_avg:33.29ms
step:440/2090 train_time:14648ms step_avg:33.29ms
step:441/2090 train_time:14681ms step_avg:33.29ms
step:442/2090 train_time:14714ms step_avg:33.29ms
step:443/2090 train_time:14747ms step_avg:33.29ms
step:444/2090 train_time:14781ms step_avg:33.29ms
step:445/2090 train_time:14813ms step_avg:33.29ms
step:446/2090 train_time:14846ms step_avg:33.29ms
step:447/2090 train_time:14879ms step_avg:33.29ms
step:448/2090 train_time:14912ms step_avg:33.29ms
step:449/2090 train_time:14945ms step_avg:33.29ms
step:450/2090 train_time:14978ms step_avg:33.28ms
step:451/2090 train_time:15011ms step_avg:33.28ms
step:452/2090 train_time:15044ms step_avg:33.28ms
step:453/2090 train_time:15077ms step_avg:33.28ms
step:454/2090 train_time:15111ms step_avg:33.28ms
step:455/2090 train_time:15143ms step_avg:33.28ms
step:456/2090 train_time:15176ms step_avg:33.28ms
step:457/2090 train_time:15209ms step_avg:33.28ms
step:458/2090 train_time:15242ms step_avg:33.28ms
step:459/2090 train_time:15275ms step_avg:33.28ms
step:460/2090 train_time:15308ms step_avg:33.28ms
step:461/2090 train_time:15341ms step_avg:33.28ms
step:462/2090 train_time:15374ms step_avg:33.28ms
step:463/2090 train_time:15407ms step_avg:33.28ms
step:464/2090 train_time:15440ms step_avg:33.28ms
step:465/2090 train_time:15473ms step_avg:33.28ms
step:466/2090 train_time:15506ms step_avg:33.28ms
step:467/2090 train_time:15540ms step_avg:33.28ms
step:468/2090 train_time:15572ms step_avg:33.27ms
step:469/2090 train_time:15605ms step_avg:33.27ms
step:470/2090 train_time:15638ms step_avg:33.27ms
step:471/2090 train_time:15672ms step_avg:33.27ms
step:472/2090 train_time:15704ms step_avg:33.27ms
step:473/2090 train_time:15737ms step_avg:33.27ms
step:474/2090 train_time:15770ms step_avg:33.27ms
step:475/2090 train_time:15804ms step_avg:33.27ms
step:476/2090 train_time:15836ms step_avg:33.27ms
step:477/2090 train_time:15870ms step_avg:33.27ms
step:478/2090 train_time:15902ms step_avg:33.27ms
step:479/2090 train_time:15936ms step_avg:33.27ms
step:480/2090 train_time:15969ms step_avg:33.27ms
step:481/2090 train_time:16001ms step_avg:33.27ms
step:482/2090 train_time:16034ms step_avg:33.27ms
step:483/2090 train_time:16067ms step_avg:33.27ms
step:484/2090 train_time:16100ms step_avg:33.26ms
step:485/2090 train_time:16133ms step_avg:33.26ms
step:486/2090 train_time:16166ms step_avg:33.26ms
step:487/2090 train_time:16199ms step_avg:33.26ms
step:488/2090 train_time:16232ms step_avg:33.26ms
step:489/2090 train_time:16266ms step_avg:33.26ms
step:490/2090 train_time:16298ms step_avg:33.26ms
step:491/2090 train_time:16331ms step_avg:33.26ms
step:492/2090 train_time:16364ms step_avg:33.26ms
step:493/2090 train_time:16397ms step_avg:33.26ms
step:494/2090 train_time:16430ms step_avg:33.26ms
step:495/2090 train_time:16463ms step_avg:33.26ms
step:496/2090 train_time:16496ms step_avg:33.26ms
step:497/2090 train_time:16529ms step_avg:33.26ms
step:498/2090 train_time:16562ms step_avg:33.26ms
step:499/2090 train_time:16596ms step_avg:33.26ms
step:500/2090 train_time:16628ms step_avg:33.26ms
step:500/2090 val_loss:4.0069 train_time:16664ms step_avg:33.33ms
step:501/2090 train_time:16685ms step_avg:33.30ms
step:502/2090 train_time:16703ms step_avg:33.27ms
step:503/2090 train_time:16732ms step_avg:33.26ms
step:504/2090 train_time:16765ms step_avg:33.26ms
step:505/2090 train_time:16800ms step_avg:33.27ms
step:506/2090 train_time:16834ms step_avg:33.27ms
step:507/2090 train_time:16868ms step_avg:33.27ms
step:508/2090 train_time:16901ms step_avg:33.27ms
step:509/2090 train_time:16935ms step_avg:33.27ms
step:510/2090 train_time:16968ms step_avg:33.27ms
step:511/2090 train_time:17001ms step_avg:33.27ms
step:512/2090 train_time:17034ms step_avg:33.27ms
step:513/2090 train_time:17067ms step_avg:33.27ms
step:514/2090 train_time:17099ms step_avg:33.27ms
step:515/2090 train_time:17132ms step_avg:33.27ms
step:516/2090 train_time:17165ms step_avg:33.27ms
step:517/2090 train_time:17198ms step_avg:33.27ms
step:518/2090 train_time:17231ms step_avg:33.26ms
step:519/2090 train_time:17264ms step_avg:33.26ms
step:520/2090 train_time:17296ms step_avg:33.26ms
step:521/2090 train_time:17329ms step_avg:33.26ms
step:522/2090 train_time:17362ms step_avg:33.26ms
step:523/2090 train_time:17395ms step_avg:33.26ms
step:524/2090 train_time:17428ms step_avg:33.26ms
step:525/2090 train_time:17460ms step_avg:33.26ms
step:526/2090 train_time:17493ms step_avg:33.26ms
step:527/2090 train_time:17526ms step_avg:33.26ms
step:528/2090 train_time:17559ms step_avg:33.26ms
step:529/2090 train_time:17592ms step_avg:33.26ms
step:530/2090 train_time:17625ms step_avg:33.25ms
step:531/2090 train_time:17658ms step_avg:33.25ms
step:532/2090 train_time:17691ms step_avg:33.25ms
step:533/2090 train_time:17724ms step_avg:33.25ms
step:534/2090 train_time:17757ms step_avg:33.25ms
step:535/2090 train_time:17791ms step_avg:33.25ms
step:536/2090 train_time:17824ms step_avg:33.25ms
step:537/2090 train_time:17858ms step_avg:33.25ms
step:538/2090 train_time:17891ms step_avg:33.25ms
step:539/2090 train_time:17925ms step_avg:33.26ms
step:540/2090 train_time:17957ms step_avg:33.25ms
step:541/2090 train_time:17991ms step_avg:33.25ms
step:542/2090 train_time:18024ms step_avg:33.25ms
step:543/2090 train_time:18057ms step_avg:33.25ms
step:544/2090 train_time:18089ms step_avg:33.25ms
step:545/2090 train_time:18123ms step_avg:33.25ms
step:546/2090 train_time:18155ms step_avg:33.25ms
step:547/2090 train_time:18189ms step_avg:33.25ms
step:548/2090 train_time:18221ms step_avg:33.25ms
step:549/2090 train_time:18255ms step_avg:33.25ms
step:550/2090 train_time:18288ms step_avg:33.25ms
step:551/2090 train_time:18320ms step_avg:33.25ms
step:552/2090 train_time:18353ms step_avg:33.25ms
step:553/2090 train_time:18386ms step_avg:33.25ms
step:554/2090 train_time:18419ms step_avg:33.25ms
step:555/2090 train_time:18452ms step_avg:33.25ms
step:556/2090 train_time:18485ms step_avg:33.25ms
step:557/2090 train_time:18518ms step_avg:33.25ms
step:558/2090 train_time:18551ms step_avg:33.24ms
step:559/2090 train_time:18584ms step_avg:33.24ms
step:560/2090 train_time:18616ms step_avg:33.24ms
step:561/2090 train_time:18650ms step_avg:33.24ms
step:562/2090 train_time:18683ms step_avg:33.24ms
step:563/2090 train_time:18716ms step_avg:33.24ms
step:564/2090 train_time:18749ms step_avg:33.24ms
step:565/2090 train_time:18782ms step_avg:33.24ms
step:566/2090 train_time:18815ms step_avg:33.24ms
step:567/2090 train_time:18849ms step_avg:33.24ms
step:568/2090 train_time:18881ms step_avg:33.24ms
step:569/2090 train_time:18915ms step_avg:33.24ms
step:570/2090 train_time:18947ms step_avg:33.24ms
step:571/2090 train_time:18981ms step_avg:33.24ms
step:572/2090 train_time:19014ms step_avg:33.24ms
step:573/2090 train_time:19047ms step_avg:33.24ms
step:574/2090 train_time:19080ms step_avg:33.24ms
step:575/2090 train_time:19114ms step_avg:33.24ms
step:576/2090 train_time:19146ms step_avg:33.24ms
step:577/2090 train_time:19180ms step_avg:33.24ms
step:578/2090 train_time:19212ms step_avg:33.24ms
step:579/2090 train_time:19246ms step_avg:33.24ms
step:580/2090 train_time:19278ms step_avg:33.24ms
step:581/2090 train_time:19312ms step_avg:33.24ms
step:582/2090 train_time:19344ms step_avg:33.24ms
step:583/2090 train_time:19377ms step_avg:33.24ms
step:584/2090 train_time:19410ms step_avg:33.24ms
step:585/2090 train_time:19443ms step_avg:33.24ms
step:586/2090 train_time:19476ms step_avg:33.24ms
step:587/2090 train_time:19509ms step_avg:33.24ms
step:588/2090 train_time:19542ms step_avg:33.23ms
step:589/2090 train_time:19575ms step_avg:33.23ms
step:590/2090 train_time:19608ms step_avg:33.23ms
step:591/2090 train_time:19641ms step_avg:33.23ms
step:592/2090 train_time:19674ms step_avg:33.23ms
step:593/2090 train_time:19707ms step_avg:33.23ms
step:594/2090 train_time:19740ms step_avg:33.23ms
step:595/2090 train_time:19773ms step_avg:33.23ms
step:596/2090 train_time:19806ms step_avg:33.23ms
step:597/2090 train_time:19839ms step_avg:33.23ms
step:598/2090 train_time:19872ms step_avg:33.23ms
step:599/2090 train_time:19906ms step_avg:33.23ms
step:600/2090 train_time:19939ms step_avg:33.23ms
step:601/2090 train_time:19971ms step_avg:33.23ms
step:602/2090 train_time:20004ms step_avg:33.23ms
step:603/2090 train_time:20037ms step_avg:33.23ms
step:604/2090 train_time:20070ms step_avg:33.23ms
step:605/2090 train_time:20103ms step_avg:33.23ms
step:606/2090 train_time:20136ms step_avg:33.23ms
step:607/2090 train_time:20169ms step_avg:33.23ms
step:608/2090 train_time:20202ms step_avg:33.23ms
step:609/2090 train_time:20235ms step_avg:33.23ms
step:610/2090 train_time:20268ms step_avg:33.23ms
step:611/2090 train_time:20301ms step_avg:33.23ms
step:612/2090 train_time:20334ms step_avg:33.22ms
step:613/2090 train_time:20367ms step_avg:33.22ms
step:614/2090 train_time:20400ms step_avg:33.22ms
step:615/2090 train_time:20433ms step_avg:33.22ms
step:616/2090 train_time:20466ms step_avg:33.22ms
step:617/2090 train_time:20499ms step_avg:33.22ms
step:618/2090 train_time:20532ms step_avg:33.22ms
step:619/2090 train_time:20565ms step_avg:33.22ms
step:620/2090 train_time:20598ms step_avg:33.22ms
step:621/2090 train_time:20631ms step_avg:33.22ms
step:622/2090 train_time:20664ms step_avg:33.22ms
step:623/2090 train_time:20697ms step_avg:33.22ms
step:624/2090 train_time:20730ms step_avg:33.22ms
step:625/2090 train_time:20763ms step_avg:33.22ms
step:626/2090 train_time:20796ms step_avg:33.22ms
step:627/2090 train_time:20829ms step_avg:33.22ms
step:628/2090 train_time:20862ms step_avg:33.22ms
step:629/2090 train_time:20895ms step_avg:33.22ms
step:630/2090 train_time:20928ms step_avg:33.22ms
step:631/2090 train_time:20961ms step_avg:33.22ms
step:632/2090 train_time:20994ms step_avg:33.22ms
step:633/2090 train_time:21027ms step_avg:33.22ms
step:634/2090 train_time:21060ms step_avg:33.22ms
step:635/2090 train_time:21093ms step_avg:33.22ms
step:636/2090 train_time:21126ms step_avg:33.22ms
step:637/2090 train_time:21159ms step_avg:33.22ms
step:638/2090 train_time:21192ms step_avg:33.22ms
step:639/2090 train_time:21225ms step_avg:33.22ms
step:640/2090 train_time:21258ms step_avg:33.22ms
step:641/2090 train_time:21291ms step_avg:33.22ms
step:642/2090 train_time:21324ms step_avg:33.22ms
step:643/2090 train_time:21358ms step_avg:33.22ms
step:644/2090 train_time:21391ms step_avg:33.22ms
step:645/2090 train_time:21424ms step_avg:33.21ms
step:646/2090 train_time:21456ms step_avg:33.21ms
step:647/2090 train_time:21490ms step_avg:33.21ms
step:648/2090 train_time:21522ms step_avg:33.21ms
step:649/2090 train_time:21556ms step_avg:33.21ms
step:650/2090 train_time:21589ms step_avg:33.21ms
step:651/2090 train_time:21622ms step_avg:33.21ms
step:652/2090 train_time:21655ms step_avg:33.21ms
step:653/2090 train_time:21688ms step_avg:33.21ms
step:654/2090 train_time:21721ms step_avg:33.21ms
step:655/2090 train_time:21754ms step_avg:33.21ms
step:656/2090 train_time:21787ms step_avg:33.21ms
step:657/2090 train_time:21820ms step_avg:33.21ms
step:658/2090 train_time:21853ms step_avg:33.21ms
step:659/2090 train_time:21886ms step_avg:33.21ms
step:660/2090 train_time:21919ms step_avg:33.21ms
step:661/2090 train_time:21953ms step_avg:33.21ms
step:662/2090 train_time:21986ms step_avg:33.21ms
step:663/2090 train_time:22019ms step_avg:33.21ms
step:664/2090 train_time:22052ms step_avg:33.21ms
step:665/2090 train_time:22085ms step_avg:33.21ms
step:666/2090 train_time:22118ms step_avg:33.21ms
step:667/2090 train_time:22151ms step_avg:33.21ms
step:668/2090 train_time:22184ms step_avg:33.21ms
step:669/2090 train_time:22217ms step_avg:33.21ms
step:670/2090 train_time:22250ms step_avg:33.21ms
step:671/2090 train_time:22283ms step_avg:33.21ms
step:672/2090 train_time:22316ms step_avg:33.21ms
step:673/2090 train_time:22349ms step_avg:33.21ms
step:674/2090 train_time:22382ms step_avg:33.21ms
step:675/2090 train_time:22415ms step_avg:33.21ms
step:676/2090 train_time:22448ms step_avg:33.21ms
step:677/2090 train_time:22481ms step_avg:33.21ms
step:678/2090 train_time:22514ms step_avg:33.21ms
step:679/2090 train_time:22547ms step_avg:33.21ms
step:680/2090 train_time:22580ms step_avg:33.21ms
step:681/2090 train_time:22613ms step_avg:33.21ms
step:682/2090 train_time:22646ms step_avg:33.21ms
step:683/2090 train_time:22679ms step_avg:33.21ms
step:684/2090 train_time:22712ms step_avg:33.20ms
step:685/2090 train_time:22746ms step_avg:33.21ms
step:686/2090 train_time:22804ms step_avg:33.24ms
step:687/2090 train_time:22864ms step_avg:33.28ms
step:688/2090 train_time:22923ms step_avg:33.32ms
step:689/2090 train_time:22983ms step_avg:33.36ms
step:690/2090 train_time:23042ms step_avg:33.39ms
step:691/2090 train_time:23102ms step_avg:33.43ms
step:692/2090 train_time:23162ms step_avg:33.47ms
step:693/2090 train_time:23222ms step_avg:33.51ms
step:694/2090 train_time:23281ms step_avg:33.55ms
step:695/2090 train_time:23342ms step_avg:33.58ms
step:696/2090 train_time:23401ms step_avg:33.62ms
step:697/2090 train_time:23461ms step_avg:33.66ms
step:698/2090 train_time:23521ms step_avg:33.70ms
step:699/2090 train_time:23582ms step_avg:33.74ms
step:700/2090 train_time:23641ms step_avg:33.77ms
step:701/2090 train_time:23702ms step_avg:33.81ms
step:702/2090 train_time:23761ms step_avg:33.85ms
step:703/2090 train_time:23822ms step_avg:33.89ms
step:704/2090 train_time:23881ms step_avg:33.92ms
step:705/2090 train_time:23942ms step_avg:33.96ms
step:706/2090 train_time:24001ms step_avg:34.00ms
step:707/2090 train_time:24061ms step_avg:34.03ms
step:708/2090 train_time:24120ms step_avg:34.07ms
step:709/2090 train_time:24181ms step_avg:34.11ms
step:710/2090 train_time:24240ms step_avg:34.14ms
step:711/2090 train_time:24301ms step_avg:34.18ms
step:712/2090 train_time:24360ms step_avg:34.21ms
step:713/2090 train_time:24420ms step_avg:34.25ms
step:714/2090 train_time:24480ms step_avg:34.29ms
step:715/2090 train_time:24541ms step_avg:34.32ms
step:716/2090 train_time:24601ms step_avg:34.36ms
step:717/2090 train_time:24661ms step_avg:34.39ms
step:718/2090 train_time:24721ms step_avg:34.43ms
step:719/2090 train_time:24782ms step_avg:34.47ms
step:720/2090 train_time:24842ms step_avg:34.50ms
step:721/2090 train_time:24902ms step_avg:34.54ms
step:722/2090 train_time:24961ms step_avg:34.57ms
step:723/2090 train_time:25021ms step_avg:34.61ms
step:724/2090 train_time:25080ms step_avg:34.64ms
step:725/2090 train_time:25142ms step_avg:34.68ms
step:726/2090 train_time:25201ms step_avg:34.71ms
step:727/2090 train_time:25262ms step_avg:34.75ms
step:728/2090 train_time:25321ms step_avg:34.78ms
step:729/2090 train_time:25381ms step_avg:34.82ms
step:730/2090 train_time:25440ms step_avg:34.85ms
step:731/2090 train_time:25501ms step_avg:34.89ms
step:732/2090 train_time:25561ms step_avg:34.92ms
step:733/2090 train_time:25621ms step_avg:34.95ms
step:734/2090 train_time:25681ms step_avg:34.99ms
step:735/2090 train_time:25742ms step_avg:35.02ms
step:736/2090 train_time:25801ms step_avg:35.06ms
step:737/2090 train_time:25862ms step_avg:35.09ms
step:738/2090 train_time:25922ms step_avg:35.12ms
step:739/2090 train_time:25981ms step_avg:35.16ms
step:740/2090 train_time:26041ms step_avg:35.19ms
step:741/2090 train_time:26101ms step_avg:35.22ms
step:742/2090 train_time:26162ms step_avg:35.26ms
step:743/2090 train_time:26222ms step_avg:35.29ms
step:744/2090 train_time:26282ms step_avg:35.32ms
step:745/2090 train_time:26342ms step_avg:35.36ms
step:746/2090 train_time:26401ms step_avg:35.39ms
step:747/2090 train_time:26461ms step_avg:35.42ms
step:748/2090 train_time:26520ms step_avg:35.45ms
step:749/2090 train_time:26581ms step_avg:35.49ms
step:750/2090 train_time:26640ms step_avg:35.52ms
step:750/2090 val_loss:3.8511 train_time:26703ms step_avg:35.60ms
step:751/2090 train_time:26722ms step_avg:35.58ms
step:752/2090 train_time:26764ms step_avg:35.59ms
step:753/2090 train_time:26827ms step_avg:35.63ms
step:754/2090 train_time:26890ms step_avg:35.66ms
step:755/2090 train_time:26950ms step_avg:35.70ms
step:756/2090 train_time:27010ms step_avg:35.73ms
step:757/2090 train_time:27070ms step_avg:35.76ms
step:758/2090 train_time:27128ms step_avg:35.79ms
step:759/2090 train_time:27189ms step_avg:35.82ms
step:760/2090 train_time:27248ms step_avg:35.85ms
step:761/2090 train_time:27308ms step_avg:35.88ms
step:762/2090 train_time:27367ms step_avg:35.91ms
step:763/2090 train_time:27427ms step_avg:35.95ms
step:764/2090 train_time:27485ms step_avg:35.98ms
step:765/2090 train_time:27545ms step_avg:36.01ms
step:766/2090 train_time:27605ms step_avg:36.04ms
step:767/2090 train_time:27666ms step_avg:36.07ms
step:768/2090 train_time:27726ms step_avg:36.10ms
step:769/2090 train_time:27790ms step_avg:36.14ms
step:770/2090 train_time:27850ms step_avg:36.17ms
step:771/2090 train_time:27912ms step_avg:36.20ms
step:772/2090 train_time:27971ms step_avg:36.23ms
step:773/2090 train_time:28030ms step_avg:36.26ms
step:774/2090 train_time:28089ms step_avg:36.29ms
step:775/2090 train_time:28149ms step_avg:36.32ms
step:776/2090 train_time:28208ms step_avg:36.35ms
step:777/2090 train_time:28268ms step_avg:36.38ms
step:778/2090 train_time:28327ms step_avg:36.41ms
step:779/2090 train_time:28387ms step_avg:36.44ms
step:780/2090 train_time:28446ms step_avg:36.47ms
step:781/2090 train_time:28505ms step_avg:36.50ms
step:782/2090 train_time:28564ms step_avg:36.53ms
step:783/2090 train_time:28624ms step_avg:36.56ms
step:784/2090 train_time:28683ms step_avg:36.59ms
step:785/2090 train_time:28745ms step_avg:36.62ms
step:786/2090 train_time:28806ms step_avg:36.65ms
step:787/2090 train_time:28867ms step_avg:36.68ms
step:788/2090 train_time:28927ms step_avg:36.71ms
step:789/2090 train_time:28988ms step_avg:36.74ms
step:790/2090 train_time:29047ms step_avg:36.77ms
step:791/2090 train_time:29108ms step_avg:36.80ms
step:792/2090 train_time:29167ms step_avg:36.83ms
step:793/2090 train_time:29227ms step_avg:36.86ms
step:794/2090 train_time:29286ms step_avg:36.88ms
step:795/2090 train_time:29346ms step_avg:36.91ms
step:796/2090 train_time:29404ms step_avg:36.94ms
step:797/2090 train_time:29464ms step_avg:36.97ms
step:798/2090 train_time:29523ms step_avg:37.00ms
step:799/2090 train_time:29584ms step_avg:37.03ms
step:800/2090 train_time:29643ms step_avg:37.05ms
step:801/2090 train_time:29705ms step_avg:37.08ms
step:802/2090 train_time:29764ms step_avg:37.11ms
step:803/2090 train_time:29825ms step_avg:37.14ms
step:804/2090 train_time:29886ms step_avg:37.17ms
step:805/2090 train_time:29947ms step_avg:37.20ms
step:806/2090 train_time:30007ms step_avg:37.23ms
step:807/2090 train_time:30067ms step_avg:37.26ms
step:808/2090 train_time:30127ms step_avg:37.29ms
step:809/2090 train_time:30187ms step_avg:37.31ms
step:810/2090 train_time:30246ms step_avg:37.34ms
step:811/2090 train_time:30307ms step_avg:37.37ms
step:812/2090 train_time:30366ms step_avg:37.40ms
step:813/2090 train_time:30426ms step_avg:37.42ms
step:814/2090 train_time:30485ms step_avg:37.45ms
step:815/2090 train_time:30545ms step_avg:37.48ms
step:816/2090 train_time:30604ms step_avg:37.51ms
step:817/2090 train_time:30665ms step_avg:37.53ms
step:818/2090 train_time:30724ms step_avg:37.56ms
step:819/2090 train_time:30785ms step_avg:37.59ms
step:820/2090 train_time:30845ms step_avg:37.62ms
step:821/2090 train_time:30906ms step_avg:37.64ms
step:822/2090 train_time:30965ms step_avg:37.67ms
step:823/2090 train_time:31026ms step_avg:37.70ms
step:824/2090 train_time:31086ms step_avg:37.73ms
step:825/2090 train_time:31146ms step_avg:37.75ms
step:826/2090 train_time:31205ms step_avg:37.78ms
step:827/2090 train_time:31265ms step_avg:37.81ms
step:828/2090 train_time:31324ms step_avg:37.83ms
step:829/2090 train_time:31385ms step_avg:37.86ms
step:830/2090 train_time:31444ms step_avg:37.88ms
step:831/2090 train_time:31503ms step_avg:37.91ms
step:832/2090 train_time:31562ms step_avg:37.94ms
step:833/2090 train_time:31623ms step_avg:37.96ms
step:834/2090 train_time:31682ms step_avg:37.99ms
step:835/2090 train_time:31743ms step_avg:38.02ms
step:836/2090 train_time:31802ms step_avg:38.04ms
step:837/2090 train_time:31863ms step_avg:38.07ms
step:838/2090 train_time:31923ms step_avg:38.09ms
step:839/2090 train_time:31984ms step_avg:38.12ms
step:840/2090 train_time:32044ms step_avg:38.15ms
step:841/2090 train_time:32105ms step_avg:38.17ms
step:842/2090 train_time:32165ms step_avg:38.20ms
step:843/2090 train_time:32225ms step_avg:38.23ms
step:844/2090 train_time:32284ms step_avg:38.25ms
step:845/2090 train_time:32345ms step_avg:38.28ms
step:846/2090 train_time:32404ms step_avg:38.30ms
step:847/2090 train_time:32464ms step_avg:38.33ms
step:848/2090 train_time:32523ms step_avg:38.35ms
step:849/2090 train_time:32584ms step_avg:38.38ms
step:850/2090 train_time:32644ms step_avg:38.40ms
step:851/2090 train_time:32704ms step_avg:38.43ms
step:852/2090 train_time:32763ms step_avg:38.45ms
step:853/2090 train_time:32824ms step_avg:38.48ms
step:854/2090 train_time:32884ms step_avg:38.51ms
step:855/2090 train_time:32944ms step_avg:38.53ms
step:856/2090 train_time:33005ms step_avg:38.56ms
step:857/2090 train_time:33065ms step_avg:38.58ms
step:858/2090 train_time:33124ms step_avg:38.61ms
step:859/2090 train_time:33185ms step_avg:38.63ms
step:860/2090 train_time:33244ms step_avg:38.66ms
step:861/2090 train_time:33304ms step_avg:38.68ms
step:862/2090 train_time:33364ms step_avg:38.70ms
step:863/2090 train_time:33424ms step_avg:38.73ms
step:864/2090 train_time:33483ms step_avg:38.75ms
step:865/2090 train_time:33544ms step_avg:38.78ms
step:866/2090 train_time:33603ms step_avg:38.80ms
step:867/2090 train_time:33663ms step_avg:38.83ms
step:868/2090 train_time:33722ms step_avg:38.85ms
step:869/2090 train_time:33783ms step_avg:38.88ms
step:870/2090 train_time:33843ms step_avg:38.90ms
step:871/2090 train_time:33904ms step_avg:38.93ms
step:872/2090 train_time:33964ms step_avg:38.95ms
step:873/2090 train_time:34024ms step_avg:38.97ms
step:874/2090 train_time:34085ms step_avg:39.00ms
step:875/2090 train_time:34146ms step_avg:39.02ms
step:876/2090 train_time:34205ms step_avg:39.05ms
step:877/2090 train_time:34266ms step_avg:39.07ms
step:878/2090 train_time:34325ms step_avg:39.09ms
step:879/2090 train_time:34385ms step_avg:39.12ms
step:880/2090 train_time:34444ms step_avg:39.14ms
step:881/2090 train_time:34504ms step_avg:39.16ms
step:882/2090 train_time:34564ms step_avg:39.19ms
step:883/2090 train_time:34624ms step_avg:39.21ms
step:884/2090 train_time:34683ms step_avg:39.23ms
step:885/2090 train_time:34744ms step_avg:39.26ms
step:886/2090 train_time:34803ms step_avg:39.28ms
step:887/2090 train_time:34863ms step_avg:39.30ms
step:888/2090 train_time:34923ms step_avg:39.33ms
step:889/2090 train_time:34984ms step_avg:39.35ms
step:890/2090 train_time:35044ms step_avg:39.38ms
step:891/2090 train_time:35105ms step_avg:39.40ms
step:892/2090 train_time:35165ms step_avg:39.42ms
step:893/2090 train_time:35225ms step_avg:39.45ms
step:894/2090 train_time:35285ms step_avg:39.47ms
step:895/2090 train_time:35346ms step_avg:39.49ms
step:896/2090 train_time:35406ms step_avg:39.52ms
step:897/2090 train_time:35466ms step_avg:39.54ms
step:898/2090 train_time:35526ms step_avg:39.56ms
step:899/2090 train_time:35588ms step_avg:39.59ms
step:900/2090 train_time:35647ms step_avg:39.61ms
step:901/2090 train_time:35706ms step_avg:39.63ms
step:902/2090 train_time:35766ms step_avg:39.65ms
step:903/2090 train_time:35827ms step_avg:39.68ms
step:904/2090 train_time:35887ms step_avg:39.70ms
step:905/2090 train_time:35947ms step_avg:39.72ms
step:906/2090 train_time:36006ms step_avg:39.74ms
step:907/2090 train_time:36067ms step_avg:39.76ms
step:908/2090 train_time:36126ms step_avg:39.79ms
step:909/2090 train_time:36187ms step_avg:39.81ms
step:910/2090 train_time:36246ms step_avg:39.83ms
step:911/2090 train_time:36307ms step_avg:39.85ms
step:912/2090 train_time:36367ms step_avg:39.88ms
step:913/2090 train_time:36427ms step_avg:39.90ms
step:914/2090 train_time:36486ms step_avg:39.92ms
step:915/2090 train_time:36547ms step_avg:39.94ms
step:916/2090 train_time:36606ms step_avg:39.96ms
step:917/2090 train_time:36666ms step_avg:39.98ms
step:918/2090 train_time:36726ms step_avg:40.01ms
step:919/2090 train_time:36787ms step_avg:40.03ms
step:920/2090 train_time:36847ms step_avg:40.05ms
step:921/2090 train_time:36907ms step_avg:40.07ms
step:922/2090 train_time:36967ms step_avg:40.09ms
step:923/2090 train_time:37027ms step_avg:40.12ms
step:924/2090 train_time:37086ms step_avg:40.14ms
step:925/2090 train_time:37147ms step_avg:40.16ms
step:926/2090 train_time:37206ms step_avg:40.18ms
step:927/2090 train_time:37267ms step_avg:40.20ms
step:928/2090 train_time:37326ms step_avg:40.22ms
step:929/2090 train_time:37387ms step_avg:40.24ms
step:930/2090 train_time:37446ms step_avg:40.26ms
step:931/2090 train_time:37506ms step_avg:40.29ms
step:932/2090 train_time:37565ms step_avg:40.31ms
step:933/2090 train_time:37626ms step_avg:40.33ms
step:934/2090 train_time:37685ms step_avg:40.35ms
step:935/2090 train_time:37745ms step_avg:40.37ms
step:936/2090 train_time:37804ms step_avg:40.39ms
step:937/2090 train_time:37865ms step_avg:40.41ms
step:938/2090 train_time:37925ms step_avg:40.43ms
step:939/2090 train_time:37986ms step_avg:40.45ms
step:940/2090 train_time:38045ms step_avg:40.47ms
step:941/2090 train_time:38106ms step_avg:40.50ms
step:942/2090 train_time:38165ms step_avg:40.52ms
step:943/2090 train_time:38227ms step_avg:40.54ms
step:944/2090 train_time:38286ms step_avg:40.56ms
step:945/2090 train_time:38347ms step_avg:40.58ms
step:946/2090 train_time:38406ms step_avg:40.60ms
step:947/2090 train_time:38466ms step_avg:40.62ms
step:948/2090 train_time:38525ms step_avg:40.64ms
step:949/2090 train_time:38586ms step_avg:40.66ms
step:950/2090 train_time:38646ms step_avg:40.68ms
step:951/2090 train_time:38706ms step_avg:40.70ms
step:952/2090 train_time:38766ms step_avg:40.72ms
step:953/2090 train_time:38826ms step_avg:40.74ms
step:954/2090 train_time:38885ms step_avg:40.76ms
step:955/2090 train_time:38945ms step_avg:40.78ms
step:956/2090 train_time:39005ms step_avg:40.80ms
step:957/2090 train_time:39066ms step_avg:40.82ms
step:958/2090 train_time:39125ms step_avg:40.84ms
step:959/2090 train_time:39187ms step_avg:40.86ms
step:960/2090 train_time:39247ms step_avg:40.88ms
step:961/2090 train_time:39308ms step_avg:40.90ms
step:962/2090 train_time:39368ms step_avg:40.92ms
step:963/2090 train_time:39429ms step_avg:40.94ms
step:964/2090 train_time:39488ms step_avg:40.96ms
step:965/2090 train_time:39548ms step_avg:40.98ms
step:966/2090 train_time:39607ms step_avg:41.00ms
step:967/2090 train_time:39668ms step_avg:41.02ms
step:968/2090 train_time:39727ms step_avg:41.04ms
step:969/2090 train_time:39788ms step_avg:41.06ms
step:970/2090 train_time:39848ms step_avg:41.08ms
step:971/2090 train_time:39908ms step_avg:41.10ms
step:972/2090 train_time:39967ms step_avg:41.12ms
step:973/2090 train_time:40027ms step_avg:41.14ms
step:974/2090 train_time:40087ms step_avg:41.16ms
step:975/2090 train_time:40148ms step_avg:41.18ms
step:976/2090 train_time:40207ms step_avg:41.20ms
step:977/2090 train_time:40268ms step_avg:41.22ms
step:978/2090 train_time:40327ms step_avg:41.23ms
step:979/2090 train_time:40387ms step_avg:41.25ms
step:980/2090 train_time:40447ms step_avg:41.27ms
step:981/2090 train_time:40508ms step_avg:41.29ms
step:982/2090 train_time:40567ms step_avg:41.31ms
step:983/2090 train_time:40628ms step_avg:41.33ms
step:984/2090 train_time:40687ms step_avg:41.35ms
step:985/2090 train_time:40748ms step_avg:41.37ms
step:986/2090 train_time:40808ms step_avg:41.39ms
step:987/2090 train_time:40868ms step_avg:41.41ms
step:988/2090 train_time:40928ms step_avg:41.42ms
step:989/2090 train_time:40988ms step_avg:41.44ms
step:990/2090 train_time:41048ms step_avg:41.46ms
step:991/2090 train_time:41109ms step_avg:41.48ms
step:992/2090 train_time:41168ms step_avg:41.50ms
step:993/2090 train_time:41228ms step_avg:41.52ms
step:994/2090 train_time:41288ms step_avg:41.54ms
step:995/2090 train_time:41349ms step_avg:41.56ms
step:996/2090 train_time:41409ms step_avg:41.57ms
step:997/2090 train_time:41470ms step_avg:41.59ms
step:998/2090 train_time:41529ms step_avg:41.61ms
step:999/2090 train_time:41589ms step_avg:41.63ms
step:1000/2090 train_time:41648ms step_avg:41.65ms
step:1000/2090 val_loss:3.7069 train_time:41711ms step_avg:41.71ms
step:1001/2090 train_time:41731ms step_avg:41.69ms
step:1002/2090 train_time:41771ms step_avg:41.69ms
step:1003/2090 train_time:41835ms step_avg:41.71ms
step:1004/2090 train_time:41897ms step_avg:41.73ms
step:1005/2090 train_time:41957ms step_avg:41.75ms
step:1006/2090 train_time:42016ms step_avg:41.77ms
step:1007/2090 train_time:42076ms step_avg:41.78ms
step:1008/2090 train_time:42135ms step_avg:41.80ms
step:1009/2090 train_time:42194ms step_avg:41.82ms
step:1010/2090 train_time:42253ms step_avg:41.83ms
step:1011/2090 train_time:42313ms step_avg:41.85ms
step:1012/2090 train_time:42372ms step_avg:41.87ms
step:1013/2090 train_time:42432ms step_avg:41.89ms
step:1014/2090 train_time:42491ms step_avg:41.90ms
step:1015/2090 train_time:42551ms step_avg:41.92ms
step:1016/2090 train_time:42610ms step_avg:41.94ms
step:1017/2090 train_time:42671ms step_avg:41.96ms
step:1018/2090 train_time:42732ms step_avg:41.98ms
step:1019/2090 train_time:42795ms step_avg:42.00ms
step:1020/2090 train_time:42856ms step_avg:42.02ms
step:1021/2090 train_time:42917ms step_avg:42.03ms
step:1022/2090 train_time:42977ms step_avg:42.05ms
step:1023/2090 train_time:43037ms step_avg:42.07ms
step:1024/2090 train_time:43096ms step_avg:42.09ms
step:1025/2090 train_time:43156ms step_avg:42.10ms
step:1026/2090 train_time:43215ms step_avg:42.12ms
step:1027/2090 train_time:43275ms step_avg:42.14ms
step:1028/2090 train_time:43334ms step_avg:42.15ms
step:1029/2090 train_time:43393ms step_avg:42.17ms
step:1030/2090 train_time:43453ms step_avg:42.19ms
step:1031/2090 train_time:43512ms step_avg:42.20ms
step:1032/2090 train_time:43571ms step_avg:42.22ms
step:1033/2090 train_time:43632ms step_avg:42.24ms
step:1034/2090 train_time:43692ms step_avg:42.26ms
step:1035/2090 train_time:43754ms step_avg:42.27ms
step:1036/2090 train_time:43814ms step_avg:42.29ms
step:1037/2090 train_time:43876ms step_avg:42.31ms
step:1038/2090 train_time:43936ms step_avg:42.33ms
step:1039/2090 train_time:43997ms step_avg:42.35ms
step:1040/2090 train_time:44056ms step_avg:42.36ms
step:1041/2090 train_time:44117ms step_avg:42.38ms
step:1042/2090 train_time:44176ms step_avg:42.40ms
step:1043/2090 train_time:44236ms step_avg:42.41ms
step:1044/2090 train_time:44295ms step_avg:42.43ms
step:1045/2090 train_time:44355ms step_avg:42.44ms
step:1046/2090 train_time:44413ms step_avg:42.46ms
step:1047/2090 train_time:44474ms step_avg:42.48ms
step:1048/2090 train_time:44533ms step_avg:42.49ms
step:1049/2090 train_time:44594ms step_avg:42.51ms
step:1050/2090 train_time:44653ms step_avg:42.53ms
step:1051/2090 train_time:44714ms step_avg:42.54ms
step:1052/2090 train_time:44774ms step_avg:42.56ms
step:1053/2090 train_time:44835ms step_avg:42.58ms
step:1054/2090 train_time:44895ms step_avg:42.60ms
step:1055/2090 train_time:44956ms step_avg:42.61ms
step:1056/2090 train_time:45015ms step_avg:42.63ms
step:1057/2090 train_time:45076ms step_avg:42.65ms
step:1058/2090 train_time:45135ms step_avg:42.66ms
step:1059/2090 train_time:45196ms step_avg:42.68ms
step:1060/2090 train_time:45255ms step_avg:42.69ms
step:1061/2090 train_time:45315ms step_avg:42.71ms
step:1062/2090 train_time:45374ms step_avg:42.73ms
step:1063/2090 train_time:45435ms step_avg:42.74ms
step:1064/2090 train_time:45494ms step_avg:42.76ms
step:1065/2090 train_time:45554ms step_avg:42.77ms
step:1066/2090 train_time:45614ms step_avg:42.79ms
step:1067/2090 train_time:45674ms step_avg:42.81ms
step:1068/2090 train_time:45734ms step_avg:42.82ms
step:1069/2090 train_time:45796ms step_avg:42.84ms
step:1070/2090 train_time:45856ms step_avg:42.86ms
step:1071/2090 train_time:45917ms step_avg:42.87ms
step:1072/2090 train_time:45976ms step_avg:42.89ms
step:1073/2090 train_time:46037ms step_avg:42.90ms
step:1074/2090 train_time:46096ms step_avg:42.92ms
step:1075/2090 train_time:46156ms step_avg:42.94ms
step:1076/2090 train_time:46216ms step_avg:42.95ms
step:1077/2090 train_time:46276ms step_avg:42.97ms
step:1078/2090 train_time:46335ms step_avg:42.98ms
step:1079/2090 train_time:46395ms step_avg:43.00ms
step:1080/2090 train_time:46454ms step_avg:43.01ms
step:1081/2090 train_time:46514ms step_avg:43.03ms
step:1082/2090 train_time:46573ms step_avg:43.04ms
step:1083/2090 train_time:46634ms step_avg:43.06ms
step:1084/2090 train_time:46695ms step_avg:43.08ms
step:1085/2090 train_time:46755ms step_avg:43.09ms
step:1086/2090 train_time:46815ms step_avg:43.11ms
step:1087/2090 train_time:46876ms step_avg:43.12ms
step:1088/2090 train_time:46935ms step_avg:43.14ms
step:1089/2090 train_time:46996ms step_avg:43.16ms
step:1090/2090 train_time:47056ms step_avg:43.17ms
step:1091/2090 train_time:47117ms step_avg:43.19ms
step:1092/2090 train_time:47176ms step_avg:43.20ms
step:1093/2090 train_time:47237ms step_avg:43.22ms
step:1094/2090 train_time:47296ms step_avg:43.23ms
step:1095/2090 train_time:47356ms step_avg:43.25ms
step:1096/2090 train_time:47416ms step_avg:43.26ms
step:1097/2090 train_time:47476ms step_avg:43.28ms
step:1098/2090 train_time:47536ms step_avg:43.29ms
step:1099/2090 train_time:47596ms step_avg:43.31ms
step:1100/2090 train_time:47655ms step_avg:43.32ms
step:1101/2090 train_time:47715ms step_avg:43.34ms
step:1102/2090 train_time:47774ms step_avg:43.35ms
step:1103/2090 train_time:47835ms step_avg:43.37ms
step:1104/2090 train_time:47895ms step_avg:43.38ms
step:1105/2090 train_time:47955ms step_avg:43.40ms
step:1106/2090 train_time:48015ms step_avg:43.41ms
step:1107/2090 train_time:48075ms step_avg:43.43ms
step:1108/2090 train_time:48135ms step_avg:43.44ms
step:1109/2090 train_time:48195ms step_avg:43.46ms
step:1110/2090 train_time:48255ms step_avg:43.47ms
step:1111/2090 train_time:48316ms step_avg:43.49ms
step:1112/2090 train_time:48374ms step_avg:43.50ms
step:1113/2090 train_time:48434ms step_avg:43.52ms
step:1114/2090 train_time:48494ms step_avg:43.53ms
step:1115/2090 train_time:48554ms step_avg:43.55ms
step:1116/2090 train_time:48613ms step_avg:43.56ms
step:1117/2090 train_time:48674ms step_avg:43.58ms
step:1118/2090 train_time:48734ms step_avg:43.59ms
step:1119/2090 train_time:48795ms step_avg:43.61ms
step:1120/2090 train_time:48855ms step_avg:43.62ms
step:1121/2090 train_time:48916ms step_avg:43.64ms
step:1122/2090 train_time:48975ms step_avg:43.65ms
step:1123/2090 train_time:49036ms step_avg:43.66ms
step:1124/2090 train_time:49095ms step_avg:43.68ms
step:1125/2090 train_time:49156ms step_avg:43.69ms
step:1126/2090 train_time:49216ms step_avg:43.71ms
step:1127/2090 train_time:49276ms step_avg:43.72ms
step:1128/2090 train_time:49336ms step_avg:43.74ms
step:1129/2090 train_time:49396ms step_avg:43.75ms
step:1130/2090 train_time:49455ms step_avg:43.77ms
step:1131/2090 train_time:49516ms step_avg:43.78ms
step:1132/2090 train_time:49576ms step_avg:43.79ms
step:1133/2090 train_time:49636ms step_avg:43.81ms
step:1134/2090 train_time:49695ms step_avg:43.82ms
step:1135/2090 train_time:49756ms step_avg:43.84ms
step:1136/2090 train_time:49816ms step_avg:43.85ms
step:1137/2090 train_time:49877ms step_avg:43.87ms
step:1138/2090 train_time:49936ms step_avg:43.88ms
step:1139/2090 train_time:49997ms step_avg:43.90ms
step:1140/2090 train_time:50057ms step_avg:43.91ms
step:1141/2090 train_time:50118ms step_avg:43.92ms
step:1142/2090 train_time:50177ms step_avg:43.94ms
step:1143/2090 train_time:50238ms step_avg:43.95ms
step:1144/2090 train_time:50297ms step_avg:43.97ms
step:1145/2090 train_time:50357ms step_avg:43.98ms
step:1146/2090 train_time:50416ms step_avg:43.99ms
step:1147/2090 train_time:50477ms step_avg:44.01ms
step:1148/2090 train_time:50536ms step_avg:44.02ms
step:1149/2090 train_time:50597ms step_avg:44.04ms
step:1150/2090 train_time:50656ms step_avg:44.05ms
step:1151/2090 train_time:50717ms step_avg:44.06ms
step:1152/2090 train_time:50776ms step_avg:44.08ms
step:1153/2090 train_time:50837ms step_avg:44.09ms
step:1154/2090 train_time:50896ms step_avg:44.10ms
step:1155/2090 train_time:50956ms step_avg:44.12ms
step:1156/2090 train_time:51016ms step_avg:44.13ms
step:1157/2090 train_time:51077ms step_avg:44.15ms
step:1158/2090 train_time:51136ms step_avg:44.16ms
step:1159/2090 train_time:51196ms step_avg:44.17ms
step:1160/2090 train_time:51256ms step_avg:44.19ms
step:1161/2090 train_time:51316ms step_avg:44.20ms
step:1162/2090 train_time:51376ms step_avg:44.21ms
step:1163/2090 train_time:51437ms step_avg:44.23ms
step:1164/2090 train_time:51496ms step_avg:44.24ms
step:1165/2090 train_time:51556ms step_avg:44.25ms
step:1166/2090 train_time:51616ms step_avg:44.27ms
step:1167/2090 train_time:51677ms step_avg:44.28ms
step:1168/2090 train_time:51736ms step_avg:44.29ms
step:1169/2090 train_time:51796ms step_avg:44.31ms
step:1170/2090 train_time:51856ms step_avg:44.32ms
step:1171/2090 train_time:51916ms step_avg:44.33ms
step:1172/2090 train_time:51976ms step_avg:44.35ms
step:1173/2090 train_time:52037ms step_avg:44.36ms
step:1174/2090 train_time:52096ms step_avg:44.37ms
step:1175/2090 train_time:52157ms step_avg:44.39ms
step:1176/2090 train_time:52216ms step_avg:44.40ms
step:1177/2090 train_time:52275ms step_avg:44.41ms
step:1178/2090 train_time:52334ms step_avg:44.43ms
step:1179/2090 train_time:52395ms step_avg:44.44ms
step:1180/2090 train_time:52454ms step_avg:44.45ms
step:1181/2090 train_time:52516ms step_avg:44.47ms
step:1182/2090 train_time:52575ms step_avg:44.48ms
step:1183/2090 train_time:52635ms step_avg:44.49ms
step:1184/2090 train_time:52695ms step_avg:44.51ms
step:1185/2090 train_time:52756ms step_avg:44.52ms
step:1186/2090 train_time:52815ms step_avg:44.53ms
step:1187/2090 train_time:52876ms step_avg:44.55ms
step:1188/2090 train_time:52935ms step_avg:44.56ms
step:1189/2090 train_time:52996ms step_avg:44.57ms
step:1190/2090 train_time:53055ms step_avg:44.58ms
step:1191/2090 train_time:53116ms step_avg:44.60ms
step:1192/2090 train_time:53176ms step_avg:44.61ms
step:1193/2090 train_time:53236ms step_avg:44.62ms
step:1194/2090 train_time:53296ms step_avg:44.64ms
step:1195/2090 train_time:53356ms step_avg:44.65ms
step:1196/2090 train_time:53415ms step_avg:44.66ms
step:1197/2090 train_time:53476ms step_avg:44.68ms
step:1198/2090 train_time:53536ms step_avg:44.69ms
step:1199/2090 train_time:53596ms step_avg:44.70ms
step:1200/2090 train_time:53656ms step_avg:44.71ms
step:1201/2090 train_time:53716ms step_avg:44.73ms
step:1202/2090 train_time:53775ms step_avg:44.74ms
step:1203/2090 train_time:53836ms step_avg:44.75ms
step:1204/2090 train_time:53896ms step_avg:44.76ms
step:1205/2090 train_time:53957ms step_avg:44.78ms
step:1206/2090 train_time:54016ms step_avg:44.79ms
step:1207/2090 train_time:54077ms step_avg:44.80ms
step:1208/2090 train_time:54136ms step_avg:44.81ms
step:1209/2090 train_time:54196ms step_avg:44.83ms
step:1210/2090 train_time:54256ms step_avg:44.84ms
step:1211/2090 train_time:54316ms step_avg:44.85ms
step:1212/2090 train_time:54376ms step_avg:44.86ms
step:1213/2090 train_time:54437ms step_avg:44.88ms
step:1214/2090 train_time:54496ms step_avg:44.89ms
step:1215/2090 train_time:54557ms step_avg:44.90ms
step:1216/2090 train_time:54616ms step_avg:44.91ms
step:1217/2090 train_time:54677ms step_avg:44.93ms
step:1218/2090 train_time:54736ms step_avg:44.94ms
step:1219/2090 train_time:54797ms step_avg:44.95ms
step:1220/2090 train_time:54856ms step_avg:44.96ms
step:1221/2090 train_time:54916ms step_avg:44.98ms
step:1222/2090 train_time:54975ms step_avg:44.99ms
step:1223/2090 train_time:55036ms step_avg:45.00ms
step:1224/2090 train_time:55095ms step_avg:45.01ms
step:1225/2090 train_time:55155ms step_avg:45.02ms
step:1226/2090 train_time:55214ms step_avg:45.04ms
step:1227/2090 train_time:55275ms step_avg:45.05ms
step:1228/2090 train_time:55335ms step_avg:45.06ms
step:1229/2090 train_time:55395ms step_avg:45.07ms
step:1230/2090 train_time:55455ms step_avg:45.09ms
step:1231/2090 train_time:55515ms step_avg:45.10ms
step:1232/2090 train_time:55574ms step_avg:45.11ms
step:1233/2090 train_time:55635ms step_avg:45.12ms
step:1234/2090 train_time:55694ms step_avg:45.13ms
step:1235/2090 train_time:55755ms step_avg:45.15ms
step:1236/2090 train_time:55814ms step_avg:45.16ms
step:1237/2090 train_time:55874ms step_avg:45.17ms
step:1238/2090 train_time:55934ms step_avg:45.18ms
step:1239/2090 train_time:55994ms step_avg:45.19ms
step:1240/2090 train_time:56052ms step_avg:45.20ms
step:1241/2090 train_time:56113ms step_avg:45.22ms
step:1242/2090 train_time:56173ms step_avg:45.23ms
step:1243/2090 train_time:56234ms step_avg:45.24ms
step:1244/2090 train_time:56293ms step_avg:45.25ms
step:1245/2090 train_time:56354ms step_avg:45.26ms
step:1246/2090 train_time:56414ms step_avg:45.28ms
step:1247/2090 train_time:56475ms step_avg:45.29ms
step:1248/2090 train_time:56535ms step_avg:45.30ms
step:1249/2090 train_time:56595ms step_avg:45.31ms
step:1250/2090 train_time:56655ms step_avg:45.32ms
step:1250/2090 val_loss:3.5861 train_time:56718ms step_avg:45.37ms
step:1251/2090 train_time:56737ms step_avg:45.35ms
step:1252/2090 train_time:56778ms step_avg:45.35ms
step:1253/2090 train_time:56842ms step_avg:45.36ms
step:1254/2090 train_time:56905ms step_avg:45.38ms
step:1255/2090 train_time:56966ms step_avg:45.39ms
step:1256/2090 train_time:57025ms step_avg:45.40ms
step:1257/2090 train_time:57086ms step_avg:45.41ms
step:1258/2090 train_time:57145ms step_avg:45.42ms
step:1259/2090 train_time:57205ms step_avg:45.44ms
step:1260/2090 train_time:57264ms step_avg:45.45ms
step:1261/2090 train_time:57323ms step_avg:45.46ms
step:1262/2090 train_time:57382ms step_avg:45.47ms
step:1263/2090 train_time:57442ms step_avg:45.48ms
step:1264/2090 train_time:57501ms step_avg:45.49ms
step:1265/2090 train_time:57561ms step_avg:45.50ms
step:1266/2090 train_time:57620ms step_avg:45.51ms
step:1267/2090 train_time:57681ms step_avg:45.53ms
step:1268/2090 train_time:57743ms step_avg:45.54ms
step:1269/2090 train_time:57806ms step_avg:45.55ms
step:1270/2090 train_time:57867ms step_avg:45.56ms
step:1271/2090 train_time:57929ms step_avg:45.58ms
step:1272/2090 train_time:57990ms step_avg:45.59ms
step:1273/2090 train_time:58050ms step_avg:45.60ms
step:1274/2090 train_time:58108ms step_avg:45.61ms
step:1275/2090 train_time:58168ms step_avg:45.62ms
step:1276/2090 train_time:58227ms step_avg:45.63ms
step:1277/2090 train_time:58287ms step_avg:45.64ms
step:1278/2090 train_time:58346ms step_avg:45.65ms
step:1279/2090 train_time:58406ms step_avg:45.67ms
step:1280/2090 train_time:58465ms step_avg:45.68ms
step:1281/2090 train_time:58525ms step_avg:45.69ms
step:1282/2090 train_time:58583ms step_avg:45.70ms
step:1283/2090 train_time:58644ms step_avg:45.71ms
step:1284/2090 train_time:58704ms step_avg:45.72ms
step:1285/2090 train_time:58765ms step_avg:45.73ms
step:1286/2090 train_time:58825ms step_avg:45.74ms
step:1287/2090 train_time:58886ms step_avg:45.75ms
step:1288/2090 train_time:58946ms step_avg:45.77ms
step:1289/2090 train_time:59007ms step_avg:45.78ms
step:1290/2090 train_time:59067ms step_avg:45.79ms
step:1291/2090 train_time:59128ms step_avg:45.80ms
step:1292/2090 train_time:59187ms step_avg:45.81ms
step:1293/2090 train_time:59247ms step_avg:45.82ms
step:1294/2090 train_time:59306ms step_avg:45.83ms
step:1295/2090 train_time:59365ms step_avg:45.84ms
step:1296/2090 train_time:59424ms step_avg:45.85ms
step:1297/2090 train_time:59484ms step_avg:45.86ms
step:1298/2090 train_time:59543ms step_avg:45.87ms
step:1299/2090 train_time:59604ms step_avg:45.88ms
step:1300/2090 train_time:59663ms step_avg:45.89ms
step:1301/2090 train_time:59724ms step_avg:45.91ms
step:1302/2090 train_time:59784ms step_avg:45.92ms
step:1303/2090 train_time:59846ms step_avg:45.93ms
step:1304/2090 train_time:59906ms step_avg:45.94ms
step:1305/2090 train_time:59966ms step_avg:45.95ms
step:1306/2090 train_time:60026ms step_avg:45.96ms
step:1307/2090 train_time:60087ms step_avg:45.97ms
step:1308/2090 train_time:60147ms step_avg:45.98ms
step:1309/2090 train_time:60207ms step_avg:45.99ms
step:1310/2090 train_time:60266ms step_avg:46.00ms
step:1311/2090 train_time:60326ms step_avg:46.02ms
step:1312/2090 train_time:60385ms step_avg:46.03ms
step:1313/2090 train_time:60445ms step_avg:46.04ms
step:1314/2090 train_time:60504ms step_avg:46.05ms
step:1315/2090 train_time:60564ms step_avg:46.06ms
step:1316/2090 train_time:60623ms step_avg:46.07ms
step:1317/2090 train_time:60683ms step_avg:46.08ms
step:1318/2090 train_time:60743ms step_avg:46.09ms
step:1319/2090 train_time:60804ms step_avg:46.10ms
step:1320/2090 train_time:60864ms step_avg:46.11ms
step:1321/2090 train_time:60925ms step_avg:46.12ms
step:1322/2090 train_time:60985ms step_avg:46.13ms
step:1323/2090 train_time:61046ms step_avg:46.14ms
step:1324/2090 train_time:61105ms step_avg:46.15ms
step:1325/2090 train_time:61166ms step_avg:46.16ms
step:1326/2090 train_time:61226ms step_avg:46.17ms
step:1327/2090 train_time:61287ms step_avg:46.18ms
step:1328/2090 train_time:61346ms step_avg:46.19ms
step:1329/2090 train_time:61405ms step_avg:46.20ms
step:1330/2090 train_time:61464ms step_avg:46.21ms
step:1331/2090 train_time:61525ms step_avg:46.22ms
step:1332/2090 train_time:61585ms step_avg:46.23ms
step:1333/2090 train_time:61645ms step_avg:46.25ms
step:1334/2090 train_time:61704ms step_avg:46.26ms
step:1335/2090 train_time:61765ms step_avg:46.27ms
step:1336/2090 train_time:61825ms step_avg:46.28ms
step:1337/2090 train_time:61885ms step_avg:46.29ms
step:1338/2090 train_time:61944ms step_avg:46.30ms
step:1339/2090 train_time:62005ms step_avg:46.31ms
step:1340/2090 train_time:62065ms step_avg:46.32ms
step:1341/2090 train_time:62125ms step_avg:46.33ms
step:1342/2090 train_time:62185ms step_avg:46.34ms
step:1343/2090 train_time:62246ms step_avg:46.35ms
step:1344/2090 train_time:62305ms step_avg:46.36ms
step:1345/2090 train_time:62365ms step_avg:46.37ms
step:1346/2090 train_time:62424ms step_avg:46.38ms
step:1347/2090 train_time:62484ms step_avg:46.39ms
step:1348/2090 train_time:62543ms step_avg:46.40ms
step:1349/2090 train_time:62604ms step_avg:46.41ms
step:1350/2090 train_time:62663ms step_avg:46.42ms
step:1351/2090 train_time:62723ms step_avg:46.43ms
step:1352/2090 train_time:62783ms step_avg:46.44ms
step:1353/2090 train_time:62843ms step_avg:46.45ms
step:1354/2090 train_time:62903ms step_avg:46.46ms
step:1355/2090 train_time:62963ms step_avg:46.47ms
step:1356/2090 train_time:63023ms step_avg:46.48ms
step:1357/2090 train_time:63084ms step_avg:46.49ms
step:1358/2090 train_time:63144ms step_avg:46.50ms
step:1359/2090 train_time:63205ms step_avg:46.51ms
step:1360/2090 train_time:63264ms step_avg:46.52ms
step:1361/2090 train_time:63325ms step_avg:46.53ms
step:1362/2090 train_time:63385ms step_avg:46.54ms
step:1363/2090 train_time:63445ms step_avg:46.55ms
step:1364/2090 train_time:63506ms step_avg:46.56ms
step:1365/2090 train_time:63566ms step_avg:46.57ms
step:1366/2090 train_time:63625ms step_avg:46.58ms
step:1367/2090 train_time:63686ms step_avg:46.59ms
step:1368/2090 train_time:63746ms step_avg:46.60ms
step:1369/2090 train_time:63834ms step_avg:46.63ms
step:1370/2090 train_time:63921ms step_avg:46.66ms
step:1371/2090 train_time:64010ms step_avg:46.69ms
step:1372/2090 train_time:64097ms step_avg:46.72ms
step:1373/2090 train_time:64184ms step_avg:46.75ms
step:1374/2090 train_time:64271ms step_avg:46.78ms
step:1375/2090 train_time:64359ms step_avg:46.81ms
step:1376/2090 train_time:64446ms step_avg:46.84ms
step:1377/2090 train_time:64535ms step_avg:46.87ms
step:1378/2090 train_time:64621ms step_avg:46.90ms
step:1379/2090 train_time:64709ms step_avg:46.92ms
step:1380/2090 train_time:64796ms step_avg:46.95ms
step:1381/2090 train_time:64883ms step_avg:46.98ms
step:1382/2090 train_time:64970ms step_avg:47.01ms
step:1383/2090 train_time:65058ms step_avg:47.04ms
step:1384/2090 train_time:65144ms step_avg:47.07ms
step:1385/2090 train_time:65233ms step_avg:47.10ms
step:1386/2090 train_time:65321ms step_avg:47.13ms
step:1387/2090 train_time:65409ms step_avg:47.16ms
step:1388/2090 train_time:65496ms step_avg:47.19ms
step:1389/2090 train_time:65583ms step_avg:47.22ms
step:1390/2090 train_time:65670ms step_avg:47.24ms
step:1391/2090 train_time:65759ms step_avg:47.27ms
step:1392/2090 train_time:65846ms step_avg:47.30ms
step:1393/2090 train_time:65933ms step_avg:47.33ms
step:1394/2090 train_time:66020ms step_avg:47.36ms
step:1395/2090 train_time:66107ms step_avg:47.39ms
step:1396/2090 train_time:66194ms step_avg:47.42ms
step:1397/2090 train_time:66282ms step_avg:47.45ms
step:1398/2090 train_time:66370ms step_avg:47.47ms
step:1399/2090 train_time:66458ms step_avg:47.50ms
step:1400/2090 train_time:66545ms step_avg:47.53ms
step:1401/2090 train_time:66632ms step_avg:47.56ms
step:1402/2090 train_time:66720ms step_avg:47.59ms
step:1403/2090 train_time:66808ms step_avg:47.62ms
step:1404/2090 train_time:66895ms step_avg:47.65ms
step:1405/2090 train_time:66983ms step_avg:47.68ms
step:1406/2090 train_time:67071ms step_avg:47.70ms
step:1407/2090 train_time:67159ms step_avg:47.73ms
step:1408/2090 train_time:67246ms step_avg:47.76ms
step:1409/2090 train_time:67335ms step_avg:47.79ms
step:1410/2090 train_time:67421ms step_avg:47.82ms
step:1411/2090 train_time:67508ms step_avg:47.84ms
step:1412/2090 train_time:67596ms step_avg:47.87ms
step:1413/2090 train_time:67684ms step_avg:47.90ms
step:1414/2090 train_time:67770ms step_avg:47.93ms
step:1415/2090 train_time:67859ms step_avg:47.96ms
step:1416/2090 train_time:67946ms step_avg:47.98ms
step:1417/2090 train_time:68034ms step_avg:48.01ms
step:1418/2090 train_time:68121ms step_avg:48.04ms
step:1419/2090 train_time:68209ms step_avg:48.07ms
step:1420/2090 train_time:68296ms step_avg:48.10ms
step:1421/2090 train_time:68384ms step_avg:48.12ms
step:1422/2090 train_time:68471ms step_avg:48.15ms
step:1423/2090 train_time:68560ms step_avg:48.18ms
step:1424/2090 train_time:68647ms step_avg:48.21ms
step:1425/2090 train_time:68735ms step_avg:48.23ms
step:1426/2090 train_time:68821ms step_avg:48.26ms
step:1427/2090 train_time:68909ms step_avg:48.29ms
step:1428/2090 train_time:68996ms step_avg:48.32ms
step:1429/2090 train_time:69083ms step_avg:48.34ms
step:1430/2090 train_time:69170ms step_avg:48.37ms
step:1431/2090 train_time:69260ms step_avg:48.40ms
step:1432/2090 train_time:69347ms step_avg:48.43ms
step:1433/2090 train_time:69435ms step_avg:48.45ms
step:1434/2090 train_time:69521ms step_avg:48.48ms
step:1435/2090 train_time:69609ms step_avg:48.51ms
step:1436/2090 train_time:69696ms step_avg:48.53ms
step:1437/2090 train_time:69784ms step_avg:48.56ms
step:1438/2090 train_time:69871ms step_avg:48.59ms
step:1439/2090 train_time:69960ms step_avg:48.62ms
step:1440/2090 train_time:70046ms step_avg:48.64ms
step:1441/2090 train_time:70134ms step_avg:48.67ms
step:1442/2090 train_time:70221ms step_avg:48.70ms
step:1443/2090 train_time:70309ms step_avg:48.72ms
step:1444/2090 train_time:70396ms step_avg:48.75ms
step:1445/2090 train_time:70484ms step_avg:48.78ms
step:1446/2090 train_time:70572ms step_avg:48.80ms
step:1447/2090 train_time:70659ms step_avg:48.83ms
step:1448/2090 train_time:70746ms step_avg:48.86ms
step:1449/2090 train_time:70834ms step_avg:48.88ms
step:1450/2090 train_time:70920ms step_avg:48.91ms
step:1451/2090 train_time:71008ms step_avg:48.94ms
step:1452/2090 train_time:71096ms step_avg:48.96ms
step:1453/2090 train_time:71183ms step_avg:48.99ms
step:1454/2090 train_time:71271ms step_avg:49.02ms
step:1455/2090 train_time:71360ms step_avg:49.04ms
step:1456/2090 train_time:71446ms step_avg:49.07ms
step:1457/2090 train_time:71534ms step_avg:49.10ms
step:1458/2090 train_time:71621ms step_avg:49.12ms
step:1459/2090 train_time:71709ms step_avg:49.15ms
step:1460/2090 train_time:71796ms step_avg:49.18ms
step:1461/2090 train_time:71884ms step_avg:49.20ms
step:1462/2090 train_time:71971ms step_avg:49.23ms
step:1463/2090 train_time:72060ms step_avg:49.25ms
step:1464/2090 train_time:72146ms step_avg:49.28ms
step:1465/2090 train_time:72235ms step_avg:49.31ms
step:1466/2090 train_time:72321ms step_avg:49.33ms
step:1467/2090 train_time:72409ms step_avg:49.36ms
step:1468/2090 train_time:72496ms step_avg:49.38ms
step:1469/2090 train_time:72583ms step_avg:49.41ms
step:1470/2090 train_time:72671ms step_avg:49.44ms
step:1471/2090 train_time:72759ms step_avg:49.46ms
step:1472/2090 train_time:72845ms step_avg:49.49ms
step:1473/2090 train_time:72933ms step_avg:49.51ms
step:1474/2090 train_time:73021ms step_avg:49.54ms
step:1475/2090 train_time:73109ms step_avg:49.57ms
step:1476/2090 train_time:73196ms step_avg:49.59ms
step:1477/2090 train_time:73284ms step_avg:49.62ms
step:1478/2090 train_time:73371ms step_avg:49.64ms
step:1479/2090 train_time:73460ms step_avg:49.67ms
step:1480/2090 train_time:73547ms step_avg:49.69ms
step:1481/2090 train_time:73635ms step_avg:49.72ms
step:1482/2090 train_time:73722ms step_avg:49.74ms
step:1483/2090 train_time:73810ms step_avg:49.77ms
step:1484/2090 train_time:73896ms step_avg:49.80ms
step:1485/2090 train_time:73984ms step_avg:49.82ms
step:1486/2090 train_time:74071ms step_avg:49.85ms
step:1487/2090 train_time:74159ms step_avg:49.87ms
step:1488/2090 train_time:74246ms step_avg:49.90ms
step:1489/2090 train_time:74334ms step_avg:49.92ms
step:1490/2090 train_time:74421ms step_avg:49.95ms
step:1491/2090 train_time:74508ms step_avg:49.97ms
step:1492/2090 train_time:74595ms step_avg:50.00ms
step:1493/2090 train_time:74683ms step_avg:50.02ms
step:1494/2090 train_time:74771ms step_avg:50.05ms
step:1495/2090 train_time:74858ms step_avg:50.07ms
step:1496/2090 train_time:74944ms step_avg:50.10ms
step:1497/2090 train_time:75032ms step_avg:50.12ms
step:1498/2090 train_time:75119ms step_avg:50.15ms
step:1499/2090 train_time:75207ms step_avg:50.17ms
step:1500/2090 train_time:75294ms step_avg:50.20ms
step:1500/2090 val_loss:3.4765 train_time:75384ms step_avg:50.26ms
step:1501/2090 train_time:75404ms step_avg:50.24ms
step:1502/2090 train_time:75474ms step_avg:50.25ms
step:1503/2090 train_time:75567ms step_avg:50.28ms
step:1504/2090 train_time:75658ms step_avg:50.30ms
step:1505/2090 train_time:75745ms step_avg:50.33ms
step:1506/2090 train_time:75831ms step_avg:50.35ms
step:1507/2090 train_time:75918ms step_avg:50.38ms
step:1508/2090 train_time:76003ms step_avg:50.40ms
step:1509/2090 train_time:76089ms step_avg:50.42ms
step:1510/2090 train_time:76175ms step_avg:50.45ms
step:1511/2090 train_time:76262ms step_avg:50.47ms
step:1512/2090 train_time:76350ms step_avg:50.50ms
step:1513/2090 train_time:76441ms step_avg:50.52ms
step:1514/2090 train_time:76530ms step_avg:50.55ms
step:1515/2090 train_time:76621ms step_avg:50.57ms
step:1516/2090 train_time:76709ms step_avg:50.60ms
step:1517/2090 train_time:76797ms step_avg:50.62ms
step:1518/2090 train_time:76883ms step_avg:50.65ms
step:1519/2090 train_time:76971ms step_avg:50.67ms
step:1520/2090 train_time:77056ms step_avg:50.69ms
step:1521/2090 train_time:77142ms step_avg:50.72ms
step:1522/2090 train_time:77229ms step_avg:50.74ms
step:1523/2090 train_time:77316ms step_avg:50.77ms
step:1524/2090 train_time:77404ms step_avg:50.79ms
step:1525/2090 train_time:77494ms step_avg:50.82ms
step:1526/2090 train_time:77582ms step_avg:50.84ms
step:1527/2090 train_time:77671ms step_avg:50.87ms
step:1528/2090 train_time:77759ms step_avg:50.89ms
step:1529/2090 train_time:77845ms step_avg:50.91ms
step:1530/2090 train_time:77932ms step_avg:50.94ms
step:1531/2090 train_time:78019ms step_avg:50.96ms
step:1532/2090 train_time:78105ms step_avg:50.98ms
step:1533/2090 train_time:78192ms step_avg:51.01ms
step:1534/2090 train_time:78278ms step_avg:51.03ms
step:1535/2090 train_time:78367ms step_avg:51.05ms
step:1536/2090 train_time:78455ms step_avg:51.08ms
step:1537/2090 train_time:78544ms step_avg:51.10ms
step:1538/2090 train_time:78632ms step_avg:51.13ms
step:1539/2090 train_time:78721ms step_avg:51.15ms
step:1540/2090 train_time:78807ms step_avg:51.17ms
step:1541/2090 train_time:78895ms step_avg:51.20ms
step:1542/2090 train_time:78981ms step_avg:51.22ms
step:1543/2090 train_time:79068ms step_avg:51.24ms
step:1544/2090 train_time:79155ms step_avg:51.27ms
step:1545/2090 train_time:79242ms step_avg:51.29ms
step:1546/2090 train_time:79329ms step_avg:51.31ms
step:1547/2090 train_time:79418ms step_avg:51.34ms
step:1548/2090 train_time:79505ms step_avg:51.36ms
step:1549/2090 train_time:79594ms step_avg:51.38ms
step:1550/2090 train_time:79682ms step_avg:51.41ms
step:1551/2090 train_time:79771ms step_avg:51.43ms
step:1552/2090 train_time:79859ms step_avg:51.46ms
step:1553/2090 train_time:79946ms step_avg:51.48ms
step:1554/2090 train_time:80033ms step_avg:51.50ms
step:1555/2090 train_time:80121ms step_avg:51.52ms
step:1556/2090 train_time:80207ms step_avg:51.55ms
step:1557/2090 train_time:80295ms step_avg:51.57ms
step:1558/2090 train_time:80382ms step_avg:51.59ms
step:1559/2090 train_time:80470ms step_avg:51.62ms
step:1560/2090 train_time:80557ms step_avg:51.64ms
step:1561/2090 train_time:80645ms step_avg:51.66ms
step:1562/2090 train_time:80733ms step_avg:51.69ms
step:1563/2090 train_time:80821ms step_avg:51.71ms
step:1564/2090 train_time:80908ms step_avg:51.73ms
step:1565/2090 train_time:80997ms step_avg:51.76ms
step:1566/2090 train_time:81083ms step_avg:51.78ms
step:1567/2090 train_time:81171ms step_avg:51.80ms
step:1568/2090 train_time:81258ms step_avg:51.82ms
step:1569/2090 train_time:81346ms step_avg:51.85ms
step:1570/2090 train_time:81433ms step_avg:51.87ms
step:1571/2090 train_time:81521ms step_avg:51.89ms
step:1572/2090 train_time:81608ms step_avg:51.91ms
step:1573/2090 train_time:81696ms step_avg:51.94ms
step:1574/2090 train_time:81783ms step_avg:51.96ms
step:1575/2090 train_time:81871ms step_avg:51.98ms
step:1576/2090 train_time:81959ms step_avg:52.00ms
step:1577/2090 train_time:82046ms step_avg:52.03ms
step:1578/2090 train_time:82133ms step_avg:52.05ms
step:1579/2090 train_time:82222ms step_avg:52.07ms
step:1580/2090 train_time:82308ms step_avg:52.09ms
step:1581/2090 train_time:82396ms step_avg:52.12ms
step:1582/2090 train_time:82483ms step_avg:52.14ms
step:1583/2090 train_time:82571ms step_avg:52.16ms
step:1584/2090 train_time:82659ms step_avg:52.18ms
step:1585/2090 train_time:82747ms step_avg:52.21ms
step:1586/2090 train_time:82835ms step_avg:52.23ms
step:1587/2090 train_time:82924ms step_avg:52.25ms
step:1588/2090 train_time:83011ms step_avg:52.27ms
step:1589/2090 train_time:83100ms step_avg:52.30ms
step:1590/2090 train_time:83187ms step_avg:52.32ms
step:1591/2090 train_time:83275ms step_avg:52.34ms
step:1592/2090 train_time:83361ms step_avg:52.36ms
step:1593/2090 train_time:83449ms step_avg:52.38ms
step:1594/2090 train_time:83536ms step_avg:52.41ms
step:1595/2090 train_time:83623ms step_avg:52.43ms
step:1596/2090 train_time:83711ms step_avg:52.45ms
step:1597/2090 train_time:83800ms step_avg:52.47ms
step:1598/2090 train_time:83888ms step_avg:52.50ms
step:1599/2090 train_time:83976ms step_avg:52.52ms
step:1600/2090 train_time:84062ms step_avg:52.54ms
step:1601/2090 train_time:84150ms step_avg:52.56ms
step:1602/2090 train_time:84237ms step_avg:52.58ms
step:1603/2090 train_time:84324ms step_avg:52.60ms
step:1604/2090 train_time:84411ms step_avg:52.63ms
step:1605/2090 train_time:84499ms step_avg:52.65ms
step:1606/2090 train_time:84585ms step_avg:52.67ms
step:1607/2090 train_time:84673ms step_avg:52.69ms
step:1608/2090 train_time:84760ms step_avg:52.71ms
step:1609/2090 train_time:84848ms step_avg:52.73ms
step:1610/2090 train_time:84936ms step_avg:52.76ms
step:1611/2090 train_time:85024ms step_avg:52.78ms
step:1612/2090 train_time:85111ms step_avg:52.80ms
step:1613/2090 train_time:85199ms step_avg:52.82ms
step:1614/2090 train_time:85286ms step_avg:52.84ms
step:1615/2090 train_time:85374ms step_avg:52.86ms
step:1616/2090 train_time:85462ms step_avg:52.88ms
step:1617/2090 train_time:85549ms step_avg:52.91ms
step:1618/2090 train_time:85636ms step_avg:52.93ms
step:1619/2090 train_time:85724ms step_avg:52.95ms
step:1620/2090 train_time:85811ms step_avg:52.97ms
step:1621/2090 train_time:85899ms step_avg:52.99ms
step:1622/2090 train_time:85986ms step_avg:53.01ms
step:1623/2090 train_time:86074ms step_avg:53.03ms
step:1624/2090 train_time:86160ms step_avg:53.05ms
step:1625/2090 train_time:86248ms step_avg:53.08ms
step:1626/2090 train_time:86334ms step_avg:53.10ms
step:1627/2090 train_time:86422ms step_avg:53.12ms
step:1628/2090 train_time:86509ms step_avg:53.14ms
step:1629/2090 train_time:86597ms step_avg:53.16ms
step:1630/2090 train_time:86683ms step_avg:53.18ms
step:1631/2090 train_time:86772ms step_avg:53.20ms
step:1632/2090 train_time:86858ms step_avg:53.22ms
step:1633/2090 train_time:86946ms step_avg:53.24ms
step:1634/2090 train_time:87034ms step_avg:53.26ms
step:1635/2090 train_time:87122ms step_avg:53.29ms
step:1636/2090 train_time:87209ms step_avg:53.31ms
step:1637/2090 train_time:87296ms step_avg:53.33ms
step:1638/2090 train_time:87383ms step_avg:53.35ms
step:1639/2090 train_time:87471ms step_avg:53.37ms
step:1640/2090 train_time:87558ms step_avg:53.39ms
step:1641/2090 train_time:87646ms step_avg:53.41ms
step:1642/2090 train_time:87733ms step_avg:53.43ms
step:1643/2090 train_time:87821ms step_avg:53.45ms
step:1644/2090 train_time:87908ms step_avg:53.47ms
step:1645/2090 train_time:87995ms step_avg:53.49ms
step:1646/2090 train_time:88082ms step_avg:53.51ms
step:1647/2090 train_time:88170ms step_avg:53.53ms
step:1648/2090 train_time:88258ms step_avg:53.55ms
step:1649/2090 train_time:88345ms step_avg:53.58ms
step:1650/2090 train_time:88432ms step_avg:53.60ms
step:1651/2090 train_time:88520ms step_avg:53.62ms
step:1652/2090 train_time:88606ms step_avg:53.64ms
step:1653/2090 train_time:88695ms step_avg:53.66ms
step:1654/2090 train_time:88781ms step_avg:53.68ms
step:1655/2090 train_time:88870ms step_avg:53.70ms
step:1656/2090 train_time:88957ms step_avg:53.72ms
step:1657/2090 train_time:89045ms step_avg:53.74ms
step:1658/2090 train_time:89132ms step_avg:53.76ms
step:1659/2090 train_time:89221ms step_avg:53.78ms
step:1660/2090 train_time:89307ms step_avg:53.80ms
step:1661/2090 train_time:89395ms step_avg:53.82ms
step:1662/2090 train_time:89482ms step_avg:53.84ms
step:1663/2090 train_time:89569ms step_avg:53.86ms
step:1664/2090 train_time:89657ms step_avg:53.88ms
step:1665/2090 train_time:89745ms step_avg:53.90ms
step:1666/2090 train_time:89832ms step_avg:53.92ms
step:1667/2090 train_time:89920ms step_avg:53.94ms
step:1668/2090 train_time:90007ms step_avg:53.96ms
step:1669/2090 train_time:90095ms step_avg:53.98ms
step:1670/2090 train_time:90182ms step_avg:54.00ms
step:1671/2090 train_time:90270ms step_avg:54.02ms
step:1672/2090 train_time:90357ms step_avg:54.04ms
step:1673/2090 train_time:90445ms step_avg:54.06ms
step:1674/2090 train_time:90531ms step_avg:54.08ms
step:1675/2090 train_time:90621ms step_avg:54.10ms
step:1676/2090 train_time:90707ms step_avg:54.12ms
step:1677/2090 train_time:90796ms step_avg:54.14ms
step:1678/2090 train_time:90883ms step_avg:54.16ms
step:1679/2090 train_time:90972ms step_avg:54.18ms
step:1680/2090 train_time:91059ms step_avg:54.20ms
step:1681/2090 train_time:91147ms step_avg:54.22ms
step:1682/2090 train_time:91233ms step_avg:54.24ms
step:1683/2090 train_time:91322ms step_avg:54.26ms
step:1684/2090 train_time:91408ms step_avg:54.28ms
step:1685/2090 train_time:91497ms step_avg:54.30ms
step:1686/2090 train_time:91584ms step_avg:54.32ms
step:1687/2090 train_time:91672ms step_avg:54.34ms
step:1688/2090 train_time:91759ms step_avg:54.36ms
step:1689/2090 train_time:91847ms step_avg:54.38ms
step:1690/2090 train_time:91934ms step_avg:54.40ms
step:1691/2090 train_time:92023ms step_avg:54.42ms
step:1692/2090 train_time:92110ms step_avg:54.44ms
step:1693/2090 train_time:92199ms step_avg:54.46ms
step:1694/2090 train_time:92286ms step_avg:54.48ms
step:1695/2090 train_time:92374ms step_avg:54.50ms
step:1696/2090 train_time:92461ms step_avg:54.52ms
step:1697/2090 train_time:92549ms step_avg:54.54ms
step:1698/2090 train_time:92637ms step_avg:54.56ms
step:1699/2090 train_time:92726ms step_avg:54.58ms
step:1700/2090 train_time:92813ms step_avg:54.60ms
step:1701/2090 train_time:92901ms step_avg:54.62ms
step:1702/2090 train_time:92988ms step_avg:54.63ms
step:1703/2090 train_time:93076ms step_avg:54.65ms
step:1704/2090 train_time:93163ms step_avg:54.67ms
step:1705/2090 train_time:93250ms step_avg:54.69ms
step:1706/2090 train_time:93338ms step_avg:54.71ms
step:1707/2090 train_time:93425ms step_avg:54.73ms
step:1708/2090 train_time:93512ms step_avg:54.75ms
step:1709/2090 train_time:93600ms step_avg:54.77ms
step:1710/2090 train_time:93688ms step_avg:54.79ms
step:1711/2090 train_time:93776ms step_avg:54.81ms
step:1712/2090 train_time:93862ms step_avg:54.83ms
step:1713/2090 train_time:93950ms step_avg:54.85ms
step:1714/2090 train_time:94038ms step_avg:54.86ms
step:1715/2090 train_time:94125ms step_avg:54.88ms
step:1716/2090 train_time:94212ms step_avg:54.90ms
step:1717/2090 train_time:94300ms step_avg:54.92ms
step:1718/2090 train_time:94388ms step_avg:54.94ms
step:1719/2090 train_time:94475ms step_avg:54.96ms
step:1720/2090 train_time:94562ms step_avg:54.98ms
step:1721/2090 train_time:94650ms step_avg:55.00ms
step:1722/2090 train_time:94738ms step_avg:55.02ms
step:1723/2090 train_time:94825ms step_avg:55.03ms
step:1724/2090 train_time:94913ms step_avg:55.05ms
step:1725/2090 train_time:95001ms step_avg:55.07ms
step:1726/2090 train_time:95088ms step_avg:55.09ms
step:1727/2090 train_time:95177ms step_avg:55.11ms
step:1728/2090 train_time:95263ms step_avg:55.13ms
step:1729/2090 train_time:95351ms step_avg:55.15ms
step:1730/2090 train_time:95438ms step_avg:55.17ms
step:1731/2090 train_time:95525ms step_avg:55.18ms
step:1732/2090 train_time:95612ms step_avg:55.20ms
step:1733/2090 train_time:95700ms step_avg:55.22ms
step:1734/2090 train_time:95788ms step_avg:55.24ms
step:1735/2090 train_time:95876ms step_avg:55.26ms
step:1736/2090 train_time:95962ms step_avg:55.28ms
step:1737/2090 train_time:96051ms step_avg:55.30ms
step:1738/2090 train_time:96138ms step_avg:55.32ms
step:1739/2090 train_time:96225ms step_avg:55.33ms
step:1740/2090 train_time:96312ms step_avg:55.35ms
step:1741/2090 train_time:96400ms step_avg:55.37ms
step:1742/2090 train_time:96488ms step_avg:55.39ms
step:1743/2090 train_time:96576ms step_avg:55.41ms
step:1744/2090 train_time:96663ms step_avg:55.43ms
step:1745/2090 train_time:96751ms step_avg:55.44ms
step:1746/2090 train_time:96838ms step_avg:55.46ms
step:1747/2090 train_time:96925ms step_avg:55.48ms
step:1748/2090 train_time:97012ms step_avg:55.50ms
step:1749/2090 train_time:97101ms step_avg:55.52ms
step:1750/2090 train_time:97188ms step_avg:55.54ms
step:1750/2090 val_loss:3.3735 train_time:97278ms step_avg:55.59ms
step:1751/2090 train_time:97298ms step_avg:55.57ms
step:1752/2090 train_time:97368ms step_avg:55.58ms
step:1753/2090 train_time:97462ms step_avg:55.60ms
step:1754/2090 train_time:97552ms step_avg:55.62ms
step:1755/2090 train_time:97639ms step_avg:55.63ms
step:1756/2090 train_time:97725ms step_avg:55.65ms
step:1757/2090 train_time:97811ms step_avg:55.67ms
step:1758/2090 train_time:97897ms step_avg:55.69ms
step:1759/2090 train_time:97984ms step_avg:55.70ms
step:1760/2090 train_time:98070ms step_avg:55.72ms
step:1761/2090 train_time:98157ms step_avg:55.74ms
step:1762/2090 train_time:98245ms step_avg:55.76ms
step:1763/2090 train_time:98335ms step_avg:55.78ms
step:1764/2090 train_time:98424ms step_avg:55.80ms
step:1765/2090 train_time:98513ms step_avg:55.81ms
step:1766/2090 train_time:98601ms step_avg:55.83ms
step:1767/2090 train_time:98689ms step_avg:55.85ms
step:1768/2090 train_time:98776ms step_avg:55.87ms
step:1769/2090 train_time:98864ms step_avg:55.89ms
step:1770/2090 train_time:98950ms step_avg:55.90ms
step:1771/2090 train_time:99037ms step_avg:55.92ms
step:1772/2090 train_time:99123ms step_avg:55.94ms
step:1773/2090 train_time:99211ms step_avg:55.96ms
step:1774/2090 train_time:99298ms step_avg:55.97ms
step:1775/2090 train_time:99388ms step_avg:55.99ms
step:1776/2090 train_time:99476ms step_avg:56.01ms
step:1777/2090 train_time:99565ms step_avg:56.03ms
step:1778/2090 train_time:99653ms step_avg:56.05ms
step:1779/2090 train_time:99740ms step_avg:56.07ms
step:1780/2090 train_time:99827ms step_avg:56.08ms
step:1781/2090 train_time:99914ms step_avg:56.10ms
step:1782/2090 train_time:99999ms step_avg:56.12ms
step:1783/2090 train_time:100087ms step_avg:56.13ms
step:1784/2090 train_time:100174ms step_avg:56.15ms
step:1785/2090 train_time:100262ms step_avg:56.17ms
step:1786/2090 train_time:100349ms step_avg:56.19ms
step:1787/2090 train_time:100439ms step_avg:56.21ms
step:1788/2090 train_time:100527ms step_avg:56.22ms
step:1789/2090 train_time:100616ms step_avg:56.24ms
step:1790/2090 train_time:100704ms step_avg:56.26ms
step:1791/2090 train_time:100792ms step_avg:56.28ms
step:1792/2090 train_time:100878ms step_avg:56.29ms
step:1793/2090 train_time:100965ms step_avg:56.31ms
step:1794/2090 train_time:101051ms step_avg:56.33ms
step:1795/2090 train_time:101140ms step_avg:56.35ms
step:1796/2090 train_time:101226ms step_avg:56.36ms
step:1797/2090 train_time:101314ms step_avg:56.38ms
step:1798/2090 train_time:101400ms step_avg:56.40ms
step:1799/2090 train_time:101489ms step_avg:56.41ms
step:1800/2090 train_time:101576ms step_avg:56.43ms
step:1801/2090 train_time:101664ms step_avg:56.45ms
step:1802/2090 train_time:101751ms step_avg:56.47ms
step:1803/2090 train_time:101840ms step_avg:56.48ms
step:1804/2090 train_time:101926ms step_avg:56.50ms
step:1805/2090 train_time:102014ms step_avg:56.52ms
step:1806/2090 train_time:102100ms step_avg:56.53ms
step:1807/2090 train_time:102187ms step_avg:56.55ms
step:1808/2090 train_time:102274ms step_avg:56.57ms
step:1809/2090 train_time:102362ms step_avg:56.58ms
step:1810/2090 train_time:102449ms step_avg:56.60ms
step:1811/2090 train_time:102538ms step_avg:56.62ms
step:1812/2090 train_time:102626ms step_avg:56.64ms
step:1813/2090 train_time:102715ms step_avg:56.65ms
step:1814/2090 train_time:102801ms step_avg:56.67ms
step:1815/2090 train_time:102889ms step_avg:56.69ms
step:1816/2090 train_time:102975ms step_avg:56.70ms
step:1817/2090 train_time:103062ms step_avg:56.72ms
step:1818/2090 train_time:103150ms step_avg:56.74ms
step:1819/2090 train_time:103239ms step_avg:56.76ms
step:1820/2090 train_time:103326ms step_avg:56.77ms
step:1821/2090 train_time:103414ms step_avg:56.79ms
step:1822/2090 train_time:103501ms step_avg:56.81ms
step:1823/2090 train_time:103591ms step_avg:56.82ms
step:1824/2090 train_time:103678ms step_avg:56.84ms
step:1825/2090 train_time:103766ms step_avg:56.86ms
step:1826/2090 train_time:103854ms step_avg:56.87ms
step:1827/2090 train_time:103940ms step_avg:56.89ms
step:1828/2090 train_time:104027ms step_avg:56.91ms
step:1829/2090 train_time:104115ms step_avg:56.92ms
step:1830/2090 train_time:104201ms step_avg:56.94ms
step:1831/2090 train_time:104290ms step_avg:56.96ms
step:1832/2090 train_time:104377ms step_avg:56.97ms
step:1833/2090 train_time:104465ms step_avg:56.99ms
step:1834/2090 train_time:104553ms step_avg:57.01ms
step:1835/2090 train_time:104641ms step_avg:57.03ms
step:1836/2090 train_time:104728ms step_avg:57.04ms
step:1837/2090 train_time:104816ms step_avg:57.06ms
step:1838/2090 train_time:104902ms step_avg:57.07ms
step:1839/2090 train_time:104991ms step_avg:57.09ms
step:1840/2090 train_time:105078ms step_avg:57.11ms
step:1841/2090 train_time:105165ms step_avg:57.12ms
step:1842/2090 train_time:105252ms step_avg:57.14ms
step:1843/2090 train_time:105339ms step_avg:57.16ms
step:1844/2090 train_time:105426ms step_avg:57.17ms
step:1845/2090 train_time:105516ms step_avg:57.19ms
step:1846/2090 train_time:105602ms step_avg:57.21ms
step:1847/2090 train_time:105691ms step_avg:57.22ms
step:1848/2090 train_time:105778ms step_avg:57.24ms
step:1849/2090 train_time:105866ms step_avg:57.26ms
step:1850/2090 train_time:105953ms step_avg:57.27ms
step:1851/2090 train_time:106041ms step_avg:57.29ms
step:1852/2090 train_time:106127ms step_avg:57.30ms
step:1853/2090 train_time:106215ms step_avg:57.32ms
step:1854/2090 train_time:106301ms step_avg:57.34ms
step:1855/2090 train_time:106389ms step_avg:57.35ms
step:1856/2090 train_time:106476ms step_avg:57.37ms
step:1857/2090 train_time:106563ms step_avg:57.38ms
step:1858/2090 train_time:106651ms step_avg:57.40ms
step:1859/2090 train_time:106739ms step_avg:57.42ms
step:1860/2090 train_time:106826ms step_avg:57.43ms
step:1861/2090 train_time:106914ms step_avg:57.45ms
step:1862/2090 train_time:107002ms step_avg:57.47ms
step:1863/2090 train_time:107090ms step_avg:57.48ms
step:1864/2090 train_time:107176ms step_avg:57.50ms
step:1865/2090 train_time:107264ms step_avg:57.51ms
step:1866/2090 train_time:107352ms step_avg:57.53ms
step:1867/2090 train_time:107440ms step_avg:57.55ms
step:1868/2090 train_time:107527ms step_avg:57.56ms
step:1869/2090 train_time:107616ms step_avg:57.58ms
step:1870/2090 train_time:107702ms step_avg:57.59ms
step:1871/2090 train_time:107791ms step_avg:57.61ms
step:1872/2090 train_time:107879ms step_avg:57.63ms
step:1873/2090 train_time:107968ms step_avg:57.64ms
step:1874/2090 train_time:108054ms step_avg:57.66ms
step:1875/2090 train_time:108141ms step_avg:57.68ms
step:1876/2090 train_time:108228ms step_avg:57.69ms
step:1877/2090 train_time:108317ms step_avg:57.71ms
step:1878/2090 train_time:108403ms step_avg:57.72ms
step:1879/2090 train_time:108492ms step_avg:57.74ms
step:1880/2090 train_time:108578ms step_avg:57.75ms
step:1881/2090 train_time:108666ms step_avg:57.77ms
step:1882/2090 train_time:108753ms step_avg:57.79ms
step:1883/2090 train_time:108841ms step_avg:57.80ms
step:1884/2090 train_time:108928ms step_avg:57.82ms
step:1885/2090 train_time:109016ms step_avg:57.83ms
step:1886/2090 train_time:109102ms step_avg:57.85ms
step:1887/2090 train_time:109190ms step_avg:57.86ms
step:1888/2090 train_time:109277ms step_avg:57.88ms
step:1889/2090 train_time:109365ms step_avg:57.90ms
step:1890/2090 train_time:109453ms step_avg:57.91ms
step:1891/2090 train_time:109540ms step_avg:57.93ms
step:1892/2090 train_time:109627ms step_avg:57.94ms
step:1893/2090 train_time:109715ms step_avg:57.96ms
step:1894/2090 train_time:109802ms step_avg:57.97ms
step:1895/2090 train_time:109890ms step_avg:57.99ms
step:1896/2090 train_time:109976ms step_avg:58.00ms
step:1897/2090 train_time:110064ms step_avg:58.02ms
step:1898/2090 train_time:110151ms step_avg:58.04ms
step:1899/2090 train_time:110239ms step_avg:58.05ms
step:1900/2090 train_time:110327ms step_avg:58.07ms
step:1901/2090 train_time:110415ms step_avg:58.08ms
step:1902/2090 train_time:110502ms step_avg:58.10ms
step:1903/2090 train_time:110590ms step_avg:58.11ms
step:1904/2090 train_time:110677ms step_avg:58.13ms
step:1905/2090 train_time:110765ms step_avg:58.14ms
step:1906/2090 train_time:110853ms step_avg:58.16ms
step:1907/2090 train_time:110941ms step_avg:58.18ms
step:1908/2090 train_time:111027ms step_avg:58.19ms
step:1909/2090 train_time:111115ms step_avg:58.21ms
step:1910/2090 train_time:111201ms step_avg:58.22ms
step:1911/2090 train_time:111289ms step_avg:58.24ms
step:1912/2090 train_time:111376ms step_avg:58.25ms
step:1913/2090 train_time:111464ms step_avg:58.27ms
step:1914/2090 train_time:111551ms step_avg:58.28ms
step:1915/2090 train_time:111639ms step_avg:58.30ms
step:1916/2090 train_time:111726ms step_avg:58.31ms
step:1917/2090 train_time:111814ms step_avg:58.33ms
step:1918/2090 train_time:111900ms step_avg:58.34ms
step:1919/2090 train_time:111989ms step_avg:58.36ms
step:1920/2090 train_time:112075ms step_avg:58.37ms
step:1921/2090 train_time:112163ms step_avg:58.39ms
step:1922/2090 train_time:112251ms step_avg:58.40ms
step:1923/2090 train_time:112339ms step_avg:58.42ms
step:1924/2090 train_time:112425ms step_avg:58.43ms
step:1925/2090 train_time:112513ms step_avg:58.45ms
step:1926/2090 train_time:112600ms step_avg:58.46ms
step:1927/2090 train_time:112689ms step_avg:58.48ms
step:1928/2090 train_time:112776ms step_avg:58.49ms
step:1929/2090 train_time:112865ms step_avg:58.51ms
step:1930/2090 train_time:112952ms step_avg:58.52ms
step:1931/2090 train_time:113039ms step_avg:58.54ms
step:1932/2090 train_time:113126ms step_avg:58.55ms
step:1933/2090 train_time:113214ms step_avg:58.57ms
step:1934/2090 train_time:113300ms step_avg:58.58ms
step:1935/2090 train_time:113388ms step_avg:58.60ms
step:1936/2090 train_time:113475ms step_avg:58.61ms
step:1937/2090 train_time:113563ms step_avg:58.63ms
step:1938/2090 train_time:113650ms step_avg:58.64ms
step:1939/2090 train_time:113739ms step_avg:58.66ms
step:1940/2090 train_time:113826ms step_avg:58.67ms
step:1941/2090 train_time:113914ms step_avg:58.69ms
step:1942/2090 train_time:114001ms step_avg:58.70ms
step:1943/2090 train_time:114089ms step_avg:58.72ms
step:1944/2090 train_time:114176ms step_avg:58.73ms
step:1945/2090 train_time:114263ms step_avg:58.75ms
step:1946/2090 train_time:114350ms step_avg:58.76ms
step:1947/2090 train_time:114439ms step_avg:58.78ms
step:1948/2090 train_time:114526ms step_avg:58.79ms
step:1949/2090 train_time:114614ms step_avg:58.81ms
step:1950/2090 train_time:114701ms step_avg:58.82ms
step:1951/2090 train_time:114790ms step_avg:58.84ms
step:1952/2090 train_time:114877ms step_avg:58.85ms
step:1953/2090 train_time:114965ms step_avg:58.87ms
step:1954/2090 train_time:115052ms step_avg:58.88ms
step:1955/2090 train_time:115141ms step_avg:58.90ms
step:1956/2090 train_time:115228ms step_avg:58.91ms
step:1957/2090 train_time:115316ms step_avg:58.92ms
step:1958/2090 train_time:115403ms step_avg:58.94ms
step:1959/2090 train_time:115491ms step_avg:58.95ms
step:1960/2090 train_time:115577ms step_avg:58.97ms
step:1961/2090 train_time:115666ms step_avg:58.98ms
step:1962/2090 train_time:115753ms step_avg:59.00ms
step:1963/2090 train_time:115840ms step_avg:59.01ms
step:1964/2090 train_time:115927ms step_avg:59.03ms
step:1965/2090 train_time:116016ms step_avg:59.04ms
step:1966/2090 train_time:116102ms step_avg:59.05ms
step:1967/2090 train_time:116190ms step_avg:59.07ms
step:1968/2090 train_time:116278ms step_avg:59.08ms
step:1969/2090 train_time:116364ms step_avg:59.10ms
step:1970/2090 train_time:116451ms step_avg:59.11ms
step:1971/2090 train_time:116540ms step_avg:59.13ms
step:1972/2090 train_time:116627ms step_avg:59.14ms
step:1973/2090 train_time:116716ms step_avg:59.16ms
step:1974/2090 train_time:116802ms step_avg:59.17ms
step:1975/2090 train_time:116891ms step_avg:59.19ms
step:1976/2090 train_time:116978ms step_avg:59.20ms
step:1977/2090 train_time:117065ms step_avg:59.21ms
step:1978/2090 train_time:117153ms step_avg:59.23ms
step:1979/2090 train_time:117241ms step_avg:59.24ms
step:1980/2090 train_time:117327ms step_avg:59.26ms
step:1981/2090 train_time:117417ms step_avg:59.27ms
step:1982/2090 train_time:117503ms step_avg:59.29ms
step:1983/2090 train_time:117591ms step_avg:59.30ms
step:1984/2090 train_time:117678ms step_avg:59.31ms
step:1985/2090 train_time:117765ms step_avg:59.33ms
step:1986/2090 train_time:117853ms step_avg:59.34ms
step:1987/2090 train_time:117941ms step_avg:59.36ms
step:1988/2090 train_time:118028ms step_avg:59.37ms
step:1989/2090 train_time:118117ms step_avg:59.39ms
step:1990/2090 train_time:118203ms step_avg:59.40ms
step:1991/2090 train_time:118292ms step_avg:59.41ms
step:1992/2090 train_time:118379ms step_avg:59.43ms
step:1993/2090 train_time:118468ms step_avg:59.44ms
step:1994/2090 train_time:118555ms step_avg:59.46ms
step:1995/2090 train_time:118642ms step_avg:59.47ms
step:1996/2090 train_time:118729ms step_avg:59.48ms
step:1997/2090 train_time:118818ms step_avg:59.50ms
step:1998/2090 train_time:118904ms step_avg:59.51ms
step:1999/2090 train_time:118992ms step_avg:59.53ms
step:2000/2090 train_time:119079ms step_avg:59.54ms
step:2000/2090 val_loss:3.2962 train_time:119169ms step_avg:59.58ms
step:2001/2090 train_time:119188ms step_avg:59.56ms
step:2002/2090 train_time:119258ms step_avg:59.57ms
step:2003/2090 train_time:119350ms step_avg:59.59ms
step:2004/2090 train_time:119438ms step_avg:59.60ms
step:2005/2090 train_time:119525ms step_avg:59.61ms
step:2006/2090 train_time:119610ms step_avg:59.63ms
step:2007/2090 train_time:119697ms step_avg:59.64ms
step:2008/2090 train_time:119783ms step_avg:59.65ms
step:2009/2090 train_time:119870ms step_avg:59.67ms
step:2010/2090 train_time:119956ms step_avg:59.68ms
step:2011/2090 train_time:120044ms step_avg:59.69ms
step:2012/2090 train_time:120132ms step_avg:59.71ms
step:2013/2090 train_time:120222ms step_avg:59.72ms
step:2014/2090 train_time:120311ms step_avg:59.74ms
step:2015/2090 train_time:120400ms step_avg:59.75ms
step:2016/2090 train_time:120487ms step_avg:59.77ms
step:2017/2090 train_time:120576ms step_avg:59.78ms
step:2018/2090 train_time:120662ms step_avg:59.79ms
step:2019/2090 train_time:120749ms step_avg:59.81ms
step:2020/2090 train_time:120835ms step_avg:59.82ms
step:2021/2090 train_time:120921ms step_avg:59.83ms
step:2022/2090 train_time:121007ms step_avg:59.85ms
step:2023/2090 train_time:121095ms step_avg:59.86ms
step:2024/2090 train_time:121183ms step_avg:59.87ms
step:2025/2090 train_time:121273ms step_avg:59.89ms
step:2026/2090 train_time:121362ms step_avg:59.90ms
step:2027/2090 train_time:121450ms step_avg:59.92ms
step:2028/2090 train_time:121537ms step_avg:59.93ms
step:2029/2090 train_time:121625ms step_avg:59.94ms
step:2030/2090 train_time:121712ms step_avg:59.96ms
step:2031/2090 train_time:121799ms step_avg:59.97ms
step:2032/2090 train_time:121885ms step_avg:59.98ms
step:2033/2090 train_time:121972ms step_avg:60.00ms
step:2034/2090 train_time:122058ms step_avg:60.01ms
step:2035/2090 train_time:122147ms step_avg:60.02ms
step:2036/2090 train_time:122235ms step_avg:60.04ms
step:2037/2090 train_time:122323ms step_avg:60.05ms
step:2038/2090 train_time:122411ms step_avg:60.06ms
step:2039/2090 train_time:122499ms step_avg:60.08ms
step:2040/2090 train_time:122586ms step_avg:60.09ms
step:2041/2090 train_time:122674ms step_avg:60.10ms
step:2042/2090 train_time:122759ms step_avg:60.12ms
step:2043/2090 train_time:122847ms step_avg:60.13ms
step:2044/2090 train_time:122934ms step_avg:60.14ms
step:2045/2090 train_time:123021ms step_avg:60.16ms
step:2046/2090 train_time:123108ms step_avg:60.17ms
step:2047/2090 train_time:123198ms step_avg:60.18ms
step:2048/2090 train_time:123285ms step_avg:60.20ms
step:2049/2090 train_time:123375ms step_avg:60.21ms
step:2050/2090 train_time:123464ms step_avg:60.23ms
step:2051/2090 train_time:123553ms step_avg:60.24ms
step:2052/2090 train_time:123640ms step_avg:60.25ms
step:2053/2090 train_time:123727ms step_avg:60.27ms
step:2054/2090 train_time:123814ms step_avg:60.28ms
step:2055/2090 train_time:123902ms step_avg:60.29ms
step:2056/2090 train_time:123989ms step_avg:60.31ms
step:2057/2090 train_time:124077ms step_avg:60.32ms
step:2058/2090 train_time:124165ms step_avg:60.33ms
step:2059/2090 train_time:124254ms step_avg:60.35ms
step:2060/2090 train_time:124341ms step_avg:60.36ms
step:2061/2090 train_time:124430ms step_avg:60.37ms
step:2062/2090 train_time:124518ms step_avg:60.39ms
step:2063/2090 train_time:124607ms step_avg:60.40ms
step:2064/2090 train_time:124695ms step_avg:60.41ms
step:2065/2090 train_time:124782ms step_avg:60.43ms
step:2066/2090 train_time:124869ms step_avg:60.44ms
step:2067/2090 train_time:124957ms step_avg:60.45ms
step:2068/2090 train_time:125044ms step_avg:60.47ms
step:2069/2090 train_time:125132ms step_avg:60.48ms
step:2070/2090 train_time:125219ms step_avg:60.49ms
step:2071/2090 train_time:125308ms step_avg:60.51ms
step:2072/2090 train_time:125396ms step_avg:60.52ms
step:2073/2090 train_time:125484ms step_avg:60.53ms
step:2074/2090 train_time:125572ms step_avg:60.55ms
step:2075/2090 train_time:125660ms step_avg:60.56ms
step:2076/2090 train_time:125747ms step_avg:60.57ms
step:2077/2090 train_time:125836ms step_avg:60.59ms
step:2078/2090 train_time:125923ms step_avg:60.60ms
step:2079/2090 train_time:126011ms step_avg:60.61ms
step:2080/2090 train_time:126098ms step_avg:60.62ms
step:2081/2090 train_time:126186ms step_avg:60.64ms
step:2082/2090 train_time:126274ms step_avg:60.65ms
step:2083/2090 train_time:126362ms step_avg:60.66ms
step:2084/2090 train_time:126450ms step_avg:60.68ms
step:2085/2090 train_time:126538ms step_avg:60.69ms
step:2086/2090 train_time:126625ms step_avg:60.70ms
step:2087/2090 train_time:126714ms step_avg:60.72ms
step:2088/2090 train_time:126801ms step_avg:60.73ms
step:2089/2090 train_time:126889ms step_avg:60.74ms
step:2090/2090 train_time:126975ms step_avg:60.75ms
step:2090/2090 val_loss:3.2746 train_time:127065ms step_avg:60.80ms
peak memory allocated: 29862 MiB reserved: 38796 MiB

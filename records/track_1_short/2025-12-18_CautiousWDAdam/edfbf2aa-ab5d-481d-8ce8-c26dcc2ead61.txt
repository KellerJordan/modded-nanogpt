import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # weight decay
                if wd != 0:
                    mask = (update * p_slice) >= 0
                    # lr as weight decay  schedule
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)

                    update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)
                
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2050  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 18 12:41:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2090 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2090 train_time:73ms step_avg:73.31ms
step:2/2090 train_time:97ms step_avg:48.54ms
step:3/2090 train_time:121ms step_avg:40.42ms
step:4/2090 train_time:153ms step_avg:38.36ms
step:5/2090 train_time:186ms step_avg:37.20ms
step:6/2090 train_time:272ms step_avg:45.41ms
step:7/2090 train_time:295ms step_avg:42.19ms
step:8/2090 train_time:328ms step_avg:41.04ms
step:9/2090 train_time:361ms step_avg:40.11ms
step:10/2090 train_time:394ms step_avg:39.38ms
step:11/2090 train_time:427ms step_avg:38.86ms
step:12/2090 train_time:460ms step_avg:38.35ms
step:13/2090 train_time:494ms step_avg:37.99ms
step:14/2090 train_time:527ms step_avg:37.62ms
step:15/2090 train_time:560ms step_avg:37.34ms
step:16/2090 train_time:593ms step_avg:37.05ms
step:17/2090 train_time:627ms step_avg:36.86ms
step:18/2090 train_time:660ms step_avg:36.64ms
step:19/2090 train_time:693ms step_avg:36.49ms
step:20/2090 train_time:726ms step_avg:36.31ms
step:21/2090 train_time:760ms step_avg:36.19ms
step:22/2090 train_time:793ms step_avg:36.04ms
step:23/2090 train_time:827ms step_avg:35.94ms
step:24/2090 train_time:860ms step_avg:35.81ms
step:25/2090 train_time:893ms step_avg:35.74ms
step:26/2090 train_time:926ms step_avg:35.62ms
step:27/2090 train_time:960ms step_avg:35.55ms
step:28/2090 train_time:993ms step_avg:35.46ms
step:29/2090 train_time:1026ms step_avg:35.39ms
step:30/2090 train_time:1059ms step_avg:35.31ms
step:31/2090 train_time:1093ms step_avg:35.25ms
step:32/2090 train_time:1126ms step_avg:35.17ms
step:33/2090 train_time:1159ms step_avg:35.13ms
step:34/2090 train_time:1193ms step_avg:35.07ms
step:35/2090 train_time:1228ms step_avg:35.07ms
step:36/2090 train_time:1261ms step_avg:35.02ms
step:37/2090 train_time:1295ms step_avg:35.00ms
step:38/2090 train_time:1328ms step_avg:34.95ms
step:39/2090 train_time:1363ms step_avg:34.94ms
step:40/2090 train_time:1395ms step_avg:34.89ms
step:41/2090 train_time:1430ms step_avg:34.87ms
step:42/2090 train_time:1463ms step_avg:34.83ms
step:43/2090 train_time:1496ms step_avg:34.79ms
step:44/2090 train_time:1529ms step_avg:34.74ms
step:45/2090 train_time:1563ms step_avg:34.73ms
step:46/2090 train_time:1596ms step_avg:34.69ms
step:47/2090 train_time:1629ms step_avg:34.67ms
step:48/2090 train_time:1662ms step_avg:34.63ms
step:49/2090 train_time:1696ms step_avg:34.60ms
step:50/2090 train_time:1728ms step_avg:34.57ms
step:51/2090 train_time:1762ms step_avg:34.54ms
step:52/2090 train_time:1795ms step_avg:34.51ms
step:53/2090 train_time:1828ms step_avg:34.49ms
step:54/2090 train_time:1861ms step_avg:34.46ms
step:55/2090 train_time:1894ms step_avg:34.44ms
step:56/2090 train_time:1927ms step_avg:34.41ms
step:57/2090 train_time:1961ms step_avg:34.40ms
step:58/2090 train_time:1994ms step_avg:34.37ms
step:59/2090 train_time:2027ms step_avg:34.36ms
step:60/2090 train_time:2060ms step_avg:34.33ms
step:61/2090 train_time:2094ms step_avg:34.32ms
step:62/2090 train_time:2126ms step_avg:34.30ms
step:63/2090 train_time:2160ms step_avg:34.29ms
step:64/2090 train_time:2193ms step_avg:34.27ms
step:65/2090 train_time:2227ms step_avg:34.26ms
step:66/2090 train_time:2260ms step_avg:34.24ms
step:67/2090 train_time:2294ms step_avg:34.24ms
step:68/2090 train_time:2327ms step_avg:34.21ms
step:69/2090 train_time:2361ms step_avg:34.21ms
step:70/2090 train_time:2393ms step_avg:34.19ms
step:71/2090 train_time:2427ms step_avg:34.19ms
step:72/2090 train_time:2460ms step_avg:34.17ms
step:73/2090 train_time:2494ms step_avg:34.16ms
step:74/2090 train_time:2527ms step_avg:34.15ms
step:75/2090 train_time:2561ms step_avg:34.15ms
step:76/2090 train_time:2594ms step_avg:34.13ms
step:77/2090 train_time:2628ms step_avg:34.13ms
step:78/2090 train_time:2661ms step_avg:34.11ms
step:79/2090 train_time:2694ms step_avg:34.10ms
step:80/2090 train_time:2727ms step_avg:34.09ms
step:81/2090 train_time:2761ms step_avg:34.08ms
step:82/2090 train_time:2794ms step_avg:34.07ms
step:83/2090 train_time:2827ms step_avg:34.06ms
step:84/2090 train_time:2860ms step_avg:34.05ms
step:85/2090 train_time:2893ms step_avg:34.04ms
step:86/2090 train_time:2926ms step_avg:34.02ms
step:87/2090 train_time:2959ms step_avg:34.01ms
step:88/2090 train_time:2992ms step_avg:34.00ms
step:89/2090 train_time:3025ms step_avg:33.99ms
step:90/2090 train_time:3058ms step_avg:33.98ms
step:91/2090 train_time:3092ms step_avg:33.97ms
step:92/2090 train_time:3124ms step_avg:33.96ms
step:93/2090 train_time:3158ms step_avg:33.95ms
step:94/2090 train_time:3191ms step_avg:33.95ms
step:95/2090 train_time:3224ms step_avg:33.94ms
step:96/2090 train_time:3257ms step_avg:33.93ms
step:97/2090 train_time:3291ms step_avg:33.92ms
step:98/2090 train_time:3323ms step_avg:33.91ms
step:99/2090 train_time:3356ms step_avg:33.90ms
step:100/2090 train_time:3389ms step_avg:33.89ms
step:101/2090 train_time:3422ms step_avg:33.88ms
step:102/2090 train_time:3455ms step_avg:33.88ms
step:103/2090 train_time:3489ms step_avg:33.88ms
step:104/2090 train_time:3522ms step_avg:33.86ms
step:105/2090 train_time:3555ms step_avg:33.86ms
step:106/2090 train_time:3588ms step_avg:33.85ms
step:107/2090 train_time:3622ms step_avg:33.85ms
step:108/2090 train_time:3655ms step_avg:33.84ms
step:109/2090 train_time:3688ms step_avg:33.84ms
step:110/2090 train_time:3721ms step_avg:33.83ms
step:111/2090 train_time:3754ms step_avg:33.82ms
step:112/2090 train_time:3787ms step_avg:33.81ms
step:113/2090 train_time:3820ms step_avg:33.81ms
step:114/2090 train_time:3853ms step_avg:33.80ms
step:115/2090 train_time:3886ms step_avg:33.80ms
step:116/2090 train_time:3919ms step_avg:33.79ms
step:117/2090 train_time:3953ms step_avg:33.79ms
step:118/2090 train_time:3986ms step_avg:33.78ms
step:119/2090 train_time:4020ms step_avg:33.78ms
step:120/2090 train_time:4053ms step_avg:33.77ms
step:121/2090 train_time:4086ms step_avg:33.77ms
step:122/2090 train_time:4118ms step_avg:33.76ms
step:123/2090 train_time:4152ms step_avg:33.76ms
step:124/2090 train_time:4185ms step_avg:33.75ms
step:125/2090 train_time:4218ms step_avg:33.74ms
step:126/2090 train_time:4251ms step_avg:33.74ms
step:127/2090 train_time:4284ms step_avg:33.74ms
step:128/2090 train_time:4317ms step_avg:33.73ms
step:129/2090 train_time:4351ms step_avg:33.73ms
step:130/2090 train_time:4383ms step_avg:33.72ms
step:131/2090 train_time:4417ms step_avg:33.72ms
step:132/2090 train_time:4449ms step_avg:33.71ms
step:133/2090 train_time:4483ms step_avg:33.70ms
step:134/2090 train_time:4515ms step_avg:33.70ms
step:135/2090 train_time:4549ms step_avg:33.70ms
step:136/2090 train_time:4582ms step_avg:33.69ms
step:137/2090 train_time:4615ms step_avg:33.69ms
step:138/2090 train_time:4648ms step_avg:33.68ms
step:139/2090 train_time:4682ms step_avg:33.68ms
step:140/2090 train_time:4714ms step_avg:33.67ms
step:141/2090 train_time:4748ms step_avg:33.67ms
step:142/2090 train_time:4781ms step_avg:33.67ms
step:143/2090 train_time:4814ms step_avg:33.67ms
step:144/2090 train_time:4847ms step_avg:33.66ms
step:145/2090 train_time:4880ms step_avg:33.66ms
step:146/2090 train_time:4913ms step_avg:33.65ms
step:147/2090 train_time:4947ms step_avg:33.65ms
step:148/2090 train_time:4980ms step_avg:33.65ms
step:149/2090 train_time:5013ms step_avg:33.64ms
step:150/2090 train_time:5046ms step_avg:33.64ms
step:151/2090 train_time:5079ms step_avg:33.64ms
step:152/2090 train_time:5112ms step_avg:33.63ms
step:153/2090 train_time:5145ms step_avg:33.63ms
step:154/2090 train_time:5178ms step_avg:33.62ms
step:155/2090 train_time:5211ms step_avg:33.62ms
step:156/2090 train_time:5244ms step_avg:33.62ms
step:157/2090 train_time:5277ms step_avg:33.61ms
step:158/2090 train_time:5310ms step_avg:33.61ms
step:159/2090 train_time:5344ms step_avg:33.61ms
step:160/2090 train_time:5376ms step_avg:33.60ms
step:161/2090 train_time:5410ms step_avg:33.60ms
step:162/2090 train_time:5442ms step_avg:33.59ms
step:163/2090 train_time:5476ms step_avg:33.59ms
step:164/2090 train_time:5508ms step_avg:33.59ms
step:165/2090 train_time:5542ms step_avg:33.59ms
step:166/2090 train_time:5575ms step_avg:33.58ms
step:167/2090 train_time:5608ms step_avg:33.58ms
step:168/2090 train_time:5641ms step_avg:33.58ms
step:169/2090 train_time:5674ms step_avg:33.58ms
step:170/2090 train_time:5707ms step_avg:33.57ms
step:171/2090 train_time:5740ms step_avg:33.57ms
step:172/2090 train_time:5773ms step_avg:33.57ms
step:173/2090 train_time:5807ms step_avg:33.57ms
step:174/2090 train_time:5840ms step_avg:33.56ms
step:175/2090 train_time:5873ms step_avg:33.56ms
step:176/2090 train_time:5905ms step_avg:33.55ms
step:177/2090 train_time:5939ms step_avg:33.55ms
step:178/2090 train_time:5972ms step_avg:33.55ms
step:179/2090 train_time:6005ms step_avg:33.55ms
step:180/2090 train_time:6038ms step_avg:33.54ms
step:181/2090 train_time:6071ms step_avg:33.54ms
step:182/2090 train_time:6104ms step_avg:33.54ms
step:183/2090 train_time:6137ms step_avg:33.54ms
step:184/2090 train_time:6170ms step_avg:33.53ms
step:185/2090 train_time:6203ms step_avg:33.53ms
step:186/2090 train_time:6236ms step_avg:33.53ms
step:187/2090 train_time:6269ms step_avg:33.52ms
step:188/2090 train_time:6302ms step_avg:33.52ms
step:189/2090 train_time:6335ms step_avg:33.52ms
step:190/2090 train_time:6368ms step_avg:33.51ms
step:191/2090 train_time:6401ms step_avg:33.51ms
step:192/2090 train_time:6434ms step_avg:33.51ms
step:193/2090 train_time:6468ms step_avg:33.51ms
step:194/2090 train_time:6500ms step_avg:33.51ms
step:195/2090 train_time:6534ms step_avg:33.51ms
step:196/2090 train_time:6567ms step_avg:33.50ms
step:197/2090 train_time:6601ms step_avg:33.51ms
step:198/2090 train_time:6633ms step_avg:33.50ms
step:199/2090 train_time:6667ms step_avg:33.50ms
step:200/2090 train_time:6700ms step_avg:33.50ms
step:201/2090 train_time:6733ms step_avg:33.50ms
step:202/2090 train_time:6766ms step_avg:33.50ms
step:203/2090 train_time:6799ms step_avg:33.49ms
step:204/2090 train_time:6832ms step_avg:33.49ms
step:205/2090 train_time:6865ms step_avg:33.49ms
step:206/2090 train_time:6898ms step_avg:33.49ms
step:207/2090 train_time:6932ms step_avg:33.49ms
step:208/2090 train_time:6964ms step_avg:33.48ms
step:209/2090 train_time:6998ms step_avg:33.48ms
step:210/2090 train_time:7030ms step_avg:33.48ms
step:211/2090 train_time:7064ms step_avg:33.48ms
step:212/2090 train_time:7096ms step_avg:33.47ms
step:213/2090 train_time:7130ms step_avg:33.47ms
step:214/2090 train_time:7162ms step_avg:33.47ms
step:215/2090 train_time:7195ms step_avg:33.47ms
step:216/2090 train_time:7228ms step_avg:33.46ms
step:217/2090 train_time:7261ms step_avg:33.46ms
step:218/2090 train_time:7294ms step_avg:33.46ms
step:219/2090 train_time:7327ms step_avg:33.46ms
step:220/2090 train_time:7360ms step_avg:33.46ms
step:221/2090 train_time:7393ms step_avg:33.45ms
step:222/2090 train_time:7426ms step_avg:33.45ms
step:223/2090 train_time:7459ms step_avg:33.45ms
step:224/2090 train_time:7492ms step_avg:33.44ms
step:225/2090 train_time:7525ms step_avg:33.45ms
step:226/2090 train_time:7558ms step_avg:33.44ms
step:227/2090 train_time:7591ms step_avg:33.44ms
step:228/2090 train_time:7624ms step_avg:33.44ms
step:229/2090 train_time:7657ms step_avg:33.44ms
step:230/2090 train_time:7689ms step_avg:33.43ms
step:231/2090 train_time:7723ms step_avg:33.43ms
step:232/2090 train_time:7756ms step_avg:33.43ms
step:233/2090 train_time:7789ms step_avg:33.43ms
step:234/2090 train_time:7822ms step_avg:33.43ms
step:235/2090 train_time:7855ms step_avg:33.43ms
step:236/2090 train_time:7888ms step_avg:33.42ms
step:237/2090 train_time:7921ms step_avg:33.42ms
step:238/2090 train_time:7954ms step_avg:33.42ms
step:239/2090 train_time:7988ms step_avg:33.42ms
step:240/2090 train_time:8020ms step_avg:33.42ms
step:241/2090 train_time:8054ms step_avg:33.42ms
step:242/2090 train_time:8086ms step_avg:33.41ms
step:243/2090 train_time:8120ms step_avg:33.41ms
step:244/2090 train_time:8153ms step_avg:33.41ms
step:245/2090 train_time:8186ms step_avg:33.41ms
step:246/2090 train_time:8219ms step_avg:33.41ms
step:247/2090 train_time:8252ms step_avg:33.41ms
step:248/2090 train_time:8285ms step_avg:33.41ms
step:249/2090 train_time:8318ms step_avg:33.41ms
step:250/2090 train_time:8351ms step_avg:33.40ms
step:250/2090 val_loss:4.2693 train_time:8386ms step_avg:33.55ms
step:251/2090 train_time:8406ms step_avg:33.49ms
step:252/2090 train_time:8426ms step_avg:33.44ms
step:253/2090 train_time:8454ms step_avg:33.41ms
step:254/2090 train_time:8487ms step_avg:33.41ms
step:255/2090 train_time:8523ms step_avg:33.42ms
step:256/2090 train_time:8556ms step_avg:33.42ms
step:257/2090 train_time:8591ms step_avg:33.43ms
step:258/2090 train_time:8624ms step_avg:33.42ms
step:259/2090 train_time:8657ms step_avg:33.43ms
step:260/2090 train_time:8690ms step_avg:33.42ms
step:261/2090 train_time:8723ms step_avg:33.42ms
step:262/2090 train_time:8756ms step_avg:33.42ms
step:263/2090 train_time:8789ms step_avg:33.42ms
step:264/2090 train_time:8822ms step_avg:33.42ms
step:265/2090 train_time:8855ms step_avg:33.41ms
step:266/2090 train_time:8887ms step_avg:33.41ms
step:267/2090 train_time:8920ms step_avg:33.41ms
step:268/2090 train_time:8953ms step_avg:33.41ms
step:269/2090 train_time:8986ms step_avg:33.41ms
step:270/2090 train_time:9019ms step_avg:33.40ms
step:271/2090 train_time:9052ms step_avg:33.40ms
step:272/2090 train_time:9085ms step_avg:33.40ms
step:273/2090 train_time:9117ms step_avg:33.40ms
step:274/2090 train_time:9150ms step_avg:33.39ms
step:275/2090 train_time:9183ms step_avg:33.39ms
step:276/2090 train_time:9216ms step_avg:33.39ms
step:277/2090 train_time:9249ms step_avg:33.39ms
step:278/2090 train_time:9281ms step_avg:33.39ms
step:279/2090 train_time:9314ms step_avg:33.38ms
step:280/2090 train_time:9347ms step_avg:33.38ms
step:281/2090 train_time:9380ms step_avg:33.38ms
step:282/2090 train_time:9412ms step_avg:33.38ms
step:283/2090 train_time:9446ms step_avg:33.38ms
step:284/2090 train_time:9479ms step_avg:33.38ms
step:285/2090 train_time:9512ms step_avg:33.37ms
step:286/2090 train_time:9545ms step_avg:33.37ms
step:287/2090 train_time:9578ms step_avg:33.37ms
step:288/2090 train_time:9611ms step_avg:33.37ms
step:289/2090 train_time:9645ms step_avg:33.37ms
step:290/2090 train_time:9677ms step_avg:33.37ms
step:291/2090 train_time:9711ms step_avg:33.37ms
step:292/2090 train_time:9743ms step_avg:33.37ms
step:293/2090 train_time:9777ms step_avg:33.37ms
step:294/2090 train_time:9809ms step_avg:33.37ms
step:295/2090 train_time:9843ms step_avg:33.37ms
step:296/2090 train_time:9876ms step_avg:33.36ms
step:297/2090 train_time:9909ms step_avg:33.36ms
step:298/2090 train_time:9942ms step_avg:33.36ms
step:299/2090 train_time:9975ms step_avg:33.36ms
step:300/2090 train_time:10007ms step_avg:33.36ms
step:301/2090 train_time:10040ms step_avg:33.36ms
step:302/2090 train_time:10073ms step_avg:33.35ms
step:303/2090 train_time:10106ms step_avg:33.35ms
step:304/2090 train_time:10139ms step_avg:33.35ms
step:305/2090 train_time:10172ms step_avg:33.35ms
step:306/2090 train_time:10204ms step_avg:33.35ms
step:307/2090 train_time:10238ms step_avg:33.35ms
step:308/2090 train_time:10270ms step_avg:33.35ms
step:309/2090 train_time:10303ms step_avg:33.34ms
step:310/2090 train_time:10336ms step_avg:33.34ms
step:311/2090 train_time:10369ms step_avg:33.34ms
step:312/2090 train_time:10401ms step_avg:33.34ms
step:313/2090 train_time:10435ms step_avg:33.34ms
step:314/2090 train_time:10467ms step_avg:33.34ms
step:315/2090 train_time:10501ms step_avg:33.34ms
step:316/2090 train_time:10533ms step_avg:33.33ms
step:317/2090 train_time:10567ms step_avg:33.33ms
step:318/2090 train_time:10599ms step_avg:33.33ms
step:319/2090 train_time:10632ms step_avg:33.33ms
step:320/2090 train_time:10665ms step_avg:33.33ms
step:321/2090 train_time:10699ms step_avg:33.33ms
step:322/2090 train_time:10732ms step_avg:33.33ms
step:323/2090 train_time:10765ms step_avg:33.33ms
step:324/2090 train_time:10798ms step_avg:33.33ms
step:325/2090 train_time:10831ms step_avg:33.33ms
step:326/2090 train_time:10864ms step_avg:33.33ms
step:327/2090 train_time:10898ms step_avg:33.33ms
step:328/2090 train_time:10930ms step_avg:33.32ms
step:329/2090 train_time:10964ms step_avg:33.32ms
step:330/2090 train_time:10997ms step_avg:33.32ms
step:331/2090 train_time:11030ms step_avg:33.32ms
step:332/2090 train_time:11062ms step_avg:33.32ms
step:333/2090 train_time:11096ms step_avg:33.32ms
step:334/2090 train_time:11128ms step_avg:33.32ms
step:335/2090 train_time:11161ms step_avg:33.32ms
step:336/2090 train_time:11194ms step_avg:33.32ms
step:337/2090 train_time:11227ms step_avg:33.32ms
step:338/2090 train_time:11260ms step_avg:33.31ms
step:339/2090 train_time:11293ms step_avg:33.31ms
step:340/2090 train_time:11325ms step_avg:33.31ms
step:341/2090 train_time:11358ms step_avg:33.31ms
step:342/2090 train_time:11391ms step_avg:33.31ms
step:343/2090 train_time:11425ms step_avg:33.31ms
step:344/2090 train_time:11457ms step_avg:33.31ms
step:345/2090 train_time:11490ms step_avg:33.31ms
step:346/2090 train_time:11523ms step_avg:33.30ms
step:347/2090 train_time:11556ms step_avg:33.30ms
step:348/2090 train_time:11589ms step_avg:33.30ms
step:349/2090 train_time:11623ms step_avg:33.30ms
step:350/2090 train_time:11655ms step_avg:33.30ms
step:351/2090 train_time:11689ms step_avg:33.30ms
step:352/2090 train_time:11721ms step_avg:33.30ms
step:353/2090 train_time:11754ms step_avg:33.30ms
step:354/2090 train_time:11787ms step_avg:33.30ms
step:355/2090 train_time:11820ms step_avg:33.30ms
step:356/2090 train_time:11853ms step_avg:33.30ms
step:357/2090 train_time:11886ms step_avg:33.30ms
step:358/2090 train_time:11919ms step_avg:33.29ms
step:359/2090 train_time:11952ms step_avg:33.29ms
step:360/2090 train_time:11985ms step_avg:33.29ms
step:361/2090 train_time:12018ms step_avg:33.29ms
step:362/2090 train_time:12051ms step_avg:33.29ms
step:363/2090 train_time:12084ms step_avg:33.29ms
step:364/2090 train_time:12117ms step_avg:33.29ms
step:365/2090 train_time:12150ms step_avg:33.29ms
step:366/2090 train_time:12183ms step_avg:33.29ms
step:367/2090 train_time:12216ms step_avg:33.29ms
step:368/2090 train_time:12249ms step_avg:33.28ms
step:369/2090 train_time:12282ms step_avg:33.28ms
step:370/2090 train_time:12314ms step_avg:33.28ms
step:371/2090 train_time:12347ms step_avg:33.28ms
step:372/2090 train_time:12380ms step_avg:33.28ms
step:373/2090 train_time:12413ms step_avg:33.28ms
step:374/2090 train_time:12446ms step_avg:33.28ms
step:375/2090 train_time:12479ms step_avg:33.28ms
step:376/2090 train_time:12512ms step_avg:33.28ms
step:377/2090 train_time:12545ms step_avg:33.28ms
step:378/2090 train_time:12578ms step_avg:33.27ms
step:379/2090 train_time:12611ms step_avg:33.27ms
step:380/2090 train_time:12644ms step_avg:33.27ms
step:381/2090 train_time:12677ms step_avg:33.27ms
step:382/2090 train_time:12709ms step_avg:33.27ms
step:383/2090 train_time:12743ms step_avg:33.27ms
step:384/2090 train_time:12775ms step_avg:33.27ms
step:385/2090 train_time:12809ms step_avg:33.27ms
step:386/2090 train_time:12841ms step_avg:33.27ms
step:387/2090 train_time:12874ms step_avg:33.27ms
step:388/2090 train_time:12907ms step_avg:33.27ms
step:389/2090 train_time:12941ms step_avg:33.27ms
step:390/2090 train_time:12973ms step_avg:33.26ms
step:391/2090 train_time:13007ms step_avg:33.27ms
step:392/2090 train_time:13039ms step_avg:33.26ms
step:393/2090 train_time:13073ms step_avg:33.26ms
step:394/2090 train_time:13105ms step_avg:33.26ms
step:395/2090 train_time:13139ms step_avg:33.26ms
step:396/2090 train_time:13171ms step_avg:33.26ms
step:397/2090 train_time:13205ms step_avg:33.26ms
step:398/2090 train_time:13237ms step_avg:33.26ms
step:399/2090 train_time:13270ms step_avg:33.26ms
step:400/2090 train_time:13303ms step_avg:33.26ms
step:401/2090 train_time:13336ms step_avg:33.26ms
step:402/2090 train_time:13369ms step_avg:33.26ms
step:403/2090 train_time:13402ms step_avg:33.26ms
step:404/2090 train_time:13435ms step_avg:33.25ms
step:405/2090 train_time:13468ms step_avg:33.25ms
step:406/2090 train_time:13501ms step_avg:33.25ms
step:407/2090 train_time:13534ms step_avg:33.25ms
step:408/2090 train_time:13567ms step_avg:33.25ms
step:409/2090 train_time:13600ms step_avg:33.25ms
step:410/2090 train_time:13633ms step_avg:33.25ms
step:411/2090 train_time:13667ms step_avg:33.25ms
step:412/2090 train_time:13699ms step_avg:33.25ms
step:413/2090 train_time:13732ms step_avg:33.25ms
step:414/2090 train_time:13765ms step_avg:33.25ms
step:415/2090 train_time:13798ms step_avg:33.25ms
step:416/2090 train_time:13831ms step_avg:33.25ms
step:417/2090 train_time:13864ms step_avg:33.25ms
step:418/2090 train_time:13897ms step_avg:33.25ms
step:419/2090 train_time:13930ms step_avg:33.25ms
step:420/2090 train_time:13962ms step_avg:33.24ms
step:421/2090 train_time:13995ms step_avg:33.24ms
step:422/2090 train_time:14028ms step_avg:33.24ms
step:423/2090 train_time:14061ms step_avg:33.24ms
step:424/2090 train_time:14094ms step_avg:33.24ms
step:425/2090 train_time:14127ms step_avg:33.24ms
step:426/2090 train_time:14160ms step_avg:33.24ms
step:427/2090 train_time:14193ms step_avg:33.24ms
step:428/2090 train_time:14226ms step_avg:33.24ms
step:429/2090 train_time:14259ms step_avg:33.24ms
step:430/2090 train_time:14292ms step_avg:33.24ms
step:431/2090 train_time:14325ms step_avg:33.24ms
step:432/2090 train_time:14357ms step_avg:33.23ms
step:433/2090 train_time:14390ms step_avg:33.23ms
step:434/2090 train_time:14423ms step_avg:33.23ms
step:435/2090 train_time:14456ms step_avg:33.23ms
step:436/2090 train_time:14489ms step_avg:33.23ms
step:437/2090 train_time:14522ms step_avg:33.23ms
step:438/2090 train_time:14555ms step_avg:33.23ms
step:439/2090 train_time:14588ms step_avg:33.23ms
step:440/2090 train_time:14620ms step_avg:33.23ms
step:441/2090 train_time:14653ms step_avg:33.23ms
step:442/2090 train_time:14686ms step_avg:33.23ms
step:443/2090 train_time:14719ms step_avg:33.23ms
step:444/2090 train_time:14752ms step_avg:33.23ms
step:445/2090 train_time:14785ms step_avg:33.23ms
step:446/2090 train_time:14818ms step_avg:33.22ms
step:447/2090 train_time:14851ms step_avg:33.22ms
step:448/2090 train_time:14884ms step_avg:33.22ms
step:449/2090 train_time:14917ms step_avg:33.22ms
step:450/2090 train_time:14950ms step_avg:33.22ms
step:451/2090 train_time:14983ms step_avg:33.22ms
step:452/2090 train_time:15016ms step_avg:33.22ms
step:453/2090 train_time:15049ms step_avg:33.22ms
step:454/2090 train_time:15082ms step_avg:33.22ms
step:455/2090 train_time:15115ms step_avg:33.22ms
step:456/2090 train_time:15147ms step_avg:33.22ms
step:457/2090 train_time:15181ms step_avg:33.22ms
step:458/2090 train_time:15213ms step_avg:33.22ms
step:459/2090 train_time:15247ms step_avg:33.22ms
step:460/2090 train_time:15279ms step_avg:33.22ms
step:461/2090 train_time:15313ms step_avg:33.22ms
step:462/2090 train_time:15346ms step_avg:33.22ms
step:463/2090 train_time:15379ms step_avg:33.22ms
step:464/2090 train_time:15412ms step_avg:33.21ms
step:465/2090 train_time:15444ms step_avg:33.21ms
step:466/2090 train_time:15477ms step_avg:33.21ms
step:467/2090 train_time:15510ms step_avg:33.21ms
step:468/2090 train_time:15543ms step_avg:33.21ms
step:469/2090 train_time:15576ms step_avg:33.21ms
step:470/2090 train_time:15609ms step_avg:33.21ms
step:471/2090 train_time:15642ms step_avg:33.21ms
step:472/2090 train_time:15675ms step_avg:33.21ms
step:473/2090 train_time:15708ms step_avg:33.21ms
step:474/2090 train_time:15741ms step_avg:33.21ms
step:475/2090 train_time:15774ms step_avg:33.21ms
step:476/2090 train_time:15807ms step_avg:33.21ms
step:477/2090 train_time:15840ms step_avg:33.21ms
step:478/2090 train_time:15873ms step_avg:33.21ms
step:479/2090 train_time:15906ms step_avg:33.21ms
step:480/2090 train_time:15939ms step_avg:33.21ms
step:481/2090 train_time:15972ms step_avg:33.21ms
step:482/2090 train_time:16005ms step_avg:33.21ms
step:483/2090 train_time:16038ms step_avg:33.21ms
step:484/2090 train_time:16071ms step_avg:33.20ms
step:485/2090 train_time:16104ms step_avg:33.20ms
step:486/2090 train_time:16136ms step_avg:33.20ms
step:487/2090 train_time:16170ms step_avg:33.20ms
step:488/2090 train_time:16202ms step_avg:33.20ms
step:489/2090 train_time:16236ms step_avg:33.20ms
step:490/2090 train_time:16268ms step_avg:33.20ms
step:491/2090 train_time:16302ms step_avg:33.20ms
step:492/2090 train_time:16335ms step_avg:33.20ms
step:493/2090 train_time:16368ms step_avg:33.20ms
step:494/2090 train_time:16400ms step_avg:33.20ms
step:495/2090 train_time:16434ms step_avg:33.20ms
step:496/2090 train_time:16466ms step_avg:33.20ms
step:497/2090 train_time:16500ms step_avg:33.20ms
step:498/2090 train_time:16533ms step_avg:33.20ms
step:499/2090 train_time:16566ms step_avg:33.20ms
step:500/2090 train_time:16599ms step_avg:33.20ms
step:500/2090 val_loss:4.0064 train_time:16634ms step_avg:33.27ms
step:501/2090 train_time:16655ms step_avg:33.24ms
step:502/2090 train_time:16674ms step_avg:33.22ms
step:503/2090 train_time:16701ms step_avg:33.20ms
step:504/2090 train_time:16734ms step_avg:33.20ms
step:505/2090 train_time:16770ms step_avg:33.21ms
step:506/2090 train_time:16804ms step_avg:33.21ms
step:507/2090 train_time:16838ms step_avg:33.21ms
step:508/2090 train_time:16871ms step_avg:33.21ms
step:509/2090 train_time:16905ms step_avg:33.21ms
step:510/2090 train_time:16938ms step_avg:33.21ms
step:511/2090 train_time:16971ms step_avg:33.21ms
step:512/2090 train_time:17004ms step_avg:33.21ms
step:513/2090 train_time:17037ms step_avg:33.21ms
step:514/2090 train_time:17069ms step_avg:33.21ms
step:515/2090 train_time:17103ms step_avg:33.21ms
step:516/2090 train_time:17135ms step_avg:33.21ms
step:517/2090 train_time:17168ms step_avg:33.21ms
step:518/2090 train_time:17201ms step_avg:33.21ms
step:519/2090 train_time:17234ms step_avg:33.21ms
step:520/2090 train_time:17267ms step_avg:33.21ms
step:521/2090 train_time:17299ms step_avg:33.20ms
step:522/2090 train_time:17332ms step_avg:33.20ms
step:523/2090 train_time:17365ms step_avg:33.20ms
step:524/2090 train_time:17397ms step_avg:33.20ms
step:525/2090 train_time:17430ms step_avg:33.20ms
step:526/2090 train_time:17463ms step_avg:33.20ms
step:527/2090 train_time:17496ms step_avg:33.20ms
step:528/2090 train_time:17529ms step_avg:33.20ms
step:529/2090 train_time:17562ms step_avg:33.20ms
step:530/2090 train_time:17595ms step_avg:33.20ms
step:531/2090 train_time:17627ms step_avg:33.20ms
step:532/2090 train_time:17660ms step_avg:33.20ms
step:533/2090 train_time:17693ms step_avg:33.20ms
step:534/2090 train_time:17726ms step_avg:33.20ms
step:535/2090 train_time:17761ms step_avg:33.20ms
step:536/2090 train_time:17793ms step_avg:33.20ms
step:537/2090 train_time:17827ms step_avg:33.20ms
step:538/2090 train_time:17859ms step_avg:33.20ms
step:539/2090 train_time:17893ms step_avg:33.20ms
step:540/2090 train_time:17926ms step_avg:33.20ms
step:541/2090 train_time:17960ms step_avg:33.20ms
step:542/2090 train_time:17992ms step_avg:33.20ms
step:543/2090 train_time:18026ms step_avg:33.20ms
step:544/2090 train_time:18059ms step_avg:33.20ms
step:545/2090 train_time:18091ms step_avg:33.20ms
step:546/2090 train_time:18124ms step_avg:33.19ms
step:547/2090 train_time:18157ms step_avg:33.19ms
step:548/2090 train_time:18190ms step_avg:33.19ms
step:549/2090 train_time:18223ms step_avg:33.19ms
step:550/2090 train_time:18256ms step_avg:33.19ms
step:551/2090 train_time:18289ms step_avg:33.19ms
step:552/2090 train_time:18322ms step_avg:33.19ms
step:553/2090 train_time:18355ms step_avg:33.19ms
step:554/2090 train_time:18387ms step_avg:33.19ms
step:555/2090 train_time:18420ms step_avg:33.19ms
step:556/2090 train_time:18453ms step_avg:33.19ms
step:557/2090 train_time:18486ms step_avg:33.19ms
step:558/2090 train_time:18518ms step_avg:33.19ms
step:559/2090 train_time:18551ms step_avg:33.19ms
step:560/2090 train_time:18584ms step_avg:33.19ms
step:561/2090 train_time:18617ms step_avg:33.19ms
step:562/2090 train_time:18650ms step_avg:33.18ms
step:563/2090 train_time:18683ms step_avg:33.18ms
step:564/2090 train_time:18716ms step_avg:33.18ms
step:565/2090 train_time:18749ms step_avg:33.18ms
step:566/2090 train_time:18782ms step_avg:33.18ms
step:567/2090 train_time:18815ms step_avg:33.18ms
step:568/2090 train_time:18848ms step_avg:33.18ms
step:569/2090 train_time:18881ms step_avg:33.18ms
step:570/2090 train_time:18914ms step_avg:33.18ms
step:571/2090 train_time:18947ms step_avg:33.18ms
step:572/2090 train_time:18980ms step_avg:33.18ms
step:573/2090 train_time:19013ms step_avg:33.18ms
step:574/2090 train_time:19046ms step_avg:33.18ms
step:575/2090 train_time:19080ms step_avg:33.18ms
step:576/2090 train_time:19112ms step_avg:33.18ms
step:577/2090 train_time:19145ms step_avg:33.18ms
step:578/2090 train_time:19178ms step_avg:33.18ms
step:579/2090 train_time:19211ms step_avg:33.18ms
step:580/2090 train_time:19244ms step_avg:33.18ms
step:581/2090 train_time:19277ms step_avg:33.18ms
step:582/2090 train_time:19310ms step_avg:33.18ms
step:583/2090 train_time:19343ms step_avg:33.18ms
step:584/2090 train_time:19376ms step_avg:33.18ms
step:585/2090 train_time:19409ms step_avg:33.18ms
step:586/2090 train_time:19441ms step_avg:33.18ms
step:587/2090 train_time:19474ms step_avg:33.18ms
step:588/2090 train_time:19507ms step_avg:33.18ms
step:589/2090 train_time:19540ms step_avg:33.18ms
step:590/2090 train_time:19573ms step_avg:33.17ms
step:591/2090 train_time:19606ms step_avg:33.17ms
step:592/2090 train_time:19639ms step_avg:33.17ms
step:593/2090 train_time:19672ms step_avg:33.17ms
step:594/2090 train_time:19705ms step_avg:33.17ms
step:595/2090 train_time:19738ms step_avg:33.17ms
step:596/2090 train_time:19770ms step_avg:33.17ms
step:597/2090 train_time:19803ms step_avg:33.17ms
step:598/2090 train_time:19836ms step_avg:33.17ms
step:599/2090 train_time:19869ms step_avg:33.17ms
step:600/2090 train_time:19902ms step_avg:33.17ms
step:601/2090 train_time:19936ms step_avg:33.17ms
step:602/2090 train_time:19968ms step_avg:33.17ms
step:603/2090 train_time:20002ms step_avg:33.17ms
step:604/2090 train_time:20035ms step_avg:33.17ms
step:605/2090 train_time:20068ms step_avg:33.17ms
step:606/2090 train_time:20101ms step_avg:33.17ms
step:607/2090 train_time:20134ms step_avg:33.17ms
step:608/2090 train_time:20166ms step_avg:33.17ms
step:609/2090 train_time:20200ms step_avg:33.17ms
step:610/2090 train_time:20233ms step_avg:33.17ms
step:611/2090 train_time:20266ms step_avg:33.17ms
step:612/2090 train_time:20299ms step_avg:33.17ms
step:613/2090 train_time:20332ms step_avg:33.17ms
step:614/2090 train_time:20365ms step_avg:33.17ms
step:615/2090 train_time:20398ms step_avg:33.17ms
step:616/2090 train_time:20430ms step_avg:33.17ms
step:617/2090 train_time:20463ms step_avg:33.17ms
step:618/2090 train_time:20496ms step_avg:33.17ms
step:619/2090 train_time:20529ms step_avg:33.16ms
step:620/2090 train_time:20562ms step_avg:33.16ms
step:621/2090 train_time:20595ms step_avg:33.16ms
step:622/2090 train_time:20628ms step_avg:33.16ms
step:623/2090 train_time:20661ms step_avg:33.16ms
step:624/2090 train_time:20694ms step_avg:33.16ms
step:625/2090 train_time:20727ms step_avg:33.16ms
step:626/2090 train_time:20760ms step_avg:33.16ms
step:627/2090 train_time:20793ms step_avg:33.16ms
step:628/2090 train_time:20826ms step_avg:33.16ms
step:629/2090 train_time:20859ms step_avg:33.16ms
step:630/2090 train_time:20892ms step_avg:33.16ms
step:631/2090 train_time:20924ms step_avg:33.16ms
step:632/2090 train_time:20957ms step_avg:33.16ms
step:633/2090 train_time:20990ms step_avg:33.16ms
step:634/2090 train_time:21023ms step_avg:33.16ms
step:635/2090 train_time:21056ms step_avg:33.16ms
step:636/2090 train_time:21089ms step_avg:33.16ms
step:637/2090 train_time:21122ms step_avg:33.16ms
step:638/2090 train_time:21155ms step_avg:33.16ms
step:639/2090 train_time:21188ms step_avg:33.16ms
step:640/2090 train_time:21221ms step_avg:33.16ms
step:641/2090 train_time:21254ms step_avg:33.16ms
step:642/2090 train_time:21287ms step_avg:33.16ms
step:643/2090 train_time:21320ms step_avg:33.16ms
step:644/2090 train_time:21353ms step_avg:33.16ms
step:645/2090 train_time:21386ms step_avg:33.16ms
step:646/2090 train_time:21419ms step_avg:33.16ms
step:647/2090 train_time:21452ms step_avg:33.16ms
step:648/2090 train_time:21485ms step_avg:33.16ms
step:649/2090 train_time:21518ms step_avg:33.16ms
step:650/2090 train_time:21551ms step_avg:33.16ms
step:651/2090 train_time:21584ms step_avg:33.16ms
step:652/2090 train_time:21617ms step_avg:33.15ms
step:653/2090 train_time:21650ms step_avg:33.15ms
step:654/2090 train_time:21683ms step_avg:33.15ms
step:655/2090 train_time:21716ms step_avg:33.15ms
step:656/2090 train_time:21749ms step_avg:33.15ms
step:657/2090 train_time:21782ms step_avg:33.15ms
step:658/2090 train_time:21815ms step_avg:33.15ms
step:659/2090 train_time:21847ms step_avg:33.15ms
step:660/2090 train_time:21880ms step_avg:33.15ms
step:661/2090 train_time:21914ms step_avg:33.15ms
step:662/2090 train_time:21946ms step_avg:33.15ms
step:663/2090 train_time:21980ms step_avg:33.15ms
step:664/2090 train_time:22013ms step_avg:33.15ms
step:665/2090 train_time:22046ms step_avg:33.15ms
step:666/2090 train_time:22079ms step_avg:33.15ms
step:667/2090 train_time:22112ms step_avg:33.15ms
step:668/2090 train_time:22144ms step_avg:33.15ms
step:669/2090 train_time:22178ms step_avg:33.15ms
step:670/2090 train_time:22211ms step_avg:33.15ms
step:671/2090 train_time:22244ms step_avg:33.15ms
step:672/2090 train_time:22277ms step_avg:33.15ms
step:673/2090 train_time:22309ms step_avg:33.15ms
step:674/2090 train_time:22342ms step_avg:33.15ms
step:675/2090 train_time:22376ms step_avg:33.15ms
step:676/2090 train_time:22408ms step_avg:33.15ms
step:677/2090 train_time:22441ms step_avg:33.15ms
step:678/2090 train_time:22474ms step_avg:33.15ms
step:679/2090 train_time:22507ms step_avg:33.15ms
step:680/2090 train_time:22540ms step_avg:33.15ms
step:681/2090 train_time:22573ms step_avg:33.15ms
step:682/2090 train_time:22606ms step_avg:33.15ms
step:683/2090 train_time:22639ms step_avg:33.15ms
step:684/2090 train_time:22672ms step_avg:33.15ms
step:685/2090 train_time:22706ms step_avg:33.15ms
step:686/2090 train_time:22764ms step_avg:33.18ms
step:687/2090 train_time:22824ms step_avg:33.22ms
step:688/2090 train_time:22884ms step_avg:33.26ms
step:689/2090 train_time:22944ms step_avg:33.30ms
step:690/2090 train_time:23004ms step_avg:33.34ms
step:691/2090 train_time:23065ms step_avg:33.38ms
step:692/2090 train_time:23124ms step_avg:33.42ms
step:693/2090 train_time:23185ms step_avg:33.46ms
step:694/2090 train_time:23244ms step_avg:33.49ms
step:695/2090 train_time:23304ms step_avg:33.53ms
step:696/2090 train_time:23363ms step_avg:33.57ms
step:697/2090 train_time:23424ms step_avg:33.61ms
step:698/2090 train_time:23484ms step_avg:33.64ms
step:699/2090 train_time:23544ms step_avg:33.68ms
step:700/2090 train_time:23604ms step_avg:33.72ms
step:701/2090 train_time:23664ms step_avg:33.76ms
step:702/2090 train_time:23723ms step_avg:33.79ms
step:703/2090 train_time:23784ms step_avg:33.83ms
step:704/2090 train_time:23843ms step_avg:33.87ms
step:705/2090 train_time:23904ms step_avg:33.91ms
step:706/2090 train_time:23963ms step_avg:33.94ms
step:707/2090 train_time:24023ms step_avg:33.98ms
step:708/2090 train_time:24082ms step_avg:34.01ms
step:709/2090 train_time:24143ms step_avg:34.05ms
step:710/2090 train_time:24202ms step_avg:34.09ms
step:711/2090 train_time:24263ms step_avg:34.12ms
step:712/2090 train_time:24322ms step_avg:34.16ms
step:713/2090 train_time:24382ms step_avg:34.20ms
step:714/2090 train_time:24441ms step_avg:34.23ms
step:715/2090 train_time:24501ms step_avg:34.27ms
step:716/2090 train_time:24561ms step_avg:34.30ms
step:717/2090 train_time:24621ms step_avg:34.34ms
step:718/2090 train_time:24680ms step_avg:34.37ms
step:719/2090 train_time:24739ms step_avg:34.41ms
step:720/2090 train_time:24798ms step_avg:34.44ms
step:721/2090 train_time:24859ms step_avg:34.48ms
step:722/2090 train_time:24918ms step_avg:34.51ms
step:723/2090 train_time:24978ms step_avg:34.55ms
step:724/2090 train_time:25037ms step_avg:34.58ms
step:725/2090 train_time:25097ms step_avg:34.62ms
step:726/2090 train_time:25157ms step_avg:34.65ms
step:727/2090 train_time:25217ms step_avg:34.69ms
step:728/2090 train_time:25276ms step_avg:34.72ms
step:729/2090 train_time:25336ms step_avg:34.75ms
step:730/2090 train_time:25395ms step_avg:34.79ms
step:731/2090 train_time:25456ms step_avg:34.82ms
step:732/2090 train_time:25515ms step_avg:34.86ms
step:733/2090 train_time:25575ms step_avg:34.89ms
step:734/2090 train_time:25634ms step_avg:34.92ms
step:735/2090 train_time:25694ms step_avg:34.96ms
step:736/2090 train_time:25753ms step_avg:34.99ms
step:737/2090 train_time:25813ms step_avg:35.02ms
step:738/2090 train_time:25873ms step_avg:35.06ms
step:739/2090 train_time:25933ms step_avg:35.09ms
step:740/2090 train_time:25993ms step_avg:35.13ms
step:741/2090 train_time:26053ms step_avg:35.16ms
step:742/2090 train_time:26112ms step_avg:35.19ms
step:743/2090 train_time:26172ms step_avg:35.22ms
step:744/2090 train_time:26231ms step_avg:35.26ms
step:745/2090 train_time:26291ms step_avg:35.29ms
step:746/2090 train_time:26351ms step_avg:35.32ms
step:747/2090 train_time:26412ms step_avg:35.36ms
step:748/2090 train_time:26471ms step_avg:35.39ms
step:749/2090 train_time:26532ms step_avg:35.42ms
step:750/2090 train_time:26591ms step_avg:35.45ms
step:750/2090 val_loss:3.8530 train_time:26654ms step_avg:35.54ms
step:751/2090 train_time:26674ms step_avg:35.52ms
step:752/2090 train_time:26713ms step_avg:35.52ms
step:753/2090 train_time:26779ms step_avg:35.56ms
step:754/2090 train_time:26841ms step_avg:35.60ms
step:755/2090 train_time:26902ms step_avg:35.63ms
step:756/2090 train_time:26961ms step_avg:35.66ms
step:757/2090 train_time:27021ms step_avg:35.70ms
step:758/2090 train_time:27080ms step_avg:35.73ms
step:759/2090 train_time:27140ms step_avg:35.76ms
step:760/2090 train_time:27199ms step_avg:35.79ms
step:761/2090 train_time:27259ms step_avg:35.82ms
step:762/2090 train_time:27317ms step_avg:35.85ms
step:763/2090 train_time:27377ms step_avg:35.88ms
step:764/2090 train_time:27435ms step_avg:35.91ms
step:765/2090 train_time:27494ms step_avg:35.94ms
step:766/2090 train_time:27553ms step_avg:35.97ms
step:767/2090 train_time:27613ms step_avg:36.00ms
step:768/2090 train_time:27673ms step_avg:36.03ms
step:769/2090 train_time:27734ms step_avg:36.07ms
step:770/2090 train_time:27796ms step_avg:36.10ms
step:771/2090 train_time:27856ms step_avg:36.13ms
step:772/2090 train_time:27916ms step_avg:36.16ms
step:773/2090 train_time:27976ms step_avg:36.19ms
step:774/2090 train_time:28035ms step_avg:36.22ms
step:775/2090 train_time:28096ms step_avg:36.25ms
step:776/2090 train_time:28155ms step_avg:36.28ms
step:777/2090 train_time:28214ms step_avg:36.31ms
step:778/2090 train_time:28273ms step_avg:36.34ms
step:779/2090 train_time:28332ms step_avg:36.37ms
step:780/2090 train_time:28391ms step_avg:36.40ms
step:781/2090 train_time:28450ms step_avg:36.43ms
step:782/2090 train_time:28509ms step_avg:36.46ms
step:783/2090 train_time:28570ms step_avg:36.49ms
step:784/2090 train_time:28629ms step_avg:36.52ms
step:785/2090 train_time:28690ms step_avg:36.55ms
step:786/2090 train_time:28750ms step_avg:36.58ms
step:787/2090 train_time:28811ms step_avg:36.61ms
step:788/2090 train_time:28871ms step_avg:36.64ms
step:789/2090 train_time:28932ms step_avg:36.67ms
step:790/2090 train_time:28991ms step_avg:36.70ms
step:791/2090 train_time:29052ms step_avg:36.73ms
step:792/2090 train_time:29111ms step_avg:36.76ms
step:793/2090 train_time:29172ms step_avg:36.79ms
step:794/2090 train_time:29231ms step_avg:36.82ms
step:795/2090 train_time:29291ms step_avg:36.84ms
step:796/2090 train_time:29349ms step_avg:36.87ms
step:797/2090 train_time:29409ms step_avg:36.90ms
step:798/2090 train_time:29468ms step_avg:36.93ms
step:799/2090 train_time:29528ms step_avg:36.96ms
step:800/2090 train_time:29587ms step_avg:36.98ms
step:801/2090 train_time:29648ms step_avg:37.01ms
step:802/2090 train_time:29707ms step_avg:37.04ms
step:803/2090 train_time:29769ms step_avg:37.07ms
step:804/2090 train_time:29829ms step_avg:37.10ms
step:805/2090 train_time:29889ms step_avg:37.13ms
step:806/2090 train_time:29949ms step_avg:37.16ms
step:807/2090 train_time:30011ms step_avg:37.19ms
step:808/2090 train_time:30071ms step_avg:37.22ms
step:809/2090 train_time:30132ms step_avg:37.25ms
step:810/2090 train_time:30191ms step_avg:37.27ms
step:811/2090 train_time:30251ms step_avg:37.30ms
step:812/2090 train_time:30311ms step_avg:37.33ms
step:813/2090 train_time:30371ms step_avg:37.36ms
step:814/2090 train_time:30430ms step_avg:37.38ms
step:815/2090 train_time:30490ms step_avg:37.41ms
step:816/2090 train_time:30549ms step_avg:37.44ms
step:817/2090 train_time:30610ms step_avg:37.47ms
step:818/2090 train_time:30669ms step_avg:37.49ms
step:819/2090 train_time:30730ms step_avg:37.52ms
step:820/2090 train_time:30790ms step_avg:37.55ms
step:821/2090 train_time:30850ms step_avg:37.58ms
step:822/2090 train_time:30909ms step_avg:37.60ms
step:823/2090 train_time:30970ms step_avg:37.63ms
step:824/2090 train_time:31030ms step_avg:37.66ms
step:825/2090 train_time:31090ms step_avg:37.69ms
step:826/2090 train_time:31150ms step_avg:37.71ms
step:827/2090 train_time:31210ms step_avg:37.74ms
step:828/2090 train_time:31270ms step_avg:37.77ms
step:829/2090 train_time:31330ms step_avg:37.79ms
step:830/2090 train_time:31389ms step_avg:37.82ms
step:831/2090 train_time:31449ms step_avg:37.85ms
step:832/2090 train_time:31509ms step_avg:37.87ms
step:833/2090 train_time:31568ms step_avg:37.90ms
step:834/2090 train_time:31628ms step_avg:37.92ms
step:835/2090 train_time:31688ms step_avg:37.95ms
step:836/2090 train_time:31748ms step_avg:37.98ms
step:837/2090 train_time:31809ms step_avg:38.00ms
step:838/2090 train_time:31868ms step_avg:38.03ms
step:839/2090 train_time:31930ms step_avg:38.06ms
step:840/2090 train_time:31989ms step_avg:38.08ms
step:841/2090 train_time:32050ms step_avg:38.11ms
step:842/2090 train_time:32109ms step_avg:38.13ms
step:843/2090 train_time:32169ms step_avg:38.16ms
step:844/2090 train_time:32229ms step_avg:38.19ms
step:845/2090 train_time:32289ms step_avg:38.21ms
step:846/2090 train_time:32348ms step_avg:38.24ms
step:847/2090 train_time:32408ms step_avg:38.26ms
step:848/2090 train_time:32467ms step_avg:38.29ms
step:849/2090 train_time:32527ms step_avg:38.31ms
step:850/2090 train_time:32587ms step_avg:38.34ms
step:851/2090 train_time:32647ms step_avg:38.36ms
step:852/2090 train_time:32706ms step_avg:38.39ms
step:853/2090 train_time:32767ms step_avg:38.41ms
step:854/2090 train_time:32826ms step_avg:38.44ms
step:855/2090 train_time:32887ms step_avg:38.46ms
step:856/2090 train_time:32947ms step_avg:38.49ms
step:857/2090 train_time:33008ms step_avg:38.52ms
step:858/2090 train_time:33068ms step_avg:38.54ms
step:859/2090 train_time:33129ms step_avg:38.57ms
step:860/2090 train_time:33188ms step_avg:38.59ms
step:861/2090 train_time:33248ms step_avg:38.62ms
step:862/2090 train_time:33307ms step_avg:38.64ms
step:863/2090 train_time:33368ms step_avg:38.66ms
step:864/2090 train_time:33427ms step_avg:38.69ms
step:865/2090 train_time:33488ms step_avg:38.71ms
step:866/2090 train_time:33547ms step_avg:38.74ms
step:867/2090 train_time:33607ms step_avg:38.76ms
step:868/2090 train_time:33667ms step_avg:38.79ms
step:869/2090 train_time:33727ms step_avg:38.81ms
step:870/2090 train_time:33787ms step_avg:38.84ms
step:871/2090 train_time:33848ms step_avg:38.86ms
step:872/2090 train_time:33907ms step_avg:38.88ms
step:873/2090 train_time:33968ms step_avg:38.91ms
step:874/2090 train_time:34028ms step_avg:38.93ms
step:875/2090 train_time:34088ms step_avg:38.96ms
step:876/2090 train_time:34148ms step_avg:38.98ms
step:877/2090 train_time:34209ms step_avg:39.01ms
step:878/2090 train_time:34268ms step_avg:39.03ms
step:879/2090 train_time:34329ms step_avg:39.05ms
step:880/2090 train_time:34389ms step_avg:39.08ms
step:881/2090 train_time:34449ms step_avg:39.10ms
step:882/2090 train_time:34509ms step_avg:39.13ms
step:883/2090 train_time:34570ms step_avg:39.15ms
step:884/2090 train_time:34630ms step_avg:39.17ms
step:885/2090 train_time:34691ms step_avg:39.20ms
step:886/2090 train_time:34750ms step_avg:39.22ms
step:887/2090 train_time:34811ms step_avg:39.25ms
step:888/2090 train_time:34870ms step_avg:39.27ms
step:889/2090 train_time:34931ms step_avg:39.29ms
step:890/2090 train_time:34991ms step_avg:39.32ms
step:891/2090 train_time:35051ms step_avg:39.34ms
step:892/2090 train_time:35110ms step_avg:39.36ms
step:893/2090 train_time:35170ms step_avg:39.38ms
step:894/2090 train_time:35229ms step_avg:39.41ms
step:895/2090 train_time:35289ms step_avg:39.43ms
step:896/2090 train_time:35348ms step_avg:39.45ms
step:897/2090 train_time:35408ms step_avg:39.47ms
step:898/2090 train_time:35468ms step_avg:39.50ms
step:899/2090 train_time:35528ms step_avg:39.52ms
step:900/2090 train_time:35588ms step_avg:39.54ms
step:901/2090 train_time:35649ms step_avg:39.57ms
step:902/2090 train_time:35708ms step_avg:39.59ms
step:903/2090 train_time:35770ms step_avg:39.61ms
step:904/2090 train_time:35830ms step_avg:39.63ms
step:905/2090 train_time:35890ms step_avg:39.66ms
step:906/2090 train_time:35950ms step_avg:39.68ms
step:907/2090 train_time:36010ms step_avg:39.70ms
step:908/2090 train_time:36070ms step_avg:39.72ms
step:909/2090 train_time:36130ms step_avg:39.75ms
step:910/2090 train_time:36190ms step_avg:39.77ms
step:911/2090 train_time:36250ms step_avg:39.79ms
step:912/2090 train_time:36310ms step_avg:39.81ms
step:913/2090 train_time:36370ms step_avg:39.84ms
step:914/2090 train_time:36429ms step_avg:39.86ms
step:915/2090 train_time:36490ms step_avg:39.88ms
step:916/2090 train_time:36549ms step_avg:39.90ms
step:917/2090 train_time:36609ms step_avg:39.92ms
step:918/2090 train_time:36668ms step_avg:39.94ms
step:919/2090 train_time:36729ms step_avg:39.97ms
step:920/2090 train_time:36788ms step_avg:39.99ms
step:921/2090 train_time:36849ms step_avg:40.01ms
step:922/2090 train_time:36908ms step_avg:40.03ms
step:923/2090 train_time:36969ms step_avg:40.05ms
step:924/2090 train_time:37030ms step_avg:40.08ms
step:925/2090 train_time:37090ms step_avg:40.10ms
step:926/2090 train_time:37149ms step_avg:40.12ms
step:927/2090 train_time:37210ms step_avg:40.14ms
step:928/2090 train_time:37269ms step_avg:40.16ms
step:929/2090 train_time:37330ms step_avg:40.18ms
step:930/2090 train_time:37389ms step_avg:40.20ms
step:931/2090 train_time:37449ms step_avg:40.22ms
step:932/2090 train_time:37508ms step_avg:40.24ms
step:933/2090 train_time:37569ms step_avg:40.27ms
step:934/2090 train_time:37629ms step_avg:40.29ms
step:935/2090 train_time:37690ms step_avg:40.31ms
step:936/2090 train_time:37750ms step_avg:40.33ms
step:937/2090 train_time:37811ms step_avg:40.35ms
step:938/2090 train_time:37871ms step_avg:40.37ms
step:939/2090 train_time:37931ms step_avg:40.40ms
step:940/2090 train_time:37990ms step_avg:40.42ms
step:941/2090 train_time:38051ms step_avg:40.44ms
step:942/2090 train_time:38110ms step_avg:40.46ms
step:943/2090 train_time:38171ms step_avg:40.48ms
step:944/2090 train_time:38230ms step_avg:40.50ms
step:945/2090 train_time:38291ms step_avg:40.52ms
step:946/2090 train_time:38350ms step_avg:40.54ms
step:947/2090 train_time:38410ms step_avg:40.56ms
step:948/2090 train_time:38470ms step_avg:40.58ms
step:949/2090 train_time:38530ms step_avg:40.60ms
step:950/2090 train_time:38590ms step_avg:40.62ms
step:951/2090 train_time:38650ms step_avg:40.64ms
step:952/2090 train_time:38709ms step_avg:40.66ms
step:953/2090 train_time:38770ms step_avg:40.68ms
step:954/2090 train_time:38830ms step_avg:40.70ms
step:955/2090 train_time:38890ms step_avg:40.72ms
step:956/2090 train_time:38950ms step_avg:40.74ms
step:957/2090 train_time:39010ms step_avg:40.76ms
step:958/2090 train_time:39069ms step_avg:40.78ms
step:959/2090 train_time:39129ms step_avg:40.80ms
step:960/2090 train_time:39189ms step_avg:40.82ms
step:961/2090 train_time:39249ms step_avg:40.84ms
step:962/2090 train_time:39308ms step_avg:40.86ms
step:963/2090 train_time:39369ms step_avg:40.88ms
step:964/2090 train_time:39428ms step_avg:40.90ms
step:965/2090 train_time:39488ms step_avg:40.92ms
step:966/2090 train_time:39547ms step_avg:40.94ms
step:967/2090 train_time:39607ms step_avg:40.96ms
step:968/2090 train_time:39667ms step_avg:40.98ms
step:969/2090 train_time:39728ms step_avg:41.00ms
step:970/2090 train_time:39788ms step_avg:41.02ms
step:971/2090 train_time:39849ms step_avg:41.04ms
step:972/2090 train_time:39909ms step_avg:41.06ms
step:973/2090 train_time:39969ms step_avg:41.08ms
step:974/2090 train_time:40029ms step_avg:41.10ms
step:975/2090 train_time:40089ms step_avg:41.12ms
step:976/2090 train_time:40149ms step_avg:41.14ms
step:977/2090 train_time:40209ms step_avg:41.16ms
step:978/2090 train_time:40268ms step_avg:41.17ms
step:979/2090 train_time:40328ms step_avg:41.19ms
step:980/2090 train_time:40388ms step_avg:41.21ms
step:981/2090 train_time:40449ms step_avg:41.23ms
step:982/2090 train_time:40507ms step_avg:41.25ms
step:983/2090 train_time:40568ms step_avg:41.27ms
step:984/2090 train_time:40627ms step_avg:41.29ms
step:985/2090 train_time:40688ms step_avg:41.31ms
step:986/2090 train_time:40747ms step_avg:41.33ms
step:987/2090 train_time:40809ms step_avg:41.35ms
step:988/2090 train_time:40870ms step_avg:41.37ms
step:989/2090 train_time:40930ms step_avg:41.39ms
step:990/2090 train_time:40990ms step_avg:41.40ms
step:991/2090 train_time:41051ms step_avg:41.42ms
step:992/2090 train_time:41110ms step_avg:41.44ms
step:993/2090 train_time:41170ms step_avg:41.46ms
step:994/2090 train_time:41229ms step_avg:41.48ms
step:995/2090 train_time:41290ms step_avg:41.50ms
step:996/2090 train_time:41349ms step_avg:41.51ms
step:997/2090 train_time:41410ms step_avg:41.53ms
step:998/2090 train_time:41469ms step_avg:41.55ms
step:999/2090 train_time:41529ms step_avg:41.57ms
step:1000/2090 train_time:41589ms step_avg:41.59ms
step:1000/2090 val_loss:3.7086 train_time:41652ms step_avg:41.65ms
step:1001/2090 train_time:41671ms step_avg:41.63ms
step:1002/2090 train_time:41711ms step_avg:41.63ms
step:1003/2090 train_time:41776ms step_avg:41.65ms
step:1004/2090 train_time:41840ms step_avg:41.67ms
step:1005/2090 train_time:41900ms step_avg:41.69ms
step:1006/2090 train_time:41959ms step_avg:41.71ms
step:1007/2090 train_time:42019ms step_avg:41.73ms
step:1008/2090 train_time:42077ms step_avg:41.74ms
step:1009/2090 train_time:42137ms step_avg:41.76ms
step:1010/2090 train_time:42195ms step_avg:41.78ms
step:1011/2090 train_time:42255ms step_avg:41.80ms
step:1012/2090 train_time:42313ms step_avg:41.81ms
step:1013/2090 train_time:42373ms step_avg:41.83ms
step:1014/2090 train_time:42431ms step_avg:41.85ms
step:1015/2090 train_time:42491ms step_avg:41.86ms
step:1016/2090 train_time:42549ms step_avg:41.88ms
step:1017/2090 train_time:42611ms step_avg:41.90ms
step:1018/2090 train_time:42671ms step_avg:41.92ms
step:1019/2090 train_time:42735ms step_avg:41.94ms
step:1020/2090 train_time:42795ms step_avg:41.96ms
step:1021/2090 train_time:42856ms step_avg:41.97ms
step:1022/2090 train_time:42916ms step_avg:41.99ms
step:1023/2090 train_time:42977ms step_avg:42.01ms
step:1024/2090 train_time:43036ms step_avg:42.03ms
step:1025/2090 train_time:43096ms step_avg:42.04ms
step:1026/2090 train_time:43155ms step_avg:42.06ms
step:1027/2090 train_time:43215ms step_avg:42.08ms
step:1028/2090 train_time:43273ms step_avg:42.09ms
step:1029/2090 train_time:43333ms step_avg:42.11ms
step:1030/2090 train_time:43391ms step_avg:42.13ms
step:1031/2090 train_time:43451ms step_avg:42.14ms
step:1032/2090 train_time:43510ms step_avg:42.16ms
step:1033/2090 train_time:43571ms step_avg:42.18ms
step:1034/2090 train_time:43630ms step_avg:42.20ms
step:1035/2090 train_time:43692ms step_avg:42.21ms
step:1036/2090 train_time:43752ms step_avg:42.23ms
step:1037/2090 train_time:43814ms step_avg:42.25ms
step:1038/2090 train_time:43874ms step_avg:42.27ms
step:1039/2090 train_time:43935ms step_avg:42.29ms
step:1040/2090 train_time:43995ms step_avg:42.30ms
step:1041/2090 train_time:44055ms step_avg:42.32ms
step:1042/2090 train_time:44114ms step_avg:42.34ms
step:1043/2090 train_time:44174ms step_avg:42.35ms
step:1044/2090 train_time:44233ms step_avg:42.37ms
step:1045/2090 train_time:44292ms step_avg:42.38ms
step:1046/2090 train_time:44351ms step_avg:42.40ms
step:1047/2090 train_time:44411ms step_avg:42.42ms
step:1048/2090 train_time:44470ms step_avg:42.43ms
step:1049/2090 train_time:44530ms step_avg:42.45ms
step:1050/2090 train_time:44589ms step_avg:42.47ms
step:1051/2090 train_time:44649ms step_avg:42.48ms
step:1052/2090 train_time:44709ms step_avg:42.50ms
step:1053/2090 train_time:44770ms step_avg:42.52ms
step:1054/2090 train_time:44831ms step_avg:42.53ms
step:1055/2090 train_time:44893ms step_avg:42.55ms
step:1056/2090 train_time:44953ms step_avg:42.57ms
step:1057/2090 train_time:45014ms step_avg:42.59ms
step:1058/2090 train_time:45073ms step_avg:42.60ms
step:1059/2090 train_time:45134ms step_avg:42.62ms
step:1060/2090 train_time:45193ms step_avg:42.63ms
step:1061/2090 train_time:45252ms step_avg:42.65ms
step:1062/2090 train_time:45311ms step_avg:42.67ms
step:1063/2090 train_time:45371ms step_avg:42.68ms
step:1064/2090 train_time:45430ms step_avg:42.70ms
step:1065/2090 train_time:45490ms step_avg:42.71ms
step:1066/2090 train_time:45549ms step_avg:42.73ms
step:1067/2090 train_time:45610ms step_avg:42.75ms
step:1068/2090 train_time:45669ms step_avg:42.76ms
step:1069/2090 train_time:45730ms step_avg:42.78ms
step:1070/2090 train_time:45790ms step_avg:42.79ms
step:1071/2090 train_time:45851ms step_avg:42.81ms
step:1072/2090 train_time:45911ms step_avg:42.83ms
step:1073/2090 train_time:45972ms step_avg:42.84ms
step:1074/2090 train_time:46032ms step_avg:42.86ms
step:1075/2090 train_time:46093ms step_avg:42.88ms
step:1076/2090 train_time:46152ms step_avg:42.89ms
step:1077/2090 train_time:46213ms step_avg:42.91ms
step:1078/2090 train_time:46272ms step_avg:42.92ms
step:1079/2090 train_time:46332ms step_avg:42.94ms
step:1080/2090 train_time:46391ms step_avg:42.95ms
step:1081/2090 train_time:46450ms step_avg:42.97ms
step:1082/2090 train_time:46509ms step_avg:42.98ms
step:1083/2090 train_time:46570ms step_avg:43.00ms
step:1084/2090 train_time:46629ms step_avg:43.02ms
step:1085/2090 train_time:46690ms step_avg:43.03ms
step:1086/2090 train_time:46749ms step_avg:43.05ms
step:1087/2090 train_time:46810ms step_avg:43.06ms
step:1088/2090 train_time:46871ms step_avg:43.08ms
step:1089/2090 train_time:46932ms step_avg:43.10ms
step:1090/2090 train_time:46992ms step_avg:43.11ms
step:1091/2090 train_time:47053ms step_avg:43.13ms
step:1092/2090 train_time:47113ms step_avg:43.14ms
step:1093/2090 train_time:47174ms step_avg:43.16ms
step:1094/2090 train_time:47233ms step_avg:43.17ms
step:1095/2090 train_time:47293ms step_avg:43.19ms
step:1096/2090 train_time:47352ms step_avg:43.20ms
step:1097/2090 train_time:47413ms step_avg:43.22ms
step:1098/2090 train_time:47473ms step_avg:43.24ms
step:1099/2090 train_time:47533ms step_avg:43.25ms
step:1100/2090 train_time:47592ms step_avg:43.27ms
step:1101/2090 train_time:47652ms step_avg:43.28ms
step:1102/2090 train_time:47712ms step_avg:43.30ms
step:1103/2090 train_time:47773ms step_avg:43.31ms
step:1104/2090 train_time:47833ms step_avg:43.33ms
step:1105/2090 train_time:47894ms step_avg:43.34ms
step:1106/2090 train_time:47953ms step_avg:43.36ms
step:1107/2090 train_time:48014ms step_avg:43.37ms
step:1108/2090 train_time:48073ms step_avg:43.39ms
step:1109/2090 train_time:48135ms step_avg:43.40ms
step:1110/2090 train_time:48194ms step_avg:43.42ms
step:1111/2090 train_time:48254ms step_avg:43.43ms
step:1112/2090 train_time:48313ms step_avg:43.45ms
step:1113/2090 train_time:48373ms step_avg:43.46ms
step:1114/2090 train_time:48432ms step_avg:43.48ms
step:1115/2090 train_time:48492ms step_avg:43.49ms
step:1116/2090 train_time:48551ms step_avg:43.50ms
step:1117/2090 train_time:48611ms step_avg:43.52ms
step:1118/2090 train_time:48670ms step_avg:43.53ms
step:1119/2090 train_time:48731ms step_avg:43.55ms
step:1120/2090 train_time:48790ms step_avg:43.56ms
step:1121/2090 train_time:48851ms step_avg:43.58ms
step:1122/2090 train_time:48911ms step_avg:43.59ms
step:1123/2090 train_time:48972ms step_avg:43.61ms
step:1124/2090 train_time:49031ms step_avg:43.62ms
step:1125/2090 train_time:49092ms step_avg:43.64ms
step:1126/2090 train_time:49151ms step_avg:43.65ms
step:1127/2090 train_time:49212ms step_avg:43.67ms
step:1128/2090 train_time:49272ms step_avg:43.68ms
step:1129/2090 train_time:49332ms step_avg:43.69ms
step:1130/2090 train_time:49391ms step_avg:43.71ms
step:1131/2090 train_time:49452ms step_avg:43.72ms
step:1132/2090 train_time:49511ms step_avg:43.74ms
step:1133/2090 train_time:49571ms step_avg:43.75ms
step:1134/2090 train_time:49630ms step_avg:43.77ms
step:1135/2090 train_time:49691ms step_avg:43.78ms
step:1136/2090 train_time:49751ms step_avg:43.79ms
step:1137/2090 train_time:49812ms step_avg:43.81ms
step:1138/2090 train_time:49872ms step_avg:43.82ms
step:1139/2090 train_time:49933ms step_avg:43.84ms
step:1140/2090 train_time:49992ms step_avg:43.85ms
step:1141/2090 train_time:50053ms step_avg:43.87ms
step:1142/2090 train_time:50112ms step_avg:43.88ms
step:1143/2090 train_time:50172ms step_avg:43.90ms
step:1144/2090 train_time:50232ms step_avg:43.91ms
step:1145/2090 train_time:50292ms step_avg:43.92ms
step:1146/2090 train_time:50351ms step_avg:43.94ms
step:1147/2090 train_time:50412ms step_avg:43.95ms
step:1148/2090 train_time:50472ms step_avg:43.97ms
step:1149/2090 train_time:50532ms step_avg:43.98ms
step:1150/2090 train_time:50592ms step_avg:43.99ms
step:1151/2090 train_time:50651ms step_avg:44.01ms
step:1152/2090 train_time:50711ms step_avg:44.02ms
step:1153/2090 train_time:50772ms step_avg:44.03ms
step:1154/2090 train_time:50831ms step_avg:44.05ms
step:1155/2090 train_time:50892ms step_avg:44.06ms
step:1156/2090 train_time:50951ms step_avg:44.08ms
step:1157/2090 train_time:51012ms step_avg:44.09ms
step:1158/2090 train_time:51073ms step_avg:44.10ms
step:1159/2090 train_time:51133ms step_avg:44.12ms
step:1160/2090 train_time:51192ms step_avg:44.13ms
step:1161/2090 train_time:51252ms step_avg:44.14ms
step:1162/2090 train_time:51311ms step_avg:44.16ms
step:1163/2090 train_time:51372ms step_avg:44.17ms
step:1164/2090 train_time:51432ms step_avg:44.19ms
step:1165/2090 train_time:51492ms step_avg:44.20ms
step:1166/2090 train_time:51552ms step_avg:44.21ms
step:1167/2090 train_time:51612ms step_avg:44.23ms
step:1168/2090 train_time:51672ms step_avg:44.24ms
step:1169/2090 train_time:51732ms step_avg:44.25ms
step:1170/2090 train_time:51791ms step_avg:44.27ms
step:1171/2090 train_time:51852ms step_avg:44.28ms
step:1172/2090 train_time:51912ms step_avg:44.29ms
step:1173/2090 train_time:51973ms step_avg:44.31ms
step:1174/2090 train_time:52033ms step_avg:44.32ms
step:1175/2090 train_time:52093ms step_avg:44.33ms
step:1176/2090 train_time:52152ms step_avg:44.35ms
step:1177/2090 train_time:52212ms step_avg:44.36ms
step:1178/2090 train_time:52272ms step_avg:44.37ms
step:1179/2090 train_time:52332ms step_avg:44.39ms
step:1180/2090 train_time:52391ms step_avg:44.40ms
step:1181/2090 train_time:52452ms step_avg:44.41ms
step:1182/2090 train_time:52512ms step_avg:44.43ms
step:1183/2090 train_time:52572ms step_avg:44.44ms
step:1184/2090 train_time:52632ms step_avg:44.45ms
step:1185/2090 train_time:52692ms step_avg:44.47ms
step:1186/2090 train_time:52750ms step_avg:44.48ms
step:1187/2090 train_time:52811ms step_avg:44.49ms
step:1188/2090 train_time:52872ms step_avg:44.50ms
step:1189/2090 train_time:52932ms step_avg:44.52ms
step:1190/2090 train_time:52992ms step_avg:44.53ms
step:1191/2090 train_time:53053ms step_avg:44.54ms
step:1192/2090 train_time:53112ms step_avg:44.56ms
step:1193/2090 train_time:53173ms step_avg:44.57ms
step:1194/2090 train_time:53232ms step_avg:44.58ms
step:1195/2090 train_time:53293ms step_avg:44.60ms
step:1196/2090 train_time:53352ms step_avg:44.61ms
step:1197/2090 train_time:53413ms step_avg:44.62ms
step:1198/2090 train_time:53473ms step_avg:44.64ms
step:1199/2090 train_time:53533ms step_avg:44.65ms
step:1200/2090 train_time:53592ms step_avg:44.66ms
step:1201/2090 train_time:53652ms step_avg:44.67ms
step:1202/2090 train_time:53712ms step_avg:44.69ms
step:1203/2090 train_time:53773ms step_avg:44.70ms
step:1204/2090 train_time:53832ms step_avg:44.71ms
step:1205/2090 train_time:53893ms step_avg:44.72ms
step:1206/2090 train_time:53952ms step_avg:44.74ms
step:1207/2090 train_time:54014ms step_avg:44.75ms
step:1208/2090 train_time:54073ms step_avg:44.76ms
step:1209/2090 train_time:54134ms step_avg:44.78ms
step:1210/2090 train_time:54193ms step_avg:44.79ms
step:1211/2090 train_time:54254ms step_avg:44.80ms
step:1212/2090 train_time:54313ms step_avg:44.81ms
step:1213/2090 train_time:54373ms step_avg:44.83ms
step:1214/2090 train_time:54432ms step_avg:44.84ms
step:1215/2090 train_time:54492ms step_avg:44.85ms
step:1216/2090 train_time:54552ms step_avg:44.86ms
step:1217/2090 train_time:54612ms step_avg:44.87ms
step:1218/2090 train_time:54672ms step_avg:44.89ms
step:1219/2090 train_time:54732ms step_avg:44.90ms
step:1220/2090 train_time:54791ms step_avg:44.91ms
step:1221/2090 train_time:54853ms step_avg:44.92ms
step:1222/2090 train_time:54913ms step_avg:44.94ms
step:1223/2090 train_time:54973ms step_avg:44.95ms
step:1224/2090 train_time:55033ms step_avg:44.96ms
step:1225/2090 train_time:55093ms step_avg:44.97ms
step:1226/2090 train_time:55152ms step_avg:44.99ms
step:1227/2090 train_time:55214ms step_avg:45.00ms
step:1228/2090 train_time:55273ms step_avg:45.01ms
step:1229/2090 train_time:55333ms step_avg:45.02ms
step:1230/2090 train_time:55392ms step_avg:45.03ms
step:1231/2090 train_time:55452ms step_avg:45.05ms
step:1232/2090 train_time:55512ms step_avg:45.06ms
step:1233/2090 train_time:55573ms step_avg:45.07ms
step:1234/2090 train_time:55632ms step_avg:45.08ms
step:1235/2090 train_time:55692ms step_avg:45.09ms
step:1236/2090 train_time:55751ms step_avg:45.11ms
step:1237/2090 train_time:55812ms step_avg:45.12ms
step:1238/2090 train_time:55871ms step_avg:45.13ms
step:1239/2090 train_time:55933ms step_avg:45.14ms
step:1240/2090 train_time:55992ms step_avg:45.16ms
step:1241/2090 train_time:56052ms step_avg:45.17ms
step:1242/2090 train_time:56112ms step_avg:45.18ms
step:1243/2090 train_time:56173ms step_avg:45.19ms
step:1244/2090 train_time:56232ms step_avg:45.20ms
step:1245/2090 train_time:56293ms step_avg:45.21ms
step:1246/2090 train_time:56351ms step_avg:45.23ms
step:1247/2090 train_time:56412ms step_avg:45.24ms
step:1248/2090 train_time:56472ms step_avg:45.25ms
step:1249/2090 train_time:56532ms step_avg:45.26ms
step:1250/2090 train_time:56592ms step_avg:45.27ms
step:1250/2090 val_loss:3.5845 train_time:56654ms step_avg:45.32ms
step:1251/2090 train_time:56674ms step_avg:45.30ms
step:1252/2090 train_time:56713ms step_avg:45.30ms
step:1253/2090 train_time:56778ms step_avg:45.31ms
step:1254/2090 train_time:56841ms step_avg:45.33ms
step:1255/2090 train_time:56901ms step_avg:45.34ms
step:1256/2090 train_time:56961ms step_avg:45.35ms
step:1257/2090 train_time:57021ms step_avg:45.36ms
step:1258/2090 train_time:57079ms step_avg:45.37ms
step:1259/2090 train_time:57139ms step_avg:45.38ms
step:1260/2090 train_time:57197ms step_avg:45.39ms
step:1261/2090 train_time:57257ms step_avg:45.41ms
step:1262/2090 train_time:57315ms step_avg:45.42ms
step:1263/2090 train_time:57374ms step_avg:45.43ms
step:1264/2090 train_time:57432ms step_avg:45.44ms
step:1265/2090 train_time:57492ms step_avg:45.45ms
step:1266/2090 train_time:57550ms step_avg:45.46ms
step:1267/2090 train_time:57612ms step_avg:45.47ms
step:1268/2090 train_time:57672ms step_avg:45.48ms
step:1269/2090 train_time:57735ms step_avg:45.50ms
step:1270/2090 train_time:57794ms step_avg:45.51ms
step:1271/2090 train_time:57855ms step_avg:45.52ms
step:1272/2090 train_time:57915ms step_avg:45.53ms
step:1273/2090 train_time:57975ms step_avg:45.54ms
step:1274/2090 train_time:58034ms step_avg:45.55ms
step:1275/2090 train_time:58094ms step_avg:45.56ms
step:1276/2090 train_time:58153ms step_avg:45.57ms
step:1277/2090 train_time:58213ms step_avg:45.59ms
step:1278/2090 train_time:58272ms step_avg:45.60ms
step:1279/2090 train_time:58332ms step_avg:45.61ms
step:1280/2090 train_time:58390ms step_avg:45.62ms
step:1281/2090 train_time:58450ms step_avg:45.63ms
step:1282/2090 train_time:58509ms step_avg:45.64ms
step:1283/2090 train_time:58569ms step_avg:45.65ms
step:1284/2090 train_time:58629ms step_avg:45.66ms
step:1285/2090 train_time:58689ms step_avg:45.67ms
step:1286/2090 train_time:58750ms step_avg:45.68ms
step:1287/2090 train_time:58812ms step_avg:45.70ms
step:1288/2090 train_time:58872ms step_avg:45.71ms
step:1289/2090 train_time:58932ms step_avg:45.72ms
step:1290/2090 train_time:58992ms step_avg:45.73ms
step:1291/2090 train_time:59053ms step_avg:45.74ms
step:1292/2090 train_time:59112ms step_avg:45.75ms
step:1293/2090 train_time:59172ms step_avg:45.76ms
step:1294/2090 train_time:59231ms step_avg:45.77ms
step:1295/2090 train_time:59291ms step_avg:45.78ms
step:1296/2090 train_time:59350ms step_avg:45.79ms
step:1297/2090 train_time:59410ms step_avg:45.81ms
step:1298/2090 train_time:59469ms step_avg:45.82ms
step:1299/2090 train_time:59528ms step_avg:45.83ms
step:1300/2090 train_time:59588ms step_avg:45.84ms
step:1301/2090 train_time:59649ms step_avg:45.85ms
step:1302/2090 train_time:59709ms step_avg:45.86ms
step:1303/2090 train_time:59771ms step_avg:45.87ms
step:1304/2090 train_time:59830ms step_avg:45.88ms
step:1305/2090 train_time:59891ms step_avg:45.89ms
step:1306/2090 train_time:59951ms step_avg:45.90ms
step:1307/2090 train_time:60012ms step_avg:45.92ms
step:1308/2090 train_time:60072ms step_avg:45.93ms
step:1309/2090 train_time:60131ms step_avg:45.94ms
step:1310/2090 train_time:60191ms step_avg:45.95ms
step:1311/2090 train_time:60251ms step_avg:45.96ms
step:1312/2090 train_time:60310ms step_avg:45.97ms
step:1313/2090 train_time:60369ms step_avg:45.98ms
step:1314/2090 train_time:60428ms step_avg:45.99ms
step:1315/2090 train_time:60489ms step_avg:46.00ms
step:1316/2090 train_time:60548ms step_avg:46.01ms
step:1317/2090 train_time:60610ms step_avg:46.02ms
step:1318/2090 train_time:60669ms step_avg:46.03ms
step:1319/2090 train_time:60730ms step_avg:46.04ms
step:1320/2090 train_time:60790ms step_avg:46.05ms
step:1321/2090 train_time:60851ms step_avg:46.06ms
step:1322/2090 train_time:60911ms step_avg:46.08ms
step:1323/2090 train_time:60972ms step_avg:46.09ms
step:1324/2090 train_time:61032ms step_avg:46.10ms
step:1325/2090 train_time:61092ms step_avg:46.11ms
step:1326/2090 train_time:61151ms step_avg:46.12ms
step:1327/2090 train_time:61211ms step_avg:46.13ms
step:1328/2090 train_time:61270ms step_avg:46.14ms
step:1329/2090 train_time:61330ms step_avg:46.15ms
step:1330/2090 train_time:61389ms step_avg:46.16ms
step:1331/2090 train_time:61450ms step_avg:46.17ms
step:1332/2090 train_time:61509ms step_avg:46.18ms
step:1333/2090 train_time:61569ms step_avg:46.19ms
step:1334/2090 train_time:61630ms step_avg:46.20ms
step:1335/2090 train_time:61690ms step_avg:46.21ms
step:1336/2090 train_time:61750ms step_avg:46.22ms
step:1337/2090 train_time:61812ms step_avg:46.23ms
step:1338/2090 train_time:61872ms step_avg:46.24ms
step:1339/2090 train_time:61932ms step_avg:46.25ms
step:1340/2090 train_time:61992ms step_avg:46.26ms
step:1341/2090 train_time:62053ms step_avg:46.27ms
step:1342/2090 train_time:62112ms step_avg:46.28ms
step:1343/2090 train_time:62172ms step_avg:46.29ms
step:1344/2090 train_time:62231ms step_avg:46.30ms
step:1345/2090 train_time:62291ms step_avg:46.31ms
step:1346/2090 train_time:62351ms step_avg:46.32ms
step:1347/2090 train_time:62411ms step_avg:46.33ms
step:1348/2090 train_time:62471ms step_avg:46.34ms
step:1349/2090 train_time:62531ms step_avg:46.35ms
step:1350/2090 train_time:62590ms step_avg:46.36ms
step:1351/2090 train_time:62651ms step_avg:46.37ms
step:1352/2090 train_time:62710ms step_avg:46.38ms
step:1353/2090 train_time:62771ms step_avg:46.39ms
step:1354/2090 train_time:62830ms step_avg:46.40ms
step:1355/2090 train_time:62891ms step_avg:46.41ms
step:1356/2090 train_time:62951ms step_avg:46.42ms
step:1357/2090 train_time:63011ms step_avg:46.43ms
step:1358/2090 train_time:63071ms step_avg:46.44ms
step:1359/2090 train_time:63131ms step_avg:46.45ms
step:1360/2090 train_time:63190ms step_avg:46.46ms
step:1361/2090 train_time:63251ms step_avg:46.47ms
step:1362/2090 train_time:63311ms step_avg:46.48ms
step:1363/2090 train_time:63371ms step_avg:46.49ms
step:1364/2090 train_time:63430ms step_avg:46.50ms
step:1365/2090 train_time:63490ms step_avg:46.51ms
step:1366/2090 train_time:63550ms step_avg:46.52ms
step:1367/2090 train_time:63610ms step_avg:46.53ms
step:1368/2090 train_time:63671ms step_avg:46.54ms
step:1369/2090 train_time:63759ms step_avg:46.57ms
step:1370/2090 train_time:63846ms step_avg:46.60ms
step:1371/2090 train_time:63934ms step_avg:46.63ms
step:1372/2090 train_time:64021ms step_avg:46.66ms
step:1373/2090 train_time:64109ms step_avg:46.69ms
step:1374/2090 train_time:64196ms step_avg:46.72ms
step:1375/2090 train_time:64285ms step_avg:46.75ms
step:1376/2090 train_time:64372ms step_avg:46.78ms
step:1377/2090 train_time:64460ms step_avg:46.81ms
step:1378/2090 train_time:64547ms step_avg:46.84ms
step:1379/2090 train_time:64634ms step_avg:46.87ms
step:1380/2090 train_time:64722ms step_avg:46.90ms
step:1381/2090 train_time:64809ms step_avg:46.93ms
step:1382/2090 train_time:64896ms step_avg:46.96ms
step:1383/2090 train_time:64985ms step_avg:46.99ms
step:1384/2090 train_time:65071ms step_avg:47.02ms
step:1385/2090 train_time:65160ms step_avg:47.05ms
step:1386/2090 train_time:65246ms step_avg:47.08ms
step:1387/2090 train_time:65334ms step_avg:47.10ms
step:1388/2090 train_time:65422ms step_avg:47.13ms
step:1389/2090 train_time:65510ms step_avg:47.16ms
step:1390/2090 train_time:65597ms step_avg:47.19ms
step:1391/2090 train_time:65685ms step_avg:47.22ms
step:1392/2090 train_time:65772ms step_avg:47.25ms
step:1393/2090 train_time:65859ms step_avg:47.28ms
step:1394/2090 train_time:65946ms step_avg:47.31ms
step:1395/2090 train_time:66034ms step_avg:47.34ms
step:1396/2090 train_time:66123ms step_avg:47.37ms
step:1397/2090 train_time:66211ms step_avg:47.40ms
step:1398/2090 train_time:66298ms step_avg:47.42ms
step:1399/2090 train_time:66386ms step_avg:47.45ms
step:1400/2090 train_time:66473ms step_avg:47.48ms
step:1401/2090 train_time:66560ms step_avg:47.51ms
step:1402/2090 train_time:66647ms step_avg:47.54ms
step:1403/2090 train_time:66735ms step_avg:47.57ms
step:1404/2090 train_time:66823ms step_avg:47.59ms
step:1405/2090 train_time:66910ms step_avg:47.62ms
step:1406/2090 train_time:66998ms step_avg:47.65ms
step:1407/2090 train_time:67086ms step_avg:47.68ms
step:1408/2090 train_time:67174ms step_avg:47.71ms
step:1409/2090 train_time:67262ms step_avg:47.74ms
step:1410/2090 train_time:67348ms step_avg:47.76ms
step:1411/2090 train_time:67436ms step_avg:47.79ms
step:1412/2090 train_time:67523ms step_avg:47.82ms
step:1413/2090 train_time:67610ms step_avg:47.85ms
step:1414/2090 train_time:67697ms step_avg:47.88ms
step:1415/2090 train_time:67786ms step_avg:47.91ms
step:1416/2090 train_time:67872ms step_avg:47.93ms
step:1417/2090 train_time:67961ms step_avg:47.96ms
step:1418/2090 train_time:68047ms step_avg:47.99ms
step:1419/2090 train_time:68135ms step_avg:48.02ms
step:1420/2090 train_time:68222ms step_avg:48.04ms
step:1421/2090 train_time:68309ms step_avg:48.07ms
step:1422/2090 train_time:68397ms step_avg:48.10ms
step:1423/2090 train_time:68485ms step_avg:48.13ms
step:1424/2090 train_time:68572ms step_avg:48.15ms
step:1425/2090 train_time:68661ms step_avg:48.18ms
step:1426/2090 train_time:68748ms step_avg:48.21ms
step:1427/2090 train_time:68835ms step_avg:48.24ms
step:1428/2090 train_time:68922ms step_avg:48.26ms
step:1429/2090 train_time:69009ms step_avg:48.29ms
step:1430/2090 train_time:69097ms step_avg:48.32ms
step:1431/2090 train_time:69185ms step_avg:48.35ms
step:1432/2090 train_time:69273ms step_avg:48.37ms
step:1433/2090 train_time:69361ms step_avg:48.40ms
step:1434/2090 train_time:69448ms step_avg:48.43ms
step:1435/2090 train_time:69536ms step_avg:48.46ms
step:1436/2090 train_time:69624ms step_avg:48.48ms
step:1437/2090 train_time:69712ms step_avg:48.51ms
step:1438/2090 train_time:69799ms step_avg:48.54ms
step:1439/2090 train_time:69887ms step_avg:48.57ms
step:1440/2090 train_time:69973ms step_avg:48.59ms
step:1441/2090 train_time:70063ms step_avg:48.62ms
step:1442/2090 train_time:70149ms step_avg:48.65ms
step:1443/2090 train_time:70237ms step_avg:48.67ms
step:1444/2090 train_time:70324ms step_avg:48.70ms
step:1445/2090 train_time:70411ms step_avg:48.73ms
step:1446/2090 train_time:70499ms step_avg:48.75ms
step:1447/2090 train_time:70588ms step_avg:48.78ms
step:1448/2090 train_time:70676ms step_avg:48.81ms
step:1449/2090 train_time:70764ms step_avg:48.84ms
step:1450/2090 train_time:70851ms step_avg:48.86ms
step:1451/2090 train_time:70939ms step_avg:48.89ms
step:1452/2090 train_time:71028ms step_avg:48.92ms
step:1453/2090 train_time:71115ms step_avg:48.94ms
step:1454/2090 train_time:71202ms step_avg:48.97ms
step:1455/2090 train_time:71290ms step_avg:49.00ms
step:1456/2090 train_time:71377ms step_avg:49.02ms
step:1457/2090 train_time:71465ms step_avg:49.05ms
step:1458/2090 train_time:71552ms step_avg:49.08ms
step:1459/2090 train_time:71642ms step_avg:49.10ms
step:1460/2090 train_time:71728ms step_avg:49.13ms
step:1461/2090 train_time:71816ms step_avg:49.16ms
step:1462/2090 train_time:71903ms step_avg:49.18ms
step:1463/2090 train_time:71991ms step_avg:49.21ms
step:1464/2090 train_time:72078ms step_avg:49.23ms
step:1465/2090 train_time:72166ms step_avg:49.26ms
step:1466/2090 train_time:72253ms step_avg:49.29ms
step:1467/2090 train_time:72341ms step_avg:49.31ms
step:1468/2090 train_time:72427ms step_avg:49.34ms
step:1469/2090 train_time:72515ms step_avg:49.36ms
step:1470/2090 train_time:72602ms step_avg:49.39ms
step:1471/2090 train_time:72689ms step_avg:49.41ms
step:1472/2090 train_time:72776ms step_avg:49.44ms
step:1473/2090 train_time:72865ms step_avg:49.47ms
step:1474/2090 train_time:72952ms step_avg:49.49ms
step:1475/2090 train_time:73041ms step_avg:49.52ms
step:1476/2090 train_time:73127ms step_avg:49.54ms
step:1477/2090 train_time:73215ms step_avg:49.57ms
step:1478/2090 train_time:73302ms step_avg:49.60ms
step:1479/2090 train_time:73389ms step_avg:49.62ms
step:1480/2090 train_time:73476ms step_avg:49.65ms
step:1481/2090 train_time:73565ms step_avg:49.67ms
step:1482/2090 train_time:73652ms step_avg:49.70ms
step:1483/2090 train_time:73739ms step_avg:49.72ms
step:1484/2090 train_time:73827ms step_avg:49.75ms
step:1485/2090 train_time:73915ms step_avg:49.77ms
step:1486/2090 train_time:74003ms step_avg:49.80ms
step:1487/2090 train_time:74090ms step_avg:49.83ms
step:1488/2090 train_time:74177ms step_avg:49.85ms
step:1489/2090 train_time:74265ms step_avg:49.88ms
step:1490/2090 train_time:74351ms step_avg:49.90ms
step:1491/2090 train_time:74439ms step_avg:49.93ms
step:1492/2090 train_time:74526ms step_avg:49.95ms
step:1493/2090 train_time:74614ms step_avg:49.98ms
step:1494/2090 train_time:74701ms step_avg:50.00ms
step:1495/2090 train_time:74789ms step_avg:50.03ms
step:1496/2090 train_time:74876ms step_avg:50.05ms
step:1497/2090 train_time:74965ms step_avg:50.08ms
step:1498/2090 train_time:75052ms step_avg:50.10ms
step:1499/2090 train_time:75140ms step_avg:50.13ms
step:1500/2090 train_time:75228ms step_avg:50.15ms
step:1500/2090 val_loss:3.4760 train_time:75317ms step_avg:50.21ms
step:1501/2090 train_time:75337ms step_avg:50.19ms
step:1502/2090 train_time:75406ms step_avg:50.20ms
step:1503/2090 train_time:75501ms step_avg:50.23ms
step:1504/2090 train_time:75588ms step_avg:50.26ms
step:1505/2090 train_time:75676ms step_avg:50.28ms
step:1506/2090 train_time:75761ms step_avg:50.31ms
step:1507/2090 train_time:75849ms step_avg:50.33ms
step:1508/2090 train_time:75935ms step_avg:50.35ms
step:1509/2090 train_time:76021ms step_avg:50.38ms
step:1510/2090 train_time:76108ms step_avg:50.40ms
step:1511/2090 train_time:76195ms step_avg:50.43ms
step:1512/2090 train_time:76282ms step_avg:50.45ms
step:1513/2090 train_time:76372ms step_avg:50.48ms
step:1514/2090 train_time:76461ms step_avg:50.50ms
step:1515/2090 train_time:76551ms step_avg:50.53ms
step:1516/2090 train_time:76640ms step_avg:50.55ms
step:1517/2090 train_time:76727ms step_avg:50.58ms
step:1518/2090 train_time:76813ms step_avg:50.60ms
step:1519/2090 train_time:76901ms step_avg:50.63ms
step:1520/2090 train_time:76987ms step_avg:50.65ms
step:1521/2090 train_time:77074ms step_avg:50.67ms
step:1522/2090 train_time:77160ms step_avg:50.70ms
step:1523/2090 train_time:77247ms step_avg:50.72ms
step:1524/2090 train_time:77335ms step_avg:50.74ms
step:1525/2090 train_time:77424ms step_avg:50.77ms
step:1526/2090 train_time:77512ms step_avg:50.79ms
step:1527/2090 train_time:77602ms step_avg:50.82ms
step:1528/2090 train_time:77689ms step_avg:50.84ms
step:1529/2090 train_time:77776ms step_avg:50.87ms
step:1530/2090 train_time:77862ms step_avg:50.89ms
step:1531/2090 train_time:77950ms step_avg:50.91ms
step:1532/2090 train_time:78037ms step_avg:50.94ms
step:1533/2090 train_time:78124ms step_avg:50.96ms
step:1534/2090 train_time:78210ms step_avg:50.98ms
step:1535/2090 train_time:78298ms step_avg:51.01ms
step:1536/2090 train_time:78386ms step_avg:51.03ms
step:1537/2090 train_time:78475ms step_avg:51.06ms
step:1538/2090 train_time:78563ms step_avg:51.08ms
step:1539/2090 train_time:78651ms step_avg:51.11ms
step:1540/2090 train_time:78739ms step_avg:51.13ms
step:1541/2090 train_time:78826ms step_avg:51.15ms
step:1542/2090 train_time:78913ms step_avg:51.18ms
step:1543/2090 train_time:79001ms step_avg:51.20ms
step:1544/2090 train_time:79087ms step_avg:51.22ms
step:1545/2090 train_time:79174ms step_avg:51.25ms
step:1546/2090 train_time:79261ms step_avg:51.27ms
step:1547/2090 train_time:79349ms step_avg:51.29ms
step:1548/2090 train_time:79437ms step_avg:51.32ms
step:1549/2090 train_time:79525ms step_avg:51.34ms
step:1550/2090 train_time:79613ms step_avg:51.36ms
step:1551/2090 train_time:79701ms step_avg:51.39ms
step:1552/2090 train_time:79788ms step_avg:51.41ms
step:1553/2090 train_time:79876ms step_avg:51.43ms
step:1554/2090 train_time:79962ms step_avg:51.46ms
step:1555/2090 train_time:80050ms step_avg:51.48ms
step:1556/2090 train_time:80137ms step_avg:51.50ms
step:1557/2090 train_time:80225ms step_avg:51.53ms
step:1558/2090 train_time:80311ms step_avg:51.55ms
step:1559/2090 train_time:80400ms step_avg:51.57ms
step:1560/2090 train_time:80487ms step_avg:51.59ms
step:1561/2090 train_time:80577ms step_avg:51.62ms
step:1562/2090 train_time:80664ms step_avg:51.64ms
step:1563/2090 train_time:80753ms step_avg:51.67ms
step:1564/2090 train_time:80839ms step_avg:51.69ms
step:1565/2090 train_time:80927ms step_avg:51.71ms
step:1566/2090 train_time:81014ms step_avg:51.73ms
step:1567/2090 train_time:81102ms step_avg:51.76ms
step:1568/2090 train_time:81188ms step_avg:51.78ms
step:1569/2090 train_time:81277ms step_avg:51.80ms
step:1570/2090 train_time:81363ms step_avg:51.82ms
step:1571/2090 train_time:81452ms step_avg:51.85ms
step:1572/2090 train_time:81539ms step_avg:51.87ms
step:1573/2090 train_time:81627ms step_avg:51.89ms
step:1574/2090 train_time:81715ms step_avg:51.92ms
step:1575/2090 train_time:81803ms step_avg:51.94ms
step:1576/2090 train_time:81889ms step_avg:51.96ms
step:1577/2090 train_time:81977ms step_avg:51.98ms
step:1578/2090 train_time:82063ms step_avg:52.00ms
step:1579/2090 train_time:82151ms step_avg:52.03ms
step:1580/2090 train_time:82237ms step_avg:52.05ms
step:1581/2090 train_time:82325ms step_avg:52.07ms
step:1582/2090 train_time:82413ms step_avg:52.09ms
step:1583/2090 train_time:82502ms step_avg:52.12ms
step:1584/2090 train_time:82589ms step_avg:52.14ms
step:1585/2090 train_time:82678ms step_avg:52.16ms
step:1586/2090 train_time:82765ms step_avg:52.18ms
step:1587/2090 train_time:82853ms step_avg:52.21ms
step:1588/2090 train_time:82940ms step_avg:52.23ms
step:1589/2090 train_time:83028ms step_avg:52.25ms
step:1590/2090 train_time:83114ms step_avg:52.27ms
step:1591/2090 train_time:83202ms step_avg:52.30ms
step:1592/2090 train_time:83289ms step_avg:52.32ms
step:1593/2090 train_time:83378ms step_avg:52.34ms
step:1594/2090 train_time:83464ms step_avg:52.36ms
step:1595/2090 train_time:83553ms step_avg:52.38ms
step:1596/2090 train_time:83640ms step_avg:52.41ms
step:1597/2090 train_time:83729ms step_avg:52.43ms
step:1598/2090 train_time:83816ms step_avg:52.45ms
step:1599/2090 train_time:83904ms step_avg:52.47ms
step:1600/2090 train_time:83991ms step_avg:52.49ms
step:1601/2090 train_time:84079ms step_avg:52.52ms
step:1602/2090 train_time:84165ms step_avg:52.54ms
step:1603/2090 train_time:84253ms step_avg:52.56ms
step:1604/2090 train_time:84341ms step_avg:52.58ms
step:1605/2090 train_time:84428ms step_avg:52.60ms
step:1606/2090 train_time:84515ms step_avg:52.62ms
step:1607/2090 train_time:84603ms step_avg:52.65ms
step:1608/2090 train_time:84691ms step_avg:52.67ms
step:1609/2090 train_time:84779ms step_avg:52.69ms
step:1610/2090 train_time:84865ms step_avg:52.71ms
step:1611/2090 train_time:84953ms step_avg:52.73ms
step:1612/2090 train_time:85040ms step_avg:52.75ms
step:1613/2090 train_time:85128ms step_avg:52.78ms
step:1614/2090 train_time:85215ms step_avg:52.80ms
step:1615/2090 train_time:85303ms step_avg:52.82ms
step:1616/2090 train_time:85390ms step_avg:52.84ms
step:1617/2090 train_time:85478ms step_avg:52.86ms
step:1618/2090 train_time:85565ms step_avg:52.88ms
step:1619/2090 train_time:85653ms step_avg:52.91ms
step:1620/2090 train_time:85740ms step_avg:52.93ms
step:1621/2090 train_time:85828ms step_avg:52.95ms
step:1622/2090 train_time:85916ms step_avg:52.97ms
step:1623/2090 train_time:86003ms step_avg:52.99ms
step:1624/2090 train_time:86090ms step_avg:53.01ms
step:1625/2090 train_time:86178ms step_avg:53.03ms
step:1626/2090 train_time:86265ms step_avg:53.05ms
step:1627/2090 train_time:86353ms step_avg:53.07ms
step:1628/2090 train_time:86440ms step_avg:53.10ms
step:1629/2090 train_time:86528ms step_avg:53.12ms
step:1630/2090 train_time:86615ms step_avg:53.14ms
step:1631/2090 train_time:86704ms step_avg:53.16ms
step:1632/2090 train_time:86790ms step_avg:53.18ms
step:1633/2090 train_time:86879ms step_avg:53.20ms
step:1634/2090 train_time:86965ms step_avg:53.22ms
step:1635/2090 train_time:87053ms step_avg:53.24ms
step:1636/2090 train_time:87140ms step_avg:53.26ms
step:1637/2090 train_time:87228ms step_avg:53.29ms
step:1638/2090 train_time:87315ms step_avg:53.31ms
step:1639/2090 train_time:87403ms step_avg:53.33ms
step:1640/2090 train_time:87490ms step_avg:53.35ms
step:1641/2090 train_time:87578ms step_avg:53.37ms
step:1642/2090 train_time:87665ms step_avg:53.39ms
step:1643/2090 train_time:87753ms step_avg:53.41ms
step:1644/2090 train_time:87841ms step_avg:53.43ms
step:1645/2090 train_time:87929ms step_avg:53.45ms
step:1646/2090 train_time:88016ms step_avg:53.47ms
step:1647/2090 train_time:88104ms step_avg:53.49ms
step:1648/2090 train_time:88191ms step_avg:53.51ms
step:1649/2090 train_time:88279ms step_avg:53.54ms
step:1650/2090 train_time:88366ms step_avg:53.56ms
step:1651/2090 train_time:88454ms step_avg:53.58ms
step:1652/2090 train_time:88541ms step_avg:53.60ms
step:1653/2090 train_time:88629ms step_avg:53.62ms
step:1654/2090 train_time:88715ms step_avg:53.64ms
step:1655/2090 train_time:88804ms step_avg:53.66ms
step:1656/2090 train_time:88891ms step_avg:53.68ms
step:1657/2090 train_time:88981ms step_avg:53.70ms
step:1658/2090 train_time:89068ms step_avg:53.72ms
step:1659/2090 train_time:89156ms step_avg:53.74ms
step:1660/2090 train_time:89242ms step_avg:53.76ms
step:1661/2090 train_time:89330ms step_avg:53.78ms
step:1662/2090 train_time:89418ms step_avg:53.80ms
step:1663/2090 train_time:89505ms step_avg:53.82ms
step:1664/2090 train_time:89592ms step_avg:53.84ms
step:1665/2090 train_time:89680ms step_avg:53.86ms
step:1666/2090 train_time:89767ms step_avg:53.88ms
step:1667/2090 train_time:89856ms step_avg:53.90ms
step:1668/2090 train_time:89942ms step_avg:53.92ms
step:1669/2090 train_time:90031ms step_avg:53.94ms
step:1670/2090 train_time:90118ms step_avg:53.96ms
step:1671/2090 train_time:90205ms step_avg:53.98ms
step:1672/2090 train_time:90292ms step_avg:54.00ms
step:1673/2090 train_time:90381ms step_avg:54.02ms
step:1674/2090 train_time:90468ms step_avg:54.04ms
step:1675/2090 train_time:90557ms step_avg:54.06ms
step:1676/2090 train_time:90643ms step_avg:54.08ms
step:1677/2090 train_time:90731ms step_avg:54.10ms
step:1678/2090 train_time:90819ms step_avg:54.12ms
step:1679/2090 train_time:90906ms step_avg:54.14ms
step:1680/2090 train_time:90993ms step_avg:54.16ms
step:1681/2090 train_time:91081ms step_avg:54.18ms
step:1682/2090 train_time:91168ms step_avg:54.20ms
step:1683/2090 train_time:91256ms step_avg:54.22ms
step:1684/2090 train_time:91342ms step_avg:54.24ms
step:1685/2090 train_time:91431ms step_avg:54.26ms
step:1686/2090 train_time:91518ms step_avg:54.28ms
step:1687/2090 train_time:91606ms step_avg:54.30ms
step:1688/2090 train_time:91692ms step_avg:54.32ms
step:1689/2090 train_time:91781ms step_avg:54.34ms
step:1690/2090 train_time:91869ms step_avg:54.36ms
step:1691/2090 train_time:91957ms step_avg:54.38ms
step:1692/2090 train_time:92044ms step_avg:54.40ms
step:1693/2090 train_time:92132ms step_avg:54.42ms
step:1694/2090 train_time:92219ms step_avg:54.44ms
step:1695/2090 train_time:92306ms step_avg:54.46ms
step:1696/2090 train_time:92393ms step_avg:54.48ms
step:1697/2090 train_time:92482ms step_avg:54.50ms
step:1698/2090 train_time:92569ms step_avg:54.52ms
step:1699/2090 train_time:92657ms step_avg:54.54ms
step:1700/2090 train_time:92744ms step_avg:54.56ms
step:1701/2090 train_time:92832ms step_avg:54.57ms
step:1702/2090 train_time:92918ms step_avg:54.59ms
step:1703/2090 train_time:93006ms step_avg:54.61ms
step:1704/2090 train_time:93093ms step_avg:54.63ms
step:1705/2090 train_time:93182ms step_avg:54.65ms
step:1706/2090 train_time:93268ms step_avg:54.67ms
step:1707/2090 train_time:93357ms step_avg:54.69ms
step:1708/2090 train_time:93443ms step_avg:54.71ms
step:1709/2090 train_time:93531ms step_avg:54.73ms
step:1710/2090 train_time:93619ms step_avg:54.75ms
step:1711/2090 train_time:93707ms step_avg:54.77ms
step:1712/2090 train_time:93793ms step_avg:54.79ms
step:1713/2090 train_time:93881ms step_avg:54.81ms
step:1714/2090 train_time:93967ms step_avg:54.82ms
step:1715/2090 train_time:94056ms step_avg:54.84ms
step:1716/2090 train_time:94142ms step_avg:54.86ms
step:1717/2090 train_time:94230ms step_avg:54.88ms
step:1718/2090 train_time:94318ms step_avg:54.90ms
step:1719/2090 train_time:94406ms step_avg:54.92ms
step:1720/2090 train_time:94493ms step_avg:54.94ms
step:1721/2090 train_time:94581ms step_avg:54.96ms
step:1722/2090 train_time:94668ms step_avg:54.98ms
step:1723/2090 train_time:94758ms step_avg:55.00ms
step:1724/2090 train_time:94844ms step_avg:55.01ms
step:1725/2090 train_time:94933ms step_avg:55.03ms
step:1726/2090 train_time:95021ms step_avg:55.05ms
step:1727/2090 train_time:95108ms step_avg:55.07ms
step:1728/2090 train_time:95196ms step_avg:55.09ms
step:1729/2090 train_time:95284ms step_avg:55.11ms
step:1730/2090 train_time:95371ms step_avg:55.13ms
step:1731/2090 train_time:95460ms step_avg:55.15ms
step:1732/2090 train_time:95547ms step_avg:55.17ms
step:1733/2090 train_time:95634ms step_avg:55.18ms
step:1734/2090 train_time:95722ms step_avg:55.20ms
step:1735/2090 train_time:95810ms step_avg:55.22ms
step:1736/2090 train_time:95897ms step_avg:55.24ms
step:1737/2090 train_time:95984ms step_avg:55.26ms
step:1738/2090 train_time:96072ms step_avg:55.28ms
step:1739/2090 train_time:96159ms step_avg:55.30ms
step:1740/2090 train_time:96246ms step_avg:55.31ms
step:1741/2090 train_time:96334ms step_avg:55.33ms
step:1742/2090 train_time:96422ms step_avg:55.35ms
step:1743/2090 train_time:96509ms step_avg:55.37ms
step:1744/2090 train_time:96596ms step_avg:55.39ms
step:1745/2090 train_time:96683ms step_avg:55.41ms
step:1746/2090 train_time:96770ms step_avg:55.42ms
step:1747/2090 train_time:96859ms step_avg:55.44ms
step:1748/2090 train_time:96946ms step_avg:55.46ms
step:1749/2090 train_time:97034ms step_avg:55.48ms
step:1750/2090 train_time:97121ms step_avg:55.50ms
step:1750/2090 val_loss:3.3744 train_time:97209ms step_avg:55.55ms
step:1751/2090 train_time:97231ms step_avg:55.53ms
step:1752/2090 train_time:97298ms step_avg:55.54ms
step:1753/2090 train_time:97392ms step_avg:55.56ms
step:1754/2090 train_time:97481ms step_avg:55.58ms
step:1755/2090 train_time:97569ms step_avg:55.59ms
step:1756/2090 train_time:97654ms step_avg:55.61ms
step:1757/2090 train_time:97741ms step_avg:55.63ms
step:1758/2090 train_time:97827ms step_avg:55.65ms
step:1759/2090 train_time:97914ms step_avg:55.66ms
step:1760/2090 train_time:97999ms step_avg:55.68ms
step:1761/2090 train_time:98086ms step_avg:55.70ms
step:1762/2090 train_time:98174ms step_avg:55.72ms
step:1763/2090 train_time:98263ms step_avg:55.74ms
step:1764/2090 train_time:98353ms step_avg:55.76ms
step:1765/2090 train_time:98441ms step_avg:55.77ms
step:1766/2090 train_time:98529ms step_avg:55.79ms
step:1767/2090 train_time:98617ms step_avg:55.81ms
step:1768/2090 train_time:98704ms step_avg:55.83ms
step:1769/2090 train_time:98791ms step_avg:55.85ms
step:1770/2090 train_time:98877ms step_avg:55.86ms
step:1771/2090 train_time:98964ms step_avg:55.88ms
step:1772/2090 train_time:99050ms step_avg:55.90ms
step:1773/2090 train_time:99138ms step_avg:55.92ms
step:1774/2090 train_time:99225ms step_avg:55.93ms
step:1775/2090 train_time:99315ms step_avg:55.95ms
step:1776/2090 train_time:99403ms step_avg:55.97ms
step:1777/2090 train_time:99492ms step_avg:55.99ms
step:1778/2090 train_time:99579ms step_avg:56.01ms
step:1779/2090 train_time:99667ms step_avg:56.02ms
step:1780/2090 train_time:99754ms step_avg:56.04ms
step:1781/2090 train_time:99840ms step_avg:56.06ms
step:1782/2090 train_time:99926ms step_avg:56.08ms
step:1783/2090 train_time:100014ms step_avg:56.09ms
step:1784/2090 train_time:100100ms step_avg:56.11ms
step:1785/2090 train_time:100189ms step_avg:56.13ms
step:1786/2090 train_time:100277ms step_avg:56.15ms
step:1787/2090 train_time:100365ms step_avg:56.16ms
step:1788/2090 train_time:100454ms step_avg:56.18ms
step:1789/2090 train_time:100541ms step_avg:56.20ms
step:1790/2090 train_time:100628ms step_avg:56.22ms
step:1791/2090 train_time:100716ms step_avg:56.23ms
step:1792/2090 train_time:100802ms step_avg:56.25ms
step:1793/2090 train_time:100889ms step_avg:56.27ms
step:1794/2090 train_time:100976ms step_avg:56.29ms
step:1795/2090 train_time:101063ms step_avg:56.30ms
step:1796/2090 train_time:101150ms step_avg:56.32ms
step:1797/2090 train_time:101239ms step_avg:56.34ms
step:1798/2090 train_time:101327ms step_avg:56.36ms
step:1799/2090 train_time:101415ms step_avg:56.37ms
step:1800/2090 train_time:101502ms step_avg:56.39ms
step:1801/2090 train_time:101590ms step_avg:56.41ms
step:1802/2090 train_time:101677ms step_avg:56.42ms
step:1803/2090 train_time:101764ms step_avg:56.44ms
step:1804/2090 train_time:101852ms step_avg:56.46ms
step:1805/2090 train_time:101939ms step_avg:56.48ms
step:1806/2090 train_time:102026ms step_avg:56.49ms
step:1807/2090 train_time:102114ms step_avg:56.51ms
step:1808/2090 train_time:102200ms step_avg:56.53ms
step:1809/2090 train_time:102289ms step_avg:56.54ms
step:1810/2090 train_time:102377ms step_avg:56.56ms
step:1811/2090 train_time:102465ms step_avg:56.58ms
step:1812/2090 train_time:102552ms step_avg:56.60ms
step:1813/2090 train_time:102639ms step_avg:56.61ms
step:1814/2090 train_time:102726ms step_avg:56.63ms
step:1815/2090 train_time:102815ms step_avg:56.65ms
step:1816/2090 train_time:102901ms step_avg:56.66ms
step:1817/2090 train_time:102988ms step_avg:56.68ms
step:1818/2090 train_time:103076ms step_avg:56.70ms
step:1819/2090 train_time:103163ms step_avg:56.71ms
step:1820/2090 train_time:103251ms step_avg:56.73ms
step:1821/2090 train_time:103339ms step_avg:56.75ms
step:1822/2090 train_time:103426ms step_avg:56.77ms
step:1823/2090 train_time:103515ms step_avg:56.78ms
step:1824/2090 train_time:103601ms step_avg:56.80ms
step:1825/2090 train_time:103690ms step_avg:56.82ms
step:1826/2090 train_time:103776ms step_avg:56.83ms
step:1827/2090 train_time:103863ms step_avg:56.85ms
step:1828/2090 train_time:103950ms step_avg:56.87ms
step:1829/2090 train_time:104038ms step_avg:56.88ms
step:1830/2090 train_time:104124ms step_avg:56.90ms
step:1831/2090 train_time:104212ms step_avg:56.92ms
step:1832/2090 train_time:104300ms step_avg:56.93ms
step:1833/2090 train_time:104388ms step_avg:56.95ms
step:1834/2090 train_time:104476ms step_avg:56.97ms
step:1835/2090 train_time:104563ms step_avg:56.98ms
step:1836/2090 train_time:104650ms step_avg:57.00ms
step:1837/2090 train_time:104739ms step_avg:57.02ms
step:1838/2090 train_time:104826ms step_avg:57.03ms
step:1839/2090 train_time:104914ms step_avg:57.05ms
step:1840/2090 train_time:105001ms step_avg:57.07ms
step:1841/2090 train_time:105089ms step_avg:57.08ms
step:1842/2090 train_time:105176ms step_avg:57.10ms
step:1843/2090 train_time:105264ms step_avg:57.12ms
step:1844/2090 train_time:105351ms step_avg:57.13ms
step:1845/2090 train_time:105438ms step_avg:57.15ms
step:1846/2090 train_time:105525ms step_avg:57.16ms
step:1847/2090 train_time:105614ms step_avg:57.18ms
step:1848/2090 train_time:105702ms step_avg:57.20ms
step:1849/2090 train_time:105790ms step_avg:57.21ms
step:1850/2090 train_time:105877ms step_avg:57.23ms
step:1851/2090 train_time:105965ms step_avg:57.25ms
step:1852/2090 train_time:106052ms step_avg:57.26ms
step:1853/2090 train_time:106140ms step_avg:57.28ms
step:1854/2090 train_time:106227ms step_avg:57.30ms
step:1855/2090 train_time:106315ms step_avg:57.31ms
step:1856/2090 train_time:106402ms step_avg:57.33ms
step:1857/2090 train_time:106489ms step_avg:57.34ms
step:1858/2090 train_time:106576ms step_avg:57.36ms
step:1859/2090 train_time:106664ms step_avg:57.38ms
step:1860/2090 train_time:106752ms step_avg:57.39ms
step:1861/2090 train_time:106839ms step_avg:57.41ms
step:1862/2090 train_time:106926ms step_avg:57.43ms
step:1863/2090 train_time:107014ms step_avg:57.44ms
step:1864/2090 train_time:107101ms step_avg:57.46ms
step:1865/2090 train_time:107189ms step_avg:57.47ms
step:1866/2090 train_time:107276ms step_avg:57.49ms
step:1867/2090 train_time:107364ms step_avg:57.51ms
step:1868/2090 train_time:107451ms step_avg:57.52ms
step:1869/2090 train_time:107539ms step_avg:57.54ms
step:1870/2090 train_time:107627ms step_avg:57.55ms
step:1871/2090 train_time:107716ms step_avg:57.57ms
step:1872/2090 train_time:107802ms step_avg:57.59ms
step:1873/2090 train_time:107890ms step_avg:57.60ms
step:1874/2090 train_time:107976ms step_avg:57.62ms
step:1875/2090 train_time:108064ms step_avg:57.63ms
step:1876/2090 train_time:108152ms step_avg:57.65ms
step:1877/2090 train_time:108240ms step_avg:57.67ms
step:1878/2090 train_time:108326ms step_avg:57.68ms
step:1879/2090 train_time:108415ms step_avg:57.70ms
step:1880/2090 train_time:108502ms step_avg:57.71ms
step:1881/2090 train_time:108590ms step_avg:57.73ms
step:1882/2090 train_time:108676ms step_avg:57.75ms
step:1883/2090 train_time:108764ms step_avg:57.76ms
step:1884/2090 train_time:108854ms step_avg:57.78ms
step:1885/2090 train_time:108941ms step_avg:57.79ms
step:1886/2090 train_time:109028ms step_avg:57.81ms
step:1887/2090 train_time:109116ms step_avg:57.83ms
step:1888/2090 train_time:109203ms step_avg:57.84ms
step:1889/2090 train_time:109291ms step_avg:57.86ms
step:1890/2090 train_time:109377ms step_avg:57.87ms
step:1891/2090 train_time:109464ms step_avg:57.89ms
step:1892/2090 train_time:109552ms step_avg:57.90ms
step:1893/2090 train_time:109639ms step_avg:57.92ms
step:1894/2090 train_time:109726ms step_avg:57.93ms
step:1895/2090 train_time:109815ms step_avg:57.95ms
step:1896/2090 train_time:109901ms step_avg:57.96ms
step:1897/2090 train_time:109990ms step_avg:57.98ms
step:1898/2090 train_time:110076ms step_avg:58.00ms
step:1899/2090 train_time:110164ms step_avg:58.01ms
step:1900/2090 train_time:110251ms step_avg:58.03ms
step:1901/2090 train_time:110339ms step_avg:58.04ms
step:1902/2090 train_time:110425ms step_avg:58.06ms
step:1903/2090 train_time:110515ms step_avg:58.07ms
step:1904/2090 train_time:110601ms step_avg:58.09ms
step:1905/2090 train_time:110689ms step_avg:58.10ms
step:1906/2090 train_time:110776ms step_avg:58.12ms
step:1907/2090 train_time:110863ms step_avg:58.13ms
step:1908/2090 train_time:110951ms step_avg:58.15ms
step:1909/2090 train_time:111039ms step_avg:58.17ms
step:1910/2090 train_time:111126ms step_avg:58.18ms
step:1911/2090 train_time:111215ms step_avg:58.20ms
step:1912/2090 train_time:111301ms step_avg:58.21ms
step:1913/2090 train_time:111389ms step_avg:58.23ms
step:1914/2090 train_time:111476ms step_avg:58.24ms
step:1915/2090 train_time:111564ms step_avg:58.26ms
step:1916/2090 train_time:111652ms step_avg:58.27ms
step:1917/2090 train_time:111740ms step_avg:58.29ms
step:1918/2090 train_time:111827ms step_avg:58.30ms
step:1919/2090 train_time:111914ms step_avg:58.32ms
step:1920/2090 train_time:112001ms step_avg:58.33ms
step:1921/2090 train_time:112089ms step_avg:58.35ms
step:1922/2090 train_time:112177ms step_avg:58.36ms
step:1923/2090 train_time:112264ms step_avg:58.38ms
step:1924/2090 train_time:112351ms step_avg:58.39ms
step:1925/2090 train_time:112438ms step_avg:58.41ms
step:1926/2090 train_time:112525ms step_avg:58.42ms
step:1927/2090 train_time:112614ms step_avg:58.44ms
step:1928/2090 train_time:112701ms step_avg:58.45ms
step:1929/2090 train_time:112789ms step_avg:58.47ms
step:1930/2090 train_time:112876ms step_avg:58.49ms
step:1931/2090 train_time:112965ms step_avg:58.50ms
step:1932/2090 train_time:113052ms step_avg:58.52ms
step:1933/2090 train_time:113139ms step_avg:58.53ms
step:1934/2090 train_time:113227ms step_avg:58.55ms
step:1935/2090 train_time:113315ms step_avg:58.56ms
step:1936/2090 train_time:113402ms step_avg:58.58ms
step:1937/2090 train_time:113490ms step_avg:58.59ms
step:1938/2090 train_time:113576ms step_avg:58.60ms
step:1939/2090 train_time:113664ms step_avg:58.62ms
step:1940/2090 train_time:113752ms step_avg:58.64ms
step:1941/2090 train_time:113840ms step_avg:58.65ms
step:1942/2090 train_time:113927ms step_avg:58.66ms
step:1943/2090 train_time:114015ms step_avg:58.68ms
step:1944/2090 train_time:114102ms step_avg:58.69ms
step:1945/2090 train_time:114190ms step_avg:58.71ms
step:1946/2090 train_time:114277ms step_avg:58.72ms
step:1947/2090 train_time:114364ms step_avg:58.74ms
step:1948/2090 train_time:114451ms step_avg:58.75ms
step:1949/2090 train_time:114539ms step_avg:58.77ms
step:1950/2090 train_time:114626ms step_avg:58.78ms
step:1951/2090 train_time:114715ms step_avg:58.80ms
step:1952/2090 train_time:114802ms step_avg:58.81ms
step:1953/2090 train_time:114890ms step_avg:58.83ms
step:1954/2090 train_time:114976ms step_avg:58.84ms
step:1955/2090 train_time:115064ms step_avg:58.86ms
step:1956/2090 train_time:115152ms step_avg:58.87ms
step:1957/2090 train_time:115239ms step_avg:58.89ms
step:1958/2090 train_time:115327ms step_avg:58.90ms
step:1959/2090 train_time:115415ms step_avg:58.92ms
step:1960/2090 train_time:115502ms step_avg:58.93ms
step:1961/2090 train_time:115590ms step_avg:58.94ms
step:1962/2090 train_time:115676ms step_avg:58.96ms
step:1963/2090 train_time:115764ms step_avg:58.97ms
step:1964/2090 train_time:115852ms step_avg:58.99ms
step:1965/2090 train_time:115940ms step_avg:59.00ms
step:1966/2090 train_time:116027ms step_avg:59.02ms
step:1967/2090 train_time:116116ms step_avg:59.03ms
step:1968/2090 train_time:116203ms step_avg:59.05ms
step:1969/2090 train_time:116291ms step_avg:59.06ms
step:1970/2090 train_time:116377ms step_avg:59.07ms
step:1971/2090 train_time:116465ms step_avg:59.09ms
step:1972/2090 train_time:116552ms step_avg:59.10ms
step:1973/2090 train_time:116640ms step_avg:59.12ms
step:1974/2090 train_time:116726ms step_avg:59.13ms
step:1975/2090 train_time:116815ms step_avg:59.15ms
step:1976/2090 train_time:116902ms step_avg:59.16ms
step:1977/2090 train_time:116990ms step_avg:59.18ms
step:1978/2090 train_time:117077ms step_avg:59.19ms
step:1979/2090 train_time:117165ms step_avg:59.20ms
step:1980/2090 train_time:117253ms step_avg:59.22ms
step:1981/2090 train_time:117341ms step_avg:59.23ms
step:1982/2090 train_time:117428ms step_avg:59.25ms
step:1983/2090 train_time:117516ms step_avg:59.26ms
step:1984/2090 train_time:117603ms step_avg:59.28ms
step:1985/2090 train_time:117691ms step_avg:59.29ms
step:1986/2090 train_time:117778ms step_avg:59.30ms
step:1987/2090 train_time:117865ms step_avg:59.32ms
step:1988/2090 train_time:117952ms step_avg:59.33ms
step:1989/2090 train_time:118040ms step_avg:59.35ms
step:1990/2090 train_time:118127ms step_avg:59.36ms
step:1991/2090 train_time:118216ms step_avg:59.37ms
step:1992/2090 train_time:118302ms step_avg:59.39ms
step:1993/2090 train_time:118391ms step_avg:59.40ms
step:1994/2090 train_time:118478ms step_avg:59.42ms
step:1995/2090 train_time:118565ms step_avg:59.43ms
step:1996/2090 train_time:118652ms step_avg:59.44ms
step:1997/2090 train_time:118739ms step_avg:59.46ms
step:1998/2090 train_time:118827ms step_avg:59.47ms
step:1999/2090 train_time:118915ms step_avg:59.49ms
step:2000/2090 train_time:119002ms step_avg:59.50ms
step:2000/2090 val_loss:3.2973 train_time:119092ms step_avg:59.55ms
step:2001/2090 train_time:119113ms step_avg:59.53ms
step:2002/2090 train_time:119183ms step_avg:59.53ms
step:2003/2090 train_time:119274ms step_avg:59.55ms
step:2004/2090 train_time:119363ms step_avg:59.56ms
step:2005/2090 train_time:119450ms step_avg:59.58ms
step:2006/2090 train_time:119537ms step_avg:59.59ms
step:2007/2090 train_time:119625ms step_avg:59.60ms
step:2008/2090 train_time:119711ms step_avg:59.62ms
step:2009/2090 train_time:119798ms step_avg:59.63ms
step:2010/2090 train_time:119884ms step_avg:59.64ms
step:2011/2090 train_time:119971ms step_avg:59.66ms
step:2012/2090 train_time:120059ms step_avg:59.67ms
step:2013/2090 train_time:120151ms step_avg:59.69ms
step:2014/2090 train_time:120241ms step_avg:59.70ms
step:2015/2090 train_time:120329ms step_avg:59.72ms
step:2016/2090 train_time:120417ms step_avg:59.73ms
step:2017/2090 train_time:120506ms step_avg:59.74ms
step:2018/2090 train_time:120593ms step_avg:59.76ms
step:2019/2090 train_time:120681ms step_avg:59.77ms
step:2020/2090 train_time:120766ms step_avg:59.79ms
step:2021/2090 train_time:120853ms step_avg:59.80ms
step:2022/2090 train_time:120939ms step_avg:59.81ms
step:2023/2090 train_time:121027ms step_avg:59.83ms
step:2024/2090 train_time:121116ms step_avg:59.84ms
step:2025/2090 train_time:121206ms step_avg:59.85ms
step:2026/2090 train_time:121294ms step_avg:59.87ms
step:2027/2090 train_time:121382ms step_avg:59.88ms
step:2028/2090 train_time:121469ms step_avg:59.90ms
step:2029/2090 train_time:121558ms step_avg:59.91ms
step:2030/2090 train_time:121644ms step_avg:59.92ms
step:2031/2090 train_time:121731ms step_avg:59.94ms
step:2032/2090 train_time:121818ms step_avg:59.95ms
step:2033/2090 train_time:121905ms step_avg:59.96ms
step:2034/2090 train_time:121991ms step_avg:59.98ms
step:2035/2090 train_time:122080ms step_avg:59.99ms
step:2036/2090 train_time:122168ms step_avg:60.00ms
step:2037/2090 train_time:122256ms step_avg:60.02ms
step:2038/2090 train_time:122344ms step_avg:60.03ms
step:2039/2090 train_time:122433ms step_avg:60.05ms
step:2040/2090 train_time:122520ms step_avg:60.06ms
step:2041/2090 train_time:122608ms step_avg:60.07ms
step:2042/2090 train_time:122694ms step_avg:60.09ms
step:2043/2090 train_time:122783ms step_avg:60.10ms
step:2044/2090 train_time:122869ms step_avg:60.11ms
step:2045/2090 train_time:122956ms step_avg:60.13ms
step:2046/2090 train_time:123043ms step_avg:60.14ms
step:2047/2090 train_time:123132ms step_avg:60.15ms
step:2048/2090 train_time:123219ms step_avg:60.17ms
step:2049/2090 train_time:123308ms step_avg:60.18ms
step:2050/2090 train_time:123396ms step_avg:60.19ms
step:2051/2090 train_time:123486ms step_avg:60.21ms
step:2052/2090 train_time:123574ms step_avg:60.22ms
step:2053/2090 train_time:123661ms step_avg:60.23ms
step:2054/2090 train_time:123748ms step_avg:60.25ms
step:2055/2090 train_time:123836ms step_avg:60.26ms
step:2056/2090 train_time:123923ms step_avg:60.27ms
step:2057/2090 train_time:124011ms step_avg:60.29ms
step:2058/2090 train_time:124098ms step_avg:60.30ms
step:2059/2090 train_time:124187ms step_avg:60.31ms
step:2060/2090 train_time:124275ms step_avg:60.33ms
step:2061/2090 train_time:124364ms step_avg:60.34ms
step:2062/2090 train_time:124451ms step_avg:60.35ms
step:2063/2090 train_time:124539ms step_avg:60.37ms
step:2064/2090 train_time:124627ms step_avg:60.38ms
step:2065/2090 train_time:124715ms step_avg:60.39ms
step:2066/2090 train_time:124801ms step_avg:60.41ms
step:2067/2090 train_time:124889ms step_avg:60.42ms
step:2068/2090 train_time:124975ms step_avg:60.43ms
step:2069/2090 train_time:125064ms step_avg:60.45ms
step:2070/2090 train_time:125151ms step_avg:60.46ms
step:2071/2090 train_time:125240ms step_avg:60.47ms
step:2072/2090 train_time:125328ms step_avg:60.49ms
step:2073/2090 train_time:125416ms step_avg:60.50ms
step:2074/2090 train_time:125503ms step_avg:60.51ms
step:2075/2090 train_time:125590ms step_avg:60.53ms
step:2076/2090 train_time:125678ms step_avg:60.54ms
step:2077/2090 train_time:125767ms step_avg:60.55ms
step:2078/2090 train_time:125854ms step_avg:60.56ms
step:2079/2090 train_time:125942ms step_avg:60.58ms
step:2080/2090 train_time:126028ms step_avg:60.59ms
step:2081/2090 train_time:126116ms step_avg:60.60ms
step:2082/2090 train_time:126204ms step_avg:60.62ms
step:2083/2090 train_time:126291ms step_avg:60.63ms
step:2084/2090 train_time:126379ms step_avg:60.64ms
step:2085/2090 train_time:126469ms step_avg:60.66ms
step:2086/2090 train_time:126556ms step_avg:60.67ms
step:2087/2090 train_time:126644ms step_avg:60.68ms
step:2088/2090 train_time:126731ms step_avg:60.69ms
step:2089/2090 train_time:126819ms step_avg:60.71ms
step:2090/2090 train_time:126907ms step_avg:60.72ms
step:2090/2090 val_loss:3.2754 train_time:126997ms step_avg:60.76ms
peak memory allocated: 30216 MiB reserved: 44676 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # weight decay
                if wd != 0:
                    mask = (update * p_slice) >= 0
                    # lr as weight decay  schedule
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)

                    update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)
                
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2050  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 18 12:22:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2090 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2090 train_time:82ms step_avg:82.21ms
step:2/2090 train_time:107ms step_avg:53.51ms
step:3/2090 train_time:128ms step_avg:42.68ms
step:4/2090 train_time:152ms step_avg:37.92ms
step:5/2090 train_time:184ms step_avg:36.88ms
step:6/2090 train_time:291ms step_avg:48.43ms
step:7/2090 train_time:309ms step_avg:44.21ms
step:8/2090 train_time:335ms step_avg:41.93ms
step:9/2090 train_time:368ms step_avg:40.94ms
step:10/2090 train_time:401ms step_avg:40.12ms
step:11/2090 train_time:435ms step_avg:39.50ms
step:12/2090 train_time:467ms step_avg:38.95ms
step:13/2090 train_time:501ms step_avg:38.56ms
step:14/2090 train_time:534ms step_avg:38.15ms
step:15/2090 train_time:568ms step_avg:37.85ms
step:16/2090 train_time:600ms step_avg:37.53ms
step:17/2090 train_time:634ms step_avg:37.30ms
step:18/2090 train_time:667ms step_avg:37.05ms
step:19/2090 train_time:701ms step_avg:36.88ms
step:20/2090 train_time:733ms step_avg:36.67ms
step:21/2090 train_time:767ms step_avg:36.54ms
step:22/2090 train_time:800ms step_avg:36.37ms
step:23/2090 train_time:834ms step_avg:36.25ms
step:24/2090 train_time:867ms step_avg:36.11ms
step:25/2090 train_time:900ms step_avg:36.01ms
step:26/2090 train_time:933ms step_avg:35.89ms
step:27/2090 train_time:967ms step_avg:35.81ms
step:28/2090 train_time:1000ms step_avg:35.71ms
step:29/2090 train_time:1033ms step_avg:35.63ms
step:30/2090 train_time:1066ms step_avg:35.54ms
step:31/2090 train_time:1100ms step_avg:35.47ms
step:32/2090 train_time:1132ms step_avg:35.39ms
step:33/2090 train_time:1167ms step_avg:35.37ms
step:34/2090 train_time:1201ms step_avg:35.31ms
step:35/2090 train_time:1236ms step_avg:35.30ms
step:36/2090 train_time:1269ms step_avg:35.25ms
step:37/2090 train_time:1303ms step_avg:35.23ms
step:38/2090 train_time:1336ms step_avg:35.16ms
step:39/2090 train_time:1371ms step_avg:35.14ms
step:40/2090 train_time:1404ms step_avg:35.09ms
step:41/2090 train_time:1438ms step_avg:35.07ms
step:42/2090 train_time:1471ms step_avg:35.01ms
step:43/2090 train_time:1505ms step_avg:34.99ms
step:44/2090 train_time:1537ms step_avg:34.94ms
step:45/2090 train_time:1571ms step_avg:34.91ms
step:46/2090 train_time:1604ms step_avg:34.86ms
step:47/2090 train_time:1637ms step_avg:34.84ms
step:48/2090 train_time:1670ms step_avg:34.80ms
step:49/2090 train_time:1704ms step_avg:34.78ms
step:50/2090 train_time:1737ms step_avg:34.74ms
step:51/2090 train_time:1771ms step_avg:34.72ms
step:52/2090 train_time:1803ms step_avg:34.68ms
step:53/2090 train_time:1837ms step_avg:34.66ms
step:54/2090 train_time:1870ms step_avg:34.62ms
step:55/2090 train_time:1903ms step_avg:34.61ms
step:56/2090 train_time:1936ms step_avg:34.57ms
step:57/2090 train_time:1970ms step_avg:34.56ms
step:58/2090 train_time:2003ms step_avg:34.53ms
step:59/2090 train_time:2036ms step_avg:34.51ms
step:60/2090 train_time:2069ms step_avg:34.48ms
step:61/2090 train_time:2103ms step_avg:34.47ms
step:62/2090 train_time:2136ms step_avg:34.44ms
step:63/2090 train_time:2169ms step_avg:34.43ms
step:64/2090 train_time:2202ms step_avg:34.41ms
step:65/2090 train_time:2236ms step_avg:34.40ms
step:66/2090 train_time:2269ms step_avg:34.38ms
step:67/2090 train_time:2303ms step_avg:34.37ms
step:68/2090 train_time:2336ms step_avg:34.35ms
step:69/2090 train_time:2370ms step_avg:34.35ms
step:70/2090 train_time:2403ms step_avg:34.33ms
step:71/2090 train_time:2437ms step_avg:34.32ms
step:72/2090 train_time:2470ms step_avg:34.30ms
step:73/2090 train_time:2504ms step_avg:34.30ms
step:74/2090 train_time:2537ms step_avg:34.28ms
step:75/2090 train_time:2571ms step_avg:34.27ms
step:76/2090 train_time:2603ms step_avg:34.25ms
step:77/2090 train_time:2637ms step_avg:34.25ms
step:78/2090 train_time:2670ms step_avg:34.23ms
step:79/2090 train_time:2703ms step_avg:34.22ms
step:80/2090 train_time:2736ms step_avg:34.20ms
step:81/2090 train_time:2770ms step_avg:34.19ms
step:82/2090 train_time:2803ms step_avg:34.18ms
step:83/2090 train_time:2836ms step_avg:34.17ms
step:84/2090 train_time:2869ms step_avg:34.15ms
step:85/2090 train_time:2902ms step_avg:34.14ms
step:86/2090 train_time:2935ms step_avg:34.13ms
step:87/2090 train_time:2968ms step_avg:34.12ms
step:88/2090 train_time:3001ms step_avg:34.10ms
step:89/2090 train_time:3034ms step_avg:34.10ms
step:90/2090 train_time:3067ms step_avg:34.08ms
step:91/2090 train_time:3101ms step_avg:34.07ms
step:92/2090 train_time:3133ms step_avg:34.06ms
step:93/2090 train_time:3167ms step_avg:34.05ms
step:94/2090 train_time:3200ms step_avg:34.04ms
step:95/2090 train_time:3234ms step_avg:34.04ms
step:96/2090 train_time:3266ms step_avg:34.03ms
step:97/2090 train_time:3300ms step_avg:34.02ms
step:98/2090 train_time:3333ms step_avg:34.01ms
step:99/2090 train_time:3367ms step_avg:34.01ms
step:100/2090 train_time:3400ms step_avg:34.00ms
step:101/2090 train_time:3433ms step_avg:33.99ms
step:102/2090 train_time:3466ms step_avg:33.98ms
step:103/2090 train_time:3500ms step_avg:33.98ms
step:104/2090 train_time:3532ms step_avg:33.96ms
step:105/2090 train_time:3567ms step_avg:33.97ms
step:106/2090 train_time:3599ms step_avg:33.96ms
step:107/2090 train_time:3633ms step_avg:33.96ms
step:108/2090 train_time:3666ms step_avg:33.94ms
step:109/2090 train_time:3700ms step_avg:33.94ms
step:110/2090 train_time:3732ms step_avg:33.93ms
step:111/2090 train_time:3766ms step_avg:33.93ms
step:112/2090 train_time:3799ms step_avg:33.92ms
step:113/2090 train_time:3832ms step_avg:33.92ms
step:114/2090 train_time:3865ms step_avg:33.91ms
step:115/2090 train_time:3899ms step_avg:33.90ms
step:116/2090 train_time:3931ms step_avg:33.89ms
step:117/2090 train_time:3965ms step_avg:33.89ms
step:118/2090 train_time:3997ms step_avg:33.88ms
step:119/2090 train_time:4031ms step_avg:33.87ms
step:120/2090 train_time:4064ms step_avg:33.86ms
step:121/2090 train_time:4097ms step_avg:33.86ms
step:122/2090 train_time:4130ms step_avg:33.85ms
step:123/2090 train_time:4163ms step_avg:33.85ms
step:124/2090 train_time:4196ms step_avg:33.84ms
step:125/2090 train_time:4230ms step_avg:33.84ms
step:126/2090 train_time:4262ms step_avg:33.83ms
step:127/2090 train_time:4296ms step_avg:33.83ms
step:128/2090 train_time:4329ms step_avg:33.82ms
step:129/2090 train_time:4362ms step_avg:33.82ms
step:130/2090 train_time:4395ms step_avg:33.81ms
step:131/2090 train_time:4429ms step_avg:33.81ms
step:132/2090 train_time:4462ms step_avg:33.80ms
step:133/2090 train_time:4496ms step_avg:33.80ms
step:134/2090 train_time:4529ms step_avg:33.79ms
step:135/2090 train_time:4563ms step_avg:33.80ms
step:136/2090 train_time:4595ms step_avg:33.79ms
step:137/2090 train_time:4629ms step_avg:33.79ms
step:138/2090 train_time:4662ms step_avg:33.78ms
step:139/2090 train_time:4695ms step_avg:33.78ms
step:140/2090 train_time:4728ms step_avg:33.77ms
step:141/2090 train_time:4761ms step_avg:33.77ms
step:142/2090 train_time:4794ms step_avg:33.76ms
step:143/2090 train_time:4828ms step_avg:33.76ms
step:144/2090 train_time:4861ms step_avg:33.76ms
step:145/2090 train_time:4894ms step_avg:33.75ms
step:146/2090 train_time:4927ms step_avg:33.75ms
step:147/2090 train_time:4961ms step_avg:33.75ms
step:148/2090 train_time:4994ms step_avg:33.74ms
step:149/2090 train_time:5027ms step_avg:33.74ms
step:150/2090 train_time:5060ms step_avg:33.73ms
step:151/2090 train_time:5093ms step_avg:33.73ms
step:152/2090 train_time:5126ms step_avg:33.72ms
step:153/2090 train_time:5159ms step_avg:33.72ms
step:154/2090 train_time:5192ms step_avg:33.71ms
step:155/2090 train_time:5225ms step_avg:33.71ms
step:156/2090 train_time:5258ms step_avg:33.70ms
step:157/2090 train_time:5291ms step_avg:33.70ms
step:158/2090 train_time:5324ms step_avg:33.69ms
step:159/2090 train_time:5357ms step_avg:33.69ms
step:160/2090 train_time:5390ms step_avg:33.69ms
step:161/2090 train_time:5424ms step_avg:33.69ms
step:162/2090 train_time:5457ms step_avg:33.68ms
step:163/2090 train_time:5490ms step_avg:33.68ms
step:164/2090 train_time:5523ms step_avg:33.68ms
step:165/2090 train_time:5557ms step_avg:33.68ms
step:166/2090 train_time:5589ms step_avg:33.67ms
step:167/2090 train_time:5623ms step_avg:33.67ms
step:168/2090 train_time:5656ms step_avg:33.67ms
step:169/2090 train_time:5689ms step_avg:33.67ms
step:170/2090 train_time:5722ms step_avg:33.66ms
step:171/2090 train_time:5756ms step_avg:33.66ms
step:172/2090 train_time:5788ms step_avg:33.65ms
step:173/2090 train_time:5822ms step_avg:33.65ms
step:174/2090 train_time:5855ms step_avg:33.65ms
step:175/2090 train_time:5888ms step_avg:33.65ms
step:176/2090 train_time:5921ms step_avg:33.64ms
step:177/2090 train_time:5954ms step_avg:33.64ms
step:178/2090 train_time:5987ms step_avg:33.63ms
step:179/2090 train_time:6021ms step_avg:33.64ms
step:180/2090 train_time:6054ms step_avg:33.63ms
step:181/2090 train_time:6087ms step_avg:33.63ms
step:182/2090 train_time:6120ms step_avg:33.62ms
step:183/2090 train_time:6153ms step_avg:33.62ms
step:184/2090 train_time:6186ms step_avg:33.62ms
step:185/2090 train_time:6219ms step_avg:33.62ms
step:186/2090 train_time:6252ms step_avg:33.61ms
step:187/2090 train_time:6286ms step_avg:33.61ms
step:188/2090 train_time:6319ms step_avg:33.61ms
step:189/2090 train_time:6352ms step_avg:33.61ms
step:190/2090 train_time:6384ms step_avg:33.60ms
step:191/2090 train_time:6418ms step_avg:33.60ms
step:192/2090 train_time:6451ms step_avg:33.60ms
step:193/2090 train_time:6484ms step_avg:33.60ms
step:194/2090 train_time:6517ms step_avg:33.59ms
step:195/2090 train_time:6550ms step_avg:33.59ms
step:196/2090 train_time:6583ms step_avg:33.58ms
step:197/2090 train_time:6616ms step_avg:33.59ms
step:198/2090 train_time:6649ms step_avg:33.58ms
step:199/2090 train_time:6683ms step_avg:33.58ms
step:200/2090 train_time:6715ms step_avg:33.58ms
step:201/2090 train_time:6749ms step_avg:33.58ms
step:202/2090 train_time:6782ms step_avg:33.57ms
step:203/2090 train_time:6815ms step_avg:33.57ms
step:204/2090 train_time:6848ms step_avg:33.57ms
step:205/2090 train_time:6881ms step_avg:33.57ms
step:206/2090 train_time:6914ms step_avg:33.56ms
step:207/2090 train_time:6948ms step_avg:33.56ms
step:208/2090 train_time:6980ms step_avg:33.56ms
step:209/2090 train_time:7014ms step_avg:33.56ms
step:210/2090 train_time:7046ms step_avg:33.55ms
step:211/2090 train_time:7080ms step_avg:33.55ms
step:212/2090 train_time:7112ms step_avg:33.55ms
step:213/2090 train_time:7146ms step_avg:33.55ms
step:214/2090 train_time:7178ms step_avg:33.54ms
step:215/2090 train_time:7211ms step_avg:33.54ms
step:216/2090 train_time:7244ms step_avg:33.54ms
step:217/2090 train_time:7277ms step_avg:33.54ms
step:218/2090 train_time:7310ms step_avg:33.53ms
step:219/2090 train_time:7344ms step_avg:33.53ms
step:220/2090 train_time:7377ms step_avg:33.53ms
step:221/2090 train_time:7410ms step_avg:33.53ms
step:222/2090 train_time:7443ms step_avg:33.53ms
step:223/2090 train_time:7476ms step_avg:33.52ms
step:224/2090 train_time:7508ms step_avg:33.52ms
step:225/2090 train_time:7542ms step_avg:33.52ms
step:226/2090 train_time:7575ms step_avg:33.52ms
step:227/2090 train_time:7608ms step_avg:33.52ms
step:228/2090 train_time:7641ms step_avg:33.51ms
step:229/2090 train_time:7674ms step_avg:33.51ms
step:230/2090 train_time:7707ms step_avg:33.51ms
step:231/2090 train_time:7740ms step_avg:33.51ms
step:232/2090 train_time:7773ms step_avg:33.50ms
step:233/2090 train_time:7806ms step_avg:33.50ms
step:234/2090 train_time:7839ms step_avg:33.50ms
step:235/2090 train_time:7873ms step_avg:33.50ms
step:236/2090 train_time:7905ms step_avg:33.50ms
step:237/2090 train_time:7939ms step_avg:33.50ms
step:238/2090 train_time:7971ms step_avg:33.49ms
step:239/2090 train_time:8005ms step_avg:33.49ms
step:240/2090 train_time:8038ms step_avg:33.49ms
step:241/2090 train_time:8071ms step_avg:33.49ms
step:242/2090 train_time:8104ms step_avg:33.49ms
step:243/2090 train_time:8137ms step_avg:33.49ms
step:244/2090 train_time:8170ms step_avg:33.48ms
step:245/2090 train_time:8204ms step_avg:33.49ms
step:246/2090 train_time:8237ms step_avg:33.48ms
step:247/2090 train_time:8270ms step_avg:33.48ms
step:248/2090 train_time:8303ms step_avg:33.48ms
step:249/2090 train_time:8336ms step_avg:33.48ms
step:250/2090 train_time:8369ms step_avg:33.48ms
step:250/2090 val_loss:4.2686 train_time:8405ms step_avg:33.62ms
step:251/2090 train_time:8426ms step_avg:33.57ms
step:252/2090 train_time:8445ms step_avg:33.51ms
step:253/2090 train_time:8472ms step_avg:33.48ms
step:254/2090 train_time:8505ms step_avg:33.49ms
step:255/2090 train_time:8541ms step_avg:33.50ms
step:256/2090 train_time:8575ms step_avg:33.50ms
step:257/2090 train_time:8609ms step_avg:33.50ms
step:258/2090 train_time:8642ms step_avg:33.50ms
step:259/2090 train_time:8676ms step_avg:33.50ms
step:260/2090 train_time:8709ms step_avg:33.49ms
step:261/2090 train_time:8742ms step_avg:33.49ms
step:262/2090 train_time:8774ms step_avg:33.49ms
step:263/2090 train_time:8807ms step_avg:33.49ms
step:264/2090 train_time:8840ms step_avg:33.48ms
step:265/2090 train_time:8874ms step_avg:33.49ms
step:266/2090 train_time:8906ms step_avg:33.48ms
step:267/2090 train_time:8939ms step_avg:33.48ms
step:268/2090 train_time:8972ms step_avg:33.48ms
step:269/2090 train_time:9005ms step_avg:33.48ms
step:270/2090 train_time:9038ms step_avg:33.47ms
step:271/2090 train_time:9071ms step_avg:33.47ms
step:272/2090 train_time:9104ms step_avg:33.47ms
step:273/2090 train_time:9137ms step_avg:33.47ms
step:274/2090 train_time:9169ms step_avg:33.46ms
step:275/2090 train_time:9202ms step_avg:33.46ms
step:276/2090 train_time:9235ms step_avg:33.46ms
step:277/2090 train_time:9268ms step_avg:33.46ms
step:278/2090 train_time:9301ms step_avg:33.46ms
step:279/2090 train_time:9334ms step_avg:33.46ms
step:280/2090 train_time:9367ms step_avg:33.45ms
step:281/2090 train_time:9400ms step_avg:33.45ms
step:282/2090 train_time:9433ms step_avg:33.45ms
step:283/2090 train_time:9466ms step_avg:33.45ms
step:284/2090 train_time:9499ms step_avg:33.45ms
step:285/2090 train_time:9533ms step_avg:33.45ms
step:286/2090 train_time:9566ms step_avg:33.45ms
step:287/2090 train_time:9600ms step_avg:33.45ms
step:288/2090 train_time:9633ms step_avg:33.45ms
step:289/2090 train_time:9667ms step_avg:33.45ms
step:290/2090 train_time:9700ms step_avg:33.45ms
step:291/2090 train_time:9734ms step_avg:33.45ms
step:292/2090 train_time:9766ms step_avg:33.45ms
step:293/2090 train_time:9800ms step_avg:33.45ms
step:294/2090 train_time:9833ms step_avg:33.44ms
step:295/2090 train_time:9866ms step_avg:33.45ms
step:296/2090 train_time:9899ms step_avg:33.44ms
step:297/2090 train_time:9933ms step_avg:33.44ms
step:298/2090 train_time:9966ms step_avg:33.44ms
step:299/2090 train_time:9999ms step_avg:33.44ms
step:300/2090 train_time:10032ms step_avg:33.44ms
step:301/2090 train_time:10065ms step_avg:33.44ms
step:302/2090 train_time:10098ms step_avg:33.44ms
step:303/2090 train_time:10131ms step_avg:33.43ms
step:304/2090 train_time:10163ms step_avg:33.43ms
step:305/2090 train_time:10197ms step_avg:33.43ms
step:306/2090 train_time:10229ms step_avg:33.43ms
step:307/2090 train_time:10263ms step_avg:33.43ms
step:308/2090 train_time:10296ms step_avg:33.43ms
step:309/2090 train_time:10329ms step_avg:33.43ms
step:310/2090 train_time:10362ms step_avg:33.42ms
step:311/2090 train_time:10395ms step_avg:33.43ms
step:312/2090 train_time:10428ms step_avg:33.42ms
step:313/2090 train_time:10461ms step_avg:33.42ms
step:314/2090 train_time:10494ms step_avg:33.42ms
step:315/2090 train_time:10528ms step_avg:33.42ms
step:316/2090 train_time:10560ms step_avg:33.42ms
step:317/2090 train_time:10594ms step_avg:33.42ms
step:318/2090 train_time:10627ms step_avg:33.42ms
step:319/2090 train_time:10660ms step_avg:33.42ms
step:320/2090 train_time:10693ms step_avg:33.42ms
step:321/2090 train_time:10726ms step_avg:33.42ms
step:322/2090 train_time:10759ms step_avg:33.41ms
step:323/2090 train_time:10793ms step_avg:33.41ms
step:324/2090 train_time:10825ms step_avg:33.41ms
step:325/2090 train_time:10859ms step_avg:33.41ms
step:326/2090 train_time:10892ms step_avg:33.41ms
step:327/2090 train_time:10926ms step_avg:33.41ms
step:328/2090 train_time:10958ms step_avg:33.41ms
step:329/2090 train_time:10992ms step_avg:33.41ms
step:330/2090 train_time:11025ms step_avg:33.41ms
step:331/2090 train_time:11058ms step_avg:33.41ms
step:332/2090 train_time:11091ms step_avg:33.41ms
step:333/2090 train_time:11124ms step_avg:33.41ms
step:334/2090 train_time:11157ms step_avg:33.40ms
step:335/2090 train_time:11190ms step_avg:33.40ms
step:336/2090 train_time:11223ms step_avg:33.40ms
step:337/2090 train_time:11256ms step_avg:33.40ms
step:338/2090 train_time:11289ms step_avg:33.40ms
step:339/2090 train_time:11323ms step_avg:33.40ms
step:340/2090 train_time:11355ms step_avg:33.40ms
step:341/2090 train_time:11389ms step_avg:33.40ms
step:342/2090 train_time:11421ms step_avg:33.40ms
step:343/2090 train_time:11455ms step_avg:33.40ms
step:344/2090 train_time:11488ms step_avg:33.39ms
step:345/2090 train_time:11521ms step_avg:33.39ms
step:346/2090 train_time:11553ms step_avg:33.39ms
step:347/2090 train_time:11587ms step_avg:33.39ms
step:348/2090 train_time:11620ms step_avg:33.39ms
step:349/2090 train_time:11653ms step_avg:33.39ms
step:350/2090 train_time:11686ms step_avg:33.39ms
step:351/2090 train_time:11719ms step_avg:33.39ms
step:352/2090 train_time:11752ms step_avg:33.39ms
step:353/2090 train_time:11785ms step_avg:33.39ms
step:354/2090 train_time:11818ms step_avg:33.38ms
step:355/2090 train_time:11852ms step_avg:33.39ms
step:356/2090 train_time:11884ms step_avg:33.38ms
step:357/2090 train_time:11918ms step_avg:33.38ms
step:358/2090 train_time:11951ms step_avg:33.38ms
step:359/2090 train_time:11985ms step_avg:33.38ms
step:360/2090 train_time:12017ms step_avg:33.38ms
step:361/2090 train_time:12051ms step_avg:33.38ms
step:362/2090 train_time:12083ms step_avg:33.38ms
step:363/2090 train_time:12117ms step_avg:33.38ms
step:364/2090 train_time:12150ms step_avg:33.38ms
step:365/2090 train_time:12183ms step_avg:33.38ms
step:366/2090 train_time:12216ms step_avg:33.38ms
step:367/2090 train_time:12249ms step_avg:33.38ms
step:368/2090 train_time:12282ms step_avg:33.37ms
step:369/2090 train_time:12315ms step_avg:33.37ms
step:370/2090 train_time:12348ms step_avg:33.37ms
step:371/2090 train_time:12381ms step_avg:33.37ms
step:372/2090 train_time:12414ms step_avg:33.37ms
step:373/2090 train_time:12447ms step_avg:33.37ms
step:374/2090 train_time:12480ms step_avg:33.37ms
step:375/2090 train_time:12513ms step_avg:33.37ms
step:376/2090 train_time:12546ms step_avg:33.37ms
step:377/2090 train_time:12580ms step_avg:33.37ms
step:378/2090 train_time:12612ms step_avg:33.37ms
step:379/2090 train_time:12646ms step_avg:33.37ms
step:380/2090 train_time:12679ms step_avg:33.36ms
step:381/2090 train_time:12712ms step_avg:33.36ms
step:382/2090 train_time:12744ms step_avg:33.36ms
step:383/2090 train_time:12778ms step_avg:33.36ms
step:384/2090 train_time:12811ms step_avg:33.36ms
step:385/2090 train_time:12844ms step_avg:33.36ms
step:386/2090 train_time:12876ms step_avg:33.36ms
step:387/2090 train_time:12910ms step_avg:33.36ms
step:388/2090 train_time:12943ms step_avg:33.36ms
step:389/2090 train_time:12977ms step_avg:33.36ms
step:390/2090 train_time:13009ms step_avg:33.36ms
step:391/2090 train_time:13043ms step_avg:33.36ms
step:392/2090 train_time:13076ms step_avg:33.36ms
step:393/2090 train_time:13109ms step_avg:33.36ms
step:394/2090 train_time:13142ms step_avg:33.36ms
step:395/2090 train_time:13176ms step_avg:33.36ms
step:396/2090 train_time:13208ms step_avg:33.35ms
step:397/2090 train_time:13241ms step_avg:33.35ms
step:398/2090 train_time:13274ms step_avg:33.35ms
step:399/2090 train_time:13307ms step_avg:33.35ms
step:400/2090 train_time:13340ms step_avg:33.35ms
step:401/2090 train_time:13374ms step_avg:33.35ms
step:402/2090 train_time:13406ms step_avg:33.35ms
step:403/2090 train_time:13440ms step_avg:33.35ms
step:404/2090 train_time:13472ms step_avg:33.35ms
step:405/2090 train_time:13506ms step_avg:33.35ms
step:406/2090 train_time:13539ms step_avg:33.35ms
step:407/2090 train_time:13572ms step_avg:33.35ms
step:408/2090 train_time:13605ms step_avg:33.35ms
step:409/2090 train_time:13639ms step_avg:33.35ms
step:410/2090 train_time:13672ms step_avg:33.35ms
step:411/2090 train_time:13705ms step_avg:33.35ms
step:412/2090 train_time:13738ms step_avg:33.35ms
step:413/2090 train_time:13772ms step_avg:33.35ms
step:414/2090 train_time:13805ms step_avg:33.34ms
step:415/2090 train_time:13838ms step_avg:33.35ms
step:416/2090 train_time:13871ms step_avg:33.34ms
step:417/2090 train_time:13904ms step_avg:33.34ms
step:418/2090 train_time:13937ms step_avg:33.34ms
step:419/2090 train_time:13970ms step_avg:33.34ms
step:420/2090 train_time:14003ms step_avg:33.34ms
step:421/2090 train_time:14036ms step_avg:33.34ms
step:422/2090 train_time:14069ms step_avg:33.34ms
step:423/2090 train_time:14103ms step_avg:33.34ms
step:424/2090 train_time:14135ms step_avg:33.34ms
step:425/2090 train_time:14169ms step_avg:33.34ms
step:426/2090 train_time:14202ms step_avg:33.34ms
step:427/2090 train_time:14235ms step_avg:33.34ms
step:428/2090 train_time:14268ms step_avg:33.34ms
step:429/2090 train_time:14301ms step_avg:33.34ms
step:430/2090 train_time:14334ms step_avg:33.34ms
step:431/2090 train_time:14367ms step_avg:33.34ms
step:432/2090 train_time:14400ms step_avg:33.33ms
step:433/2090 train_time:14433ms step_avg:33.33ms
step:434/2090 train_time:14466ms step_avg:33.33ms
step:435/2090 train_time:14499ms step_avg:33.33ms
step:436/2090 train_time:14532ms step_avg:33.33ms
step:437/2090 train_time:14565ms step_avg:33.33ms
step:438/2090 train_time:14598ms step_avg:33.33ms
step:439/2090 train_time:14631ms step_avg:33.33ms
step:440/2090 train_time:14664ms step_avg:33.33ms
step:441/2090 train_time:14697ms step_avg:33.33ms
step:442/2090 train_time:14730ms step_avg:33.33ms
step:443/2090 train_time:14763ms step_avg:33.33ms
step:444/2090 train_time:14796ms step_avg:33.32ms
step:445/2090 train_time:14829ms step_avg:33.32ms
step:446/2090 train_time:14862ms step_avg:33.32ms
step:447/2090 train_time:14896ms step_avg:33.32ms
step:448/2090 train_time:14928ms step_avg:33.32ms
step:449/2090 train_time:14962ms step_avg:33.32ms
step:450/2090 train_time:14994ms step_avg:33.32ms
step:451/2090 train_time:15028ms step_avg:33.32ms
step:452/2090 train_time:15060ms step_avg:33.32ms
step:453/2090 train_time:15094ms step_avg:33.32ms
step:454/2090 train_time:15127ms step_avg:33.32ms
step:455/2090 train_time:15160ms step_avg:33.32ms
step:456/2090 train_time:15193ms step_avg:33.32ms
step:457/2090 train_time:15226ms step_avg:33.32ms
step:458/2090 train_time:15259ms step_avg:33.32ms
step:459/2090 train_time:15293ms step_avg:33.32ms
step:460/2090 train_time:15325ms step_avg:33.32ms
step:461/2090 train_time:15359ms step_avg:33.32ms
step:462/2090 train_time:15392ms step_avg:33.32ms
step:463/2090 train_time:15425ms step_avg:33.32ms
step:464/2090 train_time:15458ms step_avg:33.31ms
step:465/2090 train_time:15492ms step_avg:33.32ms
step:466/2090 train_time:15524ms step_avg:33.31ms
step:467/2090 train_time:15558ms step_avg:33.31ms
step:468/2090 train_time:15591ms step_avg:33.31ms
step:469/2090 train_time:15624ms step_avg:33.31ms
step:470/2090 train_time:15657ms step_avg:33.31ms
step:471/2090 train_time:15690ms step_avg:33.31ms
step:472/2090 train_time:15723ms step_avg:33.31ms
step:473/2090 train_time:15756ms step_avg:33.31ms
step:474/2090 train_time:15789ms step_avg:33.31ms
step:475/2090 train_time:15822ms step_avg:33.31ms
step:476/2090 train_time:15855ms step_avg:33.31ms
step:477/2090 train_time:15889ms step_avg:33.31ms
step:478/2090 train_time:15921ms step_avg:33.31ms
step:479/2090 train_time:15955ms step_avg:33.31ms
step:480/2090 train_time:15988ms step_avg:33.31ms
step:481/2090 train_time:16021ms step_avg:33.31ms
step:482/2090 train_time:16054ms step_avg:33.31ms
step:483/2090 train_time:16087ms step_avg:33.31ms
step:484/2090 train_time:16120ms step_avg:33.31ms
step:485/2090 train_time:16153ms step_avg:33.31ms
step:486/2090 train_time:16186ms step_avg:33.30ms
step:487/2090 train_time:16220ms step_avg:33.31ms
step:488/2090 train_time:16252ms step_avg:33.30ms
step:489/2090 train_time:16286ms step_avg:33.30ms
step:490/2090 train_time:16318ms step_avg:33.30ms
step:491/2090 train_time:16352ms step_avg:33.30ms
step:492/2090 train_time:16385ms step_avg:33.30ms
step:493/2090 train_time:16418ms step_avg:33.30ms
step:494/2090 train_time:16451ms step_avg:33.30ms
step:495/2090 train_time:16484ms step_avg:33.30ms
step:496/2090 train_time:16517ms step_avg:33.30ms
step:497/2090 train_time:16551ms step_avg:33.30ms
step:498/2090 train_time:16583ms step_avg:33.30ms
step:499/2090 train_time:16617ms step_avg:33.30ms
step:500/2090 train_time:16650ms step_avg:33.30ms
step:500/2090 val_loss:4.0039 train_time:16686ms step_avg:33.37ms
step:501/2090 train_time:16706ms step_avg:33.34ms
step:502/2090 train_time:16725ms step_avg:33.32ms
step:503/2090 train_time:16755ms step_avg:33.31ms
step:504/2090 train_time:16789ms step_avg:33.31ms
step:505/2090 train_time:16825ms step_avg:33.32ms
step:506/2090 train_time:16859ms step_avg:33.32ms
step:507/2090 train_time:16894ms step_avg:33.32ms
step:508/2090 train_time:16926ms step_avg:33.32ms
step:509/2090 train_time:16960ms step_avg:33.32ms
step:510/2090 train_time:16993ms step_avg:33.32ms
step:511/2090 train_time:17026ms step_avg:33.32ms
step:512/2090 train_time:17059ms step_avg:33.32ms
step:513/2090 train_time:17092ms step_avg:33.32ms
step:514/2090 train_time:17125ms step_avg:33.32ms
step:515/2090 train_time:17158ms step_avg:33.32ms
step:516/2090 train_time:17191ms step_avg:33.32ms
step:517/2090 train_time:17224ms step_avg:33.31ms
step:518/2090 train_time:17257ms step_avg:33.31ms
step:519/2090 train_time:17289ms step_avg:33.31ms
step:520/2090 train_time:17322ms step_avg:33.31ms
step:521/2090 train_time:17355ms step_avg:33.31ms
step:522/2090 train_time:17388ms step_avg:33.31ms
step:523/2090 train_time:17421ms step_avg:33.31ms
step:524/2090 train_time:17454ms step_avg:33.31ms
step:525/2090 train_time:17487ms step_avg:33.31ms
step:526/2090 train_time:17519ms step_avg:33.31ms
step:527/2090 train_time:17553ms step_avg:33.31ms
step:528/2090 train_time:17585ms step_avg:33.31ms
step:529/2090 train_time:17619ms step_avg:33.31ms
step:530/2090 train_time:17651ms step_avg:33.30ms
step:531/2090 train_time:17684ms step_avg:33.30ms
step:532/2090 train_time:17717ms step_avg:33.30ms
step:533/2090 train_time:17751ms step_avg:33.30ms
step:534/2090 train_time:17784ms step_avg:33.30ms
step:535/2090 train_time:17818ms step_avg:33.30ms
step:536/2090 train_time:17851ms step_avg:33.30ms
step:537/2090 train_time:17885ms step_avg:33.31ms
step:538/2090 train_time:17918ms step_avg:33.30ms
step:539/2090 train_time:17952ms step_avg:33.31ms
step:540/2090 train_time:17985ms step_avg:33.30ms
step:541/2090 train_time:18019ms step_avg:33.31ms
step:542/2090 train_time:18051ms step_avg:33.31ms
step:543/2090 train_time:18085ms step_avg:33.31ms
step:544/2090 train_time:18117ms step_avg:33.30ms
step:545/2090 train_time:18151ms step_avg:33.30ms
step:546/2090 train_time:18183ms step_avg:33.30ms
step:547/2090 train_time:18217ms step_avg:33.30ms
step:548/2090 train_time:18249ms step_avg:33.30ms
step:549/2090 train_time:18283ms step_avg:33.30ms
step:550/2090 train_time:18315ms step_avg:33.30ms
step:551/2090 train_time:18349ms step_avg:33.30ms
step:552/2090 train_time:18382ms step_avg:33.30ms
step:553/2090 train_time:18414ms step_avg:33.30ms
step:554/2090 train_time:18447ms step_avg:33.30ms
step:555/2090 train_time:18480ms step_avg:33.30ms
step:556/2090 train_time:18513ms step_avg:33.30ms
step:557/2090 train_time:18547ms step_avg:33.30ms
step:558/2090 train_time:18579ms step_avg:33.30ms
step:559/2090 train_time:18612ms step_avg:33.30ms
step:560/2090 train_time:18645ms step_avg:33.29ms
step:561/2090 train_time:18678ms step_avg:33.29ms
step:562/2090 train_time:18711ms step_avg:33.29ms
step:563/2090 train_time:18745ms step_avg:33.29ms
step:564/2090 train_time:18777ms step_avg:33.29ms
step:565/2090 train_time:18811ms step_avg:33.29ms
step:566/2090 train_time:18843ms step_avg:33.29ms
step:567/2090 train_time:18877ms step_avg:33.29ms
step:568/2090 train_time:18910ms step_avg:33.29ms
step:569/2090 train_time:18943ms step_avg:33.29ms
step:570/2090 train_time:18976ms step_avg:33.29ms
step:571/2090 train_time:19010ms step_avg:33.29ms
step:572/2090 train_time:19042ms step_avg:33.29ms
step:573/2090 train_time:19076ms step_avg:33.29ms
step:574/2090 train_time:19109ms step_avg:33.29ms
step:575/2090 train_time:19142ms step_avg:33.29ms
step:576/2090 train_time:19175ms step_avg:33.29ms
step:577/2090 train_time:19208ms step_avg:33.29ms
step:578/2090 train_time:19241ms step_avg:33.29ms
step:579/2090 train_time:19275ms step_avg:33.29ms
step:580/2090 train_time:19308ms step_avg:33.29ms
step:581/2090 train_time:19341ms step_avg:33.29ms
step:582/2090 train_time:19373ms step_avg:33.29ms
step:583/2090 train_time:19407ms step_avg:33.29ms
step:584/2090 train_time:19440ms step_avg:33.29ms
step:585/2090 train_time:19473ms step_avg:33.29ms
step:586/2090 train_time:19506ms step_avg:33.29ms
step:587/2090 train_time:19539ms step_avg:33.29ms
step:588/2090 train_time:19571ms step_avg:33.28ms
step:589/2090 train_time:19605ms step_avg:33.28ms
step:590/2090 train_time:19637ms step_avg:33.28ms
step:591/2090 train_time:19671ms step_avg:33.28ms
step:592/2090 train_time:19703ms step_avg:33.28ms
step:593/2090 train_time:19736ms step_avg:33.28ms
step:594/2090 train_time:19769ms step_avg:33.28ms
step:595/2090 train_time:19802ms step_avg:33.28ms
step:596/2090 train_time:19835ms step_avg:33.28ms
step:597/2090 train_time:19869ms step_avg:33.28ms
step:598/2090 train_time:19902ms step_avg:33.28ms
step:599/2090 train_time:19935ms step_avg:33.28ms
step:600/2090 train_time:19968ms step_avg:33.28ms
step:601/2090 train_time:20001ms step_avg:33.28ms
step:602/2090 train_time:20034ms step_avg:33.28ms
step:603/2090 train_time:20067ms step_avg:33.28ms
step:604/2090 train_time:20100ms step_avg:33.28ms
step:605/2090 train_time:20133ms step_avg:33.28ms
step:606/2090 train_time:20166ms step_avg:33.28ms
step:607/2090 train_time:20199ms step_avg:33.28ms
step:608/2090 train_time:20232ms step_avg:33.28ms
step:609/2090 train_time:20266ms step_avg:33.28ms
step:610/2090 train_time:20298ms step_avg:33.28ms
step:611/2090 train_time:20332ms step_avg:33.28ms
step:612/2090 train_time:20365ms step_avg:33.28ms
step:613/2090 train_time:20398ms step_avg:33.28ms
step:614/2090 train_time:20431ms step_avg:33.28ms
step:615/2090 train_time:20464ms step_avg:33.28ms
step:616/2090 train_time:20497ms step_avg:33.27ms
step:617/2090 train_time:20531ms step_avg:33.28ms
step:618/2090 train_time:20563ms step_avg:33.27ms
step:619/2090 train_time:20597ms step_avg:33.27ms
step:620/2090 train_time:20629ms step_avg:33.27ms
step:621/2090 train_time:20663ms step_avg:33.27ms
step:622/2090 train_time:20695ms step_avg:33.27ms
step:623/2090 train_time:20729ms step_avg:33.27ms
step:624/2090 train_time:20762ms step_avg:33.27ms
step:625/2090 train_time:20796ms step_avg:33.27ms
step:626/2090 train_time:20829ms step_avg:33.27ms
step:627/2090 train_time:20862ms step_avg:33.27ms
step:628/2090 train_time:20894ms step_avg:33.27ms
step:629/2090 train_time:20928ms step_avg:33.27ms
step:630/2090 train_time:20960ms step_avg:33.27ms
step:631/2090 train_time:20994ms step_avg:33.27ms
step:632/2090 train_time:21026ms step_avg:33.27ms
step:633/2090 train_time:21060ms step_avg:33.27ms
step:634/2090 train_time:21092ms step_avg:33.27ms
step:635/2090 train_time:21126ms step_avg:33.27ms
step:636/2090 train_time:21159ms step_avg:33.27ms
step:637/2090 train_time:21192ms step_avg:33.27ms
step:638/2090 train_time:21225ms step_avg:33.27ms
step:639/2090 train_time:21258ms step_avg:33.27ms
step:640/2090 train_time:21291ms step_avg:33.27ms
step:641/2090 train_time:21324ms step_avg:33.27ms
step:642/2090 train_time:21357ms step_avg:33.27ms
step:643/2090 train_time:21390ms step_avg:33.27ms
step:644/2090 train_time:21423ms step_avg:33.27ms
step:645/2090 train_time:21456ms step_avg:33.27ms
step:646/2090 train_time:21489ms step_avg:33.26ms
step:647/2090 train_time:21522ms step_avg:33.26ms
step:648/2090 train_time:21555ms step_avg:33.26ms
step:649/2090 train_time:21588ms step_avg:33.26ms
step:650/2090 train_time:21621ms step_avg:33.26ms
step:651/2090 train_time:21654ms step_avg:33.26ms
step:652/2090 train_time:21688ms step_avg:33.26ms
step:653/2090 train_time:21720ms step_avg:33.26ms
step:654/2090 train_time:21753ms step_avg:33.26ms
step:655/2090 train_time:21787ms step_avg:33.26ms
step:656/2090 train_time:21819ms step_avg:33.26ms
step:657/2090 train_time:21853ms step_avg:33.26ms
step:658/2090 train_time:21886ms step_avg:33.26ms
step:659/2090 train_time:21919ms step_avg:33.26ms
step:660/2090 train_time:21952ms step_avg:33.26ms
step:661/2090 train_time:21985ms step_avg:33.26ms
step:662/2090 train_time:22018ms step_avg:33.26ms
step:663/2090 train_time:22052ms step_avg:33.26ms
step:664/2090 train_time:22085ms step_avg:33.26ms
step:665/2090 train_time:22118ms step_avg:33.26ms
step:666/2090 train_time:22151ms step_avg:33.26ms
step:667/2090 train_time:22184ms step_avg:33.26ms
step:668/2090 train_time:22217ms step_avg:33.26ms
step:669/2090 train_time:22251ms step_avg:33.26ms
step:670/2090 train_time:22283ms step_avg:33.26ms
step:671/2090 train_time:22317ms step_avg:33.26ms
step:672/2090 train_time:22349ms step_avg:33.26ms
step:673/2090 train_time:22383ms step_avg:33.26ms
step:674/2090 train_time:22415ms step_avg:33.26ms
step:675/2090 train_time:22449ms step_avg:33.26ms
step:676/2090 train_time:22481ms step_avg:33.26ms
step:677/2090 train_time:22515ms step_avg:33.26ms
step:678/2090 train_time:22548ms step_avg:33.26ms
step:679/2090 train_time:22581ms step_avg:33.26ms
step:680/2090 train_time:22614ms step_avg:33.26ms
step:681/2090 train_time:22647ms step_avg:33.26ms
step:682/2090 train_time:22679ms step_avg:33.25ms
step:683/2090 train_time:22713ms step_avg:33.25ms
step:684/2090 train_time:22746ms step_avg:33.25ms
step:685/2090 train_time:22780ms step_avg:33.26ms
step:686/2090 train_time:22838ms step_avg:33.29ms
step:687/2090 train_time:22898ms step_avg:33.33ms
step:688/2090 train_time:22958ms step_avg:33.37ms
step:689/2090 train_time:23020ms step_avg:33.41ms
step:690/2090 train_time:23080ms step_avg:33.45ms
step:691/2090 train_time:23141ms step_avg:33.49ms
step:692/2090 train_time:23201ms step_avg:33.53ms
step:693/2090 train_time:23262ms step_avg:33.57ms
step:694/2090 train_time:23321ms step_avg:33.60ms
step:695/2090 train_time:23382ms step_avg:33.64ms
step:696/2090 train_time:23442ms step_avg:33.68ms
step:697/2090 train_time:23502ms step_avg:33.72ms
step:698/2090 train_time:23561ms step_avg:33.76ms
step:699/2090 train_time:23622ms step_avg:33.79ms
step:700/2090 train_time:23681ms step_avg:33.83ms
step:701/2090 train_time:23741ms step_avg:33.87ms
step:702/2090 train_time:23800ms step_avg:33.90ms
step:703/2090 train_time:23860ms step_avg:33.94ms
step:704/2090 train_time:23920ms step_avg:33.98ms
step:705/2090 train_time:23981ms step_avg:34.02ms
step:706/2090 train_time:24040ms step_avg:34.05ms
step:707/2090 train_time:24101ms step_avg:34.09ms
step:708/2090 train_time:24162ms step_avg:34.13ms
step:709/2090 train_time:24222ms step_avg:34.16ms
step:710/2090 train_time:24281ms step_avg:34.20ms
step:711/2090 train_time:24342ms step_avg:34.24ms
step:712/2090 train_time:24402ms step_avg:34.27ms
step:713/2090 train_time:24463ms step_avg:34.31ms
step:714/2090 train_time:24522ms step_avg:34.34ms
step:715/2090 train_time:24583ms step_avg:34.38ms
step:716/2090 train_time:24643ms step_avg:34.42ms
step:717/2090 train_time:24703ms step_avg:34.45ms
step:718/2090 train_time:24761ms step_avg:34.49ms
step:719/2090 train_time:24822ms step_avg:34.52ms
step:720/2090 train_time:24881ms step_avg:34.56ms
step:721/2090 train_time:24942ms step_avg:34.59ms
step:722/2090 train_time:25001ms step_avg:34.63ms
step:723/2090 train_time:25062ms step_avg:34.66ms
step:724/2090 train_time:25122ms step_avg:34.70ms
step:725/2090 train_time:25183ms step_avg:34.73ms
step:726/2090 train_time:25242ms step_avg:34.77ms
step:727/2090 train_time:25303ms step_avg:34.80ms
step:728/2090 train_time:25362ms step_avg:34.84ms
step:729/2090 train_time:25422ms step_avg:34.87ms
step:730/2090 train_time:25482ms step_avg:34.91ms
step:731/2090 train_time:25542ms step_avg:34.94ms
step:732/2090 train_time:25601ms step_avg:34.97ms
step:733/2090 train_time:25662ms step_avg:35.01ms
step:734/2090 train_time:25722ms step_avg:35.04ms
step:735/2090 train_time:25782ms step_avg:35.08ms
step:736/2090 train_time:25841ms step_avg:35.11ms
step:737/2090 train_time:25901ms step_avg:35.14ms
step:738/2090 train_time:25960ms step_avg:35.18ms
step:739/2090 train_time:26022ms step_avg:35.21ms
step:740/2090 train_time:26081ms step_avg:35.24ms
step:741/2090 train_time:26141ms step_avg:35.28ms
step:742/2090 train_time:26200ms step_avg:35.31ms
step:743/2090 train_time:26261ms step_avg:35.34ms
step:744/2090 train_time:26321ms step_avg:35.38ms
step:745/2090 train_time:26382ms step_avg:35.41ms
step:746/2090 train_time:26441ms step_avg:35.44ms
step:747/2090 train_time:26501ms step_avg:35.48ms
step:748/2090 train_time:26561ms step_avg:35.51ms
step:749/2090 train_time:26622ms step_avg:35.54ms
step:750/2090 train_time:26681ms step_avg:35.58ms
step:750/2090 val_loss:3.8519 train_time:26745ms step_avg:35.66ms
step:751/2090 train_time:26765ms step_avg:35.64ms
step:752/2090 train_time:26804ms step_avg:35.64ms
step:753/2090 train_time:26868ms step_avg:35.68ms
step:754/2090 train_time:26931ms step_avg:35.72ms
step:755/2090 train_time:26991ms step_avg:35.75ms
step:756/2090 train_time:27051ms step_avg:35.78ms
step:757/2090 train_time:27111ms step_avg:35.81ms
step:758/2090 train_time:27169ms step_avg:35.84ms
step:759/2090 train_time:27229ms step_avg:35.88ms
step:760/2090 train_time:27287ms step_avg:35.90ms
step:761/2090 train_time:27347ms step_avg:35.94ms
step:762/2090 train_time:27405ms step_avg:35.97ms
step:763/2090 train_time:27465ms step_avg:36.00ms
step:764/2090 train_time:27524ms step_avg:36.03ms
step:765/2090 train_time:27583ms step_avg:36.06ms
step:766/2090 train_time:27641ms step_avg:36.09ms
step:767/2090 train_time:27702ms step_avg:36.12ms
step:768/2090 train_time:27762ms step_avg:36.15ms
step:769/2090 train_time:27824ms step_avg:36.18ms
step:770/2090 train_time:27886ms step_avg:36.22ms
step:771/2090 train_time:27948ms step_avg:36.25ms
step:772/2090 train_time:28008ms step_avg:36.28ms
step:773/2090 train_time:28068ms step_avg:36.31ms
step:774/2090 train_time:28127ms step_avg:36.34ms
step:775/2090 train_time:28187ms step_avg:36.37ms
step:776/2090 train_time:28246ms step_avg:36.40ms
step:777/2090 train_time:28305ms step_avg:36.43ms
step:778/2090 train_time:28364ms step_avg:36.46ms
step:779/2090 train_time:28423ms step_avg:36.49ms
step:780/2090 train_time:28482ms step_avg:36.52ms
step:781/2090 train_time:28542ms step_avg:36.54ms
step:782/2090 train_time:28600ms step_avg:36.57ms
step:783/2090 train_time:28661ms step_avg:36.60ms
step:784/2090 train_time:28720ms step_avg:36.63ms
step:785/2090 train_time:28782ms step_avg:36.66ms
step:786/2090 train_time:28842ms step_avg:36.69ms
step:787/2090 train_time:28904ms step_avg:36.73ms
step:788/2090 train_time:28964ms step_avg:36.76ms
step:789/2090 train_time:29026ms step_avg:36.79ms
step:790/2090 train_time:29086ms step_avg:36.82ms
step:791/2090 train_time:29147ms step_avg:36.85ms
step:792/2090 train_time:29206ms step_avg:36.88ms
step:793/2090 train_time:29266ms step_avg:36.90ms
step:794/2090 train_time:29324ms step_avg:36.93ms
step:795/2090 train_time:29384ms step_avg:36.96ms
step:796/2090 train_time:29442ms step_avg:36.99ms
step:797/2090 train_time:29502ms step_avg:37.02ms
step:798/2090 train_time:29561ms step_avg:37.04ms
step:799/2090 train_time:29621ms step_avg:37.07ms
step:800/2090 train_time:29681ms step_avg:37.10ms
step:801/2090 train_time:29741ms step_avg:37.13ms
step:802/2090 train_time:29801ms step_avg:37.16ms
step:803/2090 train_time:29863ms step_avg:37.19ms
step:804/2090 train_time:29923ms step_avg:37.22ms
step:805/2090 train_time:29984ms step_avg:37.25ms
step:806/2090 train_time:30045ms step_avg:37.28ms
step:807/2090 train_time:30105ms step_avg:37.31ms
step:808/2090 train_time:30165ms step_avg:37.33ms
step:809/2090 train_time:30224ms step_avg:37.36ms
step:810/2090 train_time:30284ms step_avg:37.39ms
step:811/2090 train_time:30344ms step_avg:37.42ms
step:812/2090 train_time:30402ms step_avg:37.44ms
step:813/2090 train_time:30463ms step_avg:37.47ms
step:814/2090 train_time:30521ms step_avg:37.50ms
step:815/2090 train_time:30581ms step_avg:37.52ms
step:816/2090 train_time:30640ms step_avg:37.55ms
step:817/2090 train_time:30701ms step_avg:37.58ms
step:818/2090 train_time:30761ms step_avg:37.60ms
step:819/2090 train_time:30821ms step_avg:37.63ms
step:820/2090 train_time:30881ms step_avg:37.66ms
step:821/2090 train_time:30942ms step_avg:37.69ms
step:822/2090 train_time:31002ms step_avg:37.72ms
step:823/2090 train_time:31063ms step_avg:37.74ms
step:824/2090 train_time:31123ms step_avg:37.77ms
step:825/2090 train_time:31184ms step_avg:37.80ms
step:826/2090 train_time:31243ms step_avg:37.82ms
step:827/2090 train_time:31303ms step_avg:37.85ms
step:828/2090 train_time:31362ms step_avg:37.88ms
step:829/2090 train_time:31422ms step_avg:37.90ms
step:830/2090 train_time:31483ms step_avg:37.93ms
step:831/2090 train_time:31543ms step_avg:37.96ms
step:832/2090 train_time:31602ms step_avg:37.98ms
step:833/2090 train_time:31662ms step_avg:38.01ms
step:834/2090 train_time:31721ms step_avg:38.04ms
step:835/2090 train_time:31782ms step_avg:38.06ms
step:836/2090 train_time:31841ms step_avg:38.09ms
step:837/2090 train_time:31903ms step_avg:38.12ms
step:838/2090 train_time:31963ms step_avg:38.14ms
step:839/2090 train_time:32023ms step_avg:38.17ms
step:840/2090 train_time:32084ms step_avg:38.19ms
step:841/2090 train_time:32144ms step_avg:38.22ms
step:842/2090 train_time:32204ms step_avg:38.25ms
step:843/2090 train_time:32264ms step_avg:38.27ms
step:844/2090 train_time:32323ms step_avg:38.30ms
step:845/2090 train_time:32384ms step_avg:38.32ms
step:846/2090 train_time:32443ms step_avg:38.35ms
step:847/2090 train_time:32503ms step_avg:38.37ms
step:848/2090 train_time:32562ms step_avg:38.40ms
step:849/2090 train_time:32622ms step_avg:38.42ms
step:850/2090 train_time:32681ms step_avg:38.45ms
step:851/2090 train_time:32741ms step_avg:38.47ms
step:852/2090 train_time:32801ms step_avg:38.50ms
step:853/2090 train_time:32862ms step_avg:38.52ms
step:854/2090 train_time:32922ms step_avg:38.55ms
step:855/2090 train_time:32982ms step_avg:38.58ms
step:856/2090 train_time:33042ms step_avg:38.60ms
step:857/2090 train_time:33103ms step_avg:38.63ms
step:858/2090 train_time:33163ms step_avg:38.65ms
step:859/2090 train_time:33223ms step_avg:38.68ms
step:860/2090 train_time:33282ms step_avg:38.70ms
step:861/2090 train_time:33343ms step_avg:38.73ms
step:862/2090 train_time:33401ms step_avg:38.75ms
step:863/2090 train_time:33462ms step_avg:38.77ms
step:864/2090 train_time:33521ms step_avg:38.80ms
step:865/2090 train_time:33582ms step_avg:38.82ms
step:866/2090 train_time:33640ms step_avg:38.85ms
step:867/2090 train_time:33700ms step_avg:38.87ms
step:868/2090 train_time:33761ms step_avg:38.89ms
step:869/2090 train_time:33822ms step_avg:38.92ms
step:870/2090 train_time:33881ms step_avg:38.94ms
step:871/2090 train_time:33942ms step_avg:38.97ms
step:872/2090 train_time:34002ms step_avg:38.99ms
step:873/2090 train_time:34064ms step_avg:39.02ms
step:874/2090 train_time:34123ms step_avg:39.04ms
step:875/2090 train_time:34184ms step_avg:39.07ms
step:876/2090 train_time:34243ms step_avg:39.09ms
step:877/2090 train_time:34304ms step_avg:39.11ms
step:878/2090 train_time:34363ms step_avg:39.14ms
step:879/2090 train_time:34422ms step_avg:39.16ms
step:880/2090 train_time:34482ms step_avg:39.18ms
step:881/2090 train_time:34543ms step_avg:39.21ms
step:882/2090 train_time:34601ms step_avg:39.23ms
step:883/2090 train_time:34662ms step_avg:39.25ms
step:884/2090 train_time:34721ms step_avg:39.28ms
step:885/2090 train_time:34782ms step_avg:39.30ms
step:886/2090 train_time:34841ms step_avg:39.32ms
step:887/2090 train_time:34902ms step_avg:39.35ms
step:888/2090 train_time:34963ms step_avg:39.37ms
step:889/2090 train_time:35024ms step_avg:39.40ms
step:890/2090 train_time:35083ms step_avg:39.42ms
step:891/2090 train_time:35143ms step_avg:39.44ms
step:892/2090 train_time:35203ms step_avg:39.47ms
step:893/2090 train_time:35264ms step_avg:39.49ms
step:894/2090 train_time:35323ms step_avg:39.51ms
step:895/2090 train_time:35383ms step_avg:39.53ms
step:896/2090 train_time:35442ms step_avg:39.56ms
step:897/2090 train_time:35503ms step_avg:39.58ms
step:898/2090 train_time:35562ms step_avg:39.60ms
step:899/2090 train_time:35622ms step_avg:39.62ms
step:900/2090 train_time:35682ms step_avg:39.65ms
step:901/2090 train_time:35742ms step_avg:39.67ms
step:902/2090 train_time:35801ms step_avg:39.69ms
step:903/2090 train_time:35862ms step_avg:39.71ms
step:904/2090 train_time:35921ms step_avg:39.74ms
step:905/2090 train_time:35982ms step_avg:39.76ms
step:906/2090 train_time:36041ms step_avg:39.78ms
step:907/2090 train_time:36102ms step_avg:39.80ms
step:908/2090 train_time:36162ms step_avg:39.83ms
step:909/2090 train_time:36223ms step_avg:39.85ms
step:910/2090 train_time:36283ms step_avg:39.87ms
step:911/2090 train_time:36343ms step_avg:39.89ms
step:912/2090 train_time:36402ms step_avg:39.91ms
step:913/2090 train_time:36463ms step_avg:39.94ms
step:914/2090 train_time:36522ms step_avg:39.96ms
step:915/2090 train_time:36583ms step_avg:39.98ms
step:916/2090 train_time:36642ms step_avg:40.00ms
step:917/2090 train_time:36702ms step_avg:40.02ms
step:918/2090 train_time:36761ms step_avg:40.04ms
step:919/2090 train_time:36822ms step_avg:40.07ms
step:920/2090 train_time:36882ms step_avg:40.09ms
step:921/2090 train_time:36942ms step_avg:40.11ms
step:922/2090 train_time:37001ms step_avg:40.13ms
step:923/2090 train_time:37061ms step_avg:40.15ms
step:924/2090 train_time:37121ms step_avg:40.17ms
step:925/2090 train_time:37181ms step_avg:40.20ms
step:926/2090 train_time:37241ms step_avg:40.22ms
step:927/2090 train_time:37301ms step_avg:40.24ms
step:928/2090 train_time:37360ms step_avg:40.26ms
step:929/2090 train_time:37421ms step_avg:40.28ms
step:930/2090 train_time:37480ms step_avg:40.30ms
step:931/2090 train_time:37540ms step_avg:40.32ms
step:932/2090 train_time:37600ms step_avg:40.34ms
step:933/2090 train_time:37660ms step_avg:40.36ms
step:934/2090 train_time:37719ms step_avg:40.38ms
step:935/2090 train_time:37780ms step_avg:40.41ms
step:936/2090 train_time:37839ms step_avg:40.43ms
step:937/2090 train_time:37900ms step_avg:40.45ms
step:938/2090 train_time:37960ms step_avg:40.47ms
step:939/2090 train_time:38022ms step_avg:40.49ms
step:940/2090 train_time:38082ms step_avg:40.51ms
step:941/2090 train_time:38142ms step_avg:40.53ms
step:942/2090 train_time:38202ms step_avg:40.55ms
step:943/2090 train_time:38263ms step_avg:40.58ms
step:944/2090 train_time:38323ms step_avg:40.60ms
step:945/2090 train_time:38383ms step_avg:40.62ms
step:946/2090 train_time:38442ms step_avg:40.64ms
step:947/2090 train_time:38503ms step_avg:40.66ms
step:948/2090 train_time:38563ms step_avg:40.68ms
step:949/2090 train_time:38623ms step_avg:40.70ms
step:950/2090 train_time:38683ms step_avg:40.72ms
step:951/2090 train_time:38743ms step_avg:40.74ms
step:952/2090 train_time:38802ms step_avg:40.76ms
step:953/2090 train_time:38863ms step_avg:40.78ms
step:954/2090 train_time:38922ms step_avg:40.80ms
step:955/2090 train_time:38983ms step_avg:40.82ms
step:956/2090 train_time:39042ms step_avg:40.84ms
step:957/2090 train_time:39104ms step_avg:40.86ms
step:958/2090 train_time:39163ms step_avg:40.88ms
step:959/2090 train_time:39224ms step_avg:40.90ms
step:960/2090 train_time:39284ms step_avg:40.92ms
step:961/2090 train_time:39344ms step_avg:40.94ms
step:962/2090 train_time:39404ms step_avg:40.96ms
step:963/2090 train_time:39465ms step_avg:40.98ms
step:964/2090 train_time:39523ms step_avg:41.00ms
step:965/2090 train_time:39584ms step_avg:41.02ms
step:966/2090 train_time:39642ms step_avg:41.04ms
step:967/2090 train_time:39703ms step_avg:41.06ms
step:968/2090 train_time:39762ms step_avg:41.08ms
step:969/2090 train_time:39823ms step_avg:41.10ms
step:970/2090 train_time:39882ms step_avg:41.12ms
step:971/2090 train_time:39943ms step_avg:41.14ms
step:972/2090 train_time:40002ms step_avg:41.15ms
step:973/2090 train_time:40063ms step_avg:41.17ms
step:974/2090 train_time:40122ms step_avg:41.19ms
step:975/2090 train_time:40183ms step_avg:41.21ms
step:976/2090 train_time:40242ms step_avg:41.23ms
step:977/2090 train_time:40303ms step_avg:41.25ms
step:978/2090 train_time:40362ms step_avg:41.27ms
step:979/2090 train_time:40423ms step_avg:41.29ms
step:980/2090 train_time:40483ms step_avg:41.31ms
step:981/2090 train_time:40543ms step_avg:41.33ms
step:982/2090 train_time:40602ms step_avg:41.35ms
step:983/2090 train_time:40663ms step_avg:41.37ms
step:984/2090 train_time:40722ms step_avg:41.38ms
step:985/2090 train_time:40782ms step_avg:41.40ms
step:986/2090 train_time:40842ms step_avg:41.42ms
step:987/2090 train_time:40902ms step_avg:41.44ms
step:988/2090 train_time:40962ms step_avg:41.46ms
step:989/2090 train_time:41023ms step_avg:41.48ms
step:990/2090 train_time:41082ms step_avg:41.50ms
step:991/2090 train_time:41143ms step_avg:41.52ms
step:992/2090 train_time:41202ms step_avg:41.53ms
step:993/2090 train_time:41263ms step_avg:41.55ms
step:994/2090 train_time:41322ms step_avg:41.57ms
step:995/2090 train_time:41383ms step_avg:41.59ms
step:996/2090 train_time:41442ms step_avg:41.61ms
step:997/2090 train_time:41503ms step_avg:41.63ms
step:998/2090 train_time:41563ms step_avg:41.65ms
step:999/2090 train_time:41624ms step_avg:41.67ms
step:1000/2090 train_time:41683ms step_avg:41.68ms
step:1000/2090 val_loss:3.7024 train_time:41745ms step_avg:41.75ms
step:1001/2090 train_time:41765ms step_avg:41.72ms
step:1002/2090 train_time:41805ms step_avg:41.72ms
step:1003/2090 train_time:41870ms step_avg:41.74ms
step:1004/2090 train_time:41933ms step_avg:41.77ms
step:1005/2090 train_time:41994ms step_avg:41.78ms
step:1006/2090 train_time:42053ms step_avg:41.80ms
step:1007/2090 train_time:42112ms step_avg:41.82ms
step:1008/2090 train_time:42171ms step_avg:41.84ms
step:1009/2090 train_time:42230ms step_avg:41.85ms
step:1010/2090 train_time:42288ms step_avg:41.87ms
step:1011/2090 train_time:42348ms step_avg:41.89ms
step:1012/2090 train_time:42407ms step_avg:41.90ms
step:1013/2090 train_time:42466ms step_avg:41.92ms
step:1014/2090 train_time:42524ms step_avg:41.94ms
step:1015/2090 train_time:42583ms step_avg:41.95ms
step:1016/2090 train_time:42642ms step_avg:41.97ms
step:1017/2090 train_time:42703ms step_avg:41.99ms
step:1018/2090 train_time:42763ms step_avg:42.01ms
step:1019/2090 train_time:42826ms step_avg:42.03ms
step:1020/2090 train_time:42888ms step_avg:42.05ms
step:1021/2090 train_time:42949ms step_avg:42.07ms
step:1022/2090 train_time:43008ms step_avg:42.08ms
step:1023/2090 train_time:43069ms step_avg:42.10ms
step:1024/2090 train_time:43128ms step_avg:42.12ms
step:1025/2090 train_time:43188ms step_avg:42.13ms
step:1026/2090 train_time:43247ms step_avg:42.15ms
step:1027/2090 train_time:43307ms step_avg:42.17ms
step:1028/2090 train_time:43365ms step_avg:42.18ms
step:1029/2090 train_time:43424ms step_avg:42.20ms
step:1030/2090 train_time:43483ms step_avg:42.22ms
step:1031/2090 train_time:43542ms step_avg:42.23ms
step:1032/2090 train_time:43601ms step_avg:42.25ms
step:1033/2090 train_time:43661ms step_avg:42.27ms
step:1034/2090 train_time:43721ms step_avg:42.28ms
step:1035/2090 train_time:43782ms step_avg:42.30ms
step:1036/2090 train_time:43843ms step_avg:42.32ms
step:1037/2090 train_time:43905ms step_avg:42.34ms
step:1038/2090 train_time:43965ms step_avg:42.36ms
step:1039/2090 train_time:44027ms step_avg:42.37ms
step:1040/2090 train_time:44086ms step_avg:42.39ms
step:1041/2090 train_time:44146ms step_avg:42.41ms
step:1042/2090 train_time:44205ms step_avg:42.42ms
step:1043/2090 train_time:44266ms step_avg:42.44ms
step:1044/2090 train_time:44325ms step_avg:42.46ms
step:1045/2090 train_time:44386ms step_avg:42.47ms
step:1046/2090 train_time:44444ms step_avg:42.49ms
step:1047/2090 train_time:44504ms step_avg:42.51ms
step:1048/2090 train_time:44562ms step_avg:42.52ms
step:1049/2090 train_time:44622ms step_avg:42.54ms
step:1050/2090 train_time:44681ms step_avg:42.55ms
step:1051/2090 train_time:44742ms step_avg:42.57ms
step:1052/2090 train_time:44802ms step_avg:42.59ms
step:1053/2090 train_time:44863ms step_avg:42.60ms
step:1054/2090 train_time:44924ms step_avg:42.62ms
step:1055/2090 train_time:44985ms step_avg:42.64ms
step:1056/2090 train_time:45044ms step_avg:42.66ms
step:1057/2090 train_time:45106ms step_avg:42.67ms
step:1058/2090 train_time:45164ms step_avg:42.69ms
step:1059/2090 train_time:45225ms step_avg:42.70ms
step:1060/2090 train_time:45284ms step_avg:42.72ms
step:1061/2090 train_time:45344ms step_avg:42.74ms
step:1062/2090 train_time:45403ms step_avg:42.75ms
step:1063/2090 train_time:45463ms step_avg:42.77ms
step:1064/2090 train_time:45522ms step_avg:42.78ms
step:1065/2090 train_time:45582ms step_avg:42.80ms
step:1066/2090 train_time:45641ms step_avg:42.81ms
step:1067/2090 train_time:45701ms step_avg:42.83ms
step:1068/2090 train_time:45761ms step_avg:42.85ms
step:1069/2090 train_time:45822ms step_avg:42.86ms
step:1070/2090 train_time:45882ms step_avg:42.88ms
step:1071/2090 train_time:45943ms step_avg:42.90ms
step:1072/2090 train_time:46003ms step_avg:42.91ms
step:1073/2090 train_time:46063ms step_avg:42.93ms
step:1074/2090 train_time:46123ms step_avg:42.95ms
step:1075/2090 train_time:46184ms step_avg:42.96ms
step:1076/2090 train_time:46243ms step_avg:42.98ms
step:1077/2090 train_time:46304ms step_avg:42.99ms
step:1078/2090 train_time:46363ms step_avg:43.01ms
step:1079/2090 train_time:46423ms step_avg:43.02ms
step:1080/2090 train_time:46482ms step_avg:43.04ms
step:1081/2090 train_time:46542ms step_avg:43.05ms
step:1082/2090 train_time:46600ms step_avg:43.07ms
step:1083/2090 train_time:46661ms step_avg:43.08ms
step:1084/2090 train_time:46720ms step_avg:43.10ms
step:1085/2090 train_time:46780ms step_avg:43.12ms
step:1086/2090 train_time:46840ms step_avg:43.13ms
step:1087/2090 train_time:46901ms step_avg:43.15ms
step:1088/2090 train_time:46961ms step_avg:43.16ms
step:1089/2090 train_time:47023ms step_avg:43.18ms
step:1090/2090 train_time:47083ms step_avg:43.20ms
step:1091/2090 train_time:47144ms step_avg:43.21ms
step:1092/2090 train_time:47204ms step_avg:43.23ms
step:1093/2090 train_time:47265ms step_avg:43.24ms
step:1094/2090 train_time:47324ms step_avg:43.26ms
step:1095/2090 train_time:47384ms step_avg:43.27ms
step:1096/2090 train_time:47443ms step_avg:43.29ms
step:1097/2090 train_time:47503ms step_avg:43.30ms
step:1098/2090 train_time:47562ms step_avg:43.32ms
step:1099/2090 train_time:47622ms step_avg:43.33ms
step:1100/2090 train_time:47681ms step_avg:43.35ms
step:1101/2090 train_time:47742ms step_avg:43.36ms
step:1102/2090 train_time:47801ms step_avg:43.38ms
step:1103/2090 train_time:47862ms step_avg:43.39ms
step:1104/2090 train_time:47922ms step_avg:43.41ms
step:1105/2090 train_time:47983ms step_avg:43.42ms
step:1106/2090 train_time:48042ms step_avg:43.44ms
step:1107/2090 train_time:48103ms step_avg:43.45ms
step:1108/2090 train_time:48163ms step_avg:43.47ms
step:1109/2090 train_time:48224ms step_avg:43.48ms
step:1110/2090 train_time:48283ms step_avg:43.50ms
step:1111/2090 train_time:48344ms step_avg:43.51ms
step:1112/2090 train_time:48403ms step_avg:43.53ms
step:1113/2090 train_time:48463ms step_avg:43.54ms
step:1114/2090 train_time:48523ms step_avg:43.56ms
step:1115/2090 train_time:48584ms step_avg:43.57ms
step:1116/2090 train_time:48642ms step_avg:43.59ms
step:1117/2090 train_time:48703ms step_avg:43.60ms
step:1118/2090 train_time:48762ms step_avg:43.61ms
step:1119/2090 train_time:48822ms step_avg:43.63ms
step:1120/2090 train_time:48882ms step_avg:43.64ms
step:1121/2090 train_time:48942ms step_avg:43.66ms
step:1122/2090 train_time:49002ms step_avg:43.67ms
step:1123/2090 train_time:49063ms step_avg:43.69ms
step:1124/2090 train_time:49123ms step_avg:43.70ms
step:1125/2090 train_time:49183ms step_avg:43.72ms
step:1126/2090 train_time:49243ms step_avg:43.73ms
step:1127/2090 train_time:49303ms step_avg:43.75ms
step:1128/2090 train_time:49362ms step_avg:43.76ms
step:1129/2090 train_time:49423ms step_avg:43.78ms
step:1130/2090 train_time:49481ms step_avg:43.79ms
step:1131/2090 train_time:49542ms step_avg:43.80ms
step:1132/2090 train_time:49601ms step_avg:43.82ms
step:1133/2090 train_time:49661ms step_avg:43.83ms
step:1134/2090 train_time:49721ms step_avg:43.85ms
step:1135/2090 train_time:49781ms step_avg:43.86ms
step:1136/2090 train_time:49840ms step_avg:43.87ms
step:1137/2090 train_time:49901ms step_avg:43.89ms
step:1138/2090 train_time:49960ms step_avg:43.90ms
step:1139/2090 train_time:50020ms step_avg:43.92ms
step:1140/2090 train_time:50079ms step_avg:43.93ms
step:1141/2090 train_time:50139ms step_avg:43.94ms
step:1142/2090 train_time:50199ms step_avg:43.96ms
step:1143/2090 train_time:50260ms step_avg:43.97ms
step:1144/2090 train_time:50319ms step_avg:43.99ms
step:1145/2090 train_time:50380ms step_avg:44.00ms
step:1146/2090 train_time:50439ms step_avg:44.01ms
step:1147/2090 train_time:50499ms step_avg:44.03ms
step:1148/2090 train_time:50558ms step_avg:44.04ms
step:1149/2090 train_time:50619ms step_avg:44.06ms
step:1150/2090 train_time:50679ms step_avg:44.07ms
step:1151/2090 train_time:50740ms step_avg:44.08ms
step:1152/2090 train_time:50799ms step_avg:44.10ms
step:1153/2090 train_time:50859ms step_avg:44.11ms
step:1154/2090 train_time:50919ms step_avg:44.12ms
step:1155/2090 train_time:50979ms step_avg:44.14ms
step:1156/2090 train_time:51039ms step_avg:44.15ms
step:1157/2090 train_time:51099ms step_avg:44.17ms
step:1158/2090 train_time:51158ms step_avg:44.18ms
step:1159/2090 train_time:51219ms step_avg:44.19ms
step:1160/2090 train_time:51278ms step_avg:44.21ms
step:1161/2090 train_time:51339ms step_avg:44.22ms
step:1162/2090 train_time:51399ms step_avg:44.23ms
step:1163/2090 train_time:51460ms step_avg:44.25ms
step:1164/2090 train_time:51520ms step_avg:44.26ms
step:1165/2090 train_time:51580ms step_avg:44.28ms
step:1166/2090 train_time:51640ms step_avg:44.29ms
step:1167/2090 train_time:51700ms step_avg:44.30ms
step:1168/2090 train_time:51759ms step_avg:44.31ms
step:1169/2090 train_time:51819ms step_avg:44.33ms
step:1170/2090 train_time:51878ms step_avg:44.34ms
step:1171/2090 train_time:51939ms step_avg:44.35ms
step:1172/2090 train_time:51999ms step_avg:44.37ms
step:1173/2090 train_time:52059ms step_avg:44.38ms
step:1174/2090 train_time:52119ms step_avg:44.39ms
step:1175/2090 train_time:52179ms step_avg:44.41ms
step:1176/2090 train_time:52239ms step_avg:44.42ms
step:1177/2090 train_time:52300ms step_avg:44.43ms
step:1178/2090 train_time:52359ms step_avg:44.45ms
step:1179/2090 train_time:52419ms step_avg:44.46ms
step:1180/2090 train_time:52478ms step_avg:44.47ms
step:1181/2090 train_time:52539ms step_avg:44.49ms
step:1182/2090 train_time:52599ms step_avg:44.50ms
step:1183/2090 train_time:52659ms step_avg:44.51ms
step:1184/2090 train_time:52718ms step_avg:44.53ms
step:1185/2090 train_time:52779ms step_avg:44.54ms
step:1186/2090 train_time:52837ms step_avg:44.55ms
step:1187/2090 train_time:52897ms step_avg:44.56ms
step:1188/2090 train_time:52957ms step_avg:44.58ms
step:1189/2090 train_time:53018ms step_avg:44.59ms
step:1190/2090 train_time:53077ms step_avg:44.60ms
step:1191/2090 train_time:53138ms step_avg:44.62ms
step:1192/2090 train_time:53198ms step_avg:44.63ms
step:1193/2090 train_time:53258ms step_avg:44.64ms
step:1194/2090 train_time:53318ms step_avg:44.65ms
step:1195/2090 train_time:53379ms step_avg:44.67ms
step:1196/2090 train_time:53439ms step_avg:44.68ms
step:1197/2090 train_time:53499ms step_avg:44.69ms
step:1198/2090 train_time:53558ms step_avg:44.71ms
step:1199/2090 train_time:53618ms step_avg:44.72ms
step:1200/2090 train_time:53677ms step_avg:44.73ms
step:1201/2090 train_time:53738ms step_avg:44.74ms
step:1202/2090 train_time:53797ms step_avg:44.76ms
step:1203/2090 train_time:53857ms step_avg:44.77ms
step:1204/2090 train_time:53917ms step_avg:44.78ms
step:1205/2090 train_time:53977ms step_avg:44.79ms
step:1206/2090 train_time:54036ms step_avg:44.81ms
step:1207/2090 train_time:54096ms step_avg:44.82ms
step:1208/2090 train_time:54156ms step_avg:44.83ms
step:1209/2090 train_time:54216ms step_avg:44.84ms
step:1210/2090 train_time:54276ms step_avg:44.86ms
step:1211/2090 train_time:54337ms step_avg:44.87ms
step:1212/2090 train_time:54397ms step_avg:44.88ms
step:1213/2090 train_time:54458ms step_avg:44.89ms
step:1214/2090 train_time:54517ms step_avg:44.91ms
step:1215/2090 train_time:54577ms step_avg:44.92ms
step:1216/2090 train_time:54636ms step_avg:44.93ms
step:1217/2090 train_time:54697ms step_avg:44.94ms
step:1218/2090 train_time:54757ms step_avg:44.96ms
step:1219/2090 train_time:54817ms step_avg:44.97ms
step:1220/2090 train_time:54876ms step_avg:44.98ms
step:1221/2090 train_time:54937ms step_avg:44.99ms
step:1222/2090 train_time:54997ms step_avg:45.01ms
step:1223/2090 train_time:55058ms step_avg:45.02ms
step:1224/2090 train_time:55117ms step_avg:45.03ms
step:1225/2090 train_time:55178ms step_avg:45.04ms
step:1226/2090 train_time:55237ms step_avg:45.05ms
step:1227/2090 train_time:55299ms step_avg:45.07ms
step:1228/2090 train_time:55358ms step_avg:45.08ms
step:1229/2090 train_time:55418ms step_avg:45.09ms
step:1230/2090 train_time:55477ms step_avg:45.10ms
step:1231/2090 train_time:55538ms step_avg:45.12ms
step:1232/2090 train_time:55598ms step_avg:45.13ms
step:1233/2090 train_time:55659ms step_avg:45.14ms
step:1234/2090 train_time:55718ms step_avg:45.15ms
step:1235/2090 train_time:55779ms step_avg:45.16ms
step:1236/2090 train_time:55838ms step_avg:45.18ms
step:1237/2090 train_time:55899ms step_avg:45.19ms
step:1238/2090 train_time:55958ms step_avg:45.20ms
step:1239/2090 train_time:56019ms step_avg:45.21ms
step:1240/2090 train_time:56078ms step_avg:45.22ms
step:1241/2090 train_time:56139ms step_avg:45.24ms
step:1242/2090 train_time:56198ms step_avg:45.25ms
step:1243/2090 train_time:56259ms step_avg:45.26ms
step:1244/2090 train_time:56318ms step_avg:45.27ms
step:1245/2090 train_time:56379ms step_avg:45.28ms
step:1246/2090 train_time:56438ms step_avg:45.30ms
step:1247/2090 train_time:56499ms step_avg:45.31ms
step:1248/2090 train_time:56558ms step_avg:45.32ms
step:1249/2090 train_time:56618ms step_avg:45.33ms
step:1250/2090 train_time:56677ms step_avg:45.34ms
step:1250/2090 val_loss:3.5830 train_time:56741ms step_avg:45.39ms
step:1251/2090 train_time:56761ms step_avg:45.37ms
step:1252/2090 train_time:56800ms step_avg:45.37ms
step:1253/2090 train_time:56865ms step_avg:45.38ms
step:1254/2090 train_time:56930ms step_avg:45.40ms
step:1255/2090 train_time:56990ms step_avg:45.41ms
step:1256/2090 train_time:57050ms step_avg:45.42ms
step:1257/2090 train_time:57110ms step_avg:45.43ms
step:1258/2090 train_time:57169ms step_avg:45.44ms
step:1259/2090 train_time:57229ms step_avg:45.46ms
step:1260/2090 train_time:57288ms step_avg:45.47ms
step:1261/2090 train_time:57349ms step_avg:45.48ms
step:1262/2090 train_time:57407ms step_avg:45.49ms
step:1263/2090 train_time:57467ms step_avg:45.50ms
step:1264/2090 train_time:57525ms step_avg:45.51ms
step:1265/2090 train_time:57585ms step_avg:45.52ms
step:1266/2090 train_time:57644ms step_avg:45.53ms
step:1267/2090 train_time:57705ms step_avg:45.54ms
step:1268/2090 train_time:57765ms step_avg:45.56ms
step:1269/2090 train_time:57828ms step_avg:45.57ms
step:1270/2090 train_time:57888ms step_avg:45.58ms
step:1271/2090 train_time:57950ms step_avg:45.59ms
step:1272/2090 train_time:58009ms step_avg:45.60ms
step:1273/2090 train_time:58070ms step_avg:45.62ms
step:1274/2090 train_time:58129ms step_avg:45.63ms
step:1275/2090 train_time:58189ms step_avg:45.64ms
step:1276/2090 train_time:58248ms step_avg:45.65ms
step:1277/2090 train_time:58309ms step_avg:45.66ms
step:1278/2090 train_time:58368ms step_avg:45.67ms
step:1279/2090 train_time:58428ms step_avg:45.68ms
step:1280/2090 train_time:58487ms step_avg:45.69ms
step:1281/2090 train_time:58546ms step_avg:45.70ms
step:1282/2090 train_time:58605ms step_avg:45.71ms
step:1283/2090 train_time:58666ms step_avg:45.73ms
step:1284/2090 train_time:58725ms step_avg:45.74ms
step:1285/2090 train_time:58787ms step_avg:45.75ms
step:1286/2090 train_time:58847ms step_avg:45.76ms
step:1287/2090 train_time:58908ms step_avg:45.77ms
step:1288/2090 train_time:58968ms step_avg:45.78ms
step:1289/2090 train_time:59029ms step_avg:45.79ms
step:1290/2090 train_time:59089ms step_avg:45.81ms
step:1291/2090 train_time:59150ms step_avg:45.82ms
step:1292/2090 train_time:59209ms step_avg:45.83ms
step:1293/2090 train_time:59269ms step_avg:45.84ms
step:1294/2090 train_time:59328ms step_avg:45.85ms
step:1295/2090 train_time:59388ms step_avg:45.86ms
step:1296/2090 train_time:59447ms step_avg:45.87ms
step:1297/2090 train_time:59507ms step_avg:45.88ms
step:1298/2090 train_time:59567ms step_avg:45.89ms
step:1299/2090 train_time:59627ms step_avg:45.90ms
step:1300/2090 train_time:59687ms step_avg:45.91ms
step:1301/2090 train_time:59748ms step_avg:45.92ms
step:1302/2090 train_time:59808ms step_avg:45.94ms
step:1303/2090 train_time:59869ms step_avg:45.95ms
step:1304/2090 train_time:59929ms step_avg:45.96ms
step:1305/2090 train_time:59990ms step_avg:45.97ms
step:1306/2090 train_time:60049ms step_avg:45.98ms
step:1307/2090 train_time:60110ms step_avg:45.99ms
step:1308/2090 train_time:60170ms step_avg:46.00ms
step:1309/2090 train_time:60230ms step_avg:46.01ms
step:1310/2090 train_time:60290ms step_avg:46.02ms
step:1311/2090 train_time:60350ms step_avg:46.03ms
step:1312/2090 train_time:60410ms step_avg:46.04ms
step:1313/2090 train_time:60470ms step_avg:46.05ms
step:1314/2090 train_time:60528ms step_avg:46.06ms
step:1315/2090 train_time:60588ms step_avg:46.07ms
step:1316/2090 train_time:60647ms step_avg:46.08ms
step:1317/2090 train_time:60708ms step_avg:46.10ms
step:1318/2090 train_time:60768ms step_avg:46.11ms
step:1319/2090 train_time:60829ms step_avg:46.12ms
step:1320/2090 train_time:60889ms step_avg:46.13ms
step:1321/2090 train_time:60950ms step_avg:46.14ms
step:1322/2090 train_time:61010ms step_avg:46.15ms
step:1323/2090 train_time:61070ms step_avg:46.16ms
step:1324/2090 train_time:61130ms step_avg:46.17ms
step:1325/2090 train_time:61190ms step_avg:46.18ms
step:1326/2090 train_time:61249ms step_avg:46.19ms
step:1327/2090 train_time:61309ms step_avg:46.20ms
step:1328/2090 train_time:61369ms step_avg:46.21ms
step:1329/2090 train_time:61429ms step_avg:46.22ms
step:1330/2090 train_time:61488ms step_avg:46.23ms
step:1331/2090 train_time:61549ms step_avg:46.24ms
step:1332/2090 train_time:61608ms step_avg:46.25ms
step:1333/2090 train_time:61668ms step_avg:46.26ms
step:1334/2090 train_time:61726ms step_avg:46.27ms
step:1335/2090 train_time:61787ms step_avg:46.28ms
step:1336/2090 train_time:61848ms step_avg:46.29ms
step:1337/2090 train_time:61909ms step_avg:46.30ms
step:1338/2090 train_time:61969ms step_avg:46.31ms
step:1339/2090 train_time:62029ms step_avg:46.33ms
step:1340/2090 train_time:62089ms step_avg:46.34ms
step:1341/2090 train_time:62150ms step_avg:46.35ms
step:1342/2090 train_time:62209ms step_avg:46.36ms
step:1343/2090 train_time:62270ms step_avg:46.37ms
step:1344/2090 train_time:62329ms step_avg:46.38ms
step:1345/2090 train_time:62389ms step_avg:46.39ms
step:1346/2090 train_time:62448ms step_avg:46.40ms
step:1347/2090 train_time:62509ms step_avg:46.41ms
step:1348/2090 train_time:62568ms step_avg:46.42ms
step:1349/2090 train_time:62629ms step_avg:46.43ms
step:1350/2090 train_time:62688ms step_avg:46.44ms
step:1351/2090 train_time:62748ms step_avg:46.45ms
step:1352/2090 train_time:62807ms step_avg:46.46ms
step:1353/2090 train_time:62868ms step_avg:46.47ms
step:1354/2090 train_time:62927ms step_avg:46.48ms
step:1355/2090 train_time:62988ms step_avg:46.49ms
step:1356/2090 train_time:63047ms step_avg:46.50ms
step:1357/2090 train_time:63108ms step_avg:46.51ms
step:1358/2090 train_time:63168ms step_avg:46.52ms
step:1359/2090 train_time:63228ms step_avg:46.53ms
step:1360/2090 train_time:63287ms step_avg:46.53ms
step:1361/2090 train_time:63347ms step_avg:46.54ms
step:1362/2090 train_time:63407ms step_avg:46.55ms
step:1363/2090 train_time:63467ms step_avg:46.56ms
step:1364/2090 train_time:63526ms step_avg:46.57ms
step:1365/2090 train_time:63587ms step_avg:46.58ms
step:1366/2090 train_time:63647ms step_avg:46.59ms
step:1367/2090 train_time:63707ms step_avg:46.60ms
step:1368/2090 train_time:63767ms step_avg:46.61ms
step:1369/2090 train_time:63855ms step_avg:46.64ms
step:1370/2090 train_time:63942ms step_avg:46.67ms
step:1371/2090 train_time:64030ms step_avg:46.70ms
step:1372/2090 train_time:64118ms step_avg:46.73ms
step:1373/2090 train_time:64205ms step_avg:46.76ms
step:1374/2090 train_time:64292ms step_avg:46.79ms
step:1375/2090 train_time:64381ms step_avg:46.82ms
step:1376/2090 train_time:64468ms step_avg:46.85ms
step:1377/2090 train_time:64555ms step_avg:46.88ms
step:1378/2090 train_time:64642ms step_avg:46.91ms
step:1379/2090 train_time:64729ms step_avg:46.94ms
step:1380/2090 train_time:64817ms step_avg:46.97ms
step:1381/2090 train_time:64905ms step_avg:47.00ms
step:1382/2090 train_time:64992ms step_avg:47.03ms
step:1383/2090 train_time:65081ms step_avg:47.06ms
step:1384/2090 train_time:65168ms step_avg:47.09ms
step:1385/2090 train_time:65256ms step_avg:47.12ms
step:1386/2090 train_time:65343ms step_avg:47.15ms
step:1387/2090 train_time:65431ms step_avg:47.17ms
step:1388/2090 train_time:65519ms step_avg:47.20ms
step:1389/2090 train_time:65606ms step_avg:47.23ms
step:1390/2090 train_time:65693ms step_avg:47.26ms
step:1391/2090 train_time:65781ms step_avg:47.29ms
step:1392/2090 train_time:65868ms step_avg:47.32ms
step:1393/2090 train_time:65956ms step_avg:47.35ms
step:1394/2090 train_time:66043ms step_avg:47.38ms
step:1395/2090 train_time:66131ms step_avg:47.41ms
step:1396/2090 train_time:66218ms step_avg:47.43ms
step:1397/2090 train_time:66306ms step_avg:47.46ms
step:1398/2090 train_time:66393ms step_avg:47.49ms
step:1399/2090 train_time:66482ms step_avg:47.52ms
step:1400/2090 train_time:66568ms step_avg:47.55ms
step:1401/2090 train_time:66657ms step_avg:47.58ms
step:1402/2090 train_time:66743ms step_avg:47.61ms
step:1403/2090 train_time:66831ms step_avg:47.63ms
step:1404/2090 train_time:66918ms step_avg:47.66ms
step:1405/2090 train_time:67006ms step_avg:47.69ms
step:1406/2090 train_time:67093ms step_avg:47.72ms
step:1407/2090 train_time:67182ms step_avg:47.75ms
step:1408/2090 train_time:67268ms step_avg:47.78ms
step:1409/2090 train_time:67357ms step_avg:47.80ms
step:1410/2090 train_time:67444ms step_avg:47.83ms
step:1411/2090 train_time:67531ms step_avg:47.86ms
step:1412/2090 train_time:67618ms step_avg:47.89ms
step:1413/2090 train_time:67705ms step_avg:47.92ms
step:1414/2090 train_time:67793ms step_avg:47.94ms
step:1415/2090 train_time:67881ms step_avg:47.97ms
step:1416/2090 train_time:67968ms step_avg:48.00ms
step:1417/2090 train_time:68056ms step_avg:48.03ms
step:1418/2090 train_time:68142ms step_avg:48.06ms
step:1419/2090 train_time:68230ms step_avg:48.08ms
step:1420/2090 train_time:68318ms step_avg:48.11ms
step:1421/2090 train_time:68406ms step_avg:48.14ms
step:1422/2090 train_time:68493ms step_avg:48.17ms
step:1423/2090 train_time:68582ms step_avg:48.20ms
step:1424/2090 train_time:68668ms step_avg:48.22ms
step:1425/2090 train_time:68756ms step_avg:48.25ms
step:1426/2090 train_time:68843ms step_avg:48.28ms
step:1427/2090 train_time:68931ms step_avg:48.31ms
step:1428/2090 train_time:69018ms step_avg:48.33ms
step:1429/2090 train_time:69105ms step_avg:48.36ms
step:1430/2090 train_time:69192ms step_avg:48.39ms
step:1431/2090 train_time:69280ms step_avg:48.41ms
step:1432/2090 train_time:69368ms step_avg:48.44ms
step:1433/2090 train_time:69456ms step_avg:48.47ms
step:1434/2090 train_time:69542ms step_avg:48.50ms
step:1435/2090 train_time:69630ms step_avg:48.52ms
step:1436/2090 train_time:69718ms step_avg:48.55ms
step:1437/2090 train_time:69806ms step_avg:48.58ms
step:1438/2090 train_time:69893ms step_avg:48.60ms
step:1439/2090 train_time:69982ms step_avg:48.63ms
step:1440/2090 train_time:70069ms step_avg:48.66ms
step:1441/2090 train_time:70157ms step_avg:48.69ms
step:1442/2090 train_time:70243ms step_avg:48.71ms
step:1443/2090 train_time:70331ms step_avg:48.74ms
step:1444/2090 train_time:70420ms step_avg:48.77ms
step:1445/2090 train_time:70507ms step_avg:48.79ms
step:1446/2090 train_time:70595ms step_avg:48.82ms
step:1447/2090 train_time:70682ms step_avg:48.85ms
step:1448/2090 train_time:70770ms step_avg:48.87ms
step:1449/2090 train_time:70858ms step_avg:48.90ms
step:1450/2090 train_time:70945ms step_avg:48.93ms
step:1451/2090 train_time:71033ms step_avg:48.95ms
step:1452/2090 train_time:71119ms step_avg:48.98ms
step:1453/2090 train_time:71207ms step_avg:49.01ms
step:1454/2090 train_time:71294ms step_avg:49.03ms
step:1455/2090 train_time:71383ms step_avg:49.06ms
step:1456/2090 train_time:71470ms step_avg:49.09ms
step:1457/2090 train_time:71559ms step_avg:49.11ms
step:1458/2090 train_time:71646ms step_avg:49.14ms
step:1459/2090 train_time:71733ms step_avg:49.17ms
step:1460/2090 train_time:71820ms step_avg:49.19ms
step:1461/2090 train_time:71908ms step_avg:49.22ms
step:1462/2090 train_time:71995ms step_avg:49.24ms
step:1463/2090 train_time:72084ms step_avg:49.27ms
step:1464/2090 train_time:72171ms step_avg:49.30ms
step:1465/2090 train_time:72260ms step_avg:49.32ms
step:1466/2090 train_time:72347ms step_avg:49.35ms
step:1467/2090 train_time:72434ms step_avg:49.38ms
step:1468/2090 train_time:72521ms step_avg:49.40ms
step:1469/2090 train_time:72609ms step_avg:49.43ms
step:1470/2090 train_time:72696ms step_avg:49.45ms
step:1471/2090 train_time:72784ms step_avg:49.48ms
step:1472/2090 train_time:72871ms step_avg:49.50ms
step:1473/2090 train_time:72959ms step_avg:49.53ms
step:1474/2090 train_time:73046ms step_avg:49.56ms
step:1475/2090 train_time:73134ms step_avg:49.58ms
step:1476/2090 train_time:73222ms step_avg:49.61ms
step:1477/2090 train_time:73310ms step_avg:49.63ms
step:1478/2090 train_time:73397ms step_avg:49.66ms
step:1479/2090 train_time:73485ms step_avg:49.69ms
step:1480/2090 train_time:73571ms step_avg:49.71ms
step:1481/2090 train_time:73659ms step_avg:49.74ms
step:1482/2090 train_time:73747ms step_avg:49.76ms
step:1483/2090 train_time:73835ms step_avg:49.79ms
step:1484/2090 train_time:73922ms step_avg:49.81ms
step:1485/2090 train_time:74010ms step_avg:49.84ms
step:1486/2090 train_time:74096ms step_avg:49.86ms
step:1487/2090 train_time:74185ms step_avg:49.89ms
step:1488/2090 train_time:74272ms step_avg:49.91ms
step:1489/2090 train_time:74360ms step_avg:49.94ms
step:1490/2090 train_time:74446ms step_avg:49.96ms
step:1491/2090 train_time:74534ms step_avg:49.99ms
step:1492/2090 train_time:74621ms step_avg:50.01ms
step:1493/2090 train_time:74709ms step_avg:50.04ms
step:1494/2090 train_time:74796ms step_avg:50.06ms
step:1495/2090 train_time:74883ms step_avg:50.09ms
step:1496/2090 train_time:74970ms step_avg:50.11ms
step:1497/2090 train_time:75058ms step_avg:50.14ms
step:1498/2090 train_time:75145ms step_avg:50.16ms
step:1499/2090 train_time:75233ms step_avg:50.19ms
step:1500/2090 train_time:75320ms step_avg:50.21ms
step:1500/2090 val_loss:3.4712 train_time:75409ms step_avg:50.27ms
step:1501/2090 train_time:75429ms step_avg:50.25ms
step:1502/2090 train_time:75500ms step_avg:50.27ms
step:1503/2090 train_time:75595ms step_avg:50.30ms
step:1504/2090 train_time:75682ms step_avg:50.32ms
step:1505/2090 train_time:75770ms step_avg:50.35ms
step:1506/2090 train_time:75856ms step_avg:50.37ms
step:1507/2090 train_time:75943ms step_avg:50.39ms
step:1508/2090 train_time:76029ms step_avg:50.42ms
step:1509/2090 train_time:76116ms step_avg:50.44ms
step:1510/2090 train_time:76201ms step_avg:50.46ms
step:1511/2090 train_time:76288ms step_avg:50.49ms
step:1512/2090 train_time:76377ms step_avg:50.51ms
step:1513/2090 train_time:76467ms step_avg:50.54ms
step:1514/2090 train_time:76557ms step_avg:50.57ms
step:1515/2090 train_time:76646ms step_avg:50.59ms
step:1516/2090 train_time:76734ms step_avg:50.62ms
step:1517/2090 train_time:76822ms step_avg:50.64ms
step:1518/2090 train_time:76908ms step_avg:50.66ms
step:1519/2090 train_time:76996ms step_avg:50.69ms
step:1520/2090 train_time:77081ms step_avg:50.71ms
step:1521/2090 train_time:77168ms step_avg:50.73ms
step:1522/2090 train_time:77254ms step_avg:50.76ms
step:1523/2090 train_time:77342ms step_avg:50.78ms
step:1524/2090 train_time:77431ms step_avg:50.81ms
step:1525/2090 train_time:77520ms step_avg:50.83ms
step:1526/2090 train_time:77608ms step_avg:50.86ms
step:1527/2090 train_time:77697ms step_avg:50.88ms
step:1528/2090 train_time:77784ms step_avg:50.91ms
step:1529/2090 train_time:77872ms step_avg:50.93ms
step:1530/2090 train_time:77959ms step_avg:50.95ms
step:1531/2090 train_time:78046ms step_avg:50.98ms
step:1532/2090 train_time:78133ms step_avg:51.00ms
step:1533/2090 train_time:78220ms step_avg:51.02ms
step:1534/2090 train_time:78307ms step_avg:51.05ms
step:1535/2090 train_time:78397ms step_avg:51.07ms
step:1536/2090 train_time:78484ms step_avg:51.10ms
step:1537/2090 train_time:78573ms step_avg:51.12ms
step:1538/2090 train_time:78661ms step_avg:51.14ms
step:1539/2090 train_time:78749ms step_avg:51.17ms
step:1540/2090 train_time:78836ms step_avg:51.19ms
step:1541/2090 train_time:78924ms step_avg:51.22ms
step:1542/2090 train_time:79010ms step_avg:51.24ms
step:1543/2090 train_time:79097ms step_avg:51.26ms
step:1544/2090 train_time:79185ms step_avg:51.29ms
step:1545/2090 train_time:79272ms step_avg:51.31ms
step:1546/2090 train_time:79359ms step_avg:51.33ms
step:1547/2090 train_time:79448ms step_avg:51.36ms
step:1548/2090 train_time:79535ms step_avg:51.38ms
step:1549/2090 train_time:79623ms step_avg:51.40ms
step:1550/2090 train_time:79711ms step_avg:51.43ms
step:1551/2090 train_time:79800ms step_avg:51.45ms
step:1552/2090 train_time:79887ms step_avg:51.47ms
step:1553/2090 train_time:79976ms step_avg:51.50ms
step:1554/2090 train_time:80062ms step_avg:51.52ms
step:1555/2090 train_time:80150ms step_avg:51.54ms
step:1556/2090 train_time:80237ms step_avg:51.57ms
step:1557/2090 train_time:80324ms step_avg:51.59ms
step:1558/2090 train_time:80412ms step_avg:51.61ms
step:1559/2090 train_time:80501ms step_avg:51.64ms
step:1560/2090 train_time:80588ms step_avg:51.66ms
step:1561/2090 train_time:80677ms step_avg:51.68ms
step:1562/2090 train_time:80763ms step_avg:51.71ms
step:1563/2090 train_time:80852ms step_avg:51.73ms
step:1564/2090 train_time:80939ms step_avg:51.75ms
step:1565/2090 train_time:81027ms step_avg:51.77ms
step:1566/2090 train_time:81113ms step_avg:51.80ms
step:1567/2090 train_time:81201ms step_avg:51.82ms
step:1568/2090 train_time:81288ms step_avg:51.84ms
step:1569/2090 train_time:81377ms step_avg:51.87ms
step:1570/2090 train_time:81464ms step_avg:51.89ms
step:1571/2090 train_time:81552ms step_avg:51.91ms
step:1572/2090 train_time:81639ms step_avg:51.93ms
step:1573/2090 train_time:81727ms step_avg:51.96ms
step:1574/2090 train_time:81814ms step_avg:51.98ms
step:1575/2090 train_time:81902ms step_avg:52.00ms
step:1576/2090 train_time:81989ms step_avg:52.02ms
step:1577/2090 train_time:82077ms step_avg:52.05ms
step:1578/2090 train_time:82163ms step_avg:52.07ms
step:1579/2090 train_time:82251ms step_avg:52.09ms
step:1580/2090 train_time:82338ms step_avg:52.11ms
step:1581/2090 train_time:82425ms step_avg:52.13ms
step:1582/2090 train_time:82512ms step_avg:52.16ms
step:1583/2090 train_time:82600ms step_avg:52.18ms
step:1584/2090 train_time:82689ms step_avg:52.20ms
step:1585/2090 train_time:82776ms step_avg:52.22ms
step:1586/2090 train_time:82863ms step_avg:52.25ms
step:1587/2090 train_time:82951ms step_avg:52.27ms
step:1588/2090 train_time:83038ms step_avg:52.29ms
step:1589/2090 train_time:83126ms step_avg:52.31ms
step:1590/2090 train_time:83213ms step_avg:52.34ms
step:1591/2090 train_time:83301ms step_avg:52.36ms
step:1592/2090 train_time:83388ms step_avg:52.38ms
step:1593/2090 train_time:83476ms step_avg:52.40ms
step:1594/2090 train_time:83563ms step_avg:52.42ms
step:1595/2090 train_time:83651ms step_avg:52.45ms
step:1596/2090 train_time:83738ms step_avg:52.47ms
step:1597/2090 train_time:83827ms step_avg:52.49ms
step:1598/2090 train_time:83914ms step_avg:52.51ms
step:1599/2090 train_time:84002ms step_avg:52.53ms
step:1600/2090 train_time:84089ms step_avg:52.56ms
step:1601/2090 train_time:84177ms step_avg:52.58ms
step:1602/2090 train_time:84264ms step_avg:52.60ms
step:1603/2090 train_time:84352ms step_avg:52.62ms
step:1604/2090 train_time:84439ms step_avg:52.64ms
step:1605/2090 train_time:84527ms step_avg:52.66ms
step:1606/2090 train_time:84615ms step_avg:52.69ms
step:1607/2090 train_time:84703ms step_avg:52.71ms
step:1608/2090 train_time:84790ms step_avg:52.73ms
step:1609/2090 train_time:84878ms step_avg:52.75ms
step:1610/2090 train_time:84965ms step_avg:52.77ms
step:1611/2090 train_time:85054ms step_avg:52.80ms
step:1612/2090 train_time:85141ms step_avg:52.82ms
step:1613/2090 train_time:85229ms step_avg:52.84ms
step:1614/2090 train_time:85316ms step_avg:52.86ms
step:1615/2090 train_time:85403ms step_avg:52.88ms
step:1616/2090 train_time:85490ms step_avg:52.90ms
step:1617/2090 train_time:85578ms step_avg:52.92ms
step:1618/2090 train_time:85667ms step_avg:52.95ms
step:1619/2090 train_time:85754ms step_avg:52.97ms
step:1620/2090 train_time:85840ms step_avg:52.99ms
step:1621/2090 train_time:85928ms step_avg:53.01ms
step:1622/2090 train_time:86016ms step_avg:53.03ms
step:1623/2090 train_time:86104ms step_avg:53.05ms
step:1624/2090 train_time:86191ms step_avg:53.07ms
step:1625/2090 train_time:86278ms step_avg:53.09ms
step:1626/2090 train_time:86364ms step_avg:53.11ms
step:1627/2090 train_time:86452ms step_avg:53.14ms
step:1628/2090 train_time:86539ms step_avg:53.16ms
step:1629/2090 train_time:86627ms step_avg:53.18ms
step:1630/2090 train_time:86714ms step_avg:53.20ms
step:1631/2090 train_time:86802ms step_avg:53.22ms
step:1632/2090 train_time:86890ms step_avg:53.24ms
step:1633/2090 train_time:86977ms step_avg:53.26ms
step:1634/2090 train_time:87064ms step_avg:53.28ms
step:1635/2090 train_time:87152ms step_avg:53.30ms
step:1636/2090 train_time:87239ms step_avg:53.32ms
step:1637/2090 train_time:87326ms step_avg:53.35ms
step:1638/2090 train_time:87414ms step_avg:53.37ms
step:1639/2090 train_time:87501ms step_avg:53.39ms
step:1640/2090 train_time:87589ms step_avg:53.41ms
step:1641/2090 train_time:87676ms step_avg:53.43ms
step:1642/2090 train_time:87762ms step_avg:53.45ms
step:1643/2090 train_time:87851ms step_avg:53.47ms
step:1644/2090 train_time:87938ms step_avg:53.49ms
step:1645/2090 train_time:88026ms step_avg:53.51ms
step:1646/2090 train_time:88112ms step_avg:53.53ms
step:1647/2090 train_time:88200ms step_avg:53.55ms
step:1648/2090 train_time:88287ms step_avg:53.57ms
step:1649/2090 train_time:88375ms step_avg:53.59ms
step:1650/2090 train_time:88462ms step_avg:53.61ms
step:1651/2090 train_time:88549ms step_avg:53.63ms
step:1652/2090 train_time:88637ms step_avg:53.65ms
step:1653/2090 train_time:88724ms step_avg:53.67ms
step:1654/2090 train_time:88811ms step_avg:53.69ms
step:1655/2090 train_time:88899ms step_avg:53.72ms
step:1656/2090 train_time:88987ms step_avg:53.74ms
step:1657/2090 train_time:89074ms step_avg:53.76ms
step:1658/2090 train_time:89161ms step_avg:53.78ms
step:1659/2090 train_time:89250ms step_avg:53.80ms
step:1660/2090 train_time:89336ms step_avg:53.82ms
step:1661/2090 train_time:89423ms step_avg:53.84ms
step:1662/2090 train_time:89510ms step_avg:53.86ms
step:1663/2090 train_time:89598ms step_avg:53.88ms
step:1664/2090 train_time:89685ms step_avg:53.90ms
step:1665/2090 train_time:89773ms step_avg:53.92ms
step:1666/2090 train_time:89860ms step_avg:53.94ms
step:1667/2090 train_time:89948ms step_avg:53.96ms
step:1668/2090 train_time:90034ms step_avg:53.98ms
step:1669/2090 train_time:90122ms step_avg:54.00ms
step:1670/2090 train_time:90209ms step_avg:54.02ms
step:1671/2090 train_time:90297ms step_avg:54.04ms
step:1672/2090 train_time:90384ms step_avg:54.06ms
step:1673/2090 train_time:90472ms step_avg:54.08ms
step:1674/2090 train_time:90560ms step_avg:54.10ms
step:1675/2090 train_time:90647ms step_avg:54.12ms
step:1676/2090 train_time:90734ms step_avg:54.14ms
step:1677/2090 train_time:90822ms step_avg:54.16ms
step:1678/2090 train_time:90909ms step_avg:54.18ms
step:1679/2090 train_time:90997ms step_avg:54.20ms
step:1680/2090 train_time:91084ms step_avg:54.22ms
step:1681/2090 train_time:91171ms step_avg:54.24ms
step:1682/2090 train_time:91258ms step_avg:54.26ms
step:1683/2090 train_time:91345ms step_avg:54.28ms
step:1684/2090 train_time:91432ms step_avg:54.29ms
step:1685/2090 train_time:91521ms step_avg:54.31ms
step:1686/2090 train_time:91608ms step_avg:54.33ms
step:1687/2090 train_time:91696ms step_avg:54.35ms
step:1688/2090 train_time:91782ms step_avg:54.37ms
step:1689/2090 train_time:91872ms step_avg:54.39ms
step:1690/2090 train_time:91958ms step_avg:54.41ms
step:1691/2090 train_time:92046ms step_avg:54.43ms
step:1692/2090 train_time:92133ms step_avg:54.45ms
step:1693/2090 train_time:92221ms step_avg:54.47ms
step:1694/2090 train_time:92308ms step_avg:54.49ms
step:1695/2090 train_time:92397ms step_avg:54.51ms
step:1696/2090 train_time:92484ms step_avg:54.53ms
step:1697/2090 train_time:92572ms step_avg:54.55ms
step:1698/2090 train_time:92659ms step_avg:54.57ms
step:1699/2090 train_time:92747ms step_avg:54.59ms
step:1700/2090 train_time:92835ms step_avg:54.61ms
step:1701/2090 train_time:92923ms step_avg:54.63ms
step:1702/2090 train_time:93010ms step_avg:54.65ms
step:1703/2090 train_time:93097ms step_avg:54.67ms
step:1704/2090 train_time:93184ms step_avg:54.69ms
step:1705/2090 train_time:93272ms step_avg:54.71ms
step:1706/2090 train_time:93359ms step_avg:54.72ms
step:1707/2090 train_time:93446ms step_avg:54.74ms
step:1708/2090 train_time:93533ms step_avg:54.76ms
step:1709/2090 train_time:93620ms step_avg:54.78ms
step:1710/2090 train_time:93708ms step_avg:54.80ms
step:1711/2090 train_time:93796ms step_avg:54.82ms
step:1712/2090 train_time:93883ms step_avg:54.84ms
step:1713/2090 train_time:93971ms step_avg:54.86ms
step:1714/2090 train_time:94058ms step_avg:54.88ms
step:1715/2090 train_time:94147ms step_avg:54.90ms
step:1716/2090 train_time:94234ms step_avg:54.91ms
step:1717/2090 train_time:94321ms step_avg:54.93ms
step:1718/2090 train_time:94408ms step_avg:54.95ms
step:1719/2090 train_time:94496ms step_avg:54.97ms
step:1720/2090 train_time:94582ms step_avg:54.99ms
step:1721/2090 train_time:94671ms step_avg:55.01ms
step:1722/2090 train_time:94757ms step_avg:55.03ms
step:1723/2090 train_time:94845ms step_avg:55.05ms
step:1724/2090 train_time:94932ms step_avg:55.06ms
step:1725/2090 train_time:95020ms step_avg:55.08ms
step:1726/2090 train_time:95107ms step_avg:55.10ms
step:1727/2090 train_time:95196ms step_avg:55.12ms
step:1728/2090 train_time:95282ms step_avg:55.14ms
step:1729/2090 train_time:95370ms step_avg:55.16ms
step:1730/2090 train_time:95457ms step_avg:55.18ms
step:1731/2090 train_time:95544ms step_avg:55.20ms
step:1732/2090 train_time:95631ms step_avg:55.21ms
step:1733/2090 train_time:95720ms step_avg:55.23ms
step:1734/2090 train_time:95807ms step_avg:55.25ms
step:1735/2090 train_time:95895ms step_avg:55.27ms
step:1736/2090 train_time:95982ms step_avg:55.29ms
step:1737/2090 train_time:96069ms step_avg:55.31ms
step:1738/2090 train_time:96156ms step_avg:55.33ms
step:1739/2090 train_time:96244ms step_avg:55.34ms
step:1740/2090 train_time:96331ms step_avg:55.36ms
step:1741/2090 train_time:96419ms step_avg:55.38ms
step:1742/2090 train_time:96506ms step_avg:55.40ms
step:1743/2090 train_time:96593ms step_avg:55.42ms
step:1744/2090 train_time:96680ms step_avg:55.44ms
step:1745/2090 train_time:96768ms step_avg:55.45ms
step:1746/2090 train_time:96855ms step_avg:55.47ms
step:1747/2090 train_time:96942ms step_avg:55.49ms
step:1748/2090 train_time:97030ms step_avg:55.51ms
step:1749/2090 train_time:97119ms step_avg:55.53ms
step:1750/2090 train_time:97205ms step_avg:55.55ms
step:1750/2090 val_loss:3.3722 train_time:97295ms step_avg:55.60ms
step:1751/2090 train_time:97315ms step_avg:55.58ms
step:1752/2090 train_time:97385ms step_avg:55.58ms
step:1753/2090 train_time:97478ms step_avg:55.61ms
step:1754/2090 train_time:97566ms step_avg:55.63ms
step:1755/2090 train_time:97654ms step_avg:55.64ms
step:1756/2090 train_time:97740ms step_avg:55.66ms
step:1757/2090 train_time:97827ms step_avg:55.68ms
step:1758/2090 train_time:97914ms step_avg:55.70ms
step:1759/2090 train_time:98001ms step_avg:55.71ms
step:1760/2090 train_time:98088ms step_avg:55.73ms
step:1761/2090 train_time:98174ms step_avg:55.75ms
step:1762/2090 train_time:98261ms step_avg:55.77ms
step:1763/2090 train_time:98351ms step_avg:55.79ms
step:1764/2090 train_time:98441ms step_avg:55.81ms
step:1765/2090 train_time:98530ms step_avg:55.82ms
step:1766/2090 train_time:98617ms step_avg:55.84ms
step:1767/2090 train_time:98705ms step_avg:55.86ms
step:1768/2090 train_time:98791ms step_avg:55.88ms
step:1769/2090 train_time:98878ms step_avg:55.89ms
step:1770/2090 train_time:98964ms step_avg:55.91ms
step:1771/2090 train_time:99051ms step_avg:55.93ms
step:1772/2090 train_time:99137ms step_avg:55.95ms
step:1773/2090 train_time:99224ms step_avg:55.96ms
step:1774/2090 train_time:99313ms step_avg:55.98ms
step:1775/2090 train_time:99402ms step_avg:56.00ms
step:1776/2090 train_time:99489ms step_avg:56.02ms
step:1777/2090 train_time:99578ms step_avg:56.04ms
step:1778/2090 train_time:99665ms step_avg:56.05ms
step:1779/2090 train_time:99752ms step_avg:56.07ms
step:1780/2090 train_time:99839ms step_avg:56.09ms
step:1781/2090 train_time:99927ms step_avg:56.11ms
step:1782/2090 train_time:100013ms step_avg:56.12ms
step:1783/2090 train_time:100099ms step_avg:56.14ms
step:1784/2090 train_time:100186ms step_avg:56.16ms
step:1785/2090 train_time:100274ms step_avg:56.18ms
step:1786/2090 train_time:100362ms step_avg:56.19ms
step:1787/2090 train_time:100451ms step_avg:56.21ms
step:1788/2090 train_time:100538ms step_avg:56.23ms
step:1789/2090 train_time:100627ms step_avg:56.25ms
step:1790/2090 train_time:100713ms step_avg:56.26ms
step:1791/2090 train_time:100801ms step_avg:56.28ms
step:1792/2090 train_time:100888ms step_avg:56.30ms
step:1793/2090 train_time:100975ms step_avg:56.32ms
step:1794/2090 train_time:101061ms step_avg:56.33ms
step:1795/2090 train_time:101149ms step_avg:56.35ms
step:1796/2090 train_time:101236ms step_avg:56.37ms
step:1797/2090 train_time:101323ms step_avg:56.38ms
step:1798/2090 train_time:101411ms step_avg:56.40ms
step:1799/2090 train_time:101499ms step_avg:56.42ms
step:1800/2090 train_time:101587ms step_avg:56.44ms
step:1801/2090 train_time:101675ms step_avg:56.45ms
step:1802/2090 train_time:101762ms step_avg:56.47ms
step:1803/2090 train_time:101850ms step_avg:56.49ms
step:1804/2090 train_time:101936ms step_avg:56.51ms
step:1805/2090 train_time:102024ms step_avg:56.52ms
step:1806/2090 train_time:102111ms step_avg:56.54ms
step:1807/2090 train_time:102198ms step_avg:56.56ms
step:1808/2090 train_time:102285ms step_avg:56.57ms
step:1809/2090 train_time:102374ms step_avg:56.59ms
step:1810/2090 train_time:102461ms step_avg:56.61ms
step:1811/2090 train_time:102550ms step_avg:56.63ms
step:1812/2090 train_time:102638ms step_avg:56.64ms
step:1813/2090 train_time:102726ms step_avg:56.66ms
step:1814/2090 train_time:102812ms step_avg:56.68ms
step:1815/2090 train_time:102900ms step_avg:56.69ms
step:1816/2090 train_time:102987ms step_avg:56.71ms
step:1817/2090 train_time:103074ms step_avg:56.73ms
step:1818/2090 train_time:103161ms step_avg:56.74ms
step:1819/2090 train_time:103249ms step_avg:56.76ms
step:1820/2090 train_time:103336ms step_avg:56.78ms
step:1821/2090 train_time:103426ms step_avg:56.80ms
step:1822/2090 train_time:103513ms step_avg:56.81ms
step:1823/2090 train_time:103601ms step_avg:56.83ms
step:1824/2090 train_time:103688ms step_avg:56.85ms
step:1825/2090 train_time:103776ms step_avg:56.86ms
step:1826/2090 train_time:103863ms step_avg:56.88ms
step:1827/2090 train_time:103951ms step_avg:56.90ms
step:1828/2090 train_time:104038ms step_avg:56.91ms
step:1829/2090 train_time:104126ms step_avg:56.93ms
step:1830/2090 train_time:104212ms step_avg:56.95ms
step:1831/2090 train_time:104301ms step_avg:56.96ms
step:1832/2090 train_time:104388ms step_avg:56.98ms
step:1833/2090 train_time:104476ms step_avg:57.00ms
step:1834/2090 train_time:104564ms step_avg:57.01ms
step:1835/2090 train_time:104653ms step_avg:57.03ms
step:1836/2090 train_time:104740ms step_avg:57.05ms
step:1837/2090 train_time:104828ms step_avg:57.06ms
step:1838/2090 train_time:104915ms step_avg:57.08ms
step:1839/2090 train_time:105003ms step_avg:57.10ms
step:1840/2090 train_time:105090ms step_avg:57.11ms
step:1841/2090 train_time:105177ms step_avg:57.13ms
step:1842/2090 train_time:105264ms step_avg:57.15ms
step:1843/2090 train_time:105353ms step_avg:57.16ms
step:1844/2090 train_time:105439ms step_avg:57.18ms
step:1845/2090 train_time:105528ms step_avg:57.20ms
step:1846/2090 train_time:105616ms step_avg:57.21ms
step:1847/2090 train_time:105704ms step_avg:57.23ms
step:1848/2090 train_time:105792ms step_avg:57.25ms
step:1849/2090 train_time:105879ms step_avg:57.26ms
step:1850/2090 train_time:105967ms step_avg:57.28ms
step:1851/2090 train_time:106055ms step_avg:57.30ms
step:1852/2090 train_time:106142ms step_avg:57.31ms
step:1853/2090 train_time:106230ms step_avg:57.33ms
step:1854/2090 train_time:106316ms step_avg:57.34ms
step:1855/2090 train_time:106405ms step_avg:57.36ms
step:1856/2090 train_time:106492ms step_avg:57.38ms
step:1857/2090 train_time:106580ms step_avg:57.39ms
step:1858/2090 train_time:106667ms step_avg:57.41ms
step:1859/2090 train_time:106755ms step_avg:57.43ms
step:1860/2090 train_time:106843ms step_avg:57.44ms
step:1861/2090 train_time:106930ms step_avg:57.46ms
step:1862/2090 train_time:107017ms step_avg:57.47ms
step:1863/2090 train_time:107104ms step_avg:57.49ms
step:1864/2090 train_time:107191ms step_avg:57.51ms
step:1865/2090 train_time:107280ms step_avg:57.52ms
step:1866/2090 train_time:107367ms step_avg:57.54ms
step:1867/2090 train_time:107456ms step_avg:57.56ms
step:1868/2090 train_time:107543ms step_avg:57.57ms
step:1869/2090 train_time:107632ms step_avg:57.59ms
step:1870/2090 train_time:107719ms step_avg:57.60ms
step:1871/2090 train_time:107807ms step_avg:57.62ms
step:1872/2090 train_time:107893ms step_avg:57.64ms
step:1873/2090 train_time:107982ms step_avg:57.65ms
step:1874/2090 train_time:108070ms step_avg:57.67ms
step:1875/2090 train_time:108158ms step_avg:57.68ms
step:1876/2090 train_time:108245ms step_avg:57.70ms
step:1877/2090 train_time:108333ms step_avg:57.72ms
step:1878/2090 train_time:108420ms step_avg:57.73ms
step:1879/2090 train_time:108508ms step_avg:57.75ms
step:1880/2090 train_time:108596ms step_avg:57.76ms
step:1881/2090 train_time:108683ms step_avg:57.78ms
step:1882/2090 train_time:108770ms step_avg:57.79ms
step:1883/2090 train_time:108858ms step_avg:57.81ms
step:1884/2090 train_time:108945ms step_avg:57.83ms
step:1885/2090 train_time:109033ms step_avg:57.84ms
step:1886/2090 train_time:109119ms step_avg:57.86ms
step:1887/2090 train_time:109208ms step_avg:57.87ms
step:1888/2090 train_time:109295ms step_avg:57.89ms
step:1889/2090 train_time:109382ms step_avg:57.90ms
step:1890/2090 train_time:109469ms step_avg:57.92ms
step:1891/2090 train_time:109557ms step_avg:57.94ms
step:1892/2090 train_time:109644ms step_avg:57.95ms
step:1893/2090 train_time:109732ms step_avg:57.97ms
step:1894/2090 train_time:109820ms step_avg:57.98ms
step:1895/2090 train_time:109907ms step_avg:58.00ms
step:1896/2090 train_time:109993ms step_avg:58.01ms
step:1897/2090 train_time:110081ms step_avg:58.03ms
step:1898/2090 train_time:110169ms step_avg:58.04ms
step:1899/2090 train_time:110257ms step_avg:58.06ms
step:1900/2090 train_time:110344ms step_avg:58.08ms
step:1901/2090 train_time:110432ms step_avg:58.09ms
step:1902/2090 train_time:110519ms step_avg:58.11ms
step:1903/2090 train_time:110606ms step_avg:58.12ms
step:1904/2090 train_time:110694ms step_avg:58.14ms
step:1905/2090 train_time:110782ms step_avg:58.15ms
step:1906/2090 train_time:110870ms step_avg:58.17ms
step:1907/2090 train_time:110958ms step_avg:58.18ms
step:1908/2090 train_time:111045ms step_avg:58.20ms
step:1909/2090 train_time:111134ms step_avg:58.22ms
step:1910/2090 train_time:111221ms step_avg:58.23ms
step:1911/2090 train_time:111311ms step_avg:58.25ms
step:1912/2090 train_time:111397ms step_avg:58.26ms
step:1913/2090 train_time:111485ms step_avg:58.28ms
step:1914/2090 train_time:111572ms step_avg:58.29ms
step:1915/2090 train_time:111659ms step_avg:58.31ms
step:1916/2090 train_time:111747ms step_avg:58.32ms
step:1917/2090 train_time:111835ms step_avg:58.34ms
step:1918/2090 train_time:111922ms step_avg:58.35ms
step:1919/2090 train_time:112010ms step_avg:58.37ms
step:1920/2090 train_time:112097ms step_avg:58.38ms
step:1921/2090 train_time:112185ms step_avg:58.40ms
step:1922/2090 train_time:112273ms step_avg:58.41ms
step:1923/2090 train_time:112360ms step_avg:58.43ms
step:1924/2090 train_time:112448ms step_avg:58.44ms
step:1925/2090 train_time:112535ms step_avg:58.46ms
step:1926/2090 train_time:112622ms step_avg:58.47ms
step:1927/2090 train_time:112710ms step_avg:58.49ms
step:1928/2090 train_time:112797ms step_avg:58.50ms
step:1929/2090 train_time:112885ms step_avg:58.52ms
step:1930/2090 train_time:112972ms step_avg:58.53ms
step:1931/2090 train_time:113060ms step_avg:58.55ms
step:1932/2090 train_time:113148ms step_avg:58.57ms
step:1933/2090 train_time:113235ms step_avg:58.58ms
step:1934/2090 train_time:113323ms step_avg:58.59ms
step:1935/2090 train_time:113412ms step_avg:58.61ms
step:1936/2090 train_time:113499ms step_avg:58.63ms
step:1937/2090 train_time:113587ms step_avg:58.64ms
step:1938/2090 train_time:113673ms step_avg:58.66ms
step:1939/2090 train_time:113761ms step_avg:58.67ms
step:1940/2090 train_time:113850ms step_avg:58.69ms
step:1941/2090 train_time:113938ms step_avg:58.70ms
step:1942/2090 train_time:114025ms step_avg:58.72ms
step:1943/2090 train_time:114113ms step_avg:58.73ms
step:1944/2090 train_time:114200ms step_avg:58.74ms
step:1945/2090 train_time:114289ms step_avg:58.76ms
step:1946/2090 train_time:114376ms step_avg:58.77ms
step:1947/2090 train_time:114464ms step_avg:58.79ms
step:1948/2090 train_time:114550ms step_avg:58.80ms
step:1949/2090 train_time:114638ms step_avg:58.82ms
step:1950/2090 train_time:114725ms step_avg:58.83ms
step:1951/2090 train_time:114813ms step_avg:58.85ms
step:1952/2090 train_time:114901ms step_avg:58.86ms
step:1953/2090 train_time:114989ms step_avg:58.88ms
step:1954/2090 train_time:115076ms step_avg:58.89ms
step:1955/2090 train_time:115163ms step_avg:58.91ms
step:1956/2090 train_time:115251ms step_avg:58.92ms
step:1957/2090 train_time:115339ms step_avg:58.94ms
step:1958/2090 train_time:115426ms step_avg:58.95ms
step:1959/2090 train_time:115514ms step_avg:58.97ms
step:1960/2090 train_time:115602ms step_avg:58.98ms
step:1961/2090 train_time:115689ms step_avg:59.00ms
step:1962/2090 train_time:115777ms step_avg:59.01ms
step:1963/2090 train_time:115864ms step_avg:59.02ms
step:1964/2090 train_time:115951ms step_avg:59.04ms
step:1965/2090 train_time:116039ms step_avg:59.05ms
step:1966/2090 train_time:116126ms step_avg:59.07ms
step:1967/2090 train_time:116214ms step_avg:59.08ms
step:1968/2090 train_time:116301ms step_avg:59.10ms
step:1969/2090 train_time:116389ms step_avg:59.11ms
step:1970/2090 train_time:116476ms step_avg:59.12ms
step:1971/2090 train_time:116565ms step_avg:59.14ms
step:1972/2090 train_time:116651ms step_avg:59.15ms
step:1973/2090 train_time:116739ms step_avg:59.17ms
step:1974/2090 train_time:116827ms step_avg:59.18ms
step:1975/2090 train_time:116915ms step_avg:59.20ms
step:1976/2090 train_time:117002ms step_avg:59.21ms
step:1977/2090 train_time:117091ms step_avg:59.23ms
step:1978/2090 train_time:117178ms step_avg:59.24ms
step:1979/2090 train_time:117266ms step_avg:59.25ms
step:1980/2090 train_time:117352ms step_avg:59.27ms
step:1981/2090 train_time:117440ms step_avg:59.28ms
step:1982/2090 train_time:117527ms step_avg:59.30ms
step:1983/2090 train_time:117615ms step_avg:59.31ms
step:1984/2090 train_time:117703ms step_avg:59.33ms
step:1985/2090 train_time:117790ms step_avg:59.34ms
step:1986/2090 train_time:117877ms step_avg:59.35ms
step:1987/2090 train_time:117965ms step_avg:59.37ms
step:1988/2090 train_time:118052ms step_avg:59.38ms
step:1989/2090 train_time:118140ms step_avg:59.40ms
step:1990/2090 train_time:118226ms step_avg:59.41ms
step:1991/2090 train_time:118315ms step_avg:59.42ms
step:1992/2090 train_time:118402ms step_avg:59.44ms
step:1993/2090 train_time:118490ms step_avg:59.45ms
step:1994/2090 train_time:118577ms step_avg:59.47ms
step:1995/2090 train_time:118666ms step_avg:59.48ms
step:1996/2090 train_time:118752ms step_avg:59.50ms
step:1997/2090 train_time:118840ms step_avg:59.51ms
step:1998/2090 train_time:118927ms step_avg:59.52ms
step:1999/2090 train_time:119015ms step_avg:59.54ms
step:2000/2090 train_time:119102ms step_avg:59.55ms
step:2000/2090 val_loss:3.2954 train_time:119192ms step_avg:59.60ms
step:2001/2090 train_time:119212ms step_avg:59.58ms
step:2002/2090 train_time:119284ms step_avg:59.58ms
step:2003/2090 train_time:119376ms step_avg:59.60ms
step:2004/2090 train_time:119464ms step_avg:59.61ms
step:2005/2090 train_time:119550ms step_avg:59.63ms
step:2006/2090 train_time:119637ms step_avg:59.64ms
step:2007/2090 train_time:119724ms step_avg:59.65ms
step:2008/2090 train_time:119810ms step_avg:59.67ms
step:2009/2090 train_time:119896ms step_avg:59.68ms
step:2010/2090 train_time:119982ms step_avg:59.69ms
step:2011/2090 train_time:120068ms step_avg:59.71ms
step:2012/2090 train_time:120157ms step_avg:59.72ms
step:2013/2090 train_time:120249ms step_avg:59.74ms
step:2014/2090 train_time:120338ms step_avg:59.75ms
step:2015/2090 train_time:120427ms step_avg:59.77ms
step:2016/2090 train_time:120514ms step_avg:59.78ms
step:2017/2090 train_time:120602ms step_avg:59.79ms
step:2018/2090 train_time:120687ms step_avg:59.81ms
step:2019/2090 train_time:120775ms step_avg:59.82ms
step:2020/2090 train_time:120860ms step_avg:59.83ms
step:2021/2090 train_time:120947ms step_avg:59.85ms
step:2022/2090 train_time:121033ms step_avg:59.86ms
step:2023/2090 train_time:121121ms step_avg:59.87ms
step:2024/2090 train_time:121210ms step_avg:59.89ms
step:2025/2090 train_time:121299ms step_avg:59.90ms
step:2026/2090 train_time:121388ms step_avg:59.92ms
step:2027/2090 train_time:121477ms step_avg:59.93ms
step:2028/2090 train_time:121564ms step_avg:59.94ms
step:2029/2090 train_time:121652ms step_avg:59.96ms
step:2030/2090 train_time:121739ms step_avg:59.97ms
step:2031/2090 train_time:121826ms step_avg:59.98ms
step:2032/2090 train_time:121912ms step_avg:60.00ms
step:2033/2090 train_time:121999ms step_avg:60.01ms
step:2034/2090 train_time:122085ms step_avg:60.02ms
step:2035/2090 train_time:122174ms step_avg:60.04ms
step:2036/2090 train_time:122261ms step_avg:60.05ms
step:2037/2090 train_time:122350ms step_avg:60.06ms
step:2038/2090 train_time:122438ms step_avg:60.08ms
step:2039/2090 train_time:122527ms step_avg:60.09ms
step:2040/2090 train_time:122615ms step_avg:60.11ms
step:2041/2090 train_time:122703ms step_avg:60.12ms
step:2042/2090 train_time:122789ms step_avg:60.13ms
step:2043/2090 train_time:122876ms step_avg:60.15ms
step:2044/2090 train_time:122962ms step_avg:60.16ms
step:2045/2090 train_time:123050ms step_avg:60.17ms
step:2046/2090 train_time:123138ms step_avg:60.18ms
step:2047/2090 train_time:123226ms step_avg:60.20ms
step:2048/2090 train_time:123314ms step_avg:60.21ms
step:2049/2090 train_time:123404ms step_avg:60.23ms
step:2050/2090 train_time:123492ms step_avg:60.24ms
step:2051/2090 train_time:123582ms step_avg:60.25ms
step:2052/2090 train_time:123669ms step_avg:60.27ms
step:2053/2090 train_time:123756ms step_avg:60.28ms
step:2054/2090 train_time:123843ms step_avg:60.29ms
step:2055/2090 train_time:123930ms step_avg:60.31ms
step:2056/2090 train_time:124017ms step_avg:60.32ms
step:2057/2090 train_time:124105ms step_avg:60.33ms
step:2058/2090 train_time:124193ms step_avg:60.35ms
step:2059/2090 train_time:124281ms step_avg:60.36ms
step:2060/2090 train_time:124369ms step_avg:60.37ms
step:2061/2090 train_time:124458ms step_avg:60.39ms
step:2062/2090 train_time:124545ms step_avg:60.40ms
step:2063/2090 train_time:124633ms step_avg:60.41ms
step:2064/2090 train_time:124721ms step_avg:60.43ms
step:2065/2090 train_time:124809ms step_avg:60.44ms
step:2066/2090 train_time:124896ms step_avg:60.45ms
step:2067/2090 train_time:124983ms step_avg:60.47ms
step:2068/2090 train_time:125070ms step_avg:60.48ms
step:2069/2090 train_time:125158ms step_avg:60.49ms
step:2070/2090 train_time:125247ms step_avg:60.51ms
step:2071/2090 train_time:125335ms step_avg:60.52ms
step:2072/2090 train_time:125423ms step_avg:60.53ms
step:2073/2090 train_time:125513ms step_avg:60.55ms
step:2074/2090 train_time:125600ms step_avg:60.56ms
step:2075/2090 train_time:125688ms step_avg:60.57ms
step:2076/2090 train_time:125775ms step_avg:60.59ms
step:2077/2090 train_time:125863ms step_avg:60.60ms
step:2078/2090 train_time:125949ms step_avg:60.61ms
step:2079/2090 train_time:126038ms step_avg:60.62ms
step:2080/2090 train_time:126125ms step_avg:60.64ms
step:2081/2090 train_time:126214ms step_avg:60.65ms
step:2082/2090 train_time:126301ms step_avg:60.66ms
step:2083/2090 train_time:126389ms step_avg:60.68ms
step:2084/2090 train_time:126478ms step_avg:60.69ms
step:2085/2090 train_time:126567ms step_avg:60.70ms
step:2086/2090 train_time:126655ms step_avg:60.72ms
step:2087/2090 train_time:126743ms step_avg:60.73ms
step:2088/2090 train_time:126830ms step_avg:60.74ms
step:2089/2090 train_time:126917ms step_avg:60.76ms
step:2090/2090 train_time:127004ms step_avg:60.77ms
step:2090/2090 val_loss:3.2740 train_time:127094ms step_avg:60.81ms
peak memory allocated: 29862 MiB reserved: 44716 MiB

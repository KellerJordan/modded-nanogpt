import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # weight decay
                if wd != 0:
                    mask = (update * p_slice) >= 0
                    # lr as weight decay  schedule
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)

                    update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)
                
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2050  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 18 12:33:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2090 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2090 train_time:66ms step_avg:66.24ms
step:2/2090 train_time:89ms step_avg:44.29ms
step:3/2090 train_time:120ms step_avg:40.16ms
step:4/2090 train_time:153ms step_avg:38.16ms
step:5/2090 train_time:185ms step_avg:37.08ms
step:6/2090 train_time:253ms step_avg:42.24ms
step:7/2090 train_time:320ms step_avg:45.71ms
step:8/2090 train_time:353ms step_avg:44.10ms
step:9/2090 train_time:386ms step_avg:42.87ms
step:10/2090 train_time:419ms step_avg:41.88ms
step:11/2090 train_time:452ms step_avg:41.08ms
step:12/2090 train_time:485ms step_avg:40.41ms
step:13/2090 train_time:519ms step_avg:39.89ms
step:14/2090 train_time:552ms step_avg:39.39ms
step:15/2090 train_time:585ms step_avg:39.00ms
step:16/2090 train_time:618ms step_avg:38.62ms
step:17/2090 train_time:652ms step_avg:38.32ms
step:18/2090 train_time:684ms step_avg:38.02ms
step:19/2090 train_time:718ms step_avg:37.80ms
step:20/2090 train_time:751ms step_avg:37.55ms
step:21/2090 train_time:785ms step_avg:37.36ms
step:22/2090 train_time:818ms step_avg:37.16ms
step:23/2090 train_time:851ms step_avg:37.00ms
step:24/2090 train_time:884ms step_avg:36.83ms
step:25/2090 train_time:917ms step_avg:36.69ms
step:26/2090 train_time:950ms step_avg:36.55ms
step:27/2090 train_time:984ms step_avg:36.43ms
step:28/2090 train_time:1017ms step_avg:36.31ms
step:29/2090 train_time:1050ms step_avg:36.21ms
step:30/2090 train_time:1083ms step_avg:36.10ms
step:31/2090 train_time:1116ms step_avg:36.01ms
step:32/2090 train_time:1149ms step_avg:35.91ms
step:33/2090 train_time:1183ms step_avg:35.85ms
step:34/2090 train_time:1216ms step_avg:35.77ms
step:35/2090 train_time:1251ms step_avg:35.74ms
step:36/2090 train_time:1284ms step_avg:35.67ms
step:37/2090 train_time:1318ms step_avg:35.63ms
step:38/2090 train_time:1351ms step_avg:35.56ms
step:39/2090 train_time:1386ms step_avg:35.53ms
step:40/2090 train_time:1418ms step_avg:35.46ms
step:41/2090 train_time:1452ms step_avg:35.42ms
step:42/2090 train_time:1485ms step_avg:35.36ms
step:43/2090 train_time:1519ms step_avg:35.32ms
step:44/2090 train_time:1552ms step_avg:35.26ms
step:45/2090 train_time:1585ms step_avg:35.22ms
step:46/2090 train_time:1618ms step_avg:35.18ms
step:47/2090 train_time:1652ms step_avg:35.14ms
step:48/2090 train_time:1684ms step_avg:35.09ms
step:49/2090 train_time:1718ms step_avg:35.07ms
step:50/2090 train_time:1751ms step_avg:35.02ms
step:51/2090 train_time:1785ms step_avg:34.99ms
step:52/2090 train_time:1818ms step_avg:34.96ms
step:53/2090 train_time:1851ms step_avg:34.92ms
step:54/2090 train_time:1884ms step_avg:34.89ms
step:55/2090 train_time:1917ms step_avg:34.86ms
step:56/2090 train_time:1950ms step_avg:34.83ms
step:57/2090 train_time:1984ms step_avg:34.80ms
step:58/2090 train_time:2017ms step_avg:34.77ms
step:59/2090 train_time:2050ms step_avg:34.74ms
step:60/2090 train_time:2083ms step_avg:34.71ms
step:61/2090 train_time:2116ms step_avg:34.69ms
step:62/2090 train_time:2149ms step_avg:34.67ms
step:63/2090 train_time:2183ms step_avg:34.66ms
step:64/2090 train_time:2216ms step_avg:34.63ms
step:65/2090 train_time:2250ms step_avg:34.62ms
step:66/2090 train_time:2283ms step_avg:34.59ms
step:67/2090 train_time:2317ms step_avg:34.58ms
step:68/2090 train_time:2350ms step_avg:34.55ms
step:69/2090 train_time:2383ms step_avg:34.54ms
step:70/2090 train_time:2416ms step_avg:34.52ms
step:71/2090 train_time:2450ms step_avg:34.50ms
step:72/2090 train_time:2483ms step_avg:34.48ms
step:73/2090 train_time:2516ms step_avg:34.47ms
step:74/2090 train_time:2549ms step_avg:34.45ms
step:75/2090 train_time:2583ms step_avg:34.43ms
step:76/2090 train_time:2615ms step_avg:34.41ms
step:77/2090 train_time:2649ms step_avg:34.40ms
step:78/2090 train_time:2682ms step_avg:34.38ms
step:79/2090 train_time:2716ms step_avg:34.37ms
step:80/2090 train_time:2748ms step_avg:34.35ms
step:81/2090 train_time:2782ms step_avg:34.34ms
step:82/2090 train_time:2815ms step_avg:34.32ms
step:83/2090 train_time:2848ms step_avg:34.32ms
step:84/2090 train_time:2881ms step_avg:34.30ms
step:85/2090 train_time:2914ms step_avg:34.29ms
step:86/2090 train_time:2947ms step_avg:34.27ms
step:87/2090 train_time:2981ms step_avg:34.26ms
step:88/2090 train_time:3014ms step_avg:34.25ms
step:89/2090 train_time:3047ms step_avg:34.24ms
step:90/2090 train_time:3080ms step_avg:34.22ms
step:91/2090 train_time:3113ms step_avg:34.21ms
step:92/2090 train_time:3146ms step_avg:34.20ms
step:93/2090 train_time:3179ms step_avg:34.19ms
step:94/2090 train_time:3212ms step_avg:34.17ms
step:95/2090 train_time:3246ms step_avg:34.17ms
step:96/2090 train_time:3279ms step_avg:34.15ms
step:97/2090 train_time:3312ms step_avg:34.15ms
step:98/2090 train_time:3345ms step_avg:34.14ms
step:99/2090 train_time:3379ms step_avg:34.13ms
step:100/2090 train_time:3411ms step_avg:34.11ms
step:101/2090 train_time:3445ms step_avg:34.11ms
step:102/2090 train_time:3478ms step_avg:34.09ms
step:103/2090 train_time:3511ms step_avg:34.09ms
step:104/2090 train_time:3544ms step_avg:34.07ms
step:105/2090 train_time:3577ms step_avg:34.07ms
step:106/2090 train_time:3610ms step_avg:34.06ms
step:107/2090 train_time:3644ms step_avg:34.05ms
step:108/2090 train_time:3677ms step_avg:34.04ms
step:109/2090 train_time:3710ms step_avg:34.04ms
step:110/2090 train_time:3743ms step_avg:34.03ms
step:111/2090 train_time:3777ms step_avg:34.02ms
step:112/2090 train_time:3809ms step_avg:34.01ms
step:113/2090 train_time:3843ms step_avg:34.01ms
step:114/2090 train_time:3876ms step_avg:34.00ms
step:115/2090 train_time:3909ms step_avg:33.99ms
step:116/2090 train_time:3942ms step_avg:33.98ms
step:117/2090 train_time:3976ms step_avg:33.98ms
step:118/2090 train_time:4009ms step_avg:33.97ms
step:119/2090 train_time:4042ms step_avg:33.96ms
step:120/2090 train_time:4075ms step_avg:33.95ms
step:121/2090 train_time:4107ms step_avg:33.95ms
step:122/2090 train_time:4141ms step_avg:33.94ms
step:123/2090 train_time:4173ms step_avg:33.93ms
step:124/2090 train_time:4206ms step_avg:33.92ms
step:125/2090 train_time:4240ms step_avg:33.92ms
step:126/2090 train_time:4273ms step_avg:33.91ms
step:127/2090 train_time:4306ms step_avg:33.91ms
step:128/2090 train_time:4340ms step_avg:33.90ms
step:129/2090 train_time:4373ms step_avg:33.90ms
step:130/2090 train_time:4405ms step_avg:33.89ms
step:131/2090 train_time:4439ms step_avg:33.88ms
step:132/2090 train_time:4472ms step_avg:33.88ms
step:133/2090 train_time:4505ms step_avg:33.87ms
step:134/2090 train_time:4538ms step_avg:33.86ms
step:135/2090 train_time:4571ms step_avg:33.86ms
step:136/2090 train_time:4604ms step_avg:33.85ms
step:137/2090 train_time:4637ms step_avg:33.85ms
step:138/2090 train_time:4670ms step_avg:33.84ms
step:139/2090 train_time:4703ms step_avg:33.84ms
step:140/2090 train_time:4736ms step_avg:33.83ms
step:141/2090 train_time:4769ms step_avg:33.82ms
step:142/2090 train_time:4802ms step_avg:33.82ms
step:143/2090 train_time:4836ms step_avg:33.82ms
step:144/2090 train_time:4869ms step_avg:33.81ms
step:145/2090 train_time:4902ms step_avg:33.81ms
step:146/2090 train_time:4935ms step_avg:33.80ms
step:147/2090 train_time:4968ms step_avg:33.80ms
step:148/2090 train_time:5001ms step_avg:33.79ms
step:149/2090 train_time:5034ms step_avg:33.79ms
step:150/2090 train_time:5067ms step_avg:33.78ms
step:151/2090 train_time:5101ms step_avg:33.78ms
step:152/2090 train_time:5133ms step_avg:33.77ms
step:153/2090 train_time:5166ms step_avg:33.77ms
step:154/2090 train_time:5199ms step_avg:33.76ms
step:155/2090 train_time:5232ms step_avg:33.76ms
step:156/2090 train_time:5265ms step_avg:33.75ms
step:157/2090 train_time:5298ms step_avg:33.75ms
step:158/2090 train_time:5331ms step_avg:33.74ms
step:159/2090 train_time:5365ms step_avg:33.74ms
step:160/2090 train_time:5397ms step_avg:33.73ms
step:161/2090 train_time:5431ms step_avg:33.73ms
step:162/2090 train_time:5464ms step_avg:33.73ms
step:163/2090 train_time:5497ms step_avg:33.72ms
step:164/2090 train_time:5530ms step_avg:33.72ms
step:165/2090 train_time:5563ms step_avg:33.71ms
step:166/2090 train_time:5596ms step_avg:33.71ms
step:167/2090 train_time:5629ms step_avg:33.71ms
step:168/2090 train_time:5662ms step_avg:33.70ms
step:169/2090 train_time:5696ms step_avg:33.70ms
step:170/2090 train_time:5728ms step_avg:33.70ms
step:171/2090 train_time:5762ms step_avg:33.69ms
step:172/2090 train_time:5794ms step_avg:33.69ms
step:173/2090 train_time:5828ms step_avg:33.69ms
step:174/2090 train_time:5861ms step_avg:33.68ms
step:175/2090 train_time:5893ms step_avg:33.68ms
step:176/2090 train_time:5926ms step_avg:33.67ms
step:177/2090 train_time:5960ms step_avg:33.67ms
step:178/2090 train_time:5993ms step_avg:33.67ms
step:179/2090 train_time:6026ms step_avg:33.66ms
step:180/2090 train_time:6059ms step_avg:33.66ms
step:181/2090 train_time:6092ms step_avg:33.66ms
step:182/2090 train_time:6125ms step_avg:33.65ms
step:183/2090 train_time:6158ms step_avg:33.65ms
step:184/2090 train_time:6191ms step_avg:33.65ms
step:185/2090 train_time:6224ms step_avg:33.64ms
step:186/2090 train_time:6257ms step_avg:33.64ms
step:187/2090 train_time:6290ms step_avg:33.64ms
step:188/2090 train_time:6323ms step_avg:33.63ms
step:189/2090 train_time:6357ms step_avg:33.63ms
step:190/2090 train_time:6389ms step_avg:33.63ms
step:191/2090 train_time:6423ms step_avg:33.63ms
step:192/2090 train_time:6455ms step_avg:33.62ms
step:193/2090 train_time:6489ms step_avg:33.62ms
step:194/2090 train_time:6522ms step_avg:33.62ms
step:195/2090 train_time:6555ms step_avg:33.61ms
step:196/2090 train_time:6587ms step_avg:33.61ms
step:197/2090 train_time:6621ms step_avg:33.61ms
step:198/2090 train_time:6653ms step_avg:33.60ms
step:199/2090 train_time:6687ms step_avg:33.60ms
step:200/2090 train_time:6719ms step_avg:33.60ms
step:201/2090 train_time:6753ms step_avg:33.60ms
step:202/2090 train_time:6785ms step_avg:33.59ms
step:203/2090 train_time:6819ms step_avg:33.59ms
step:204/2090 train_time:6852ms step_avg:33.59ms
step:205/2090 train_time:6885ms step_avg:33.59ms
step:206/2090 train_time:6918ms step_avg:33.58ms
step:207/2090 train_time:6951ms step_avg:33.58ms
step:208/2090 train_time:6984ms step_avg:33.58ms
step:209/2090 train_time:7018ms step_avg:33.58ms
step:210/2090 train_time:7050ms step_avg:33.57ms
step:211/2090 train_time:7084ms step_avg:33.57ms
step:212/2090 train_time:7116ms step_avg:33.57ms
step:213/2090 train_time:7150ms step_avg:33.57ms
step:214/2090 train_time:7182ms step_avg:33.56ms
step:215/2090 train_time:7216ms step_avg:33.56ms
step:216/2090 train_time:7249ms step_avg:33.56ms
step:217/2090 train_time:7282ms step_avg:33.56ms
step:218/2090 train_time:7315ms step_avg:33.55ms
step:219/2090 train_time:7348ms step_avg:33.55ms
step:220/2090 train_time:7381ms step_avg:33.55ms
step:221/2090 train_time:7414ms step_avg:33.55ms
step:222/2090 train_time:7447ms step_avg:33.55ms
step:223/2090 train_time:7480ms step_avg:33.54ms
step:224/2090 train_time:7513ms step_avg:33.54ms
step:225/2090 train_time:7547ms step_avg:33.54ms
step:226/2090 train_time:7579ms step_avg:33.54ms
step:227/2090 train_time:7613ms step_avg:33.54ms
step:228/2090 train_time:7645ms step_avg:33.53ms
step:229/2090 train_time:7679ms step_avg:33.53ms
step:230/2090 train_time:7711ms step_avg:33.53ms
step:231/2090 train_time:7744ms step_avg:33.53ms
step:232/2090 train_time:7777ms step_avg:33.52ms
step:233/2090 train_time:7810ms step_avg:33.52ms
step:234/2090 train_time:7843ms step_avg:33.52ms
step:235/2090 train_time:7876ms step_avg:33.52ms
step:236/2090 train_time:7909ms step_avg:33.51ms
step:237/2090 train_time:7942ms step_avg:33.51ms
step:238/2090 train_time:7975ms step_avg:33.51ms
step:239/2090 train_time:8009ms step_avg:33.51ms
step:240/2090 train_time:8042ms step_avg:33.51ms
step:241/2090 train_time:8075ms step_avg:33.50ms
step:242/2090 train_time:8107ms step_avg:33.50ms
step:243/2090 train_time:8141ms step_avg:33.50ms
step:244/2090 train_time:8173ms step_avg:33.50ms
step:245/2090 train_time:8207ms step_avg:33.50ms
step:246/2090 train_time:8240ms step_avg:33.49ms
step:247/2090 train_time:8273ms step_avg:33.49ms
step:248/2090 train_time:8305ms step_avg:33.49ms
step:249/2090 train_time:8339ms step_avg:33.49ms
step:250/2090 train_time:8372ms step_avg:33.49ms
step:250/2090 val_loss:4.2788 train_time:8407ms step_avg:33.63ms
step:251/2090 train_time:8427ms step_avg:33.57ms
step:252/2090 train_time:8446ms step_avg:33.52ms
step:253/2090 train_time:8475ms step_avg:33.50ms
step:254/2090 train_time:8509ms step_avg:33.50ms
step:255/2090 train_time:8543ms step_avg:33.50ms
step:256/2090 train_time:8576ms step_avg:33.50ms
step:257/2090 train_time:8610ms step_avg:33.50ms
step:258/2090 train_time:8643ms step_avg:33.50ms
step:259/2090 train_time:8676ms step_avg:33.50ms
step:260/2090 train_time:8709ms step_avg:33.50ms
step:261/2090 train_time:8742ms step_avg:33.49ms
step:262/2090 train_time:8775ms step_avg:33.49ms
step:263/2090 train_time:8808ms step_avg:33.49ms
step:264/2090 train_time:8841ms step_avg:33.49ms
step:265/2090 train_time:8874ms step_avg:33.49ms
step:266/2090 train_time:8906ms step_avg:33.48ms
step:267/2090 train_time:8939ms step_avg:33.48ms
step:268/2090 train_time:8972ms step_avg:33.48ms
step:269/2090 train_time:9005ms step_avg:33.48ms
step:270/2090 train_time:9038ms step_avg:33.47ms
step:271/2090 train_time:9071ms step_avg:33.47ms
step:272/2090 train_time:9104ms step_avg:33.47ms
step:273/2090 train_time:9137ms step_avg:33.47ms
step:274/2090 train_time:9169ms step_avg:33.46ms
step:275/2090 train_time:9203ms step_avg:33.46ms
step:276/2090 train_time:9235ms step_avg:33.46ms
step:277/2090 train_time:9268ms step_avg:33.46ms
step:278/2090 train_time:9301ms step_avg:33.46ms
step:279/2090 train_time:9334ms step_avg:33.46ms
step:280/2090 train_time:9367ms step_avg:33.45ms
step:281/2090 train_time:9400ms step_avg:33.45ms
step:282/2090 train_time:9433ms step_avg:33.45ms
step:283/2090 train_time:9467ms step_avg:33.45ms
step:284/2090 train_time:9500ms step_avg:33.45ms
step:285/2090 train_time:9534ms step_avg:33.45ms
step:286/2090 train_time:9566ms step_avg:33.45ms
step:287/2090 train_time:9600ms step_avg:33.45ms
step:288/2090 train_time:9633ms step_avg:33.45ms
step:289/2090 train_time:9666ms step_avg:33.45ms
step:290/2090 train_time:9699ms step_avg:33.44ms
step:291/2090 train_time:9733ms step_avg:33.45ms
step:292/2090 train_time:9765ms step_avg:33.44ms
step:293/2090 train_time:9799ms step_avg:33.44ms
step:294/2090 train_time:9831ms step_avg:33.44ms
step:295/2090 train_time:9865ms step_avg:33.44ms
step:296/2090 train_time:9898ms step_avg:33.44ms
step:297/2090 train_time:9931ms step_avg:33.44ms
step:298/2090 train_time:9963ms step_avg:33.43ms
step:299/2090 train_time:9996ms step_avg:33.43ms
step:300/2090 train_time:10029ms step_avg:33.43ms
step:301/2090 train_time:10062ms step_avg:33.43ms
step:302/2090 train_time:10095ms step_avg:33.43ms
step:303/2090 train_time:10128ms step_avg:33.43ms
step:304/2090 train_time:10161ms step_avg:33.42ms
step:305/2090 train_time:10194ms step_avg:33.42ms
step:306/2090 train_time:10227ms step_avg:33.42ms
step:307/2090 train_time:10260ms step_avg:33.42ms
step:308/2090 train_time:10292ms step_avg:33.42ms
step:309/2090 train_time:10326ms step_avg:33.42ms
step:310/2090 train_time:10359ms step_avg:33.41ms
step:311/2090 train_time:10392ms step_avg:33.41ms
step:312/2090 train_time:10425ms step_avg:33.41ms
step:313/2090 train_time:10458ms step_avg:33.41ms
step:314/2090 train_time:10491ms step_avg:33.41ms
step:315/2090 train_time:10524ms step_avg:33.41ms
step:316/2090 train_time:10557ms step_avg:33.41ms
step:317/2090 train_time:10590ms step_avg:33.41ms
step:318/2090 train_time:10623ms step_avg:33.41ms
step:319/2090 train_time:10656ms step_avg:33.41ms
step:320/2090 train_time:10689ms step_avg:33.40ms
step:321/2090 train_time:10723ms step_avg:33.40ms
step:322/2090 train_time:10756ms step_avg:33.40ms
step:323/2090 train_time:10789ms step_avg:33.40ms
step:324/2090 train_time:10822ms step_avg:33.40ms
step:325/2090 train_time:10855ms step_avg:33.40ms
step:326/2090 train_time:10888ms step_avg:33.40ms
step:327/2090 train_time:10921ms step_avg:33.40ms
step:328/2090 train_time:10954ms step_avg:33.40ms
step:329/2090 train_time:10987ms step_avg:33.40ms
step:330/2090 train_time:11020ms step_avg:33.39ms
step:331/2090 train_time:11053ms step_avg:33.39ms
step:332/2090 train_time:11086ms step_avg:33.39ms
step:333/2090 train_time:11119ms step_avg:33.39ms
step:334/2090 train_time:11152ms step_avg:33.39ms
step:335/2090 train_time:11185ms step_avg:33.39ms
step:336/2090 train_time:11218ms step_avg:33.39ms
step:337/2090 train_time:11251ms step_avg:33.38ms
step:338/2090 train_time:11283ms step_avg:33.38ms
step:339/2090 train_time:11317ms step_avg:33.38ms
step:340/2090 train_time:11349ms step_avg:33.38ms
step:341/2090 train_time:11383ms step_avg:33.38ms
step:342/2090 train_time:11416ms step_avg:33.38ms
step:343/2090 train_time:11449ms step_avg:33.38ms
step:344/2090 train_time:11482ms step_avg:33.38ms
step:345/2090 train_time:11516ms step_avg:33.38ms
step:346/2090 train_time:11548ms step_avg:33.38ms
step:347/2090 train_time:11582ms step_avg:33.38ms
step:348/2090 train_time:11614ms step_avg:33.37ms
step:349/2090 train_time:11648ms step_avg:33.37ms
step:350/2090 train_time:11681ms step_avg:33.37ms
step:351/2090 train_time:11714ms step_avg:33.37ms
step:352/2090 train_time:11747ms step_avg:33.37ms
step:353/2090 train_time:11780ms step_avg:33.37ms
step:354/2090 train_time:11813ms step_avg:33.37ms
step:355/2090 train_time:11847ms step_avg:33.37ms
step:356/2090 train_time:11879ms step_avg:33.37ms
step:357/2090 train_time:11913ms step_avg:33.37ms
step:358/2090 train_time:11946ms step_avg:33.37ms
step:359/2090 train_time:11979ms step_avg:33.37ms
step:360/2090 train_time:12012ms step_avg:33.37ms
step:361/2090 train_time:12045ms step_avg:33.36ms
step:362/2090 train_time:12077ms step_avg:33.36ms
step:363/2090 train_time:12111ms step_avg:33.36ms
step:364/2090 train_time:12143ms step_avg:33.36ms
step:365/2090 train_time:12176ms step_avg:33.36ms
step:366/2090 train_time:12209ms step_avg:33.36ms
step:367/2090 train_time:12242ms step_avg:33.36ms
step:368/2090 train_time:12275ms step_avg:33.36ms
step:369/2090 train_time:12308ms step_avg:33.36ms
step:370/2090 train_time:12341ms step_avg:33.35ms
step:371/2090 train_time:12374ms step_avg:33.35ms
step:372/2090 train_time:12407ms step_avg:33.35ms
step:373/2090 train_time:12440ms step_avg:33.35ms
step:374/2090 train_time:12473ms step_avg:33.35ms
step:375/2090 train_time:12506ms step_avg:33.35ms
step:376/2090 train_time:12539ms step_avg:33.35ms
step:377/2090 train_time:12572ms step_avg:33.35ms
step:378/2090 train_time:12605ms step_avg:33.35ms
step:379/2090 train_time:12639ms step_avg:33.35ms
step:380/2090 train_time:12671ms step_avg:33.35ms
step:381/2090 train_time:12705ms step_avg:33.35ms
step:382/2090 train_time:12737ms step_avg:33.34ms
step:383/2090 train_time:12771ms step_avg:33.34ms
step:384/2090 train_time:12804ms step_avg:33.34ms
step:385/2090 train_time:12837ms step_avg:33.34ms
step:386/2090 train_time:12870ms step_avg:33.34ms
step:387/2090 train_time:12903ms step_avg:33.34ms
step:388/2090 train_time:12936ms step_avg:33.34ms
step:389/2090 train_time:12969ms step_avg:33.34ms
step:390/2090 train_time:13002ms step_avg:33.34ms
step:391/2090 train_time:13035ms step_avg:33.34ms
step:392/2090 train_time:13068ms step_avg:33.34ms
step:393/2090 train_time:13101ms step_avg:33.34ms
step:394/2090 train_time:13134ms step_avg:33.33ms
step:395/2090 train_time:13167ms step_avg:33.33ms
step:396/2090 train_time:13200ms step_avg:33.33ms
step:397/2090 train_time:13233ms step_avg:33.33ms
step:398/2090 train_time:13266ms step_avg:33.33ms
step:399/2090 train_time:13299ms step_avg:33.33ms
step:400/2090 train_time:13331ms step_avg:33.33ms
step:401/2090 train_time:13364ms step_avg:33.33ms
step:402/2090 train_time:13397ms step_avg:33.33ms
step:403/2090 train_time:13430ms step_avg:33.33ms
step:404/2090 train_time:13463ms step_avg:33.32ms
step:405/2090 train_time:13496ms step_avg:33.32ms
step:406/2090 train_time:13529ms step_avg:33.32ms
step:407/2090 train_time:13562ms step_avg:33.32ms
step:408/2090 train_time:13595ms step_avg:33.32ms
step:409/2090 train_time:13628ms step_avg:33.32ms
step:410/2090 train_time:13661ms step_avg:33.32ms
step:411/2090 train_time:13694ms step_avg:33.32ms
step:412/2090 train_time:13728ms step_avg:33.32ms
step:413/2090 train_time:13760ms step_avg:33.32ms
step:414/2090 train_time:13793ms step_avg:33.32ms
step:415/2090 train_time:13827ms step_avg:33.32ms
step:416/2090 train_time:13859ms step_avg:33.32ms
step:417/2090 train_time:13893ms step_avg:33.32ms
step:418/2090 train_time:13926ms step_avg:33.32ms
step:419/2090 train_time:13959ms step_avg:33.32ms
step:420/2090 train_time:13992ms step_avg:33.31ms
step:421/2090 train_time:14025ms step_avg:33.31ms
step:422/2090 train_time:14058ms step_avg:33.31ms
step:423/2090 train_time:14091ms step_avg:33.31ms
step:424/2090 train_time:14124ms step_avg:33.31ms
step:425/2090 train_time:14157ms step_avg:33.31ms
step:426/2090 train_time:14190ms step_avg:33.31ms
step:427/2090 train_time:14223ms step_avg:33.31ms
step:428/2090 train_time:14256ms step_avg:33.31ms
step:429/2090 train_time:14289ms step_avg:33.31ms
step:430/2090 train_time:14322ms step_avg:33.31ms
step:431/2090 train_time:14355ms step_avg:33.31ms
step:432/2090 train_time:14388ms step_avg:33.31ms
step:433/2090 train_time:14421ms step_avg:33.31ms
step:434/2090 train_time:14454ms step_avg:33.30ms
step:435/2090 train_time:14487ms step_avg:33.30ms
step:436/2090 train_time:14520ms step_avg:33.30ms
step:437/2090 train_time:14553ms step_avg:33.30ms
step:438/2090 train_time:14586ms step_avg:33.30ms
step:439/2090 train_time:14619ms step_avg:33.30ms
step:440/2090 train_time:14652ms step_avg:33.30ms
step:441/2090 train_time:14685ms step_avg:33.30ms
step:442/2090 train_time:14718ms step_avg:33.30ms
step:443/2090 train_time:14752ms step_avg:33.30ms
step:444/2090 train_time:14785ms step_avg:33.30ms
step:445/2090 train_time:14818ms step_avg:33.30ms
step:446/2090 train_time:14851ms step_avg:33.30ms
step:447/2090 train_time:14884ms step_avg:33.30ms
step:448/2090 train_time:14917ms step_avg:33.30ms
step:449/2090 train_time:14950ms step_avg:33.30ms
step:450/2090 train_time:14983ms step_avg:33.30ms
step:451/2090 train_time:15016ms step_avg:33.30ms
step:452/2090 train_time:15049ms step_avg:33.30ms
step:453/2090 train_time:15082ms step_avg:33.29ms
step:454/2090 train_time:15115ms step_avg:33.29ms
step:455/2090 train_time:15148ms step_avg:33.29ms
step:456/2090 train_time:15181ms step_avg:33.29ms
step:457/2090 train_time:15214ms step_avg:33.29ms
step:458/2090 train_time:15247ms step_avg:33.29ms
step:459/2090 train_time:15280ms step_avg:33.29ms
step:460/2090 train_time:15313ms step_avg:33.29ms
step:461/2090 train_time:15346ms step_avg:33.29ms
step:462/2090 train_time:15378ms step_avg:33.29ms
step:463/2090 train_time:15412ms step_avg:33.29ms
step:464/2090 train_time:15444ms step_avg:33.29ms
step:465/2090 train_time:15478ms step_avg:33.29ms
step:466/2090 train_time:15511ms step_avg:33.29ms
step:467/2090 train_time:15544ms step_avg:33.29ms
step:468/2090 train_time:15577ms step_avg:33.28ms
step:469/2090 train_time:15611ms step_avg:33.28ms
step:470/2090 train_time:15643ms step_avg:33.28ms
step:471/2090 train_time:15676ms step_avg:33.28ms
step:472/2090 train_time:15709ms step_avg:33.28ms
step:473/2090 train_time:15742ms step_avg:33.28ms
step:474/2090 train_time:15775ms step_avg:33.28ms
step:475/2090 train_time:15808ms step_avg:33.28ms
step:476/2090 train_time:15841ms step_avg:33.28ms
step:477/2090 train_time:15874ms step_avg:33.28ms
step:478/2090 train_time:15907ms step_avg:33.28ms
step:479/2090 train_time:15940ms step_avg:33.28ms
step:480/2090 train_time:15973ms step_avg:33.28ms
step:481/2090 train_time:16006ms step_avg:33.28ms
step:482/2090 train_time:16039ms step_avg:33.28ms
step:483/2090 train_time:16072ms step_avg:33.28ms
step:484/2090 train_time:16105ms step_avg:33.27ms
step:485/2090 train_time:16138ms step_avg:33.27ms
step:486/2090 train_time:16171ms step_avg:33.27ms
step:487/2090 train_time:16205ms step_avg:33.27ms
step:488/2090 train_time:16237ms step_avg:33.27ms
step:489/2090 train_time:16271ms step_avg:33.27ms
step:490/2090 train_time:16303ms step_avg:33.27ms
step:491/2090 train_time:16336ms step_avg:33.27ms
step:492/2090 train_time:16369ms step_avg:33.27ms
step:493/2090 train_time:16402ms step_avg:33.27ms
step:494/2090 train_time:16435ms step_avg:33.27ms
step:495/2090 train_time:16469ms step_avg:33.27ms
step:496/2090 train_time:16501ms step_avg:33.27ms
step:497/2090 train_time:16535ms step_avg:33.27ms
step:498/2090 train_time:16567ms step_avg:33.27ms
step:499/2090 train_time:16601ms step_avg:33.27ms
step:500/2090 train_time:16633ms step_avg:33.27ms
step:500/2090 val_loss:4.0152 train_time:16669ms step_avg:33.34ms
step:501/2090 train_time:16689ms step_avg:33.31ms
step:502/2090 train_time:16709ms step_avg:33.28ms
step:503/2090 train_time:16737ms step_avg:33.27ms
step:504/2090 train_time:16770ms step_avg:33.27ms
step:505/2090 train_time:16804ms step_avg:33.28ms
step:506/2090 train_time:16837ms step_avg:33.28ms
step:507/2090 train_time:16872ms step_avg:33.28ms
step:508/2090 train_time:16905ms step_avg:33.28ms
step:509/2090 train_time:16938ms step_avg:33.28ms
step:510/2090 train_time:16971ms step_avg:33.28ms
step:511/2090 train_time:17004ms step_avg:33.28ms
step:512/2090 train_time:17036ms step_avg:33.27ms
step:513/2090 train_time:17069ms step_avg:33.27ms
step:514/2090 train_time:17102ms step_avg:33.27ms
step:515/2090 train_time:17135ms step_avg:33.27ms
step:516/2090 train_time:17168ms step_avg:33.27ms
step:517/2090 train_time:17201ms step_avg:33.27ms
step:518/2090 train_time:17234ms step_avg:33.27ms
step:519/2090 train_time:17266ms step_avg:33.27ms
step:520/2090 train_time:17299ms step_avg:33.27ms
step:521/2090 train_time:17333ms step_avg:33.27ms
step:522/2090 train_time:17365ms step_avg:33.27ms
step:523/2090 train_time:17398ms step_avg:33.27ms
step:524/2090 train_time:17431ms step_avg:33.26ms
step:525/2090 train_time:17463ms step_avg:33.26ms
step:526/2090 train_time:17496ms step_avg:33.26ms
step:527/2090 train_time:17529ms step_avg:33.26ms
step:528/2090 train_time:17562ms step_avg:33.26ms
step:529/2090 train_time:17594ms step_avg:33.26ms
step:530/2090 train_time:17627ms step_avg:33.26ms
step:531/2090 train_time:17660ms step_avg:33.26ms
step:532/2090 train_time:17693ms step_avg:33.26ms
step:533/2090 train_time:17726ms step_avg:33.26ms
step:534/2090 train_time:17759ms step_avg:33.26ms
step:535/2090 train_time:17793ms step_avg:33.26ms
step:536/2090 train_time:17826ms step_avg:33.26ms
step:537/2090 train_time:17859ms step_avg:33.26ms
step:538/2090 train_time:17892ms step_avg:33.26ms
step:539/2090 train_time:17925ms step_avg:33.26ms
step:540/2090 train_time:17958ms step_avg:33.25ms
step:541/2090 train_time:17991ms step_avg:33.25ms
step:542/2090 train_time:18024ms step_avg:33.25ms
step:543/2090 train_time:18057ms step_avg:33.25ms
step:544/2090 train_time:18090ms step_avg:33.25ms
step:545/2090 train_time:18123ms step_avg:33.25ms
step:546/2090 train_time:18156ms step_avg:33.25ms
step:547/2090 train_time:18189ms step_avg:33.25ms
step:548/2090 train_time:18222ms step_avg:33.25ms
step:549/2090 train_time:18255ms step_avg:33.25ms
step:550/2090 train_time:18288ms step_avg:33.25ms
step:551/2090 train_time:18321ms step_avg:33.25ms
step:552/2090 train_time:18353ms step_avg:33.25ms
step:553/2090 train_time:18386ms step_avg:33.25ms
step:554/2090 train_time:18419ms step_avg:33.25ms
step:555/2090 train_time:18452ms step_avg:33.25ms
step:556/2090 train_time:18485ms step_avg:33.25ms
step:557/2090 train_time:18519ms step_avg:33.25ms
step:558/2090 train_time:18551ms step_avg:33.24ms
step:559/2090 train_time:18583ms step_avg:33.24ms
step:560/2090 train_time:18616ms step_avg:33.24ms
step:561/2090 train_time:18649ms step_avg:33.24ms
step:562/2090 train_time:18682ms step_avg:33.24ms
step:563/2090 train_time:18715ms step_avg:33.24ms
step:564/2090 train_time:18748ms step_avg:33.24ms
step:565/2090 train_time:18781ms step_avg:33.24ms
step:566/2090 train_time:18814ms step_avg:33.24ms
step:567/2090 train_time:18847ms step_avg:33.24ms
step:568/2090 train_time:18880ms step_avg:33.24ms
step:569/2090 train_time:18913ms step_avg:33.24ms
step:570/2090 train_time:18946ms step_avg:33.24ms
step:571/2090 train_time:18979ms step_avg:33.24ms
step:572/2090 train_time:19012ms step_avg:33.24ms
step:573/2090 train_time:19045ms step_avg:33.24ms
step:574/2090 train_time:19078ms step_avg:33.24ms
step:575/2090 train_time:19111ms step_avg:33.24ms
step:576/2090 train_time:19144ms step_avg:33.24ms
step:577/2090 train_time:19177ms step_avg:33.24ms
step:578/2090 train_time:19210ms step_avg:33.24ms
step:579/2090 train_time:19243ms step_avg:33.24ms
step:580/2090 train_time:19276ms step_avg:33.23ms
step:581/2090 train_time:19309ms step_avg:33.23ms
step:582/2090 train_time:19342ms step_avg:33.23ms
step:583/2090 train_time:19375ms step_avg:33.23ms
step:584/2090 train_time:19408ms step_avg:33.23ms
step:585/2090 train_time:19441ms step_avg:33.23ms
step:586/2090 train_time:19474ms step_avg:33.23ms
step:587/2090 train_time:19507ms step_avg:33.23ms
step:588/2090 train_time:19539ms step_avg:33.23ms
step:589/2090 train_time:19572ms step_avg:33.23ms
step:590/2090 train_time:19605ms step_avg:33.23ms
step:591/2090 train_time:19638ms step_avg:33.23ms
step:592/2090 train_time:19671ms step_avg:33.23ms
step:593/2090 train_time:19704ms step_avg:33.23ms
step:594/2090 train_time:19737ms step_avg:33.23ms
step:595/2090 train_time:19770ms step_avg:33.23ms
step:596/2090 train_time:19803ms step_avg:33.23ms
step:597/2090 train_time:19836ms step_avg:33.23ms
step:598/2090 train_time:19869ms step_avg:33.23ms
step:599/2090 train_time:19902ms step_avg:33.23ms
step:600/2090 train_time:19935ms step_avg:33.22ms
step:601/2090 train_time:19968ms step_avg:33.23ms
step:602/2090 train_time:20001ms step_avg:33.22ms
step:603/2090 train_time:20034ms step_avg:33.22ms
step:604/2090 train_time:20067ms step_avg:33.22ms
step:605/2090 train_time:20100ms step_avg:33.22ms
step:606/2090 train_time:20133ms step_avg:33.22ms
step:607/2090 train_time:20166ms step_avg:33.22ms
step:608/2090 train_time:20199ms step_avg:33.22ms
step:609/2090 train_time:20232ms step_avg:33.22ms
step:610/2090 train_time:20265ms step_avg:33.22ms
step:611/2090 train_time:20298ms step_avg:33.22ms
step:612/2090 train_time:20331ms step_avg:33.22ms
step:613/2090 train_time:20364ms step_avg:33.22ms
step:614/2090 train_time:20397ms step_avg:33.22ms
step:615/2090 train_time:20429ms step_avg:33.22ms
step:616/2090 train_time:20462ms step_avg:33.22ms
step:617/2090 train_time:20495ms step_avg:33.22ms
step:618/2090 train_time:20528ms step_avg:33.22ms
step:619/2090 train_time:20561ms step_avg:33.22ms
step:620/2090 train_time:20594ms step_avg:33.22ms
step:621/2090 train_time:20627ms step_avg:33.22ms
step:622/2090 train_time:20660ms step_avg:33.21ms
step:623/2090 train_time:20693ms step_avg:33.22ms
step:624/2090 train_time:20726ms step_avg:33.21ms
step:625/2090 train_time:20759ms step_avg:33.21ms
step:626/2090 train_time:20792ms step_avg:33.21ms
step:627/2090 train_time:20825ms step_avg:33.21ms
step:628/2090 train_time:20857ms step_avg:33.21ms
step:629/2090 train_time:20891ms step_avg:33.21ms
step:630/2090 train_time:20923ms step_avg:33.21ms
step:631/2090 train_time:20956ms step_avg:33.21ms
step:632/2090 train_time:20989ms step_avg:33.21ms
step:633/2090 train_time:21022ms step_avg:33.21ms
step:634/2090 train_time:21055ms step_avg:33.21ms
step:635/2090 train_time:21088ms step_avg:33.21ms
step:636/2090 train_time:21121ms step_avg:33.21ms
step:637/2090 train_time:21154ms step_avg:33.21ms
step:638/2090 train_time:21187ms step_avg:33.21ms
step:639/2090 train_time:21220ms step_avg:33.21ms
step:640/2090 train_time:21253ms step_avg:33.21ms
step:641/2090 train_time:21286ms step_avg:33.21ms
step:642/2090 train_time:21319ms step_avg:33.21ms
step:643/2090 train_time:21352ms step_avg:33.21ms
step:644/2090 train_time:21385ms step_avg:33.21ms
step:645/2090 train_time:21418ms step_avg:33.21ms
step:646/2090 train_time:21451ms step_avg:33.21ms
step:647/2090 train_time:21484ms step_avg:33.20ms
step:648/2090 train_time:21516ms step_avg:33.20ms
step:649/2090 train_time:21549ms step_avg:33.20ms
step:650/2090 train_time:21582ms step_avg:33.20ms
step:651/2090 train_time:21616ms step_avg:33.20ms
step:652/2090 train_time:21648ms step_avg:33.20ms
step:653/2090 train_time:21682ms step_avg:33.20ms
step:654/2090 train_time:21714ms step_avg:33.20ms
step:655/2090 train_time:21747ms step_avg:33.20ms
step:656/2090 train_time:21780ms step_avg:33.20ms
step:657/2090 train_time:21813ms step_avg:33.20ms
step:658/2090 train_time:21846ms step_avg:33.20ms
step:659/2090 train_time:21879ms step_avg:33.20ms
step:660/2090 train_time:21912ms step_avg:33.20ms
step:661/2090 train_time:21945ms step_avg:33.20ms
step:662/2090 train_time:21978ms step_avg:33.20ms
step:663/2090 train_time:22011ms step_avg:33.20ms
step:664/2090 train_time:22044ms step_avg:33.20ms
step:665/2090 train_time:22077ms step_avg:33.20ms
step:666/2090 train_time:22110ms step_avg:33.20ms
step:667/2090 train_time:22143ms step_avg:33.20ms
step:668/2090 train_time:22176ms step_avg:33.20ms
step:669/2090 train_time:22209ms step_avg:33.20ms
step:670/2090 train_time:22242ms step_avg:33.20ms
step:671/2090 train_time:22275ms step_avg:33.20ms
step:672/2090 train_time:22308ms step_avg:33.20ms
step:673/2090 train_time:22341ms step_avg:33.20ms
step:674/2090 train_time:22373ms step_avg:33.20ms
step:675/2090 train_time:22406ms step_avg:33.19ms
step:676/2090 train_time:22439ms step_avg:33.19ms
step:677/2090 train_time:22473ms step_avg:33.19ms
step:678/2090 train_time:22506ms step_avg:33.19ms
step:679/2090 train_time:22538ms step_avg:33.19ms
step:680/2090 train_time:22571ms step_avg:33.19ms
step:681/2090 train_time:22604ms step_avg:33.19ms
step:682/2090 train_time:22637ms step_avg:33.19ms
step:683/2090 train_time:22670ms step_avg:33.19ms
step:684/2090 train_time:22703ms step_avg:33.19ms
step:685/2090 train_time:22737ms step_avg:33.19ms
step:686/2090 train_time:22795ms step_avg:33.23ms
step:687/2090 train_time:22854ms step_avg:33.27ms
step:688/2090 train_time:22913ms step_avg:33.30ms
step:689/2090 train_time:22973ms step_avg:33.34ms
step:690/2090 train_time:23032ms step_avg:33.38ms
step:691/2090 train_time:23092ms step_avg:33.42ms
step:692/2090 train_time:23151ms step_avg:33.46ms
step:693/2090 train_time:23211ms step_avg:33.49ms
step:694/2090 train_time:23270ms step_avg:33.53ms
step:695/2090 train_time:23330ms step_avg:33.57ms
step:696/2090 train_time:23389ms step_avg:33.61ms
step:697/2090 train_time:23449ms step_avg:33.64ms
step:698/2090 train_time:23509ms step_avg:33.68ms
step:699/2090 train_time:23569ms step_avg:33.72ms
step:700/2090 train_time:23628ms step_avg:33.75ms
step:701/2090 train_time:23689ms step_avg:33.79ms
step:702/2090 train_time:23748ms step_avg:33.83ms
step:703/2090 train_time:23808ms step_avg:33.87ms
step:704/2090 train_time:23867ms step_avg:33.90ms
step:705/2090 train_time:23928ms step_avg:33.94ms
step:706/2090 train_time:23987ms step_avg:33.98ms
step:707/2090 train_time:24048ms step_avg:34.01ms
step:708/2090 train_time:24107ms step_avg:34.05ms
step:709/2090 train_time:24168ms step_avg:34.09ms
step:710/2090 train_time:24227ms step_avg:34.12ms
step:711/2090 train_time:24287ms step_avg:34.16ms
step:712/2090 train_time:24347ms step_avg:34.19ms
step:713/2090 train_time:24407ms step_avg:34.23ms
step:714/2090 train_time:24466ms step_avg:34.27ms
step:715/2090 train_time:24527ms step_avg:34.30ms
step:716/2090 train_time:24586ms step_avg:34.34ms
step:717/2090 train_time:24646ms step_avg:34.37ms
step:718/2090 train_time:24706ms step_avg:34.41ms
step:719/2090 train_time:24766ms step_avg:34.45ms
step:720/2090 train_time:24826ms step_avg:34.48ms
step:721/2090 train_time:24887ms step_avg:34.52ms
step:722/2090 train_time:24946ms step_avg:34.55ms
step:723/2090 train_time:25007ms step_avg:34.59ms
step:724/2090 train_time:25067ms step_avg:34.62ms
step:725/2090 train_time:25127ms step_avg:34.66ms
step:726/2090 train_time:25186ms step_avg:34.69ms
step:727/2090 train_time:25247ms step_avg:34.73ms
step:728/2090 train_time:25306ms step_avg:34.76ms
step:729/2090 train_time:25366ms step_avg:34.80ms
step:730/2090 train_time:25425ms step_avg:34.83ms
step:731/2090 train_time:25485ms step_avg:34.86ms
step:732/2090 train_time:25544ms step_avg:34.90ms
step:733/2090 train_time:25605ms step_avg:34.93ms
step:734/2090 train_time:25665ms step_avg:34.97ms
step:735/2090 train_time:25725ms step_avg:35.00ms
step:736/2090 train_time:25785ms step_avg:35.03ms
step:737/2090 train_time:25846ms step_avg:35.07ms
step:738/2090 train_time:25905ms step_avg:35.10ms
step:739/2090 train_time:25966ms step_avg:35.14ms
step:740/2090 train_time:26025ms step_avg:35.17ms
step:741/2090 train_time:26086ms step_avg:35.20ms
step:742/2090 train_time:26146ms step_avg:35.24ms
step:743/2090 train_time:26206ms step_avg:35.27ms
step:744/2090 train_time:26266ms step_avg:35.30ms
step:745/2090 train_time:26326ms step_avg:35.34ms
step:746/2090 train_time:26385ms step_avg:35.37ms
step:747/2090 train_time:26446ms step_avg:35.40ms
step:748/2090 train_time:26505ms step_avg:35.43ms
step:749/2090 train_time:26566ms step_avg:35.47ms
step:750/2090 train_time:26626ms step_avg:35.50ms
step:750/2090 val_loss:3.8494 train_time:26688ms step_avg:35.58ms
step:751/2090 train_time:26709ms step_avg:35.56ms
step:752/2090 train_time:26748ms step_avg:35.57ms
step:753/2090 train_time:26810ms step_avg:35.60ms
step:754/2090 train_time:26870ms step_avg:35.64ms
step:755/2090 train_time:26932ms step_avg:35.67ms
step:756/2090 train_time:26991ms step_avg:35.70ms
step:757/2090 train_time:27051ms step_avg:35.73ms
step:758/2090 train_time:27109ms step_avg:35.76ms
step:759/2090 train_time:27169ms step_avg:35.80ms
step:760/2090 train_time:27227ms step_avg:35.83ms
step:761/2090 train_time:27287ms step_avg:35.86ms
step:762/2090 train_time:27345ms step_avg:35.89ms
step:763/2090 train_time:27404ms step_avg:35.92ms
step:764/2090 train_time:27463ms step_avg:35.95ms
step:765/2090 train_time:27522ms step_avg:35.98ms
step:766/2090 train_time:27581ms step_avg:36.01ms
step:767/2090 train_time:27641ms step_avg:36.04ms
step:768/2090 train_time:27701ms step_avg:36.07ms
step:769/2090 train_time:27763ms step_avg:36.10ms
step:770/2090 train_time:27823ms step_avg:36.13ms
step:771/2090 train_time:27884ms step_avg:36.17ms
step:772/2090 train_time:27943ms step_avg:36.20ms
step:773/2090 train_time:28003ms step_avg:36.23ms
step:774/2090 train_time:28062ms step_avg:36.26ms
step:775/2090 train_time:28122ms step_avg:36.29ms
step:776/2090 train_time:28181ms step_avg:36.32ms
step:777/2090 train_time:28241ms step_avg:36.35ms
step:778/2090 train_time:28300ms step_avg:36.38ms
step:779/2090 train_time:28360ms step_avg:36.41ms
step:780/2090 train_time:28418ms step_avg:36.43ms
step:781/2090 train_time:28478ms step_avg:36.46ms
step:782/2090 train_time:28537ms step_avg:36.49ms
step:783/2090 train_time:28597ms step_avg:36.52ms
step:784/2090 train_time:28657ms step_avg:36.55ms
step:785/2090 train_time:28717ms step_avg:36.58ms
step:786/2090 train_time:28777ms step_avg:36.61ms
step:787/2090 train_time:28838ms step_avg:36.64ms
step:788/2090 train_time:28897ms step_avg:36.67ms
step:789/2090 train_time:28958ms step_avg:36.70ms
step:790/2090 train_time:29018ms step_avg:36.73ms
step:791/2090 train_time:29079ms step_avg:36.76ms
step:792/2090 train_time:29139ms step_avg:36.79ms
step:793/2090 train_time:29199ms step_avg:36.82ms
step:794/2090 train_time:29258ms step_avg:36.85ms
step:795/2090 train_time:29319ms step_avg:36.88ms
step:796/2090 train_time:29379ms step_avg:36.91ms
step:797/2090 train_time:29439ms step_avg:36.94ms
step:798/2090 train_time:29498ms step_avg:36.96ms
step:799/2090 train_time:29558ms step_avg:36.99ms
step:800/2090 train_time:29617ms step_avg:37.02ms
step:801/2090 train_time:29677ms step_avg:37.05ms
step:802/2090 train_time:29737ms step_avg:37.08ms
step:803/2090 train_time:29799ms step_avg:37.11ms
step:804/2090 train_time:29859ms step_avg:37.14ms
step:805/2090 train_time:29920ms step_avg:37.17ms
step:806/2090 train_time:29979ms step_avg:37.20ms
step:807/2090 train_time:30040ms step_avg:37.22ms
step:808/2090 train_time:30100ms step_avg:37.25ms
step:809/2090 train_time:30160ms step_avg:37.28ms
step:810/2090 train_time:30219ms step_avg:37.31ms
step:811/2090 train_time:30279ms step_avg:37.34ms
step:812/2090 train_time:30338ms step_avg:37.36ms
step:813/2090 train_time:30398ms step_avg:37.39ms
step:814/2090 train_time:30457ms step_avg:37.42ms
step:815/2090 train_time:30518ms step_avg:37.44ms
step:816/2090 train_time:30577ms step_avg:37.47ms
step:817/2090 train_time:30637ms step_avg:37.50ms
step:818/2090 train_time:30696ms step_avg:37.53ms
step:819/2090 train_time:30757ms step_avg:37.55ms
step:820/2090 train_time:30817ms step_avg:37.58ms
step:821/2090 train_time:30877ms step_avg:37.61ms
step:822/2090 train_time:30938ms step_avg:37.64ms
step:823/2090 train_time:30999ms step_avg:37.67ms
step:824/2090 train_time:31058ms step_avg:37.69ms
step:825/2090 train_time:31118ms step_avg:37.72ms
step:826/2090 train_time:31178ms step_avg:37.75ms
step:827/2090 train_time:31238ms step_avg:37.77ms
step:828/2090 train_time:31297ms step_avg:37.80ms
step:829/2090 train_time:31358ms step_avg:37.83ms
step:830/2090 train_time:31418ms step_avg:37.85ms
step:831/2090 train_time:31478ms step_avg:37.88ms
step:832/2090 train_time:31537ms step_avg:37.91ms
step:833/2090 train_time:31598ms step_avg:37.93ms
step:834/2090 train_time:31657ms step_avg:37.96ms
step:835/2090 train_time:31718ms step_avg:37.99ms
step:836/2090 train_time:31778ms step_avg:38.01ms
step:837/2090 train_time:31838ms step_avg:38.04ms
step:838/2090 train_time:31898ms step_avg:38.06ms
step:839/2090 train_time:31958ms step_avg:38.09ms
step:840/2090 train_time:32018ms step_avg:38.12ms
step:841/2090 train_time:32079ms step_avg:38.14ms
step:842/2090 train_time:32139ms step_avg:38.17ms
step:843/2090 train_time:32199ms step_avg:38.20ms
step:844/2090 train_time:32259ms step_avg:38.22ms
step:845/2090 train_time:32319ms step_avg:38.25ms
step:846/2090 train_time:32378ms step_avg:38.27ms
step:847/2090 train_time:32439ms step_avg:38.30ms
step:848/2090 train_time:32498ms step_avg:38.32ms
step:849/2090 train_time:32559ms step_avg:38.35ms
step:850/2090 train_time:32618ms step_avg:38.37ms
step:851/2090 train_time:32679ms step_avg:38.40ms
step:852/2090 train_time:32738ms step_avg:38.42ms
step:853/2090 train_time:32798ms step_avg:38.45ms
step:854/2090 train_time:32858ms step_avg:38.47ms
step:855/2090 train_time:32919ms step_avg:38.50ms
step:856/2090 train_time:32979ms step_avg:38.53ms
step:857/2090 train_time:33039ms step_avg:38.55ms
step:858/2090 train_time:33099ms step_avg:38.58ms
step:859/2090 train_time:33159ms step_avg:38.60ms
step:860/2090 train_time:33219ms step_avg:38.63ms
step:861/2090 train_time:33279ms step_avg:38.65ms
step:862/2090 train_time:33338ms step_avg:38.68ms
step:863/2090 train_time:33398ms step_avg:38.70ms
step:864/2090 train_time:33458ms step_avg:38.72ms
step:865/2090 train_time:33518ms step_avg:38.75ms
step:866/2090 train_time:33577ms step_avg:38.77ms
step:867/2090 train_time:33638ms step_avg:38.80ms
step:868/2090 train_time:33698ms step_avg:38.82ms
step:869/2090 train_time:33759ms step_avg:38.85ms
step:870/2090 train_time:33820ms step_avg:38.87ms
step:871/2090 train_time:33881ms step_avg:38.90ms
step:872/2090 train_time:33940ms step_avg:38.92ms
step:873/2090 train_time:34000ms step_avg:38.95ms
step:874/2090 train_time:34060ms step_avg:38.97ms
step:875/2090 train_time:34121ms step_avg:38.99ms
step:876/2090 train_time:34180ms step_avg:39.02ms
step:877/2090 train_time:34241ms step_avg:39.04ms
step:878/2090 train_time:34299ms step_avg:39.07ms
step:879/2090 train_time:34360ms step_avg:39.09ms
step:880/2090 train_time:34420ms step_avg:39.11ms
step:881/2090 train_time:34480ms step_avg:39.14ms
step:882/2090 train_time:34540ms step_avg:39.16ms
step:883/2090 train_time:34600ms step_avg:39.18ms
step:884/2090 train_time:34659ms step_avg:39.21ms
step:885/2090 train_time:34720ms step_avg:39.23ms
step:886/2090 train_time:34780ms step_avg:39.26ms
step:887/2090 train_time:34841ms step_avg:39.28ms
step:888/2090 train_time:34900ms step_avg:39.30ms
step:889/2090 train_time:34961ms step_avg:39.33ms
step:890/2090 train_time:35020ms step_avg:39.35ms
step:891/2090 train_time:35081ms step_avg:39.37ms
step:892/2090 train_time:35141ms step_avg:39.40ms
step:893/2090 train_time:35201ms step_avg:39.42ms
step:894/2090 train_time:35260ms step_avg:39.44ms
step:895/2090 train_time:35321ms step_avg:39.46ms
step:896/2090 train_time:35380ms step_avg:39.49ms
step:897/2090 train_time:35440ms step_avg:39.51ms
step:898/2090 train_time:35500ms step_avg:39.53ms
step:899/2090 train_time:35560ms step_avg:39.56ms
step:900/2090 train_time:35620ms step_avg:39.58ms
step:901/2090 train_time:35680ms step_avg:39.60ms
step:902/2090 train_time:35740ms step_avg:39.62ms
step:903/2090 train_time:35801ms step_avg:39.65ms
step:904/2090 train_time:35860ms step_avg:39.67ms
step:905/2090 train_time:35921ms step_avg:39.69ms
step:906/2090 train_time:35981ms step_avg:39.71ms
step:907/2090 train_time:36041ms step_avg:39.74ms
step:908/2090 train_time:36100ms step_avg:39.76ms
step:909/2090 train_time:36160ms step_avg:39.78ms
step:910/2090 train_time:36219ms step_avg:39.80ms
step:911/2090 train_time:36279ms step_avg:39.82ms
step:912/2090 train_time:36338ms step_avg:39.84ms
step:913/2090 train_time:36399ms step_avg:39.87ms
step:914/2090 train_time:36458ms step_avg:39.89ms
step:915/2090 train_time:36519ms step_avg:39.91ms
step:916/2090 train_time:36579ms step_avg:39.93ms
step:917/2090 train_time:36639ms step_avg:39.96ms
step:918/2090 train_time:36699ms step_avg:39.98ms
step:919/2090 train_time:36759ms step_avg:40.00ms
step:920/2090 train_time:36819ms step_avg:40.02ms
step:921/2090 train_time:36880ms step_avg:40.04ms
step:922/2090 train_time:36939ms step_avg:40.06ms
step:923/2090 train_time:36999ms step_avg:40.09ms
step:924/2090 train_time:37059ms step_avg:40.11ms
step:925/2090 train_time:37119ms step_avg:40.13ms
step:926/2090 train_time:37178ms step_avg:40.15ms
step:927/2090 train_time:37239ms step_avg:40.17ms
step:928/2090 train_time:37298ms step_avg:40.19ms
step:929/2090 train_time:37358ms step_avg:40.21ms
step:930/2090 train_time:37418ms step_avg:40.23ms
step:931/2090 train_time:37478ms step_avg:40.26ms
step:932/2090 train_time:37537ms step_avg:40.28ms
step:933/2090 train_time:37598ms step_avg:40.30ms
step:934/2090 train_time:37657ms step_avg:40.32ms
step:935/2090 train_time:37718ms step_avg:40.34ms
step:936/2090 train_time:37778ms step_avg:40.36ms
step:937/2090 train_time:37839ms step_avg:40.38ms
step:938/2090 train_time:37899ms step_avg:40.40ms
step:939/2090 train_time:37960ms step_avg:40.43ms
step:940/2090 train_time:38019ms step_avg:40.45ms
step:941/2090 train_time:38080ms step_avg:40.47ms
step:942/2090 train_time:38139ms step_avg:40.49ms
step:943/2090 train_time:38200ms step_avg:40.51ms
step:944/2090 train_time:38260ms step_avg:40.53ms
step:945/2090 train_time:38320ms step_avg:40.55ms
step:946/2090 train_time:38379ms step_avg:40.57ms
step:947/2090 train_time:38440ms step_avg:40.59ms
step:948/2090 train_time:38500ms step_avg:40.61ms
step:949/2090 train_time:38560ms step_avg:40.63ms
step:950/2090 train_time:38620ms step_avg:40.65ms
step:951/2090 train_time:38681ms step_avg:40.67ms
step:952/2090 train_time:38740ms step_avg:40.69ms
step:953/2090 train_time:38801ms step_avg:40.71ms
step:954/2090 train_time:38860ms step_avg:40.73ms
step:955/2090 train_time:38920ms step_avg:40.75ms
step:956/2090 train_time:38980ms step_avg:40.77ms
step:957/2090 train_time:39041ms step_avg:40.80ms
step:958/2090 train_time:39100ms step_avg:40.81ms
step:959/2090 train_time:39160ms step_avg:40.83ms
step:960/2090 train_time:39219ms step_avg:40.85ms
step:961/2090 train_time:39279ms step_avg:40.87ms
step:962/2090 train_time:39339ms step_avg:40.89ms
step:963/2090 train_time:39399ms step_avg:40.91ms
step:964/2090 train_time:39459ms step_avg:40.93ms
step:965/2090 train_time:39520ms step_avg:40.95ms
step:966/2090 train_time:39579ms step_avg:40.97ms
step:967/2090 train_time:39640ms step_avg:40.99ms
step:968/2090 train_time:39699ms step_avg:41.01ms
step:969/2090 train_time:39760ms step_avg:41.03ms
step:970/2090 train_time:39820ms step_avg:41.05ms
step:971/2090 train_time:39879ms step_avg:41.07ms
step:972/2090 train_time:39940ms step_avg:41.09ms
step:973/2090 train_time:40000ms step_avg:41.11ms
step:974/2090 train_time:40059ms step_avg:41.13ms
step:975/2090 train_time:40120ms step_avg:41.15ms
step:976/2090 train_time:40179ms step_avg:41.17ms
step:977/2090 train_time:40239ms step_avg:41.19ms
step:978/2090 train_time:40298ms step_avg:41.20ms
step:979/2090 train_time:40359ms step_avg:41.22ms
step:980/2090 train_time:40418ms step_avg:41.24ms
step:981/2090 train_time:40479ms step_avg:41.26ms
step:982/2090 train_time:40538ms step_avg:41.28ms
step:983/2090 train_time:40599ms step_avg:41.30ms
step:984/2090 train_time:40658ms step_avg:41.32ms
step:985/2090 train_time:40719ms step_avg:41.34ms
step:986/2090 train_time:40779ms step_avg:41.36ms
step:987/2090 train_time:40840ms step_avg:41.38ms
step:988/2090 train_time:40899ms step_avg:41.40ms
step:989/2090 train_time:40959ms step_avg:41.41ms
step:990/2090 train_time:41019ms step_avg:41.43ms
step:991/2090 train_time:41080ms step_avg:41.45ms
step:992/2090 train_time:41139ms step_avg:41.47ms
step:993/2090 train_time:41199ms step_avg:41.49ms
step:994/2090 train_time:41259ms step_avg:41.51ms
step:995/2090 train_time:41320ms step_avg:41.53ms
step:996/2090 train_time:41379ms step_avg:41.55ms
step:997/2090 train_time:41440ms step_avg:41.56ms
step:998/2090 train_time:41499ms step_avg:41.58ms
step:999/2090 train_time:41560ms step_avg:41.60ms
step:1000/2090 train_time:41619ms step_avg:41.62ms
step:1000/2090 val_loss:3.7053 train_time:41682ms step_avg:41.68ms
step:1001/2090 train_time:41702ms step_avg:41.66ms
step:1002/2090 train_time:41741ms step_avg:41.66ms
step:1003/2090 train_time:41803ms step_avg:41.68ms
step:1004/2090 train_time:41864ms step_avg:41.70ms
step:1005/2090 train_time:41924ms step_avg:41.72ms
step:1006/2090 train_time:41983ms step_avg:41.73ms
step:1007/2090 train_time:42043ms step_avg:41.75ms
step:1008/2090 train_time:42101ms step_avg:41.77ms
step:1009/2090 train_time:42161ms step_avg:41.78ms
step:1010/2090 train_time:42219ms step_avg:41.80ms
step:1011/2090 train_time:42278ms step_avg:41.82ms
step:1012/2090 train_time:42337ms step_avg:41.83ms
step:1013/2090 train_time:42396ms step_avg:41.85ms
step:1014/2090 train_time:42455ms step_avg:41.87ms
step:1015/2090 train_time:42515ms step_avg:41.89ms
step:1016/2090 train_time:42574ms step_avg:41.90ms
step:1017/2090 train_time:42636ms step_avg:41.92ms
step:1018/2090 train_time:42697ms step_avg:41.94ms
step:1019/2090 train_time:42758ms step_avg:41.96ms
step:1020/2090 train_time:42818ms step_avg:41.98ms
step:1021/2090 train_time:42880ms step_avg:42.00ms
step:1022/2090 train_time:42940ms step_avg:42.02ms
step:1023/2090 train_time:43000ms step_avg:42.03ms
step:1024/2090 train_time:43058ms step_avg:42.05ms
step:1025/2090 train_time:43118ms step_avg:42.07ms
step:1026/2090 train_time:43176ms step_avg:42.08ms
step:1027/2090 train_time:43236ms step_avg:42.10ms
step:1028/2090 train_time:43295ms step_avg:42.12ms
step:1029/2090 train_time:43354ms step_avg:42.13ms
step:1030/2090 train_time:43413ms step_avg:42.15ms
step:1031/2090 train_time:43474ms step_avg:42.17ms
step:1032/2090 train_time:43533ms step_avg:42.18ms
step:1033/2090 train_time:43594ms step_avg:42.20ms
step:1034/2090 train_time:43653ms step_avg:42.22ms
step:1035/2090 train_time:43715ms step_avg:42.24ms
step:1036/2090 train_time:43775ms step_avg:42.25ms
step:1037/2090 train_time:43837ms step_avg:42.27ms
step:1038/2090 train_time:43898ms step_avg:42.29ms
step:1039/2090 train_time:43958ms step_avg:42.31ms
step:1040/2090 train_time:44017ms step_avg:42.32ms
step:1041/2090 train_time:44078ms step_avg:42.34ms
step:1042/2090 train_time:44137ms step_avg:42.36ms
step:1043/2090 train_time:44196ms step_avg:42.37ms
step:1044/2090 train_time:44256ms step_avg:42.39ms
step:1045/2090 train_time:44315ms step_avg:42.41ms
step:1046/2090 train_time:44374ms step_avg:42.42ms
step:1047/2090 train_time:44434ms step_avg:42.44ms
step:1048/2090 train_time:44493ms step_avg:42.45ms
step:1049/2090 train_time:44553ms step_avg:42.47ms
step:1050/2090 train_time:44612ms step_avg:42.49ms
step:1051/2090 train_time:44674ms step_avg:42.51ms
step:1052/2090 train_time:44734ms step_avg:42.52ms
step:1053/2090 train_time:44795ms step_avg:42.54ms
step:1054/2090 train_time:44855ms step_avg:42.56ms
step:1055/2090 train_time:44917ms step_avg:42.58ms
step:1056/2090 train_time:44977ms step_avg:42.59ms
step:1057/2090 train_time:45038ms step_avg:42.61ms
step:1058/2090 train_time:45098ms step_avg:42.63ms
step:1059/2090 train_time:45157ms step_avg:42.64ms
step:1060/2090 train_time:45216ms step_avg:42.66ms
step:1061/2090 train_time:45277ms step_avg:42.67ms
step:1062/2090 train_time:45335ms step_avg:42.69ms
step:1063/2090 train_time:45395ms step_avg:42.70ms
step:1064/2090 train_time:45454ms step_avg:42.72ms
step:1065/2090 train_time:45514ms step_avg:42.74ms
step:1066/2090 train_time:45574ms step_avg:42.75ms
step:1067/2090 train_time:45634ms step_avg:42.77ms
step:1068/2090 train_time:45694ms step_avg:42.79ms
step:1069/2090 train_time:45755ms step_avg:42.80ms
step:1070/2090 train_time:45815ms step_avg:42.82ms
step:1071/2090 train_time:45877ms step_avg:42.84ms
step:1072/2090 train_time:45937ms step_avg:42.85ms
step:1073/2090 train_time:45997ms step_avg:42.87ms
step:1074/2090 train_time:46056ms step_avg:42.88ms
step:1075/2090 train_time:46117ms step_avg:42.90ms
step:1076/2090 train_time:46176ms step_avg:42.91ms
step:1077/2090 train_time:46236ms step_avg:42.93ms
step:1078/2090 train_time:46295ms step_avg:42.95ms
step:1079/2090 train_time:46355ms step_avg:42.96ms
step:1080/2090 train_time:46414ms step_avg:42.98ms
step:1081/2090 train_time:46474ms step_avg:42.99ms
step:1082/2090 train_time:46533ms step_avg:43.01ms
step:1083/2090 train_time:46594ms step_avg:43.02ms
step:1084/2090 train_time:46653ms step_avg:43.04ms
step:1085/2090 train_time:46714ms step_avg:43.05ms
step:1086/2090 train_time:46775ms step_avg:43.07ms
step:1087/2090 train_time:46835ms step_avg:43.09ms
step:1088/2090 train_time:46895ms step_avg:43.10ms
step:1089/2090 train_time:46955ms step_avg:43.12ms
step:1090/2090 train_time:47015ms step_avg:43.13ms
step:1091/2090 train_time:47076ms step_avg:43.15ms
step:1092/2090 train_time:47136ms step_avg:43.16ms
step:1093/2090 train_time:47196ms step_avg:43.18ms
step:1094/2090 train_time:47255ms step_avg:43.19ms
step:1095/2090 train_time:47315ms step_avg:43.21ms
step:1096/2090 train_time:47375ms step_avg:43.22ms
step:1097/2090 train_time:47434ms step_avg:43.24ms
step:1098/2090 train_time:47493ms step_avg:43.25ms
step:1099/2090 train_time:47553ms step_avg:43.27ms
step:1100/2090 train_time:47612ms step_avg:43.28ms
step:1101/2090 train_time:47673ms step_avg:43.30ms
step:1102/2090 train_time:47733ms step_avg:43.31ms
step:1103/2090 train_time:47794ms step_avg:43.33ms
step:1104/2090 train_time:47854ms step_avg:43.35ms
step:1105/2090 train_time:47915ms step_avg:43.36ms
step:1106/2090 train_time:47975ms step_avg:43.38ms
step:1107/2090 train_time:48036ms step_avg:43.39ms
step:1108/2090 train_time:48095ms step_avg:43.41ms
step:1109/2090 train_time:48156ms step_avg:43.42ms
step:1110/2090 train_time:48215ms step_avg:43.44ms
step:1111/2090 train_time:48276ms step_avg:43.45ms
step:1112/2090 train_time:48335ms step_avg:43.47ms
step:1113/2090 train_time:48395ms step_avg:43.48ms
step:1114/2090 train_time:48453ms step_avg:43.50ms
step:1115/2090 train_time:48514ms step_avg:43.51ms
step:1116/2090 train_time:48574ms step_avg:43.53ms
step:1117/2090 train_time:48635ms step_avg:43.54ms
step:1118/2090 train_time:48694ms step_avg:43.55ms
step:1119/2090 train_time:48755ms step_avg:43.57ms
step:1120/2090 train_time:48814ms step_avg:43.58ms
step:1121/2090 train_time:48875ms step_avg:43.60ms
step:1122/2090 train_time:48935ms step_avg:43.61ms
step:1123/2090 train_time:48995ms step_avg:43.63ms
step:1124/2090 train_time:49055ms step_avg:43.64ms
step:1125/2090 train_time:49115ms step_avg:43.66ms
step:1126/2090 train_time:49174ms step_avg:43.67ms
step:1127/2090 train_time:49235ms step_avg:43.69ms
step:1128/2090 train_time:49294ms step_avg:43.70ms
step:1129/2090 train_time:49354ms step_avg:43.72ms
step:1130/2090 train_time:49413ms step_avg:43.73ms
step:1131/2090 train_time:49473ms step_avg:43.74ms
step:1132/2090 train_time:49533ms step_avg:43.76ms
step:1133/2090 train_time:49594ms step_avg:43.77ms
step:1134/2090 train_time:49653ms step_avg:43.79ms
step:1135/2090 train_time:49714ms step_avg:43.80ms
step:1136/2090 train_time:49773ms step_avg:43.81ms
step:1137/2090 train_time:49834ms step_avg:43.83ms
step:1138/2090 train_time:49895ms step_avg:43.84ms
step:1139/2090 train_time:49955ms step_avg:43.86ms
step:1140/2090 train_time:50015ms step_avg:43.87ms
step:1141/2090 train_time:50076ms step_avg:43.89ms
step:1142/2090 train_time:50135ms step_avg:43.90ms
step:1143/2090 train_time:50195ms step_avg:43.92ms
step:1144/2090 train_time:50254ms step_avg:43.93ms
step:1145/2090 train_time:50314ms step_avg:43.94ms
step:1146/2090 train_time:50373ms step_avg:43.96ms
step:1147/2090 train_time:50434ms step_avg:43.97ms
step:1148/2090 train_time:50493ms step_avg:43.98ms
step:1149/2090 train_time:50553ms step_avg:44.00ms
step:1150/2090 train_time:50612ms step_avg:44.01ms
step:1151/2090 train_time:50673ms step_avg:44.03ms
step:1152/2090 train_time:50733ms step_avg:44.04ms
step:1153/2090 train_time:50793ms step_avg:44.05ms
step:1154/2090 train_time:50853ms step_avg:44.07ms
step:1155/2090 train_time:50915ms step_avg:44.08ms
step:1156/2090 train_time:50974ms step_avg:44.10ms
step:1157/2090 train_time:51035ms step_avg:44.11ms
step:1158/2090 train_time:51094ms step_avg:44.12ms
step:1159/2090 train_time:51155ms step_avg:44.14ms
step:1160/2090 train_time:51214ms step_avg:44.15ms
step:1161/2090 train_time:51274ms step_avg:44.16ms
step:1162/2090 train_time:51334ms step_avg:44.18ms
step:1163/2090 train_time:51394ms step_avg:44.19ms
step:1164/2090 train_time:51453ms step_avg:44.20ms
step:1165/2090 train_time:51514ms step_avg:44.22ms
step:1166/2090 train_time:51574ms step_avg:44.23ms
step:1167/2090 train_time:51634ms step_avg:44.25ms
step:1168/2090 train_time:51693ms step_avg:44.26ms
step:1169/2090 train_time:51754ms step_avg:44.27ms
step:1170/2090 train_time:51813ms step_avg:44.28ms
step:1171/2090 train_time:51874ms step_avg:44.30ms
step:1172/2090 train_time:51934ms step_avg:44.31ms
step:1173/2090 train_time:51995ms step_avg:44.33ms
step:1174/2090 train_time:52054ms step_avg:44.34ms
step:1175/2090 train_time:52115ms step_avg:44.35ms
step:1176/2090 train_time:52174ms step_avg:44.37ms
step:1177/2090 train_time:52235ms step_avg:44.38ms
step:1178/2090 train_time:52294ms step_avg:44.39ms
step:1179/2090 train_time:52355ms step_avg:44.41ms
step:1180/2090 train_time:52414ms step_avg:44.42ms
step:1181/2090 train_time:52475ms step_avg:44.43ms
step:1182/2090 train_time:52535ms step_avg:44.45ms
step:1183/2090 train_time:52596ms step_avg:44.46ms
step:1184/2090 train_time:52655ms step_avg:44.47ms
step:1185/2090 train_time:52715ms step_avg:44.49ms
step:1186/2090 train_time:52774ms step_avg:44.50ms
step:1187/2090 train_time:52835ms step_avg:44.51ms
step:1188/2090 train_time:52895ms step_avg:44.52ms
step:1189/2090 train_time:52956ms step_avg:44.54ms
step:1190/2090 train_time:53015ms step_avg:44.55ms
step:1191/2090 train_time:53076ms step_avg:44.56ms
step:1192/2090 train_time:53135ms step_avg:44.58ms
step:1193/2090 train_time:53196ms step_avg:44.59ms
step:1194/2090 train_time:53254ms step_avg:44.60ms
step:1195/2090 train_time:53314ms step_avg:44.61ms
step:1196/2090 train_time:53374ms step_avg:44.63ms
step:1197/2090 train_time:53435ms step_avg:44.64ms
step:1198/2090 train_time:53495ms step_avg:44.65ms
step:1199/2090 train_time:53556ms step_avg:44.67ms
step:1200/2090 train_time:53615ms step_avg:44.68ms
step:1201/2090 train_time:53676ms step_avg:44.69ms
step:1202/2090 train_time:53735ms step_avg:44.70ms
step:1203/2090 train_time:53796ms step_avg:44.72ms
step:1204/2090 train_time:53856ms step_avg:44.73ms
step:1205/2090 train_time:53916ms step_avg:44.74ms
step:1206/2090 train_time:53976ms step_avg:44.76ms
step:1207/2090 train_time:54037ms step_avg:44.77ms
step:1208/2090 train_time:54097ms step_avg:44.78ms
step:1209/2090 train_time:54157ms step_avg:44.80ms
step:1210/2090 train_time:54216ms step_avg:44.81ms
step:1211/2090 train_time:54277ms step_avg:44.82ms
step:1212/2090 train_time:54337ms step_avg:44.83ms
step:1213/2090 train_time:54397ms step_avg:44.85ms
step:1214/2090 train_time:54456ms step_avg:44.86ms
step:1215/2090 train_time:54517ms step_avg:44.87ms
step:1216/2090 train_time:54576ms step_avg:44.88ms
step:1217/2090 train_time:54637ms step_avg:44.89ms
step:1218/2090 train_time:54696ms step_avg:44.91ms
step:1219/2090 train_time:54757ms step_avg:44.92ms
step:1220/2090 train_time:54816ms step_avg:44.93ms
step:1221/2090 train_time:54877ms step_avg:44.94ms
step:1222/2090 train_time:54936ms step_avg:44.96ms
step:1223/2090 train_time:54997ms step_avg:44.97ms
step:1224/2090 train_time:55057ms step_avg:44.98ms
step:1225/2090 train_time:55117ms step_avg:44.99ms
step:1226/2090 train_time:55176ms step_avg:45.01ms
step:1227/2090 train_time:55237ms step_avg:45.02ms
step:1228/2090 train_time:55296ms step_avg:45.03ms
step:1229/2090 train_time:55356ms step_avg:45.04ms
step:1230/2090 train_time:55415ms step_avg:45.05ms
step:1231/2090 train_time:55476ms step_avg:45.07ms
step:1232/2090 train_time:55535ms step_avg:45.08ms
step:1233/2090 train_time:55596ms step_avg:45.09ms
step:1234/2090 train_time:55656ms step_avg:45.10ms
step:1235/2090 train_time:55717ms step_avg:45.11ms
step:1236/2090 train_time:55776ms step_avg:45.13ms
step:1237/2090 train_time:55836ms step_avg:45.14ms
step:1238/2090 train_time:55896ms step_avg:45.15ms
step:1239/2090 train_time:55956ms step_avg:45.16ms
step:1240/2090 train_time:56016ms step_avg:45.17ms
step:1241/2090 train_time:56076ms step_avg:45.19ms
step:1242/2090 train_time:56136ms step_avg:45.20ms
step:1243/2090 train_time:56196ms step_avg:45.21ms
step:1244/2090 train_time:56256ms step_avg:45.22ms
step:1245/2090 train_time:56316ms step_avg:45.23ms
step:1246/2090 train_time:56375ms step_avg:45.24ms
step:1247/2090 train_time:56436ms step_avg:45.26ms
step:1248/2090 train_time:56495ms step_avg:45.27ms
step:1249/2090 train_time:56556ms step_avg:45.28ms
step:1250/2090 train_time:56616ms step_avg:45.29ms
step:1250/2090 val_loss:3.5849 train_time:56678ms step_avg:45.34ms
step:1251/2090 train_time:56699ms step_avg:45.32ms
step:1252/2090 train_time:56737ms step_avg:45.32ms
step:1253/2090 train_time:56801ms step_avg:45.33ms
step:1254/2090 train_time:56862ms step_avg:45.34ms
step:1255/2090 train_time:56923ms step_avg:45.36ms
step:1256/2090 train_time:56982ms step_avg:45.37ms
step:1257/2090 train_time:57042ms step_avg:45.38ms
step:1258/2090 train_time:57100ms step_avg:45.39ms
step:1259/2090 train_time:57160ms step_avg:45.40ms
step:1260/2090 train_time:57219ms step_avg:45.41ms
step:1261/2090 train_time:57278ms step_avg:45.42ms
step:1262/2090 train_time:57337ms step_avg:45.43ms
step:1263/2090 train_time:57397ms step_avg:45.44ms
step:1264/2090 train_time:57455ms step_avg:45.46ms
step:1265/2090 train_time:57515ms step_avg:45.47ms
step:1266/2090 train_time:57575ms step_avg:45.48ms
step:1267/2090 train_time:57637ms step_avg:45.49ms
step:1268/2090 train_time:57697ms step_avg:45.50ms
step:1269/2090 train_time:57759ms step_avg:45.52ms
step:1270/2090 train_time:57820ms step_avg:45.53ms
step:1271/2090 train_time:57881ms step_avg:45.54ms
step:1272/2090 train_time:57941ms step_avg:45.55ms
step:1273/2090 train_time:58001ms step_avg:45.56ms
step:1274/2090 train_time:58060ms step_avg:45.57ms
step:1275/2090 train_time:58120ms step_avg:45.58ms
step:1276/2090 train_time:58179ms step_avg:45.59ms
step:1277/2090 train_time:58239ms step_avg:45.61ms
step:1278/2090 train_time:58297ms step_avg:45.62ms
step:1279/2090 train_time:58357ms step_avg:45.63ms
step:1280/2090 train_time:58415ms step_avg:45.64ms
step:1281/2090 train_time:58475ms step_avg:45.65ms
step:1282/2090 train_time:58535ms step_avg:45.66ms
step:1283/2090 train_time:58596ms step_avg:45.67ms
step:1284/2090 train_time:58656ms step_avg:45.68ms
step:1285/2090 train_time:58718ms step_avg:45.69ms
step:1286/2090 train_time:58778ms step_avg:45.71ms
step:1287/2090 train_time:58839ms step_avg:45.72ms
step:1288/2090 train_time:58899ms step_avg:45.73ms
step:1289/2090 train_time:58960ms step_avg:45.74ms
step:1290/2090 train_time:59019ms step_avg:45.75ms
step:1291/2090 train_time:59079ms step_avg:45.76ms
step:1292/2090 train_time:59139ms step_avg:45.77ms
step:1293/2090 train_time:59199ms step_avg:45.78ms
step:1294/2090 train_time:59258ms step_avg:45.79ms
step:1295/2090 train_time:59318ms step_avg:45.81ms
step:1296/2090 train_time:59377ms step_avg:45.82ms
step:1297/2090 train_time:59436ms step_avg:45.83ms
step:1298/2090 train_time:59495ms step_avg:45.84ms
step:1299/2090 train_time:59555ms step_avg:45.85ms
step:1300/2090 train_time:59615ms step_avg:45.86ms
step:1301/2090 train_time:59677ms step_avg:45.87ms
step:1302/2090 train_time:59737ms step_avg:45.88ms
step:1303/2090 train_time:59798ms step_avg:45.89ms
step:1304/2090 train_time:59857ms step_avg:45.90ms
step:1305/2090 train_time:59918ms step_avg:45.91ms
step:1306/2090 train_time:59978ms step_avg:45.93ms
step:1307/2090 train_time:60039ms step_avg:45.94ms
step:1308/2090 train_time:60099ms step_avg:45.95ms
step:1309/2090 train_time:60159ms step_avg:45.96ms
step:1310/2090 train_time:60218ms step_avg:45.97ms
step:1311/2090 train_time:60279ms step_avg:45.98ms
step:1312/2090 train_time:60337ms step_avg:45.99ms
step:1313/2090 train_time:60397ms step_avg:46.00ms
step:1314/2090 train_time:60455ms step_avg:46.01ms
step:1315/2090 train_time:60515ms step_avg:46.02ms
step:1316/2090 train_time:60575ms step_avg:46.03ms
step:1317/2090 train_time:60636ms step_avg:46.04ms
step:1318/2090 train_time:60696ms step_avg:46.05ms
step:1319/2090 train_time:60756ms step_avg:46.06ms
step:1320/2090 train_time:60816ms step_avg:46.07ms
step:1321/2090 train_time:60877ms step_avg:46.08ms
step:1322/2090 train_time:60938ms step_avg:46.10ms
step:1323/2090 train_time:60999ms step_avg:46.11ms
step:1324/2090 train_time:61058ms step_avg:46.12ms
step:1325/2090 train_time:61119ms step_avg:46.13ms
step:1326/2090 train_time:61178ms step_avg:46.14ms
step:1327/2090 train_time:61238ms step_avg:46.15ms
step:1328/2090 train_time:61297ms step_avg:46.16ms
step:1329/2090 train_time:61357ms step_avg:46.17ms
step:1330/2090 train_time:61416ms step_avg:46.18ms
step:1331/2090 train_time:61476ms step_avg:46.19ms
step:1332/2090 train_time:61535ms step_avg:46.20ms
step:1333/2090 train_time:61595ms step_avg:46.21ms
step:1334/2090 train_time:61655ms step_avg:46.22ms
step:1335/2090 train_time:61715ms step_avg:46.23ms
step:1336/2090 train_time:61776ms step_avg:46.24ms
step:1337/2090 train_time:61836ms step_avg:46.25ms
step:1338/2090 train_time:61896ms step_avg:46.26ms
step:1339/2090 train_time:61957ms step_avg:46.27ms
step:1340/2090 train_time:62017ms step_avg:46.28ms
step:1341/2090 train_time:62077ms step_avg:46.29ms
step:1342/2090 train_time:62137ms step_avg:46.30ms
step:1343/2090 train_time:62197ms step_avg:46.31ms
step:1344/2090 train_time:62256ms step_avg:46.32ms
step:1345/2090 train_time:62317ms step_avg:46.33ms
step:1346/2090 train_time:62376ms step_avg:46.34ms
step:1347/2090 train_time:62437ms step_avg:46.35ms
step:1348/2090 train_time:62496ms step_avg:46.36ms
step:1349/2090 train_time:62556ms step_avg:46.37ms
step:1350/2090 train_time:62616ms step_avg:46.38ms
step:1351/2090 train_time:62676ms step_avg:46.39ms
step:1352/2090 train_time:62736ms step_avg:46.40ms
step:1353/2090 train_time:62796ms step_avg:46.41ms
step:1354/2090 train_time:62856ms step_avg:46.42ms
step:1355/2090 train_time:62917ms step_avg:46.43ms
step:1356/2090 train_time:62977ms step_avg:46.44ms
step:1357/2090 train_time:63037ms step_avg:46.45ms
step:1358/2090 train_time:63097ms step_avg:46.46ms
step:1359/2090 train_time:63157ms step_avg:46.47ms
step:1360/2090 train_time:63217ms step_avg:46.48ms
step:1361/2090 train_time:63278ms step_avg:46.49ms
step:1362/2090 train_time:63337ms step_avg:46.50ms
step:1363/2090 train_time:63398ms step_avg:46.51ms
step:1364/2090 train_time:63457ms step_avg:46.52ms
step:1365/2090 train_time:63517ms step_avg:46.53ms
step:1366/2090 train_time:63577ms step_avg:46.54ms
step:1367/2090 train_time:63637ms step_avg:46.55ms
step:1368/2090 train_time:63697ms step_avg:46.56ms
step:1369/2090 train_time:63786ms step_avg:46.59ms
step:1370/2090 train_time:63873ms step_avg:46.62ms
step:1371/2090 train_time:63962ms step_avg:46.65ms
step:1372/2090 train_time:64049ms step_avg:46.68ms
step:1373/2090 train_time:64137ms step_avg:46.71ms
step:1374/2090 train_time:64223ms step_avg:46.74ms
step:1375/2090 train_time:64312ms step_avg:46.77ms
step:1376/2090 train_time:64399ms step_avg:46.80ms
step:1377/2090 train_time:64487ms step_avg:46.83ms
step:1378/2090 train_time:64574ms step_avg:46.86ms
step:1379/2090 train_time:64661ms step_avg:46.89ms
step:1380/2090 train_time:64748ms step_avg:46.92ms
step:1381/2090 train_time:64835ms step_avg:46.95ms
step:1382/2090 train_time:64923ms step_avg:46.98ms
step:1383/2090 train_time:65012ms step_avg:47.01ms
step:1384/2090 train_time:65098ms step_avg:47.04ms
step:1385/2090 train_time:65186ms step_avg:47.07ms
step:1386/2090 train_time:65273ms step_avg:47.09ms
step:1387/2090 train_time:65361ms step_avg:47.12ms
step:1388/2090 train_time:65448ms step_avg:47.15ms
step:1389/2090 train_time:65535ms step_avg:47.18ms
step:1390/2090 train_time:65623ms step_avg:47.21ms
step:1391/2090 train_time:65712ms step_avg:47.24ms
step:1392/2090 train_time:65799ms step_avg:47.27ms
step:1393/2090 train_time:65887ms step_avg:47.30ms
step:1394/2090 train_time:65974ms step_avg:47.33ms
step:1395/2090 train_time:66061ms step_avg:47.36ms
step:1396/2090 train_time:66149ms step_avg:47.38ms
step:1397/2090 train_time:66237ms step_avg:47.41ms
step:1398/2090 train_time:66323ms step_avg:47.44ms
step:1399/2090 train_time:66411ms step_avg:47.47ms
step:1400/2090 train_time:66497ms step_avg:47.50ms
step:1401/2090 train_time:66585ms step_avg:47.53ms
step:1402/2090 train_time:66672ms step_avg:47.55ms
step:1403/2090 train_time:66760ms step_avg:47.58ms
step:1404/2090 train_time:66847ms step_avg:47.61ms
step:1405/2090 train_time:66935ms step_avg:47.64ms
step:1406/2090 train_time:67022ms step_avg:47.67ms
step:1407/2090 train_time:67110ms step_avg:47.70ms
step:1408/2090 train_time:67197ms step_avg:47.73ms
step:1409/2090 train_time:67285ms step_avg:47.75ms
step:1410/2090 train_time:67372ms step_avg:47.78ms
step:1411/2090 train_time:67459ms step_avg:47.81ms
step:1412/2090 train_time:67546ms step_avg:47.84ms
step:1413/2090 train_time:67634ms step_avg:47.87ms
step:1414/2090 train_time:67721ms step_avg:47.89ms
step:1415/2090 train_time:67810ms step_avg:47.92ms
step:1416/2090 train_time:67896ms step_avg:47.95ms
step:1417/2090 train_time:67984ms step_avg:47.98ms
step:1418/2090 train_time:68072ms step_avg:48.01ms
step:1419/2090 train_time:68160ms step_avg:48.03ms
step:1420/2090 train_time:68247ms step_avg:48.06ms
step:1421/2090 train_time:68335ms step_avg:48.09ms
step:1422/2090 train_time:68422ms step_avg:48.12ms
step:1423/2090 train_time:68510ms step_avg:48.14ms
step:1424/2090 train_time:68596ms step_avg:48.17ms
step:1425/2090 train_time:68684ms step_avg:48.20ms
step:1426/2090 train_time:68772ms step_avg:48.23ms
step:1427/2090 train_time:68860ms step_avg:48.25ms
step:1428/2090 train_time:68948ms step_avg:48.28ms
step:1429/2090 train_time:69035ms step_avg:48.31ms
step:1430/2090 train_time:69123ms step_avg:48.34ms
step:1431/2090 train_time:69211ms step_avg:48.37ms
step:1432/2090 train_time:69298ms step_avg:48.39ms
step:1433/2090 train_time:69386ms step_avg:48.42ms
step:1434/2090 train_time:69473ms step_avg:48.45ms
step:1435/2090 train_time:69561ms step_avg:48.47ms
step:1436/2090 train_time:69649ms step_avg:48.50ms
step:1437/2090 train_time:69735ms step_avg:48.53ms
step:1438/2090 train_time:69822ms step_avg:48.55ms
step:1439/2090 train_time:69912ms step_avg:48.58ms
step:1440/2090 train_time:69998ms step_avg:48.61ms
step:1441/2090 train_time:70087ms step_avg:48.64ms
step:1442/2090 train_time:70173ms step_avg:48.66ms
step:1443/2090 train_time:70261ms step_avg:48.69ms
step:1444/2090 train_time:70349ms step_avg:48.72ms
step:1445/2090 train_time:70436ms step_avg:48.74ms
step:1446/2090 train_time:70523ms step_avg:48.77ms
step:1447/2090 train_time:70611ms step_avg:48.80ms
step:1448/2090 train_time:70698ms step_avg:48.82ms
step:1449/2090 train_time:70785ms step_avg:48.85ms
step:1450/2090 train_time:70873ms step_avg:48.88ms
step:1451/2090 train_time:70960ms step_avg:48.90ms
step:1452/2090 train_time:71048ms step_avg:48.93ms
step:1453/2090 train_time:71136ms step_avg:48.96ms
step:1454/2090 train_time:71223ms step_avg:48.98ms
step:1455/2090 train_time:71311ms step_avg:49.01ms
step:1456/2090 train_time:71398ms step_avg:49.04ms
step:1457/2090 train_time:71486ms step_avg:49.06ms
step:1458/2090 train_time:71572ms step_avg:49.09ms
step:1459/2090 train_time:71660ms step_avg:49.12ms
step:1460/2090 train_time:71747ms step_avg:49.14ms
step:1461/2090 train_time:71835ms step_avg:49.17ms
step:1462/2090 train_time:71922ms step_avg:49.19ms
step:1463/2090 train_time:72011ms step_avg:49.22ms
step:1464/2090 train_time:72098ms step_avg:49.25ms
step:1465/2090 train_time:72185ms step_avg:49.27ms
step:1466/2090 train_time:72272ms step_avg:49.30ms
step:1467/2090 train_time:72359ms step_avg:49.32ms
step:1468/2090 train_time:72446ms step_avg:49.35ms
step:1469/2090 train_time:72533ms step_avg:49.38ms
step:1470/2090 train_time:72620ms step_avg:49.40ms
step:1471/2090 train_time:72708ms step_avg:49.43ms
step:1472/2090 train_time:72795ms step_avg:49.45ms
step:1473/2090 train_time:72883ms step_avg:49.48ms
step:1474/2090 train_time:72971ms step_avg:49.51ms
step:1475/2090 train_time:73058ms step_avg:49.53ms
step:1476/2090 train_time:73147ms step_avg:49.56ms
step:1477/2090 train_time:73235ms step_avg:49.58ms
step:1478/2090 train_time:73322ms step_avg:49.61ms
step:1479/2090 train_time:73412ms step_avg:49.64ms
step:1480/2090 train_time:73498ms step_avg:49.66ms
step:1481/2090 train_time:73587ms step_avg:49.69ms
step:1482/2090 train_time:73673ms step_avg:49.71ms
step:1483/2090 train_time:73761ms step_avg:49.74ms
step:1484/2090 train_time:73848ms step_avg:49.76ms
step:1485/2090 train_time:73936ms step_avg:49.79ms
step:1486/2090 train_time:74022ms step_avg:49.81ms
step:1487/2090 train_time:74111ms step_avg:49.84ms
step:1488/2090 train_time:74198ms step_avg:49.86ms
step:1489/2090 train_time:74285ms step_avg:49.89ms
step:1490/2090 train_time:74372ms step_avg:49.91ms
step:1491/2090 train_time:74459ms step_avg:49.94ms
step:1492/2090 train_time:74547ms step_avg:49.96ms
step:1493/2090 train_time:74634ms step_avg:49.99ms
step:1494/2090 train_time:74722ms step_avg:50.01ms
step:1495/2090 train_time:74810ms step_avg:50.04ms
step:1496/2090 train_time:74896ms step_avg:50.06ms
step:1497/2090 train_time:74984ms step_avg:50.09ms
step:1498/2090 train_time:75071ms step_avg:50.11ms
step:1499/2090 train_time:75159ms step_avg:50.14ms
step:1500/2090 train_time:75246ms step_avg:50.16ms
step:1500/2090 val_loss:3.4758 train_time:75336ms step_avg:50.22ms
step:1501/2090 train_time:75356ms step_avg:50.20ms
step:1502/2090 train_time:75426ms step_avg:50.22ms
step:1503/2090 train_time:75516ms step_avg:50.24ms
step:1504/2090 train_time:75602ms step_avg:50.27ms
step:1505/2090 train_time:75690ms step_avg:50.29ms
step:1506/2090 train_time:75776ms step_avg:50.32ms
step:1507/2090 train_time:75863ms step_avg:50.34ms
step:1508/2090 train_time:75949ms step_avg:50.36ms
step:1509/2090 train_time:76035ms step_avg:50.39ms
step:1510/2090 train_time:76122ms step_avg:50.41ms
step:1511/2090 train_time:76209ms step_avg:50.44ms
step:1512/2090 train_time:76298ms step_avg:50.46ms
step:1513/2090 train_time:76390ms step_avg:50.49ms
step:1514/2090 train_time:76478ms step_avg:50.51ms
step:1515/2090 train_time:76567ms step_avg:50.54ms
step:1516/2090 train_time:76653ms step_avg:50.56ms
step:1517/2090 train_time:76742ms step_avg:50.59ms
step:1518/2090 train_time:76828ms step_avg:50.61ms
step:1519/2090 train_time:76915ms step_avg:50.64ms
step:1520/2090 train_time:77001ms step_avg:50.66ms
step:1521/2090 train_time:77088ms step_avg:50.68ms
step:1522/2090 train_time:77174ms step_avg:50.71ms
step:1523/2090 train_time:77262ms step_avg:50.73ms
step:1524/2090 train_time:77350ms step_avg:50.75ms
step:1525/2090 train_time:77439ms step_avg:50.78ms
step:1526/2090 train_time:77528ms step_avg:50.80ms
step:1527/2090 train_time:77616ms step_avg:50.83ms
step:1528/2090 train_time:77703ms step_avg:50.85ms
step:1529/2090 train_time:77791ms step_avg:50.88ms
step:1530/2090 train_time:77877ms step_avg:50.90ms
step:1531/2090 train_time:77965ms step_avg:50.92ms
step:1532/2090 train_time:78050ms step_avg:50.95ms
step:1533/2090 train_time:78138ms step_avg:50.97ms
step:1534/2090 train_time:78225ms step_avg:50.99ms
step:1535/2090 train_time:78313ms step_avg:51.02ms
step:1536/2090 train_time:78402ms step_avg:51.04ms
step:1537/2090 train_time:78490ms step_avg:51.07ms
step:1538/2090 train_time:78579ms step_avg:51.09ms
step:1539/2090 train_time:78666ms step_avg:51.12ms
step:1540/2090 train_time:78753ms step_avg:51.14ms
step:1541/2090 train_time:78841ms step_avg:51.16ms
step:1542/2090 train_time:78927ms step_avg:51.19ms
step:1543/2090 train_time:79015ms step_avg:51.21ms
step:1544/2090 train_time:79102ms step_avg:51.23ms
step:1545/2090 train_time:79190ms step_avg:51.26ms
step:1546/2090 train_time:79276ms step_avg:51.28ms
step:1547/2090 train_time:79365ms step_avg:51.30ms
step:1548/2090 train_time:79453ms step_avg:51.33ms
step:1549/2090 train_time:79542ms step_avg:51.35ms
step:1550/2090 train_time:79629ms step_avg:51.37ms
step:1551/2090 train_time:79717ms step_avg:51.40ms
step:1552/2090 train_time:79804ms step_avg:51.42ms
step:1553/2090 train_time:79891ms step_avg:51.44ms
step:1554/2090 train_time:79978ms step_avg:51.47ms
step:1555/2090 train_time:80066ms step_avg:51.49ms
step:1556/2090 train_time:80152ms step_avg:51.51ms
step:1557/2090 train_time:80240ms step_avg:51.54ms
step:1558/2090 train_time:80328ms step_avg:51.56ms
step:1559/2090 train_time:80416ms step_avg:51.58ms
step:1560/2090 train_time:80503ms step_avg:51.60ms
step:1561/2090 train_time:80593ms step_avg:51.63ms
step:1562/2090 train_time:80681ms step_avg:51.65ms
step:1563/2090 train_time:80769ms step_avg:51.68ms
step:1564/2090 train_time:80857ms step_avg:51.70ms
step:1565/2090 train_time:80945ms step_avg:51.72ms
step:1566/2090 train_time:81031ms step_avg:51.74ms
step:1567/2090 train_time:81119ms step_avg:51.77ms
step:1568/2090 train_time:81206ms step_avg:51.79ms
step:1569/2090 train_time:81294ms step_avg:51.81ms
step:1570/2090 train_time:81381ms step_avg:51.83ms
step:1571/2090 train_time:81469ms step_avg:51.86ms
step:1572/2090 train_time:81557ms step_avg:51.88ms
step:1573/2090 train_time:81644ms step_avg:51.90ms
step:1574/2090 train_time:81730ms step_avg:51.93ms
step:1575/2090 train_time:81819ms step_avg:51.95ms
step:1576/2090 train_time:81906ms step_avg:51.97ms
step:1577/2090 train_time:81992ms step_avg:51.99ms
step:1578/2090 train_time:82079ms step_avg:52.01ms
step:1579/2090 train_time:82167ms step_avg:52.04ms
step:1580/2090 train_time:82253ms step_avg:52.06ms
step:1581/2090 train_time:82341ms step_avg:52.08ms
step:1582/2090 train_time:82428ms step_avg:52.10ms
step:1583/2090 train_time:82517ms step_avg:52.13ms
step:1584/2090 train_time:82604ms step_avg:52.15ms
step:1585/2090 train_time:82692ms step_avg:52.17ms
step:1586/2090 train_time:82779ms step_avg:52.19ms
step:1587/2090 train_time:82868ms step_avg:52.22ms
step:1588/2090 train_time:82954ms step_avg:52.24ms
step:1589/2090 train_time:83042ms step_avg:52.26ms
step:1590/2090 train_time:83129ms step_avg:52.28ms
step:1591/2090 train_time:83217ms step_avg:52.30ms
step:1592/2090 train_time:83304ms step_avg:52.33ms
step:1593/2090 train_time:83392ms step_avg:52.35ms
step:1594/2090 train_time:83479ms step_avg:52.37ms
step:1595/2090 train_time:83567ms step_avg:52.39ms
step:1596/2090 train_time:83654ms step_avg:52.41ms
step:1597/2090 train_time:83742ms step_avg:52.44ms
step:1598/2090 train_time:83829ms step_avg:52.46ms
step:1599/2090 train_time:83917ms step_avg:52.48ms
step:1600/2090 train_time:84005ms step_avg:52.50ms
step:1601/2090 train_time:84092ms step_avg:52.52ms
step:1602/2090 train_time:84179ms step_avg:52.55ms
step:1603/2090 train_time:84267ms step_avg:52.57ms
step:1604/2090 train_time:84354ms step_avg:52.59ms
step:1605/2090 train_time:84443ms step_avg:52.61ms
step:1606/2090 train_time:84530ms step_avg:52.63ms
step:1607/2090 train_time:84618ms step_avg:52.66ms
step:1608/2090 train_time:84706ms step_avg:52.68ms
step:1609/2090 train_time:84793ms step_avg:52.70ms
step:1610/2090 train_time:84880ms step_avg:52.72ms
step:1611/2090 train_time:84969ms step_avg:52.74ms
step:1612/2090 train_time:85056ms step_avg:52.76ms
step:1613/2090 train_time:85144ms step_avg:52.79ms
step:1614/2090 train_time:85230ms step_avg:52.81ms
step:1615/2090 train_time:85318ms step_avg:52.83ms
step:1616/2090 train_time:85406ms step_avg:52.85ms
step:1617/2090 train_time:85495ms step_avg:52.87ms
step:1618/2090 train_time:85582ms step_avg:52.89ms
step:1619/2090 train_time:85670ms step_avg:52.92ms
step:1620/2090 train_time:85756ms step_avg:52.94ms
step:1621/2090 train_time:85844ms step_avg:52.96ms
step:1622/2090 train_time:85931ms step_avg:52.98ms
step:1623/2090 train_time:86019ms step_avg:53.00ms
step:1624/2090 train_time:86107ms step_avg:53.02ms
step:1625/2090 train_time:86194ms step_avg:53.04ms
step:1626/2090 train_time:86281ms step_avg:53.06ms
step:1627/2090 train_time:86369ms step_avg:53.08ms
step:1628/2090 train_time:86456ms step_avg:53.11ms
step:1629/2090 train_time:86544ms step_avg:53.13ms
step:1630/2090 train_time:86631ms step_avg:53.15ms
step:1631/2090 train_time:86718ms step_avg:53.17ms
step:1632/2090 train_time:86806ms step_avg:53.19ms
step:1633/2090 train_time:86894ms step_avg:53.21ms
step:1634/2090 train_time:86983ms step_avg:53.23ms
step:1635/2090 train_time:87070ms step_avg:53.25ms
step:1636/2090 train_time:87157ms step_avg:53.27ms
step:1637/2090 train_time:87245ms step_avg:53.30ms
step:1638/2090 train_time:87332ms step_avg:53.32ms
step:1639/2090 train_time:87420ms step_avg:53.34ms
step:1640/2090 train_time:87508ms step_avg:53.36ms
step:1641/2090 train_time:87596ms step_avg:53.38ms
step:1642/2090 train_time:87683ms step_avg:53.40ms
step:1643/2090 train_time:87770ms step_avg:53.42ms
step:1644/2090 train_time:87857ms step_avg:53.44ms
step:1645/2090 train_time:87947ms step_avg:53.46ms
step:1646/2090 train_time:88033ms step_avg:53.48ms
step:1647/2090 train_time:88121ms step_avg:53.50ms
step:1648/2090 train_time:88209ms step_avg:53.52ms
step:1649/2090 train_time:88296ms step_avg:53.54ms
step:1650/2090 train_time:88383ms step_avg:53.57ms
step:1651/2090 train_time:88472ms step_avg:53.59ms
step:1652/2090 train_time:88559ms step_avg:53.61ms
step:1653/2090 train_time:88647ms step_avg:53.63ms
step:1654/2090 train_time:88733ms step_avg:53.65ms
step:1655/2090 train_time:88823ms step_avg:53.67ms
step:1656/2090 train_time:88909ms step_avg:53.69ms
step:1657/2090 train_time:88997ms step_avg:53.71ms
step:1658/2090 train_time:89085ms step_avg:53.73ms
step:1659/2090 train_time:89173ms step_avg:53.75ms
step:1660/2090 train_time:89259ms step_avg:53.77ms
step:1661/2090 train_time:89347ms step_avg:53.79ms
step:1662/2090 train_time:89434ms step_avg:53.81ms
step:1663/2090 train_time:89522ms step_avg:53.83ms
step:1664/2090 train_time:89609ms step_avg:53.85ms
step:1665/2090 train_time:89697ms step_avg:53.87ms
step:1666/2090 train_time:89784ms step_avg:53.89ms
step:1667/2090 train_time:89871ms step_avg:53.91ms
step:1668/2090 train_time:89958ms step_avg:53.93ms
step:1669/2090 train_time:90047ms step_avg:53.95ms
step:1670/2090 train_time:90134ms step_avg:53.97ms
step:1671/2090 train_time:90222ms step_avg:53.99ms
step:1672/2090 train_time:90309ms step_avg:54.01ms
step:1673/2090 train_time:90397ms step_avg:54.03ms
step:1674/2090 train_time:90483ms step_avg:54.05ms
step:1675/2090 train_time:90571ms step_avg:54.07ms
step:1676/2090 train_time:90657ms step_avg:54.09ms
step:1677/2090 train_time:90746ms step_avg:54.11ms
step:1678/2090 train_time:90832ms step_avg:54.13ms
step:1679/2090 train_time:90920ms step_avg:54.15ms
step:1680/2090 train_time:91007ms step_avg:54.17ms
step:1681/2090 train_time:91095ms step_avg:54.19ms
step:1682/2090 train_time:91183ms step_avg:54.21ms
step:1683/2090 train_time:91270ms step_avg:54.23ms
step:1684/2090 train_time:91357ms step_avg:54.25ms
step:1685/2090 train_time:91446ms step_avg:54.27ms
step:1686/2090 train_time:91532ms step_avg:54.29ms
step:1687/2090 train_time:91620ms step_avg:54.31ms
step:1688/2090 train_time:91707ms step_avg:54.33ms
step:1689/2090 train_time:91794ms step_avg:54.35ms
step:1690/2090 train_time:91881ms step_avg:54.37ms
step:1691/2090 train_time:91969ms step_avg:54.39ms
step:1692/2090 train_time:92056ms step_avg:54.41ms
step:1693/2090 train_time:92145ms step_avg:54.43ms
step:1694/2090 train_time:92231ms step_avg:54.45ms
step:1695/2090 train_time:92320ms step_avg:54.47ms
step:1696/2090 train_time:92407ms step_avg:54.49ms
step:1697/2090 train_time:92494ms step_avg:54.50ms
step:1698/2090 train_time:92582ms step_avg:54.52ms
step:1699/2090 train_time:92669ms step_avg:54.54ms
step:1700/2090 train_time:92756ms step_avg:54.56ms
step:1701/2090 train_time:92844ms step_avg:54.58ms
step:1702/2090 train_time:92930ms step_avg:54.60ms
step:1703/2090 train_time:93019ms step_avg:54.62ms
step:1704/2090 train_time:93107ms step_avg:54.64ms
step:1705/2090 train_time:93194ms step_avg:54.66ms
step:1706/2090 train_time:93281ms step_avg:54.68ms
step:1707/2090 train_time:93370ms step_avg:54.70ms
step:1708/2090 train_time:93456ms step_avg:54.72ms
step:1709/2090 train_time:93544ms step_avg:54.74ms
step:1710/2090 train_time:93631ms step_avg:54.75ms
step:1711/2090 train_time:93718ms step_avg:54.77ms
step:1712/2090 train_time:93806ms step_avg:54.79ms
step:1713/2090 train_time:93893ms step_avg:54.81ms
step:1714/2090 train_time:93980ms step_avg:54.83ms
step:1715/2090 train_time:94069ms step_avg:54.85ms
step:1716/2090 train_time:94156ms step_avg:54.87ms
step:1717/2090 train_time:94245ms step_avg:54.89ms
step:1718/2090 train_time:94331ms step_avg:54.91ms
step:1719/2090 train_time:94419ms step_avg:54.93ms
step:1720/2090 train_time:94506ms step_avg:54.95ms
step:1721/2090 train_time:94593ms step_avg:54.96ms
step:1722/2090 train_time:94680ms step_avg:54.98ms
step:1723/2090 train_time:94769ms step_avg:55.00ms
step:1724/2090 train_time:94855ms step_avg:55.02ms
step:1725/2090 train_time:94944ms step_avg:55.04ms
step:1726/2090 train_time:95031ms step_avg:55.06ms
step:1727/2090 train_time:95119ms step_avg:55.08ms
step:1728/2090 train_time:95206ms step_avg:55.10ms
step:1729/2090 train_time:95294ms step_avg:55.12ms
step:1730/2090 train_time:95381ms step_avg:55.13ms
step:1731/2090 train_time:95469ms step_avg:55.15ms
step:1732/2090 train_time:95556ms step_avg:55.17ms
step:1733/2090 train_time:95644ms step_avg:55.19ms
step:1734/2090 train_time:95731ms step_avg:55.21ms
step:1735/2090 train_time:95819ms step_avg:55.23ms
step:1736/2090 train_time:95905ms step_avg:55.24ms
step:1737/2090 train_time:95993ms step_avg:55.26ms
step:1738/2090 train_time:96081ms step_avg:55.28ms
step:1739/2090 train_time:96170ms step_avg:55.30ms
step:1740/2090 train_time:96257ms step_avg:55.32ms
step:1741/2090 train_time:96346ms step_avg:55.34ms
step:1742/2090 train_time:96432ms step_avg:55.36ms
step:1743/2090 train_time:96520ms step_avg:55.38ms
step:1744/2090 train_time:96608ms step_avg:55.39ms
step:1745/2090 train_time:96695ms step_avg:55.41ms
step:1746/2090 train_time:96782ms step_avg:55.43ms
step:1747/2090 train_time:96870ms step_avg:55.45ms
step:1748/2090 train_time:96956ms step_avg:55.47ms
step:1749/2090 train_time:97045ms step_avg:55.49ms
step:1750/2090 train_time:97131ms step_avg:55.50ms
step:1750/2090 val_loss:3.3743 train_time:97221ms step_avg:55.55ms
step:1751/2090 train_time:97241ms step_avg:55.53ms
step:1752/2090 train_time:97312ms step_avg:55.54ms
step:1753/2090 train_time:97404ms step_avg:55.56ms
step:1754/2090 train_time:97490ms step_avg:55.58ms
step:1755/2090 train_time:97577ms step_avg:55.60ms
step:1756/2090 train_time:97663ms step_avg:55.62ms
step:1757/2090 train_time:97749ms step_avg:55.63ms
step:1758/2090 train_time:97835ms step_avg:55.65ms
step:1759/2090 train_time:97923ms step_avg:55.67ms
step:1760/2090 train_time:98008ms step_avg:55.69ms
step:1761/2090 train_time:98096ms step_avg:55.70ms
step:1762/2090 train_time:98189ms step_avg:55.73ms
step:1763/2090 train_time:98279ms step_avg:55.75ms
step:1764/2090 train_time:98367ms step_avg:55.76ms
step:1765/2090 train_time:98455ms step_avg:55.78ms
step:1766/2090 train_time:98542ms step_avg:55.80ms
step:1767/2090 train_time:98630ms step_avg:55.82ms
step:1768/2090 train_time:98717ms step_avg:55.84ms
step:1769/2090 train_time:98804ms step_avg:55.85ms
step:1770/2090 train_time:98890ms step_avg:55.87ms
step:1771/2090 train_time:98977ms step_avg:55.89ms
step:1772/2090 train_time:99064ms step_avg:55.90ms
step:1773/2090 train_time:99152ms step_avg:55.92ms
step:1774/2090 train_time:99242ms step_avg:55.94ms
step:1775/2090 train_time:99331ms step_avg:55.96ms
step:1776/2090 train_time:99419ms step_avg:55.98ms
step:1777/2090 train_time:99507ms step_avg:56.00ms
step:1778/2090 train_time:99593ms step_avg:56.01ms
step:1779/2090 train_time:99681ms step_avg:56.03ms
step:1780/2090 train_time:99767ms step_avg:56.05ms
step:1781/2090 train_time:99854ms step_avg:56.07ms
step:1782/2090 train_time:99940ms step_avg:56.08ms
step:1783/2090 train_time:100028ms step_avg:56.10ms
step:1784/2090 train_time:100114ms step_avg:56.12ms
step:1785/2090 train_time:100203ms step_avg:56.14ms
step:1786/2090 train_time:100291ms step_avg:56.15ms
step:1787/2090 train_time:100380ms step_avg:56.17ms
step:1788/2090 train_time:100468ms step_avg:56.19ms
step:1789/2090 train_time:100556ms step_avg:56.21ms
step:1790/2090 train_time:100643ms step_avg:56.22ms
step:1791/2090 train_time:100731ms step_avg:56.24ms
step:1792/2090 train_time:100817ms step_avg:56.26ms
step:1793/2090 train_time:100904ms step_avg:56.28ms
step:1794/2090 train_time:100990ms step_avg:56.29ms
step:1795/2090 train_time:101077ms step_avg:56.31ms
step:1796/2090 train_time:101165ms step_avg:56.33ms
step:1797/2090 train_time:101253ms step_avg:56.35ms
step:1798/2090 train_time:101341ms step_avg:56.36ms
step:1799/2090 train_time:101430ms step_avg:56.38ms
step:1800/2090 train_time:101517ms step_avg:56.40ms
step:1801/2090 train_time:101605ms step_avg:56.42ms
step:1802/2090 train_time:101692ms step_avg:56.43ms
step:1803/2090 train_time:101780ms step_avg:56.45ms
step:1804/2090 train_time:101865ms step_avg:56.47ms
step:1805/2090 train_time:101952ms step_avg:56.48ms
step:1806/2090 train_time:102039ms step_avg:56.50ms
step:1807/2090 train_time:102127ms step_avg:56.52ms
step:1808/2090 train_time:102214ms step_avg:56.53ms
step:1809/2090 train_time:102303ms step_avg:56.55ms
step:1810/2090 train_time:102390ms step_avg:56.57ms
step:1811/2090 train_time:102477ms step_avg:56.59ms
step:1812/2090 train_time:102565ms step_avg:56.60ms
step:1813/2090 train_time:102653ms step_avg:56.62ms
step:1814/2090 train_time:102740ms step_avg:56.64ms
step:1815/2090 train_time:102827ms step_avg:56.65ms
step:1816/2090 train_time:102913ms step_avg:56.67ms
step:1817/2090 train_time:103001ms step_avg:56.69ms
step:1818/2090 train_time:103088ms step_avg:56.70ms
step:1819/2090 train_time:103176ms step_avg:56.72ms
step:1820/2090 train_time:103263ms step_avg:56.74ms
step:1821/2090 train_time:103351ms step_avg:56.76ms
step:1822/2090 train_time:103438ms step_avg:56.77ms
step:1823/2090 train_time:103527ms step_avg:56.79ms
step:1824/2090 train_time:103615ms step_avg:56.81ms
step:1825/2090 train_time:103703ms step_avg:56.82ms
step:1826/2090 train_time:103789ms step_avg:56.84ms
step:1827/2090 train_time:103876ms step_avg:56.86ms
step:1828/2090 train_time:103962ms step_avg:56.87ms
step:1829/2090 train_time:104050ms step_avg:56.89ms
step:1830/2090 train_time:104137ms step_avg:56.91ms
step:1831/2090 train_time:104225ms step_avg:56.92ms
step:1832/2090 train_time:104312ms step_avg:56.94ms
step:1833/2090 train_time:104399ms step_avg:56.96ms
step:1834/2090 train_time:104487ms step_avg:56.97ms
step:1835/2090 train_time:104575ms step_avg:56.99ms
step:1836/2090 train_time:104663ms step_avg:57.01ms
step:1837/2090 train_time:104750ms step_avg:57.02ms
step:1838/2090 train_time:104837ms step_avg:57.04ms
step:1839/2090 train_time:104925ms step_avg:57.06ms
step:1840/2090 train_time:105011ms step_avg:57.07ms
step:1841/2090 train_time:105099ms step_avg:57.09ms
step:1842/2090 train_time:105186ms step_avg:57.10ms
step:1843/2090 train_time:105274ms step_avg:57.12ms
step:1844/2090 train_time:105361ms step_avg:57.14ms
step:1845/2090 train_time:105449ms step_avg:57.15ms
step:1846/2090 train_time:105536ms step_avg:57.17ms
step:1847/2090 train_time:105625ms step_avg:57.19ms
step:1848/2090 train_time:105711ms step_avg:57.20ms
step:1849/2090 train_time:105800ms step_avg:57.22ms
step:1850/2090 train_time:105886ms step_avg:57.24ms
step:1851/2090 train_time:105974ms step_avg:57.25ms
step:1852/2090 train_time:106060ms step_avg:57.27ms
step:1853/2090 train_time:106149ms step_avg:57.28ms
step:1854/2090 train_time:106235ms step_avg:57.30ms
step:1855/2090 train_time:106323ms step_avg:57.32ms
step:1856/2090 train_time:106409ms step_avg:57.33ms
step:1857/2090 train_time:106498ms step_avg:57.35ms
step:1858/2090 train_time:106586ms step_avg:57.37ms
step:1859/2090 train_time:106674ms step_avg:57.38ms
step:1860/2090 train_time:106760ms step_avg:57.40ms
step:1861/2090 train_time:106848ms step_avg:57.41ms
step:1862/2090 train_time:106935ms step_avg:57.43ms
step:1863/2090 train_time:107023ms step_avg:57.45ms
step:1864/2090 train_time:107109ms step_avg:57.46ms
step:1865/2090 train_time:107198ms step_avg:57.48ms
step:1866/2090 train_time:107286ms step_avg:57.50ms
step:1867/2090 train_time:107373ms step_avg:57.51ms
step:1868/2090 train_time:107461ms step_avg:57.53ms
step:1869/2090 train_time:107549ms step_avg:57.54ms
step:1870/2090 train_time:107636ms step_avg:57.56ms
step:1871/2090 train_time:107725ms step_avg:57.58ms
step:1872/2090 train_time:107811ms step_avg:57.59ms
step:1873/2090 train_time:107900ms step_avg:57.61ms
step:1874/2090 train_time:107987ms step_avg:57.62ms
step:1875/2090 train_time:108074ms step_avg:57.64ms
step:1876/2090 train_time:108161ms step_avg:57.66ms
step:1877/2090 train_time:108249ms step_avg:57.67ms
step:1878/2090 train_time:108336ms step_avg:57.69ms
step:1879/2090 train_time:108424ms step_avg:57.70ms
step:1880/2090 train_time:108510ms step_avg:57.72ms
step:1881/2090 train_time:108599ms step_avg:57.73ms
step:1882/2090 train_time:108686ms step_avg:57.75ms
step:1883/2090 train_time:108774ms step_avg:57.77ms
step:1884/2090 train_time:108861ms step_avg:57.78ms
step:1885/2090 train_time:108949ms step_avg:57.80ms
step:1886/2090 train_time:109035ms step_avg:57.81ms
step:1887/2090 train_time:109124ms step_avg:57.83ms
step:1888/2090 train_time:109211ms step_avg:57.84ms
step:1889/2090 train_time:109299ms step_avg:57.86ms
step:1890/2090 train_time:109387ms step_avg:57.88ms
step:1891/2090 train_time:109474ms step_avg:57.89ms
step:1892/2090 train_time:109561ms step_avg:57.91ms
step:1893/2090 train_time:109650ms step_avg:57.92ms
step:1894/2090 train_time:109737ms step_avg:57.94ms
step:1895/2090 train_time:109825ms step_avg:57.96ms
step:1896/2090 train_time:109912ms step_avg:57.97ms
step:1897/2090 train_time:109999ms step_avg:57.99ms
step:1898/2090 train_time:110086ms step_avg:58.00ms
step:1899/2090 train_time:110174ms step_avg:58.02ms
step:1900/2090 train_time:110260ms step_avg:58.03ms
step:1901/2090 train_time:110348ms step_avg:58.05ms
step:1902/2090 train_time:110435ms step_avg:58.06ms
step:1903/2090 train_time:110523ms step_avg:58.08ms
step:1904/2090 train_time:110610ms step_avg:58.09ms
step:1905/2090 train_time:110699ms step_avg:58.11ms
step:1906/2090 train_time:110786ms step_avg:58.12ms
step:1907/2090 train_time:110874ms step_avg:58.14ms
step:1908/2090 train_time:110960ms step_avg:58.16ms
step:1909/2090 train_time:111049ms step_avg:58.17ms
step:1910/2090 train_time:111136ms step_avg:58.19ms
step:1911/2090 train_time:111224ms step_avg:58.20ms
step:1912/2090 train_time:111310ms step_avg:58.22ms
step:1913/2090 train_time:111399ms step_avg:58.23ms
step:1914/2090 train_time:111486ms step_avg:58.25ms
step:1915/2090 train_time:111573ms step_avg:58.26ms
step:1916/2090 train_time:111660ms step_avg:58.28ms
step:1917/2090 train_time:111749ms step_avg:58.29ms
step:1918/2090 train_time:111836ms step_avg:58.31ms
step:1919/2090 train_time:111924ms step_avg:58.32ms
step:1920/2090 train_time:112011ms step_avg:58.34ms
step:1921/2090 train_time:112099ms step_avg:58.35ms
step:1922/2090 train_time:112186ms step_avg:58.37ms
step:1923/2090 train_time:112273ms step_avg:58.38ms
step:1924/2090 train_time:112360ms step_avg:58.40ms
step:1925/2090 train_time:112448ms step_avg:58.41ms
step:1926/2090 train_time:112535ms step_avg:58.43ms
step:1927/2090 train_time:112623ms step_avg:58.44ms
step:1928/2090 train_time:112710ms step_avg:58.46ms
step:1929/2090 train_time:112799ms step_avg:58.48ms
step:1930/2090 train_time:112885ms step_avg:58.49ms
step:1931/2090 train_time:112972ms step_avg:58.50ms
step:1932/2090 train_time:113059ms step_avg:58.52ms
step:1933/2090 train_time:113148ms step_avg:58.53ms
step:1934/2090 train_time:113235ms step_avg:58.55ms
step:1935/2090 train_time:113323ms step_avg:58.56ms
step:1936/2090 train_time:113409ms step_avg:58.58ms
step:1937/2090 train_time:113496ms step_avg:58.59ms
step:1938/2090 train_time:113584ms step_avg:58.61ms
step:1939/2090 train_time:113671ms step_avg:58.62ms
step:1940/2090 train_time:113758ms step_avg:58.64ms
step:1941/2090 train_time:113847ms step_avg:58.65ms
step:1942/2090 train_time:113933ms step_avg:58.67ms
step:1943/2090 train_time:114022ms step_avg:58.68ms
step:1944/2090 train_time:114108ms step_avg:58.70ms
step:1945/2090 train_time:114196ms step_avg:58.71ms
step:1946/2090 train_time:114283ms step_avg:58.73ms
step:1947/2090 train_time:114371ms step_avg:58.74ms
step:1948/2090 train_time:114457ms step_avg:58.76ms
step:1949/2090 train_time:114547ms step_avg:58.77ms
step:1950/2090 train_time:114634ms step_avg:58.79ms
step:1951/2090 train_time:114722ms step_avg:58.80ms
step:1952/2090 train_time:114809ms step_avg:58.82ms
step:1953/2090 train_time:114896ms step_avg:58.83ms
step:1954/2090 train_time:114983ms step_avg:58.85ms
step:1955/2090 train_time:115070ms step_avg:58.86ms
step:1956/2090 train_time:115157ms step_avg:58.87ms
step:1957/2090 train_time:115246ms step_avg:58.89ms
step:1958/2090 train_time:115333ms step_avg:58.90ms
step:1959/2090 train_time:115422ms step_avg:58.92ms
step:1960/2090 train_time:115508ms step_avg:58.93ms
step:1961/2090 train_time:115596ms step_avg:58.95ms
step:1962/2090 train_time:115684ms step_avg:58.96ms
step:1963/2090 train_time:115771ms step_avg:58.98ms
step:1964/2090 train_time:115858ms step_avg:58.99ms
step:1965/2090 train_time:115946ms step_avg:59.01ms
step:1966/2090 train_time:116033ms step_avg:59.02ms
step:1967/2090 train_time:116122ms step_avg:59.04ms
step:1968/2090 train_time:116208ms step_avg:59.05ms
step:1969/2090 train_time:116296ms step_avg:59.06ms
step:1970/2090 train_time:116383ms step_avg:59.08ms
step:1971/2090 train_time:116471ms step_avg:59.09ms
step:1972/2090 train_time:116558ms step_avg:59.11ms
step:1973/2090 train_time:116646ms step_avg:59.12ms
step:1974/2090 train_time:116733ms step_avg:59.14ms
step:1975/2090 train_time:116821ms step_avg:59.15ms
step:1976/2090 train_time:116907ms step_avg:59.16ms
step:1977/2090 train_time:116995ms step_avg:59.18ms
step:1978/2090 train_time:117083ms step_avg:59.19ms
step:1979/2090 train_time:117170ms step_avg:59.21ms
step:1980/2090 train_time:117258ms step_avg:59.22ms
step:1981/2090 train_time:117347ms step_avg:59.24ms
step:1982/2090 train_time:117434ms step_avg:59.25ms
step:1983/2090 train_time:117522ms step_avg:59.26ms
step:1984/2090 train_time:117608ms step_avg:59.28ms
step:1985/2090 train_time:117696ms step_avg:59.29ms
step:1986/2090 train_time:117784ms step_avg:59.31ms
step:1987/2090 train_time:117871ms step_avg:59.32ms
step:1988/2090 train_time:117958ms step_avg:59.33ms
step:1989/2090 train_time:118046ms step_avg:59.35ms
step:1990/2090 train_time:118133ms step_avg:59.36ms
step:1991/2090 train_time:118221ms step_avg:59.38ms
step:1992/2090 train_time:118307ms step_avg:59.39ms
step:1993/2090 train_time:118395ms step_avg:59.41ms
step:1994/2090 train_time:118483ms step_avg:59.42ms
step:1995/2090 train_time:118570ms step_avg:59.43ms
step:1996/2090 train_time:118657ms step_avg:59.45ms
step:1997/2090 train_time:118746ms step_avg:59.46ms
step:1998/2090 train_time:118833ms step_avg:59.48ms
step:1999/2090 train_time:118922ms step_avg:59.49ms
step:2000/2090 train_time:119008ms step_avg:59.50ms
step:2000/2090 val_loss:3.2980 train_time:119097ms step_avg:59.55ms
step:2001/2090 train_time:119117ms step_avg:59.53ms
step:2002/2090 train_time:119186ms step_avg:59.53ms
step:2003/2090 train_time:119277ms step_avg:59.55ms
step:2004/2090 train_time:119364ms step_avg:59.56ms
step:2005/2090 train_time:119452ms step_avg:59.58ms
step:2006/2090 train_time:119537ms step_avg:59.59ms
step:2007/2090 train_time:119624ms step_avg:59.60ms
step:2008/2090 train_time:119710ms step_avg:59.62ms
step:2009/2090 train_time:119797ms step_avg:59.63ms
step:2010/2090 train_time:119883ms step_avg:59.64ms
step:2011/2090 train_time:119970ms step_avg:59.66ms
step:2012/2090 train_time:120061ms step_avg:59.67ms
step:2013/2090 train_time:120152ms step_avg:59.69ms
step:2014/2090 train_time:120239ms step_avg:59.70ms
step:2015/2090 train_time:120328ms step_avg:59.72ms
step:2016/2090 train_time:120416ms step_avg:59.73ms
step:2017/2090 train_time:120503ms step_avg:59.74ms
step:2018/2090 train_time:120589ms step_avg:59.76ms
step:2019/2090 train_time:120678ms step_avg:59.77ms
step:2020/2090 train_time:120763ms step_avg:59.78ms
step:2021/2090 train_time:120850ms step_avg:59.80ms
step:2022/2090 train_time:120936ms step_avg:59.81ms
step:2023/2090 train_time:121024ms step_avg:59.82ms
step:2024/2090 train_time:121114ms step_avg:59.84ms
step:2025/2090 train_time:121202ms step_avg:59.85ms
step:2026/2090 train_time:121290ms step_avg:59.87ms
step:2027/2090 train_time:121378ms step_avg:59.88ms
step:2028/2090 train_time:121466ms step_avg:59.89ms
step:2029/2090 train_time:121553ms step_avg:59.91ms
step:2030/2090 train_time:121640ms step_avg:59.92ms
step:2031/2090 train_time:121727ms step_avg:59.93ms
step:2032/2090 train_time:121813ms step_avg:59.95ms
step:2033/2090 train_time:121900ms step_avg:59.96ms
step:2034/2090 train_time:121987ms step_avg:59.97ms
step:2035/2090 train_time:122076ms step_avg:59.99ms
step:2036/2090 train_time:122164ms step_avg:60.00ms
step:2037/2090 train_time:122253ms step_avg:60.02ms
step:2038/2090 train_time:122339ms step_avg:60.03ms
step:2039/2090 train_time:122428ms step_avg:60.04ms
step:2040/2090 train_time:122515ms step_avg:60.06ms
step:2041/2090 train_time:122602ms step_avg:60.07ms
step:2042/2090 train_time:122688ms step_avg:60.08ms
step:2043/2090 train_time:122776ms step_avg:60.10ms
step:2044/2090 train_time:122862ms step_avg:60.11ms
step:2045/2090 train_time:122951ms step_avg:60.12ms
step:2046/2090 train_time:123038ms step_avg:60.14ms
step:2047/2090 train_time:123125ms step_avg:60.15ms
step:2048/2090 train_time:123214ms step_avg:60.16ms
step:2049/2090 train_time:123302ms step_avg:60.18ms
step:2050/2090 train_time:123390ms step_avg:60.19ms
step:2051/2090 train_time:123478ms step_avg:60.20ms
step:2052/2090 train_time:123564ms step_avg:60.22ms
step:2053/2090 train_time:123652ms step_avg:60.23ms
step:2054/2090 train_time:123738ms step_avg:60.24ms
step:2055/2090 train_time:123826ms step_avg:60.26ms
step:2056/2090 train_time:123913ms step_avg:60.27ms
step:2057/2090 train_time:124002ms step_avg:60.28ms
step:2058/2090 train_time:124089ms step_avg:60.30ms
step:2059/2090 train_time:124177ms step_avg:60.31ms
step:2060/2090 train_time:124264ms step_avg:60.32ms
step:2061/2090 train_time:124355ms step_avg:60.34ms
step:2062/2090 train_time:124442ms step_avg:60.35ms
step:2063/2090 train_time:124530ms step_avg:60.36ms
step:2064/2090 train_time:124617ms step_avg:60.38ms
step:2065/2090 train_time:124705ms step_avg:60.39ms
step:2066/2090 train_time:124791ms step_avg:60.40ms
step:2067/2090 train_time:124880ms step_avg:60.42ms
step:2068/2090 train_time:124966ms step_avg:60.43ms
step:2069/2090 train_time:125055ms step_avg:60.44ms
step:2070/2090 train_time:125141ms step_avg:60.45ms
step:2071/2090 train_time:125230ms step_avg:60.47ms
step:2072/2090 train_time:125318ms step_avg:60.48ms
step:2073/2090 train_time:125406ms step_avg:60.49ms
step:2074/2090 train_time:125493ms step_avg:60.51ms
step:2075/2090 train_time:125581ms step_avg:60.52ms
step:2076/2090 train_time:125668ms step_avg:60.53ms
step:2077/2090 train_time:125756ms step_avg:60.55ms
step:2078/2090 train_time:125842ms step_avg:60.56ms
step:2079/2090 train_time:125931ms step_avg:60.57ms
step:2080/2090 train_time:126018ms step_avg:60.59ms
step:2081/2090 train_time:126106ms step_avg:60.60ms
step:2082/2090 train_time:126194ms step_avg:60.61ms
step:2083/2090 train_time:126282ms step_avg:60.63ms
step:2084/2090 train_time:126370ms step_avg:60.64ms
step:2085/2090 train_time:126459ms step_avg:60.65ms
step:2086/2090 train_time:126547ms step_avg:60.67ms
step:2087/2090 train_time:126636ms step_avg:60.68ms
step:2088/2090 train_time:126724ms step_avg:60.69ms
step:2089/2090 train_time:126812ms step_avg:60.70ms
step:2090/2090 train_time:126899ms step_avg:60.72ms
step:2090/2090 val_loss:3.2771 train_time:126988ms step_avg:60.76ms
peak memory allocated: 29975 MiB reserved: 44976 MiB

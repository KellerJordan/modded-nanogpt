import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # weight decay
                if wd != 0:
                    mask = (update * p_slice) >= 0
                    # lr as weight decay  schedule
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)

                    update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)
                
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2050  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.005,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Dec 18 11:56:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   25C    P0            115W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   23C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   25C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   24C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2090 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2090 train_time:93ms step_avg:93.39ms
step:2/2090 train_time:119ms step_avg:59.67ms
step:3/2090 train_time:143ms step_avg:47.61ms
step:4/2090 train_time:175ms step_avg:43.74ms
step:5/2090 train_time:208ms step_avg:41.51ms
step:6/2090 train_time:287ms step_avg:47.75ms
step:7/2090 train_time:310ms step_avg:44.26ms
step:8/2090 train_time:343ms step_avg:42.84ms
step:9/2090 train_time:376ms step_avg:41.73ms
step:10/2090 train_time:408ms step_avg:40.83ms
step:11/2090 train_time:442ms step_avg:40.19ms
step:12/2090 train_time:475ms step_avg:39.60ms
step:13/2090 train_time:509ms step_avg:39.12ms
step:14/2090 train_time:541ms step_avg:38.67ms
step:15/2090 train_time:575ms step_avg:38.34ms
step:16/2090 train_time:608ms step_avg:38.00ms
step:17/2090 train_time:641ms step_avg:37.73ms
step:18/2090 train_time:674ms step_avg:37.46ms
step:19/2090 train_time:708ms step_avg:37.26ms
step:20/2090 train_time:741ms step_avg:37.03ms
step:21/2090 train_time:774ms step_avg:36.86ms
step:22/2090 train_time:807ms step_avg:36.68ms
step:23/2090 train_time:840ms step_avg:36.54ms
step:24/2090 train_time:873ms step_avg:36.39ms
step:25/2090 train_time:907ms step_avg:36.27ms
step:26/2090 train_time:940ms step_avg:36.14ms
step:27/2090 train_time:973ms step_avg:36.04ms
step:28/2090 train_time:1006ms step_avg:35.92ms
step:29/2090 train_time:1039ms step_avg:35.83ms
step:30/2090 train_time:1072ms step_avg:35.73ms
step:31/2090 train_time:1105ms step_avg:35.66ms
step:32/2090 train_time:1138ms step_avg:35.57ms
step:33/2090 train_time:1172ms step_avg:35.53ms
step:34/2090 train_time:1206ms step_avg:35.46ms
step:35/2090 train_time:1240ms step_avg:35.43ms
step:36/2090 train_time:1273ms step_avg:35.36ms
step:37/2090 train_time:1307ms step_avg:35.31ms
step:38/2090 train_time:1339ms step_avg:35.25ms
step:39/2090 train_time:1373ms step_avg:35.21ms
step:40/2090 train_time:1406ms step_avg:35.15ms
step:41/2090 train_time:1440ms step_avg:35.12ms
step:42/2090 train_time:1473ms step_avg:35.06ms
step:43/2090 train_time:1506ms step_avg:35.02ms
step:44/2090 train_time:1539ms step_avg:34.98ms
step:45/2090 train_time:1572ms step_avg:34.94ms
step:46/2090 train_time:1605ms step_avg:34.90ms
step:47/2090 train_time:1639ms step_avg:34.87ms
step:48/2090 train_time:1672ms step_avg:34.83ms
step:49/2090 train_time:1705ms step_avg:34.80ms
step:50/2090 train_time:1738ms step_avg:34.76ms
step:51/2090 train_time:1772ms step_avg:34.74ms
step:52/2090 train_time:1804ms step_avg:34.70ms
step:53/2090 train_time:1838ms step_avg:34.68ms
step:54/2090 train_time:1871ms step_avg:34.64ms
step:55/2090 train_time:1904ms step_avg:34.62ms
step:56/2090 train_time:1937ms step_avg:34.59ms
step:57/2090 train_time:1971ms step_avg:34.57ms
step:58/2090 train_time:2004ms step_avg:34.55ms
step:59/2090 train_time:2037ms step_avg:34.52ms
step:60/2090 train_time:2070ms step_avg:34.49ms
step:61/2090 train_time:2104ms step_avg:34.49ms
step:62/2090 train_time:2136ms step_avg:34.46ms
step:63/2090 train_time:2170ms step_avg:34.45ms
step:64/2090 train_time:2203ms step_avg:34.42ms
step:65/2090 train_time:2236ms step_avg:34.41ms
step:66/2090 train_time:2269ms step_avg:34.38ms
step:67/2090 train_time:2303ms step_avg:34.37ms
step:68/2090 train_time:2336ms step_avg:34.35ms
step:69/2090 train_time:2369ms step_avg:34.34ms
step:70/2090 train_time:2402ms step_avg:34.31ms
step:71/2090 train_time:2435ms step_avg:34.30ms
step:72/2090 train_time:2468ms step_avg:34.28ms
step:73/2090 train_time:2502ms step_avg:34.27ms
step:74/2090 train_time:2535ms step_avg:34.25ms
step:75/2090 train_time:2568ms step_avg:34.24ms
step:76/2090 train_time:2601ms step_avg:34.22ms
step:77/2090 train_time:2634ms step_avg:34.21ms
step:78/2090 train_time:2667ms step_avg:34.19ms
step:79/2090 train_time:2700ms step_avg:34.18ms
step:80/2090 train_time:2733ms step_avg:34.16ms
step:81/2090 train_time:2766ms step_avg:34.15ms
step:82/2090 train_time:2799ms step_avg:34.13ms
step:83/2090 train_time:2832ms step_avg:34.12ms
step:84/2090 train_time:2865ms step_avg:34.10ms
step:85/2090 train_time:2898ms step_avg:34.09ms
step:86/2090 train_time:2931ms step_avg:34.08ms
step:87/2090 train_time:2964ms step_avg:34.07ms
step:88/2090 train_time:2997ms step_avg:34.06ms
step:89/2090 train_time:3030ms step_avg:34.05ms
step:90/2090 train_time:3063ms step_avg:34.03ms
step:91/2090 train_time:3096ms step_avg:34.02ms
step:92/2090 train_time:3129ms step_avg:34.01ms
step:93/2090 train_time:3162ms step_avg:34.00ms
step:94/2090 train_time:3195ms step_avg:33.99ms
step:95/2090 train_time:3229ms step_avg:33.99ms
step:96/2090 train_time:3261ms step_avg:33.97ms
step:97/2090 train_time:3295ms step_avg:33.97ms
step:98/2090 train_time:3328ms step_avg:33.96ms
step:99/2090 train_time:3361ms step_avg:33.95ms
step:100/2090 train_time:3394ms step_avg:33.94ms
step:101/2090 train_time:3428ms step_avg:33.94ms
step:102/2090 train_time:3461ms step_avg:33.93ms
step:103/2090 train_time:3494ms step_avg:33.92ms
step:104/2090 train_time:3527ms step_avg:33.91ms
step:105/2090 train_time:3560ms step_avg:33.91ms
step:106/2090 train_time:3593ms step_avg:33.90ms
step:107/2090 train_time:3626ms step_avg:33.89ms
step:108/2090 train_time:3660ms step_avg:33.88ms
step:109/2090 train_time:3692ms step_avg:33.88ms
step:110/2090 train_time:3725ms step_avg:33.87ms
step:111/2090 train_time:3758ms step_avg:33.86ms
step:112/2090 train_time:3791ms step_avg:33.85ms
step:113/2090 train_time:3824ms step_avg:33.84ms
step:114/2090 train_time:3857ms step_avg:33.83ms
step:115/2090 train_time:3890ms step_avg:33.83ms
step:116/2090 train_time:3923ms step_avg:33.82ms
step:117/2090 train_time:3956ms step_avg:33.82ms
step:118/2090 train_time:3989ms step_avg:33.81ms
step:119/2090 train_time:4022ms step_avg:33.80ms
step:120/2090 train_time:4055ms step_avg:33.79ms
step:121/2090 train_time:4088ms step_avg:33.79ms
step:122/2090 train_time:4121ms step_avg:33.78ms
step:123/2090 train_time:4154ms step_avg:33.78ms
step:124/2090 train_time:4187ms step_avg:33.77ms
step:125/2090 train_time:4220ms step_avg:33.76ms
step:126/2090 train_time:4253ms step_avg:33.76ms
step:127/2090 train_time:4287ms step_avg:33.75ms
step:128/2090 train_time:4320ms step_avg:33.75ms
step:129/2090 train_time:4353ms step_avg:33.74ms
step:130/2090 train_time:4386ms step_avg:33.73ms
step:131/2090 train_time:4419ms step_avg:33.73ms
step:132/2090 train_time:4452ms step_avg:33.73ms
step:133/2090 train_time:4486ms step_avg:33.73ms
step:134/2090 train_time:4518ms step_avg:33.72ms
step:135/2090 train_time:4552ms step_avg:33.72ms
step:136/2090 train_time:4584ms step_avg:33.71ms
step:137/2090 train_time:4618ms step_avg:33.71ms
step:138/2090 train_time:4650ms step_avg:33.70ms
step:139/2090 train_time:4684ms step_avg:33.70ms
step:140/2090 train_time:4716ms step_avg:33.69ms
step:141/2090 train_time:4750ms step_avg:33.68ms
step:142/2090 train_time:4782ms step_avg:33.68ms
step:143/2090 train_time:4815ms step_avg:33.67ms
step:144/2090 train_time:4848ms step_avg:33.67ms
step:145/2090 train_time:4881ms step_avg:33.66ms
step:146/2090 train_time:4914ms step_avg:33.66ms
step:147/2090 train_time:4947ms step_avg:33.65ms
step:148/2090 train_time:4980ms step_avg:33.65ms
step:149/2090 train_time:5013ms step_avg:33.64ms
step:150/2090 train_time:5046ms step_avg:33.64ms
step:151/2090 train_time:5079ms step_avg:33.64ms
step:152/2090 train_time:5112ms step_avg:33.63ms
step:153/2090 train_time:5145ms step_avg:33.63ms
step:154/2090 train_time:5178ms step_avg:33.62ms
step:155/2090 train_time:5211ms step_avg:33.62ms
step:156/2090 train_time:5244ms step_avg:33.61ms
step:157/2090 train_time:5277ms step_avg:33.61ms
step:158/2090 train_time:5310ms step_avg:33.61ms
step:159/2090 train_time:5343ms step_avg:33.61ms
step:160/2090 train_time:5376ms step_avg:33.60ms
step:161/2090 train_time:5409ms step_avg:33.60ms
step:162/2090 train_time:5442ms step_avg:33.59ms
step:163/2090 train_time:5476ms step_avg:33.59ms
step:164/2090 train_time:5508ms step_avg:33.59ms
step:165/2090 train_time:5541ms step_avg:33.58ms
step:166/2090 train_time:5574ms step_avg:33.58ms
step:167/2090 train_time:5607ms step_avg:33.58ms
step:168/2090 train_time:5640ms step_avg:33.57ms
step:169/2090 train_time:5673ms step_avg:33.57ms
step:170/2090 train_time:5706ms step_avg:33.56ms
step:171/2090 train_time:5739ms step_avg:33.56ms
step:172/2090 train_time:5771ms step_avg:33.55ms
step:173/2090 train_time:5805ms step_avg:33.55ms
step:174/2090 train_time:5837ms step_avg:33.55ms
step:175/2090 train_time:5871ms step_avg:33.55ms
step:176/2090 train_time:5903ms step_avg:33.54ms
step:177/2090 train_time:5937ms step_avg:33.54ms
step:178/2090 train_time:5969ms step_avg:33.54ms
step:179/2090 train_time:6003ms step_avg:33.54ms
step:180/2090 train_time:6036ms step_avg:33.53ms
step:181/2090 train_time:6069ms step_avg:33.53ms
step:182/2090 train_time:6102ms step_avg:33.53ms
step:183/2090 train_time:6135ms step_avg:33.52ms
step:184/2090 train_time:6167ms step_avg:33.52ms
step:185/2090 train_time:6200ms step_avg:33.51ms
step:186/2090 train_time:6233ms step_avg:33.51ms
step:187/2090 train_time:6266ms step_avg:33.51ms
step:188/2090 train_time:6299ms step_avg:33.50ms
step:189/2090 train_time:6332ms step_avg:33.50ms
step:190/2090 train_time:6365ms step_avg:33.50ms
step:191/2090 train_time:6399ms step_avg:33.50ms
step:192/2090 train_time:6431ms step_avg:33.50ms
step:193/2090 train_time:6465ms step_avg:33.50ms
step:194/2090 train_time:6498ms step_avg:33.49ms
step:195/2090 train_time:6531ms step_avg:33.49ms
step:196/2090 train_time:6564ms step_avg:33.49ms
step:197/2090 train_time:6597ms step_avg:33.49ms
step:198/2090 train_time:6630ms step_avg:33.48ms
step:199/2090 train_time:6663ms step_avg:33.48ms
step:200/2090 train_time:6696ms step_avg:33.48ms
step:201/2090 train_time:6729ms step_avg:33.48ms
step:202/2090 train_time:6762ms step_avg:33.47ms
step:203/2090 train_time:6795ms step_avg:33.47ms
step:204/2090 train_time:6827ms step_avg:33.47ms
step:205/2090 train_time:6861ms step_avg:33.47ms
step:206/2090 train_time:6893ms step_avg:33.46ms
step:207/2090 train_time:6927ms step_avg:33.46ms
step:208/2090 train_time:6959ms step_avg:33.46ms
step:209/2090 train_time:6992ms step_avg:33.46ms
step:210/2090 train_time:7025ms step_avg:33.45ms
step:211/2090 train_time:7058ms step_avg:33.45ms
step:212/2090 train_time:7091ms step_avg:33.45ms
step:213/2090 train_time:7124ms step_avg:33.45ms
step:214/2090 train_time:7157ms step_avg:33.44ms
step:215/2090 train_time:7190ms step_avg:33.44ms
step:216/2090 train_time:7223ms step_avg:33.44ms
step:217/2090 train_time:7256ms step_avg:33.44ms
step:218/2090 train_time:7289ms step_avg:33.44ms
step:219/2090 train_time:7323ms step_avg:33.44ms
step:220/2090 train_time:7355ms step_avg:33.43ms
step:221/2090 train_time:7389ms step_avg:33.43ms
step:222/2090 train_time:7421ms step_avg:33.43ms
step:223/2090 train_time:7454ms step_avg:33.43ms
step:224/2090 train_time:7487ms step_avg:33.42ms
step:225/2090 train_time:7521ms step_avg:33.42ms
step:226/2090 train_time:7553ms step_avg:33.42ms
step:227/2090 train_time:7587ms step_avg:33.42ms
step:228/2090 train_time:7620ms step_avg:33.42ms
step:229/2090 train_time:7653ms step_avg:33.42ms
step:230/2090 train_time:7686ms step_avg:33.42ms
step:231/2090 train_time:7719ms step_avg:33.41ms
step:232/2090 train_time:7751ms step_avg:33.41ms
step:233/2090 train_time:7785ms step_avg:33.41ms
step:234/2090 train_time:7817ms step_avg:33.41ms
step:235/2090 train_time:7851ms step_avg:33.41ms
step:236/2090 train_time:7883ms step_avg:33.40ms
step:237/2090 train_time:7916ms step_avg:33.40ms
step:238/2090 train_time:7949ms step_avg:33.40ms
step:239/2090 train_time:7982ms step_avg:33.40ms
step:240/2090 train_time:8014ms step_avg:33.39ms
step:241/2090 train_time:8047ms step_avg:33.39ms
step:242/2090 train_time:8080ms step_avg:33.39ms
step:243/2090 train_time:8113ms step_avg:33.39ms
step:244/2090 train_time:8146ms step_avg:33.38ms
step:245/2090 train_time:8179ms step_avg:33.39ms
step:246/2090 train_time:8212ms step_avg:33.38ms
step:247/2090 train_time:8246ms step_avg:33.38ms
step:248/2090 train_time:8279ms step_avg:33.38ms
step:249/2090 train_time:8312ms step_avg:33.38ms
step:250/2090 train_time:8344ms step_avg:33.38ms
step:250/2090 val_loss:4.3296 train_time:8380ms step_avg:33.52ms
step:251/2090 train_time:8400ms step_avg:33.47ms
step:252/2090 train_time:8423ms step_avg:33.42ms
step:253/2090 train_time:8447ms step_avg:33.39ms
step:254/2090 train_time:8481ms step_avg:33.39ms
step:255/2090 train_time:8519ms step_avg:33.41ms
step:256/2090 train_time:8553ms step_avg:33.41ms
step:257/2090 train_time:8588ms step_avg:33.41ms
step:258/2090 train_time:8620ms step_avg:33.41ms
step:259/2090 train_time:8654ms step_avg:33.41ms
step:260/2090 train_time:8687ms step_avg:33.41ms
step:261/2090 train_time:8720ms step_avg:33.41ms
step:262/2090 train_time:8753ms step_avg:33.41ms
step:263/2090 train_time:8786ms step_avg:33.41ms
step:264/2090 train_time:8819ms step_avg:33.40ms
step:265/2090 train_time:8851ms step_avg:33.40ms
step:266/2090 train_time:8884ms step_avg:33.40ms
step:267/2090 train_time:8917ms step_avg:33.40ms
step:268/2090 train_time:8950ms step_avg:33.40ms
step:269/2090 train_time:8983ms step_avg:33.39ms
step:270/2090 train_time:9016ms step_avg:33.39ms
step:271/2090 train_time:9048ms step_avg:33.39ms
step:272/2090 train_time:9081ms step_avg:33.39ms
step:273/2090 train_time:9115ms step_avg:33.39ms
step:274/2090 train_time:9147ms step_avg:33.38ms
step:275/2090 train_time:9180ms step_avg:33.38ms
step:276/2090 train_time:9213ms step_avg:33.38ms
step:277/2090 train_time:9246ms step_avg:33.38ms
step:278/2090 train_time:9279ms step_avg:33.38ms
step:279/2090 train_time:9311ms step_avg:33.37ms
step:280/2090 train_time:9344ms step_avg:33.37ms
step:281/2090 train_time:9377ms step_avg:33.37ms
step:282/2090 train_time:9410ms step_avg:33.37ms
step:283/2090 train_time:9443ms step_avg:33.37ms
step:284/2090 train_time:9476ms step_avg:33.37ms
step:285/2090 train_time:9510ms step_avg:33.37ms
step:286/2090 train_time:9543ms step_avg:33.37ms
step:287/2090 train_time:9576ms step_avg:33.37ms
step:288/2090 train_time:9609ms step_avg:33.36ms
step:289/2090 train_time:9643ms step_avg:33.37ms
step:290/2090 train_time:9675ms step_avg:33.36ms
step:291/2090 train_time:9709ms step_avg:33.36ms
step:292/2090 train_time:9741ms step_avg:33.36ms
step:293/2090 train_time:9775ms step_avg:33.36ms
step:294/2090 train_time:9808ms step_avg:33.36ms
step:295/2090 train_time:9841ms step_avg:33.36ms
step:296/2090 train_time:9874ms step_avg:33.36ms
step:297/2090 train_time:9907ms step_avg:33.36ms
step:298/2090 train_time:9940ms step_avg:33.35ms
step:299/2090 train_time:9973ms step_avg:33.35ms
step:300/2090 train_time:10005ms step_avg:33.35ms
step:301/2090 train_time:10038ms step_avg:33.35ms
step:302/2090 train_time:10071ms step_avg:33.35ms
step:303/2090 train_time:10104ms step_avg:33.35ms
step:304/2090 train_time:10137ms step_avg:33.34ms
step:305/2090 train_time:10170ms step_avg:33.34ms
step:306/2090 train_time:10203ms step_avg:33.34ms
step:307/2090 train_time:10236ms step_avg:33.34ms
step:308/2090 train_time:10268ms step_avg:33.34ms
step:309/2090 train_time:10301ms step_avg:33.34ms
step:310/2090 train_time:10334ms step_avg:33.34ms
step:311/2090 train_time:10367ms step_avg:33.34ms
step:312/2090 train_time:10400ms step_avg:33.33ms
step:313/2090 train_time:10433ms step_avg:33.33ms
step:314/2090 train_time:10466ms step_avg:33.33ms
step:315/2090 train_time:10500ms step_avg:33.33ms
step:316/2090 train_time:10532ms step_avg:33.33ms
step:317/2090 train_time:10566ms step_avg:33.33ms
step:318/2090 train_time:10599ms step_avg:33.33ms
step:319/2090 train_time:10632ms step_avg:33.33ms
step:320/2090 train_time:10664ms step_avg:33.33ms
step:321/2090 train_time:10698ms step_avg:33.33ms
step:322/2090 train_time:10731ms step_avg:33.33ms
step:323/2090 train_time:10764ms step_avg:33.33ms
step:324/2090 train_time:10797ms step_avg:33.32ms
step:325/2090 train_time:10830ms step_avg:33.32ms
step:326/2090 train_time:10863ms step_avg:33.32ms
step:327/2090 train_time:10896ms step_avg:33.32ms
step:328/2090 train_time:10929ms step_avg:33.32ms
step:329/2090 train_time:10962ms step_avg:33.32ms
step:330/2090 train_time:10994ms step_avg:33.32ms
step:331/2090 train_time:11028ms step_avg:33.32ms
step:332/2090 train_time:11060ms step_avg:33.31ms
step:333/2090 train_time:11094ms step_avg:33.31ms
step:334/2090 train_time:11127ms step_avg:33.31ms
step:335/2090 train_time:11159ms step_avg:33.31ms
step:336/2090 train_time:11192ms step_avg:33.31ms
step:337/2090 train_time:11225ms step_avg:33.31ms
step:338/2090 train_time:11258ms step_avg:33.31ms
step:339/2090 train_time:11291ms step_avg:33.31ms
step:340/2090 train_time:11324ms step_avg:33.30ms
step:341/2090 train_time:11357ms step_avg:33.30ms
step:342/2090 train_time:11390ms step_avg:33.30ms
step:343/2090 train_time:11423ms step_avg:33.30ms
step:344/2090 train_time:11455ms step_avg:33.30ms
step:345/2090 train_time:11489ms step_avg:33.30ms
step:346/2090 train_time:11522ms step_avg:33.30ms
step:347/2090 train_time:11555ms step_avg:33.30ms
step:348/2090 train_time:11587ms step_avg:33.30ms
step:349/2090 train_time:11621ms step_avg:33.30ms
step:350/2090 train_time:11654ms step_avg:33.30ms
step:351/2090 train_time:11687ms step_avg:33.30ms
step:352/2090 train_time:11720ms step_avg:33.29ms
step:353/2090 train_time:11753ms step_avg:33.30ms
step:354/2090 train_time:11786ms step_avg:33.29ms
step:355/2090 train_time:11820ms step_avg:33.30ms
step:356/2090 train_time:11853ms step_avg:33.29ms
step:357/2090 train_time:11886ms step_avg:33.30ms
step:358/2090 train_time:11919ms step_avg:33.29ms
step:359/2090 train_time:11952ms step_avg:33.29ms
step:360/2090 train_time:11985ms step_avg:33.29ms
step:361/2090 train_time:12018ms step_avg:33.29ms
step:362/2090 train_time:12051ms step_avg:33.29ms
step:363/2090 train_time:12084ms step_avg:33.29ms
step:364/2090 train_time:12116ms step_avg:33.29ms
step:365/2090 train_time:12149ms step_avg:33.28ms
step:366/2090 train_time:12182ms step_avg:33.28ms
step:367/2090 train_time:12215ms step_avg:33.28ms
step:368/2090 train_time:12247ms step_avg:33.28ms
step:369/2090 train_time:12280ms step_avg:33.28ms
step:370/2090 train_time:12313ms step_avg:33.28ms
step:371/2090 train_time:12346ms step_avg:33.28ms
step:372/2090 train_time:12379ms step_avg:33.28ms
step:373/2090 train_time:12412ms step_avg:33.28ms
step:374/2090 train_time:12445ms step_avg:33.28ms
step:375/2090 train_time:12478ms step_avg:33.27ms
step:376/2090 train_time:12511ms step_avg:33.27ms
step:377/2090 train_time:12543ms step_avg:33.27ms
step:378/2090 train_time:12576ms step_avg:33.27ms
step:379/2090 train_time:12609ms step_avg:33.27ms
step:380/2090 train_time:12642ms step_avg:33.27ms
step:381/2090 train_time:12675ms step_avg:33.27ms
step:382/2090 train_time:12708ms step_avg:33.27ms
step:383/2090 train_time:12741ms step_avg:33.27ms
step:384/2090 train_time:12774ms step_avg:33.27ms
step:385/2090 train_time:12807ms step_avg:33.26ms
step:386/2090 train_time:12840ms step_avg:33.26ms
step:387/2090 train_time:12874ms step_avg:33.27ms
step:388/2090 train_time:12907ms step_avg:33.26ms
step:389/2090 train_time:12940ms step_avg:33.27ms
step:390/2090 train_time:12973ms step_avg:33.26ms
step:391/2090 train_time:13006ms step_avg:33.26ms
step:392/2090 train_time:13039ms step_avg:33.26ms
step:393/2090 train_time:13073ms step_avg:33.26ms
step:394/2090 train_time:13106ms step_avg:33.26ms
step:395/2090 train_time:13139ms step_avg:33.26ms
step:396/2090 train_time:13172ms step_avg:33.26ms
step:397/2090 train_time:13205ms step_avg:33.26ms
step:398/2090 train_time:13238ms step_avg:33.26ms
step:399/2090 train_time:13270ms step_avg:33.26ms
step:400/2090 train_time:13303ms step_avg:33.26ms
step:401/2090 train_time:13336ms step_avg:33.26ms
step:402/2090 train_time:13369ms step_avg:33.26ms
step:403/2090 train_time:13402ms step_avg:33.26ms
step:404/2090 train_time:13435ms step_avg:33.25ms
step:405/2090 train_time:13468ms step_avg:33.25ms
step:406/2090 train_time:13501ms step_avg:33.25ms
step:407/2090 train_time:13534ms step_avg:33.25ms
step:408/2090 train_time:13566ms step_avg:33.25ms
step:409/2090 train_time:13599ms step_avg:33.25ms
step:410/2090 train_time:13632ms step_avg:33.25ms
step:411/2090 train_time:13665ms step_avg:33.25ms
step:412/2090 train_time:13698ms step_avg:33.25ms
step:413/2090 train_time:13731ms step_avg:33.25ms
step:414/2090 train_time:13764ms step_avg:33.25ms
step:415/2090 train_time:13797ms step_avg:33.25ms
step:416/2090 train_time:13830ms step_avg:33.25ms
step:417/2090 train_time:13863ms step_avg:33.25ms
step:418/2090 train_time:13896ms step_avg:33.24ms
step:419/2090 train_time:13930ms step_avg:33.24ms
step:420/2090 train_time:13963ms step_avg:33.24ms
step:421/2090 train_time:13996ms step_avg:33.24ms
step:422/2090 train_time:14029ms step_avg:33.24ms
step:423/2090 train_time:14062ms step_avg:33.24ms
step:424/2090 train_time:14095ms step_avg:33.24ms
step:425/2090 train_time:14128ms step_avg:33.24ms
step:426/2090 train_time:14161ms step_avg:33.24ms
step:427/2090 train_time:14194ms step_avg:33.24ms
step:428/2090 train_time:14226ms step_avg:33.24ms
step:429/2090 train_time:14260ms step_avg:33.24ms
step:430/2090 train_time:14292ms step_avg:33.24ms
step:431/2090 train_time:14325ms step_avg:33.24ms
step:432/2090 train_time:14358ms step_avg:33.24ms
step:433/2090 train_time:14391ms step_avg:33.23ms
step:434/2090 train_time:14423ms step_avg:33.23ms
step:435/2090 train_time:14457ms step_avg:33.23ms
step:436/2090 train_time:14490ms step_avg:33.23ms
step:437/2090 train_time:14523ms step_avg:33.23ms
step:438/2090 train_time:14555ms step_avg:33.23ms
step:439/2090 train_time:14588ms step_avg:33.23ms
step:440/2090 train_time:14621ms step_avg:33.23ms
step:441/2090 train_time:14654ms step_avg:33.23ms
step:442/2090 train_time:14687ms step_avg:33.23ms
step:443/2090 train_time:14720ms step_avg:33.23ms
step:444/2090 train_time:14753ms step_avg:33.23ms
step:445/2090 train_time:14786ms step_avg:33.23ms
step:446/2090 train_time:14819ms step_avg:33.23ms
step:447/2090 train_time:14852ms step_avg:33.23ms
step:448/2090 train_time:14885ms step_avg:33.22ms
step:449/2090 train_time:14918ms step_avg:33.23ms
step:450/2090 train_time:14951ms step_avg:33.22ms
step:451/2090 train_time:14984ms step_avg:33.22ms
step:452/2090 train_time:15017ms step_avg:33.22ms
step:453/2090 train_time:15050ms step_avg:33.22ms
step:454/2090 train_time:15083ms step_avg:33.22ms
step:455/2090 train_time:15116ms step_avg:33.22ms
step:456/2090 train_time:15149ms step_avg:33.22ms
step:457/2090 train_time:15182ms step_avg:33.22ms
step:458/2090 train_time:15214ms step_avg:33.22ms
step:459/2090 train_time:15247ms step_avg:33.22ms
step:460/2090 train_time:15280ms step_avg:33.22ms
step:461/2090 train_time:15313ms step_avg:33.22ms
step:462/2090 train_time:15346ms step_avg:33.22ms
step:463/2090 train_time:15379ms step_avg:33.22ms
step:464/2090 train_time:15412ms step_avg:33.22ms
step:465/2090 train_time:15445ms step_avg:33.22ms
step:466/2090 train_time:15478ms step_avg:33.21ms
step:467/2090 train_time:15511ms step_avg:33.21ms
step:468/2090 train_time:15544ms step_avg:33.21ms
step:469/2090 train_time:15577ms step_avg:33.21ms
step:470/2090 train_time:15610ms step_avg:33.21ms
step:471/2090 train_time:15643ms step_avg:33.21ms
step:472/2090 train_time:15675ms step_avg:33.21ms
step:473/2090 train_time:15708ms step_avg:33.21ms
step:474/2090 train_time:15741ms step_avg:33.21ms
step:475/2090 train_time:15774ms step_avg:33.21ms
step:476/2090 train_time:15807ms step_avg:33.21ms
step:477/2090 train_time:15841ms step_avg:33.21ms
step:478/2090 train_time:15873ms step_avg:33.21ms
step:479/2090 train_time:15907ms step_avg:33.21ms
step:480/2090 train_time:15939ms step_avg:33.21ms
step:481/2090 train_time:15973ms step_avg:33.21ms
step:482/2090 train_time:16006ms step_avg:33.21ms
step:483/2090 train_time:16040ms step_avg:33.21ms
step:484/2090 train_time:16072ms step_avg:33.21ms
step:485/2090 train_time:16106ms step_avg:33.21ms
step:486/2090 train_time:16138ms step_avg:33.21ms
step:487/2090 train_time:16171ms step_avg:33.21ms
step:488/2090 train_time:16204ms step_avg:33.21ms
step:489/2090 train_time:16237ms step_avg:33.21ms
step:490/2090 train_time:16270ms step_avg:33.20ms
step:491/2090 train_time:16303ms step_avg:33.20ms
step:492/2090 train_time:16336ms step_avg:33.20ms
step:493/2090 train_time:16369ms step_avg:33.20ms
step:494/2090 train_time:16402ms step_avg:33.20ms
step:495/2090 train_time:16435ms step_avg:33.20ms
step:496/2090 train_time:16468ms step_avg:33.20ms
step:497/2090 train_time:16501ms step_avg:33.20ms
step:498/2090 train_time:16534ms step_avg:33.20ms
step:499/2090 train_time:16567ms step_avg:33.20ms
step:500/2090 train_time:16600ms step_avg:33.20ms
step:500/2090 val_loss:4.0058 train_time:16636ms step_avg:33.27ms
step:501/2090 train_time:16656ms step_avg:33.25ms
step:502/2090 train_time:16676ms step_avg:33.22ms
step:503/2090 train_time:16704ms step_avg:33.21ms
step:504/2090 train_time:16738ms step_avg:33.21ms
step:505/2090 train_time:16773ms step_avg:33.21ms
step:506/2090 train_time:16806ms step_avg:33.21ms
step:507/2090 train_time:16841ms step_avg:33.22ms
step:508/2090 train_time:16873ms step_avg:33.22ms
step:509/2090 train_time:16907ms step_avg:33.22ms
step:510/2090 train_time:16940ms step_avg:33.22ms
step:511/2090 train_time:16973ms step_avg:33.21ms
step:512/2090 train_time:17005ms step_avg:33.21ms
step:513/2090 train_time:17038ms step_avg:33.21ms
step:514/2090 train_time:17071ms step_avg:33.21ms
step:515/2090 train_time:17104ms step_avg:33.21ms
step:516/2090 train_time:17137ms step_avg:33.21ms
step:517/2090 train_time:17170ms step_avg:33.21ms
step:518/2090 train_time:17203ms step_avg:33.21ms
step:519/2090 train_time:17235ms step_avg:33.21ms
step:520/2090 train_time:17268ms step_avg:33.21ms
step:521/2090 train_time:17301ms step_avg:33.21ms
step:522/2090 train_time:17334ms step_avg:33.21ms
step:523/2090 train_time:17366ms step_avg:33.21ms
step:524/2090 train_time:17399ms step_avg:33.20ms
step:525/2090 train_time:17432ms step_avg:33.20ms
step:526/2090 train_time:17465ms step_avg:33.20ms
step:527/2090 train_time:17498ms step_avg:33.20ms
step:528/2090 train_time:17530ms step_avg:33.20ms
step:529/2090 train_time:17564ms step_avg:33.20ms
step:530/2090 train_time:17596ms step_avg:33.20ms
step:531/2090 train_time:17629ms step_avg:33.20ms
step:532/2090 train_time:17662ms step_avg:33.20ms
step:533/2090 train_time:17696ms step_avg:33.20ms
step:534/2090 train_time:17729ms step_avg:33.20ms
step:535/2090 train_time:17763ms step_avg:33.20ms
step:536/2090 train_time:17796ms step_avg:33.20ms
step:537/2090 train_time:17829ms step_avg:33.20ms
step:538/2090 train_time:17862ms step_avg:33.20ms
step:539/2090 train_time:17896ms step_avg:33.20ms
step:540/2090 train_time:17929ms step_avg:33.20ms
step:541/2090 train_time:17962ms step_avg:33.20ms
step:542/2090 train_time:17995ms step_avg:33.20ms
step:543/2090 train_time:18028ms step_avg:33.20ms
step:544/2090 train_time:18061ms step_avg:33.20ms
step:545/2090 train_time:18094ms step_avg:33.20ms
step:546/2090 train_time:18126ms step_avg:33.20ms
step:547/2090 train_time:18159ms step_avg:33.20ms
step:548/2090 train_time:18192ms step_avg:33.20ms
step:549/2090 train_time:18225ms step_avg:33.20ms
step:550/2090 train_time:18258ms step_avg:33.20ms
step:551/2090 train_time:18291ms step_avg:33.20ms
step:552/2090 train_time:18323ms step_avg:33.19ms
step:553/2090 train_time:18356ms step_avg:33.19ms
step:554/2090 train_time:18389ms step_avg:33.19ms
step:555/2090 train_time:18422ms step_avg:33.19ms
step:556/2090 train_time:18455ms step_avg:33.19ms
step:557/2090 train_time:18488ms step_avg:33.19ms
step:558/2090 train_time:18521ms step_avg:33.19ms
step:559/2090 train_time:18554ms step_avg:33.19ms
step:560/2090 train_time:18587ms step_avg:33.19ms
step:561/2090 train_time:18620ms step_avg:33.19ms
step:562/2090 train_time:18653ms step_avg:33.19ms
step:563/2090 train_time:18687ms step_avg:33.19ms
step:564/2090 train_time:18720ms step_avg:33.19ms
step:565/2090 train_time:18753ms step_avg:33.19ms
step:566/2090 train_time:18786ms step_avg:33.19ms
step:567/2090 train_time:18819ms step_avg:33.19ms
step:568/2090 train_time:18852ms step_avg:33.19ms
step:569/2090 train_time:18885ms step_avg:33.19ms
step:570/2090 train_time:18918ms step_avg:33.19ms
step:571/2090 train_time:18951ms step_avg:33.19ms
step:572/2090 train_time:18984ms step_avg:33.19ms
step:573/2090 train_time:19017ms step_avg:33.19ms
step:574/2090 train_time:19050ms step_avg:33.19ms
step:575/2090 train_time:19083ms step_avg:33.19ms
step:576/2090 train_time:19116ms step_avg:33.19ms
step:577/2090 train_time:19150ms step_avg:33.19ms
step:578/2090 train_time:19182ms step_avg:33.19ms
step:579/2090 train_time:19215ms step_avg:33.19ms
step:580/2090 train_time:19248ms step_avg:33.19ms
step:581/2090 train_time:19281ms step_avg:33.19ms
step:582/2090 train_time:19314ms step_avg:33.19ms
step:583/2090 train_time:19347ms step_avg:33.18ms
step:584/2090 train_time:19380ms step_avg:33.18ms
step:585/2090 train_time:19413ms step_avg:33.18ms
step:586/2090 train_time:19445ms step_avg:33.18ms
step:587/2090 train_time:19478ms step_avg:33.18ms
step:588/2090 train_time:19511ms step_avg:33.18ms
step:589/2090 train_time:19544ms step_avg:33.18ms
step:590/2090 train_time:19576ms step_avg:33.18ms
step:591/2090 train_time:19610ms step_avg:33.18ms
step:592/2090 train_time:19642ms step_avg:33.18ms
step:593/2090 train_time:19676ms step_avg:33.18ms
step:594/2090 train_time:19708ms step_avg:33.18ms
step:595/2090 train_time:19742ms step_avg:33.18ms
step:596/2090 train_time:19775ms step_avg:33.18ms
step:597/2090 train_time:19808ms step_avg:33.18ms
step:598/2090 train_time:19841ms step_avg:33.18ms
step:599/2090 train_time:19874ms step_avg:33.18ms
step:600/2090 train_time:19906ms step_avg:33.18ms
step:601/2090 train_time:19940ms step_avg:33.18ms
step:602/2090 train_time:19973ms step_avg:33.18ms
step:603/2090 train_time:20006ms step_avg:33.18ms
step:604/2090 train_time:20039ms step_avg:33.18ms
step:605/2090 train_time:20072ms step_avg:33.18ms
step:606/2090 train_time:20105ms step_avg:33.18ms
step:607/2090 train_time:20138ms step_avg:33.18ms
step:608/2090 train_time:20170ms step_avg:33.18ms
step:609/2090 train_time:20204ms step_avg:33.18ms
step:610/2090 train_time:20237ms step_avg:33.17ms
step:611/2090 train_time:20270ms step_avg:33.18ms
step:612/2090 train_time:20303ms step_avg:33.18ms
step:613/2090 train_time:20337ms step_avg:33.18ms
step:614/2090 train_time:20369ms step_avg:33.17ms
step:615/2090 train_time:20402ms step_avg:33.17ms
step:616/2090 train_time:20435ms step_avg:33.17ms
step:617/2090 train_time:20468ms step_avg:33.17ms
step:618/2090 train_time:20501ms step_avg:33.17ms
step:619/2090 train_time:20534ms step_avg:33.17ms
step:620/2090 train_time:20567ms step_avg:33.17ms
step:621/2090 train_time:20599ms step_avg:33.17ms
step:622/2090 train_time:20632ms step_avg:33.17ms
step:623/2090 train_time:20665ms step_avg:33.17ms
step:624/2090 train_time:20698ms step_avg:33.17ms
step:625/2090 train_time:20731ms step_avg:33.17ms
step:626/2090 train_time:20764ms step_avg:33.17ms
step:627/2090 train_time:20797ms step_avg:33.17ms
step:628/2090 train_time:20830ms step_avg:33.17ms
step:629/2090 train_time:20863ms step_avg:33.17ms
step:630/2090 train_time:20896ms step_avg:33.17ms
step:631/2090 train_time:20929ms step_avg:33.17ms
step:632/2090 train_time:20962ms step_avg:33.17ms
step:633/2090 train_time:20995ms step_avg:33.17ms
step:634/2090 train_time:21028ms step_avg:33.17ms
step:635/2090 train_time:21062ms step_avg:33.17ms
step:636/2090 train_time:21095ms step_avg:33.17ms
step:637/2090 train_time:21128ms step_avg:33.17ms
step:638/2090 train_time:21161ms step_avg:33.17ms
step:639/2090 train_time:21194ms step_avg:33.17ms
step:640/2090 train_time:21227ms step_avg:33.17ms
step:641/2090 train_time:21261ms step_avg:33.17ms
step:642/2090 train_time:21293ms step_avg:33.17ms
step:643/2090 train_time:21327ms step_avg:33.17ms
step:644/2090 train_time:21359ms step_avg:33.17ms
step:645/2090 train_time:21392ms step_avg:33.17ms
step:646/2090 train_time:21425ms step_avg:33.17ms
step:647/2090 train_time:21458ms step_avg:33.17ms
step:648/2090 train_time:21491ms step_avg:33.17ms
step:649/2090 train_time:21524ms step_avg:33.16ms
step:650/2090 train_time:21557ms step_avg:33.16ms
step:651/2090 train_time:21590ms step_avg:33.16ms
step:652/2090 train_time:21623ms step_avg:33.16ms
step:653/2090 train_time:21656ms step_avg:33.16ms
step:654/2090 train_time:21688ms step_avg:33.16ms
step:655/2090 train_time:21722ms step_avg:33.16ms
step:656/2090 train_time:21755ms step_avg:33.16ms
step:657/2090 train_time:21788ms step_avg:33.16ms
step:658/2090 train_time:21820ms step_avg:33.16ms
step:659/2090 train_time:21854ms step_avg:33.16ms
step:660/2090 train_time:21886ms step_avg:33.16ms
step:661/2090 train_time:21920ms step_avg:33.16ms
step:662/2090 train_time:21952ms step_avg:33.16ms
step:663/2090 train_time:21986ms step_avg:33.16ms
step:664/2090 train_time:22019ms step_avg:33.16ms
step:665/2090 train_time:22052ms step_avg:33.16ms
step:666/2090 train_time:22084ms step_avg:33.16ms
step:667/2090 train_time:22118ms step_avg:33.16ms
step:668/2090 train_time:22151ms step_avg:33.16ms
step:669/2090 train_time:22184ms step_avg:33.16ms
step:670/2090 train_time:22217ms step_avg:33.16ms
step:671/2090 train_time:22250ms step_avg:33.16ms
step:672/2090 train_time:22283ms step_avg:33.16ms
step:673/2090 train_time:22316ms step_avg:33.16ms
step:674/2090 train_time:22349ms step_avg:33.16ms
step:675/2090 train_time:22382ms step_avg:33.16ms
step:676/2090 train_time:22415ms step_avg:33.16ms
step:677/2090 train_time:22448ms step_avg:33.16ms
step:678/2090 train_time:22481ms step_avg:33.16ms
step:679/2090 train_time:22514ms step_avg:33.16ms
step:680/2090 train_time:22547ms step_avg:33.16ms
step:681/2090 train_time:22580ms step_avg:33.16ms
step:682/2090 train_time:22613ms step_avg:33.16ms
step:683/2090 train_time:22646ms step_avg:33.16ms
step:684/2090 train_time:22679ms step_avg:33.16ms
step:685/2090 train_time:22713ms step_avg:33.16ms
step:686/2090 train_time:22772ms step_avg:33.19ms
step:687/2090 train_time:22832ms step_avg:33.23ms
step:688/2090 train_time:22892ms step_avg:33.27ms
step:689/2090 train_time:22954ms step_avg:33.31ms
step:690/2090 train_time:23013ms step_avg:33.35ms
step:691/2090 train_time:23075ms step_avg:33.39ms
step:692/2090 train_time:23136ms step_avg:33.43ms
step:693/2090 train_time:23196ms step_avg:33.47ms
step:694/2090 train_time:23256ms step_avg:33.51ms
step:695/2090 train_time:23316ms step_avg:33.55ms
step:696/2090 train_time:23376ms step_avg:33.59ms
step:697/2090 train_time:23436ms step_avg:33.62ms
step:698/2090 train_time:23496ms step_avg:33.66ms
step:699/2090 train_time:23556ms step_avg:33.70ms
step:700/2090 train_time:23616ms step_avg:33.74ms
step:701/2090 train_time:23677ms step_avg:33.78ms
step:702/2090 train_time:23736ms step_avg:33.81ms
step:703/2090 train_time:23797ms step_avg:33.85ms
step:704/2090 train_time:23856ms step_avg:33.89ms
step:705/2090 train_time:23917ms step_avg:33.92ms
step:706/2090 train_time:23977ms step_avg:33.96ms
step:707/2090 train_time:24037ms step_avg:34.00ms
step:708/2090 train_time:24097ms step_avg:34.03ms
step:709/2090 train_time:24158ms step_avg:34.07ms
step:710/2090 train_time:24217ms step_avg:34.11ms
step:711/2090 train_time:24277ms step_avg:34.14ms
step:712/2090 train_time:24336ms step_avg:34.18ms
step:713/2090 train_time:24397ms step_avg:34.22ms
step:714/2090 train_time:24456ms step_avg:34.25ms
step:715/2090 train_time:24516ms step_avg:34.29ms
step:716/2090 train_time:24576ms step_avg:34.32ms
step:717/2090 train_time:24636ms step_avg:34.36ms
step:718/2090 train_time:24696ms step_avg:34.40ms
step:719/2090 train_time:24757ms step_avg:34.43ms
step:720/2090 train_time:24816ms step_avg:34.47ms
step:721/2090 train_time:24877ms step_avg:34.50ms
step:722/2090 train_time:24937ms step_avg:34.54ms
step:723/2090 train_time:24997ms step_avg:34.57ms
step:724/2090 train_time:25056ms step_avg:34.61ms
step:725/2090 train_time:25117ms step_avg:34.64ms
step:726/2090 train_time:25176ms step_avg:34.68ms
step:727/2090 train_time:25237ms step_avg:34.71ms
step:728/2090 train_time:25297ms step_avg:34.75ms
step:729/2090 train_time:25358ms step_avg:34.78ms
step:730/2090 train_time:25417ms step_avg:34.82ms
step:731/2090 train_time:25477ms step_avg:34.85ms
step:732/2090 train_time:25537ms step_avg:34.89ms
step:733/2090 train_time:25597ms step_avg:34.92ms
step:734/2090 train_time:25656ms step_avg:34.95ms
step:735/2090 train_time:25717ms step_avg:34.99ms
step:736/2090 train_time:25776ms step_avg:35.02ms
step:737/2090 train_time:25837ms step_avg:35.06ms
step:738/2090 train_time:25897ms step_avg:35.09ms
step:739/2090 train_time:25957ms step_avg:35.13ms
step:740/2090 train_time:26017ms step_avg:35.16ms
step:741/2090 train_time:26077ms step_avg:35.19ms
step:742/2090 train_time:26137ms step_avg:35.22ms
step:743/2090 train_time:26198ms step_avg:35.26ms
step:744/2090 train_time:26257ms step_avg:35.29ms
step:745/2090 train_time:26318ms step_avg:35.33ms
step:746/2090 train_time:26377ms step_avg:35.36ms
step:747/2090 train_time:26437ms step_avg:35.39ms
step:748/2090 train_time:26496ms step_avg:35.42ms
step:749/2090 train_time:26556ms step_avg:35.46ms
step:750/2090 train_time:26616ms step_avg:35.49ms
step:750/2090 val_loss:3.8518 train_time:26678ms step_avg:35.57ms
step:751/2090 train_time:26699ms step_avg:35.55ms
step:752/2090 train_time:26737ms step_avg:35.55ms
step:753/2090 train_time:26800ms step_avg:35.59ms
step:754/2090 train_time:26862ms step_avg:35.63ms
step:755/2090 train_time:26923ms step_avg:35.66ms
step:756/2090 train_time:26983ms step_avg:35.69ms
step:757/2090 train_time:27042ms step_avg:35.72ms
step:758/2090 train_time:27101ms step_avg:35.75ms
step:759/2090 train_time:27160ms step_avg:35.78ms
step:760/2090 train_time:27219ms step_avg:35.81ms
step:761/2090 train_time:27278ms step_avg:35.85ms
step:762/2090 train_time:27337ms step_avg:35.88ms
step:763/2090 train_time:27397ms step_avg:35.91ms
step:764/2090 train_time:27456ms step_avg:35.94ms
step:765/2090 train_time:27516ms step_avg:35.97ms
step:766/2090 train_time:27575ms step_avg:36.00ms
step:767/2090 train_time:27637ms step_avg:36.03ms
step:768/2090 train_time:27698ms step_avg:36.06ms
step:769/2090 train_time:27760ms step_avg:36.10ms
step:770/2090 train_time:27821ms step_avg:36.13ms
step:771/2090 train_time:27881ms step_avg:36.16ms
step:772/2090 train_time:27942ms step_avg:36.19ms
step:773/2090 train_time:28001ms step_avg:36.22ms
step:774/2090 train_time:28061ms step_avg:36.25ms
step:775/2090 train_time:28120ms step_avg:36.28ms
step:776/2090 train_time:28179ms step_avg:36.31ms
step:777/2090 train_time:28239ms step_avg:36.34ms
step:778/2090 train_time:28297ms step_avg:36.37ms
step:779/2090 train_time:28358ms step_avg:36.40ms
step:780/2090 train_time:28416ms step_avg:36.43ms
step:781/2090 train_time:28476ms step_avg:36.46ms
step:782/2090 train_time:28535ms step_avg:36.49ms
step:783/2090 train_time:28595ms step_avg:36.52ms
step:784/2090 train_time:28655ms step_avg:36.55ms
step:785/2090 train_time:28717ms step_avg:36.58ms
step:786/2090 train_time:28778ms step_avg:36.61ms
step:787/2090 train_time:28838ms step_avg:36.64ms
step:788/2090 train_time:28898ms step_avg:36.67ms
step:789/2090 train_time:28958ms step_avg:36.70ms
step:790/2090 train_time:29018ms step_avg:36.73ms
step:791/2090 train_time:29079ms step_avg:36.76ms
step:792/2090 train_time:29138ms step_avg:36.79ms
step:793/2090 train_time:29198ms step_avg:36.82ms
step:794/2090 train_time:29257ms step_avg:36.85ms
step:795/2090 train_time:29317ms step_avg:36.88ms
step:796/2090 train_time:29376ms step_avg:36.90ms
step:797/2090 train_time:29436ms step_avg:36.93ms
step:798/2090 train_time:29495ms step_avg:36.96ms
step:799/2090 train_time:29556ms step_avg:36.99ms
step:800/2090 train_time:29615ms step_avg:37.02ms
step:801/2090 train_time:29676ms step_avg:37.05ms
step:802/2090 train_time:29736ms step_avg:37.08ms
step:803/2090 train_time:29797ms step_avg:37.11ms
step:804/2090 train_time:29856ms step_avg:37.13ms
step:805/2090 train_time:29917ms step_avg:37.16ms
step:806/2090 train_time:29977ms step_avg:37.19ms
step:807/2090 train_time:30039ms step_avg:37.22ms
step:808/2090 train_time:30098ms step_avg:37.25ms
step:809/2090 train_time:30159ms step_avg:37.28ms
step:810/2090 train_time:30218ms step_avg:37.31ms
step:811/2090 train_time:30278ms step_avg:37.33ms
step:812/2090 train_time:30337ms step_avg:37.36ms
step:813/2090 train_time:30397ms step_avg:37.39ms
step:814/2090 train_time:30456ms step_avg:37.42ms
step:815/2090 train_time:30517ms step_avg:37.44ms
step:816/2090 train_time:30576ms step_avg:37.47ms
step:817/2090 train_time:30636ms step_avg:37.50ms
step:818/2090 train_time:30696ms step_avg:37.53ms
step:819/2090 train_time:30757ms step_avg:37.55ms
step:820/2090 train_time:30817ms step_avg:37.58ms
step:821/2090 train_time:30877ms step_avg:37.61ms
step:822/2090 train_time:30937ms step_avg:37.64ms
step:823/2090 train_time:30997ms step_avg:37.66ms
step:824/2090 train_time:31057ms step_avg:37.69ms
step:825/2090 train_time:31117ms step_avg:37.72ms
step:826/2090 train_time:31176ms step_avg:37.74ms
step:827/2090 train_time:31236ms step_avg:37.77ms
step:828/2090 train_time:31295ms step_avg:37.80ms
step:829/2090 train_time:31355ms step_avg:37.82ms
step:830/2090 train_time:31414ms step_avg:37.85ms
step:831/2090 train_time:31475ms step_avg:37.88ms
step:832/2090 train_time:31534ms step_avg:37.90ms
step:833/2090 train_time:31594ms step_avg:37.93ms
step:834/2090 train_time:31653ms step_avg:37.95ms
step:835/2090 train_time:31714ms step_avg:37.98ms
step:836/2090 train_time:31774ms step_avg:38.01ms
step:837/2090 train_time:31834ms step_avg:38.03ms
step:838/2090 train_time:31894ms step_avg:38.06ms
step:839/2090 train_time:31954ms step_avg:38.09ms
step:840/2090 train_time:32014ms step_avg:38.11ms
step:841/2090 train_time:32075ms step_avg:38.14ms
step:842/2090 train_time:32135ms step_avg:38.16ms
step:843/2090 train_time:32195ms step_avg:38.19ms
step:844/2090 train_time:32254ms step_avg:38.22ms
step:845/2090 train_time:32314ms step_avg:38.24ms
step:846/2090 train_time:32373ms step_avg:38.27ms
step:847/2090 train_time:32434ms step_avg:38.29ms
step:848/2090 train_time:32494ms step_avg:38.32ms
step:849/2090 train_time:32554ms step_avg:38.34ms
step:850/2090 train_time:32612ms step_avg:38.37ms
step:851/2090 train_time:32672ms step_avg:38.39ms
step:852/2090 train_time:32732ms step_avg:38.42ms
step:853/2090 train_time:32793ms step_avg:38.44ms
step:854/2090 train_time:32853ms step_avg:38.47ms
step:855/2090 train_time:32914ms step_avg:38.50ms
step:856/2090 train_time:32973ms step_avg:38.52ms
step:857/2090 train_time:33034ms step_avg:38.55ms
step:858/2090 train_time:33093ms step_avg:38.57ms
step:859/2090 train_time:33153ms step_avg:38.59ms
step:860/2090 train_time:33212ms step_avg:38.62ms
step:861/2090 train_time:33272ms step_avg:38.64ms
step:862/2090 train_time:33331ms step_avg:38.67ms
step:863/2090 train_time:33391ms step_avg:38.69ms
step:864/2090 train_time:33450ms step_avg:38.72ms
step:865/2090 train_time:33510ms step_avg:38.74ms
step:866/2090 train_time:33569ms step_avg:38.76ms
step:867/2090 train_time:33630ms step_avg:38.79ms
step:868/2090 train_time:33689ms step_avg:38.81ms
step:869/2090 train_time:33750ms step_avg:38.84ms
step:870/2090 train_time:33810ms step_avg:38.86ms
step:871/2090 train_time:33871ms step_avg:38.89ms
step:872/2090 train_time:33931ms step_avg:38.91ms
step:873/2090 train_time:33992ms step_avg:38.94ms
step:874/2090 train_time:34052ms step_avg:38.96ms
step:875/2090 train_time:34113ms step_avg:38.99ms
step:876/2090 train_time:34172ms step_avg:39.01ms
step:877/2090 train_time:34234ms step_avg:39.03ms
step:878/2090 train_time:34293ms step_avg:39.06ms
step:879/2090 train_time:34353ms step_avg:39.08ms
step:880/2090 train_time:34412ms step_avg:39.10ms
step:881/2090 train_time:34473ms step_avg:39.13ms
step:882/2090 train_time:34532ms step_avg:39.15ms
step:883/2090 train_time:34593ms step_avg:39.18ms
step:884/2090 train_time:34653ms step_avg:39.20ms
step:885/2090 train_time:34714ms step_avg:39.22ms
step:886/2090 train_time:34774ms step_avg:39.25ms
step:887/2090 train_time:34835ms step_avg:39.27ms
step:888/2090 train_time:34895ms step_avg:39.30ms
step:889/2090 train_time:34955ms step_avg:39.32ms
step:890/2090 train_time:35016ms step_avg:39.34ms
step:891/2090 train_time:35076ms step_avg:39.37ms
step:892/2090 train_time:35136ms step_avg:39.39ms
step:893/2090 train_time:35196ms step_avg:39.41ms
step:894/2090 train_time:35256ms step_avg:39.44ms
step:895/2090 train_time:35317ms step_avg:39.46ms
step:896/2090 train_time:35377ms step_avg:39.48ms
step:897/2090 train_time:35437ms step_avg:39.51ms
step:898/2090 train_time:35497ms step_avg:39.53ms
step:899/2090 train_time:35557ms step_avg:39.55ms
step:900/2090 train_time:35617ms step_avg:39.57ms
step:901/2090 train_time:35677ms step_avg:39.60ms
step:902/2090 train_time:35737ms step_avg:39.62ms
step:903/2090 train_time:35798ms step_avg:39.64ms
step:904/2090 train_time:35857ms step_avg:39.66ms
step:905/2090 train_time:35917ms step_avg:39.69ms
step:906/2090 train_time:35977ms step_avg:39.71ms
step:907/2090 train_time:36037ms step_avg:39.73ms
step:908/2090 train_time:36097ms step_avg:39.75ms
step:909/2090 train_time:36157ms step_avg:39.78ms
step:910/2090 train_time:36217ms step_avg:39.80ms
step:911/2090 train_time:36278ms step_avg:39.82ms
step:912/2090 train_time:36338ms step_avg:39.84ms
step:913/2090 train_time:36398ms step_avg:39.87ms
step:914/2090 train_time:36457ms step_avg:39.89ms
step:915/2090 train_time:36517ms step_avg:39.91ms
step:916/2090 train_time:36577ms step_avg:39.93ms
step:917/2090 train_time:36637ms step_avg:39.95ms
step:918/2090 train_time:36697ms step_avg:39.97ms
step:919/2090 train_time:36758ms step_avg:40.00ms
step:920/2090 train_time:36817ms step_avg:40.02ms
step:921/2090 train_time:36877ms step_avg:40.04ms
step:922/2090 train_time:36936ms step_avg:40.06ms
step:923/2090 train_time:36997ms step_avg:40.08ms
step:924/2090 train_time:37057ms step_avg:40.10ms
step:925/2090 train_time:37116ms step_avg:40.13ms
step:926/2090 train_time:37176ms step_avg:40.15ms
step:927/2090 train_time:37237ms step_avg:40.17ms
step:928/2090 train_time:37296ms step_avg:40.19ms
step:929/2090 train_time:37356ms step_avg:40.21ms
step:930/2090 train_time:37415ms step_avg:40.23ms
step:931/2090 train_time:37476ms step_avg:40.25ms
step:932/2090 train_time:37535ms step_avg:40.27ms
step:933/2090 train_time:37596ms step_avg:40.30ms
step:934/2090 train_time:37656ms step_avg:40.32ms
step:935/2090 train_time:37716ms step_avg:40.34ms
step:936/2090 train_time:37775ms step_avg:40.36ms
step:937/2090 train_time:37836ms step_avg:40.38ms
step:938/2090 train_time:37896ms step_avg:40.40ms
step:939/2090 train_time:37957ms step_avg:40.42ms
step:940/2090 train_time:38016ms step_avg:40.44ms
step:941/2090 train_time:38077ms step_avg:40.46ms
step:942/2090 train_time:38135ms step_avg:40.48ms
step:943/2090 train_time:38196ms step_avg:40.50ms
step:944/2090 train_time:38255ms step_avg:40.52ms
step:945/2090 train_time:38316ms step_avg:40.55ms
step:946/2090 train_time:38375ms step_avg:40.57ms
step:947/2090 train_time:38436ms step_avg:40.59ms
step:948/2090 train_time:38496ms step_avg:40.61ms
step:949/2090 train_time:38556ms step_avg:40.63ms
step:950/2090 train_time:38616ms step_avg:40.65ms
step:951/2090 train_time:38677ms step_avg:40.67ms
step:952/2090 train_time:38736ms step_avg:40.69ms
step:953/2090 train_time:38796ms step_avg:40.71ms
step:954/2090 train_time:38855ms step_avg:40.73ms
step:955/2090 train_time:38916ms step_avg:40.75ms
step:956/2090 train_time:38977ms step_avg:40.77ms
step:957/2090 train_time:39037ms step_avg:40.79ms
step:958/2090 train_time:39097ms step_avg:40.81ms
step:959/2090 train_time:39157ms step_avg:40.83ms
step:960/2090 train_time:39217ms step_avg:40.85ms
step:961/2090 train_time:39277ms step_avg:40.87ms
step:962/2090 train_time:39337ms step_avg:40.89ms
step:963/2090 train_time:39397ms step_avg:40.91ms
step:964/2090 train_time:39457ms step_avg:40.93ms
step:965/2090 train_time:39517ms step_avg:40.95ms
step:966/2090 train_time:39576ms step_avg:40.97ms
step:967/2090 train_time:39637ms step_avg:40.99ms
step:968/2090 train_time:39697ms step_avg:41.01ms
step:969/2090 train_time:39758ms step_avg:41.03ms
step:970/2090 train_time:39818ms step_avg:41.05ms
step:971/2090 train_time:39879ms step_avg:41.07ms
step:972/2090 train_time:39939ms step_avg:41.09ms
step:973/2090 train_time:39999ms step_avg:41.11ms
step:974/2090 train_time:40058ms step_avg:41.13ms
step:975/2090 train_time:40119ms step_avg:41.15ms
step:976/2090 train_time:40179ms step_avg:41.17ms
step:977/2090 train_time:40239ms step_avg:41.19ms
step:978/2090 train_time:40298ms step_avg:41.21ms
step:979/2090 train_time:40359ms step_avg:41.22ms
step:980/2090 train_time:40418ms step_avg:41.24ms
step:981/2090 train_time:40479ms step_avg:41.26ms
step:982/2090 train_time:40538ms step_avg:41.28ms
step:983/2090 train_time:40598ms step_avg:41.30ms
step:984/2090 train_time:40657ms step_avg:41.32ms
step:985/2090 train_time:40717ms step_avg:41.34ms
step:986/2090 train_time:40777ms step_avg:41.36ms
step:987/2090 train_time:40837ms step_avg:41.37ms
step:988/2090 train_time:40897ms step_avg:41.39ms
step:989/2090 train_time:40957ms step_avg:41.41ms
step:990/2090 train_time:41017ms step_avg:41.43ms
step:991/2090 train_time:41078ms step_avg:41.45ms
step:992/2090 train_time:41137ms step_avg:41.47ms
step:993/2090 train_time:41198ms step_avg:41.49ms
step:994/2090 train_time:41257ms step_avg:41.51ms
step:995/2090 train_time:41318ms step_avg:41.53ms
step:996/2090 train_time:41378ms step_avg:41.54ms
step:997/2090 train_time:41438ms step_avg:41.56ms
step:998/2090 train_time:41498ms step_avg:41.58ms
step:999/2090 train_time:41557ms step_avg:41.60ms
step:1000/2090 train_time:41617ms step_avg:41.62ms
step:1000/2090 val_loss:3.7043 train_time:41678ms step_avg:41.68ms
step:1001/2090 train_time:41701ms step_avg:41.66ms
step:1002/2090 train_time:41740ms step_avg:41.66ms
step:1003/2090 train_time:41805ms step_avg:41.68ms
step:1004/2090 train_time:41867ms step_avg:41.70ms
step:1005/2090 train_time:41928ms step_avg:41.72ms
step:1006/2090 train_time:41987ms step_avg:41.74ms
step:1007/2090 train_time:42047ms step_avg:41.75ms
step:1008/2090 train_time:42106ms step_avg:41.77ms
step:1009/2090 train_time:42166ms step_avg:41.79ms
step:1010/2090 train_time:42224ms step_avg:41.81ms
step:1011/2090 train_time:42284ms step_avg:41.82ms
step:1012/2090 train_time:42343ms step_avg:41.84ms
step:1013/2090 train_time:42403ms step_avg:41.86ms
step:1014/2090 train_time:42462ms step_avg:41.88ms
step:1015/2090 train_time:42521ms step_avg:41.89ms
step:1016/2090 train_time:42580ms step_avg:41.91ms
step:1017/2090 train_time:42641ms step_avg:41.93ms
step:1018/2090 train_time:42701ms step_avg:41.95ms
step:1019/2090 train_time:42763ms step_avg:41.97ms
step:1020/2090 train_time:42825ms step_avg:41.99ms
step:1021/2090 train_time:42887ms step_avg:42.00ms
step:1022/2090 train_time:42947ms step_avg:42.02ms
step:1023/2090 train_time:43007ms step_avg:42.04ms
step:1024/2090 train_time:43067ms step_avg:42.06ms
step:1025/2090 train_time:43126ms step_avg:42.07ms
step:1026/2090 train_time:43185ms step_avg:42.09ms
step:1027/2090 train_time:43245ms step_avg:42.11ms
step:1028/2090 train_time:43304ms step_avg:42.12ms
step:1029/2090 train_time:43364ms step_avg:42.14ms
step:1030/2090 train_time:43423ms step_avg:42.16ms
step:1031/2090 train_time:43483ms step_avg:42.18ms
step:1032/2090 train_time:43542ms step_avg:42.19ms
step:1033/2090 train_time:43602ms step_avg:42.21ms
step:1034/2090 train_time:43662ms step_avg:42.23ms
step:1035/2090 train_time:43723ms step_avg:42.24ms
step:1036/2090 train_time:43784ms step_avg:42.26ms
step:1037/2090 train_time:43846ms step_avg:42.28ms
step:1038/2090 train_time:43906ms step_avg:42.30ms
step:1039/2090 train_time:43966ms step_avg:42.32ms
step:1040/2090 train_time:44026ms step_avg:42.33ms
step:1041/2090 train_time:44087ms step_avg:42.35ms
step:1042/2090 train_time:44146ms step_avg:42.37ms
step:1043/2090 train_time:44206ms step_avg:42.38ms
step:1044/2090 train_time:44265ms step_avg:42.40ms
step:1045/2090 train_time:44325ms step_avg:42.42ms
step:1046/2090 train_time:44384ms step_avg:42.43ms
step:1047/2090 train_time:44444ms step_avg:42.45ms
step:1048/2090 train_time:44503ms step_avg:42.46ms
step:1049/2090 train_time:44563ms step_avg:42.48ms
step:1050/2090 train_time:44623ms step_avg:42.50ms
step:1051/2090 train_time:44684ms step_avg:42.52ms
step:1052/2090 train_time:44743ms step_avg:42.53ms
step:1053/2090 train_time:44804ms step_avg:42.55ms
step:1054/2090 train_time:44865ms step_avg:42.57ms
step:1055/2090 train_time:44926ms step_avg:42.58ms
step:1056/2090 train_time:44986ms step_avg:42.60ms
step:1057/2090 train_time:45047ms step_avg:42.62ms
step:1058/2090 train_time:45106ms step_avg:42.63ms
step:1059/2090 train_time:45165ms step_avg:42.65ms
step:1060/2090 train_time:45225ms step_avg:42.66ms
step:1061/2090 train_time:45285ms step_avg:42.68ms
step:1062/2090 train_time:45345ms step_avg:42.70ms
step:1063/2090 train_time:45404ms step_avg:42.71ms
step:1064/2090 train_time:45463ms step_avg:42.73ms
step:1065/2090 train_time:45524ms step_avg:42.75ms
step:1066/2090 train_time:45583ms step_avg:42.76ms
step:1067/2090 train_time:45644ms step_avg:42.78ms
step:1068/2090 train_time:45703ms step_avg:42.79ms
step:1069/2090 train_time:45764ms step_avg:42.81ms
step:1070/2090 train_time:45824ms step_avg:42.83ms
step:1071/2090 train_time:45886ms step_avg:42.84ms
step:1072/2090 train_time:45946ms step_avg:42.86ms
step:1073/2090 train_time:46006ms step_avg:42.88ms
step:1074/2090 train_time:46065ms step_avg:42.89ms
step:1075/2090 train_time:46125ms step_avg:42.91ms
step:1076/2090 train_time:46184ms step_avg:42.92ms
step:1077/2090 train_time:46245ms step_avg:42.94ms
step:1078/2090 train_time:46304ms step_avg:42.95ms
step:1079/2090 train_time:46363ms step_avg:42.97ms
step:1080/2090 train_time:46422ms step_avg:42.98ms
step:1081/2090 train_time:46482ms step_avg:43.00ms
step:1082/2090 train_time:46541ms step_avg:43.01ms
step:1083/2090 train_time:46601ms step_avg:43.03ms
step:1084/2090 train_time:46661ms step_avg:43.05ms
step:1085/2090 train_time:46722ms step_avg:43.06ms
step:1086/2090 train_time:46782ms step_avg:43.08ms
step:1087/2090 train_time:46844ms step_avg:43.09ms
step:1088/2090 train_time:46904ms step_avg:43.11ms
step:1089/2090 train_time:46965ms step_avg:43.13ms
step:1090/2090 train_time:47025ms step_avg:43.14ms
step:1091/2090 train_time:47085ms step_avg:43.16ms
step:1092/2090 train_time:47145ms step_avg:43.17ms
step:1093/2090 train_time:47205ms step_avg:43.19ms
step:1094/2090 train_time:47264ms step_avg:43.20ms
step:1095/2090 train_time:47324ms step_avg:43.22ms
step:1096/2090 train_time:47383ms step_avg:43.23ms
step:1097/2090 train_time:47443ms step_avg:43.25ms
step:1098/2090 train_time:47503ms step_avg:43.26ms
step:1099/2090 train_time:47563ms step_avg:43.28ms
step:1100/2090 train_time:47622ms step_avg:43.29ms
step:1101/2090 train_time:47683ms step_avg:43.31ms
step:1102/2090 train_time:47742ms step_avg:43.32ms
step:1103/2090 train_time:47803ms step_avg:43.34ms
step:1104/2090 train_time:47862ms step_avg:43.35ms
step:1105/2090 train_time:47923ms step_avg:43.37ms
step:1106/2090 train_time:47983ms step_avg:43.38ms
step:1107/2090 train_time:48044ms step_avg:43.40ms
step:1108/2090 train_time:48103ms step_avg:43.41ms
step:1109/2090 train_time:48164ms step_avg:43.43ms
step:1110/2090 train_time:48224ms step_avg:43.44ms
step:1111/2090 train_time:48284ms step_avg:43.46ms
step:1112/2090 train_time:48342ms step_avg:43.47ms
step:1113/2090 train_time:48402ms step_avg:43.49ms
step:1114/2090 train_time:48461ms step_avg:43.50ms
step:1115/2090 train_time:48522ms step_avg:43.52ms
step:1116/2090 train_time:48581ms step_avg:43.53ms
step:1117/2090 train_time:48642ms step_avg:43.55ms
step:1118/2090 train_time:48701ms step_avg:43.56ms
step:1119/2090 train_time:48761ms step_avg:43.58ms
step:1120/2090 train_time:48821ms step_avg:43.59ms
step:1121/2090 train_time:48881ms step_avg:43.61ms
step:1122/2090 train_time:48941ms step_avg:43.62ms
step:1123/2090 train_time:49002ms step_avg:43.63ms
step:1124/2090 train_time:49062ms step_avg:43.65ms
step:1125/2090 train_time:49122ms step_avg:43.66ms
step:1126/2090 train_time:49181ms step_avg:43.68ms
step:1127/2090 train_time:49241ms step_avg:43.69ms
step:1128/2090 train_time:49300ms step_avg:43.71ms
step:1129/2090 train_time:49360ms step_avg:43.72ms
step:1130/2090 train_time:49420ms step_avg:43.73ms
step:1131/2090 train_time:49480ms step_avg:43.75ms
step:1132/2090 train_time:49540ms step_avg:43.76ms
step:1133/2090 train_time:49600ms step_avg:43.78ms
step:1134/2090 train_time:49660ms step_avg:43.79ms
step:1135/2090 train_time:49720ms step_avg:43.81ms
step:1136/2090 train_time:49780ms step_avg:43.82ms
step:1137/2090 train_time:49842ms step_avg:43.84ms
step:1138/2090 train_time:49902ms step_avg:43.85ms
step:1139/2090 train_time:49963ms step_avg:43.87ms
step:1140/2090 train_time:50023ms step_avg:43.88ms
step:1141/2090 train_time:50084ms step_avg:43.89ms
step:1142/2090 train_time:50144ms step_avg:43.91ms
step:1143/2090 train_time:50204ms step_avg:43.92ms
step:1144/2090 train_time:50263ms step_avg:43.94ms
step:1145/2090 train_time:50323ms step_avg:43.95ms
step:1146/2090 train_time:50382ms step_avg:43.96ms
step:1147/2090 train_time:50442ms step_avg:43.98ms
step:1148/2090 train_time:50502ms step_avg:43.99ms
step:1149/2090 train_time:50562ms step_avg:44.01ms
step:1150/2090 train_time:50622ms step_avg:44.02ms
step:1151/2090 train_time:50683ms step_avg:44.03ms
step:1152/2090 train_time:50742ms step_avg:44.05ms
step:1153/2090 train_time:50803ms step_avg:44.06ms
step:1154/2090 train_time:50863ms step_avg:44.08ms
step:1155/2090 train_time:50924ms step_avg:44.09ms
step:1156/2090 train_time:50983ms step_avg:44.10ms
step:1157/2090 train_time:51044ms step_avg:44.12ms
step:1158/2090 train_time:51103ms step_avg:44.13ms
step:1159/2090 train_time:51164ms step_avg:44.15ms
step:1160/2090 train_time:51224ms step_avg:44.16ms
step:1161/2090 train_time:51284ms step_avg:44.17ms
step:1162/2090 train_time:51343ms step_avg:44.19ms
step:1163/2090 train_time:51404ms step_avg:44.20ms
step:1164/2090 train_time:51463ms step_avg:44.21ms
step:1165/2090 train_time:51523ms step_avg:44.23ms
step:1166/2090 train_time:51583ms step_avg:44.24ms
step:1167/2090 train_time:51643ms step_avg:44.25ms
step:1168/2090 train_time:51703ms step_avg:44.27ms
step:1169/2090 train_time:51763ms step_avg:44.28ms
step:1170/2090 train_time:51823ms step_avg:44.29ms
step:1171/2090 train_time:51883ms step_avg:44.31ms
step:1172/2090 train_time:51943ms step_avg:44.32ms
step:1173/2090 train_time:52003ms step_avg:44.33ms
step:1174/2090 train_time:52063ms step_avg:44.35ms
step:1175/2090 train_time:52124ms step_avg:44.36ms
step:1176/2090 train_time:52183ms step_avg:44.37ms
step:1177/2090 train_time:52243ms step_avg:44.39ms
step:1178/2090 train_time:52302ms step_avg:44.40ms
step:1179/2090 train_time:52362ms step_avg:44.41ms
step:1180/2090 train_time:52421ms step_avg:44.42ms
step:1181/2090 train_time:52481ms step_avg:44.44ms
step:1182/2090 train_time:52541ms step_avg:44.45ms
step:1183/2090 train_time:52602ms step_avg:44.46ms
step:1184/2090 train_time:52661ms step_avg:44.48ms
step:1185/2090 train_time:52721ms step_avg:44.49ms
step:1186/2090 train_time:52781ms step_avg:44.50ms
step:1187/2090 train_time:52842ms step_avg:44.52ms
step:1188/2090 train_time:52901ms step_avg:44.53ms
step:1189/2090 train_time:52962ms step_avg:44.54ms
step:1190/2090 train_time:53022ms step_avg:44.56ms
step:1191/2090 train_time:53083ms step_avg:44.57ms
step:1192/2090 train_time:53143ms step_avg:44.58ms
step:1193/2090 train_time:53203ms step_avg:44.60ms
step:1194/2090 train_time:53262ms step_avg:44.61ms
step:1195/2090 train_time:53323ms step_avg:44.62ms
step:1196/2090 train_time:53382ms step_avg:44.63ms
step:1197/2090 train_time:53442ms step_avg:44.65ms
step:1198/2090 train_time:53502ms step_avg:44.66ms
step:1199/2090 train_time:53563ms step_avg:44.67ms
step:1200/2090 train_time:53622ms step_avg:44.69ms
step:1201/2090 train_time:53682ms step_avg:44.70ms
step:1202/2090 train_time:53742ms step_avg:44.71ms
step:1203/2090 train_time:53803ms step_avg:44.72ms
step:1204/2090 train_time:53863ms step_avg:44.74ms
step:1205/2090 train_time:53925ms step_avg:44.75ms
step:1206/2090 train_time:53984ms step_avg:44.76ms
step:1207/2090 train_time:54045ms step_avg:44.78ms
step:1208/2090 train_time:54105ms step_avg:44.79ms
step:1209/2090 train_time:54165ms step_avg:44.80ms
step:1210/2090 train_time:54225ms step_avg:44.81ms
step:1211/2090 train_time:54286ms step_avg:44.83ms
step:1212/2090 train_time:54345ms step_avg:44.84ms
step:1213/2090 train_time:54406ms step_avg:44.85ms
step:1214/2090 train_time:54465ms step_avg:44.86ms
step:1215/2090 train_time:54526ms step_avg:44.88ms
step:1216/2090 train_time:54585ms step_avg:44.89ms
step:1217/2090 train_time:54646ms step_avg:44.90ms
step:1218/2090 train_time:54706ms step_avg:44.91ms
step:1219/2090 train_time:54766ms step_avg:44.93ms
step:1220/2090 train_time:54825ms step_avg:44.94ms
step:1221/2090 train_time:54886ms step_avg:44.95ms
step:1222/2090 train_time:54945ms step_avg:44.96ms
step:1223/2090 train_time:55005ms step_avg:44.98ms
step:1224/2090 train_time:55065ms step_avg:44.99ms
step:1225/2090 train_time:55125ms step_avg:45.00ms
step:1226/2090 train_time:55185ms step_avg:45.01ms
step:1227/2090 train_time:55245ms step_avg:45.02ms
step:1228/2090 train_time:55305ms step_avg:45.04ms
step:1229/2090 train_time:55365ms step_avg:45.05ms
step:1230/2090 train_time:55424ms step_avg:45.06ms
step:1231/2090 train_time:55485ms step_avg:45.07ms
step:1232/2090 train_time:55545ms step_avg:45.08ms
step:1233/2090 train_time:55605ms step_avg:45.10ms
step:1234/2090 train_time:55665ms step_avg:45.11ms
step:1235/2090 train_time:55726ms step_avg:45.12ms
step:1236/2090 train_time:55785ms step_avg:45.13ms
step:1237/2090 train_time:55846ms step_avg:45.15ms
step:1238/2090 train_time:55906ms step_avg:45.16ms
step:1239/2090 train_time:55966ms step_avg:45.17ms
step:1240/2090 train_time:56026ms step_avg:45.18ms
step:1241/2090 train_time:56087ms step_avg:45.19ms
step:1242/2090 train_time:56146ms step_avg:45.21ms
step:1243/2090 train_time:56206ms step_avg:45.22ms
step:1244/2090 train_time:56266ms step_avg:45.23ms
step:1245/2090 train_time:56327ms step_avg:45.24ms
step:1246/2090 train_time:56386ms step_avg:45.25ms
step:1247/2090 train_time:56447ms step_avg:45.27ms
step:1248/2090 train_time:56506ms step_avg:45.28ms
step:1249/2090 train_time:56567ms step_avg:45.29ms
step:1250/2090 train_time:56626ms step_avg:45.30ms
step:1250/2090 val_loss:3.5862 train_time:56688ms step_avg:45.35ms
step:1251/2090 train_time:56709ms step_avg:45.33ms
step:1252/2090 train_time:56748ms step_avg:45.33ms
step:1253/2090 train_time:56808ms step_avg:45.34ms
step:1254/2090 train_time:56867ms step_avg:45.35ms
step:1255/2090 train_time:56927ms step_avg:45.36ms
step:1256/2090 train_time:56987ms step_avg:45.37ms
step:1257/2090 train_time:57047ms step_avg:45.38ms
step:1258/2090 train_time:57105ms step_avg:45.39ms
step:1259/2090 train_time:57164ms step_avg:45.40ms
step:1260/2090 train_time:57223ms step_avg:45.41ms
step:1261/2090 train_time:57283ms step_avg:45.43ms
step:1262/2090 train_time:57342ms step_avg:45.44ms
step:1263/2090 train_time:57401ms step_avg:45.45ms
step:1264/2090 train_time:57461ms step_avg:45.46ms
step:1265/2090 train_time:57519ms step_avg:45.47ms
step:1266/2090 train_time:57580ms step_avg:45.48ms
step:1267/2090 train_time:57646ms step_avg:45.50ms
step:1268/2090 train_time:57707ms step_avg:45.51ms
step:1269/2090 train_time:57768ms step_avg:45.52ms
step:1270/2090 train_time:57828ms step_avg:45.53ms
step:1271/2090 train_time:57889ms step_avg:45.55ms
step:1272/2090 train_time:57948ms step_avg:45.56ms
step:1273/2090 train_time:58007ms step_avg:45.57ms
step:1274/2090 train_time:58066ms step_avg:45.58ms
step:1275/2090 train_time:58125ms step_avg:45.59ms
step:1276/2090 train_time:58184ms step_avg:45.60ms
step:1277/2090 train_time:58243ms step_avg:45.61ms
step:1278/2090 train_time:58302ms step_avg:45.62ms
step:1279/2090 train_time:58362ms step_avg:45.63ms
step:1280/2090 train_time:58420ms step_avg:45.64ms
step:1281/2090 train_time:58480ms step_avg:45.65ms
step:1282/2090 train_time:58541ms step_avg:45.66ms
step:1283/2090 train_time:58603ms step_avg:45.68ms
step:1284/2090 train_time:58663ms step_avg:45.69ms
step:1285/2090 train_time:58725ms step_avg:45.70ms
step:1286/2090 train_time:58786ms step_avg:45.71ms
step:1287/2090 train_time:58846ms step_avg:45.72ms
step:1288/2090 train_time:58905ms step_avg:45.73ms
step:1289/2090 train_time:58965ms step_avg:45.75ms
step:1290/2090 train_time:59024ms step_avg:45.76ms
step:1291/2090 train_time:59084ms step_avg:45.77ms
step:1292/2090 train_time:59143ms step_avg:45.78ms
step:1293/2090 train_time:59203ms step_avg:45.79ms
step:1294/2090 train_time:59261ms step_avg:45.80ms
step:1295/2090 train_time:59321ms step_avg:45.81ms
step:1296/2090 train_time:59381ms step_avg:45.82ms
step:1297/2090 train_time:59440ms step_avg:45.83ms
step:1298/2090 train_time:59499ms step_avg:45.84ms
step:1299/2090 train_time:59559ms step_avg:45.85ms
step:1300/2090 train_time:59619ms step_avg:45.86ms
step:1301/2090 train_time:59680ms step_avg:45.87ms
step:1302/2090 train_time:59741ms step_avg:45.88ms
step:1303/2090 train_time:59803ms step_avg:45.90ms
step:1304/2090 train_time:59863ms step_avg:45.91ms
step:1305/2090 train_time:59923ms step_avg:45.92ms
step:1306/2090 train_time:59983ms step_avg:45.93ms
step:1307/2090 train_time:60043ms step_avg:45.94ms
step:1308/2090 train_time:60102ms step_avg:45.95ms
step:1309/2090 train_time:60161ms step_avg:45.96ms
step:1310/2090 train_time:60220ms step_avg:45.97ms
step:1311/2090 train_time:60280ms step_avg:45.98ms
step:1312/2090 train_time:60339ms step_avg:45.99ms
step:1313/2090 train_time:60399ms step_avg:46.00ms
step:1314/2090 train_time:60458ms step_avg:46.01ms
step:1315/2090 train_time:60519ms step_avg:46.02ms
step:1316/2090 train_time:60578ms step_avg:46.03ms
step:1317/2090 train_time:60640ms step_avg:46.04ms
step:1318/2090 train_time:60700ms step_avg:46.05ms
step:1319/2090 train_time:60762ms step_avg:46.07ms
step:1320/2090 train_time:60822ms step_avg:46.08ms
step:1321/2090 train_time:60883ms step_avg:46.09ms
step:1322/2090 train_time:60942ms step_avg:46.10ms
step:1323/2090 train_time:61003ms step_avg:46.11ms
step:1324/2090 train_time:61062ms step_avg:46.12ms
step:1325/2090 train_time:61122ms step_avg:46.13ms
step:1326/2090 train_time:61181ms step_avg:46.14ms
step:1327/2090 train_time:61241ms step_avg:46.15ms
step:1328/2090 train_time:61299ms step_avg:46.16ms
step:1329/2090 train_time:61359ms step_avg:46.17ms
step:1330/2090 train_time:61419ms step_avg:46.18ms
step:1331/2090 train_time:61479ms step_avg:46.19ms
step:1332/2090 train_time:61538ms step_avg:46.20ms
step:1333/2090 train_time:61599ms step_avg:46.21ms
step:1334/2090 train_time:61659ms step_avg:46.22ms
step:1335/2090 train_time:61719ms step_avg:46.23ms
step:1336/2090 train_time:61779ms step_avg:46.24ms
step:1337/2090 train_time:61841ms step_avg:46.25ms
step:1338/2090 train_time:61901ms step_avg:46.26ms
step:1339/2090 train_time:61962ms step_avg:46.27ms
step:1340/2090 train_time:62022ms step_avg:46.28ms
step:1341/2090 train_time:62082ms step_avg:46.30ms
step:1342/2090 train_time:62142ms step_avg:46.31ms
step:1343/2090 train_time:62202ms step_avg:46.32ms
step:1344/2090 train_time:62261ms step_avg:46.33ms
step:1345/2090 train_time:62321ms step_avg:46.34ms
step:1346/2090 train_time:62381ms step_avg:46.35ms
step:1347/2090 train_time:62441ms step_avg:46.36ms
step:1348/2090 train_time:62500ms step_avg:46.36ms
step:1349/2090 train_time:62560ms step_avg:46.38ms
step:1350/2090 train_time:62619ms step_avg:46.38ms
step:1351/2090 train_time:62680ms step_avg:46.40ms
step:1352/2090 train_time:62740ms step_avg:46.41ms
step:1353/2090 train_time:62801ms step_avg:46.42ms
step:1354/2090 train_time:62861ms step_avg:46.43ms
step:1355/2090 train_time:62922ms step_avg:46.44ms
step:1356/2090 train_time:62981ms step_avg:46.45ms
step:1357/2090 train_time:63041ms step_avg:46.46ms
step:1358/2090 train_time:63101ms step_avg:46.47ms
step:1359/2090 train_time:63161ms step_avg:46.48ms
step:1360/2090 train_time:63221ms step_avg:46.49ms
step:1361/2090 train_time:63281ms step_avg:46.50ms
step:1362/2090 train_time:63340ms step_avg:46.51ms
step:1363/2090 train_time:63401ms step_avg:46.52ms
step:1364/2090 train_time:63461ms step_avg:46.53ms
step:1365/2090 train_time:63521ms step_avg:46.54ms
step:1366/2090 train_time:63582ms step_avg:46.55ms
step:1367/2090 train_time:63642ms step_avg:46.56ms
step:1368/2090 train_time:63702ms step_avg:46.57ms
step:1369/2090 train_time:63791ms step_avg:46.60ms
step:1370/2090 train_time:63878ms step_avg:46.63ms
step:1371/2090 train_time:63966ms step_avg:46.66ms
step:1372/2090 train_time:64053ms step_avg:46.69ms
step:1373/2090 train_time:64141ms step_avg:46.72ms
step:1374/2090 train_time:64228ms step_avg:46.75ms
step:1375/2090 train_time:64317ms step_avg:46.78ms
step:1376/2090 train_time:64404ms step_avg:46.81ms
step:1377/2090 train_time:64492ms step_avg:46.84ms
step:1378/2090 train_time:64580ms step_avg:46.86ms
step:1379/2090 train_time:64667ms step_avg:46.89ms
step:1380/2090 train_time:64754ms step_avg:46.92ms
step:1381/2090 train_time:64841ms step_avg:46.95ms
step:1382/2090 train_time:64928ms step_avg:46.98ms
step:1383/2090 train_time:65017ms step_avg:47.01ms
step:1384/2090 train_time:65103ms step_avg:47.04ms
step:1385/2090 train_time:65192ms step_avg:47.07ms
step:1386/2090 train_time:65279ms step_avg:47.10ms
step:1387/2090 train_time:65367ms step_avg:47.13ms
step:1388/2090 train_time:65453ms step_avg:47.16ms
step:1389/2090 train_time:65541ms step_avg:47.19ms
step:1390/2090 train_time:65629ms step_avg:47.21ms
step:1391/2090 train_time:65717ms step_avg:47.24ms
step:1392/2090 train_time:65804ms step_avg:47.27ms
step:1393/2090 train_time:65893ms step_avg:47.30ms
step:1394/2090 train_time:65980ms step_avg:47.33ms
step:1395/2090 train_time:66068ms step_avg:47.36ms
step:1396/2090 train_time:66155ms step_avg:47.39ms
step:1397/2090 train_time:66245ms step_avg:47.42ms
step:1398/2090 train_time:66332ms step_avg:47.45ms
step:1399/2090 train_time:66421ms step_avg:47.48ms
step:1400/2090 train_time:66507ms step_avg:47.51ms
step:1401/2090 train_time:66596ms step_avg:47.53ms
step:1402/2090 train_time:66682ms step_avg:47.56ms
step:1403/2090 train_time:66770ms step_avg:47.59ms
step:1404/2090 train_time:66857ms step_avg:47.62ms
step:1405/2090 train_time:66945ms step_avg:47.65ms
step:1406/2090 train_time:67032ms step_avg:47.68ms
step:1407/2090 train_time:67120ms step_avg:47.70ms
step:1408/2090 train_time:67207ms step_avg:47.73ms
step:1409/2090 train_time:67296ms step_avg:47.76ms
step:1410/2090 train_time:67383ms step_avg:47.79ms
step:1411/2090 train_time:67471ms step_avg:47.82ms
step:1412/2090 train_time:67558ms step_avg:47.85ms
step:1413/2090 train_time:67645ms step_avg:47.87ms
step:1414/2090 train_time:67732ms step_avg:47.90ms
step:1415/2090 train_time:67820ms step_avg:47.93ms
step:1416/2090 train_time:67907ms step_avg:47.96ms
step:1417/2090 train_time:67996ms step_avg:47.99ms
step:1418/2090 train_time:68082ms step_avg:48.01ms
step:1419/2090 train_time:68170ms step_avg:48.04ms
step:1420/2090 train_time:68257ms step_avg:48.07ms
step:1421/2090 train_time:68345ms step_avg:48.10ms
step:1422/2090 train_time:68432ms step_avg:48.12ms
step:1423/2090 train_time:68520ms step_avg:48.15ms
step:1424/2090 train_time:68607ms step_avg:48.18ms
step:1425/2090 train_time:68696ms step_avg:48.21ms
step:1426/2090 train_time:68783ms step_avg:48.23ms
step:1427/2090 train_time:68871ms step_avg:48.26ms
step:1428/2090 train_time:68959ms step_avg:48.29ms
step:1429/2090 train_time:69047ms step_avg:48.32ms
step:1430/2090 train_time:69133ms step_avg:48.34ms
step:1431/2090 train_time:69221ms step_avg:48.37ms
step:1432/2090 train_time:69309ms step_avg:48.40ms
step:1433/2090 train_time:69397ms step_avg:48.43ms
step:1434/2090 train_time:69484ms step_avg:48.45ms
step:1435/2090 train_time:69574ms step_avg:48.48ms
step:1436/2090 train_time:69660ms step_avg:48.51ms
step:1437/2090 train_time:69749ms step_avg:48.54ms
step:1438/2090 train_time:69836ms step_avg:48.56ms
step:1439/2090 train_time:69924ms step_avg:48.59ms
step:1440/2090 train_time:70012ms step_avg:48.62ms
step:1441/2090 train_time:70100ms step_avg:48.65ms
step:1442/2090 train_time:70187ms step_avg:48.67ms
step:1443/2090 train_time:70275ms step_avg:48.70ms
step:1444/2090 train_time:70362ms step_avg:48.73ms
step:1445/2090 train_time:70450ms step_avg:48.75ms
step:1446/2090 train_time:70537ms step_avg:48.78ms
step:1447/2090 train_time:70624ms step_avg:48.81ms
step:1448/2090 train_time:70713ms step_avg:48.83ms
step:1449/2090 train_time:70801ms step_avg:48.86ms
step:1450/2090 train_time:70887ms step_avg:48.89ms
step:1451/2090 train_time:70976ms step_avg:48.91ms
step:1452/2090 train_time:71063ms step_avg:48.94ms
step:1453/2090 train_time:71151ms step_avg:48.97ms
step:1454/2090 train_time:71238ms step_avg:48.99ms
step:1455/2090 train_time:71326ms step_avg:49.02ms
step:1456/2090 train_time:71413ms step_avg:49.05ms
step:1457/2090 train_time:71501ms step_avg:49.07ms
step:1458/2090 train_time:71588ms step_avg:49.10ms
step:1459/2090 train_time:71677ms step_avg:49.13ms
step:1460/2090 train_time:71764ms step_avg:49.15ms
step:1461/2090 train_time:71853ms step_avg:49.18ms
step:1462/2090 train_time:71940ms step_avg:49.21ms
step:1463/2090 train_time:72027ms step_avg:49.23ms
step:1464/2090 train_time:72114ms step_avg:49.26ms
step:1465/2090 train_time:72202ms step_avg:49.28ms
step:1466/2090 train_time:72289ms step_avg:49.31ms
step:1467/2090 train_time:72377ms step_avg:49.34ms
step:1468/2090 train_time:72463ms step_avg:49.36ms
step:1469/2090 train_time:72551ms step_avg:49.39ms
step:1470/2090 train_time:72638ms step_avg:49.41ms
step:1471/2090 train_time:72726ms step_avg:49.44ms
step:1472/2090 train_time:72814ms step_avg:49.47ms
step:1473/2090 train_time:72902ms step_avg:49.49ms
step:1474/2090 train_time:72989ms step_avg:49.52ms
step:1475/2090 train_time:73077ms step_avg:49.54ms
step:1476/2090 train_time:73166ms step_avg:49.57ms
step:1477/2090 train_time:73254ms step_avg:49.60ms
step:1478/2090 train_time:73341ms step_avg:49.62ms
step:1479/2090 train_time:73428ms step_avg:49.65ms
step:1480/2090 train_time:73515ms step_avg:49.67ms
step:1481/2090 train_time:73603ms step_avg:49.70ms
step:1482/2090 train_time:73689ms step_avg:49.72ms
step:1483/2090 train_time:73777ms step_avg:49.75ms
step:1484/2090 train_time:73864ms step_avg:49.77ms
step:1485/2090 train_time:73952ms step_avg:49.80ms
step:1486/2090 train_time:74039ms step_avg:49.82ms
step:1487/2090 train_time:74127ms step_avg:49.85ms
step:1488/2090 train_time:74214ms step_avg:49.87ms
step:1489/2090 train_time:74302ms step_avg:49.90ms
step:1490/2090 train_time:74389ms step_avg:49.93ms
step:1491/2090 train_time:74477ms step_avg:49.95ms
step:1492/2090 train_time:74565ms step_avg:49.98ms
step:1493/2090 train_time:74653ms step_avg:50.00ms
step:1494/2090 train_time:74740ms step_avg:50.03ms
step:1495/2090 train_time:74827ms step_avg:50.05ms
step:1496/2090 train_time:74914ms step_avg:50.08ms
step:1497/2090 train_time:75002ms step_avg:50.10ms
step:1498/2090 train_time:75090ms step_avg:50.13ms
step:1499/2090 train_time:75178ms step_avg:50.15ms
step:1500/2090 train_time:75265ms step_avg:50.18ms
step:1500/2090 val_loss:3.4756 train_time:75355ms step_avg:50.24ms
step:1501/2090 train_time:75379ms step_avg:50.22ms
step:1502/2090 train_time:75445ms step_avg:50.23ms
step:1503/2090 train_time:75539ms step_avg:50.26ms
step:1504/2090 train_time:75626ms step_avg:50.28ms
step:1505/2090 train_time:75714ms step_avg:50.31ms
step:1506/2090 train_time:75800ms step_avg:50.33ms
step:1507/2090 train_time:75887ms step_avg:50.36ms
step:1508/2090 train_time:75973ms step_avg:50.38ms
step:1509/2090 train_time:76060ms step_avg:50.40ms
step:1510/2090 train_time:76146ms step_avg:50.43ms
step:1511/2090 train_time:76233ms step_avg:50.45ms
step:1512/2090 train_time:76320ms step_avg:50.48ms
step:1513/2090 train_time:76411ms step_avg:50.50ms
step:1514/2090 train_time:76501ms step_avg:50.53ms
step:1515/2090 train_time:76590ms step_avg:50.55ms
step:1516/2090 train_time:76678ms step_avg:50.58ms
step:1517/2090 train_time:76766ms step_avg:50.60ms
step:1518/2090 train_time:76852ms step_avg:50.63ms
step:1519/2090 train_time:76940ms step_avg:50.65ms
step:1520/2090 train_time:77026ms step_avg:50.68ms
step:1521/2090 train_time:77114ms step_avg:50.70ms
step:1522/2090 train_time:77199ms step_avg:50.72ms
step:1523/2090 train_time:77288ms step_avg:50.75ms
step:1524/2090 train_time:77376ms step_avg:50.77ms
step:1525/2090 train_time:77465ms step_avg:50.80ms
step:1526/2090 train_time:77555ms step_avg:50.82ms
step:1527/2090 train_time:77644ms step_avg:50.85ms
step:1528/2090 train_time:77732ms step_avg:50.87ms
step:1529/2090 train_time:77819ms step_avg:50.90ms
step:1530/2090 train_time:77905ms step_avg:50.92ms
step:1531/2090 train_time:77993ms step_avg:50.94ms
step:1532/2090 train_time:78079ms step_avg:50.97ms
step:1533/2090 train_time:78167ms step_avg:50.99ms
step:1534/2090 train_time:78254ms step_avg:51.01ms
step:1535/2090 train_time:78342ms step_avg:51.04ms
step:1536/2090 train_time:78431ms step_avg:51.06ms
step:1537/2090 train_time:78520ms step_avg:51.09ms
step:1538/2090 train_time:78608ms step_avg:51.11ms
step:1539/2090 train_time:78697ms step_avg:51.14ms
step:1540/2090 train_time:78784ms step_avg:51.16ms
step:1541/2090 train_time:78872ms step_avg:51.18ms
step:1542/2090 train_time:78958ms step_avg:51.21ms
step:1543/2090 train_time:79046ms step_avg:51.23ms
step:1544/2090 train_time:79132ms step_avg:51.25ms
step:1545/2090 train_time:79219ms step_avg:51.27ms
step:1546/2090 train_time:79306ms step_avg:51.30ms
step:1547/2090 train_time:79394ms step_avg:51.32ms
step:1548/2090 train_time:79483ms step_avg:51.35ms
step:1549/2090 train_time:79572ms step_avg:51.37ms
step:1550/2090 train_time:79659ms step_avg:51.39ms
step:1551/2090 train_time:79747ms step_avg:51.42ms
step:1552/2090 train_time:79834ms step_avg:51.44ms
step:1553/2090 train_time:79922ms step_avg:51.46ms
step:1554/2090 train_time:80008ms step_avg:51.49ms
step:1555/2090 train_time:80096ms step_avg:51.51ms
step:1556/2090 train_time:80183ms step_avg:51.53ms
step:1557/2090 train_time:80272ms step_avg:51.56ms
step:1558/2090 train_time:80358ms step_avg:51.58ms
step:1559/2090 train_time:80446ms step_avg:51.60ms
step:1560/2090 train_time:80534ms step_avg:51.62ms
step:1561/2090 train_time:80622ms step_avg:51.65ms
step:1562/2090 train_time:80710ms step_avg:51.67ms
step:1563/2090 train_time:80799ms step_avg:51.69ms
step:1564/2090 train_time:80885ms step_avg:51.72ms
step:1565/2090 train_time:80974ms step_avg:51.74ms
step:1566/2090 train_time:81060ms step_avg:51.76ms
step:1567/2090 train_time:81147ms step_avg:51.79ms
step:1568/2090 train_time:81235ms step_avg:51.81ms
step:1569/2090 train_time:81323ms step_avg:51.83ms
step:1570/2090 train_time:81410ms step_avg:51.85ms
step:1571/2090 train_time:81498ms step_avg:51.88ms
step:1572/2090 train_time:81585ms step_avg:51.90ms
step:1573/2090 train_time:81674ms step_avg:51.92ms
step:1574/2090 train_time:81761ms step_avg:51.94ms
step:1575/2090 train_time:81850ms step_avg:51.97ms
step:1576/2090 train_time:81936ms step_avg:51.99ms
step:1577/2090 train_time:82024ms step_avg:52.01ms
step:1578/2090 train_time:82110ms step_avg:52.03ms
step:1579/2090 train_time:82199ms step_avg:52.06ms
step:1580/2090 train_time:82286ms step_avg:52.08ms
step:1581/2090 train_time:82374ms step_avg:52.10ms
step:1582/2090 train_time:82461ms step_avg:52.12ms
step:1583/2090 train_time:82550ms step_avg:52.15ms
step:1584/2090 train_time:82636ms step_avg:52.17ms
step:1585/2090 train_time:82725ms step_avg:52.19ms
step:1586/2090 train_time:82813ms step_avg:52.22ms
step:1587/2090 train_time:82901ms step_avg:52.24ms
step:1588/2090 train_time:82988ms step_avg:52.26ms
step:1589/2090 train_time:83075ms step_avg:52.28ms
step:1590/2090 train_time:83162ms step_avg:52.30ms
step:1591/2090 train_time:83251ms step_avg:52.33ms
step:1592/2090 train_time:83337ms step_avg:52.35ms
step:1593/2090 train_time:83426ms step_avg:52.37ms
step:1594/2090 train_time:83513ms step_avg:52.39ms
step:1595/2090 train_time:83601ms step_avg:52.41ms
step:1596/2090 train_time:83688ms step_avg:52.44ms
step:1597/2090 train_time:83776ms step_avg:52.46ms
step:1598/2090 train_time:83863ms step_avg:52.48ms
step:1599/2090 train_time:83953ms step_avg:52.50ms
step:1600/2090 train_time:84039ms step_avg:52.52ms
step:1601/2090 train_time:84127ms step_avg:52.55ms
step:1602/2090 train_time:84214ms step_avg:52.57ms
step:1603/2090 train_time:84301ms step_avg:52.59ms
step:1604/2090 train_time:84388ms step_avg:52.61ms
step:1605/2090 train_time:84476ms step_avg:52.63ms
step:1606/2090 train_time:84563ms step_avg:52.65ms
step:1607/2090 train_time:84652ms step_avg:52.68ms
step:1608/2090 train_time:84739ms step_avg:52.70ms
step:1609/2090 train_time:84827ms step_avg:52.72ms
step:1610/2090 train_time:84914ms step_avg:52.74ms
step:1611/2090 train_time:85002ms step_avg:52.76ms
step:1612/2090 train_time:85090ms step_avg:52.79ms
step:1613/2090 train_time:85177ms step_avg:52.81ms
step:1614/2090 train_time:85264ms step_avg:52.83ms
step:1615/2090 train_time:85353ms step_avg:52.85ms
step:1616/2090 train_time:85440ms step_avg:52.87ms
step:1617/2090 train_time:85528ms step_avg:52.89ms
step:1618/2090 train_time:85615ms step_avg:52.91ms
step:1619/2090 train_time:85703ms step_avg:52.94ms
step:1620/2090 train_time:85790ms step_avg:52.96ms
step:1621/2090 train_time:85877ms step_avg:52.98ms
step:1622/2090 train_time:85965ms step_avg:53.00ms
step:1623/2090 train_time:86053ms step_avg:53.02ms
step:1624/2090 train_time:86140ms step_avg:53.04ms
step:1625/2090 train_time:86229ms step_avg:53.06ms
step:1626/2090 train_time:86315ms step_avg:53.08ms
step:1627/2090 train_time:86403ms step_avg:53.11ms
step:1628/2090 train_time:86490ms step_avg:53.13ms
step:1629/2090 train_time:86577ms step_avg:53.15ms
step:1630/2090 train_time:86664ms step_avg:53.17ms
step:1631/2090 train_time:86753ms step_avg:53.19ms
step:1632/2090 train_time:86841ms step_avg:53.21ms
step:1633/2090 train_time:86929ms step_avg:53.23ms
step:1634/2090 train_time:87016ms step_avg:53.25ms
step:1635/2090 train_time:87104ms step_avg:53.27ms
step:1636/2090 train_time:87191ms step_avg:53.30ms
step:1637/2090 train_time:87279ms step_avg:53.32ms
step:1638/2090 train_time:87367ms step_avg:53.34ms
step:1639/2090 train_time:87455ms step_avg:53.36ms
step:1640/2090 train_time:87542ms step_avg:53.38ms
step:1641/2090 train_time:87630ms step_avg:53.40ms
step:1642/2090 train_time:87716ms step_avg:53.42ms
step:1643/2090 train_time:87804ms step_avg:53.44ms
step:1644/2090 train_time:87892ms step_avg:53.46ms
step:1645/2090 train_time:87980ms step_avg:53.48ms
step:1646/2090 train_time:88067ms step_avg:53.50ms
step:1647/2090 train_time:88155ms step_avg:53.52ms
step:1648/2090 train_time:88242ms step_avg:53.54ms
step:1649/2090 train_time:88331ms step_avg:53.57ms
step:1650/2090 train_time:88417ms step_avg:53.59ms
step:1651/2090 train_time:88505ms step_avg:53.61ms
step:1652/2090 train_time:88592ms step_avg:53.63ms
step:1653/2090 train_time:88681ms step_avg:53.65ms
step:1654/2090 train_time:88767ms step_avg:53.67ms
step:1655/2090 train_time:88856ms step_avg:53.69ms
step:1656/2090 train_time:88943ms step_avg:53.71ms
step:1657/2090 train_time:89031ms step_avg:53.73ms
step:1658/2090 train_time:89119ms step_avg:53.75ms
step:1659/2090 train_time:89207ms step_avg:53.77ms
step:1660/2090 train_time:89293ms step_avg:53.79ms
step:1661/2090 train_time:89381ms step_avg:53.81ms
step:1662/2090 train_time:89469ms step_avg:53.83ms
step:1663/2090 train_time:89556ms step_avg:53.85ms
step:1664/2090 train_time:89644ms step_avg:53.87ms
step:1665/2090 train_time:89732ms step_avg:53.89ms
step:1666/2090 train_time:89818ms step_avg:53.91ms
step:1667/2090 train_time:89906ms step_avg:53.93ms
step:1668/2090 train_time:89993ms step_avg:53.95ms
step:1669/2090 train_time:90082ms step_avg:53.97ms
step:1670/2090 train_time:90169ms step_avg:53.99ms
step:1671/2090 train_time:90256ms step_avg:54.01ms
step:1672/2090 train_time:90344ms step_avg:54.03ms
step:1673/2090 train_time:90433ms step_avg:54.05ms
step:1674/2090 train_time:90520ms step_avg:54.07ms
step:1675/2090 train_time:90608ms step_avg:54.09ms
step:1676/2090 train_time:90696ms step_avg:54.11ms
step:1677/2090 train_time:90783ms step_avg:54.13ms
step:1678/2090 train_time:90870ms step_avg:54.15ms
step:1679/2090 train_time:90958ms step_avg:54.17ms
step:1680/2090 train_time:91045ms step_avg:54.19ms
step:1681/2090 train_time:91133ms step_avg:54.21ms
step:1682/2090 train_time:91219ms step_avg:54.23ms
step:1683/2090 train_time:91308ms step_avg:54.25ms
step:1684/2090 train_time:91395ms step_avg:54.27ms
step:1685/2090 train_time:91484ms step_avg:54.29ms
step:1686/2090 train_time:91571ms step_avg:54.31ms
step:1687/2090 train_time:91658ms step_avg:54.33ms
step:1688/2090 train_time:91745ms step_avg:54.35ms
step:1689/2090 train_time:91834ms step_avg:54.37ms
step:1690/2090 train_time:91920ms step_avg:54.39ms
step:1691/2090 train_time:92009ms step_avg:54.41ms
step:1692/2090 train_time:92095ms step_avg:54.43ms
step:1693/2090 train_time:92183ms step_avg:54.45ms
step:1694/2090 train_time:92270ms step_avg:54.47ms
step:1695/2090 train_time:92359ms step_avg:54.49ms
step:1696/2090 train_time:92446ms step_avg:54.51ms
step:1697/2090 train_time:92534ms step_avg:54.53ms
step:1698/2090 train_time:92621ms step_avg:54.55ms
step:1699/2090 train_time:92709ms step_avg:54.57ms
step:1700/2090 train_time:92796ms step_avg:54.59ms
step:1701/2090 train_time:92884ms step_avg:54.61ms
step:1702/2090 train_time:92971ms step_avg:54.62ms
step:1703/2090 train_time:93060ms step_avg:54.64ms
step:1704/2090 train_time:93146ms step_avg:54.66ms
step:1705/2090 train_time:93234ms step_avg:54.68ms
step:1706/2090 train_time:93321ms step_avg:54.70ms
step:1707/2090 train_time:93409ms step_avg:54.72ms
step:1708/2090 train_time:93496ms step_avg:54.74ms
step:1709/2090 train_time:93584ms step_avg:54.76ms
step:1710/2090 train_time:93671ms step_avg:54.78ms
step:1711/2090 train_time:93759ms step_avg:54.80ms
step:1712/2090 train_time:93847ms step_avg:54.82ms
step:1713/2090 train_time:93934ms step_avg:54.84ms
step:1714/2090 train_time:94021ms step_avg:54.85ms
step:1715/2090 train_time:94111ms step_avg:54.88ms
step:1716/2090 train_time:94197ms step_avg:54.89ms
step:1717/2090 train_time:94286ms step_avg:54.91ms
step:1718/2090 train_time:94372ms step_avg:54.93ms
step:1719/2090 train_time:94460ms step_avg:54.95ms
step:1720/2090 train_time:94548ms step_avg:54.97ms
step:1721/2090 train_time:94636ms step_avg:54.99ms
step:1722/2090 train_time:94723ms step_avg:55.01ms
step:1723/2090 train_time:94811ms step_avg:55.03ms
step:1724/2090 train_time:94899ms step_avg:55.05ms
step:1725/2090 train_time:94987ms step_avg:55.06ms
step:1726/2090 train_time:95074ms step_avg:55.08ms
step:1727/2090 train_time:95162ms step_avg:55.10ms
step:1728/2090 train_time:95249ms step_avg:55.12ms
step:1729/2090 train_time:95336ms step_avg:55.14ms
step:1730/2090 train_time:95423ms step_avg:55.16ms
step:1731/2090 train_time:95513ms step_avg:55.18ms
step:1732/2090 train_time:95600ms step_avg:55.20ms
step:1733/2090 train_time:95688ms step_avg:55.21ms
step:1734/2090 train_time:95775ms step_avg:55.23ms
step:1735/2090 train_time:95863ms step_avg:55.25ms
step:1736/2090 train_time:95950ms step_avg:55.27ms
step:1737/2090 train_time:96038ms step_avg:55.29ms
step:1738/2090 train_time:96125ms step_avg:55.31ms
step:1739/2090 train_time:96214ms step_avg:55.33ms
step:1740/2090 train_time:96300ms step_avg:55.35ms
step:1741/2090 train_time:96388ms step_avg:55.36ms
step:1742/2090 train_time:96475ms step_avg:55.38ms
step:1743/2090 train_time:96563ms step_avg:55.40ms
step:1744/2090 train_time:96651ms step_avg:55.42ms
step:1745/2090 train_time:96739ms step_avg:55.44ms
step:1746/2090 train_time:96826ms step_avg:55.46ms
step:1747/2090 train_time:96915ms step_avg:55.47ms
step:1748/2090 train_time:97002ms step_avg:55.49ms
step:1749/2090 train_time:97089ms step_avg:55.51ms
step:1750/2090 train_time:97176ms step_avg:55.53ms
step:1750/2090 val_loss:3.3757 train_time:97266ms step_avg:55.58ms
step:1751/2090 train_time:97287ms step_avg:55.56ms
step:1752/2090 train_time:97354ms step_avg:55.57ms
step:1753/2090 train_time:97448ms step_avg:55.59ms
step:1754/2090 train_time:97535ms step_avg:55.61ms
step:1755/2090 train_time:97622ms step_avg:55.63ms
step:1756/2090 train_time:97708ms step_avg:55.64ms
step:1757/2090 train_time:97794ms step_avg:55.66ms
step:1758/2090 train_time:97881ms step_avg:55.68ms
step:1759/2090 train_time:97967ms step_avg:55.69ms
step:1760/2090 train_time:98053ms step_avg:55.71ms
step:1761/2090 train_time:98141ms step_avg:55.73ms
step:1762/2090 train_time:98229ms step_avg:55.75ms
step:1763/2090 train_time:98319ms step_avg:55.77ms
step:1764/2090 train_time:98409ms step_avg:55.79ms
step:1765/2090 train_time:98497ms step_avg:55.81ms
step:1766/2090 train_time:98585ms step_avg:55.82ms
step:1767/2090 train_time:98673ms step_avg:55.84ms
step:1768/2090 train_time:98760ms step_avg:55.86ms
step:1769/2090 train_time:98847ms step_avg:55.88ms
step:1770/2090 train_time:98933ms step_avg:55.89ms
step:1771/2090 train_time:99021ms step_avg:55.91ms
step:1772/2090 train_time:99107ms step_avg:55.93ms
step:1773/2090 train_time:99195ms step_avg:55.95ms
step:1774/2090 train_time:99283ms step_avg:55.97ms
step:1775/2090 train_time:99373ms step_avg:55.98ms
step:1776/2090 train_time:99461ms step_avg:56.00ms
step:1777/2090 train_time:99549ms step_avg:56.02ms
step:1778/2090 train_time:99636ms step_avg:56.04ms
step:1779/2090 train_time:99724ms step_avg:56.06ms
step:1780/2090 train_time:99810ms step_avg:56.07ms
step:1781/2090 train_time:99898ms step_avg:56.09ms
step:1782/2090 train_time:99984ms step_avg:56.11ms
step:1783/2090 train_time:100071ms step_avg:56.12ms
step:1784/2090 train_time:100157ms step_avg:56.14ms
step:1785/2090 train_time:100246ms step_avg:56.16ms
step:1786/2090 train_time:100335ms step_avg:56.18ms
step:1787/2090 train_time:100425ms step_avg:56.20ms
step:1788/2090 train_time:100513ms step_avg:56.22ms
step:1789/2090 train_time:100601ms step_avg:56.23ms
step:1790/2090 train_time:100688ms step_avg:56.25ms
step:1791/2090 train_time:100775ms step_avg:56.27ms
step:1792/2090 train_time:100862ms step_avg:56.28ms
step:1793/2090 train_time:100948ms step_avg:56.30ms
step:1794/2090 train_time:101035ms step_avg:56.32ms
step:1795/2090 train_time:101122ms step_avg:56.34ms
step:1796/2090 train_time:101210ms step_avg:56.35ms
step:1797/2090 train_time:101298ms step_avg:56.37ms
step:1798/2090 train_time:101386ms step_avg:56.39ms
step:1799/2090 train_time:101474ms step_avg:56.41ms
step:1800/2090 train_time:101562ms step_avg:56.42ms
step:1801/2090 train_time:101650ms step_avg:56.44ms
step:1802/2090 train_time:101737ms step_avg:56.46ms
step:1803/2090 train_time:101824ms step_avg:56.47ms
step:1804/2090 train_time:101911ms step_avg:56.49ms
step:1805/2090 train_time:101998ms step_avg:56.51ms
step:1806/2090 train_time:102085ms step_avg:56.53ms
step:1807/2090 train_time:102173ms step_avg:56.54ms
step:1808/2090 train_time:102260ms step_avg:56.56ms
step:1809/2090 train_time:102349ms step_avg:56.58ms
step:1810/2090 train_time:102436ms step_avg:56.59ms
step:1811/2090 train_time:102525ms step_avg:56.61ms
step:1812/2090 train_time:102613ms step_avg:56.63ms
step:1813/2090 train_time:102702ms step_avg:56.65ms
step:1814/2090 train_time:102789ms step_avg:56.66ms
step:1815/2090 train_time:102875ms step_avg:56.68ms
step:1816/2090 train_time:102962ms step_avg:56.70ms
step:1817/2090 train_time:103049ms step_avg:56.71ms
step:1818/2090 train_time:103136ms step_avg:56.73ms
step:1819/2090 train_time:103224ms step_avg:56.75ms
step:1820/2090 train_time:103311ms step_avg:56.76ms
step:1821/2090 train_time:103399ms step_avg:56.78ms
step:1822/2090 train_time:103486ms step_avg:56.80ms
step:1823/2090 train_time:103574ms step_avg:56.82ms
step:1824/2090 train_time:103662ms step_avg:56.83ms
step:1825/2090 train_time:103750ms step_avg:56.85ms
step:1826/2090 train_time:103837ms step_avg:56.87ms
step:1827/2090 train_time:103925ms step_avg:56.88ms
step:1828/2090 train_time:104011ms step_avg:56.90ms
step:1829/2090 train_time:104099ms step_avg:56.92ms
step:1830/2090 train_time:104186ms step_avg:56.93ms
step:1831/2090 train_time:104274ms step_avg:56.95ms
step:1832/2090 train_time:104361ms step_avg:56.97ms
step:1833/2090 train_time:104449ms step_avg:56.98ms
step:1834/2090 train_time:104538ms step_avg:57.00ms
step:1835/2090 train_time:104626ms step_avg:57.02ms
step:1836/2090 train_time:104714ms step_avg:57.03ms
step:1837/2090 train_time:104803ms step_avg:57.05ms
step:1838/2090 train_time:104890ms step_avg:57.07ms
step:1839/2090 train_time:104978ms step_avg:57.08ms
step:1840/2090 train_time:105066ms step_avg:57.10ms
step:1841/2090 train_time:105152ms step_avg:57.12ms
step:1842/2090 train_time:105240ms step_avg:57.13ms
step:1843/2090 train_time:105327ms step_avg:57.15ms
step:1844/2090 train_time:105414ms step_avg:57.17ms
step:1845/2090 train_time:105503ms step_avg:57.18ms
step:1846/2090 train_time:105590ms step_avg:57.20ms
step:1847/2090 train_time:105678ms step_avg:57.22ms
step:1848/2090 train_time:105765ms step_avg:57.23ms
step:1849/2090 train_time:105852ms step_avg:57.25ms
step:1850/2090 train_time:105940ms step_avg:57.26ms
step:1851/2090 train_time:106028ms step_avg:57.28ms
step:1852/2090 train_time:106115ms step_avg:57.30ms
step:1853/2090 train_time:106204ms step_avg:57.31ms
step:1854/2090 train_time:106289ms step_avg:57.33ms
step:1855/2090 train_time:106378ms step_avg:57.35ms
step:1856/2090 train_time:106465ms step_avg:57.36ms
step:1857/2090 train_time:106553ms step_avg:57.38ms
step:1858/2090 train_time:106641ms step_avg:57.40ms
step:1859/2090 train_time:106728ms step_avg:57.41ms
step:1860/2090 train_time:106814ms step_avg:57.43ms
step:1861/2090 train_time:106903ms step_avg:57.44ms
step:1862/2090 train_time:106990ms step_avg:57.46ms
step:1863/2090 train_time:107077ms step_avg:57.48ms
step:1864/2090 train_time:107164ms step_avg:57.49ms
step:1865/2090 train_time:107252ms step_avg:57.51ms
step:1866/2090 train_time:107340ms step_avg:57.52ms
step:1867/2090 train_time:107428ms step_avg:57.54ms
step:1868/2090 train_time:107515ms step_avg:57.56ms
step:1869/2090 train_time:107604ms step_avg:57.57ms
step:1870/2090 train_time:107690ms step_avg:57.59ms
step:1871/2090 train_time:107779ms step_avg:57.61ms
step:1872/2090 train_time:107866ms step_avg:57.62ms
step:1873/2090 train_time:107953ms step_avg:57.64ms
step:1874/2090 train_time:108041ms step_avg:57.65ms
step:1875/2090 train_time:108129ms step_avg:57.67ms
step:1876/2090 train_time:108216ms step_avg:57.68ms
step:1877/2090 train_time:108305ms step_avg:57.70ms
step:1878/2090 train_time:108391ms step_avg:57.72ms
step:1879/2090 train_time:108479ms step_avg:57.73ms
step:1880/2090 train_time:108566ms step_avg:57.75ms
step:1881/2090 train_time:108653ms step_avg:57.76ms
step:1882/2090 train_time:108740ms step_avg:57.78ms
step:1883/2090 train_time:108828ms step_avg:57.80ms
step:1884/2090 train_time:108915ms step_avg:57.81ms
step:1885/2090 train_time:109004ms step_avg:57.83ms
step:1886/2090 train_time:109091ms step_avg:57.84ms
step:1887/2090 train_time:109179ms step_avg:57.86ms
step:1888/2090 train_time:109265ms step_avg:57.87ms
step:1889/2090 train_time:109353ms step_avg:57.89ms
step:1890/2090 train_time:109440ms step_avg:57.91ms
step:1891/2090 train_time:109528ms step_avg:57.92ms
step:1892/2090 train_time:109615ms step_avg:57.94ms
step:1893/2090 train_time:109703ms step_avg:57.95ms
step:1894/2090 train_time:109790ms step_avg:57.97ms
step:1895/2090 train_time:109878ms step_avg:57.98ms
step:1896/2090 train_time:109966ms step_avg:58.00ms
step:1897/2090 train_time:110053ms step_avg:58.01ms
step:1898/2090 train_time:110140ms step_avg:58.03ms
step:1899/2090 train_time:110228ms step_avg:58.05ms
step:1900/2090 train_time:110315ms step_avg:58.06ms
step:1901/2090 train_time:110403ms step_avg:58.08ms
step:1902/2090 train_time:110489ms step_avg:58.09ms
step:1903/2090 train_time:110577ms step_avg:58.11ms
step:1904/2090 train_time:110664ms step_avg:58.12ms
step:1905/2090 train_time:110752ms step_avg:58.14ms
step:1906/2090 train_time:110839ms step_avg:58.15ms
step:1907/2090 train_time:110926ms step_avg:58.17ms
step:1908/2090 train_time:111013ms step_avg:58.18ms
step:1909/2090 train_time:111101ms step_avg:58.20ms
step:1910/2090 train_time:111189ms step_avg:58.21ms
step:1911/2090 train_time:111276ms step_avg:58.23ms
step:1912/2090 train_time:111364ms step_avg:58.25ms
step:1913/2090 train_time:111452ms step_avg:58.26ms
step:1914/2090 train_time:111539ms step_avg:58.28ms
step:1915/2090 train_time:111627ms step_avg:58.29ms
step:1916/2090 train_time:111714ms step_avg:58.31ms
step:1917/2090 train_time:111802ms step_avg:58.32ms
step:1918/2090 train_time:111890ms step_avg:58.34ms
step:1919/2090 train_time:111977ms step_avg:58.35ms
step:1920/2090 train_time:112064ms step_avg:58.37ms
step:1921/2090 train_time:112152ms step_avg:58.38ms
step:1922/2090 train_time:112239ms step_avg:58.40ms
step:1923/2090 train_time:112327ms step_avg:58.41ms
step:1924/2090 train_time:112414ms step_avg:58.43ms
step:1925/2090 train_time:112501ms step_avg:58.44ms
step:1926/2090 train_time:112589ms step_avg:58.46ms
step:1927/2090 train_time:112676ms step_avg:58.47ms
step:1928/2090 train_time:112764ms step_avg:58.49ms
step:1929/2090 train_time:112852ms step_avg:58.50ms
step:1930/2090 train_time:112938ms step_avg:58.52ms
step:1931/2090 train_time:113026ms step_avg:58.53ms
step:1932/2090 train_time:113113ms step_avg:58.55ms
step:1933/2090 train_time:113202ms step_avg:58.56ms
step:1934/2090 train_time:113290ms step_avg:58.58ms
step:1935/2090 train_time:113377ms step_avg:58.59ms
step:1936/2090 train_time:113465ms step_avg:58.61ms
step:1937/2090 train_time:113552ms step_avg:58.62ms
step:1938/2090 train_time:113639ms step_avg:58.64ms
step:1939/2090 train_time:113727ms step_avg:58.65ms
step:1940/2090 train_time:113815ms step_avg:58.67ms
step:1941/2090 train_time:113903ms step_avg:58.68ms
step:1942/2090 train_time:113990ms step_avg:58.70ms
step:1943/2090 train_time:114079ms step_avg:58.71ms
step:1944/2090 train_time:114166ms step_avg:58.73ms
step:1945/2090 train_time:114254ms step_avg:58.74ms
step:1946/2090 train_time:114341ms step_avg:58.76ms
step:1947/2090 train_time:114429ms step_avg:58.77ms
step:1948/2090 train_time:114516ms step_avg:58.79ms
step:1949/2090 train_time:114605ms step_avg:58.80ms
step:1950/2090 train_time:114692ms step_avg:58.82ms
step:1951/2090 train_time:114780ms step_avg:58.83ms
step:1952/2090 train_time:114867ms step_avg:58.85ms
step:1953/2090 train_time:114955ms step_avg:58.86ms
step:1954/2090 train_time:115041ms step_avg:58.87ms
step:1955/2090 train_time:115130ms step_avg:58.89ms
step:1956/2090 train_time:115217ms step_avg:58.90ms
step:1957/2090 train_time:115306ms step_avg:58.92ms
step:1958/2090 train_time:115392ms step_avg:58.93ms
step:1959/2090 train_time:115481ms step_avg:58.95ms
step:1960/2090 train_time:115568ms step_avg:58.96ms
step:1961/2090 train_time:115655ms step_avg:58.98ms
step:1962/2090 train_time:115743ms step_avg:58.99ms
step:1963/2090 train_time:115831ms step_avg:59.01ms
step:1964/2090 train_time:115918ms step_avg:59.02ms
step:1965/2090 train_time:116006ms step_avg:59.04ms
step:1966/2090 train_time:116092ms step_avg:59.05ms
step:1967/2090 train_time:116181ms step_avg:59.06ms
step:1968/2090 train_time:116268ms step_avg:59.08ms
step:1969/2090 train_time:116356ms step_avg:59.09ms
step:1970/2090 train_time:116443ms step_avg:59.11ms
step:1971/2090 train_time:116531ms step_avg:59.12ms
step:1972/2090 train_time:116618ms step_avg:59.14ms
step:1973/2090 train_time:116707ms step_avg:59.15ms
step:1974/2090 train_time:116794ms step_avg:59.17ms
step:1975/2090 train_time:116883ms step_avg:59.18ms
step:1976/2090 train_time:116969ms step_avg:59.19ms
step:1977/2090 train_time:117057ms step_avg:59.21ms
step:1978/2090 train_time:117144ms step_avg:59.22ms
step:1979/2090 train_time:117232ms step_avg:59.24ms
step:1980/2090 train_time:117319ms step_avg:59.25ms
step:1981/2090 train_time:117407ms step_avg:59.27ms
step:1982/2090 train_time:117493ms step_avg:59.28ms
step:1983/2090 train_time:117582ms step_avg:59.29ms
step:1984/2090 train_time:117669ms step_avg:59.31ms
step:1985/2090 train_time:117757ms step_avg:59.32ms
step:1986/2090 train_time:117844ms step_avg:59.34ms
step:1987/2090 train_time:117931ms step_avg:59.35ms
step:1988/2090 train_time:118018ms step_avg:59.37ms
step:1989/2090 train_time:118106ms step_avg:59.38ms
step:1990/2090 train_time:118193ms step_avg:59.39ms
step:1991/2090 train_time:118281ms step_avg:59.41ms
step:1992/2090 train_time:118369ms step_avg:59.42ms
step:1993/2090 train_time:118456ms step_avg:59.44ms
step:1994/2090 train_time:118545ms step_avg:59.45ms
step:1995/2090 train_time:118632ms step_avg:59.46ms
step:1996/2090 train_time:118720ms step_avg:59.48ms
step:1997/2090 train_time:118807ms step_avg:59.49ms
step:1998/2090 train_time:118894ms step_avg:59.51ms
step:1999/2090 train_time:118982ms step_avg:59.52ms
step:2000/2090 train_time:119069ms step_avg:59.53ms
step:2000/2090 val_loss:3.3006 train_time:119158ms step_avg:59.58ms
step:2001/2090 train_time:119180ms step_avg:59.56ms
step:2002/2090 train_time:119248ms step_avg:59.56ms
step:2003/2090 train_time:119340ms step_avg:59.58ms
step:2004/2090 train_time:119428ms step_avg:59.59ms
step:2005/2090 train_time:119516ms step_avg:59.61ms
step:2006/2090 train_time:119602ms step_avg:59.62ms
step:2007/2090 train_time:119690ms step_avg:59.64ms
step:2008/2090 train_time:119776ms step_avg:59.65ms
step:2009/2090 train_time:119862ms step_avg:59.66ms
step:2010/2090 train_time:119950ms step_avg:59.68ms
step:2011/2090 train_time:120037ms step_avg:59.69ms
step:2012/2090 train_time:120127ms step_avg:59.71ms
step:2013/2090 train_time:120218ms step_avg:59.72ms
step:2014/2090 train_time:120308ms step_avg:59.74ms
step:2015/2090 train_time:120397ms step_avg:59.75ms
step:2016/2090 train_time:120484ms step_avg:59.76ms
step:2017/2090 train_time:120571ms step_avg:59.78ms
step:2018/2090 train_time:120658ms step_avg:59.79ms
step:2019/2090 train_time:120745ms step_avg:59.80ms
step:2020/2090 train_time:120830ms step_avg:59.82ms
step:2021/2090 train_time:120919ms step_avg:59.83ms
step:2022/2090 train_time:121005ms step_avg:59.84ms
step:2023/2090 train_time:121093ms step_avg:59.86ms
step:2024/2090 train_time:121181ms step_avg:59.87ms
step:2025/2090 train_time:121271ms step_avg:59.89ms
step:2026/2090 train_time:121359ms step_avg:59.90ms
step:2027/2090 train_time:121448ms step_avg:59.92ms
step:2028/2090 train_time:121535ms step_avg:59.93ms
step:2029/2090 train_time:121623ms step_avg:59.94ms
step:2030/2090 train_time:121709ms step_avg:59.96ms
step:2031/2090 train_time:121797ms step_avg:59.97ms
step:2032/2090 train_time:121883ms step_avg:59.98ms
step:2033/2090 train_time:121970ms step_avg:60.00ms
step:2034/2090 train_time:122057ms step_avg:60.01ms
step:2035/2090 train_time:122145ms step_avg:60.02ms
step:2036/2090 train_time:122232ms step_avg:60.04ms
step:2037/2090 train_time:122321ms step_avg:60.05ms
step:2038/2090 train_time:122408ms step_avg:60.06ms
step:2039/2090 train_time:122497ms step_avg:60.08ms
step:2040/2090 train_time:122584ms step_avg:60.09ms
step:2041/2090 train_time:122672ms step_avg:60.10ms
step:2042/2090 train_time:122758ms step_avg:60.12ms
step:2043/2090 train_time:122846ms step_avg:60.13ms
step:2044/2090 train_time:122932ms step_avg:60.14ms
step:2045/2090 train_time:123020ms step_avg:60.16ms
step:2046/2090 train_time:123107ms step_avg:60.17ms
step:2047/2090 train_time:123195ms step_avg:60.18ms
step:2048/2090 train_time:123283ms step_avg:60.20ms
step:2049/2090 train_time:123371ms step_avg:60.21ms
step:2050/2090 train_time:123458ms step_avg:60.22ms
step:2051/2090 train_time:123547ms step_avg:60.24ms
step:2052/2090 train_time:123634ms step_avg:60.25ms
step:2053/2090 train_time:123722ms step_avg:60.26ms
step:2054/2090 train_time:123809ms step_avg:60.28ms
step:2055/2090 train_time:123897ms step_avg:60.29ms
step:2056/2090 train_time:123983ms step_avg:60.30ms
step:2057/2090 train_time:124072ms step_avg:60.32ms
step:2058/2090 train_time:124159ms step_avg:60.33ms
step:2059/2090 train_time:124248ms step_avg:60.34ms
step:2060/2090 train_time:124336ms step_avg:60.36ms
step:2061/2090 train_time:124426ms step_avg:60.37ms
step:2062/2090 train_time:124514ms step_avg:60.38ms
step:2063/2090 train_time:124602ms step_avg:60.40ms
step:2064/2090 train_time:124688ms step_avg:60.41ms
step:2065/2090 train_time:124776ms step_avg:60.42ms
step:2066/2090 train_time:124862ms step_avg:60.44ms
step:2067/2090 train_time:124950ms step_avg:60.45ms
step:2068/2090 train_time:125037ms step_avg:60.46ms
step:2069/2090 train_time:125126ms step_avg:60.48ms
step:2070/2090 train_time:125213ms step_avg:60.49ms
step:2071/2090 train_time:125302ms step_avg:60.50ms
step:2072/2090 train_time:125389ms step_avg:60.52ms
step:2073/2090 train_time:125479ms step_avg:60.53ms
step:2074/2090 train_time:125566ms step_avg:60.54ms
step:2075/2090 train_time:125654ms step_avg:60.56ms
step:2076/2090 train_time:125740ms step_avg:60.57ms
step:2077/2090 train_time:125829ms step_avg:60.58ms
step:2078/2090 train_time:125915ms step_avg:60.59ms
step:2079/2090 train_time:126004ms step_avg:60.61ms
step:2080/2090 train_time:126090ms step_avg:60.62ms
step:2081/2090 train_time:126179ms step_avg:60.63ms
step:2082/2090 train_time:126266ms step_avg:60.65ms
step:2083/2090 train_time:126355ms step_avg:60.66ms
step:2084/2090 train_time:126443ms step_avg:60.67ms
step:2085/2090 train_time:126531ms step_avg:60.69ms
step:2086/2090 train_time:126619ms step_avg:60.70ms
step:2087/2090 train_time:126707ms step_avg:60.71ms
step:2088/2090 train_time:126794ms step_avg:60.73ms
step:2089/2090 train_time:126883ms step_avg:60.74ms
step:2090/2090 train_time:126969ms step_avg:60.75ms
step:2090/2090 val_loss:3.2870 train_time:127059ms step_avg:60.79ms
peak memory allocated: 29892 MiB reserved: 44436 MiB

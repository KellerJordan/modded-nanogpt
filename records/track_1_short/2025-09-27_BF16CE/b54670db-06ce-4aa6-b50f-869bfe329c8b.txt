import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:29:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            124W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            123W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            116W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    158005      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    158006      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    158007      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    158008      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    158009      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    158010      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    158011      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    158012      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    158006      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    158007      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    158008      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    158009      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    158010      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    158011      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    158012      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:147ms step_avg:147.10ms
step:2/1680 train_time:167ms step_avg:83.35ms
step:3/1680 train_time:231ms step_avg:76.88ms
step:4/1680 train_time:316ms step_avg:79.00ms
step:5/1680 train_time:402ms step_avg:80.35ms
step:6/1680 train_time:489ms step_avg:81.50ms
step:7/1680 train_time:575ms step_avg:82.17ms
step:8/1680 train_time:661ms step_avg:82.67ms
step:9/1680 train_time:747ms step_avg:83.03ms
step:10/1680 train_time:834ms step_avg:83.39ms
step:11/1680 train_time:920ms step_avg:83.63ms
step:12/1680 train_time:1009ms step_avg:84.07ms
step:13/1680 train_time:1101ms step_avg:84.67ms
step:14/1680 train_time:1190ms step_avg:85.03ms
step:15/1680 train_time:1278ms step_avg:85.22ms
step:16/1680 train_time:1365ms step_avg:85.34ms
step:17/1680 train_time:1453ms step_avg:85.46ms
step:18/1680 train_time:1539ms step_avg:85.52ms
step:19/1680 train_time:1626ms step_avg:85.57ms
step:20/1680 train_time:1712ms step_avg:85.62ms
step:21/1680 train_time:1799ms step_avg:85.65ms
step:22/1680 train_time:1885ms step_avg:85.69ms
step:23/1680 train_time:1972ms step_avg:85.75ms
step:24/1680 train_time:2061ms step_avg:85.88ms
step:25/1680 train_time:2149ms step_avg:85.97ms
step:26/1680 train_time:2238ms step_avg:86.06ms
step:27/1680 train_time:2326ms step_avg:86.14ms
step:28/1680 train_time:2413ms step_avg:86.18ms
step:29/1680 train_time:2500ms step_avg:86.20ms
step:30/1680 train_time:2587ms step_avg:86.23ms
step:31/1680 train_time:2674ms step_avg:86.25ms
step:32/1680 train_time:2760ms step_avg:86.26ms
step:33/1680 train_time:2847ms step_avg:86.28ms
step:34/1680 train_time:2935ms step_avg:86.31ms
step:35/1680 train_time:3023ms step_avg:86.39ms
step:36/1680 train_time:3111ms step_avg:86.42ms
step:37/1680 train_time:3199ms step_avg:86.46ms
step:38/1680 train_time:3287ms step_avg:86.50ms
step:39/1680 train_time:3375ms step_avg:86.53ms
step:40/1680 train_time:3462ms step_avg:86.55ms
step:41/1680 train_time:3549ms step_avg:86.57ms
step:42/1680 train_time:3636ms step_avg:86.58ms
step:43/1680 train_time:3724ms step_avg:86.60ms
step:44/1680 train_time:3810ms step_avg:86.59ms
step:45/1680 train_time:3897ms step_avg:86.60ms
step:46/1680 train_time:3985ms step_avg:86.63ms
step:47/1680 train_time:4073ms step_avg:86.67ms
step:48/1680 train_time:4162ms step_avg:86.71ms
step:49/1680 train_time:4250ms step_avg:86.73ms
step:50/1680 train_time:4337ms step_avg:86.74ms
step:51/1680 train_time:4425ms step_avg:86.76ms
step:52/1680 train_time:4512ms step_avg:86.78ms
step:53/1680 train_time:4600ms step_avg:86.78ms
step:54/1680 train_time:4687ms step_avg:86.80ms
step:55/1680 train_time:4774ms step_avg:86.80ms
step:56/1680 train_time:4862ms step_avg:86.82ms
step:57/1680 train_time:4949ms step_avg:86.83ms
step:58/1680 train_time:5036ms step_avg:86.84ms
step:59/1680 train_time:5124ms step_avg:86.85ms
step:60/1680 train_time:5211ms step_avg:86.86ms
step:61/1680 train_time:5299ms step_avg:86.87ms
step:62/1680 train_time:5387ms step_avg:86.88ms
step:63/1680 train_time:5474ms step_avg:86.89ms
step:64/1680 train_time:5561ms step_avg:86.90ms
step:65/1680 train_time:5648ms step_avg:86.90ms
step:66/1680 train_time:5737ms step_avg:86.92ms
step:67/1680 train_time:5824ms step_avg:86.92ms
step:68/1680 train_time:5911ms step_avg:86.93ms
step:69/1680 train_time:5998ms step_avg:86.93ms
step:70/1680 train_time:6086ms step_avg:86.95ms
step:71/1680 train_time:6174ms step_avg:86.96ms
step:72/1680 train_time:6262ms step_avg:86.97ms
step:73/1680 train_time:6349ms step_avg:86.97ms
step:74/1680 train_time:6437ms step_avg:86.98ms
step:75/1680 train_time:6523ms step_avg:86.98ms
step:76/1680 train_time:6611ms step_avg:86.99ms
step:77/1680 train_time:6698ms step_avg:86.99ms
step:78/1680 train_time:6785ms step_avg:86.99ms
step:79/1680 train_time:6873ms step_avg:87.00ms
step:80/1680 train_time:6960ms step_avg:87.00ms
step:81/1680 train_time:7047ms step_avg:87.00ms
step:82/1680 train_time:7135ms step_avg:87.01ms
step:83/1680 train_time:7222ms step_avg:87.02ms
step:84/1680 train_time:7309ms step_avg:87.02ms
step:85/1680 train_time:7397ms step_avg:87.02ms
step:86/1680 train_time:7484ms step_avg:87.02ms
step:87/1680 train_time:7571ms step_avg:87.02ms
step:88/1680 train_time:7658ms step_avg:87.02ms
step:89/1680 train_time:7744ms step_avg:87.02ms
step:90/1680 train_time:7832ms step_avg:87.02ms
step:91/1680 train_time:7919ms step_avg:87.02ms
step:92/1680 train_time:8005ms step_avg:87.02ms
step:93/1680 train_time:8093ms step_avg:87.03ms
step:94/1680 train_time:8181ms step_avg:87.03ms
step:95/1680 train_time:8268ms step_avg:87.03ms
step:96/1680 train_time:8355ms step_avg:87.04ms
step:97/1680 train_time:8442ms step_avg:87.03ms
step:98/1680 train_time:8530ms step_avg:87.04ms
step:99/1680 train_time:8617ms step_avg:87.04ms
step:100/1680 train_time:8704ms step_avg:87.04ms
step:101/1680 train_time:8792ms step_avg:87.05ms
step:102/1680 train_time:8879ms step_avg:87.04ms
step:103/1680 train_time:8966ms step_avg:87.05ms
step:104/1680 train_time:9054ms step_avg:87.05ms
step:105/1680 train_time:9141ms step_avg:87.05ms
step:106/1680 train_time:9228ms step_avg:87.06ms
step:107/1680 train_time:9315ms step_avg:87.06ms
step:108/1680 train_time:9402ms step_avg:87.06ms
step:109/1680 train_time:9489ms step_avg:87.06ms
step:110/1680 train_time:9577ms step_avg:87.06ms
step:111/1680 train_time:9664ms step_avg:87.06ms
step:112/1680 train_time:9751ms step_avg:87.06ms
step:113/1680 train_time:9838ms step_avg:87.06ms
step:114/1680 train_time:9925ms step_avg:87.07ms
step:115/1680 train_time:10013ms step_avg:87.07ms
step:116/1680 train_time:10100ms step_avg:87.07ms
step:117/1680 train_time:10187ms step_avg:87.07ms
step:118/1680 train_time:10275ms step_avg:87.08ms
step:119/1680 train_time:10362ms step_avg:87.08ms
step:120/1680 train_time:10449ms step_avg:87.08ms
step:121/1680 train_time:10537ms step_avg:87.08ms
step:122/1680 train_time:10625ms step_avg:87.09ms
step:123/1680 train_time:10711ms step_avg:87.08ms
step:124/1680 train_time:10798ms step_avg:87.08ms
step:125/1680 train_time:10885ms step_avg:87.08ms
step:125/1680 val_loss:4.3150 train_time:10973ms step_avg:87.78ms
step:126/1680 train_time:10992ms step_avg:87.24ms
step:127/1680 train_time:11064ms step_avg:87.12ms
step:128/1680 train_time:11159ms step_avg:87.18ms
step:129/1680 train_time:11252ms step_avg:87.22ms
step:130/1680 train_time:11340ms step_avg:87.23ms
step:131/1680 train_time:11427ms step_avg:87.23ms
step:132/1680 train_time:11514ms step_avg:87.22ms
step:133/1680 train_time:11599ms step_avg:87.21ms
step:134/1680 train_time:11685ms step_avg:87.20ms
step:135/1680 train_time:11771ms step_avg:87.19ms
step:136/1680 train_time:11857ms step_avg:87.19ms
step:137/1680 train_time:11944ms step_avg:87.18ms
step:138/1680 train_time:12032ms step_avg:87.19ms
step:139/1680 train_time:12120ms step_avg:87.20ms
step:140/1680 train_time:12209ms step_avg:87.21ms
step:141/1680 train_time:12298ms step_avg:87.22ms
step:142/1680 train_time:12386ms step_avg:87.22ms
step:143/1680 train_time:12473ms step_avg:87.22ms
step:144/1680 train_time:12560ms step_avg:87.22ms
step:145/1680 train_time:12646ms step_avg:87.22ms
step:146/1680 train_time:12732ms step_avg:87.21ms
step:147/1680 train_time:12818ms step_avg:87.20ms
step:148/1680 train_time:12904ms step_avg:87.19ms
step:149/1680 train_time:12991ms step_avg:87.19ms
step:150/1680 train_time:13079ms step_avg:87.19ms
step:151/1680 train_time:13167ms step_avg:87.20ms
step:152/1680 train_time:13255ms step_avg:87.20ms
step:153/1680 train_time:13342ms step_avg:87.20ms
step:154/1680 train_time:13429ms step_avg:87.20ms
step:155/1680 train_time:13516ms step_avg:87.20ms
step:156/1680 train_time:13603ms step_avg:87.20ms
step:157/1680 train_time:13690ms step_avg:87.19ms
step:158/1680 train_time:13776ms step_avg:87.19ms
step:159/1680 train_time:13862ms step_avg:87.18ms
step:160/1680 train_time:13949ms step_avg:87.18ms
step:161/1680 train_time:14036ms step_avg:87.18ms
step:162/1680 train_time:14124ms step_avg:87.18ms
step:163/1680 train_time:14212ms step_avg:87.19ms
step:164/1680 train_time:14300ms step_avg:87.19ms
step:165/1680 train_time:14387ms step_avg:87.20ms
step:166/1680 train_time:14475ms step_avg:87.20ms
step:167/1680 train_time:14562ms step_avg:87.20ms
step:168/1680 train_time:14649ms step_avg:87.20ms
step:169/1680 train_time:14736ms step_avg:87.20ms
step:170/1680 train_time:14822ms step_avg:87.19ms
step:171/1680 train_time:14909ms step_avg:87.19ms
step:172/1680 train_time:14996ms step_avg:87.19ms
step:173/1680 train_time:15083ms step_avg:87.18ms
step:174/1680 train_time:15171ms step_avg:87.19ms
step:175/1680 train_time:15259ms step_avg:87.19ms
step:176/1680 train_time:15346ms step_avg:87.19ms
step:177/1680 train_time:15434ms step_avg:87.20ms
step:178/1680 train_time:15521ms step_avg:87.19ms
step:179/1680 train_time:15608ms step_avg:87.19ms
step:180/1680 train_time:15695ms step_avg:87.19ms
step:181/1680 train_time:15781ms step_avg:87.19ms
step:182/1680 train_time:15868ms step_avg:87.19ms
step:183/1680 train_time:15955ms step_avg:87.19ms
step:184/1680 train_time:16042ms step_avg:87.18ms
step:185/1680 train_time:16130ms step_avg:87.19ms
step:186/1680 train_time:16217ms step_avg:87.19ms
step:187/1680 train_time:16304ms step_avg:87.19ms
step:188/1680 train_time:16391ms step_avg:87.19ms
step:189/1680 train_time:16479ms step_avg:87.19ms
step:190/1680 train_time:16566ms step_avg:87.19ms
step:191/1680 train_time:16654ms step_avg:87.19ms
step:192/1680 train_time:16740ms step_avg:87.19ms
step:193/1680 train_time:16827ms step_avg:87.19ms
step:194/1680 train_time:16913ms step_avg:87.18ms
step:195/1680 train_time:17000ms step_avg:87.18ms
step:196/1680 train_time:17087ms step_avg:87.18ms
step:197/1680 train_time:17174ms step_avg:87.18ms
step:198/1680 train_time:17261ms step_avg:87.18ms
step:199/1680 train_time:17349ms step_avg:87.18ms
step:200/1680 train_time:17436ms step_avg:87.18ms
step:201/1680 train_time:17523ms step_avg:87.18ms
step:202/1680 train_time:17610ms step_avg:87.18ms
step:203/1680 train_time:17698ms step_avg:87.18ms
step:204/1680 train_time:17785ms step_avg:87.18ms
step:205/1680 train_time:17871ms step_avg:87.18ms
step:206/1680 train_time:17958ms step_avg:87.18ms
step:207/1680 train_time:18045ms step_avg:87.17ms
step:208/1680 train_time:18132ms step_avg:87.17ms
step:209/1680 train_time:18219ms step_avg:87.17ms
step:210/1680 train_time:18307ms step_avg:87.18ms
step:211/1680 train_time:18394ms step_avg:87.18ms
step:212/1680 train_time:18482ms step_avg:87.18ms
step:213/1680 train_time:18569ms step_avg:87.18ms
step:214/1680 train_time:18656ms step_avg:87.18ms
step:215/1680 train_time:18743ms step_avg:87.18ms
step:216/1680 train_time:18830ms step_avg:87.18ms
step:217/1680 train_time:18917ms step_avg:87.18ms
step:218/1680 train_time:19004ms step_avg:87.17ms
step:219/1680 train_time:19091ms step_avg:87.17ms
step:220/1680 train_time:19178ms step_avg:87.17ms
step:221/1680 train_time:19265ms step_avg:87.17ms
step:222/1680 train_time:19353ms step_avg:87.17ms
step:223/1680 train_time:19440ms step_avg:87.18ms
step:224/1680 train_time:19527ms step_avg:87.17ms
step:225/1680 train_time:19615ms step_avg:87.18ms
step:226/1680 train_time:19701ms step_avg:87.17ms
step:227/1680 train_time:19788ms step_avg:87.17ms
step:228/1680 train_time:19874ms step_avg:87.17ms
step:229/1680 train_time:19961ms step_avg:87.17ms
step:230/1680 train_time:20049ms step_avg:87.17ms
step:231/1680 train_time:20136ms step_avg:87.17ms
step:232/1680 train_time:20223ms step_avg:87.17ms
step:233/1680 train_time:20311ms step_avg:87.17ms
step:234/1680 train_time:20398ms step_avg:87.17ms
step:235/1680 train_time:20486ms step_avg:87.17ms
step:236/1680 train_time:20573ms step_avg:87.17ms
step:237/1680 train_time:20660ms step_avg:87.17ms
step:238/1680 train_time:20747ms step_avg:87.17ms
step:239/1680 train_time:20834ms step_avg:87.17ms
step:240/1680 train_time:20921ms step_avg:87.17ms
step:241/1680 train_time:21008ms step_avg:87.17ms
step:242/1680 train_time:21095ms step_avg:87.17ms
step:243/1680 train_time:21182ms step_avg:87.17ms
step:244/1680 train_time:21269ms step_avg:87.17ms
step:245/1680 train_time:21356ms step_avg:87.17ms
step:246/1680 train_time:21444ms step_avg:87.17ms
step:247/1680 train_time:21531ms step_avg:87.17ms
step:248/1680 train_time:21618ms step_avg:87.17ms
step:249/1680 train_time:21705ms step_avg:87.17ms
step:250/1680 train_time:21792ms step_avg:87.17ms
step:250/1680 val_loss:3.9741 train_time:21881ms step_avg:87.52ms
step:251/1680 train_time:21899ms step_avg:87.25ms
step:252/1680 train_time:21969ms step_avg:87.18ms
step:253/1680 train_time:22061ms step_avg:87.20ms
step:254/1680 train_time:22150ms step_avg:87.21ms
step:255/1680 train_time:22238ms step_avg:87.21ms
step:256/1680 train_time:22325ms step_avg:87.21ms
step:257/1680 train_time:22411ms step_avg:87.20ms
step:258/1680 train_time:22497ms step_avg:87.20ms
step:259/1680 train_time:22583ms step_avg:87.19ms
step:260/1680 train_time:22670ms step_avg:87.19ms
step:261/1680 train_time:22756ms step_avg:87.19ms
step:262/1680 train_time:22843ms step_avg:87.19ms
step:263/1680 train_time:22931ms step_avg:87.19ms
step:264/1680 train_time:23020ms step_avg:87.20ms
step:265/1680 train_time:23108ms step_avg:87.20ms
step:266/1680 train_time:23196ms step_avg:87.20ms
step:267/1680 train_time:23282ms step_avg:87.20ms
step:268/1680 train_time:23369ms step_avg:87.20ms
step:269/1680 train_time:23456ms step_avg:87.20ms
step:270/1680 train_time:23543ms step_avg:87.19ms
step:271/1680 train_time:23629ms step_avg:87.19ms
step:272/1680 train_time:23717ms step_avg:87.19ms
step:273/1680 train_time:23803ms step_avg:87.19ms
step:274/1680 train_time:23891ms step_avg:87.19ms
step:275/1680 train_time:23979ms step_avg:87.20ms
step:276/1680 train_time:24066ms step_avg:87.20ms
step:277/1680 train_time:24153ms step_avg:87.20ms
step:278/1680 train_time:24242ms step_avg:87.20ms
step:279/1680 train_time:24329ms step_avg:87.20ms
step:280/1680 train_time:24416ms step_avg:87.20ms
step:281/1680 train_time:24503ms step_avg:87.20ms
step:282/1680 train_time:24590ms step_avg:87.20ms
step:283/1680 train_time:24677ms step_avg:87.20ms
step:284/1680 train_time:24763ms step_avg:87.19ms
step:285/1680 train_time:24851ms step_avg:87.20ms
step:286/1680 train_time:24938ms step_avg:87.19ms
step:287/1680 train_time:25025ms step_avg:87.20ms
step:288/1680 train_time:25113ms step_avg:87.20ms
step:289/1680 train_time:25200ms step_avg:87.20ms
step:290/1680 train_time:25288ms step_avg:87.20ms
step:291/1680 train_time:25375ms step_avg:87.20ms
step:292/1680 train_time:25463ms step_avg:87.20ms
step:293/1680 train_time:25549ms step_avg:87.20ms
step:294/1680 train_time:25636ms step_avg:87.20ms
step:295/1680 train_time:25722ms step_avg:87.19ms
step:296/1680 train_time:25809ms step_avg:87.19ms
step:297/1680 train_time:25896ms step_avg:87.19ms
step:298/1680 train_time:25984ms step_avg:87.19ms
step:299/1680 train_time:26071ms step_avg:87.19ms
step:300/1680 train_time:26158ms step_avg:87.19ms
step:301/1680 train_time:26247ms step_avg:87.20ms
step:302/1680 train_time:26334ms step_avg:87.20ms
step:303/1680 train_time:26421ms step_avg:87.20ms
step:304/1680 train_time:26508ms step_avg:87.20ms
step:305/1680 train_time:26595ms step_avg:87.20ms
step:306/1680 train_time:26682ms step_avg:87.20ms
step:307/1680 train_time:26769ms step_avg:87.20ms
step:308/1680 train_time:26856ms step_avg:87.20ms
step:309/1680 train_time:26943ms step_avg:87.20ms
step:310/1680 train_time:27030ms step_avg:87.19ms
step:311/1680 train_time:27118ms step_avg:87.20ms
step:312/1680 train_time:27205ms step_avg:87.20ms
step:313/1680 train_time:27292ms step_avg:87.20ms
step:314/1680 train_time:27379ms step_avg:87.20ms
step:315/1680 train_time:27467ms step_avg:87.20ms
step:316/1680 train_time:27554ms step_avg:87.20ms
step:317/1680 train_time:27640ms step_avg:87.19ms
step:318/1680 train_time:27727ms step_avg:87.19ms
step:319/1680 train_time:27814ms step_avg:87.19ms
step:320/1680 train_time:27901ms step_avg:87.19ms
step:321/1680 train_time:27989ms step_avg:87.19ms
step:322/1680 train_time:28076ms step_avg:87.19ms
step:323/1680 train_time:28163ms step_avg:87.19ms
step:324/1680 train_time:28250ms step_avg:87.19ms
step:325/1680 train_time:28338ms step_avg:87.19ms
step:326/1680 train_time:28425ms step_avg:87.19ms
step:327/1680 train_time:28512ms step_avg:87.19ms
step:328/1680 train_time:28599ms step_avg:87.19ms
step:329/1680 train_time:28685ms step_avg:87.19ms
step:330/1680 train_time:28772ms step_avg:87.19ms
step:331/1680 train_time:28859ms step_avg:87.19ms
step:332/1680 train_time:28946ms step_avg:87.19ms
step:333/1680 train_time:29034ms step_avg:87.19ms
step:334/1680 train_time:29121ms step_avg:87.19ms
step:335/1680 train_time:29208ms step_avg:87.19ms
step:336/1680 train_time:29296ms step_avg:87.19ms
step:337/1680 train_time:29383ms step_avg:87.19ms
step:338/1680 train_time:29470ms step_avg:87.19ms
step:339/1680 train_time:29557ms step_avg:87.19ms
step:340/1680 train_time:29644ms step_avg:87.19ms
step:341/1680 train_time:29731ms step_avg:87.19ms
step:342/1680 train_time:29818ms step_avg:87.19ms
step:343/1680 train_time:29905ms step_avg:87.19ms
step:344/1680 train_time:29992ms step_avg:87.19ms
step:345/1680 train_time:30080ms step_avg:87.19ms
step:346/1680 train_time:30167ms step_avg:87.19ms
step:347/1680 train_time:30254ms step_avg:87.19ms
step:348/1680 train_time:30341ms step_avg:87.19ms
step:349/1680 train_time:30428ms step_avg:87.19ms
step:350/1680 train_time:30516ms step_avg:87.19ms
step:351/1680 train_time:30603ms step_avg:87.19ms
step:352/1680 train_time:30690ms step_avg:87.19ms
step:353/1680 train_time:30777ms step_avg:87.19ms
step:354/1680 train_time:30865ms step_avg:87.19ms
step:355/1680 train_time:30952ms step_avg:87.19ms
step:356/1680 train_time:31039ms step_avg:87.19ms
step:357/1680 train_time:31127ms step_avg:87.19ms
step:358/1680 train_time:31214ms step_avg:87.19ms
step:359/1680 train_time:31301ms step_avg:87.19ms
step:360/1680 train_time:31388ms step_avg:87.19ms
step:361/1680 train_time:31475ms step_avg:87.19ms
step:362/1680 train_time:31562ms step_avg:87.19ms
step:363/1680 train_time:31649ms step_avg:87.19ms
step:364/1680 train_time:31736ms step_avg:87.19ms
step:365/1680 train_time:31823ms step_avg:87.19ms
step:366/1680 train_time:31911ms step_avg:87.19ms
step:367/1680 train_time:31998ms step_avg:87.19ms
step:368/1680 train_time:32086ms step_avg:87.19ms
step:369/1680 train_time:32173ms step_avg:87.19ms
step:370/1680 train_time:32260ms step_avg:87.19ms
step:371/1680 train_time:32347ms step_avg:87.19ms
step:372/1680 train_time:32434ms step_avg:87.19ms
step:373/1680 train_time:32521ms step_avg:87.19ms
step:374/1680 train_time:32608ms step_avg:87.19ms
step:375/1680 train_time:32695ms step_avg:87.19ms
step:375/1680 val_loss:3.8226 train_time:32783ms step_avg:87.42ms
step:376/1680 train_time:32802ms step_avg:87.24ms
step:377/1680 train_time:32876ms step_avg:87.20ms
step:378/1680 train_time:32967ms step_avg:87.21ms
step:379/1680 train_time:33055ms step_avg:87.22ms
step:380/1680 train_time:33142ms step_avg:87.22ms
step:381/1680 train_time:33228ms step_avg:87.21ms
step:382/1680 train_time:33315ms step_avg:87.21ms
step:383/1680 train_time:33401ms step_avg:87.21ms
step:384/1680 train_time:33487ms step_avg:87.21ms
step:385/1680 train_time:33574ms step_avg:87.20ms
step:386/1680 train_time:33660ms step_avg:87.20ms
step:387/1680 train_time:33747ms step_avg:87.20ms
step:388/1680 train_time:33836ms step_avg:87.21ms
step:389/1680 train_time:33925ms step_avg:87.21ms
step:390/1680 train_time:34014ms step_avg:87.21ms
step:391/1680 train_time:34102ms step_avg:87.22ms
step:392/1680 train_time:34188ms step_avg:87.22ms
step:393/1680 train_time:34275ms step_avg:87.21ms
step:394/1680 train_time:34362ms step_avg:87.21ms
step:395/1680 train_time:34448ms step_avg:87.21ms
step:396/1680 train_time:34534ms step_avg:87.21ms
step:397/1680 train_time:34621ms step_avg:87.21ms
step:398/1680 train_time:34708ms step_avg:87.20ms
step:399/1680 train_time:34795ms step_avg:87.21ms
step:400/1680 train_time:34882ms step_avg:87.21ms
step:401/1680 train_time:34970ms step_avg:87.21ms
step:402/1680 train_time:35057ms step_avg:87.21ms
step:403/1680 train_time:35145ms step_avg:87.21ms
step:404/1680 train_time:35232ms step_avg:87.21ms
step:405/1680 train_time:35319ms step_avg:87.21ms
step:406/1680 train_time:35405ms step_avg:87.21ms
step:407/1680 train_time:35492ms step_avg:87.20ms
step:408/1680 train_time:35579ms step_avg:87.20ms
step:409/1680 train_time:35665ms step_avg:87.20ms
step:410/1680 train_time:35753ms step_avg:87.20ms
step:411/1680 train_time:35840ms step_avg:87.20ms
step:412/1680 train_time:35927ms step_avg:87.20ms
step:413/1680 train_time:36015ms step_avg:87.20ms
step:414/1680 train_time:36102ms step_avg:87.20ms
step:415/1680 train_time:36189ms step_avg:87.20ms
step:416/1680 train_time:36276ms step_avg:87.20ms
step:417/1680 train_time:36363ms step_avg:87.20ms
step:418/1680 train_time:36450ms step_avg:87.20ms
step:419/1680 train_time:36538ms step_avg:87.20ms
step:420/1680 train_time:36624ms step_avg:87.20ms
step:421/1680 train_time:36711ms step_avg:87.20ms
step:422/1680 train_time:36798ms step_avg:87.20ms
step:423/1680 train_time:36885ms step_avg:87.20ms
step:424/1680 train_time:36973ms step_avg:87.20ms
step:425/1680 train_time:37061ms step_avg:87.20ms
step:426/1680 train_time:37148ms step_avg:87.20ms
step:427/1680 train_time:37234ms step_avg:87.20ms
step:428/1680 train_time:37321ms step_avg:87.20ms
step:429/1680 train_time:37408ms step_avg:87.20ms
step:430/1680 train_time:37495ms step_avg:87.20ms
step:431/1680 train_time:37582ms step_avg:87.20ms
step:432/1680 train_time:37669ms step_avg:87.20ms
step:433/1680 train_time:37756ms step_avg:87.20ms
step:434/1680 train_time:37842ms step_avg:87.19ms
step:435/1680 train_time:37930ms step_avg:87.19ms
step:436/1680 train_time:38017ms step_avg:87.20ms
step:437/1680 train_time:38105ms step_avg:87.20ms
step:438/1680 train_time:38192ms step_avg:87.20ms
step:439/1680 train_time:38280ms step_avg:87.20ms
step:440/1680 train_time:38368ms step_avg:87.20ms
step:441/1680 train_time:38454ms step_avg:87.20ms
step:442/1680 train_time:38541ms step_avg:87.20ms
step:443/1680 train_time:38627ms step_avg:87.20ms
step:444/1680 train_time:38715ms step_avg:87.20ms
step:445/1680 train_time:38802ms step_avg:87.20ms
step:446/1680 train_time:38889ms step_avg:87.19ms
step:447/1680 train_time:38977ms step_avg:87.20ms
step:448/1680 train_time:39065ms step_avg:87.20ms
step:449/1680 train_time:39152ms step_avg:87.20ms
step:450/1680 train_time:39239ms step_avg:87.20ms
step:451/1680 train_time:39326ms step_avg:87.20ms
step:452/1680 train_time:39414ms step_avg:87.20ms
step:453/1680 train_time:39500ms step_avg:87.20ms
step:454/1680 train_time:39587ms step_avg:87.20ms
step:455/1680 train_time:39674ms step_avg:87.20ms
step:456/1680 train_time:39761ms step_avg:87.19ms
step:457/1680 train_time:39848ms step_avg:87.19ms
step:458/1680 train_time:39935ms step_avg:87.19ms
step:459/1680 train_time:40022ms step_avg:87.19ms
step:460/1680 train_time:40110ms step_avg:87.20ms
step:461/1680 train_time:40198ms step_avg:87.20ms
step:462/1680 train_time:40285ms step_avg:87.20ms
step:463/1680 train_time:40372ms step_avg:87.20ms
step:464/1680 train_time:40459ms step_avg:87.20ms
step:465/1680 train_time:40546ms step_avg:87.19ms
step:466/1680 train_time:40633ms step_avg:87.19ms
step:467/1680 train_time:40720ms step_avg:87.19ms
step:468/1680 train_time:40808ms step_avg:87.20ms
step:469/1680 train_time:40895ms step_avg:87.20ms
step:470/1680 train_time:40982ms step_avg:87.20ms
step:471/1680 train_time:41069ms step_avg:87.20ms
step:472/1680 train_time:41156ms step_avg:87.20ms
step:473/1680 train_time:41244ms step_avg:87.20ms
step:474/1680 train_time:41331ms step_avg:87.20ms
step:475/1680 train_time:41417ms step_avg:87.19ms
step:476/1680 train_time:41504ms step_avg:87.19ms
step:477/1680 train_time:41591ms step_avg:87.19ms
step:478/1680 train_time:41679ms step_avg:87.19ms
step:479/1680 train_time:41767ms step_avg:87.20ms
step:480/1680 train_time:41854ms step_avg:87.19ms
step:481/1680 train_time:41940ms step_avg:87.19ms
step:482/1680 train_time:42028ms step_avg:87.19ms
step:483/1680 train_time:42115ms step_avg:87.19ms
step:484/1680 train_time:42202ms step_avg:87.19ms
step:485/1680 train_time:42289ms step_avg:87.19ms
step:486/1680 train_time:42377ms step_avg:87.19ms
step:487/1680 train_time:42463ms step_avg:87.19ms
step:488/1680 train_time:42550ms step_avg:87.19ms
step:489/1680 train_time:42637ms step_avg:87.19ms
step:490/1680 train_time:42725ms step_avg:87.19ms
step:491/1680 train_time:42812ms step_avg:87.19ms
step:492/1680 train_time:42900ms step_avg:87.20ms
step:493/1680 train_time:42987ms step_avg:87.19ms
step:494/1680 train_time:43074ms step_avg:87.19ms
step:495/1680 train_time:43161ms step_avg:87.19ms
step:496/1680 train_time:43248ms step_avg:87.19ms
step:497/1680 train_time:43335ms step_avg:87.19ms
step:498/1680 train_time:43422ms step_avg:87.19ms
step:499/1680 train_time:43509ms step_avg:87.19ms
step:500/1680 train_time:43596ms step_avg:87.19ms
step:500/1680 val_loss:3.7220 train_time:43685ms step_avg:87.37ms
step:501/1680 train_time:43704ms step_avg:87.23ms
step:502/1680 train_time:43773ms step_avg:87.20ms
step:503/1680 train_time:43864ms step_avg:87.21ms
step:504/1680 train_time:43956ms step_avg:87.21ms
step:505/1680 train_time:44042ms step_avg:87.21ms
step:506/1680 train_time:44129ms step_avg:87.21ms
step:507/1680 train_time:44215ms step_avg:87.21ms
step:508/1680 train_time:44301ms step_avg:87.21ms
step:509/1680 train_time:44387ms step_avg:87.20ms
step:510/1680 train_time:44474ms step_avg:87.20ms
step:511/1680 train_time:44560ms step_avg:87.20ms
step:512/1680 train_time:44646ms step_avg:87.20ms
step:513/1680 train_time:44735ms step_avg:87.20ms
step:514/1680 train_time:44825ms step_avg:87.21ms
step:515/1680 train_time:44914ms step_avg:87.21ms
step:516/1680 train_time:45002ms step_avg:87.21ms
step:517/1680 train_time:45089ms step_avg:87.21ms
step:518/1680 train_time:45175ms step_avg:87.21ms
step:519/1680 train_time:45262ms step_avg:87.21ms
step:520/1680 train_time:45348ms step_avg:87.21ms
step:521/1680 train_time:45434ms step_avg:87.21ms
step:522/1680 train_time:45520ms step_avg:87.20ms
step:523/1680 train_time:45607ms step_avg:87.20ms
step:524/1680 train_time:45694ms step_avg:87.20ms
step:525/1680 train_time:45782ms step_avg:87.20ms
step:526/1680 train_time:45870ms step_avg:87.21ms
step:527/1680 train_time:45959ms step_avg:87.21ms
step:528/1680 train_time:46047ms step_avg:87.21ms
step:529/1680 train_time:46134ms step_avg:87.21ms
step:530/1680 train_time:46221ms step_avg:87.21ms
step:531/1680 train_time:46308ms step_avg:87.21ms
step:532/1680 train_time:46395ms step_avg:87.21ms
step:533/1680 train_time:46481ms step_avg:87.21ms
step:534/1680 train_time:46568ms step_avg:87.21ms
step:535/1680 train_time:46655ms step_avg:87.20ms
step:536/1680 train_time:46742ms step_avg:87.20ms
step:537/1680 train_time:46829ms step_avg:87.21ms
step:538/1680 train_time:46917ms step_avg:87.21ms
step:539/1680 train_time:47004ms step_avg:87.21ms
step:540/1680 train_time:47092ms step_avg:87.21ms
step:541/1680 train_time:47179ms step_avg:87.21ms
step:542/1680 train_time:47266ms step_avg:87.21ms
step:543/1680 train_time:47352ms step_avg:87.20ms
step:544/1680 train_time:47439ms step_avg:87.20ms
step:545/1680 train_time:47526ms step_avg:87.20ms
step:546/1680 train_time:47613ms step_avg:87.20ms
step:547/1680 train_time:47700ms step_avg:87.20ms
step:548/1680 train_time:47787ms step_avg:87.20ms
step:549/1680 train_time:47876ms step_avg:87.21ms
step:550/1680 train_time:47965ms step_avg:87.21ms
step:551/1680 train_time:48053ms step_avg:87.21ms
step:552/1680 train_time:48141ms step_avg:87.21ms
step:553/1680 train_time:48230ms step_avg:87.22ms
step:554/1680 train_time:48318ms step_avg:87.22ms
step:555/1680 train_time:48406ms step_avg:87.22ms
step:556/1680 train_time:48493ms step_avg:87.22ms
step:557/1680 train_time:48582ms step_avg:87.22ms
step:558/1680 train_time:48669ms step_avg:87.22ms
step:559/1680 train_time:48758ms step_avg:87.22ms
step:560/1680 train_time:48847ms step_avg:87.23ms
step:561/1680 train_time:48935ms step_avg:87.23ms
step:562/1680 train_time:49023ms step_avg:87.23ms
step:563/1680 train_time:49112ms step_avg:87.23ms
step:564/1680 train_time:49201ms step_avg:87.24ms
step:565/1680 train_time:49289ms step_avg:87.24ms
step:566/1680 train_time:49377ms step_avg:87.24ms
step:567/1680 train_time:49466ms step_avg:87.24ms
step:568/1680 train_time:49554ms step_avg:87.24ms
step:569/1680 train_time:49643ms step_avg:87.25ms
step:570/1680 train_time:49731ms step_avg:87.25ms
step:571/1680 train_time:49820ms step_avg:87.25ms
step:572/1680 train_time:49908ms step_avg:87.25ms
step:573/1680 train_time:49996ms step_avg:87.25ms
step:574/1680 train_time:50085ms step_avg:87.26ms
step:575/1680 train_time:50173ms step_avg:87.26ms
step:576/1680 train_time:50262ms step_avg:87.26ms
step:577/1680 train_time:50350ms step_avg:87.26ms
step:578/1680 train_time:50438ms step_avg:87.26ms
step:579/1680 train_time:50526ms step_avg:87.26ms
step:580/1680 train_time:50614ms step_avg:87.27ms
step:581/1680 train_time:50703ms step_avg:87.27ms
step:582/1680 train_time:50791ms step_avg:87.27ms
step:583/1680 train_time:50879ms step_avg:87.27ms
step:584/1680 train_time:50968ms step_avg:87.27ms
step:585/1680 train_time:51057ms step_avg:87.28ms
step:586/1680 train_time:51145ms step_avg:87.28ms
step:587/1680 train_time:51234ms step_avg:87.28ms
step:588/1680 train_time:51322ms step_avg:87.28ms
step:589/1680 train_time:51410ms step_avg:87.28ms
step:590/1680 train_time:51497ms step_avg:87.28ms
step:591/1680 train_time:51586ms step_avg:87.29ms
step:592/1680 train_time:51674ms step_avg:87.29ms
step:593/1680 train_time:51762ms step_avg:87.29ms
step:594/1680 train_time:51850ms step_avg:87.29ms
step:595/1680 train_time:51938ms step_avg:87.29ms
step:596/1680 train_time:52027ms step_avg:87.29ms
step:597/1680 train_time:52116ms step_avg:87.30ms
step:598/1680 train_time:52204ms step_avg:87.30ms
step:599/1680 train_time:52292ms step_avg:87.30ms
step:600/1680 train_time:52381ms step_avg:87.30ms
step:601/1680 train_time:52469ms step_avg:87.30ms
step:602/1680 train_time:52556ms step_avg:87.30ms
step:603/1680 train_time:52644ms step_avg:87.30ms
step:604/1680 train_time:52732ms step_avg:87.30ms
step:605/1680 train_time:52821ms step_avg:87.31ms
step:606/1680 train_time:52909ms step_avg:87.31ms
step:607/1680 train_time:52998ms step_avg:87.31ms
step:608/1680 train_time:53086ms step_avg:87.31ms
step:609/1680 train_time:53174ms step_avg:87.31ms
step:610/1680 train_time:53262ms step_avg:87.32ms
step:611/1680 train_time:53351ms step_avg:87.32ms
step:612/1680 train_time:53440ms step_avg:87.32ms
step:613/1680 train_time:53528ms step_avg:87.32ms
step:614/1680 train_time:53616ms step_avg:87.32ms
step:615/1680 train_time:53704ms step_avg:87.32ms
step:616/1680 train_time:53792ms step_avg:87.32ms
step:617/1680 train_time:53880ms step_avg:87.33ms
step:618/1680 train_time:53968ms step_avg:87.33ms
step:619/1680 train_time:54057ms step_avg:87.33ms
step:620/1680 train_time:54145ms step_avg:87.33ms
step:621/1680 train_time:54233ms step_avg:87.33ms
step:622/1680 train_time:54322ms step_avg:87.33ms
step:623/1680 train_time:54410ms step_avg:87.34ms
step:624/1680 train_time:54498ms step_avg:87.34ms
step:625/1680 train_time:54586ms step_avg:87.34ms
step:625/1680 val_loss:3.6192 train_time:54676ms step_avg:87.48ms
step:626/1680 train_time:54696ms step_avg:87.37ms
step:627/1680 train_time:54767ms step_avg:87.35ms
step:628/1680 train_time:54857ms step_avg:87.35ms
step:629/1680 train_time:54949ms step_avg:87.36ms
step:630/1680 train_time:55037ms step_avg:87.36ms
step:631/1680 train_time:55124ms step_avg:87.36ms
step:632/1680 train_time:55211ms step_avg:87.36ms
step:633/1680 train_time:55298ms step_avg:87.36ms
step:634/1680 train_time:55385ms step_avg:87.36ms
step:635/1680 train_time:55473ms step_avg:87.36ms
step:636/1680 train_time:55561ms step_avg:87.36ms
step:637/1680 train_time:55651ms step_avg:87.36ms
step:638/1680 train_time:55740ms step_avg:87.37ms
step:639/1680 train_time:55831ms step_avg:87.37ms
step:640/1680 train_time:55921ms step_avg:87.38ms
step:641/1680 train_time:56009ms step_avg:87.38ms
step:642/1680 train_time:56096ms step_avg:87.38ms
step:643/1680 train_time:56184ms step_avg:87.38ms
step:644/1680 train_time:56272ms step_avg:87.38ms
step:645/1680 train_time:56359ms step_avg:87.38ms
step:646/1680 train_time:56447ms step_avg:87.38ms
step:647/1680 train_time:56535ms step_avg:87.38ms
step:648/1680 train_time:56624ms step_avg:87.38ms
step:649/1680 train_time:56712ms step_avg:87.38ms
step:650/1680 train_time:56801ms step_avg:87.39ms
step:651/1680 train_time:56890ms step_avg:87.39ms
step:652/1680 train_time:56979ms step_avg:87.39ms
step:653/1680 train_time:57067ms step_avg:87.39ms
step:654/1680 train_time:57156ms step_avg:87.39ms
step:655/1680 train_time:57244ms step_avg:87.40ms
step:656/1680 train_time:57332ms step_avg:87.40ms
step:657/1680 train_time:57419ms step_avg:87.40ms
step:658/1680 train_time:57507ms step_avg:87.40ms
step:659/1680 train_time:57596ms step_avg:87.40ms
step:660/1680 train_time:57685ms step_avg:87.40ms
step:661/1680 train_time:57775ms step_avg:87.41ms
step:662/1680 train_time:57864ms step_avg:87.41ms
step:663/1680 train_time:57953ms step_avg:87.41ms
step:664/1680 train_time:58041ms step_avg:87.41ms
step:665/1680 train_time:58130ms step_avg:87.41ms
step:666/1680 train_time:58218ms step_avg:87.41ms
step:667/1680 train_time:58306ms step_avg:87.41ms
step:668/1680 train_time:58394ms step_avg:87.42ms
step:669/1680 train_time:58481ms step_avg:87.42ms
step:670/1680 train_time:58569ms step_avg:87.42ms
step:671/1680 train_time:58658ms step_avg:87.42ms
step:672/1680 train_time:58746ms step_avg:87.42ms
step:673/1680 train_time:58836ms step_avg:87.42ms
step:674/1680 train_time:58925ms step_avg:87.43ms
step:675/1680 train_time:59014ms step_avg:87.43ms
step:676/1680 train_time:59102ms step_avg:87.43ms
step:677/1680 train_time:59189ms step_avg:87.43ms
step:678/1680 train_time:59277ms step_avg:87.43ms
step:679/1680 train_time:59366ms step_avg:87.43ms
step:680/1680 train_time:59454ms step_avg:87.43ms
step:681/1680 train_time:59541ms step_avg:87.43ms
step:682/1680 train_time:59630ms step_avg:87.43ms
step:683/1680 train_time:59718ms step_avg:87.44ms
step:684/1680 train_time:59807ms step_avg:87.44ms
step:685/1680 train_time:59896ms step_avg:87.44ms
step:686/1680 train_time:59985ms step_avg:87.44ms
step:687/1680 train_time:60073ms step_avg:87.44ms
step:688/1680 train_time:60161ms step_avg:87.44ms
step:689/1680 train_time:60250ms step_avg:87.45ms
step:690/1680 train_time:60338ms step_avg:87.45ms
step:691/1680 train_time:60426ms step_avg:87.45ms
step:692/1680 train_time:60514ms step_avg:87.45ms
step:693/1680 train_time:60602ms step_avg:87.45ms
step:694/1680 train_time:60691ms step_avg:87.45ms
step:695/1680 train_time:60779ms step_avg:87.45ms
step:696/1680 train_time:60868ms step_avg:87.45ms
step:697/1680 train_time:60957ms step_avg:87.46ms
step:698/1680 train_time:61045ms step_avg:87.46ms
step:699/1680 train_time:61133ms step_avg:87.46ms
step:700/1680 train_time:61222ms step_avg:87.46ms
step:701/1680 train_time:61310ms step_avg:87.46ms
step:702/1680 train_time:61398ms step_avg:87.46ms
step:703/1680 train_time:61486ms step_avg:87.46ms
step:704/1680 train_time:61575ms step_avg:87.46ms
step:705/1680 train_time:61663ms step_avg:87.47ms
step:706/1680 train_time:61752ms step_avg:87.47ms
step:707/1680 train_time:61840ms step_avg:87.47ms
step:708/1680 train_time:61929ms step_avg:87.47ms
step:709/1680 train_time:62018ms step_avg:87.47ms
step:710/1680 train_time:62106ms step_avg:87.47ms
step:711/1680 train_time:62194ms step_avg:87.47ms
step:712/1680 train_time:62283ms step_avg:87.48ms
step:713/1680 train_time:62371ms step_avg:87.48ms
step:714/1680 train_time:62459ms step_avg:87.48ms
step:715/1680 train_time:62547ms step_avg:87.48ms
step:716/1680 train_time:62635ms step_avg:87.48ms
step:717/1680 train_time:62724ms step_avg:87.48ms
step:718/1680 train_time:62813ms step_avg:87.48ms
step:719/1680 train_time:62900ms step_avg:87.48ms
step:720/1680 train_time:62988ms step_avg:87.48ms
step:721/1680 train_time:63076ms step_avg:87.48ms
step:722/1680 train_time:63165ms step_avg:87.49ms
step:723/1680 train_time:63254ms step_avg:87.49ms
step:724/1680 train_time:63341ms step_avg:87.49ms
step:725/1680 train_time:63430ms step_avg:87.49ms
step:726/1680 train_time:63518ms step_avg:87.49ms
step:727/1680 train_time:63606ms step_avg:87.49ms
step:728/1680 train_time:63695ms step_avg:87.49ms
step:729/1680 train_time:63784ms step_avg:87.50ms
step:730/1680 train_time:63873ms step_avg:87.50ms
step:731/1680 train_time:63960ms step_avg:87.50ms
step:732/1680 train_time:64049ms step_avg:87.50ms
step:733/1680 train_time:64137ms step_avg:87.50ms
step:734/1680 train_time:64225ms step_avg:87.50ms
step:735/1680 train_time:64314ms step_avg:87.50ms
step:736/1680 train_time:64402ms step_avg:87.50ms
step:737/1680 train_time:64489ms step_avg:87.50ms
step:738/1680 train_time:64578ms step_avg:87.50ms
step:739/1680 train_time:64666ms step_avg:87.51ms
step:740/1680 train_time:64755ms step_avg:87.51ms
step:741/1680 train_time:64844ms step_avg:87.51ms
step:742/1680 train_time:64932ms step_avg:87.51ms
step:743/1680 train_time:65020ms step_avg:87.51ms
step:744/1680 train_time:65109ms step_avg:87.51ms
step:745/1680 train_time:65197ms step_avg:87.51ms
step:746/1680 train_time:65286ms step_avg:87.51ms
step:747/1680 train_time:65374ms step_avg:87.51ms
step:748/1680 train_time:65462ms step_avg:87.52ms
step:749/1680 train_time:65549ms step_avg:87.52ms
step:750/1680 train_time:65637ms step_avg:87.52ms
step:750/1680 val_loss:3.5679 train_time:65727ms step_avg:87.64ms
step:751/1680 train_time:65745ms step_avg:87.54ms
step:752/1680 train_time:65818ms step_avg:87.52ms
step:753/1680 train_time:65914ms step_avg:87.53ms
step:754/1680 train_time:66003ms step_avg:87.54ms
step:755/1680 train_time:66090ms step_avg:87.54ms
step:756/1680 train_time:66178ms step_avg:87.54ms
step:757/1680 train_time:66265ms step_avg:87.54ms
step:758/1680 train_time:66352ms step_avg:87.54ms
step:759/1680 train_time:66439ms step_avg:87.53ms
step:760/1680 train_time:66526ms step_avg:87.53ms
step:761/1680 train_time:66613ms step_avg:87.53ms
step:762/1680 train_time:66703ms step_avg:87.54ms
step:763/1680 train_time:66792ms step_avg:87.54ms
step:764/1680 train_time:66884ms step_avg:87.54ms
step:765/1680 train_time:66974ms step_avg:87.55ms
step:766/1680 train_time:67063ms step_avg:87.55ms
step:767/1680 train_time:67151ms step_avg:87.55ms
step:768/1680 train_time:67239ms step_avg:87.55ms
step:769/1680 train_time:67327ms step_avg:87.55ms
step:770/1680 train_time:67414ms step_avg:87.55ms
step:771/1680 train_time:67502ms step_avg:87.55ms
step:772/1680 train_time:67589ms step_avg:87.55ms
step:773/1680 train_time:67677ms step_avg:87.55ms
step:774/1680 train_time:67766ms step_avg:87.55ms
step:775/1680 train_time:67856ms step_avg:87.56ms
step:776/1680 train_time:67947ms step_avg:87.56ms
step:777/1680 train_time:68035ms step_avg:87.56ms
step:778/1680 train_time:68123ms step_avg:87.56ms
step:779/1680 train_time:68211ms step_avg:87.56ms
step:780/1680 train_time:68299ms step_avg:87.56ms
step:781/1680 train_time:68387ms step_avg:87.56ms
step:782/1680 train_time:68475ms step_avg:87.56ms
step:783/1680 train_time:68562ms step_avg:87.56ms
step:784/1680 train_time:68651ms step_avg:87.56ms
step:785/1680 train_time:68739ms step_avg:87.57ms
step:786/1680 train_time:68827ms step_avg:87.57ms
step:787/1680 train_time:68917ms step_avg:87.57ms
step:788/1680 train_time:69007ms step_avg:87.57ms
step:789/1680 train_time:69096ms step_avg:87.57ms
step:790/1680 train_time:69184ms step_avg:87.57ms
step:791/1680 train_time:69272ms step_avg:87.57ms
step:792/1680 train_time:69359ms step_avg:87.58ms
step:793/1680 train_time:69447ms step_avg:87.58ms
step:794/1680 train_time:69536ms step_avg:87.58ms
step:795/1680 train_time:69625ms step_avg:87.58ms
step:796/1680 train_time:69713ms step_avg:87.58ms
step:797/1680 train_time:69803ms step_avg:87.58ms
step:798/1680 train_time:69891ms step_avg:87.58ms
step:799/1680 train_time:69979ms step_avg:87.58ms
step:800/1680 train_time:70067ms step_avg:87.58ms
step:801/1680 train_time:70155ms step_avg:87.58ms
step:802/1680 train_time:70243ms step_avg:87.59ms
step:803/1680 train_time:70331ms step_avg:87.59ms
step:804/1680 train_time:70418ms step_avg:87.59ms
step:805/1680 train_time:70507ms step_avg:87.59ms
step:806/1680 train_time:70595ms step_avg:87.59ms
step:807/1680 train_time:70683ms step_avg:87.59ms
step:808/1680 train_time:70771ms step_avg:87.59ms
step:809/1680 train_time:70859ms step_avg:87.59ms
step:810/1680 train_time:70947ms step_avg:87.59ms
step:811/1680 train_time:71036ms step_avg:87.59ms
step:812/1680 train_time:71123ms step_avg:87.59ms
step:813/1680 train_time:71212ms step_avg:87.59ms
step:814/1680 train_time:71300ms step_avg:87.59ms
step:815/1680 train_time:71388ms step_avg:87.59ms
step:816/1680 train_time:71476ms step_avg:87.59ms
step:817/1680 train_time:71564ms step_avg:87.59ms
step:818/1680 train_time:71651ms step_avg:87.59ms
step:819/1680 train_time:71741ms step_avg:87.60ms
step:820/1680 train_time:71828ms step_avg:87.60ms
step:821/1680 train_time:71917ms step_avg:87.60ms
step:822/1680 train_time:72006ms step_avg:87.60ms
step:823/1680 train_time:72094ms step_avg:87.60ms
step:824/1680 train_time:72182ms step_avg:87.60ms
step:825/1680 train_time:72270ms step_avg:87.60ms
step:826/1680 train_time:72358ms step_avg:87.60ms
step:827/1680 train_time:72446ms step_avg:87.60ms
step:828/1680 train_time:72534ms step_avg:87.60ms
step:829/1680 train_time:72622ms step_avg:87.60ms
step:830/1680 train_time:72710ms step_avg:87.60ms
step:831/1680 train_time:72798ms step_avg:87.60ms
step:832/1680 train_time:72886ms step_avg:87.60ms
step:833/1680 train_time:72975ms step_avg:87.60ms
step:834/1680 train_time:73063ms step_avg:87.60ms
step:835/1680 train_time:73151ms step_avg:87.61ms
step:836/1680 train_time:73239ms step_avg:87.61ms
step:837/1680 train_time:73327ms step_avg:87.61ms
step:838/1680 train_time:73416ms step_avg:87.61ms
step:839/1680 train_time:73504ms step_avg:87.61ms
step:840/1680 train_time:73592ms step_avg:87.61ms
step:841/1680 train_time:73680ms step_avg:87.61ms
step:842/1680 train_time:73768ms step_avg:87.61ms
step:843/1680 train_time:73857ms step_avg:87.61ms
step:844/1680 train_time:73945ms step_avg:87.61ms
step:845/1680 train_time:74033ms step_avg:87.61ms
step:846/1680 train_time:74121ms step_avg:87.61ms
step:847/1680 train_time:74209ms step_avg:87.61ms
step:848/1680 train_time:74298ms step_avg:87.62ms
step:849/1680 train_time:74386ms step_avg:87.62ms
step:850/1680 train_time:74474ms step_avg:87.62ms
step:851/1680 train_time:74563ms step_avg:87.62ms
step:852/1680 train_time:74652ms step_avg:87.62ms
step:853/1680 train_time:74740ms step_avg:87.62ms
step:854/1680 train_time:74829ms step_avg:87.62ms
step:855/1680 train_time:74917ms step_avg:87.62ms
step:856/1680 train_time:75006ms step_avg:87.62ms
step:857/1680 train_time:75095ms step_avg:87.62ms
step:858/1680 train_time:75183ms step_avg:87.63ms
step:859/1680 train_time:75271ms step_avg:87.63ms
step:860/1680 train_time:75359ms step_avg:87.63ms
step:861/1680 train_time:75447ms step_avg:87.63ms
step:862/1680 train_time:75535ms step_avg:87.63ms
step:863/1680 train_time:75623ms step_avg:87.63ms
step:864/1680 train_time:75711ms step_avg:87.63ms
step:865/1680 train_time:75799ms step_avg:87.63ms
step:866/1680 train_time:75887ms step_avg:87.63ms
step:867/1680 train_time:75976ms step_avg:87.63ms
step:868/1680 train_time:76065ms step_avg:87.63ms
step:869/1680 train_time:76153ms step_avg:87.63ms
step:870/1680 train_time:76242ms step_avg:87.63ms
step:871/1680 train_time:76330ms step_avg:87.63ms
step:872/1680 train_time:76418ms step_avg:87.64ms
step:873/1680 train_time:76506ms step_avg:87.64ms
step:874/1680 train_time:76594ms step_avg:87.64ms
step:875/1680 train_time:76682ms step_avg:87.64ms
step:875/1680 val_loss:3.5211 train_time:76771ms step_avg:87.74ms
step:876/1680 train_time:76790ms step_avg:87.66ms
step:877/1680 train_time:76863ms step_avg:87.64ms
step:878/1680 train_time:76954ms step_avg:87.65ms
step:879/1680 train_time:77043ms step_avg:87.65ms
step:880/1680 train_time:77131ms step_avg:87.65ms
step:881/1680 train_time:77218ms step_avg:87.65ms
step:882/1680 train_time:77306ms step_avg:87.65ms
step:883/1680 train_time:77392ms step_avg:87.65ms
step:884/1680 train_time:77479ms step_avg:87.65ms
step:885/1680 train_time:77567ms step_avg:87.65ms
step:886/1680 train_time:77655ms step_avg:87.65ms
step:887/1680 train_time:77745ms step_avg:87.65ms
step:888/1680 train_time:77836ms step_avg:87.65ms
step:889/1680 train_time:77927ms step_avg:87.66ms
step:890/1680 train_time:78016ms step_avg:87.66ms
step:891/1680 train_time:78105ms step_avg:87.66ms
step:892/1680 train_time:78193ms step_avg:87.66ms
step:893/1680 train_time:78281ms step_avg:87.66ms
step:894/1680 train_time:78368ms step_avg:87.66ms
step:895/1680 train_time:78455ms step_avg:87.66ms
step:896/1680 train_time:78542ms step_avg:87.66ms
step:897/1680 train_time:78630ms step_avg:87.66ms
step:898/1680 train_time:78718ms step_avg:87.66ms
step:899/1680 train_time:78808ms step_avg:87.66ms
step:900/1680 train_time:78898ms step_avg:87.66ms
step:901/1680 train_time:78987ms step_avg:87.67ms
step:902/1680 train_time:79076ms step_avg:87.67ms
step:903/1680 train_time:79165ms step_avg:87.67ms
step:904/1680 train_time:79253ms step_avg:87.67ms
step:905/1680 train_time:79341ms step_avg:87.67ms
step:906/1680 train_time:79429ms step_avg:87.67ms
step:907/1680 train_time:79516ms step_avg:87.67ms
step:908/1680 train_time:79603ms step_avg:87.67ms
step:909/1680 train_time:79692ms step_avg:87.67ms
step:910/1680 train_time:79780ms step_avg:87.67ms
step:911/1680 train_time:79870ms step_avg:87.67ms
step:912/1680 train_time:79959ms step_avg:87.67ms
step:913/1680 train_time:80048ms step_avg:87.68ms
step:914/1680 train_time:80136ms step_avg:87.68ms
step:915/1680 train_time:80225ms step_avg:87.68ms
step:916/1680 train_time:80313ms step_avg:87.68ms
step:917/1680 train_time:80401ms step_avg:87.68ms
step:918/1680 train_time:80488ms step_avg:87.68ms
step:919/1680 train_time:80576ms step_avg:87.68ms
step:920/1680 train_time:80665ms step_avg:87.68ms
step:921/1680 train_time:80753ms step_avg:87.68ms
step:922/1680 train_time:80841ms step_avg:87.68ms
step:923/1680 train_time:80930ms step_avg:87.68ms
step:924/1680 train_time:81019ms step_avg:87.68ms
step:925/1680 train_time:81108ms step_avg:87.68ms
step:926/1680 train_time:81196ms step_avg:87.68ms
step:927/1680 train_time:81284ms step_avg:87.69ms
step:928/1680 train_time:81372ms step_avg:87.69ms
step:929/1680 train_time:81460ms step_avg:87.69ms
step:930/1680 train_time:81548ms step_avg:87.69ms
step:931/1680 train_time:81636ms step_avg:87.69ms
step:932/1680 train_time:81724ms step_avg:87.69ms
step:933/1680 train_time:81812ms step_avg:87.69ms
step:934/1680 train_time:81901ms step_avg:87.69ms
step:935/1680 train_time:81991ms step_avg:87.69ms
step:936/1680 train_time:82079ms step_avg:87.69ms
step:937/1680 train_time:82168ms step_avg:87.69ms
step:938/1680 train_time:82256ms step_avg:87.69ms
step:939/1680 train_time:82345ms step_avg:87.69ms
step:940/1680 train_time:82433ms step_avg:87.69ms
step:941/1680 train_time:82521ms step_avg:87.69ms
step:942/1680 train_time:82609ms step_avg:87.70ms
step:943/1680 train_time:82697ms step_avg:87.70ms
step:944/1680 train_time:82786ms step_avg:87.70ms
step:945/1680 train_time:82875ms step_avg:87.70ms
step:946/1680 train_time:82963ms step_avg:87.70ms
step:947/1680 train_time:83053ms step_avg:87.70ms
step:948/1680 train_time:83141ms step_avg:87.70ms
step:949/1680 train_time:83229ms step_avg:87.70ms
step:950/1680 train_time:83317ms step_avg:87.70ms
step:951/1680 train_time:83405ms step_avg:87.70ms
step:952/1680 train_time:83494ms step_avg:87.70ms
step:953/1680 train_time:83582ms step_avg:87.70ms
step:954/1680 train_time:83670ms step_avg:87.70ms
step:955/1680 train_time:83758ms step_avg:87.70ms
step:956/1680 train_time:83847ms step_avg:87.71ms
step:957/1680 train_time:83935ms step_avg:87.71ms
step:958/1680 train_time:84023ms step_avg:87.71ms
step:959/1680 train_time:84112ms step_avg:87.71ms
step:960/1680 train_time:84200ms step_avg:87.71ms
step:961/1680 train_time:84288ms step_avg:87.71ms
step:962/1680 train_time:84376ms step_avg:87.71ms
step:963/1680 train_time:84465ms step_avg:87.71ms
step:964/1680 train_time:84553ms step_avg:87.71ms
step:965/1680 train_time:84641ms step_avg:87.71ms
step:966/1680 train_time:84730ms step_avg:87.71ms
step:967/1680 train_time:84818ms step_avg:87.71ms
step:968/1680 train_time:84906ms step_avg:87.71ms
step:969/1680 train_time:84995ms step_avg:87.71ms
step:970/1680 train_time:85084ms step_avg:87.72ms
step:971/1680 train_time:85172ms step_avg:87.72ms
step:972/1680 train_time:85260ms step_avg:87.72ms
step:973/1680 train_time:85349ms step_avg:87.72ms
step:974/1680 train_time:85437ms step_avg:87.72ms
step:975/1680 train_time:85524ms step_avg:87.72ms
step:976/1680 train_time:85613ms step_avg:87.72ms
step:977/1680 train_time:85701ms step_avg:87.72ms
step:978/1680 train_time:85789ms step_avg:87.72ms
step:979/1680 train_time:85877ms step_avg:87.72ms
step:980/1680 train_time:85965ms step_avg:87.72ms
step:981/1680 train_time:86055ms step_avg:87.72ms
step:982/1680 train_time:86144ms step_avg:87.72ms
step:983/1680 train_time:86232ms step_avg:87.72ms
step:984/1680 train_time:86319ms step_avg:87.72ms
step:985/1680 train_time:86408ms step_avg:87.72ms
step:986/1680 train_time:86496ms step_avg:87.72ms
step:987/1680 train_time:86585ms step_avg:87.73ms
step:988/1680 train_time:86673ms step_avg:87.73ms
step:989/1680 train_time:86761ms step_avg:87.73ms
step:990/1680 train_time:86850ms step_avg:87.73ms
step:991/1680 train_time:86938ms step_avg:87.73ms
step:992/1680 train_time:87027ms step_avg:87.73ms
step:993/1680 train_time:87115ms step_avg:87.73ms
step:994/1680 train_time:87204ms step_avg:87.73ms
step:995/1680 train_time:87292ms step_avg:87.73ms
step:996/1680 train_time:87380ms step_avg:87.73ms
step:997/1680 train_time:87469ms step_avg:87.73ms
step:998/1680 train_time:87557ms step_avg:87.73ms
step:999/1680 train_time:87645ms step_avg:87.73ms
step:1000/1680 train_time:87733ms step_avg:87.73ms
step:1000/1680 val_loss:3.4714 train_time:87822ms step_avg:87.82ms
step:1001/1680 train_time:87841ms step_avg:87.75ms
step:1002/1680 train_time:87915ms step_avg:87.74ms
step:1003/1680 train_time:88007ms step_avg:87.74ms
step:1004/1680 train_time:88097ms step_avg:87.75ms
step:1005/1680 train_time:88185ms step_avg:87.75ms
step:1006/1680 train_time:88273ms step_avg:87.75ms
step:1007/1680 train_time:88359ms step_avg:87.75ms
step:1008/1680 train_time:88447ms step_avg:87.74ms
step:1009/1680 train_time:88534ms step_avg:87.74ms
step:1010/1680 train_time:88621ms step_avg:87.74ms
step:1011/1680 train_time:88709ms step_avg:87.74ms
step:1012/1680 train_time:88798ms step_avg:87.75ms
step:1013/1680 train_time:88888ms step_avg:87.75ms
step:1014/1680 train_time:88979ms step_avg:87.75ms
step:1015/1680 train_time:89069ms step_avg:87.75ms
step:1016/1680 train_time:89158ms step_avg:87.75ms
step:1017/1680 train_time:89246ms step_avg:87.75ms
step:1018/1680 train_time:89334ms step_avg:87.75ms
step:1019/1680 train_time:89421ms step_avg:87.75ms
step:1020/1680 train_time:89509ms step_avg:87.75ms
step:1021/1680 train_time:89597ms step_avg:87.75ms
step:1022/1680 train_time:89684ms step_avg:87.75ms
step:1023/1680 train_time:89773ms step_avg:87.75ms
step:1024/1680 train_time:89861ms step_avg:87.76ms
step:1025/1680 train_time:89950ms step_avg:87.76ms
step:1026/1680 train_time:90040ms step_avg:87.76ms
step:1027/1680 train_time:90128ms step_avg:87.76ms
step:1028/1680 train_time:90218ms step_avg:87.76ms
step:1029/1680 train_time:90306ms step_avg:87.76ms
step:1030/1680 train_time:90394ms step_avg:87.76ms
step:1031/1680 train_time:90481ms step_avg:87.76ms
step:1032/1680 train_time:90569ms step_avg:87.76ms
step:1033/1680 train_time:90657ms step_avg:87.76ms
step:1034/1680 train_time:90745ms step_avg:87.76ms
step:1035/1680 train_time:90833ms step_avg:87.76ms
step:1036/1680 train_time:90923ms step_avg:87.76ms
step:1037/1680 train_time:91012ms step_avg:87.76ms
step:1038/1680 train_time:91101ms step_avg:87.77ms
step:1039/1680 train_time:91189ms step_avg:87.77ms
step:1040/1680 train_time:91278ms step_avg:87.77ms
step:1041/1680 train_time:91366ms step_avg:87.77ms
step:1042/1680 train_time:91454ms step_avg:87.77ms
step:1043/1680 train_time:91542ms step_avg:87.77ms
step:1044/1680 train_time:91629ms step_avg:87.77ms
step:1045/1680 train_time:91717ms step_avg:87.77ms
step:1046/1680 train_time:91806ms step_avg:87.77ms
step:1047/1680 train_time:91895ms step_avg:87.77ms
step:1048/1680 train_time:91985ms step_avg:87.77ms
step:1049/1680 train_time:92074ms step_avg:87.77ms
step:1050/1680 train_time:92162ms step_avg:87.77ms
step:1051/1680 train_time:92251ms step_avg:87.77ms
step:1052/1680 train_time:92339ms step_avg:87.77ms
step:1053/1680 train_time:92427ms step_avg:87.77ms
step:1054/1680 train_time:92515ms step_avg:87.78ms
step:1055/1680 train_time:92603ms step_avg:87.78ms
step:1056/1680 train_time:92691ms step_avg:87.78ms
step:1057/1680 train_time:92779ms step_avg:87.78ms
step:1058/1680 train_time:92867ms step_avg:87.78ms
step:1059/1680 train_time:92956ms step_avg:87.78ms
step:1060/1680 train_time:93046ms step_avg:87.78ms
step:1061/1680 train_time:93134ms step_avg:87.78ms
step:1062/1680 train_time:93222ms step_avg:87.78ms
step:1063/1680 train_time:93311ms step_avg:87.78ms
step:1064/1680 train_time:93399ms step_avg:87.78ms
step:1065/1680 train_time:93487ms step_avg:87.78ms
step:1066/1680 train_time:93575ms step_avg:87.78ms
step:1067/1680 train_time:93663ms step_avg:87.78ms
step:1068/1680 train_time:93752ms step_avg:87.78ms
step:1069/1680 train_time:93840ms step_avg:87.78ms
step:1070/1680 train_time:93929ms step_avg:87.78ms
step:1071/1680 train_time:94017ms step_avg:87.78ms
step:1072/1680 train_time:94107ms step_avg:87.79ms
step:1073/1680 train_time:94196ms step_avg:87.79ms
step:1074/1680 train_time:94283ms step_avg:87.79ms
step:1075/1680 train_time:94372ms step_avg:87.79ms
step:1076/1680 train_time:94460ms step_avg:87.79ms
step:1077/1680 train_time:94548ms step_avg:87.79ms
step:1078/1680 train_time:94636ms step_avg:87.79ms
step:1079/1680 train_time:94725ms step_avg:87.79ms
step:1080/1680 train_time:94813ms step_avg:87.79ms
step:1081/1680 train_time:94901ms step_avg:87.79ms
step:1082/1680 train_time:94990ms step_avg:87.79ms
step:1083/1680 train_time:95078ms step_avg:87.79ms
step:1084/1680 train_time:95167ms step_avg:87.79ms
step:1085/1680 train_time:95255ms step_avg:87.79ms
step:1086/1680 train_time:95343ms step_avg:87.79ms
step:1087/1680 train_time:95432ms step_avg:87.79ms
step:1088/1680 train_time:95520ms step_avg:87.79ms
step:1089/1680 train_time:95608ms step_avg:87.79ms
step:1090/1680 train_time:95697ms step_avg:87.80ms
step:1091/1680 train_time:95786ms step_avg:87.80ms
step:1092/1680 train_time:95874ms step_avg:87.80ms
step:1093/1680 train_time:95963ms step_avg:87.80ms
step:1094/1680 train_time:96051ms step_avg:87.80ms
step:1095/1680 train_time:96139ms step_avg:87.80ms
step:1096/1680 train_time:96229ms step_avg:87.80ms
step:1097/1680 train_time:96317ms step_avg:87.80ms
step:1098/1680 train_time:96406ms step_avg:87.80ms
step:1099/1680 train_time:96495ms step_avg:87.80ms
step:1100/1680 train_time:96584ms step_avg:87.80ms
step:1101/1680 train_time:96672ms step_avg:87.80ms
step:1102/1680 train_time:96761ms step_avg:87.81ms
step:1103/1680 train_time:96850ms step_avg:87.81ms
step:1104/1680 train_time:96939ms step_avg:87.81ms
step:1105/1680 train_time:97028ms step_avg:87.81ms
step:1106/1680 train_time:97117ms step_avg:87.81ms
step:1107/1680 train_time:97206ms step_avg:87.81ms
step:1108/1680 train_time:97297ms step_avg:87.81ms
step:1109/1680 train_time:97387ms step_avg:87.81ms
step:1110/1680 train_time:97475ms step_avg:87.82ms
step:1111/1680 train_time:97565ms step_avg:87.82ms
step:1112/1680 train_time:97654ms step_avg:87.82ms
step:1113/1680 train_time:97744ms step_avg:87.82ms
step:1114/1680 train_time:97832ms step_avg:87.82ms
step:1115/1680 train_time:97922ms step_avg:87.82ms
step:1116/1680 train_time:98011ms step_avg:87.82ms
step:1117/1680 train_time:98100ms step_avg:87.82ms
step:1118/1680 train_time:98188ms step_avg:87.83ms
step:1119/1680 train_time:98278ms step_avg:87.83ms
step:1120/1680 train_time:98366ms step_avg:87.83ms
step:1121/1680 train_time:98455ms step_avg:87.83ms
step:1122/1680 train_time:98544ms step_avg:87.83ms
step:1123/1680 train_time:98633ms step_avg:87.83ms
step:1124/1680 train_time:98722ms step_avg:87.83ms
step:1125/1680 train_time:98811ms step_avg:87.83ms
step:1125/1680 val_loss:3.4185 train_time:98901ms step_avg:87.91ms
step:1126/1680 train_time:98922ms step_avg:87.85ms
step:1127/1680 train_time:98993ms step_avg:87.84ms
step:1128/1680 train_time:99083ms step_avg:87.84ms
step:1129/1680 train_time:99174ms step_avg:87.84ms
step:1130/1680 train_time:99263ms step_avg:87.84ms
step:1131/1680 train_time:99351ms step_avg:87.84ms
step:1132/1680 train_time:99439ms step_avg:87.84ms
step:1133/1680 train_time:99527ms step_avg:87.84ms
step:1134/1680 train_time:99615ms step_avg:87.84ms
step:1135/1680 train_time:99703ms step_avg:87.84ms
step:1136/1680 train_time:99792ms step_avg:87.85ms
step:1137/1680 train_time:99883ms step_avg:87.85ms
step:1138/1680 train_time:99974ms step_avg:87.85ms
step:1139/1680 train_time:100064ms step_avg:87.85ms
step:1140/1680 train_time:100155ms step_avg:87.86ms
step:1141/1680 train_time:100243ms step_avg:87.86ms
step:1142/1680 train_time:100333ms step_avg:87.86ms
step:1143/1680 train_time:100422ms step_avg:87.86ms
step:1144/1680 train_time:100510ms step_avg:87.86ms
step:1145/1680 train_time:100598ms step_avg:87.86ms
step:1146/1680 train_time:100686ms step_avg:87.86ms
step:1147/1680 train_time:100775ms step_avg:87.86ms
step:1148/1680 train_time:100863ms step_avg:87.86ms
step:1149/1680 train_time:100953ms step_avg:87.86ms
step:1150/1680 train_time:101043ms step_avg:87.86ms
step:1151/1680 train_time:101132ms step_avg:87.86ms
step:1152/1680 train_time:101221ms step_avg:87.87ms
step:1153/1680 train_time:101311ms step_avg:87.87ms
step:1154/1680 train_time:101401ms step_avg:87.87ms
step:1155/1680 train_time:101491ms step_avg:87.87ms
step:1156/1680 train_time:101579ms step_avg:87.87ms
step:1157/1680 train_time:101667ms step_avg:87.87ms
step:1158/1680 train_time:101756ms step_avg:87.87ms
step:1159/1680 train_time:101845ms step_avg:87.87ms
step:1160/1680 train_time:101933ms step_avg:87.87ms
step:1161/1680 train_time:102024ms step_avg:87.88ms
step:1162/1680 train_time:102113ms step_avg:87.88ms
step:1163/1680 train_time:102202ms step_avg:87.88ms
step:1164/1680 train_time:102292ms step_avg:87.88ms
step:1165/1680 train_time:102380ms step_avg:87.88ms
step:1166/1680 train_time:102470ms step_avg:87.88ms
step:1167/1680 train_time:102559ms step_avg:87.88ms
step:1168/1680 train_time:102648ms step_avg:87.88ms
step:1169/1680 train_time:102737ms step_avg:87.88ms
step:1170/1680 train_time:102825ms step_avg:87.88ms
step:1171/1680 train_time:102914ms step_avg:87.89ms
step:1172/1680 train_time:103002ms step_avg:87.89ms
step:1173/1680 train_time:103091ms step_avg:87.89ms
step:1174/1680 train_time:103181ms step_avg:87.89ms
step:1175/1680 train_time:103270ms step_avg:87.89ms
step:1176/1680 train_time:103359ms step_avg:87.89ms
step:1177/1680 train_time:103448ms step_avg:87.89ms
step:1178/1680 train_time:103538ms step_avg:87.89ms
step:1179/1680 train_time:103626ms step_avg:87.89ms
step:1180/1680 train_time:103715ms step_avg:87.89ms
step:1181/1680 train_time:103804ms step_avg:87.89ms
step:1182/1680 train_time:103893ms step_avg:87.90ms
step:1183/1680 train_time:103982ms step_avg:87.90ms
step:1184/1680 train_time:104072ms step_avg:87.90ms
step:1185/1680 train_time:104161ms step_avg:87.90ms
step:1186/1680 train_time:104250ms step_avg:87.90ms
step:1187/1680 train_time:104339ms step_avg:87.90ms
step:1188/1680 train_time:104428ms step_avg:87.90ms
step:1189/1680 train_time:104516ms step_avg:87.90ms
step:1190/1680 train_time:104605ms step_avg:87.90ms
step:1191/1680 train_time:104694ms step_avg:87.90ms
step:1192/1680 train_time:104783ms step_avg:87.91ms
step:1193/1680 train_time:104873ms step_avg:87.91ms
step:1194/1680 train_time:104962ms step_avg:87.91ms
step:1195/1680 train_time:105051ms step_avg:87.91ms
step:1196/1680 train_time:105139ms step_avg:87.91ms
step:1197/1680 train_time:105228ms step_avg:87.91ms
step:1198/1680 train_time:105317ms step_avg:87.91ms
step:1199/1680 train_time:105407ms step_avg:87.91ms
step:1200/1680 train_time:105495ms step_avg:87.91ms
step:1201/1680 train_time:105584ms step_avg:87.91ms
step:1202/1680 train_time:105673ms step_avg:87.91ms
step:1203/1680 train_time:105761ms step_avg:87.91ms
step:1204/1680 train_time:105850ms step_avg:87.92ms
step:1205/1680 train_time:105939ms step_avg:87.92ms
step:1206/1680 train_time:106028ms step_avg:87.92ms
step:1207/1680 train_time:106117ms step_avg:87.92ms
step:1208/1680 train_time:106206ms step_avg:87.92ms
step:1209/1680 train_time:106295ms step_avg:87.92ms
step:1210/1680 train_time:106384ms step_avg:87.92ms
step:1211/1680 train_time:106472ms step_avg:87.92ms
step:1212/1680 train_time:106562ms step_avg:87.92ms
step:1213/1680 train_time:106650ms step_avg:87.92ms
step:1214/1680 train_time:106739ms step_avg:87.92ms
step:1215/1680 train_time:106828ms step_avg:87.92ms
step:1216/1680 train_time:106917ms step_avg:87.93ms
step:1217/1680 train_time:107006ms step_avg:87.93ms
step:1218/1680 train_time:107096ms step_avg:87.93ms
step:1219/1680 train_time:107185ms step_avg:87.93ms
step:1220/1680 train_time:107274ms step_avg:87.93ms
step:1221/1680 train_time:107363ms step_avg:87.93ms
step:1222/1680 train_time:107452ms step_avg:87.93ms
step:1223/1680 train_time:107541ms step_avg:87.93ms
step:1224/1680 train_time:107630ms step_avg:87.93ms
step:1225/1680 train_time:107719ms step_avg:87.93ms
step:1226/1680 train_time:107807ms step_avg:87.93ms
step:1227/1680 train_time:107896ms step_avg:87.93ms
step:1228/1680 train_time:107985ms step_avg:87.94ms
step:1229/1680 train_time:108074ms step_avg:87.94ms
step:1230/1680 train_time:108164ms step_avg:87.94ms
step:1231/1680 train_time:108253ms step_avg:87.94ms
step:1232/1680 train_time:108342ms step_avg:87.94ms
step:1233/1680 train_time:108431ms step_avg:87.94ms
step:1234/1680 train_time:108520ms step_avg:87.94ms
step:1235/1680 train_time:108610ms step_avg:87.94ms
step:1236/1680 train_time:108699ms step_avg:87.94ms
step:1237/1680 train_time:108787ms step_avg:87.94ms
step:1238/1680 train_time:108877ms step_avg:87.95ms
step:1239/1680 train_time:108966ms step_avg:87.95ms
step:1240/1680 train_time:109055ms step_avg:87.95ms
step:1241/1680 train_time:109144ms step_avg:87.95ms
step:1242/1680 train_time:109234ms step_avg:87.95ms
step:1243/1680 train_time:109322ms step_avg:87.95ms
step:1244/1680 train_time:109412ms step_avg:87.95ms
step:1245/1680 train_time:109500ms step_avg:87.95ms
step:1246/1680 train_time:109589ms step_avg:87.95ms
step:1247/1680 train_time:109678ms step_avg:87.95ms
step:1248/1680 train_time:109767ms step_avg:87.95ms
step:1249/1680 train_time:109856ms step_avg:87.96ms
step:1250/1680 train_time:109945ms step_avg:87.96ms
step:1250/1680 val_loss:3.3794 train_time:110036ms step_avg:88.03ms
step:1251/1680 train_time:110054ms step_avg:87.97ms
step:1252/1680 train_time:110128ms step_avg:87.96ms
step:1253/1680 train_time:110219ms step_avg:87.96ms
step:1254/1680 train_time:110308ms step_avg:87.96ms
step:1255/1680 train_time:110397ms step_avg:87.97ms
step:1256/1680 train_time:110485ms step_avg:87.97ms
step:1257/1680 train_time:110573ms step_avg:87.97ms
step:1258/1680 train_time:110661ms step_avg:87.97ms
step:1259/1680 train_time:110750ms step_avg:87.97ms
step:1260/1680 train_time:110838ms step_avg:87.97ms
step:1261/1680 train_time:110927ms step_avg:87.97ms
step:1262/1680 train_time:111017ms step_avg:87.97ms
step:1263/1680 train_time:111109ms step_avg:87.97ms
step:1264/1680 train_time:111198ms step_avg:87.97ms
step:1265/1680 train_time:111288ms step_avg:87.97ms
step:1266/1680 train_time:111377ms step_avg:87.98ms
step:1267/1680 train_time:111466ms step_avg:87.98ms
step:1268/1680 train_time:111554ms step_avg:87.98ms
step:1269/1680 train_time:111642ms step_avg:87.98ms
step:1270/1680 train_time:111730ms step_avg:87.98ms
step:1271/1680 train_time:111819ms step_avg:87.98ms
step:1272/1680 train_time:111908ms step_avg:87.98ms
step:1273/1680 train_time:112000ms step_avg:87.98ms
step:1274/1680 train_time:112089ms step_avg:87.98ms
step:1275/1680 train_time:112179ms step_avg:87.98ms
step:1276/1680 train_time:112269ms step_avg:87.99ms
step:1277/1680 train_time:112359ms step_avg:87.99ms
step:1278/1680 train_time:112448ms step_avg:87.99ms
step:1279/1680 train_time:112536ms step_avg:87.99ms
step:1280/1680 train_time:112625ms step_avg:87.99ms
step:1281/1680 train_time:112713ms step_avg:87.99ms
step:1282/1680 train_time:112802ms step_avg:87.99ms
step:1283/1680 train_time:112891ms step_avg:87.99ms
step:1284/1680 train_time:112981ms step_avg:87.99ms
step:1285/1680 train_time:113072ms step_avg:87.99ms
step:1286/1680 train_time:113161ms step_avg:87.99ms
step:1287/1680 train_time:113251ms step_avg:88.00ms
step:1288/1680 train_time:113340ms step_avg:88.00ms
step:1289/1680 train_time:113429ms step_avg:88.00ms
step:1290/1680 train_time:113519ms step_avg:88.00ms
step:1291/1680 train_time:113608ms step_avg:88.00ms
step:1292/1680 train_time:113696ms step_avg:88.00ms
step:1293/1680 train_time:113785ms step_avg:88.00ms
step:1294/1680 train_time:113874ms step_avg:88.00ms
step:1295/1680 train_time:113963ms step_avg:88.00ms
step:1296/1680 train_time:114053ms step_avg:88.00ms
step:1297/1680 train_time:114143ms step_avg:88.01ms
step:1298/1680 train_time:114233ms step_avg:88.01ms
step:1299/1680 train_time:114322ms step_avg:88.01ms
step:1300/1680 train_time:114412ms step_avg:88.01ms
step:1301/1680 train_time:114501ms step_avg:88.01ms
step:1302/1680 train_time:114589ms step_avg:88.01ms
step:1303/1680 train_time:114678ms step_avg:88.01ms
step:1304/1680 train_time:114767ms step_avg:88.01ms
step:1305/1680 train_time:114855ms step_avg:88.01ms
step:1306/1680 train_time:114944ms step_avg:88.01ms
step:1307/1680 train_time:115033ms step_avg:88.01ms
step:1308/1680 train_time:115122ms step_avg:88.01ms
step:1309/1680 train_time:115211ms step_avg:88.01ms
step:1310/1680 train_time:115301ms step_avg:88.02ms
step:1311/1680 train_time:115390ms step_avg:88.02ms
step:1312/1680 train_time:115479ms step_avg:88.02ms
step:1313/1680 train_time:115568ms step_avg:88.02ms
step:1314/1680 train_time:115657ms step_avg:88.02ms
step:1315/1680 train_time:115746ms step_avg:88.02ms
step:1316/1680 train_time:115835ms step_avg:88.02ms
step:1317/1680 train_time:115924ms step_avg:88.02ms
step:1318/1680 train_time:116013ms step_avg:88.02ms
step:1319/1680 train_time:116102ms step_avg:88.02ms
step:1320/1680 train_time:116192ms step_avg:88.02ms
step:1321/1680 train_time:116281ms step_avg:88.02ms
step:1322/1680 train_time:116370ms step_avg:88.03ms
step:1323/1680 train_time:116459ms step_avg:88.03ms
step:1324/1680 train_time:116548ms step_avg:88.03ms
step:1325/1680 train_time:116637ms step_avg:88.03ms
step:1326/1680 train_time:116725ms step_avg:88.03ms
step:1327/1680 train_time:116815ms step_avg:88.03ms
step:1328/1680 train_time:116904ms step_avg:88.03ms
step:1329/1680 train_time:116993ms step_avg:88.03ms
step:1330/1680 train_time:117082ms step_avg:88.03ms
step:1331/1680 train_time:117172ms step_avg:88.03ms
step:1332/1680 train_time:117262ms step_avg:88.03ms
step:1333/1680 train_time:117351ms step_avg:88.04ms
step:1334/1680 train_time:117440ms step_avg:88.04ms
step:1335/1680 train_time:117529ms step_avg:88.04ms
step:1336/1680 train_time:117619ms step_avg:88.04ms
step:1337/1680 train_time:117709ms step_avg:88.04ms
step:1338/1680 train_time:117799ms step_avg:88.04ms
step:1339/1680 train_time:117888ms step_avg:88.04ms
step:1340/1680 train_time:117977ms step_avg:88.04ms
step:1341/1680 train_time:118065ms step_avg:88.04ms
step:1342/1680 train_time:118154ms step_avg:88.04ms
step:1343/1680 train_time:118243ms step_avg:88.04ms
step:1344/1680 train_time:118333ms step_avg:88.05ms
step:1345/1680 train_time:118423ms step_avg:88.05ms
step:1346/1680 train_time:118513ms step_avg:88.05ms
step:1347/1680 train_time:118602ms step_avg:88.05ms
step:1348/1680 train_time:118692ms step_avg:88.05ms
step:1349/1680 train_time:118782ms step_avg:88.05ms
step:1350/1680 train_time:118871ms step_avg:88.05ms
step:1351/1680 train_time:118961ms step_avg:88.05ms
step:1352/1680 train_time:119051ms step_avg:88.06ms
step:1353/1680 train_time:119141ms step_avg:88.06ms
step:1354/1680 train_time:119229ms step_avg:88.06ms
step:1355/1680 train_time:119319ms step_avg:88.06ms
step:1356/1680 train_time:119408ms step_avg:88.06ms
step:1357/1680 train_time:119497ms step_avg:88.06ms
step:1358/1680 train_time:119586ms step_avg:88.06ms
step:1359/1680 train_time:119674ms step_avg:88.06ms
step:1360/1680 train_time:119763ms step_avg:88.06ms
step:1361/1680 train_time:119852ms step_avg:88.06ms
step:1362/1680 train_time:119941ms step_avg:88.06ms
step:1363/1680 train_time:120030ms step_avg:88.06ms
step:1364/1680 train_time:120119ms step_avg:88.06ms
step:1365/1680 train_time:120208ms step_avg:88.06ms
step:1366/1680 train_time:120299ms step_avg:88.07ms
step:1367/1680 train_time:120387ms step_avg:88.07ms
step:1368/1680 train_time:120477ms step_avg:88.07ms
step:1369/1680 train_time:120565ms step_avg:88.07ms
step:1370/1680 train_time:120654ms step_avg:88.07ms
step:1371/1680 train_time:120743ms step_avg:88.07ms
step:1372/1680 train_time:120832ms step_avg:88.07ms
step:1373/1680 train_time:120921ms step_avg:88.07ms
step:1374/1680 train_time:121011ms step_avg:88.07ms
step:1375/1680 train_time:121101ms step_avg:88.07ms
step:1375/1680 val_loss:3.3446 train_time:121191ms step_avg:88.14ms
step:1376/1680 train_time:121209ms step_avg:88.09ms
step:1377/1680 train_time:121282ms step_avg:88.08ms
step:1378/1680 train_time:121377ms step_avg:88.08ms
step:1379/1680 train_time:121467ms step_avg:88.08ms
step:1380/1680 train_time:121555ms step_avg:88.08ms
step:1381/1680 train_time:121643ms step_avg:88.08ms
step:1382/1680 train_time:121730ms step_avg:88.08ms
step:1383/1680 train_time:121818ms step_avg:88.08ms
step:1384/1680 train_time:121905ms step_avg:88.08ms
step:1385/1680 train_time:121993ms step_avg:88.08ms
step:1386/1680 train_time:122081ms step_avg:88.08ms
step:1387/1680 train_time:122171ms step_avg:88.08ms
step:1388/1680 train_time:122263ms step_avg:88.09ms
step:1389/1680 train_time:122355ms step_avg:88.09ms
step:1390/1680 train_time:122446ms step_avg:88.09ms
step:1391/1680 train_time:122536ms step_avg:88.09ms
step:1392/1680 train_time:122624ms step_avg:88.09ms
step:1393/1680 train_time:122712ms step_avg:88.09ms
step:1394/1680 train_time:122800ms step_avg:88.09ms
step:1395/1680 train_time:122889ms step_avg:88.09ms
step:1396/1680 train_time:122976ms step_avg:88.09ms
step:1397/1680 train_time:123065ms step_avg:88.09ms
step:1398/1680 train_time:123155ms step_avg:88.09ms
step:1399/1680 train_time:123246ms step_avg:88.10ms
step:1400/1680 train_time:123336ms step_avg:88.10ms
step:1401/1680 train_time:123425ms step_avg:88.10ms
step:1402/1680 train_time:123514ms step_avg:88.10ms
step:1403/1680 train_time:123603ms step_avg:88.10ms
step:1404/1680 train_time:123692ms step_avg:88.10ms
step:1405/1680 train_time:123781ms step_avg:88.10ms
step:1406/1680 train_time:123869ms step_avg:88.10ms
step:1407/1680 train_time:123957ms step_avg:88.10ms
step:1408/1680 train_time:124046ms step_avg:88.10ms
step:1409/1680 train_time:124135ms step_avg:88.10ms
step:1410/1680 train_time:124225ms step_avg:88.10ms
step:1411/1680 train_time:124315ms step_avg:88.10ms
step:1412/1680 train_time:124405ms step_avg:88.11ms
step:1413/1680 train_time:124495ms step_avg:88.11ms
step:1414/1680 train_time:124584ms step_avg:88.11ms
step:1415/1680 train_time:124674ms step_avg:88.11ms
step:1416/1680 train_time:124763ms step_avg:88.11ms
step:1417/1680 train_time:124852ms step_avg:88.11ms
step:1418/1680 train_time:124941ms step_avg:88.11ms
step:1419/1680 train_time:125029ms step_avg:88.11ms
step:1420/1680 train_time:125118ms step_avg:88.11ms
step:1421/1680 train_time:125207ms step_avg:88.11ms
step:1422/1680 train_time:125297ms step_avg:88.11ms
step:1423/1680 train_time:125386ms step_avg:88.11ms
step:1424/1680 train_time:125476ms step_avg:88.11ms
step:1425/1680 train_time:125565ms step_avg:88.12ms
step:1426/1680 train_time:125654ms step_avg:88.12ms
step:1427/1680 train_time:125743ms step_avg:88.12ms
step:1428/1680 train_time:125832ms step_avg:88.12ms
step:1429/1680 train_time:125921ms step_avg:88.12ms
step:1430/1680 train_time:126009ms step_avg:88.12ms
step:1431/1680 train_time:126099ms step_avg:88.12ms
step:1432/1680 train_time:126188ms step_avg:88.12ms
step:1433/1680 train_time:126277ms step_avg:88.12ms
step:1434/1680 train_time:126366ms step_avg:88.12ms
step:1435/1680 train_time:126455ms step_avg:88.12ms
step:1436/1680 train_time:126545ms step_avg:88.12ms
step:1437/1680 train_time:126634ms step_avg:88.12ms
step:1438/1680 train_time:126723ms step_avg:88.12ms
step:1439/1680 train_time:126812ms step_avg:88.13ms
step:1440/1680 train_time:126901ms step_avg:88.13ms
step:1441/1680 train_time:126990ms step_avg:88.13ms
step:1442/1680 train_time:127078ms step_avg:88.13ms
step:1443/1680 train_time:127167ms step_avg:88.13ms
step:1444/1680 train_time:127256ms step_avg:88.13ms
step:1445/1680 train_time:127346ms step_avg:88.13ms
step:1446/1680 train_time:127435ms step_avg:88.13ms
step:1447/1680 train_time:127524ms step_avg:88.13ms
step:1448/1680 train_time:127614ms step_avg:88.13ms
step:1449/1680 train_time:127704ms step_avg:88.13ms
step:1450/1680 train_time:127793ms step_avg:88.13ms
step:1451/1680 train_time:127882ms step_avg:88.13ms
step:1452/1680 train_time:127970ms step_avg:88.13ms
step:1453/1680 train_time:128059ms step_avg:88.13ms
step:1454/1680 train_time:128148ms step_avg:88.14ms
step:1455/1680 train_time:128237ms step_avg:88.14ms
step:1456/1680 train_time:128327ms step_avg:88.14ms
step:1457/1680 train_time:128415ms step_avg:88.14ms
step:1458/1680 train_time:128505ms step_avg:88.14ms
step:1459/1680 train_time:128595ms step_avg:88.14ms
step:1460/1680 train_time:128684ms step_avg:88.14ms
step:1461/1680 train_time:128774ms step_avg:88.14ms
step:1462/1680 train_time:128863ms step_avg:88.14ms
step:1463/1680 train_time:128952ms step_avg:88.14ms
step:1464/1680 train_time:129041ms step_avg:88.14ms
step:1465/1680 train_time:129129ms step_avg:88.14ms
step:1466/1680 train_time:129218ms step_avg:88.14ms
step:1467/1680 train_time:129307ms step_avg:88.14ms
step:1468/1680 train_time:129396ms step_avg:88.14ms
step:1469/1680 train_time:129486ms step_avg:88.15ms
step:1470/1680 train_time:129574ms step_avg:88.15ms
step:1471/1680 train_time:129664ms step_avg:88.15ms
step:1472/1680 train_time:129753ms step_avg:88.15ms
step:1473/1680 train_time:129843ms step_avg:88.15ms
step:1474/1680 train_time:129932ms step_avg:88.15ms
step:1475/1680 train_time:130021ms step_avg:88.15ms
step:1476/1680 train_time:130109ms step_avg:88.15ms
step:1477/1680 train_time:130198ms step_avg:88.15ms
step:1478/1680 train_time:130287ms step_avg:88.15ms
step:1479/1680 train_time:130377ms step_avg:88.15ms
step:1480/1680 train_time:130467ms step_avg:88.15ms
step:1481/1680 train_time:130556ms step_avg:88.15ms
step:1482/1680 train_time:130645ms step_avg:88.15ms
step:1483/1680 train_time:130735ms step_avg:88.16ms
step:1484/1680 train_time:130824ms step_avg:88.16ms
step:1485/1680 train_time:130914ms step_avg:88.16ms
step:1486/1680 train_time:131002ms step_avg:88.16ms
step:1487/1680 train_time:131091ms step_avg:88.16ms
step:1488/1680 train_time:131180ms step_avg:88.16ms
step:1489/1680 train_time:131269ms step_avg:88.16ms
step:1490/1680 train_time:131358ms step_avg:88.16ms
step:1491/1680 train_time:131447ms step_avg:88.16ms
step:1492/1680 train_time:131537ms step_avg:88.16ms
step:1493/1680 train_time:131626ms step_avg:88.16ms
step:1494/1680 train_time:131714ms step_avg:88.16ms
step:1495/1680 train_time:131803ms step_avg:88.16ms
step:1496/1680 train_time:131892ms step_avg:88.16ms
step:1497/1680 train_time:131981ms step_avg:88.16ms
step:1498/1680 train_time:132070ms step_avg:88.16ms
step:1499/1680 train_time:132158ms step_avg:88.16ms
step:1500/1680 train_time:132248ms step_avg:88.17ms
step:1500/1680 val_loss:3.3149 train_time:132337ms step_avg:88.22ms
step:1501/1680 train_time:132357ms step_avg:88.18ms
step:1502/1680 train_time:132432ms step_avg:88.17ms
step:1503/1680 train_time:132525ms step_avg:88.17ms
step:1504/1680 train_time:132615ms step_avg:88.17ms
step:1505/1680 train_time:132703ms step_avg:88.17ms
step:1506/1680 train_time:132791ms step_avg:88.17ms
step:1507/1680 train_time:132879ms step_avg:88.17ms
step:1508/1680 train_time:132967ms step_avg:88.17ms
step:1509/1680 train_time:133054ms step_avg:88.17ms
step:1510/1680 train_time:133143ms step_avg:88.17ms
step:1511/1680 train_time:133231ms step_avg:88.17ms
step:1512/1680 train_time:133321ms step_avg:88.18ms
step:1513/1680 train_time:133413ms step_avg:88.18ms
step:1514/1680 train_time:133504ms step_avg:88.18ms
step:1515/1680 train_time:133595ms step_avg:88.18ms
step:1516/1680 train_time:133685ms step_avg:88.18ms
step:1517/1680 train_time:133774ms step_avg:88.18ms
step:1518/1680 train_time:133862ms step_avg:88.18ms
step:1519/1680 train_time:133950ms step_avg:88.18ms
step:1520/1680 train_time:134039ms step_avg:88.18ms
step:1521/1680 train_time:134127ms step_avg:88.18ms
step:1522/1680 train_time:134216ms step_avg:88.18ms
step:1523/1680 train_time:134305ms step_avg:88.18ms
step:1524/1680 train_time:134395ms step_avg:88.19ms
step:1525/1680 train_time:134486ms step_avg:88.19ms
step:1526/1680 train_time:134576ms step_avg:88.19ms
step:1527/1680 train_time:134666ms step_avg:88.19ms
step:1528/1680 train_time:134756ms step_avg:88.19ms
step:1529/1680 train_time:134843ms step_avg:88.19ms
step:1530/1680 train_time:134932ms step_avg:88.19ms
step:1531/1680 train_time:135020ms step_avg:88.19ms
step:1532/1680 train_time:135108ms step_avg:88.19ms
step:1533/1680 train_time:135196ms step_avg:88.19ms
step:1534/1680 train_time:135286ms step_avg:88.19ms
step:1535/1680 train_time:135375ms step_avg:88.19ms
step:1536/1680 train_time:135465ms step_avg:88.19ms
step:1537/1680 train_time:135555ms step_avg:88.19ms
step:1538/1680 train_time:135644ms step_avg:88.19ms
step:1539/1680 train_time:135734ms step_avg:88.20ms
step:1540/1680 train_time:135823ms step_avg:88.20ms
step:1541/1680 train_time:135911ms step_avg:88.20ms
step:1542/1680 train_time:136000ms step_avg:88.20ms
step:1543/1680 train_time:136088ms step_avg:88.20ms
step:1544/1680 train_time:136176ms step_avg:88.20ms
step:1545/1680 train_time:136266ms step_avg:88.20ms
step:1546/1680 train_time:136357ms step_avg:88.20ms
step:1547/1680 train_time:136446ms step_avg:88.20ms
step:1548/1680 train_time:136536ms step_avg:88.20ms
step:1549/1680 train_time:136625ms step_avg:88.20ms
step:1550/1680 train_time:136715ms step_avg:88.20ms
step:1551/1680 train_time:136804ms step_avg:88.20ms
step:1552/1680 train_time:136893ms step_avg:88.20ms
step:1553/1680 train_time:136981ms step_avg:88.20ms
step:1554/1680 train_time:137070ms step_avg:88.20ms
step:1555/1680 train_time:137158ms step_avg:88.20ms
step:1556/1680 train_time:137248ms step_avg:88.21ms
step:1557/1680 train_time:137337ms step_avg:88.21ms
step:1558/1680 train_time:137426ms step_avg:88.21ms
step:1559/1680 train_time:137516ms step_avg:88.21ms
step:1560/1680 train_time:137606ms step_avg:88.21ms
step:1561/1680 train_time:137695ms step_avg:88.21ms
step:1562/1680 train_time:137785ms step_avg:88.21ms
step:1563/1680 train_time:137874ms step_avg:88.21ms
step:1564/1680 train_time:137962ms step_avg:88.21ms
step:1565/1680 train_time:138051ms step_avg:88.21ms
step:1566/1680 train_time:138140ms step_avg:88.21ms
step:1567/1680 train_time:138229ms step_avg:88.21ms
step:1568/1680 train_time:138319ms step_avg:88.21ms
step:1569/1680 train_time:138408ms step_avg:88.21ms
step:1570/1680 train_time:138497ms step_avg:88.21ms
step:1571/1680 train_time:138586ms step_avg:88.22ms
step:1572/1680 train_time:138675ms step_avg:88.22ms
step:1573/1680 train_time:138763ms step_avg:88.22ms
step:1574/1680 train_time:138853ms step_avg:88.22ms
step:1575/1680 train_time:138942ms step_avg:88.22ms
step:1576/1680 train_time:139031ms step_avg:88.22ms
step:1577/1680 train_time:139120ms step_avg:88.22ms
step:1578/1680 train_time:139208ms step_avg:88.22ms
step:1579/1680 train_time:139298ms step_avg:88.22ms
step:1580/1680 train_time:139387ms step_avg:88.22ms
step:1581/1680 train_time:139477ms step_avg:88.22ms
step:1582/1680 train_time:139566ms step_avg:88.22ms
step:1583/1680 train_time:139655ms step_avg:88.22ms
step:1584/1680 train_time:139744ms step_avg:88.22ms
step:1585/1680 train_time:139834ms step_avg:88.22ms
step:1586/1680 train_time:139923ms step_avg:88.22ms
step:1587/1680 train_time:140012ms step_avg:88.22ms
step:1588/1680 train_time:140101ms step_avg:88.22ms
step:1589/1680 train_time:140189ms step_avg:88.22ms
step:1590/1680 train_time:140278ms step_avg:88.23ms
step:1591/1680 train_time:140367ms step_avg:88.23ms
step:1592/1680 train_time:140457ms step_avg:88.23ms
step:1593/1680 train_time:140546ms step_avg:88.23ms
step:1594/1680 train_time:140636ms step_avg:88.23ms
step:1595/1680 train_time:140725ms step_avg:88.23ms
step:1596/1680 train_time:140813ms step_avg:88.23ms
step:1597/1680 train_time:140903ms step_avg:88.23ms
step:1598/1680 train_time:140992ms step_avg:88.23ms
step:1599/1680 train_time:141081ms step_avg:88.23ms
step:1600/1680 train_time:141170ms step_avg:88.23ms
step:1601/1680 train_time:141259ms step_avg:88.23ms
step:1602/1680 train_time:141348ms step_avg:88.23ms
step:1603/1680 train_time:141437ms step_avg:88.23ms
step:1604/1680 train_time:141527ms step_avg:88.23ms
step:1605/1680 train_time:141616ms step_avg:88.23ms
step:1606/1680 train_time:141704ms step_avg:88.23ms
step:1607/1680 train_time:141793ms step_avg:88.23ms
step:1608/1680 train_time:141883ms step_avg:88.24ms
step:1609/1680 train_time:141973ms step_avg:88.24ms
step:1610/1680 train_time:142061ms step_avg:88.24ms
step:1611/1680 train_time:142150ms step_avg:88.24ms
step:1612/1680 train_time:142240ms step_avg:88.24ms
step:1613/1680 train_time:142328ms step_avg:88.24ms
step:1614/1680 train_time:142418ms step_avg:88.24ms
step:1615/1680 train_time:142507ms step_avg:88.24ms
step:1616/1680 train_time:142596ms step_avg:88.24ms
step:1617/1680 train_time:142685ms step_avg:88.24ms
step:1618/1680 train_time:142774ms step_avg:88.24ms
step:1619/1680 train_time:142863ms step_avg:88.24ms
step:1620/1680 train_time:142952ms step_avg:88.24ms
step:1621/1680 train_time:143042ms step_avg:88.24ms
step:1622/1680 train_time:143131ms step_avg:88.24ms
step:1623/1680 train_time:143220ms step_avg:88.24ms
step:1624/1680 train_time:143309ms step_avg:88.24ms
step:1625/1680 train_time:143399ms step_avg:88.25ms
step:1625/1680 val_loss:3.2908 train_time:143489ms step_avg:88.30ms
step:1626/1680 train_time:143507ms step_avg:88.26ms
step:1627/1680 train_time:143581ms step_avg:88.25ms
step:1628/1680 train_time:143673ms step_avg:88.25ms
step:1629/1680 train_time:143763ms step_avg:88.25ms
step:1630/1680 train_time:143852ms step_avg:88.25ms
step:1631/1680 train_time:143942ms step_avg:88.25ms
step:1632/1680 train_time:144030ms step_avg:88.25ms
step:1633/1680 train_time:144118ms step_avg:88.25ms
step:1634/1680 train_time:144206ms step_avg:88.25ms
step:1635/1680 train_time:144294ms step_avg:88.25ms
step:1636/1680 train_time:144382ms step_avg:88.25ms
step:1637/1680 train_time:144472ms step_avg:88.25ms
step:1638/1680 train_time:144564ms step_avg:88.26ms
step:1639/1680 train_time:144655ms step_avg:88.26ms
step:1640/1680 train_time:144745ms step_avg:88.26ms
step:1641/1680 train_time:144835ms step_avg:88.26ms
step:1642/1680 train_time:144924ms step_avg:88.26ms
step:1643/1680 train_time:145012ms step_avg:88.26ms
step:1644/1680 train_time:145100ms step_avg:88.26ms
step:1645/1680 train_time:145188ms step_avg:88.26ms
step:1646/1680 train_time:145276ms step_avg:88.26ms
step:1647/1680 train_time:145365ms step_avg:88.26ms
step:1648/1680 train_time:145455ms step_avg:88.26ms
step:1649/1680 train_time:145544ms step_avg:88.26ms
step:1650/1680 train_time:145634ms step_avg:88.26ms
step:1651/1680 train_time:145725ms step_avg:88.26ms
step:1652/1680 train_time:145815ms step_avg:88.27ms
step:1653/1680 train_time:145904ms step_avg:88.27ms
step:1654/1680 train_time:145993ms step_avg:88.27ms
step:1655/1680 train_time:146083ms step_avg:88.27ms
step:1656/1680 train_time:146171ms step_avg:88.27ms
step:1657/1680 train_time:146260ms step_avg:88.27ms
step:1658/1680 train_time:146348ms step_avg:88.27ms
step:1659/1680 train_time:146438ms step_avg:88.27ms
step:1660/1680 train_time:146527ms step_avg:88.27ms
step:1661/1680 train_time:146618ms step_avg:88.27ms
step:1662/1680 train_time:146707ms step_avg:88.27ms
step:1663/1680 train_time:146797ms step_avg:88.27ms
step:1664/1680 train_time:146886ms step_avg:88.27ms
step:1665/1680 train_time:146975ms step_avg:88.27ms
step:1666/1680 train_time:147064ms step_avg:88.27ms
step:1667/1680 train_time:147153ms step_avg:88.27ms
step:1668/1680 train_time:147241ms step_avg:88.27ms
step:1669/1680 train_time:147330ms step_avg:88.27ms
step:1670/1680 train_time:147419ms step_avg:88.27ms
step:1671/1680 train_time:147508ms step_avg:88.28ms
step:1672/1680 train_time:147597ms step_avg:88.28ms
step:1673/1680 train_time:147686ms step_avg:88.28ms
step:1674/1680 train_time:147776ms step_avg:88.28ms
step:1675/1680 train_time:147865ms step_avg:88.28ms
step:1676/1680 train_time:147954ms step_avg:88.28ms
step:1677/1680 train_time:148044ms step_avg:88.28ms
step:1678/1680 train_time:148132ms step_avg:88.28ms
step:1679/1680 train_time:148221ms step_avg:88.28ms
step:1680/1680 train_time:148310ms step_avg:88.28ms
step:1680/1680 val_loss:3.2803 train_time:148400ms step_avg:88.33ms
peak memory allocated: 30760 MiB reserved: 45934 MiB

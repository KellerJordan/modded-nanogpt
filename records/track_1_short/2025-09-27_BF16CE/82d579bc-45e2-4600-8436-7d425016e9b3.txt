import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:04:08 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   25C    P0            119W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   24C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   25C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   26C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    150803      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    150804      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    150805      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    150806      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    150807      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    150808      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    150809      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    150810      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    150804      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    150805      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    150806      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    150807      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    150808      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    150809      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    150810      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:146ms step_avg:146.04ms
step:2/1680 train_time:166ms step_avg:82.79ms
step:3/1680 train_time:230ms step_avg:76.51ms
step:4/1680 train_time:315ms step_avg:78.72ms
step:5/1680 train_time:401ms step_avg:80.17ms
step:6/1680 train_time:487ms step_avg:81.17ms
step:7/1680 train_time:573ms step_avg:81.85ms
step:8/1680 train_time:659ms step_avg:82.38ms
step:9/1680 train_time:745ms step_avg:82.79ms
step:10/1680 train_time:831ms step_avg:83.14ms
step:11/1680 train_time:918ms step_avg:83.42ms
step:12/1680 train_time:1004ms step_avg:83.71ms
step:13/1680 train_time:1094ms step_avg:84.16ms
step:14/1680 train_time:1184ms step_avg:84.55ms
step:15/1680 train_time:1274ms step_avg:84.92ms
step:16/1680 train_time:1361ms step_avg:85.07ms
step:17/1680 train_time:1448ms step_avg:85.17ms
step:18/1680 train_time:1535ms step_avg:85.25ms
step:19/1680 train_time:1621ms step_avg:85.32ms
step:20/1680 train_time:1708ms step_avg:85.39ms
step:21/1680 train_time:1795ms step_avg:85.46ms
step:22/1680 train_time:1881ms step_avg:85.49ms
step:23/1680 train_time:1967ms step_avg:85.54ms
step:24/1680 train_time:2055ms step_avg:85.62ms
step:25/1680 train_time:2144ms step_avg:85.74ms
step:26/1680 train_time:2232ms step_avg:85.85ms
step:27/1680 train_time:2321ms step_avg:85.95ms
step:28/1680 train_time:2409ms step_avg:86.02ms
step:29/1680 train_time:2496ms step_avg:86.06ms
step:30/1680 train_time:2583ms step_avg:86.11ms
step:31/1680 train_time:2670ms step_avg:86.14ms
step:32/1680 train_time:2757ms step_avg:86.16ms
step:33/1680 train_time:2844ms step_avg:86.19ms
step:34/1680 train_time:2931ms step_avg:86.21ms
step:35/1680 train_time:3018ms step_avg:86.24ms
step:36/1680 train_time:3107ms step_avg:86.29ms
step:37/1680 train_time:3195ms step_avg:86.34ms
step:38/1680 train_time:3283ms step_avg:86.39ms
step:39/1680 train_time:3371ms step_avg:86.44ms
step:40/1680 train_time:3458ms step_avg:86.46ms
step:41/1680 train_time:3545ms step_avg:86.47ms
step:42/1680 train_time:3633ms step_avg:86.50ms
step:43/1680 train_time:3720ms step_avg:86.50ms
step:44/1680 train_time:3806ms step_avg:86.50ms
step:45/1680 train_time:3893ms step_avg:86.51ms
step:46/1680 train_time:3980ms step_avg:86.52ms
step:47/1680 train_time:4067ms step_avg:86.54ms
step:48/1680 train_time:4155ms step_avg:86.56ms
step:49/1680 train_time:4243ms step_avg:86.58ms
step:50/1680 train_time:4330ms step_avg:86.61ms
step:51/1680 train_time:4417ms step_avg:86.61ms
step:52/1680 train_time:4504ms step_avg:86.62ms
step:53/1680 train_time:4592ms step_avg:86.64ms
step:54/1680 train_time:4680ms step_avg:86.66ms
step:55/1680 train_time:4766ms step_avg:86.66ms
step:56/1680 train_time:4853ms step_avg:86.66ms
step:57/1680 train_time:4940ms step_avg:86.66ms
step:58/1680 train_time:5027ms step_avg:86.67ms
step:59/1680 train_time:5116ms step_avg:86.70ms
step:60/1680 train_time:5203ms step_avg:86.71ms
step:61/1680 train_time:5290ms step_avg:86.72ms
step:62/1680 train_time:5378ms step_avg:86.74ms
step:63/1680 train_time:5465ms step_avg:86.75ms
step:64/1680 train_time:5552ms step_avg:86.75ms
step:65/1680 train_time:5639ms step_avg:86.76ms
step:66/1680 train_time:5726ms step_avg:86.76ms
step:67/1680 train_time:5813ms step_avg:86.77ms
step:68/1680 train_time:5900ms step_avg:86.77ms
step:69/1680 train_time:5987ms step_avg:86.76ms
step:70/1680 train_time:6074ms step_avg:86.77ms
step:71/1680 train_time:6162ms step_avg:86.79ms
step:72/1680 train_time:6250ms step_avg:86.81ms
step:73/1680 train_time:6337ms step_avg:86.81ms
step:74/1680 train_time:6424ms step_avg:86.81ms
step:75/1680 train_time:6511ms step_avg:86.82ms
step:76/1680 train_time:6598ms step_avg:86.82ms
step:77/1680 train_time:6685ms step_avg:86.82ms
step:78/1680 train_time:6772ms step_avg:86.82ms
step:79/1680 train_time:6859ms step_avg:86.83ms
step:80/1680 train_time:6946ms step_avg:86.83ms
step:81/1680 train_time:7035ms step_avg:86.85ms
step:82/1680 train_time:7121ms step_avg:86.85ms
step:83/1680 train_time:7209ms step_avg:86.85ms
step:84/1680 train_time:7297ms step_avg:86.86ms
step:85/1680 train_time:7384ms step_avg:86.86ms
step:86/1680 train_time:7470ms step_avg:86.86ms
step:87/1680 train_time:7558ms step_avg:86.87ms
step:88/1680 train_time:7645ms step_avg:86.87ms
step:89/1680 train_time:7732ms step_avg:86.87ms
step:90/1680 train_time:7819ms step_avg:86.88ms
step:91/1680 train_time:7906ms step_avg:86.88ms
step:92/1680 train_time:7993ms step_avg:86.88ms
step:93/1680 train_time:8080ms step_avg:86.88ms
step:94/1680 train_time:8166ms step_avg:86.88ms
step:95/1680 train_time:8254ms step_avg:86.88ms
step:96/1680 train_time:8340ms step_avg:86.87ms
step:97/1680 train_time:8426ms step_avg:86.87ms
step:98/1680 train_time:8514ms step_avg:86.88ms
step:99/1680 train_time:8603ms step_avg:86.90ms
step:100/1680 train_time:8690ms step_avg:86.90ms
step:101/1680 train_time:8777ms step_avg:86.90ms
step:102/1680 train_time:8863ms step_avg:86.90ms
step:103/1680 train_time:8950ms step_avg:86.90ms
step:104/1680 train_time:9037ms step_avg:86.90ms
step:105/1680 train_time:9124ms step_avg:86.90ms
step:106/1680 train_time:9212ms step_avg:86.91ms
step:107/1680 train_time:9299ms step_avg:86.90ms
step:108/1680 train_time:9386ms step_avg:86.91ms
step:109/1680 train_time:9473ms step_avg:86.91ms
step:110/1680 train_time:9560ms step_avg:86.91ms
step:111/1680 train_time:9647ms step_avg:86.91ms
step:112/1680 train_time:9734ms step_avg:86.91ms
step:113/1680 train_time:9820ms step_avg:86.91ms
step:114/1680 train_time:9907ms step_avg:86.91ms
step:115/1680 train_time:9994ms step_avg:86.91ms
step:116/1680 train_time:10081ms step_avg:86.91ms
step:117/1680 train_time:10169ms step_avg:86.91ms
step:118/1680 train_time:10256ms step_avg:86.91ms
step:119/1680 train_time:10343ms step_avg:86.92ms
step:120/1680 train_time:10431ms step_avg:86.92ms
step:121/1680 train_time:10518ms step_avg:86.93ms
step:122/1680 train_time:10606ms step_avg:86.93ms
step:123/1680 train_time:10693ms step_avg:86.93ms
step:124/1680 train_time:10780ms step_avg:86.94ms
step:125/1680 train_time:10867ms step_avg:86.94ms
step:125/1680 val_loss:4.3408 train_time:10955ms step_avg:87.64ms
step:126/1680 train_time:10976ms step_avg:87.11ms
step:127/1680 train_time:11043ms step_avg:86.95ms
step:128/1680 train_time:11142ms step_avg:87.04ms
step:129/1680 train_time:11235ms step_avg:87.09ms
step:130/1680 train_time:11322ms step_avg:87.09ms
step:131/1680 train_time:11408ms step_avg:87.08ms
step:132/1680 train_time:11494ms step_avg:87.07ms
step:133/1680 train_time:11580ms step_avg:87.06ms
step:134/1680 train_time:11665ms step_avg:87.06ms
step:135/1680 train_time:11751ms step_avg:87.04ms
step:136/1680 train_time:11837ms step_avg:87.04ms
step:137/1680 train_time:11923ms step_avg:87.03ms
step:138/1680 train_time:12010ms step_avg:87.03ms
step:139/1680 train_time:12100ms step_avg:87.05ms
step:140/1680 train_time:12191ms step_avg:87.08ms
step:141/1680 train_time:12279ms step_avg:87.09ms
step:142/1680 train_time:12367ms step_avg:87.09ms
step:143/1680 train_time:12453ms step_avg:87.08ms
step:144/1680 train_time:12539ms step_avg:87.08ms
step:145/1680 train_time:12626ms step_avg:87.08ms
step:146/1680 train_time:12712ms step_avg:87.07ms
step:147/1680 train_time:12798ms step_avg:87.06ms
step:148/1680 train_time:12884ms step_avg:87.05ms
step:149/1680 train_time:12971ms step_avg:87.05ms
step:150/1680 train_time:13060ms step_avg:87.06ms
step:151/1680 train_time:13148ms step_avg:87.07ms
step:152/1680 train_time:13236ms step_avg:87.08ms
step:153/1680 train_time:13324ms step_avg:87.08ms
step:154/1680 train_time:13412ms step_avg:87.09ms
step:155/1680 train_time:13499ms step_avg:87.09ms
step:156/1680 train_time:13585ms step_avg:87.08ms
step:157/1680 train_time:13671ms step_avg:87.08ms
step:158/1680 train_time:13757ms step_avg:87.07ms
step:159/1680 train_time:13844ms step_avg:87.07ms
step:160/1680 train_time:13931ms step_avg:87.07ms
step:161/1680 train_time:14019ms step_avg:87.07ms
step:162/1680 train_time:14107ms step_avg:87.08ms
step:163/1680 train_time:14195ms step_avg:87.08ms
step:164/1680 train_time:14282ms step_avg:87.08ms
step:165/1680 train_time:14369ms step_avg:87.09ms
step:166/1680 train_time:14457ms step_avg:87.09ms
step:167/1680 train_time:14545ms step_avg:87.09ms
step:168/1680 train_time:14631ms step_avg:87.09ms
step:169/1680 train_time:14717ms step_avg:87.08ms
step:170/1680 train_time:14803ms step_avg:87.08ms
step:171/1680 train_time:14889ms step_avg:87.07ms
step:172/1680 train_time:14976ms step_avg:87.07ms
step:173/1680 train_time:15063ms step_avg:87.07ms
step:174/1680 train_time:15152ms step_avg:87.08ms
step:175/1680 train_time:15240ms step_avg:87.09ms
step:176/1680 train_time:15328ms step_avg:87.09ms
step:177/1680 train_time:15416ms step_avg:87.10ms
step:178/1680 train_time:15503ms step_avg:87.10ms
step:179/1680 train_time:15590ms step_avg:87.09ms
step:180/1680 train_time:15676ms step_avg:87.09ms
step:181/1680 train_time:15763ms step_avg:87.09ms
step:182/1680 train_time:15849ms step_avg:87.08ms
step:183/1680 train_time:15936ms step_avg:87.08ms
step:184/1680 train_time:16023ms step_avg:87.08ms
step:185/1680 train_time:16110ms step_avg:87.08ms
step:186/1680 train_time:16197ms step_avg:87.08ms
step:187/1680 train_time:16285ms step_avg:87.09ms
step:188/1680 train_time:16373ms step_avg:87.09ms
step:189/1680 train_time:16460ms step_avg:87.09ms
step:190/1680 train_time:16547ms step_avg:87.09ms
step:191/1680 train_time:16634ms step_avg:87.09ms
step:192/1680 train_time:16721ms step_avg:87.09ms
step:193/1680 train_time:16808ms step_avg:87.09ms
step:194/1680 train_time:16894ms step_avg:87.08ms
step:195/1680 train_time:16982ms step_avg:87.09ms
step:196/1680 train_time:17068ms step_avg:87.08ms
step:197/1680 train_time:17155ms step_avg:87.08ms
step:198/1680 train_time:17242ms step_avg:87.08ms
step:199/1680 train_time:17330ms step_avg:87.08ms
step:200/1680 train_time:17416ms step_avg:87.08ms
step:201/1680 train_time:17503ms step_avg:87.08ms
step:202/1680 train_time:17591ms step_avg:87.08ms
step:203/1680 train_time:17678ms step_avg:87.08ms
step:204/1680 train_time:17765ms step_avg:87.08ms
step:205/1680 train_time:17851ms step_avg:87.08ms
step:206/1680 train_time:17938ms step_avg:87.08ms
step:207/1680 train_time:18025ms step_avg:87.08ms
step:208/1680 train_time:18112ms step_avg:87.08ms
step:209/1680 train_time:18200ms step_avg:87.08ms
step:210/1680 train_time:18287ms step_avg:87.08ms
step:211/1680 train_time:18375ms step_avg:87.08ms
step:212/1680 train_time:18462ms step_avg:87.08ms
step:213/1680 train_time:18549ms step_avg:87.08ms
step:214/1680 train_time:18636ms step_avg:87.08ms
step:215/1680 train_time:18723ms step_avg:87.08ms
step:216/1680 train_time:18810ms step_avg:87.08ms
step:217/1680 train_time:18896ms step_avg:87.08ms
step:218/1680 train_time:18983ms step_avg:87.08ms
step:219/1680 train_time:19070ms step_avg:87.08ms
step:220/1680 train_time:19157ms step_avg:87.08ms
step:221/1680 train_time:19244ms step_avg:87.08ms
step:222/1680 train_time:19330ms step_avg:87.07ms
step:223/1680 train_time:19418ms step_avg:87.07ms
step:224/1680 train_time:19505ms step_avg:87.08ms
step:225/1680 train_time:19592ms step_avg:87.08ms
step:226/1680 train_time:19679ms step_avg:87.08ms
step:227/1680 train_time:19766ms step_avg:87.08ms
step:228/1680 train_time:19853ms step_avg:87.08ms
step:229/1680 train_time:19940ms step_avg:87.07ms
step:230/1680 train_time:20027ms step_avg:87.07ms
step:231/1680 train_time:20114ms step_avg:87.07ms
step:232/1680 train_time:20201ms step_avg:87.07ms
step:233/1680 train_time:20288ms step_avg:87.07ms
step:234/1680 train_time:20374ms step_avg:87.07ms
step:235/1680 train_time:20462ms step_avg:87.07ms
step:236/1680 train_time:20549ms step_avg:87.07ms
step:237/1680 train_time:20636ms step_avg:87.07ms
step:238/1680 train_time:20723ms step_avg:87.07ms
step:239/1680 train_time:20810ms step_avg:87.07ms
step:240/1680 train_time:20897ms step_avg:87.07ms
step:241/1680 train_time:20983ms step_avg:87.07ms
step:242/1680 train_time:21070ms step_avg:87.07ms
step:243/1680 train_time:21157ms step_avg:87.07ms
step:244/1680 train_time:21245ms step_avg:87.07ms
step:245/1680 train_time:21332ms step_avg:87.07ms
step:246/1680 train_time:21419ms step_avg:87.07ms
step:247/1680 train_time:21506ms step_avg:87.07ms
step:248/1680 train_time:21592ms step_avg:87.07ms
step:249/1680 train_time:21680ms step_avg:87.07ms
step:250/1680 train_time:21767ms step_avg:87.07ms
step:250/1680 val_loss:3.9769 train_time:21855ms step_avg:87.42ms
step:251/1680 train_time:21874ms step_avg:87.15ms
step:252/1680 train_time:21944ms step_avg:87.08ms
step:253/1680 train_time:22036ms step_avg:87.10ms
step:254/1680 train_time:22124ms step_avg:87.10ms
step:255/1680 train_time:22211ms step_avg:87.10ms
step:256/1680 train_time:22297ms step_avg:87.10ms
step:257/1680 train_time:22383ms step_avg:87.09ms
step:258/1680 train_time:22469ms step_avg:87.09ms
step:259/1680 train_time:22555ms step_avg:87.09ms
step:260/1680 train_time:22642ms step_avg:87.08ms
step:261/1680 train_time:22728ms step_avg:87.08ms
step:262/1680 train_time:22816ms step_avg:87.09ms
step:263/1680 train_time:22904ms step_avg:87.09ms
step:264/1680 train_time:22994ms step_avg:87.10ms
step:265/1680 train_time:23083ms step_avg:87.11ms
step:266/1680 train_time:23171ms step_avg:87.11ms
step:267/1680 train_time:23257ms step_avg:87.10ms
step:268/1680 train_time:23344ms step_avg:87.10ms
step:269/1680 train_time:23430ms step_avg:87.10ms
step:270/1680 train_time:23516ms step_avg:87.10ms
step:271/1680 train_time:23602ms step_avg:87.09ms
step:272/1680 train_time:23689ms step_avg:87.09ms
step:273/1680 train_time:23776ms step_avg:87.09ms
step:274/1680 train_time:23863ms step_avg:87.09ms
step:275/1680 train_time:23951ms step_avg:87.10ms
step:276/1680 train_time:24039ms step_avg:87.10ms
step:277/1680 train_time:24126ms step_avg:87.10ms
step:278/1680 train_time:24214ms step_avg:87.10ms
step:279/1680 train_time:24300ms step_avg:87.10ms
step:280/1680 train_time:24387ms step_avg:87.10ms
step:281/1680 train_time:24474ms step_avg:87.09ms
step:282/1680 train_time:24560ms step_avg:87.09ms
step:283/1680 train_time:24647ms step_avg:87.09ms
step:284/1680 train_time:24734ms step_avg:87.09ms
step:285/1680 train_time:24821ms step_avg:87.09ms
step:286/1680 train_time:24909ms step_avg:87.09ms
step:287/1680 train_time:24997ms step_avg:87.10ms
step:288/1680 train_time:25084ms step_avg:87.10ms
step:289/1680 train_time:25171ms step_avg:87.10ms
step:290/1680 train_time:25258ms step_avg:87.09ms
step:291/1680 train_time:25345ms step_avg:87.10ms
step:292/1680 train_time:25432ms step_avg:87.10ms
step:293/1680 train_time:25518ms step_avg:87.09ms
step:294/1680 train_time:25605ms step_avg:87.09ms
step:295/1680 train_time:25691ms step_avg:87.09ms
step:296/1680 train_time:25779ms step_avg:87.09ms
step:297/1680 train_time:25866ms step_avg:87.09ms
step:298/1680 train_time:25954ms step_avg:87.09ms
step:299/1680 train_time:26041ms step_avg:87.09ms
step:300/1680 train_time:26129ms step_avg:87.10ms
step:301/1680 train_time:26216ms step_avg:87.10ms
step:302/1680 train_time:26302ms step_avg:87.09ms
step:303/1680 train_time:26389ms step_avg:87.09ms
step:304/1680 train_time:26476ms step_avg:87.09ms
step:305/1680 train_time:26562ms step_avg:87.09ms
step:306/1680 train_time:26649ms step_avg:87.09ms
step:307/1680 train_time:26736ms step_avg:87.09ms
step:308/1680 train_time:26823ms step_avg:87.09ms
step:309/1680 train_time:26910ms step_avg:87.09ms
step:310/1680 train_time:26998ms step_avg:87.09ms
step:311/1680 train_time:27085ms step_avg:87.09ms
step:312/1680 train_time:27172ms step_avg:87.09ms
step:313/1680 train_time:27260ms step_avg:87.09ms
step:314/1680 train_time:27347ms step_avg:87.09ms
step:315/1680 train_time:27434ms step_avg:87.09ms
step:316/1680 train_time:27521ms step_avg:87.09ms
step:317/1680 train_time:27608ms step_avg:87.09ms
step:318/1680 train_time:27694ms step_avg:87.09ms
step:319/1680 train_time:27782ms step_avg:87.09ms
step:320/1680 train_time:27869ms step_avg:87.09ms
step:321/1680 train_time:27956ms step_avg:87.09ms
step:322/1680 train_time:28043ms step_avg:87.09ms
step:323/1680 train_time:28130ms step_avg:87.09ms
step:324/1680 train_time:28218ms step_avg:87.09ms
step:325/1680 train_time:28305ms step_avg:87.09ms
step:326/1680 train_time:28392ms step_avg:87.09ms
step:327/1680 train_time:28479ms step_avg:87.09ms
step:328/1680 train_time:28566ms step_avg:87.09ms
step:329/1680 train_time:28653ms step_avg:87.09ms
step:330/1680 train_time:28739ms step_avg:87.09ms
step:331/1680 train_time:28827ms step_avg:87.09ms
step:332/1680 train_time:28914ms step_avg:87.09ms
step:333/1680 train_time:29001ms step_avg:87.09ms
step:334/1680 train_time:29087ms step_avg:87.09ms
step:335/1680 train_time:29175ms step_avg:87.09ms
step:336/1680 train_time:29262ms step_avg:87.09ms
step:337/1680 train_time:29350ms step_avg:87.09ms
step:338/1680 train_time:29437ms step_avg:87.09ms
step:339/1680 train_time:29523ms step_avg:87.09ms
step:340/1680 train_time:29609ms step_avg:87.09ms
step:341/1680 train_time:29696ms step_avg:87.09ms
step:342/1680 train_time:29784ms step_avg:87.09ms
step:343/1680 train_time:29871ms step_avg:87.09ms
step:344/1680 train_time:29959ms step_avg:87.09ms
step:345/1680 train_time:30047ms step_avg:87.09ms
step:346/1680 train_time:30133ms step_avg:87.09ms
step:347/1680 train_time:30221ms step_avg:87.09ms
step:348/1680 train_time:30308ms step_avg:87.09ms
step:349/1680 train_time:30396ms step_avg:87.09ms
step:350/1680 train_time:30483ms step_avg:87.09ms
step:351/1680 train_time:30570ms step_avg:87.09ms
step:352/1680 train_time:30656ms step_avg:87.09ms
step:353/1680 train_time:30743ms step_avg:87.09ms
step:354/1680 train_time:30831ms step_avg:87.09ms
step:355/1680 train_time:30918ms step_avg:87.09ms
step:356/1680 train_time:31005ms step_avg:87.09ms
step:357/1680 train_time:31093ms step_avg:87.09ms
step:358/1680 train_time:31180ms step_avg:87.10ms
step:359/1680 train_time:31267ms step_avg:87.10ms
step:360/1680 train_time:31354ms step_avg:87.09ms
step:361/1680 train_time:31441ms step_avg:87.09ms
step:362/1680 train_time:31528ms step_avg:87.09ms
step:363/1680 train_time:31615ms step_avg:87.09ms
step:364/1680 train_time:31702ms step_avg:87.09ms
step:365/1680 train_time:31789ms step_avg:87.09ms
step:366/1680 train_time:31876ms step_avg:87.09ms
step:367/1680 train_time:31962ms step_avg:87.09ms
step:368/1680 train_time:32050ms step_avg:87.09ms
step:369/1680 train_time:32137ms step_avg:87.09ms
step:370/1680 train_time:32224ms step_avg:87.09ms
step:371/1680 train_time:32311ms step_avg:87.09ms
step:372/1680 train_time:32398ms step_avg:87.09ms
step:373/1680 train_time:32485ms step_avg:87.09ms
step:374/1680 train_time:32573ms step_avg:87.09ms
step:375/1680 train_time:32660ms step_avg:87.09ms
step:375/1680 val_loss:3.8239 train_time:32748ms step_avg:87.33ms
step:376/1680 train_time:32768ms step_avg:87.15ms
step:377/1680 train_time:32837ms step_avg:87.10ms
step:378/1680 train_time:32927ms step_avg:87.11ms
step:379/1680 train_time:33017ms step_avg:87.12ms
step:380/1680 train_time:33103ms step_avg:87.11ms
step:381/1680 train_time:33190ms step_avg:87.11ms
step:382/1680 train_time:33276ms step_avg:87.11ms
step:383/1680 train_time:33362ms step_avg:87.11ms
step:384/1680 train_time:33449ms step_avg:87.11ms
step:385/1680 train_time:33535ms step_avg:87.10ms
step:386/1680 train_time:33621ms step_avg:87.10ms
step:387/1680 train_time:33708ms step_avg:87.10ms
step:388/1680 train_time:33796ms step_avg:87.10ms
step:389/1680 train_time:33885ms step_avg:87.11ms
step:390/1680 train_time:33974ms step_avg:87.11ms
step:391/1680 train_time:34061ms step_avg:87.11ms
step:392/1680 train_time:34149ms step_avg:87.11ms
step:393/1680 train_time:34235ms step_avg:87.11ms
step:394/1680 train_time:34322ms step_avg:87.11ms
step:395/1680 train_time:34408ms step_avg:87.11ms
step:396/1680 train_time:34494ms step_avg:87.11ms
step:397/1680 train_time:34581ms step_avg:87.10ms
step:398/1680 train_time:34667ms step_avg:87.10ms
step:399/1680 train_time:34755ms step_avg:87.10ms
step:400/1680 train_time:34842ms step_avg:87.11ms
step:401/1680 train_time:34930ms step_avg:87.11ms
step:402/1680 train_time:35018ms step_avg:87.11ms
step:403/1680 train_time:35105ms step_avg:87.11ms
step:404/1680 train_time:35192ms step_avg:87.11ms
step:405/1680 train_time:35279ms step_avg:87.11ms
step:406/1680 train_time:35365ms step_avg:87.11ms
step:407/1680 train_time:35451ms step_avg:87.10ms
step:408/1680 train_time:35538ms step_avg:87.10ms
step:409/1680 train_time:35625ms step_avg:87.10ms
step:410/1680 train_time:35712ms step_avg:87.10ms
step:411/1680 train_time:35799ms step_avg:87.10ms
step:412/1680 train_time:35886ms step_avg:87.10ms
step:413/1680 train_time:35974ms step_avg:87.11ms
step:414/1680 train_time:36062ms step_avg:87.11ms
step:415/1680 train_time:36149ms step_avg:87.11ms
step:416/1680 train_time:36236ms step_avg:87.11ms
step:417/1680 train_time:36323ms step_avg:87.10ms
step:418/1680 train_time:36410ms step_avg:87.10ms
step:419/1680 train_time:36496ms step_avg:87.10ms
step:420/1680 train_time:36583ms step_avg:87.10ms
step:421/1680 train_time:36670ms step_avg:87.10ms
step:422/1680 train_time:36758ms step_avg:87.10ms
step:423/1680 train_time:36845ms step_avg:87.10ms
step:424/1680 train_time:36933ms step_avg:87.11ms
step:425/1680 train_time:37019ms step_avg:87.10ms
step:426/1680 train_time:37107ms step_avg:87.10ms
step:427/1680 train_time:37194ms step_avg:87.11ms
step:428/1680 train_time:37281ms step_avg:87.11ms
step:429/1680 train_time:37368ms step_avg:87.11ms
step:430/1680 train_time:37455ms step_avg:87.10ms
step:431/1680 train_time:37542ms step_avg:87.10ms
step:432/1680 train_time:37629ms step_avg:87.10ms
step:433/1680 train_time:37715ms step_avg:87.10ms
step:434/1680 train_time:37802ms step_avg:87.10ms
step:435/1680 train_time:37889ms step_avg:87.10ms
step:436/1680 train_time:37977ms step_avg:87.10ms
step:437/1680 train_time:38064ms step_avg:87.10ms
step:438/1680 train_time:38151ms step_avg:87.10ms
step:439/1680 train_time:38238ms step_avg:87.10ms
step:440/1680 train_time:38325ms step_avg:87.10ms
step:441/1680 train_time:38412ms step_avg:87.10ms
step:442/1680 train_time:38499ms step_avg:87.10ms
step:443/1680 train_time:38586ms step_avg:87.10ms
step:444/1680 train_time:38672ms step_avg:87.10ms
step:445/1680 train_time:38760ms step_avg:87.10ms
step:446/1680 train_time:38847ms step_avg:87.10ms
step:447/1680 train_time:38934ms step_avg:87.10ms
step:448/1680 train_time:39021ms step_avg:87.10ms
step:449/1680 train_time:39109ms step_avg:87.10ms
step:450/1680 train_time:39196ms step_avg:87.10ms
step:451/1680 train_time:39283ms step_avg:87.10ms
step:452/1680 train_time:39370ms step_avg:87.10ms
step:453/1680 train_time:39457ms step_avg:87.10ms
step:454/1680 train_time:39545ms step_avg:87.10ms
step:455/1680 train_time:39631ms step_avg:87.10ms
step:456/1680 train_time:39718ms step_avg:87.10ms
step:457/1680 train_time:39805ms step_avg:87.10ms
step:458/1680 train_time:39892ms step_avg:87.10ms
step:459/1680 train_time:39979ms step_avg:87.10ms
step:460/1680 train_time:40066ms step_avg:87.10ms
step:461/1680 train_time:40153ms step_avg:87.10ms
step:462/1680 train_time:40240ms step_avg:87.10ms
step:463/1680 train_time:40327ms step_avg:87.10ms
step:464/1680 train_time:40415ms step_avg:87.10ms
step:465/1680 train_time:40502ms step_avg:87.10ms
step:466/1680 train_time:40589ms step_avg:87.10ms
step:467/1680 train_time:40676ms step_avg:87.10ms
step:468/1680 train_time:40763ms step_avg:87.10ms
step:469/1680 train_time:40850ms step_avg:87.10ms
step:470/1680 train_time:40937ms step_avg:87.10ms
step:471/1680 train_time:41023ms step_avg:87.10ms
step:472/1680 train_time:41110ms step_avg:87.10ms
step:473/1680 train_time:41198ms step_avg:87.10ms
step:474/1680 train_time:41285ms step_avg:87.10ms
step:475/1680 train_time:41371ms step_avg:87.10ms
step:476/1680 train_time:41459ms step_avg:87.10ms
step:477/1680 train_time:41546ms step_avg:87.10ms
step:478/1680 train_time:41633ms step_avg:87.10ms
step:479/1680 train_time:41720ms step_avg:87.10ms
step:480/1680 train_time:41808ms step_avg:87.10ms
step:481/1680 train_time:41895ms step_avg:87.10ms
step:482/1680 train_time:41983ms step_avg:87.10ms
step:483/1680 train_time:42069ms step_avg:87.10ms
step:484/1680 train_time:42157ms step_avg:87.10ms
step:485/1680 train_time:42244ms step_avg:87.10ms
step:486/1680 train_time:42331ms step_avg:87.10ms
step:487/1680 train_time:42418ms step_avg:87.10ms
step:488/1680 train_time:42505ms step_avg:87.10ms
step:489/1680 train_time:42592ms step_avg:87.10ms
step:490/1680 train_time:42679ms step_avg:87.10ms
step:491/1680 train_time:42766ms step_avg:87.10ms
step:492/1680 train_time:42852ms step_avg:87.10ms
step:493/1680 train_time:42939ms step_avg:87.10ms
step:494/1680 train_time:43026ms step_avg:87.10ms
step:495/1680 train_time:43113ms step_avg:87.10ms
step:496/1680 train_time:43200ms step_avg:87.10ms
step:497/1680 train_time:43287ms step_avg:87.10ms
step:498/1680 train_time:43374ms step_avg:87.10ms
step:499/1680 train_time:43461ms step_avg:87.10ms
step:500/1680 train_time:43549ms step_avg:87.10ms
step:500/1680 val_loss:3.7206 train_time:43637ms step_avg:87.27ms
step:501/1680 train_time:43658ms step_avg:87.14ms
step:502/1680 train_time:43728ms step_avg:87.11ms
step:503/1680 train_time:43819ms step_avg:87.12ms
step:504/1680 train_time:43907ms step_avg:87.12ms
step:505/1680 train_time:43994ms step_avg:87.12ms
step:506/1680 train_time:44080ms step_avg:87.12ms
step:507/1680 train_time:44166ms step_avg:87.11ms
step:508/1680 train_time:44252ms step_avg:87.11ms
step:509/1680 train_time:44338ms step_avg:87.11ms
step:510/1680 train_time:44424ms step_avg:87.11ms
step:511/1680 train_time:44510ms step_avg:87.10ms
step:512/1680 train_time:44598ms step_avg:87.11ms
step:513/1680 train_time:44686ms step_avg:87.11ms
step:514/1680 train_time:44775ms step_avg:87.11ms
step:515/1680 train_time:44863ms step_avg:87.11ms
step:516/1680 train_time:44951ms step_avg:87.11ms
step:517/1680 train_time:45038ms step_avg:87.12ms
step:518/1680 train_time:45125ms step_avg:87.11ms
step:519/1680 train_time:45212ms step_avg:87.11ms
step:520/1680 train_time:45298ms step_avg:87.11ms
step:521/1680 train_time:45385ms step_avg:87.11ms
step:522/1680 train_time:45471ms step_avg:87.11ms
step:523/1680 train_time:45559ms step_avg:87.11ms
step:524/1680 train_time:45646ms step_avg:87.11ms
step:525/1680 train_time:45734ms step_avg:87.11ms
step:526/1680 train_time:45822ms step_avg:87.11ms
step:527/1680 train_time:45910ms step_avg:87.12ms
step:528/1680 train_time:45997ms step_avg:87.12ms
step:529/1680 train_time:46084ms step_avg:87.12ms
step:530/1680 train_time:46170ms step_avg:87.11ms
step:531/1680 train_time:46257ms step_avg:87.11ms
step:532/1680 train_time:46343ms step_avg:87.11ms
step:533/1680 train_time:46430ms step_avg:87.11ms
step:534/1680 train_time:46516ms step_avg:87.11ms
step:535/1680 train_time:46604ms step_avg:87.11ms
step:536/1680 train_time:46691ms step_avg:87.11ms
step:537/1680 train_time:46779ms step_avg:87.11ms
step:538/1680 train_time:46867ms step_avg:87.11ms
step:539/1680 train_time:46954ms step_avg:87.11ms
step:540/1680 train_time:47042ms step_avg:87.11ms
step:541/1680 train_time:47129ms step_avg:87.12ms
step:542/1680 train_time:47216ms step_avg:87.11ms
step:543/1680 train_time:47303ms step_avg:87.11ms
step:544/1680 train_time:47389ms step_avg:87.11ms
step:545/1680 train_time:47476ms step_avg:87.11ms
step:546/1680 train_time:47563ms step_avg:87.11ms
step:547/1680 train_time:47650ms step_avg:87.11ms
step:548/1680 train_time:47737ms step_avg:87.11ms
step:549/1680 train_time:47827ms step_avg:87.12ms
step:550/1680 train_time:47916ms step_avg:87.12ms
step:551/1680 train_time:48003ms step_avg:87.12ms
step:552/1680 train_time:48091ms step_avg:87.12ms
step:553/1680 train_time:48180ms step_avg:87.12ms
step:554/1680 train_time:48268ms step_avg:87.13ms
step:555/1680 train_time:48356ms step_avg:87.13ms
step:556/1680 train_time:48443ms step_avg:87.13ms
step:557/1680 train_time:48531ms step_avg:87.13ms
step:558/1680 train_time:48619ms step_avg:87.13ms
step:559/1680 train_time:48708ms step_avg:87.13ms
step:560/1680 train_time:48796ms step_avg:87.14ms
step:561/1680 train_time:48885ms step_avg:87.14ms
step:562/1680 train_time:48973ms step_avg:87.14ms
step:563/1680 train_time:49062ms step_avg:87.14ms
step:564/1680 train_time:49150ms step_avg:87.15ms
step:565/1680 train_time:49239ms step_avg:87.15ms
step:566/1680 train_time:49327ms step_avg:87.15ms
step:567/1680 train_time:49414ms step_avg:87.15ms
step:568/1680 train_time:49503ms step_avg:87.15ms
step:569/1680 train_time:49591ms step_avg:87.15ms
step:570/1680 train_time:49679ms step_avg:87.16ms
step:571/1680 train_time:49768ms step_avg:87.16ms
step:572/1680 train_time:49856ms step_avg:87.16ms
step:573/1680 train_time:49945ms step_avg:87.16ms
step:574/1680 train_time:50033ms step_avg:87.17ms
step:575/1680 train_time:50122ms step_avg:87.17ms
step:576/1680 train_time:50211ms step_avg:87.17ms
step:577/1680 train_time:50299ms step_avg:87.17ms
step:578/1680 train_time:50387ms step_avg:87.17ms
step:579/1680 train_time:50476ms step_avg:87.18ms
step:580/1680 train_time:50563ms step_avg:87.18ms
step:581/1680 train_time:50651ms step_avg:87.18ms
step:582/1680 train_time:50740ms step_avg:87.18ms
step:583/1680 train_time:50828ms step_avg:87.18ms
step:584/1680 train_time:50916ms step_avg:87.18ms
step:585/1680 train_time:51005ms step_avg:87.19ms
step:586/1680 train_time:51093ms step_avg:87.19ms
step:587/1680 train_time:51182ms step_avg:87.19ms
step:588/1680 train_time:51269ms step_avg:87.19ms
step:589/1680 train_time:51357ms step_avg:87.19ms
step:590/1680 train_time:51445ms step_avg:87.20ms
step:591/1680 train_time:51534ms step_avg:87.20ms
step:592/1680 train_time:51622ms step_avg:87.20ms
step:593/1680 train_time:51711ms step_avg:87.20ms
step:594/1680 train_time:51798ms step_avg:87.20ms
step:595/1680 train_time:51886ms step_avg:87.20ms
step:596/1680 train_time:51974ms step_avg:87.20ms
step:597/1680 train_time:52062ms step_avg:87.21ms
step:598/1680 train_time:52152ms step_avg:87.21ms
step:599/1680 train_time:52240ms step_avg:87.21ms
step:600/1680 train_time:52329ms step_avg:87.21ms
step:601/1680 train_time:52417ms step_avg:87.22ms
step:602/1680 train_time:52505ms step_avg:87.22ms
step:603/1680 train_time:52593ms step_avg:87.22ms
step:604/1680 train_time:52681ms step_avg:87.22ms
step:605/1680 train_time:52769ms step_avg:87.22ms
step:606/1680 train_time:52857ms step_avg:87.22ms
step:607/1680 train_time:52946ms step_avg:87.23ms
step:608/1680 train_time:53034ms step_avg:87.23ms
step:609/1680 train_time:53123ms step_avg:87.23ms
step:610/1680 train_time:53211ms step_avg:87.23ms
step:611/1680 train_time:53300ms step_avg:87.23ms
step:612/1680 train_time:53388ms step_avg:87.24ms
step:613/1680 train_time:53477ms step_avg:87.24ms
step:614/1680 train_time:53565ms step_avg:87.24ms
step:615/1680 train_time:53653ms step_avg:87.24ms
step:616/1680 train_time:53741ms step_avg:87.24ms
step:617/1680 train_time:53829ms step_avg:87.24ms
step:618/1680 train_time:53918ms step_avg:87.25ms
step:619/1680 train_time:54006ms step_avg:87.25ms
step:620/1680 train_time:54094ms step_avg:87.25ms
step:621/1680 train_time:54182ms step_avg:87.25ms
step:622/1680 train_time:54270ms step_avg:87.25ms
step:623/1680 train_time:54359ms step_avg:87.25ms
step:624/1680 train_time:54447ms step_avg:87.25ms
step:625/1680 train_time:54535ms step_avg:87.26ms
step:625/1680 val_loss:3.6201 train_time:54625ms step_avg:87.40ms
step:626/1680 train_time:54645ms step_avg:87.29ms
step:627/1680 train_time:54714ms step_avg:87.26ms
step:628/1680 train_time:54804ms step_avg:87.27ms
step:629/1680 train_time:54895ms step_avg:87.27ms
step:630/1680 train_time:54983ms step_avg:87.27ms
step:631/1680 train_time:55070ms step_avg:87.27ms
step:632/1680 train_time:55156ms step_avg:87.27ms
step:633/1680 train_time:55243ms step_avg:87.27ms
step:634/1680 train_time:55330ms step_avg:87.27ms
step:635/1680 train_time:55417ms step_avg:87.27ms
step:636/1680 train_time:55505ms step_avg:87.27ms
step:637/1680 train_time:55594ms step_avg:87.28ms
step:638/1680 train_time:55685ms step_avg:87.28ms
step:639/1680 train_time:55775ms step_avg:87.28ms
step:640/1680 train_time:55864ms step_avg:87.29ms
step:641/1680 train_time:55953ms step_avg:87.29ms
step:642/1680 train_time:56040ms step_avg:87.29ms
step:643/1680 train_time:56127ms step_avg:87.29ms
step:644/1680 train_time:56214ms step_avg:87.29ms
step:645/1680 train_time:56302ms step_avg:87.29ms
step:646/1680 train_time:56389ms step_avg:87.29ms
step:647/1680 train_time:56477ms step_avg:87.29ms
step:648/1680 train_time:56565ms step_avg:87.29ms
step:649/1680 train_time:56653ms step_avg:87.29ms
step:650/1680 train_time:56742ms step_avg:87.30ms
step:651/1680 train_time:56831ms step_avg:87.30ms
step:652/1680 train_time:56919ms step_avg:87.30ms
step:653/1680 train_time:57008ms step_avg:87.30ms
step:654/1680 train_time:57096ms step_avg:87.30ms
step:655/1680 train_time:57183ms step_avg:87.30ms
step:656/1680 train_time:57271ms step_avg:87.30ms
step:657/1680 train_time:57358ms step_avg:87.30ms
step:658/1680 train_time:57446ms step_avg:87.30ms
step:659/1680 train_time:57533ms step_avg:87.30ms
step:660/1680 train_time:57622ms step_avg:87.31ms
step:661/1680 train_time:57710ms step_avg:87.31ms
step:662/1680 train_time:57799ms step_avg:87.31ms
step:663/1680 train_time:57888ms step_avg:87.31ms
step:664/1680 train_time:57976ms step_avg:87.31ms
step:665/1680 train_time:58064ms step_avg:87.31ms
step:666/1680 train_time:58152ms step_avg:87.31ms
step:667/1680 train_time:58239ms step_avg:87.32ms
step:668/1680 train_time:58328ms step_avg:87.32ms
step:669/1680 train_time:58416ms step_avg:87.32ms
step:670/1680 train_time:58504ms step_avg:87.32ms
step:671/1680 train_time:58592ms step_avg:87.32ms
step:672/1680 train_time:58680ms step_avg:87.32ms
step:673/1680 train_time:58768ms step_avg:87.32ms
step:674/1680 train_time:58857ms step_avg:87.32ms
step:675/1680 train_time:58945ms step_avg:87.33ms
step:676/1680 train_time:59033ms step_avg:87.33ms
step:677/1680 train_time:59121ms step_avg:87.33ms
step:678/1680 train_time:59209ms step_avg:87.33ms
step:679/1680 train_time:59298ms step_avg:87.33ms
step:680/1680 train_time:59386ms step_avg:87.33ms
step:681/1680 train_time:59474ms step_avg:87.33ms
step:682/1680 train_time:59562ms step_avg:87.33ms
step:683/1680 train_time:59651ms step_avg:87.34ms
step:684/1680 train_time:59739ms step_avg:87.34ms
step:685/1680 train_time:59828ms step_avg:87.34ms
step:686/1680 train_time:59916ms step_avg:87.34ms
step:687/1680 train_time:60005ms step_avg:87.34ms
step:688/1680 train_time:60092ms step_avg:87.34ms
step:689/1680 train_time:60180ms step_avg:87.34ms
step:690/1680 train_time:60269ms step_avg:87.35ms
step:691/1680 train_time:60357ms step_avg:87.35ms
step:692/1680 train_time:60445ms step_avg:87.35ms
step:693/1680 train_time:60533ms step_avg:87.35ms
step:694/1680 train_time:60621ms step_avg:87.35ms
step:695/1680 train_time:60709ms step_avg:87.35ms
step:696/1680 train_time:60798ms step_avg:87.35ms
step:697/1680 train_time:60886ms step_avg:87.35ms
step:698/1680 train_time:60975ms step_avg:87.36ms
step:699/1680 train_time:61064ms step_avg:87.36ms
step:700/1680 train_time:61152ms step_avg:87.36ms
step:701/1680 train_time:61240ms step_avg:87.36ms
step:702/1680 train_time:61329ms step_avg:87.36ms
step:703/1680 train_time:61417ms step_avg:87.36ms
step:704/1680 train_time:61505ms step_avg:87.36ms
step:705/1680 train_time:61593ms step_avg:87.37ms
step:706/1680 train_time:61680ms step_avg:87.37ms
step:707/1680 train_time:61769ms step_avg:87.37ms
step:708/1680 train_time:61858ms step_avg:87.37ms
step:709/1680 train_time:61946ms step_avg:87.37ms
step:710/1680 train_time:62034ms step_avg:87.37ms
step:711/1680 train_time:62122ms step_avg:87.37ms
step:712/1680 train_time:62210ms step_avg:87.37ms
step:713/1680 train_time:62298ms step_avg:87.38ms
step:714/1680 train_time:62387ms step_avg:87.38ms
step:715/1680 train_time:62475ms step_avg:87.38ms
step:716/1680 train_time:62564ms step_avg:87.38ms
step:717/1680 train_time:62652ms step_avg:87.38ms
step:718/1680 train_time:62740ms step_avg:87.38ms
step:719/1680 train_time:62828ms step_avg:87.38ms
step:720/1680 train_time:62916ms step_avg:87.38ms
step:721/1680 train_time:63005ms step_avg:87.38ms
step:722/1680 train_time:63093ms step_avg:87.39ms
step:723/1680 train_time:63181ms step_avg:87.39ms
step:724/1680 train_time:63269ms step_avg:87.39ms
step:725/1680 train_time:63357ms step_avg:87.39ms
step:726/1680 train_time:63444ms step_avg:87.39ms
step:727/1680 train_time:63533ms step_avg:87.39ms
step:728/1680 train_time:63622ms step_avg:87.39ms
step:729/1680 train_time:63710ms step_avg:87.39ms
step:730/1680 train_time:63798ms step_avg:87.39ms
step:731/1680 train_time:63886ms step_avg:87.40ms
step:732/1680 train_time:63974ms step_avg:87.40ms
step:733/1680 train_time:64063ms step_avg:87.40ms
step:734/1680 train_time:64150ms step_avg:87.40ms
step:735/1680 train_time:64239ms step_avg:87.40ms
step:736/1680 train_time:64327ms step_avg:87.40ms
step:737/1680 train_time:64415ms step_avg:87.40ms
step:738/1680 train_time:64504ms step_avg:87.40ms
step:739/1680 train_time:64592ms step_avg:87.41ms
step:740/1680 train_time:64680ms step_avg:87.41ms
step:741/1680 train_time:64768ms step_avg:87.41ms
step:742/1680 train_time:64856ms step_avg:87.41ms
step:743/1680 train_time:64945ms step_avg:87.41ms
step:744/1680 train_time:65033ms step_avg:87.41ms
step:745/1680 train_time:65121ms step_avg:87.41ms
step:746/1680 train_time:65209ms step_avg:87.41ms
step:747/1680 train_time:65297ms step_avg:87.41ms
step:748/1680 train_time:65385ms step_avg:87.41ms
step:749/1680 train_time:65473ms step_avg:87.41ms
step:750/1680 train_time:65561ms step_avg:87.41ms
step:750/1680 val_loss:3.5669 train_time:65652ms step_avg:87.54ms
step:751/1680 train_time:65670ms step_avg:87.44ms
step:752/1680 train_time:65742ms step_avg:87.42ms
step:753/1680 train_time:65834ms step_avg:87.43ms
step:754/1680 train_time:65924ms step_avg:87.43ms
step:755/1680 train_time:66012ms step_avg:87.43ms
step:756/1680 train_time:66099ms step_avg:87.43ms
step:757/1680 train_time:66186ms step_avg:87.43ms
step:758/1680 train_time:66274ms step_avg:87.43ms
step:759/1680 train_time:66361ms step_avg:87.43ms
step:760/1680 train_time:66450ms step_avg:87.43ms
step:761/1680 train_time:66537ms step_avg:87.43ms
step:762/1680 train_time:66626ms step_avg:87.44ms
step:763/1680 train_time:66716ms step_avg:87.44ms
step:764/1680 train_time:66806ms step_avg:87.44ms
step:765/1680 train_time:66895ms step_avg:87.44ms
step:766/1680 train_time:66983ms step_avg:87.45ms
step:767/1680 train_time:67071ms step_avg:87.45ms
step:768/1680 train_time:67159ms step_avg:87.45ms
step:769/1680 train_time:67246ms step_avg:87.45ms
step:770/1680 train_time:67333ms step_avg:87.45ms
step:771/1680 train_time:67421ms step_avg:87.45ms
step:772/1680 train_time:67509ms step_avg:87.45ms
step:773/1680 train_time:67597ms step_avg:87.45ms
step:774/1680 train_time:67685ms step_avg:87.45ms
step:775/1680 train_time:67774ms step_avg:87.45ms
step:776/1680 train_time:67863ms step_avg:87.45ms
step:777/1680 train_time:67951ms step_avg:87.45ms
step:778/1680 train_time:68039ms step_avg:87.45ms
step:779/1680 train_time:68127ms step_avg:87.46ms
step:780/1680 train_time:68215ms step_avg:87.45ms
step:781/1680 train_time:68302ms step_avg:87.46ms
step:782/1680 train_time:68390ms step_avg:87.46ms
step:783/1680 train_time:68479ms step_avg:87.46ms
step:784/1680 train_time:68567ms step_avg:87.46ms
step:785/1680 train_time:68657ms step_avg:87.46ms
step:786/1680 train_time:68745ms step_avg:87.46ms
step:787/1680 train_time:68834ms step_avg:87.46ms
step:788/1680 train_time:68922ms step_avg:87.46ms
step:789/1680 train_time:69010ms step_avg:87.47ms
step:790/1680 train_time:69098ms step_avg:87.47ms
step:791/1680 train_time:69186ms step_avg:87.47ms
step:792/1680 train_time:69275ms step_avg:87.47ms
step:793/1680 train_time:69362ms step_avg:87.47ms
step:794/1680 train_time:69450ms step_avg:87.47ms
step:795/1680 train_time:69538ms step_avg:87.47ms
step:796/1680 train_time:69626ms step_avg:87.47ms
step:797/1680 train_time:69715ms step_avg:87.47ms
step:798/1680 train_time:69803ms step_avg:87.47ms
step:799/1680 train_time:69892ms step_avg:87.47ms
step:800/1680 train_time:69979ms step_avg:87.47ms
step:801/1680 train_time:70067ms step_avg:87.47ms
step:802/1680 train_time:70156ms step_avg:87.48ms
step:803/1680 train_time:70244ms step_avg:87.48ms
step:804/1680 train_time:70331ms step_avg:87.48ms
step:805/1680 train_time:70419ms step_avg:87.48ms
step:806/1680 train_time:70507ms step_avg:87.48ms
step:807/1680 train_time:70595ms step_avg:87.48ms
step:808/1680 train_time:70683ms step_avg:87.48ms
step:809/1680 train_time:70772ms step_avg:87.48ms
step:810/1680 train_time:70862ms step_avg:87.48ms
step:811/1680 train_time:70950ms step_avg:87.48ms
step:812/1680 train_time:71038ms step_avg:87.49ms
step:813/1680 train_time:71127ms step_avg:87.49ms
step:814/1680 train_time:71215ms step_avg:87.49ms
step:815/1680 train_time:71303ms step_avg:87.49ms
step:816/1680 train_time:71391ms step_avg:87.49ms
step:817/1680 train_time:71478ms step_avg:87.49ms
step:818/1680 train_time:71567ms step_avg:87.49ms
step:819/1680 train_time:71655ms step_avg:87.49ms
step:820/1680 train_time:71743ms step_avg:87.49ms
step:821/1680 train_time:71832ms step_avg:87.49ms
step:822/1680 train_time:71920ms step_avg:87.49ms
step:823/1680 train_time:72009ms step_avg:87.50ms
step:824/1680 train_time:72097ms step_avg:87.50ms
step:825/1680 train_time:72184ms step_avg:87.50ms
step:826/1680 train_time:72273ms step_avg:87.50ms
step:827/1680 train_time:72361ms step_avg:87.50ms
step:828/1680 train_time:72449ms step_avg:87.50ms
step:829/1680 train_time:72537ms step_avg:87.50ms
step:830/1680 train_time:72625ms step_avg:87.50ms
step:831/1680 train_time:72713ms step_avg:87.50ms
step:832/1680 train_time:72801ms step_avg:87.50ms
step:833/1680 train_time:72889ms step_avg:87.50ms
step:834/1680 train_time:72978ms step_avg:87.50ms
step:835/1680 train_time:73066ms step_avg:87.50ms
step:836/1680 train_time:73154ms step_avg:87.50ms
step:837/1680 train_time:73243ms step_avg:87.51ms
step:838/1680 train_time:73331ms step_avg:87.51ms
step:839/1680 train_time:73419ms step_avg:87.51ms
step:840/1680 train_time:73507ms step_avg:87.51ms
step:841/1680 train_time:73595ms step_avg:87.51ms
step:842/1680 train_time:73683ms step_avg:87.51ms
step:843/1680 train_time:73771ms step_avg:87.51ms
step:844/1680 train_time:73859ms step_avg:87.51ms
step:845/1680 train_time:73947ms step_avg:87.51ms
step:846/1680 train_time:74036ms step_avg:87.51ms
step:847/1680 train_time:74124ms step_avg:87.51ms
step:848/1680 train_time:74213ms step_avg:87.52ms
step:849/1680 train_time:74300ms step_avg:87.52ms
step:850/1680 train_time:74388ms step_avg:87.52ms
step:851/1680 train_time:74477ms step_avg:87.52ms
step:852/1680 train_time:74565ms step_avg:87.52ms
step:853/1680 train_time:74654ms step_avg:87.52ms
step:854/1680 train_time:74741ms step_avg:87.52ms
step:855/1680 train_time:74830ms step_avg:87.52ms
step:856/1680 train_time:74918ms step_avg:87.52ms
step:857/1680 train_time:75006ms step_avg:87.52ms
step:858/1680 train_time:75095ms step_avg:87.52ms
step:859/1680 train_time:75183ms step_avg:87.52ms
step:860/1680 train_time:75271ms step_avg:87.52ms
step:861/1680 train_time:75359ms step_avg:87.53ms
step:862/1680 train_time:75448ms step_avg:87.53ms
step:863/1680 train_time:75536ms step_avg:87.53ms
step:864/1680 train_time:75624ms step_avg:87.53ms
step:865/1680 train_time:75712ms step_avg:87.53ms
step:866/1680 train_time:75801ms step_avg:87.53ms
step:867/1680 train_time:75889ms step_avg:87.53ms
step:868/1680 train_time:75977ms step_avg:87.53ms
step:869/1680 train_time:76065ms step_avg:87.53ms
step:870/1680 train_time:76153ms step_avg:87.53ms
step:871/1680 train_time:76242ms step_avg:87.53ms
step:872/1680 train_time:76331ms step_avg:87.54ms
step:873/1680 train_time:76419ms step_avg:87.54ms
step:874/1680 train_time:76506ms step_avg:87.54ms
step:875/1680 train_time:76594ms step_avg:87.54ms
step:875/1680 val_loss:3.5207 train_time:76683ms step_avg:87.64ms
step:876/1680 train_time:76703ms step_avg:87.56ms
step:877/1680 train_time:76774ms step_avg:87.54ms
step:878/1680 train_time:76867ms step_avg:87.55ms
step:879/1680 train_time:76955ms step_avg:87.55ms
step:880/1680 train_time:77042ms step_avg:87.55ms
step:881/1680 train_time:77129ms step_avg:87.55ms
step:882/1680 train_time:77216ms step_avg:87.55ms
step:883/1680 train_time:77303ms step_avg:87.55ms
step:884/1680 train_time:77390ms step_avg:87.55ms
step:885/1680 train_time:77479ms step_avg:87.55ms
step:886/1680 train_time:77567ms step_avg:87.55ms
step:887/1680 train_time:77657ms step_avg:87.55ms
step:888/1680 train_time:77746ms step_avg:87.55ms
step:889/1680 train_time:77836ms step_avg:87.55ms
step:890/1680 train_time:77924ms step_avg:87.56ms
step:891/1680 train_time:78012ms step_avg:87.56ms
step:892/1680 train_time:78100ms step_avg:87.56ms
step:893/1680 train_time:78188ms step_avg:87.56ms
step:894/1680 train_time:78276ms step_avg:87.56ms
step:895/1680 train_time:78363ms step_avg:87.56ms
step:896/1680 train_time:78451ms step_avg:87.56ms
step:897/1680 train_time:78538ms step_avg:87.56ms
step:898/1680 train_time:78627ms step_avg:87.56ms
step:899/1680 train_time:78716ms step_avg:87.56ms
step:900/1680 train_time:78805ms step_avg:87.56ms
step:901/1680 train_time:78893ms step_avg:87.56ms
step:902/1680 train_time:78982ms step_avg:87.56ms
step:903/1680 train_time:79070ms step_avg:87.56ms
step:904/1680 train_time:79158ms step_avg:87.56ms
step:905/1680 train_time:79246ms step_avg:87.56ms
step:906/1680 train_time:79334ms step_avg:87.56ms
step:907/1680 train_time:79421ms step_avg:87.56ms
step:908/1680 train_time:79510ms step_avg:87.57ms
step:909/1680 train_time:79598ms step_avg:87.57ms
step:910/1680 train_time:79688ms step_avg:87.57ms
step:911/1680 train_time:79776ms step_avg:87.57ms
step:912/1680 train_time:79864ms step_avg:87.57ms
step:913/1680 train_time:79953ms step_avg:87.57ms
step:914/1680 train_time:80042ms step_avg:87.57ms
step:915/1680 train_time:80130ms step_avg:87.57ms
step:916/1680 train_time:80218ms step_avg:87.57ms
step:917/1680 train_time:80306ms step_avg:87.58ms
step:918/1680 train_time:80394ms step_avg:87.58ms
step:919/1680 train_time:80483ms step_avg:87.58ms
step:920/1680 train_time:80572ms step_avg:87.58ms
step:921/1680 train_time:80660ms step_avg:87.58ms
step:922/1680 train_time:80749ms step_avg:87.58ms
step:923/1680 train_time:80837ms step_avg:87.58ms
step:924/1680 train_time:80925ms step_avg:87.58ms
step:925/1680 train_time:81013ms step_avg:87.58ms
step:926/1680 train_time:81102ms step_avg:87.58ms
step:927/1680 train_time:81190ms step_avg:87.58ms
step:928/1680 train_time:81278ms step_avg:87.58ms
step:929/1680 train_time:81366ms step_avg:87.58ms
step:930/1680 train_time:81454ms step_avg:87.59ms
step:931/1680 train_time:81543ms step_avg:87.59ms
step:932/1680 train_time:81632ms step_avg:87.59ms
step:933/1680 train_time:81721ms step_avg:87.59ms
step:934/1680 train_time:81809ms step_avg:87.59ms
step:935/1680 train_time:81898ms step_avg:87.59ms
step:936/1680 train_time:81987ms step_avg:87.59ms
step:937/1680 train_time:82075ms step_avg:87.59ms
step:938/1680 train_time:82163ms step_avg:87.59ms
step:939/1680 train_time:82252ms step_avg:87.59ms
step:940/1680 train_time:82341ms step_avg:87.60ms
step:941/1680 train_time:82428ms step_avg:87.60ms
step:942/1680 train_time:82517ms step_avg:87.60ms
step:943/1680 train_time:82605ms step_avg:87.60ms
step:944/1680 train_time:82693ms step_avg:87.60ms
step:945/1680 train_time:82782ms step_avg:87.60ms
step:946/1680 train_time:82870ms step_avg:87.60ms
step:947/1680 train_time:82959ms step_avg:87.60ms
step:948/1680 train_time:83047ms step_avg:87.60ms
step:949/1680 train_time:83136ms step_avg:87.60ms
step:950/1680 train_time:83224ms step_avg:87.60ms
step:951/1680 train_time:83312ms step_avg:87.60ms
step:952/1680 train_time:83401ms step_avg:87.61ms
step:953/1680 train_time:83488ms step_avg:87.61ms
step:954/1680 train_time:83577ms step_avg:87.61ms
step:955/1680 train_time:83665ms step_avg:87.61ms
step:956/1680 train_time:83753ms step_avg:87.61ms
step:957/1680 train_time:83842ms step_avg:87.61ms
step:958/1680 train_time:83930ms step_avg:87.61ms
step:959/1680 train_time:84019ms step_avg:87.61ms
step:960/1680 train_time:84108ms step_avg:87.61ms
step:961/1680 train_time:84195ms step_avg:87.61ms
step:962/1680 train_time:84284ms step_avg:87.61ms
step:963/1680 train_time:84372ms step_avg:87.61ms
step:964/1680 train_time:84460ms step_avg:87.61ms
step:965/1680 train_time:84548ms step_avg:87.61ms
step:966/1680 train_time:84636ms step_avg:87.61ms
step:967/1680 train_time:84725ms step_avg:87.62ms
step:968/1680 train_time:84813ms step_avg:87.62ms
step:969/1680 train_time:84902ms step_avg:87.62ms
step:970/1680 train_time:84990ms step_avg:87.62ms
step:971/1680 train_time:85078ms step_avg:87.62ms
step:972/1680 train_time:85166ms step_avg:87.62ms
step:973/1680 train_time:85255ms step_avg:87.62ms
step:974/1680 train_time:85342ms step_avg:87.62ms
step:975/1680 train_time:85430ms step_avg:87.62ms
step:976/1680 train_time:85518ms step_avg:87.62ms
step:977/1680 train_time:85606ms step_avg:87.62ms
step:978/1680 train_time:85694ms step_avg:87.62ms
step:979/1680 train_time:85783ms step_avg:87.62ms
step:980/1680 train_time:85872ms step_avg:87.62ms
step:981/1680 train_time:85960ms step_avg:87.63ms
step:982/1680 train_time:86048ms step_avg:87.63ms
step:983/1680 train_time:86137ms step_avg:87.63ms
step:984/1680 train_time:86224ms step_avg:87.63ms
step:985/1680 train_time:86313ms step_avg:87.63ms
step:986/1680 train_time:86401ms step_avg:87.63ms
step:987/1680 train_time:86489ms step_avg:87.63ms
step:988/1680 train_time:86578ms step_avg:87.63ms
step:989/1680 train_time:86666ms step_avg:87.63ms
step:990/1680 train_time:86754ms step_avg:87.63ms
step:991/1680 train_time:86843ms step_avg:87.63ms
step:992/1680 train_time:86932ms step_avg:87.63ms
step:993/1680 train_time:87020ms step_avg:87.63ms
step:994/1680 train_time:87108ms step_avg:87.63ms
step:995/1680 train_time:87197ms step_avg:87.63ms
step:996/1680 train_time:87285ms step_avg:87.64ms
step:997/1680 train_time:87373ms step_avg:87.64ms
step:998/1680 train_time:87461ms step_avg:87.64ms
step:999/1680 train_time:87550ms step_avg:87.64ms
step:1000/1680 train_time:87638ms step_avg:87.64ms
step:1000/1680 val_loss:3.4703 train_time:87728ms step_avg:87.73ms
step:1001/1680 train_time:87747ms step_avg:87.66ms
step:1002/1680 train_time:87817ms step_avg:87.64ms
step:1003/1680 train_time:87910ms step_avg:87.65ms
step:1004/1680 train_time:87999ms step_avg:87.65ms
step:1005/1680 train_time:88088ms step_avg:87.65ms
step:1006/1680 train_time:88176ms step_avg:87.65ms
step:1007/1680 train_time:88263ms step_avg:87.65ms
step:1008/1680 train_time:88351ms step_avg:87.65ms
step:1009/1680 train_time:88438ms step_avg:87.65ms
step:1010/1680 train_time:88527ms step_avg:87.65ms
step:1011/1680 train_time:88614ms step_avg:87.65ms
step:1012/1680 train_time:88704ms step_avg:87.65ms
step:1013/1680 train_time:88794ms step_avg:87.65ms
step:1014/1680 train_time:88884ms step_avg:87.66ms
step:1015/1680 train_time:88973ms step_avg:87.66ms
step:1016/1680 train_time:89062ms step_avg:87.66ms
step:1017/1680 train_time:89150ms step_avg:87.66ms
step:1018/1680 train_time:89239ms step_avg:87.66ms
step:1019/1680 train_time:89327ms step_avg:87.66ms
step:1020/1680 train_time:89414ms step_avg:87.66ms
step:1021/1680 train_time:89502ms step_avg:87.66ms
step:1022/1680 train_time:89590ms step_avg:87.66ms
step:1023/1680 train_time:89677ms step_avg:87.66ms
step:1024/1680 train_time:89767ms step_avg:87.66ms
step:1025/1680 train_time:89856ms step_avg:87.66ms
step:1026/1680 train_time:89945ms step_avg:87.67ms
step:1027/1680 train_time:90034ms step_avg:87.67ms
step:1028/1680 train_time:90122ms step_avg:87.67ms
step:1029/1680 train_time:90210ms step_avg:87.67ms
step:1030/1680 train_time:90298ms step_avg:87.67ms
step:1031/1680 train_time:90385ms step_avg:87.67ms
step:1032/1680 train_time:90473ms step_avg:87.67ms
step:1033/1680 train_time:90561ms step_avg:87.67ms
step:1034/1680 train_time:90649ms step_avg:87.67ms
step:1035/1680 train_time:90738ms step_avg:87.67ms
step:1036/1680 train_time:90827ms step_avg:87.67ms
step:1037/1680 train_time:90915ms step_avg:87.67ms
step:1038/1680 train_time:91004ms step_avg:87.67ms
step:1039/1680 train_time:91092ms step_avg:87.67ms
step:1040/1680 train_time:91180ms step_avg:87.67ms
step:1041/1680 train_time:91268ms step_avg:87.67ms
step:1042/1680 train_time:91356ms step_avg:87.67ms
step:1043/1680 train_time:91444ms step_avg:87.67ms
step:1044/1680 train_time:91532ms step_avg:87.67ms
step:1045/1680 train_time:91621ms step_avg:87.68ms
step:1046/1680 train_time:91709ms step_avg:87.68ms
step:1047/1680 train_time:91797ms step_avg:87.68ms
step:1048/1680 train_time:91886ms step_avg:87.68ms
step:1049/1680 train_time:91974ms step_avg:87.68ms
step:1050/1680 train_time:92063ms step_avg:87.68ms
step:1051/1680 train_time:92152ms step_avg:87.68ms
step:1052/1680 train_time:92239ms step_avg:87.68ms
step:1053/1680 train_time:92327ms step_avg:87.68ms
step:1054/1680 train_time:92416ms step_avg:87.68ms
step:1055/1680 train_time:92505ms step_avg:87.68ms
step:1056/1680 train_time:92593ms step_avg:87.68ms
step:1057/1680 train_time:92681ms step_avg:87.68ms
step:1058/1680 train_time:92770ms step_avg:87.68ms
step:1059/1680 train_time:92858ms step_avg:87.68ms
step:1060/1680 train_time:92948ms step_avg:87.69ms
step:1061/1680 train_time:93035ms step_avg:87.69ms
step:1062/1680 train_time:93124ms step_avg:87.69ms
step:1063/1680 train_time:93212ms step_avg:87.69ms
step:1064/1680 train_time:93300ms step_avg:87.69ms
step:1065/1680 train_time:93389ms step_avg:87.69ms
step:1066/1680 train_time:93477ms step_avg:87.69ms
step:1067/1680 train_time:93565ms step_avg:87.69ms
step:1068/1680 train_time:93653ms step_avg:87.69ms
step:1069/1680 train_time:93741ms step_avg:87.69ms
step:1070/1680 train_time:93830ms step_avg:87.69ms
step:1071/1680 train_time:93919ms step_avg:87.69ms
step:1072/1680 train_time:94007ms step_avg:87.69ms
step:1073/1680 train_time:94095ms step_avg:87.69ms
step:1074/1680 train_time:94184ms step_avg:87.69ms
step:1075/1680 train_time:94272ms step_avg:87.69ms
step:1076/1680 train_time:94360ms step_avg:87.69ms
step:1077/1680 train_time:94448ms step_avg:87.70ms
step:1078/1680 train_time:94536ms step_avg:87.70ms
step:1079/1680 train_time:94624ms step_avg:87.70ms
step:1080/1680 train_time:94714ms step_avg:87.70ms
step:1081/1680 train_time:94802ms step_avg:87.70ms
step:1082/1680 train_time:94890ms step_avg:87.70ms
step:1083/1680 train_time:94978ms step_avg:87.70ms
step:1084/1680 train_time:95067ms step_avg:87.70ms
step:1085/1680 train_time:95155ms step_avg:87.70ms
step:1086/1680 train_time:95243ms step_avg:87.70ms
step:1087/1680 train_time:95331ms step_avg:87.70ms
step:1088/1680 train_time:95419ms step_avg:87.70ms
step:1089/1680 train_time:95507ms step_avg:87.70ms
step:1090/1680 train_time:95595ms step_avg:87.70ms
step:1091/1680 train_time:95683ms step_avg:87.70ms
step:1092/1680 train_time:95772ms step_avg:87.70ms
step:1093/1680 train_time:95860ms step_avg:87.70ms
step:1094/1680 train_time:95949ms step_avg:87.70ms
step:1095/1680 train_time:96037ms step_avg:87.70ms
step:1096/1680 train_time:96126ms step_avg:87.71ms
step:1097/1680 train_time:96214ms step_avg:87.71ms
step:1098/1680 train_time:96303ms step_avg:87.71ms
step:1099/1680 train_time:96392ms step_avg:87.71ms
step:1100/1680 train_time:96481ms step_avg:87.71ms
step:1101/1680 train_time:96569ms step_avg:87.71ms
step:1102/1680 train_time:96659ms step_avg:87.71ms
step:1103/1680 train_time:96748ms step_avg:87.71ms
step:1104/1680 train_time:96837ms step_avg:87.71ms
step:1105/1680 train_time:96927ms step_avg:87.72ms
step:1106/1680 train_time:97017ms step_avg:87.72ms
step:1107/1680 train_time:97106ms step_avg:87.72ms
step:1108/1680 train_time:97195ms step_avg:87.72ms
step:1109/1680 train_time:97284ms step_avg:87.72ms
step:1110/1680 train_time:97373ms step_avg:87.72ms
step:1111/1680 train_time:97461ms step_avg:87.72ms
step:1112/1680 train_time:97550ms step_avg:87.72ms
step:1113/1680 train_time:97639ms step_avg:87.73ms
step:1114/1680 train_time:97728ms step_avg:87.73ms
step:1115/1680 train_time:97817ms step_avg:87.73ms
step:1116/1680 train_time:97907ms step_avg:87.73ms
step:1117/1680 train_time:97996ms step_avg:87.73ms
step:1118/1680 train_time:98086ms step_avg:87.73ms
step:1119/1680 train_time:98174ms step_avg:87.73ms
step:1120/1680 train_time:98263ms step_avg:87.74ms
step:1121/1680 train_time:98352ms step_avg:87.74ms
step:1122/1680 train_time:98441ms step_avg:87.74ms
step:1123/1680 train_time:98530ms step_avg:87.74ms
step:1124/1680 train_time:98618ms step_avg:87.74ms
step:1125/1680 train_time:98707ms step_avg:87.74ms
step:1125/1680 val_loss:3.4176 train_time:98798ms step_avg:87.82ms
step:1126/1680 train_time:98817ms step_avg:87.76ms
step:1127/1680 train_time:98888ms step_avg:87.74ms
step:1128/1680 train_time:98981ms step_avg:87.75ms
step:1129/1680 train_time:99073ms step_avg:87.75ms
step:1130/1680 train_time:99161ms step_avg:87.75ms
step:1131/1680 train_time:99249ms step_avg:87.75ms
step:1132/1680 train_time:99336ms step_avg:87.75ms
step:1133/1680 train_time:99424ms step_avg:87.75ms
step:1134/1680 train_time:99512ms step_avg:87.75ms
step:1135/1680 train_time:99600ms step_avg:87.75ms
step:1136/1680 train_time:99689ms step_avg:87.75ms
step:1137/1680 train_time:99779ms step_avg:87.76ms
step:1138/1680 train_time:99869ms step_avg:87.76ms
step:1139/1680 train_time:99961ms step_avg:87.76ms
step:1140/1680 train_time:100051ms step_avg:87.76ms
step:1141/1680 train_time:100140ms step_avg:87.77ms
step:1142/1680 train_time:100229ms step_avg:87.77ms
step:1143/1680 train_time:100318ms step_avg:87.77ms
step:1144/1680 train_time:100406ms step_avg:87.77ms
step:1145/1680 train_time:100494ms step_avg:87.77ms
step:1146/1680 train_time:100583ms step_avg:87.77ms
step:1147/1680 train_time:100671ms step_avg:87.77ms
step:1148/1680 train_time:100759ms step_avg:87.77ms
step:1149/1680 train_time:100849ms step_avg:87.77ms
step:1150/1680 train_time:100939ms step_avg:87.77ms
step:1151/1680 train_time:101029ms step_avg:87.78ms
step:1152/1680 train_time:101119ms step_avg:87.78ms
step:1153/1680 train_time:101209ms step_avg:87.78ms
step:1154/1680 train_time:101297ms step_avg:87.78ms
step:1155/1680 train_time:101385ms step_avg:87.78ms
step:1156/1680 train_time:101474ms step_avg:87.78ms
step:1157/1680 train_time:101562ms step_avg:87.78ms
step:1158/1680 train_time:101651ms step_avg:87.78ms
step:1159/1680 train_time:101739ms step_avg:87.78ms
step:1160/1680 train_time:101828ms step_avg:87.78ms
step:1161/1680 train_time:101919ms step_avg:87.79ms
step:1162/1680 train_time:102008ms step_avg:87.79ms
step:1163/1680 train_time:102098ms step_avg:87.79ms
step:1164/1680 train_time:102187ms step_avg:87.79ms
step:1165/1680 train_time:102276ms step_avg:87.79ms
step:1166/1680 train_time:102365ms step_avg:87.79ms
step:1167/1680 train_time:102453ms step_avg:87.79ms
step:1168/1680 train_time:102542ms step_avg:87.79ms
step:1169/1680 train_time:102631ms step_avg:87.79ms
step:1170/1680 train_time:102720ms step_avg:87.80ms
step:1171/1680 train_time:102810ms step_avg:87.80ms
step:1172/1680 train_time:102899ms step_avg:87.80ms
step:1173/1680 train_time:102989ms step_avg:87.80ms
step:1174/1680 train_time:103079ms step_avg:87.80ms
step:1175/1680 train_time:103168ms step_avg:87.80ms
step:1176/1680 train_time:103257ms step_avg:87.80ms
step:1177/1680 train_time:103346ms step_avg:87.80ms
step:1178/1680 train_time:103435ms step_avg:87.81ms
step:1179/1680 train_time:103524ms step_avg:87.81ms
step:1180/1680 train_time:103612ms step_avg:87.81ms
step:1181/1680 train_time:103701ms step_avg:87.81ms
step:1182/1680 train_time:103790ms step_avg:87.81ms
step:1183/1680 train_time:103880ms step_avg:87.81ms
step:1184/1680 train_time:103968ms step_avg:87.81ms
step:1185/1680 train_time:104058ms step_avg:87.81ms
step:1186/1680 train_time:104147ms step_avg:87.81ms
step:1187/1680 train_time:104236ms step_avg:87.81ms
step:1188/1680 train_time:104324ms step_avg:87.82ms
step:1189/1680 train_time:104413ms step_avg:87.82ms
step:1190/1680 train_time:104502ms step_avg:87.82ms
step:1191/1680 train_time:104592ms step_avg:87.82ms
step:1192/1680 train_time:104680ms step_avg:87.82ms
step:1193/1680 train_time:104769ms step_avg:87.82ms
step:1194/1680 train_time:104858ms step_avg:87.82ms
step:1195/1680 train_time:104947ms step_avg:87.82ms
step:1196/1680 train_time:105036ms step_avg:87.82ms
step:1197/1680 train_time:105125ms step_avg:87.82ms
step:1198/1680 train_time:105214ms step_avg:87.82ms
step:1199/1680 train_time:105302ms step_avg:87.83ms
step:1200/1680 train_time:105391ms step_avg:87.83ms
step:1201/1680 train_time:105481ms step_avg:87.83ms
step:1202/1680 train_time:105570ms step_avg:87.83ms
step:1203/1680 train_time:105658ms step_avg:87.83ms
step:1204/1680 train_time:105747ms step_avg:87.83ms
step:1205/1680 train_time:105835ms step_avg:87.83ms
step:1206/1680 train_time:105924ms step_avg:87.83ms
step:1207/1680 train_time:106013ms step_avg:87.83ms
step:1208/1680 train_time:106102ms step_avg:87.83ms
step:1209/1680 train_time:106191ms step_avg:87.83ms
step:1210/1680 train_time:106279ms step_avg:87.83ms
step:1211/1680 train_time:106368ms step_avg:87.84ms
step:1212/1680 train_time:106458ms step_avg:87.84ms
step:1213/1680 train_time:106546ms step_avg:87.84ms
step:1214/1680 train_time:106635ms step_avg:87.84ms
step:1215/1680 train_time:106724ms step_avg:87.84ms
step:1216/1680 train_time:106813ms step_avg:87.84ms
step:1217/1680 train_time:106901ms step_avg:87.84ms
step:1218/1680 train_time:106990ms step_avg:87.84ms
step:1219/1680 train_time:107080ms step_avg:87.84ms
step:1220/1680 train_time:107169ms step_avg:87.84ms
step:1221/1680 train_time:107258ms step_avg:87.84ms
step:1222/1680 train_time:107348ms step_avg:87.85ms
step:1223/1680 train_time:107436ms step_avg:87.85ms
step:1224/1680 train_time:107525ms step_avg:87.85ms
step:1225/1680 train_time:107614ms step_avg:87.85ms
step:1226/1680 train_time:107703ms step_avg:87.85ms
step:1227/1680 train_time:107792ms step_avg:87.85ms
step:1228/1680 train_time:107882ms step_avg:87.85ms
step:1229/1680 train_time:107970ms step_avg:87.85ms
step:1230/1680 train_time:108059ms step_avg:87.85ms
step:1231/1680 train_time:108148ms step_avg:87.85ms
step:1232/1680 train_time:108236ms step_avg:87.85ms
step:1233/1680 train_time:108325ms step_avg:87.85ms
step:1234/1680 train_time:108415ms step_avg:87.86ms
step:1235/1680 train_time:108504ms step_avg:87.86ms
step:1236/1680 train_time:108593ms step_avg:87.86ms
step:1237/1680 train_time:108682ms step_avg:87.86ms
step:1238/1680 train_time:108770ms step_avg:87.86ms
step:1239/1680 train_time:108859ms step_avg:87.86ms
step:1240/1680 train_time:108948ms step_avg:87.86ms
step:1241/1680 train_time:109037ms step_avg:87.86ms
step:1242/1680 train_time:109126ms step_avg:87.86ms
step:1243/1680 train_time:109215ms step_avg:87.86ms
step:1244/1680 train_time:109303ms step_avg:87.86ms
step:1245/1680 train_time:109393ms step_avg:87.87ms
step:1246/1680 train_time:109483ms step_avg:87.87ms
step:1247/1680 train_time:109572ms step_avg:87.87ms
step:1248/1680 train_time:109661ms step_avg:87.87ms
step:1249/1680 train_time:109749ms step_avg:87.87ms
step:1250/1680 train_time:109838ms step_avg:87.87ms
step:1250/1680 val_loss:3.3791 train_time:109929ms step_avg:87.94ms
step:1251/1680 train_time:109948ms step_avg:87.89ms
step:1252/1680 train_time:110019ms step_avg:87.87ms
step:1253/1680 train_time:110111ms step_avg:87.88ms
step:1254/1680 train_time:110200ms step_avg:87.88ms
step:1255/1680 train_time:110288ms step_avg:87.88ms
step:1256/1680 train_time:110377ms step_avg:87.88ms
step:1257/1680 train_time:110466ms step_avg:87.88ms
step:1258/1680 train_time:110554ms step_avg:87.88ms
step:1259/1680 train_time:110643ms step_avg:87.88ms
step:1260/1680 train_time:110732ms step_avg:87.88ms
step:1261/1680 train_time:110820ms step_avg:87.88ms
step:1262/1680 train_time:110911ms step_avg:87.89ms
step:1263/1680 train_time:111003ms step_avg:87.89ms
step:1264/1680 train_time:111094ms step_avg:87.89ms
step:1265/1680 train_time:111183ms step_avg:87.89ms
step:1266/1680 train_time:111272ms step_avg:87.89ms
step:1267/1680 train_time:111360ms step_avg:87.89ms
step:1268/1680 train_time:111449ms step_avg:87.89ms
step:1269/1680 train_time:111538ms step_avg:87.89ms
step:1270/1680 train_time:111627ms step_avg:87.89ms
step:1271/1680 train_time:111716ms step_avg:87.90ms
step:1272/1680 train_time:111804ms step_avg:87.90ms
step:1273/1680 train_time:111894ms step_avg:87.90ms
step:1274/1680 train_time:111984ms step_avg:87.90ms
step:1275/1680 train_time:112074ms step_avg:87.90ms
step:1276/1680 train_time:112165ms step_avg:87.90ms
step:1277/1680 train_time:112254ms step_avg:87.90ms
step:1278/1680 train_time:112343ms step_avg:87.91ms
step:1279/1680 train_time:112432ms step_avg:87.91ms
step:1280/1680 train_time:112521ms step_avg:87.91ms
step:1281/1680 train_time:112609ms step_avg:87.91ms
step:1282/1680 train_time:112697ms step_avg:87.91ms
step:1283/1680 train_time:112787ms step_avg:87.91ms
step:1284/1680 train_time:112876ms step_avg:87.91ms
step:1285/1680 train_time:112966ms step_avg:87.91ms
step:1286/1680 train_time:113056ms step_avg:87.91ms
step:1287/1680 train_time:113146ms step_avg:87.91ms
step:1288/1680 train_time:113236ms step_avg:87.92ms
step:1289/1680 train_time:113325ms step_avg:87.92ms
step:1290/1680 train_time:113414ms step_avg:87.92ms
step:1291/1680 train_time:113503ms step_avg:87.92ms
step:1292/1680 train_time:113592ms step_avg:87.92ms
step:1293/1680 train_time:113681ms step_avg:87.92ms
step:1294/1680 train_time:113770ms step_avg:87.92ms
step:1295/1680 train_time:113859ms step_avg:87.92ms
step:1296/1680 train_time:113948ms step_avg:87.92ms
step:1297/1680 train_time:114037ms step_avg:87.92ms
step:1298/1680 train_time:114127ms step_avg:87.93ms
step:1299/1680 train_time:114216ms step_avg:87.93ms
step:1300/1680 train_time:114306ms step_avg:87.93ms
step:1301/1680 train_time:114394ms step_avg:87.93ms
step:1302/1680 train_time:114484ms step_avg:87.93ms
step:1303/1680 train_time:114573ms step_avg:87.93ms
step:1304/1680 train_time:114662ms step_avg:87.93ms
step:1305/1680 train_time:114751ms step_avg:87.93ms
step:1306/1680 train_time:114842ms step_avg:87.93ms
step:1307/1680 train_time:114931ms step_avg:87.93ms
step:1308/1680 train_time:115020ms step_avg:87.94ms
step:1309/1680 train_time:115109ms step_avg:87.94ms
step:1310/1680 train_time:115199ms step_avg:87.94ms
step:1311/1680 train_time:115289ms step_avg:87.94ms
step:1312/1680 train_time:115377ms step_avg:87.94ms
step:1313/1680 train_time:115466ms step_avg:87.94ms
step:1314/1680 train_time:115556ms step_avg:87.94ms
step:1315/1680 train_time:115645ms step_avg:87.94ms
step:1316/1680 train_time:115734ms step_avg:87.94ms
step:1317/1680 train_time:115823ms step_avg:87.94ms
step:1318/1680 train_time:115912ms step_avg:87.95ms
step:1319/1680 train_time:116002ms step_avg:87.95ms
step:1320/1680 train_time:116091ms step_avg:87.95ms
step:1321/1680 train_time:116181ms step_avg:87.95ms
step:1322/1680 train_time:116269ms step_avg:87.95ms
step:1323/1680 train_time:116359ms step_avg:87.95ms
step:1324/1680 train_time:116448ms step_avg:87.95ms
step:1325/1680 train_time:116537ms step_avg:87.95ms
step:1326/1680 train_time:116625ms step_avg:87.95ms
step:1327/1680 train_time:116714ms step_avg:87.95ms
step:1328/1680 train_time:116803ms step_avg:87.95ms
step:1329/1680 train_time:116893ms step_avg:87.96ms
step:1330/1680 train_time:116982ms step_avg:87.96ms
step:1331/1680 train_time:117071ms step_avg:87.96ms
step:1332/1680 train_time:117161ms step_avg:87.96ms
step:1333/1680 train_time:117250ms step_avg:87.96ms
step:1334/1680 train_time:117339ms step_avg:87.96ms
step:1335/1680 train_time:117428ms step_avg:87.96ms
step:1336/1680 train_time:117517ms step_avg:87.96ms
step:1337/1680 train_time:117606ms step_avg:87.96ms
step:1338/1680 train_time:117694ms step_avg:87.96ms
step:1339/1680 train_time:117783ms step_avg:87.96ms
step:1340/1680 train_time:117872ms step_avg:87.96ms
step:1341/1680 train_time:117961ms step_avg:87.96ms
step:1342/1680 train_time:118051ms step_avg:87.97ms
step:1343/1680 train_time:118139ms step_avg:87.97ms
step:1344/1680 train_time:118228ms step_avg:87.97ms
step:1345/1680 train_time:118317ms step_avg:87.97ms
step:1346/1680 train_time:118407ms step_avg:87.97ms
step:1347/1680 train_time:118495ms step_avg:87.97ms
step:1348/1680 train_time:118584ms step_avg:87.97ms
step:1349/1680 train_time:118673ms step_avg:87.97ms
step:1350/1680 train_time:118762ms step_avg:87.97ms
step:1351/1680 train_time:118851ms step_avg:87.97ms
step:1352/1680 train_time:118939ms step_avg:87.97ms
step:1353/1680 train_time:119028ms step_avg:87.97ms
step:1354/1680 train_time:119118ms step_avg:87.98ms
step:1355/1680 train_time:119208ms step_avg:87.98ms
step:1356/1680 train_time:119297ms step_avg:87.98ms
step:1357/1680 train_time:119386ms step_avg:87.98ms
step:1358/1680 train_time:119475ms step_avg:87.98ms
step:1359/1680 train_time:119563ms step_avg:87.98ms
step:1360/1680 train_time:119652ms step_avg:87.98ms
step:1361/1680 train_time:119741ms step_avg:87.98ms
step:1362/1680 train_time:119830ms step_avg:87.98ms
step:1363/1680 train_time:119918ms step_avg:87.98ms
step:1364/1680 train_time:120008ms step_avg:87.98ms
step:1365/1680 train_time:120097ms step_avg:87.98ms
step:1366/1680 train_time:120186ms step_avg:87.98ms
step:1367/1680 train_time:120276ms step_avg:87.99ms
step:1368/1680 train_time:120365ms step_avg:87.99ms
step:1369/1680 train_time:120455ms step_avg:87.99ms
step:1370/1680 train_time:120543ms step_avg:87.99ms
step:1371/1680 train_time:120633ms step_avg:87.99ms
step:1372/1680 train_time:120721ms step_avg:87.99ms
step:1373/1680 train_time:120810ms step_avg:87.99ms
step:1374/1680 train_time:120899ms step_avg:87.99ms
step:1375/1680 train_time:120988ms step_avg:87.99ms
step:1375/1680 val_loss:3.3448 train_time:121080ms step_avg:88.06ms
step:1376/1680 train_time:121098ms step_avg:88.01ms
step:1377/1680 train_time:121174ms step_avg:88.00ms
step:1378/1680 train_time:121268ms step_avg:88.00ms
step:1379/1680 train_time:121357ms step_avg:88.00ms
step:1380/1680 train_time:121446ms step_avg:88.00ms
step:1381/1680 train_time:121534ms step_avg:88.00ms
step:1382/1680 train_time:121622ms step_avg:88.00ms
step:1383/1680 train_time:121710ms step_avg:88.00ms
step:1384/1680 train_time:121799ms step_avg:88.00ms
step:1385/1680 train_time:121886ms step_avg:88.00ms
step:1386/1680 train_time:121974ms step_avg:88.00ms
step:1387/1680 train_time:122063ms step_avg:88.01ms
step:1388/1680 train_time:122154ms step_avg:88.01ms
step:1389/1680 train_time:122246ms step_avg:88.01ms
step:1390/1680 train_time:122336ms step_avg:88.01ms
step:1391/1680 train_time:122427ms step_avg:88.01ms
step:1392/1680 train_time:122516ms step_avg:88.01ms
step:1393/1680 train_time:122604ms step_avg:88.01ms
step:1394/1680 train_time:122692ms step_avg:88.01ms
step:1395/1680 train_time:122780ms step_avg:88.01ms
step:1396/1680 train_time:122868ms step_avg:88.01ms
step:1397/1680 train_time:122957ms step_avg:88.01ms
step:1398/1680 train_time:123046ms step_avg:88.02ms
step:1399/1680 train_time:123136ms step_avg:88.02ms
step:1400/1680 train_time:123225ms step_avg:88.02ms
step:1401/1680 train_time:123316ms step_avg:88.02ms
step:1402/1680 train_time:123407ms step_avg:88.02ms
step:1403/1680 train_time:123496ms step_avg:88.02ms
step:1404/1680 train_time:123585ms step_avg:88.02ms
step:1405/1680 train_time:123673ms step_avg:88.02ms
step:1406/1680 train_time:123761ms step_avg:88.02ms
step:1407/1680 train_time:123849ms step_avg:88.02ms
step:1408/1680 train_time:123938ms step_avg:88.02ms
step:1409/1680 train_time:124027ms step_avg:88.02ms
step:1410/1680 train_time:124116ms step_avg:88.03ms
step:1411/1680 train_time:124205ms step_avg:88.03ms
step:1412/1680 train_time:124295ms step_avg:88.03ms
step:1413/1680 train_time:124385ms step_avg:88.03ms
step:1414/1680 train_time:124475ms step_avg:88.03ms
step:1415/1680 train_time:124564ms step_avg:88.03ms
step:1416/1680 train_time:124652ms step_avg:88.03ms
step:1417/1680 train_time:124740ms step_avg:88.03ms
step:1418/1680 train_time:124829ms step_avg:88.03ms
step:1419/1680 train_time:124918ms step_avg:88.03ms
step:1420/1680 train_time:125006ms step_avg:88.03ms
step:1421/1680 train_time:125096ms step_avg:88.03ms
step:1422/1680 train_time:125185ms step_avg:88.03ms
step:1423/1680 train_time:125275ms step_avg:88.04ms
step:1424/1680 train_time:125364ms step_avg:88.04ms
step:1425/1680 train_time:125453ms step_avg:88.04ms
step:1426/1680 train_time:125541ms step_avg:88.04ms
step:1427/1680 train_time:125630ms step_avg:88.04ms
step:1428/1680 train_time:125719ms step_avg:88.04ms
step:1429/1680 train_time:125808ms step_avg:88.04ms
step:1430/1680 train_time:125897ms step_avg:88.04ms
step:1431/1680 train_time:125985ms step_avg:88.04ms
step:1432/1680 train_time:126074ms step_avg:88.04ms
step:1433/1680 train_time:126163ms step_avg:88.04ms
step:1434/1680 train_time:126253ms step_avg:88.04ms
step:1435/1680 train_time:126342ms step_avg:88.04ms
step:1436/1680 train_time:126431ms step_avg:88.04ms
step:1437/1680 train_time:126520ms step_avg:88.04ms
step:1438/1680 train_time:126609ms step_avg:88.04ms
step:1439/1680 train_time:126698ms step_avg:88.05ms
step:1440/1680 train_time:126787ms step_avg:88.05ms
step:1441/1680 train_time:126876ms step_avg:88.05ms
step:1442/1680 train_time:126965ms step_avg:88.05ms
step:1443/1680 train_time:127054ms step_avg:88.05ms
step:1444/1680 train_time:127143ms step_avg:88.05ms
step:1445/1680 train_time:127232ms step_avg:88.05ms
step:1446/1680 train_time:127321ms step_avg:88.05ms
step:1447/1680 train_time:127411ms step_avg:88.05ms
step:1448/1680 train_time:127500ms step_avg:88.05ms
step:1449/1680 train_time:127589ms step_avg:88.05ms
step:1450/1680 train_time:127678ms step_avg:88.05ms
step:1451/1680 train_time:127767ms step_avg:88.05ms
step:1452/1680 train_time:127856ms step_avg:88.06ms
step:1453/1680 train_time:127945ms step_avg:88.06ms
step:1454/1680 train_time:128034ms step_avg:88.06ms
step:1455/1680 train_time:128123ms step_avg:88.06ms
step:1456/1680 train_time:128213ms step_avg:88.06ms
step:1457/1680 train_time:128302ms step_avg:88.06ms
step:1458/1680 train_time:128392ms step_avg:88.06ms
step:1459/1680 train_time:128480ms step_avg:88.06ms
step:1460/1680 train_time:128569ms step_avg:88.06ms
step:1461/1680 train_time:128658ms step_avg:88.06ms
step:1462/1680 train_time:128747ms step_avg:88.06ms
step:1463/1680 train_time:128836ms step_avg:88.06ms
step:1464/1680 train_time:128926ms step_avg:88.06ms
step:1465/1680 train_time:129015ms step_avg:88.06ms
step:1466/1680 train_time:129103ms step_avg:88.06ms
step:1467/1680 train_time:129193ms step_avg:88.07ms
step:1468/1680 train_time:129282ms step_avg:88.07ms
step:1469/1680 train_time:129372ms step_avg:88.07ms
step:1470/1680 train_time:129460ms step_avg:88.07ms
step:1471/1680 train_time:129549ms step_avg:88.07ms
step:1472/1680 train_time:129638ms step_avg:88.07ms
step:1473/1680 train_time:129727ms step_avg:88.07ms
step:1474/1680 train_time:129815ms step_avg:88.07ms
step:1475/1680 train_time:129906ms step_avg:88.07ms
step:1476/1680 train_time:129994ms step_avg:88.07ms
step:1477/1680 train_time:130082ms step_avg:88.07ms
step:1478/1680 train_time:130172ms step_avg:88.07ms
step:1479/1680 train_time:130262ms step_avg:88.07ms
step:1480/1680 train_time:130353ms step_avg:88.08ms
step:1481/1680 train_time:130442ms step_avg:88.08ms
step:1482/1680 train_time:130532ms step_avg:88.08ms
step:1483/1680 train_time:130620ms step_avg:88.08ms
step:1484/1680 train_time:130709ms step_avg:88.08ms
step:1485/1680 train_time:130798ms step_avg:88.08ms
step:1486/1680 train_time:130888ms step_avg:88.08ms
step:1487/1680 train_time:130976ms step_avg:88.08ms
step:1488/1680 train_time:131065ms step_avg:88.08ms
step:1489/1680 train_time:131155ms step_avg:88.08ms
step:1490/1680 train_time:131243ms step_avg:88.08ms
step:1491/1680 train_time:131334ms step_avg:88.08ms
step:1492/1680 train_time:131422ms step_avg:88.08ms
step:1493/1680 train_time:131512ms step_avg:88.09ms
step:1494/1680 train_time:131601ms step_avg:88.09ms
step:1495/1680 train_time:131691ms step_avg:88.09ms
step:1496/1680 train_time:131779ms step_avg:88.09ms
step:1497/1680 train_time:131867ms step_avg:88.09ms
step:1498/1680 train_time:131957ms step_avg:88.09ms
step:1499/1680 train_time:132046ms step_avg:88.09ms
step:1500/1680 train_time:132135ms step_avg:88.09ms
step:1500/1680 val_loss:3.3152 train_time:132225ms step_avg:88.15ms
step:1501/1680 train_time:132244ms step_avg:88.10ms
step:1502/1680 train_time:132317ms step_avg:88.09ms
step:1503/1680 train_time:132410ms step_avg:88.10ms
step:1504/1680 train_time:132499ms step_avg:88.10ms
step:1505/1680 train_time:132587ms step_avg:88.10ms
step:1506/1680 train_time:132675ms step_avg:88.10ms
step:1507/1680 train_time:132763ms step_avg:88.10ms
step:1508/1680 train_time:132852ms step_avg:88.10ms
step:1509/1680 train_time:132939ms step_avg:88.10ms
step:1510/1680 train_time:133028ms step_avg:88.10ms
step:1511/1680 train_time:133117ms step_avg:88.10ms
step:1512/1680 train_time:133208ms step_avg:88.10ms
step:1513/1680 train_time:133299ms step_avg:88.10ms
step:1514/1680 train_time:133391ms step_avg:88.10ms
step:1515/1680 train_time:133481ms step_avg:88.11ms
step:1516/1680 train_time:133570ms step_avg:88.11ms
step:1517/1680 train_time:133659ms step_avg:88.11ms
step:1518/1680 train_time:133748ms step_avg:88.11ms
step:1519/1680 train_time:133837ms step_avg:88.11ms
step:1520/1680 train_time:133925ms step_avg:88.11ms
step:1521/1680 train_time:134013ms step_avg:88.11ms
step:1522/1680 train_time:134102ms step_avg:88.11ms
step:1523/1680 train_time:134191ms step_avg:88.11ms
step:1524/1680 train_time:134282ms step_avg:88.11ms
step:1525/1680 train_time:134373ms step_avg:88.11ms
step:1526/1680 train_time:134464ms step_avg:88.12ms
step:1527/1680 train_time:134553ms step_avg:88.12ms
step:1528/1680 train_time:134641ms step_avg:88.12ms
step:1529/1680 train_time:134729ms step_avg:88.12ms
step:1530/1680 train_time:134818ms step_avg:88.12ms
step:1531/1680 train_time:134907ms step_avg:88.12ms
step:1532/1680 train_time:134996ms step_avg:88.12ms
step:1533/1680 train_time:135084ms step_avg:88.12ms
step:1534/1680 train_time:135173ms step_avg:88.12ms
step:1535/1680 train_time:135263ms step_avg:88.12ms
step:1536/1680 train_time:135353ms step_avg:88.12ms
step:1537/1680 train_time:135443ms step_avg:88.12ms
step:1538/1680 train_time:135532ms step_avg:88.12ms
step:1539/1680 train_time:135622ms step_avg:88.12ms
step:1540/1680 train_time:135710ms step_avg:88.12ms
step:1541/1680 train_time:135799ms step_avg:88.12ms
step:1542/1680 train_time:135888ms step_avg:88.12ms
step:1543/1680 train_time:135976ms step_avg:88.12ms
step:1544/1680 train_time:136064ms step_avg:88.12ms
step:1545/1680 train_time:136153ms step_avg:88.13ms
step:1546/1680 train_time:136243ms step_avg:88.13ms
step:1547/1680 train_time:136332ms step_avg:88.13ms
step:1548/1680 train_time:136422ms step_avg:88.13ms
step:1549/1680 train_time:136511ms step_avg:88.13ms
step:1550/1680 train_time:136600ms step_avg:88.13ms
step:1551/1680 train_time:136688ms step_avg:88.13ms
step:1552/1680 train_time:136777ms step_avg:88.13ms
step:1553/1680 train_time:136866ms step_avg:88.13ms
step:1554/1680 train_time:136955ms step_avg:88.13ms
step:1555/1680 train_time:137044ms step_avg:88.13ms
step:1556/1680 train_time:137134ms step_avg:88.13ms
step:1557/1680 train_time:137224ms step_avg:88.13ms
step:1558/1680 train_time:137313ms step_avg:88.13ms
step:1559/1680 train_time:137402ms step_avg:88.13ms
step:1560/1680 train_time:137491ms step_avg:88.14ms
step:1561/1680 train_time:137582ms step_avg:88.14ms
step:1562/1680 train_time:137672ms step_avg:88.14ms
step:1563/1680 train_time:137760ms step_avg:88.14ms
step:1564/1680 train_time:137849ms step_avg:88.14ms
step:1565/1680 train_time:137937ms step_avg:88.14ms
step:1566/1680 train_time:138026ms step_avg:88.14ms
step:1567/1680 train_time:138116ms step_avg:88.14ms
step:1568/1680 train_time:138205ms step_avg:88.14ms
step:1569/1680 train_time:138294ms step_avg:88.14ms
step:1570/1680 train_time:138384ms step_avg:88.14ms
step:1571/1680 train_time:138473ms step_avg:88.14ms
step:1572/1680 train_time:138562ms step_avg:88.14ms
step:1573/1680 train_time:138651ms step_avg:88.14ms
step:1574/1680 train_time:138741ms step_avg:88.15ms
step:1575/1680 train_time:138830ms step_avg:88.15ms
step:1576/1680 train_time:138919ms step_avg:88.15ms
step:1577/1680 train_time:139008ms step_avg:88.15ms
step:1578/1680 train_time:139098ms step_avg:88.15ms
step:1579/1680 train_time:139187ms step_avg:88.15ms
step:1580/1680 train_time:139276ms step_avg:88.15ms
step:1581/1680 train_time:139364ms step_avg:88.15ms
step:1582/1680 train_time:139453ms step_avg:88.15ms
step:1583/1680 train_time:139542ms step_avg:88.15ms
step:1584/1680 train_time:139631ms step_avg:88.15ms
step:1585/1680 train_time:139720ms step_avg:88.15ms
step:1586/1680 train_time:139810ms step_avg:88.15ms
step:1587/1680 train_time:139899ms step_avg:88.15ms
step:1588/1680 train_time:139988ms step_avg:88.15ms
step:1589/1680 train_time:140078ms step_avg:88.15ms
step:1590/1680 train_time:140166ms step_avg:88.15ms
step:1591/1680 train_time:140255ms step_avg:88.16ms
step:1592/1680 train_time:140344ms step_avg:88.16ms
step:1593/1680 train_time:140434ms step_avg:88.16ms
step:1594/1680 train_time:140523ms step_avg:88.16ms
step:1595/1680 train_time:140612ms step_avg:88.16ms
step:1596/1680 train_time:140702ms step_avg:88.16ms
step:1597/1680 train_time:140791ms step_avg:88.16ms
step:1598/1680 train_time:140880ms step_avg:88.16ms
step:1599/1680 train_time:140970ms step_avg:88.16ms
step:1600/1680 train_time:141059ms step_avg:88.16ms
step:1601/1680 train_time:141148ms step_avg:88.16ms
step:1602/1680 train_time:141237ms step_avg:88.16ms
step:1603/1680 train_time:141326ms step_avg:88.16ms
step:1604/1680 train_time:141415ms step_avg:88.16ms
step:1605/1680 train_time:141504ms step_avg:88.16ms
step:1606/1680 train_time:141593ms step_avg:88.16ms
step:1607/1680 train_time:141682ms step_avg:88.17ms
step:1608/1680 train_time:141771ms step_avg:88.17ms
step:1609/1680 train_time:141860ms step_avg:88.17ms
step:1610/1680 train_time:141950ms step_avg:88.17ms
step:1611/1680 train_time:142039ms step_avg:88.17ms
step:1612/1680 train_time:142128ms step_avg:88.17ms
step:1613/1680 train_time:142218ms step_avg:88.17ms
step:1614/1680 train_time:142307ms step_avg:88.17ms
step:1615/1680 train_time:142396ms step_avg:88.17ms
step:1616/1680 train_time:142484ms step_avg:88.17ms
step:1617/1680 train_time:142573ms step_avg:88.17ms
step:1618/1680 train_time:142662ms step_avg:88.17ms
step:1619/1680 train_time:142752ms step_avg:88.17ms
step:1620/1680 train_time:142841ms step_avg:88.17ms
step:1621/1680 train_time:142930ms step_avg:88.17ms
step:1622/1680 train_time:143020ms step_avg:88.18ms
step:1623/1680 train_time:143110ms step_avg:88.18ms
step:1624/1680 train_time:143199ms step_avg:88.18ms
step:1625/1680 train_time:143288ms step_avg:88.18ms
step:1625/1680 val_loss:3.2912 train_time:143378ms step_avg:88.23ms
step:1626/1680 train_time:143397ms step_avg:88.19ms
step:1627/1680 train_time:143472ms step_avg:88.18ms
step:1628/1680 train_time:143565ms step_avg:88.19ms
step:1629/1680 train_time:143656ms step_avg:88.19ms
step:1630/1680 train_time:143744ms step_avg:88.19ms
step:1631/1680 train_time:143831ms step_avg:88.19ms
step:1632/1680 train_time:143919ms step_avg:88.19ms
step:1633/1680 train_time:144007ms step_avg:88.19ms
step:1634/1680 train_time:144095ms step_avg:88.19ms
step:1635/1680 train_time:144183ms step_avg:88.19ms
step:1636/1680 train_time:144271ms step_avg:88.19ms
step:1637/1680 train_time:144363ms step_avg:88.19ms
step:1638/1680 train_time:144455ms step_avg:88.19ms
step:1639/1680 train_time:144546ms step_avg:88.19ms
step:1640/1680 train_time:144637ms step_avg:88.19ms
step:1641/1680 train_time:144726ms step_avg:88.19ms
step:1642/1680 train_time:144815ms step_avg:88.19ms
step:1643/1680 train_time:144903ms step_avg:88.19ms
step:1644/1680 train_time:144992ms step_avg:88.19ms
step:1645/1680 train_time:145080ms step_avg:88.19ms
step:1646/1680 train_time:145167ms step_avg:88.19ms
step:1647/1680 train_time:145256ms step_avg:88.19ms
step:1648/1680 train_time:145345ms step_avg:88.19ms
step:1649/1680 train_time:145435ms step_avg:88.20ms
step:1650/1680 train_time:145526ms step_avg:88.20ms
step:1651/1680 train_time:145615ms step_avg:88.20ms
step:1652/1680 train_time:145705ms step_avg:88.20ms
step:1653/1680 train_time:145793ms step_avg:88.20ms
step:1654/1680 train_time:145882ms step_avg:88.20ms
step:1655/1680 train_time:145971ms step_avg:88.20ms
step:1656/1680 train_time:146059ms step_avg:88.20ms
step:1657/1680 train_time:146147ms step_avg:88.20ms
step:1658/1680 train_time:146236ms step_avg:88.20ms
step:1659/1680 train_time:146326ms step_avg:88.20ms
step:1660/1680 train_time:146416ms step_avg:88.20ms
step:1661/1680 train_time:146506ms step_avg:88.20ms
step:1662/1680 train_time:146596ms step_avg:88.20ms
step:1663/1680 train_time:146686ms step_avg:88.21ms
step:1664/1680 train_time:146774ms step_avg:88.21ms
step:1665/1680 train_time:146863ms step_avg:88.21ms
step:1666/1680 train_time:146952ms step_avg:88.21ms
step:1667/1680 train_time:147040ms step_avg:88.21ms
step:1668/1680 train_time:147129ms step_avg:88.21ms
step:1669/1680 train_time:147218ms step_avg:88.21ms
step:1670/1680 train_time:147307ms step_avg:88.21ms
step:1671/1680 train_time:147396ms step_avg:88.21ms
step:1672/1680 train_time:147485ms step_avg:88.21ms
step:1673/1680 train_time:147574ms step_avg:88.21ms
step:1674/1680 train_time:147664ms step_avg:88.21ms
step:1675/1680 train_time:147753ms step_avg:88.21ms
step:1676/1680 train_time:147843ms step_avg:88.21ms
step:1677/1680 train_time:147932ms step_avg:88.21ms
step:1678/1680 train_time:148020ms step_avg:88.21ms
step:1679/1680 train_time:148109ms step_avg:88.21ms
step:1680/1680 train_time:148198ms step_avg:88.21ms
step:1680/1680 val_loss:3.2805 train_time:148289ms step_avg:88.27ms
peak memory allocated: 30760 MiB reserved: 45834 MiB

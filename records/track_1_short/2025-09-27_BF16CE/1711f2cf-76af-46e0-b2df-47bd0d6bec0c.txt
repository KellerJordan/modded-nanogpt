import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:46:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   23C    P0            116W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0            114W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    162824      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    162825      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    162826      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    162827      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    162828      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    162829      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    162830      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    162831      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    162825      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    162826      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    162827      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    162828      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    162829      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    162830      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    162831      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:144ms step_avg:144.30ms
step:2/1680 train_time:164ms step_avg:82.21ms
step:3/1680 train_time:228ms step_avg:76.15ms
step:4/1680 train_time:313ms step_avg:78.37ms
step:5/1680 train_time:399ms step_avg:79.85ms
step:6/1680 train_time:485ms step_avg:80.87ms
step:7/1680 train_time:572ms step_avg:81.76ms
step:8/1680 train_time:658ms step_avg:82.27ms
step:9/1680 train_time:744ms step_avg:82.70ms
step:10/1680 train_time:830ms step_avg:83.03ms
step:11/1680 train_time:916ms step_avg:83.32ms
step:12/1680 train_time:1004ms step_avg:83.68ms
step:13/1680 train_time:1094ms step_avg:84.18ms
step:14/1680 train_time:1185ms step_avg:84.62ms
step:15/1680 train_time:1273ms step_avg:84.84ms
step:16/1680 train_time:1359ms step_avg:84.96ms
step:17/1680 train_time:1446ms step_avg:85.07ms
step:18/1680 train_time:1533ms step_avg:85.14ms
step:19/1680 train_time:1619ms step_avg:85.23ms
step:20/1680 train_time:1706ms step_avg:85.29ms
step:21/1680 train_time:1792ms step_avg:85.35ms
step:22/1680 train_time:1879ms step_avg:85.40ms
step:23/1680 train_time:1965ms step_avg:85.45ms
step:24/1680 train_time:2054ms step_avg:85.60ms
step:25/1680 train_time:2143ms step_avg:85.71ms
step:26/1680 train_time:2231ms step_avg:85.82ms
step:27/1680 train_time:2319ms step_avg:85.90ms
step:28/1680 train_time:2407ms step_avg:85.97ms
step:29/1680 train_time:2494ms step_avg:86.01ms
step:30/1680 train_time:2581ms step_avg:86.04ms
step:31/1680 train_time:2668ms step_avg:86.05ms
step:32/1680 train_time:2754ms step_avg:86.07ms
step:33/1680 train_time:2841ms step_avg:86.10ms
step:34/1680 train_time:2928ms step_avg:86.12ms
step:35/1680 train_time:3016ms step_avg:86.17ms
step:36/1680 train_time:3104ms step_avg:86.21ms
step:37/1680 train_time:3193ms step_avg:86.30ms
step:38/1680 train_time:3280ms step_avg:86.32ms
step:39/1680 train_time:3367ms step_avg:86.34ms
step:40/1680 train_time:3456ms step_avg:86.39ms
step:41/1680 train_time:3543ms step_avg:86.41ms
step:42/1680 train_time:3630ms step_avg:86.42ms
step:43/1680 train_time:3716ms step_avg:86.43ms
step:44/1680 train_time:3803ms step_avg:86.43ms
step:45/1680 train_time:3890ms step_avg:86.44ms
step:46/1680 train_time:3977ms step_avg:86.46ms
step:47/1680 train_time:4065ms step_avg:86.49ms
step:48/1680 train_time:4153ms step_avg:86.53ms
step:49/1680 train_time:4241ms step_avg:86.56ms
step:50/1680 train_time:4328ms step_avg:86.56ms
step:51/1680 train_time:4416ms step_avg:86.58ms
step:52/1680 train_time:4503ms step_avg:86.59ms
step:53/1680 train_time:4589ms step_avg:86.59ms
step:54/1680 train_time:4676ms step_avg:86.59ms
step:55/1680 train_time:4763ms step_avg:86.60ms
step:56/1680 train_time:4850ms step_avg:86.61ms
step:57/1680 train_time:4937ms step_avg:86.62ms
step:58/1680 train_time:5024ms step_avg:86.63ms
step:59/1680 train_time:5111ms step_avg:86.64ms
step:60/1680 train_time:5199ms step_avg:86.65ms
step:61/1680 train_time:5287ms step_avg:86.68ms
step:62/1680 train_time:5375ms step_avg:86.70ms
step:63/1680 train_time:5463ms step_avg:86.71ms
step:64/1680 train_time:5550ms step_avg:86.73ms
step:65/1680 train_time:5638ms step_avg:86.73ms
step:66/1680 train_time:5725ms step_avg:86.74ms
step:67/1680 train_time:5811ms step_avg:86.73ms
step:68/1680 train_time:5899ms step_avg:86.75ms
step:69/1680 train_time:5986ms step_avg:86.75ms
step:70/1680 train_time:6073ms step_avg:86.75ms
step:71/1680 train_time:6160ms step_avg:86.75ms
step:72/1680 train_time:6247ms step_avg:86.77ms
step:73/1680 train_time:6335ms step_avg:86.77ms
step:74/1680 train_time:6422ms step_avg:86.78ms
step:75/1680 train_time:6510ms step_avg:86.80ms
step:76/1680 train_time:6597ms step_avg:86.81ms
step:77/1680 train_time:6684ms step_avg:86.80ms
step:78/1680 train_time:6771ms step_avg:86.81ms
step:79/1680 train_time:6857ms step_avg:86.80ms
step:80/1680 train_time:6945ms step_avg:86.81ms
step:81/1680 train_time:7032ms step_avg:86.81ms
step:82/1680 train_time:7119ms step_avg:86.82ms
step:83/1680 train_time:7207ms step_avg:86.83ms
step:84/1680 train_time:7294ms step_avg:86.84ms
step:85/1680 train_time:7381ms step_avg:86.84ms
step:86/1680 train_time:7468ms step_avg:86.84ms
step:87/1680 train_time:7556ms step_avg:86.85ms
step:88/1680 train_time:7643ms step_avg:86.85ms
step:89/1680 train_time:7730ms step_avg:86.85ms
step:90/1680 train_time:7817ms step_avg:86.85ms
step:91/1680 train_time:7903ms step_avg:86.85ms
step:92/1680 train_time:7990ms step_avg:86.85ms
step:93/1680 train_time:8077ms step_avg:86.85ms
step:94/1680 train_time:8164ms step_avg:86.85ms
step:95/1680 train_time:8250ms step_avg:86.84ms
step:96/1680 train_time:8338ms step_avg:86.85ms
step:97/1680 train_time:8425ms step_avg:86.86ms
step:98/1680 train_time:8513ms step_avg:86.87ms
step:99/1680 train_time:8600ms step_avg:86.87ms
step:100/1680 train_time:8687ms step_avg:86.87ms
step:101/1680 train_time:8774ms step_avg:86.88ms
step:102/1680 train_time:8861ms step_avg:86.87ms
step:103/1680 train_time:8949ms step_avg:86.89ms
step:104/1680 train_time:9036ms step_avg:86.89ms
step:105/1680 train_time:9123ms step_avg:86.88ms
step:106/1680 train_time:9210ms step_avg:86.89ms
step:107/1680 train_time:9298ms step_avg:86.90ms
step:108/1680 train_time:9385ms step_avg:86.90ms
step:109/1680 train_time:9472ms step_avg:86.90ms
step:110/1680 train_time:9559ms step_avg:86.90ms
step:111/1680 train_time:9646ms step_avg:86.90ms
step:112/1680 train_time:9734ms step_avg:86.91ms
step:113/1680 train_time:9821ms step_avg:86.91ms
step:114/1680 train_time:9908ms step_avg:86.91ms
step:115/1680 train_time:9995ms step_avg:86.91ms
step:116/1680 train_time:10081ms step_avg:86.91ms
step:117/1680 train_time:10168ms step_avg:86.91ms
step:118/1680 train_time:10256ms step_avg:86.91ms
step:119/1680 train_time:10342ms step_avg:86.91ms
step:120/1680 train_time:10430ms step_avg:86.91ms
step:121/1680 train_time:10517ms step_avg:86.92ms
step:122/1680 train_time:10604ms step_avg:86.92ms
step:123/1680 train_time:10691ms step_avg:86.92ms
step:124/1680 train_time:10778ms step_avg:86.92ms
step:125/1680 train_time:10866ms step_avg:86.93ms
step:125/1680 val_loss:4.3230 train_time:10955ms step_avg:87.64ms
step:126/1680 train_time:10974ms step_avg:87.10ms
step:127/1680 train_time:11044ms step_avg:86.96ms
step:128/1680 train_time:11140ms step_avg:87.03ms
step:129/1680 train_time:11232ms step_avg:87.07ms
step:130/1680 train_time:11319ms step_avg:87.07ms
step:131/1680 train_time:11405ms step_avg:87.06ms
step:132/1680 train_time:11492ms step_avg:87.06ms
step:133/1680 train_time:11578ms step_avg:87.05ms
step:134/1680 train_time:11664ms step_avg:87.04ms
step:135/1680 train_time:11750ms step_avg:87.03ms
step:136/1680 train_time:11835ms step_avg:87.02ms
step:137/1680 train_time:11921ms step_avg:87.01ms
step:138/1680 train_time:12007ms step_avg:87.01ms
step:139/1680 train_time:12096ms step_avg:87.02ms
step:140/1680 train_time:12186ms step_avg:87.04ms
step:141/1680 train_time:12274ms step_avg:87.05ms
step:142/1680 train_time:12362ms step_avg:87.05ms
step:143/1680 train_time:12448ms step_avg:87.05ms
step:144/1680 train_time:12535ms step_avg:87.05ms
step:145/1680 train_time:12622ms step_avg:87.05ms
step:146/1680 train_time:12707ms step_avg:87.04ms
step:147/1680 train_time:12794ms step_avg:87.03ms
step:148/1680 train_time:12881ms step_avg:87.03ms
step:149/1680 train_time:12967ms step_avg:87.02ms
step:150/1680 train_time:13055ms step_avg:87.03ms
step:151/1680 train_time:13143ms step_avg:87.04ms
step:152/1680 train_time:13231ms step_avg:87.05ms
step:153/1680 train_time:13318ms step_avg:87.05ms
step:154/1680 train_time:13406ms step_avg:87.05ms
step:155/1680 train_time:13493ms step_avg:87.05ms
step:156/1680 train_time:13580ms step_avg:87.05ms
step:157/1680 train_time:13666ms step_avg:87.05ms
step:158/1680 train_time:13754ms step_avg:87.05ms
step:159/1680 train_time:13840ms step_avg:87.04ms
step:160/1680 train_time:13926ms step_avg:87.04ms
step:161/1680 train_time:14013ms step_avg:87.04ms
step:162/1680 train_time:14101ms step_avg:87.04ms
step:163/1680 train_time:14189ms step_avg:87.05ms
step:164/1680 train_time:14277ms step_avg:87.05ms
step:165/1680 train_time:14364ms step_avg:87.05ms
step:166/1680 train_time:14452ms step_avg:87.06ms
step:167/1680 train_time:14538ms step_avg:87.06ms
step:168/1680 train_time:14625ms step_avg:87.05ms
step:169/1680 train_time:14712ms step_avg:87.05ms
step:170/1680 train_time:14799ms step_avg:87.05ms
step:171/1680 train_time:14886ms step_avg:87.05ms
step:172/1680 train_time:14973ms step_avg:87.05ms
step:173/1680 train_time:15060ms step_avg:87.05ms
step:174/1680 train_time:15148ms step_avg:87.06ms
step:175/1680 train_time:15236ms step_avg:87.06ms
step:176/1680 train_time:15324ms step_avg:87.07ms
step:177/1680 train_time:15412ms step_avg:87.07ms
step:178/1680 train_time:15499ms step_avg:87.07ms
step:179/1680 train_time:15586ms step_avg:87.07ms
step:180/1680 train_time:15673ms step_avg:87.07ms
step:181/1680 train_time:15759ms step_avg:87.07ms
step:182/1680 train_time:15846ms step_avg:87.07ms
step:183/1680 train_time:15933ms step_avg:87.07ms
step:184/1680 train_time:16019ms step_avg:87.06ms
step:185/1680 train_time:16107ms step_avg:87.07ms
step:186/1680 train_time:16195ms step_avg:87.07ms
step:187/1680 train_time:16282ms step_avg:87.07ms
step:188/1680 train_time:16369ms step_avg:87.07ms
step:189/1680 train_time:16456ms step_avg:87.07ms
step:190/1680 train_time:16543ms step_avg:87.07ms
step:191/1680 train_time:16631ms step_avg:87.07ms
step:192/1680 train_time:16717ms step_avg:87.07ms
step:193/1680 train_time:16804ms step_avg:87.07ms
step:194/1680 train_time:16890ms step_avg:87.06ms
step:195/1680 train_time:16977ms step_avg:87.06ms
step:196/1680 train_time:17064ms step_avg:87.06ms
step:197/1680 train_time:17152ms step_avg:87.07ms
step:198/1680 train_time:17239ms step_avg:87.07ms
step:199/1680 train_time:17326ms step_avg:87.07ms
step:200/1680 train_time:17414ms step_avg:87.07ms
step:201/1680 train_time:17500ms step_avg:87.06ms
step:202/1680 train_time:17587ms step_avg:87.07ms
step:203/1680 train_time:17675ms step_avg:87.07ms
step:204/1680 train_time:17761ms step_avg:87.07ms
step:205/1680 train_time:17848ms step_avg:87.06ms
step:206/1680 train_time:17935ms step_avg:87.06ms
step:207/1680 train_time:18022ms step_avg:87.06ms
step:208/1680 train_time:18110ms step_avg:87.07ms
step:209/1680 train_time:18197ms step_avg:87.07ms
step:210/1680 train_time:18284ms step_avg:87.07ms
step:211/1680 train_time:18372ms step_avg:87.07ms
step:212/1680 train_time:18458ms step_avg:87.07ms
step:213/1680 train_time:18546ms step_avg:87.07ms
step:214/1680 train_time:18633ms step_avg:87.07ms
step:215/1680 train_time:18719ms step_avg:87.07ms
step:216/1680 train_time:18807ms step_avg:87.07ms
step:217/1680 train_time:18894ms step_avg:87.07ms
step:218/1680 train_time:18980ms step_avg:87.07ms
step:219/1680 train_time:19067ms step_avg:87.06ms
step:220/1680 train_time:19154ms step_avg:87.07ms
step:221/1680 train_time:19242ms step_avg:87.07ms
step:222/1680 train_time:19329ms step_avg:87.07ms
step:223/1680 train_time:19415ms step_avg:87.06ms
step:224/1680 train_time:19502ms step_avg:87.06ms
step:225/1680 train_time:19589ms step_avg:87.06ms
step:226/1680 train_time:19676ms step_avg:87.06ms
step:227/1680 train_time:19763ms step_avg:87.06ms
step:228/1680 train_time:19850ms step_avg:87.06ms
step:229/1680 train_time:19937ms step_avg:87.06ms
step:230/1680 train_time:20024ms step_avg:87.06ms
step:231/1680 train_time:20112ms step_avg:87.06ms
step:232/1680 train_time:20199ms step_avg:87.06ms
step:233/1680 train_time:20285ms step_avg:87.06ms
step:234/1680 train_time:20373ms step_avg:87.06ms
step:235/1680 train_time:20460ms step_avg:87.06ms
step:236/1680 train_time:20546ms step_avg:87.06ms
step:237/1680 train_time:20633ms step_avg:87.06ms
step:238/1680 train_time:20721ms step_avg:87.06ms
step:239/1680 train_time:20808ms step_avg:87.06ms
step:240/1680 train_time:20895ms step_avg:87.06ms
step:241/1680 train_time:20982ms step_avg:87.06ms
step:242/1680 train_time:21069ms step_avg:87.06ms
step:243/1680 train_time:21156ms step_avg:87.06ms
step:244/1680 train_time:21243ms step_avg:87.06ms
step:245/1680 train_time:21331ms step_avg:87.06ms
step:246/1680 train_time:21417ms step_avg:87.06ms
step:247/1680 train_time:21504ms step_avg:87.06ms
step:248/1680 train_time:21592ms step_avg:87.06ms
step:249/1680 train_time:21679ms step_avg:87.06ms
step:250/1680 train_time:21767ms step_avg:87.07ms
step:250/1680 val_loss:3.9722 train_time:21856ms step_avg:87.42ms
step:251/1680 train_time:21877ms step_avg:87.16ms
step:252/1680 train_time:21945ms step_avg:87.08ms
step:253/1680 train_time:22038ms step_avg:87.11ms
step:254/1680 train_time:22126ms step_avg:87.11ms
step:255/1680 train_time:22213ms step_avg:87.11ms
step:256/1680 train_time:22300ms step_avg:87.11ms
step:257/1680 train_time:22386ms step_avg:87.10ms
step:258/1680 train_time:22472ms step_avg:87.10ms
step:259/1680 train_time:22558ms step_avg:87.10ms
step:260/1680 train_time:22644ms step_avg:87.09ms
step:261/1680 train_time:22730ms step_avg:87.09ms
step:262/1680 train_time:22818ms step_avg:87.09ms
step:263/1680 train_time:22906ms step_avg:87.10ms
step:264/1680 train_time:22996ms step_avg:87.11ms
step:265/1680 train_time:23085ms step_avg:87.11ms
step:266/1680 train_time:23173ms step_avg:87.12ms
step:267/1680 train_time:23260ms step_avg:87.12ms
step:268/1680 train_time:23346ms step_avg:87.11ms
step:269/1680 train_time:23433ms step_avg:87.11ms
step:270/1680 train_time:23519ms step_avg:87.11ms
step:271/1680 train_time:23605ms step_avg:87.10ms
step:272/1680 train_time:23691ms step_avg:87.10ms
step:273/1680 train_time:23778ms step_avg:87.10ms
step:274/1680 train_time:23865ms step_avg:87.10ms
step:275/1680 train_time:23953ms step_avg:87.10ms
step:276/1680 train_time:24042ms step_avg:87.11ms
step:277/1680 train_time:24131ms step_avg:87.12ms
step:278/1680 train_time:24218ms step_avg:87.11ms
step:279/1680 train_time:24305ms step_avg:87.12ms
step:280/1680 train_time:24392ms step_avg:87.11ms
step:281/1680 train_time:24479ms step_avg:87.11ms
step:282/1680 train_time:24565ms step_avg:87.11ms
step:283/1680 train_time:24652ms step_avg:87.11ms
step:284/1680 train_time:24738ms step_avg:87.11ms
step:285/1680 train_time:24825ms step_avg:87.11ms
step:286/1680 train_time:24913ms step_avg:87.11ms
step:287/1680 train_time:25000ms step_avg:87.11ms
step:288/1680 train_time:25089ms step_avg:87.11ms
step:289/1680 train_time:25176ms step_avg:87.12ms
step:290/1680 train_time:25264ms step_avg:87.12ms
step:291/1680 train_time:25350ms step_avg:87.11ms
step:292/1680 train_time:25437ms step_avg:87.11ms
step:293/1680 train_time:25523ms step_avg:87.11ms
step:294/1680 train_time:25610ms step_avg:87.11ms
step:295/1680 train_time:25696ms step_avg:87.11ms
step:296/1680 train_time:25783ms step_avg:87.10ms
step:297/1680 train_time:25870ms step_avg:87.11ms
step:298/1680 train_time:25958ms step_avg:87.11ms
step:299/1680 train_time:26046ms step_avg:87.11ms
step:300/1680 train_time:26133ms step_avg:87.11ms
step:301/1680 train_time:26220ms step_avg:87.11ms
step:302/1680 train_time:26307ms step_avg:87.11ms
step:303/1680 train_time:26394ms step_avg:87.11ms
step:304/1680 train_time:26481ms step_avg:87.11ms
step:305/1680 train_time:26569ms step_avg:87.11ms
step:306/1680 train_time:26655ms step_avg:87.11ms
step:307/1680 train_time:26742ms step_avg:87.11ms
step:308/1680 train_time:26829ms step_avg:87.11ms
step:309/1680 train_time:26917ms step_avg:87.11ms
step:310/1680 train_time:27004ms step_avg:87.11ms
step:311/1680 train_time:27092ms step_avg:87.11ms
step:312/1680 train_time:27179ms step_avg:87.11ms
step:313/1680 train_time:27266ms step_avg:87.11ms
step:314/1680 train_time:27353ms step_avg:87.11ms
step:315/1680 train_time:27440ms step_avg:87.11ms
step:316/1680 train_time:27526ms step_avg:87.11ms
step:317/1680 train_time:27613ms step_avg:87.11ms
step:318/1680 train_time:27700ms step_avg:87.11ms
step:319/1680 train_time:27788ms step_avg:87.11ms
step:320/1680 train_time:27874ms step_avg:87.11ms
step:321/1680 train_time:27961ms step_avg:87.11ms
step:322/1680 train_time:28048ms step_avg:87.11ms
step:323/1680 train_time:28136ms step_avg:87.11ms
step:324/1680 train_time:28223ms step_avg:87.11ms
step:325/1680 train_time:28311ms step_avg:87.11ms
step:326/1680 train_time:28398ms step_avg:87.11ms
step:327/1680 train_time:28485ms step_avg:87.11ms
step:328/1680 train_time:28572ms step_avg:87.11ms
step:329/1680 train_time:28659ms step_avg:87.11ms
step:330/1680 train_time:28746ms step_avg:87.11ms
step:331/1680 train_time:28832ms step_avg:87.11ms
step:332/1680 train_time:28920ms step_avg:87.11ms
step:333/1680 train_time:29007ms step_avg:87.11ms
step:334/1680 train_time:29094ms step_avg:87.11ms
step:335/1680 train_time:29182ms step_avg:87.11ms
step:336/1680 train_time:29269ms step_avg:87.11ms
step:337/1680 train_time:29356ms step_avg:87.11ms
step:338/1680 train_time:29442ms step_avg:87.11ms
step:339/1680 train_time:29529ms step_avg:87.11ms
step:340/1680 train_time:29616ms step_avg:87.11ms
step:341/1680 train_time:29703ms step_avg:87.11ms
step:342/1680 train_time:29790ms step_avg:87.11ms
step:343/1680 train_time:29877ms step_avg:87.11ms
step:344/1680 train_time:29964ms step_avg:87.10ms
step:345/1680 train_time:30051ms step_avg:87.11ms
step:346/1680 train_time:30138ms step_avg:87.10ms
step:347/1680 train_time:30225ms step_avg:87.10ms
step:348/1680 train_time:30313ms step_avg:87.11ms
step:349/1680 train_time:30400ms step_avg:87.11ms
step:350/1680 train_time:30487ms step_avg:87.11ms
step:351/1680 train_time:30574ms step_avg:87.10ms
step:352/1680 train_time:30661ms step_avg:87.10ms
step:353/1680 train_time:30748ms step_avg:87.11ms
step:354/1680 train_time:30836ms step_avg:87.11ms
step:355/1680 train_time:30922ms step_avg:87.10ms
step:356/1680 train_time:31010ms step_avg:87.11ms
step:357/1680 train_time:31097ms step_avg:87.11ms
step:358/1680 train_time:31184ms step_avg:87.11ms
step:359/1680 train_time:31272ms step_avg:87.11ms
step:360/1680 train_time:31359ms step_avg:87.11ms
step:361/1680 train_time:31446ms step_avg:87.11ms
step:362/1680 train_time:31533ms step_avg:87.11ms
step:363/1680 train_time:31620ms step_avg:87.11ms
step:364/1680 train_time:31708ms step_avg:87.11ms
step:365/1680 train_time:31795ms step_avg:87.11ms
step:366/1680 train_time:31882ms step_avg:87.11ms
step:367/1680 train_time:31970ms step_avg:87.11ms
step:368/1680 train_time:32057ms step_avg:87.11ms
step:369/1680 train_time:32144ms step_avg:87.11ms
step:370/1680 train_time:32232ms step_avg:87.11ms
step:371/1680 train_time:32318ms step_avg:87.11ms
step:372/1680 train_time:32404ms step_avg:87.11ms
step:373/1680 train_time:32491ms step_avg:87.11ms
step:374/1680 train_time:32577ms step_avg:87.11ms
step:375/1680 train_time:32665ms step_avg:87.11ms
step:375/1680 val_loss:3.8220 train_time:32753ms step_avg:87.34ms
step:376/1680 train_time:32772ms step_avg:87.16ms
step:377/1680 train_time:32841ms step_avg:87.11ms
step:378/1680 train_time:32930ms step_avg:87.12ms
step:379/1680 train_time:33016ms step_avg:87.11ms
step:380/1680 train_time:33103ms step_avg:87.11ms
step:381/1680 train_time:33189ms step_avg:87.11ms
step:382/1680 train_time:33274ms step_avg:87.11ms
step:383/1680 train_time:33362ms step_avg:87.11ms
step:384/1680 train_time:33448ms step_avg:87.10ms
step:385/1680 train_time:33534ms step_avg:87.10ms
step:386/1680 train_time:33621ms step_avg:87.10ms
step:387/1680 train_time:33709ms step_avg:87.10ms
step:388/1680 train_time:33798ms step_avg:87.11ms
step:389/1680 train_time:33886ms step_avg:87.11ms
step:390/1680 train_time:33973ms step_avg:87.11ms
step:391/1680 train_time:34062ms step_avg:87.11ms
step:392/1680 train_time:34148ms step_avg:87.11ms
step:393/1680 train_time:34235ms step_avg:87.11ms
step:394/1680 train_time:34322ms step_avg:87.11ms
step:395/1680 train_time:34408ms step_avg:87.11ms
step:396/1680 train_time:34494ms step_avg:87.11ms
step:397/1680 train_time:34581ms step_avg:87.11ms
step:398/1680 train_time:34668ms step_avg:87.10ms
step:399/1680 train_time:34756ms step_avg:87.11ms
step:400/1680 train_time:34845ms step_avg:87.11ms
step:401/1680 train_time:34933ms step_avg:87.11ms
step:402/1680 train_time:35021ms step_avg:87.12ms
step:403/1680 train_time:35109ms step_avg:87.12ms
step:404/1680 train_time:35195ms step_avg:87.12ms
step:405/1680 train_time:35282ms step_avg:87.12ms
step:406/1680 train_time:35369ms step_avg:87.12ms
step:407/1680 train_time:35455ms step_avg:87.11ms
step:408/1680 train_time:35542ms step_avg:87.11ms
step:409/1680 train_time:35629ms step_avg:87.11ms
step:410/1680 train_time:35717ms step_avg:87.11ms
step:411/1680 train_time:35804ms step_avg:87.11ms
step:412/1680 train_time:35892ms step_avg:87.12ms
step:413/1680 train_time:35979ms step_avg:87.12ms
step:414/1680 train_time:36066ms step_avg:87.12ms
step:415/1680 train_time:36154ms step_avg:87.12ms
step:416/1680 train_time:36240ms step_avg:87.12ms
step:417/1680 train_time:36327ms step_avg:87.12ms
step:418/1680 train_time:36414ms step_avg:87.11ms
step:419/1680 train_time:36501ms step_avg:87.11ms
step:420/1680 train_time:36587ms step_avg:87.11ms
step:421/1680 train_time:36675ms step_avg:87.11ms
step:422/1680 train_time:36763ms step_avg:87.11ms
step:423/1680 train_time:36850ms step_avg:87.12ms
step:424/1680 train_time:36938ms step_avg:87.12ms
step:425/1680 train_time:37025ms step_avg:87.12ms
step:426/1680 train_time:37112ms step_avg:87.12ms
step:427/1680 train_time:37199ms step_avg:87.12ms
step:428/1680 train_time:37285ms step_avg:87.12ms
step:429/1680 train_time:37372ms step_avg:87.11ms
step:430/1680 train_time:37460ms step_avg:87.12ms
step:431/1680 train_time:37546ms step_avg:87.11ms
step:432/1680 train_time:37634ms step_avg:87.11ms
step:433/1680 train_time:37721ms step_avg:87.11ms
step:434/1680 train_time:37808ms step_avg:87.11ms
step:435/1680 train_time:37896ms step_avg:87.12ms
step:436/1680 train_time:37983ms step_avg:87.12ms
step:437/1680 train_time:38070ms step_avg:87.12ms
step:438/1680 train_time:38157ms step_avg:87.12ms
step:439/1680 train_time:38244ms step_avg:87.12ms
step:440/1680 train_time:38330ms step_avg:87.11ms
step:441/1680 train_time:38417ms step_avg:87.11ms
step:442/1680 train_time:38504ms step_avg:87.11ms
step:443/1680 train_time:38590ms step_avg:87.11ms
step:444/1680 train_time:38677ms step_avg:87.11ms
step:445/1680 train_time:38764ms step_avg:87.11ms
step:446/1680 train_time:38851ms step_avg:87.11ms
step:447/1680 train_time:38939ms step_avg:87.11ms
step:448/1680 train_time:39026ms step_avg:87.11ms
step:449/1680 train_time:39113ms step_avg:87.11ms
step:450/1680 train_time:39200ms step_avg:87.11ms
step:451/1680 train_time:39286ms step_avg:87.11ms
step:452/1680 train_time:39374ms step_avg:87.11ms
step:453/1680 train_time:39461ms step_avg:87.11ms
step:454/1680 train_time:39547ms step_avg:87.11ms
step:455/1680 train_time:39635ms step_avg:87.11ms
step:456/1680 train_time:39723ms step_avg:87.11ms
step:457/1680 train_time:39810ms step_avg:87.11ms
step:458/1680 train_time:39897ms step_avg:87.11ms
step:459/1680 train_time:39984ms step_avg:87.11ms
step:460/1680 train_time:40071ms step_avg:87.11ms
step:461/1680 train_time:40158ms step_avg:87.11ms
step:462/1680 train_time:40245ms step_avg:87.11ms
step:463/1680 train_time:40332ms step_avg:87.11ms
step:464/1680 train_time:40420ms step_avg:87.11ms
step:465/1680 train_time:40506ms step_avg:87.11ms
step:466/1680 train_time:40594ms step_avg:87.11ms
step:467/1680 train_time:40681ms step_avg:87.11ms
step:468/1680 train_time:40768ms step_avg:87.11ms
step:469/1680 train_time:40855ms step_avg:87.11ms
step:470/1680 train_time:40943ms step_avg:87.11ms
step:471/1680 train_time:41029ms step_avg:87.11ms
step:472/1680 train_time:41116ms step_avg:87.11ms
step:473/1680 train_time:41203ms step_avg:87.11ms
step:474/1680 train_time:41290ms step_avg:87.11ms
step:475/1680 train_time:41377ms step_avg:87.11ms
step:476/1680 train_time:41464ms step_avg:87.11ms
step:477/1680 train_time:41551ms step_avg:87.11ms
step:478/1680 train_time:41638ms step_avg:87.11ms
step:479/1680 train_time:41724ms step_avg:87.11ms
step:480/1680 train_time:41811ms step_avg:87.11ms
step:481/1680 train_time:41898ms step_avg:87.11ms
step:482/1680 train_time:41985ms step_avg:87.11ms
step:483/1680 train_time:42072ms step_avg:87.11ms
step:484/1680 train_time:42159ms step_avg:87.11ms
step:485/1680 train_time:42247ms step_avg:87.11ms
step:486/1680 train_time:42334ms step_avg:87.11ms
step:487/1680 train_time:42421ms step_avg:87.11ms
step:488/1680 train_time:42508ms step_avg:87.11ms
step:489/1680 train_time:42596ms step_avg:87.11ms
step:490/1680 train_time:42683ms step_avg:87.11ms
step:491/1680 train_time:42769ms step_avg:87.11ms
step:492/1680 train_time:42857ms step_avg:87.11ms
step:493/1680 train_time:42944ms step_avg:87.11ms
step:494/1680 train_time:43030ms step_avg:87.11ms
step:495/1680 train_time:43118ms step_avg:87.11ms
step:496/1680 train_time:43205ms step_avg:87.11ms
step:497/1680 train_time:43292ms step_avg:87.11ms
step:498/1680 train_time:43379ms step_avg:87.11ms
step:499/1680 train_time:43466ms step_avg:87.11ms
step:500/1680 train_time:43554ms step_avg:87.11ms
step:500/1680 val_loss:3.7208 train_time:43643ms step_avg:87.29ms
step:501/1680 train_time:43662ms step_avg:87.15ms
step:502/1680 train_time:43732ms step_avg:87.12ms
step:503/1680 train_time:43825ms step_avg:87.13ms
step:504/1680 train_time:43915ms step_avg:87.13ms
step:505/1680 train_time:44001ms step_avg:87.13ms
step:506/1680 train_time:44088ms step_avg:87.13ms
step:507/1680 train_time:44175ms step_avg:87.13ms
step:508/1680 train_time:44261ms step_avg:87.13ms
step:509/1680 train_time:44347ms step_avg:87.13ms
step:510/1680 train_time:44433ms step_avg:87.12ms
step:511/1680 train_time:44519ms step_avg:87.12ms
step:512/1680 train_time:44606ms step_avg:87.12ms
step:513/1680 train_time:44694ms step_avg:87.12ms
step:514/1680 train_time:44783ms step_avg:87.13ms
step:515/1680 train_time:44871ms step_avg:87.13ms
step:516/1680 train_time:44959ms step_avg:87.13ms
step:517/1680 train_time:45046ms step_avg:87.13ms
step:518/1680 train_time:45132ms step_avg:87.13ms
step:519/1680 train_time:45219ms step_avg:87.13ms
step:520/1680 train_time:45306ms step_avg:87.13ms
step:521/1680 train_time:45393ms step_avg:87.13ms
step:522/1680 train_time:45480ms step_avg:87.13ms
step:523/1680 train_time:45566ms step_avg:87.12ms
step:524/1680 train_time:45654ms step_avg:87.13ms
step:525/1680 train_time:45742ms step_avg:87.13ms
step:526/1680 train_time:45830ms step_avg:87.13ms
step:527/1680 train_time:45918ms step_avg:87.13ms
step:528/1680 train_time:46005ms step_avg:87.13ms
step:529/1680 train_time:46094ms step_avg:87.13ms
step:530/1680 train_time:46180ms step_avg:87.13ms
step:531/1680 train_time:46267ms step_avg:87.13ms
step:532/1680 train_time:46354ms step_avg:87.13ms
step:533/1680 train_time:46440ms step_avg:87.13ms
step:534/1680 train_time:46527ms step_avg:87.13ms
step:535/1680 train_time:46613ms step_avg:87.13ms
step:536/1680 train_time:46701ms step_avg:87.13ms
step:537/1680 train_time:46788ms step_avg:87.13ms
step:538/1680 train_time:46876ms step_avg:87.13ms
step:539/1680 train_time:46963ms step_avg:87.13ms
step:540/1680 train_time:47051ms step_avg:87.13ms
step:541/1680 train_time:47139ms step_avg:87.13ms
step:542/1680 train_time:47226ms step_avg:87.13ms
step:543/1680 train_time:47313ms step_avg:87.13ms
step:544/1680 train_time:47399ms step_avg:87.13ms
step:545/1680 train_time:47486ms step_avg:87.13ms
step:546/1680 train_time:47572ms step_avg:87.13ms
step:547/1680 train_time:47659ms step_avg:87.13ms
step:548/1680 train_time:47746ms step_avg:87.13ms
step:549/1680 train_time:47835ms step_avg:87.13ms
step:550/1680 train_time:47923ms step_avg:87.13ms
step:551/1680 train_time:48012ms step_avg:87.14ms
step:552/1680 train_time:48101ms step_avg:87.14ms
step:553/1680 train_time:48190ms step_avg:87.14ms
step:554/1680 train_time:48278ms step_avg:87.14ms
step:555/1680 train_time:48366ms step_avg:87.15ms
step:556/1680 train_time:48454ms step_avg:87.15ms
step:557/1680 train_time:48543ms step_avg:87.15ms
step:558/1680 train_time:48630ms step_avg:87.15ms
step:559/1680 train_time:48718ms step_avg:87.15ms
step:560/1680 train_time:48806ms step_avg:87.15ms
step:561/1680 train_time:48895ms step_avg:87.16ms
step:562/1680 train_time:48984ms step_avg:87.16ms
step:563/1680 train_time:49073ms step_avg:87.16ms
step:564/1680 train_time:49162ms step_avg:87.17ms
step:565/1680 train_time:49250ms step_avg:87.17ms
step:566/1680 train_time:49338ms step_avg:87.17ms
step:567/1680 train_time:49426ms step_avg:87.17ms
step:568/1680 train_time:49514ms step_avg:87.17ms
step:569/1680 train_time:49602ms step_avg:87.17ms
step:570/1680 train_time:49691ms step_avg:87.18ms
step:571/1680 train_time:49779ms step_avg:87.18ms
step:572/1680 train_time:49866ms step_avg:87.18ms
step:573/1680 train_time:49955ms step_avg:87.18ms
step:574/1680 train_time:50043ms step_avg:87.18ms
step:575/1680 train_time:50132ms step_avg:87.19ms
step:576/1680 train_time:50220ms step_avg:87.19ms
step:577/1680 train_time:50309ms step_avg:87.19ms
step:578/1680 train_time:50397ms step_avg:87.19ms
step:579/1680 train_time:50486ms step_avg:87.20ms
step:580/1680 train_time:50575ms step_avg:87.20ms
step:581/1680 train_time:50662ms step_avg:87.20ms
step:582/1680 train_time:50750ms step_avg:87.20ms
step:583/1680 train_time:50838ms step_avg:87.20ms
step:584/1680 train_time:50926ms step_avg:87.20ms
step:585/1680 train_time:51015ms step_avg:87.21ms
step:586/1680 train_time:51104ms step_avg:87.21ms
step:587/1680 train_time:51193ms step_avg:87.21ms
step:588/1680 train_time:51282ms step_avg:87.21ms
step:589/1680 train_time:51370ms step_avg:87.22ms
step:590/1680 train_time:51459ms step_avg:87.22ms
step:591/1680 train_time:51547ms step_avg:87.22ms
step:592/1680 train_time:51635ms step_avg:87.22ms
step:593/1680 train_time:51723ms step_avg:87.22ms
step:594/1680 train_time:51810ms step_avg:87.22ms
step:595/1680 train_time:51898ms step_avg:87.22ms
step:596/1680 train_time:51986ms step_avg:87.23ms
step:597/1680 train_time:52075ms step_avg:87.23ms
step:598/1680 train_time:52163ms step_avg:87.23ms
step:599/1680 train_time:52252ms step_avg:87.23ms
step:600/1680 train_time:52341ms step_avg:87.23ms
step:601/1680 train_time:52429ms step_avg:87.24ms
step:602/1680 train_time:52517ms step_avg:87.24ms
step:603/1680 train_time:52606ms step_avg:87.24ms
step:604/1680 train_time:52694ms step_avg:87.24ms
step:605/1680 train_time:52782ms step_avg:87.24ms
step:606/1680 train_time:52871ms step_avg:87.25ms
step:607/1680 train_time:52959ms step_avg:87.25ms
step:608/1680 train_time:53047ms step_avg:87.25ms
step:609/1680 train_time:53135ms step_avg:87.25ms
step:610/1680 train_time:53223ms step_avg:87.25ms
step:611/1680 train_time:53312ms step_avg:87.25ms
step:612/1680 train_time:53400ms step_avg:87.26ms
step:613/1680 train_time:53489ms step_avg:87.26ms
step:614/1680 train_time:53577ms step_avg:87.26ms
step:615/1680 train_time:53666ms step_avg:87.26ms
step:616/1680 train_time:53754ms step_avg:87.26ms
step:617/1680 train_time:53843ms step_avg:87.26ms
step:618/1680 train_time:53931ms step_avg:87.27ms
step:619/1680 train_time:54020ms step_avg:87.27ms
step:620/1680 train_time:54108ms step_avg:87.27ms
step:621/1680 train_time:54196ms step_avg:87.27ms
step:622/1680 train_time:54284ms step_avg:87.27ms
step:623/1680 train_time:54372ms step_avg:87.27ms
step:624/1680 train_time:54461ms step_avg:87.28ms
step:625/1680 train_time:54549ms step_avg:87.28ms
step:625/1680 val_loss:3.6197 train_time:54639ms step_avg:87.42ms
step:626/1680 train_time:54665ms step_avg:87.32ms
step:627/1680 train_time:54730ms step_avg:87.29ms
step:628/1680 train_time:54817ms step_avg:87.29ms
step:629/1680 train_time:54908ms step_avg:87.29ms
step:630/1680 train_time:54994ms step_avg:87.29ms
step:631/1680 train_time:55081ms step_avg:87.29ms
step:632/1680 train_time:55168ms step_avg:87.29ms
step:633/1680 train_time:55255ms step_avg:87.29ms
step:634/1680 train_time:55342ms step_avg:87.29ms
step:635/1680 train_time:55429ms step_avg:87.29ms
step:636/1680 train_time:55517ms step_avg:87.29ms
step:637/1680 train_time:55609ms step_avg:87.30ms
step:638/1680 train_time:55701ms step_avg:87.31ms
step:639/1680 train_time:55791ms step_avg:87.31ms
step:640/1680 train_time:55879ms step_avg:87.31ms
step:641/1680 train_time:55967ms step_avg:87.31ms
step:642/1680 train_time:56055ms step_avg:87.31ms
step:643/1680 train_time:56142ms step_avg:87.31ms
step:644/1680 train_time:56229ms step_avg:87.31ms
step:645/1680 train_time:56317ms step_avg:87.31ms
step:646/1680 train_time:56404ms step_avg:87.31ms
step:647/1680 train_time:56492ms step_avg:87.31ms
step:648/1680 train_time:56582ms step_avg:87.32ms
step:649/1680 train_time:56672ms step_avg:87.32ms
step:650/1680 train_time:56760ms step_avg:87.32ms
step:651/1680 train_time:56849ms step_avg:87.33ms
step:652/1680 train_time:56937ms step_avg:87.33ms
step:653/1680 train_time:57026ms step_avg:87.33ms
step:654/1680 train_time:57113ms step_avg:87.33ms
step:655/1680 train_time:57201ms step_avg:87.33ms
step:656/1680 train_time:57288ms step_avg:87.33ms
step:657/1680 train_time:57376ms step_avg:87.33ms
step:658/1680 train_time:57464ms step_avg:87.33ms
step:659/1680 train_time:57552ms step_avg:87.33ms
step:660/1680 train_time:57641ms step_avg:87.34ms
step:661/1680 train_time:57730ms step_avg:87.34ms
step:662/1680 train_time:57819ms step_avg:87.34ms
step:663/1680 train_time:57908ms step_avg:87.34ms
step:664/1680 train_time:57996ms step_avg:87.34ms
step:665/1680 train_time:58083ms step_avg:87.34ms
step:666/1680 train_time:58171ms step_avg:87.34ms
step:667/1680 train_time:58259ms step_avg:87.35ms
step:668/1680 train_time:58347ms step_avg:87.35ms
step:669/1680 train_time:58434ms step_avg:87.35ms
step:670/1680 train_time:58523ms step_avg:87.35ms
step:671/1680 train_time:58612ms step_avg:87.35ms
step:672/1680 train_time:58701ms step_avg:87.35ms
step:673/1680 train_time:58789ms step_avg:87.35ms
step:674/1680 train_time:58878ms step_avg:87.36ms
step:675/1680 train_time:58966ms step_avg:87.36ms
step:676/1680 train_time:59054ms step_avg:87.36ms
step:677/1680 train_time:59143ms step_avg:87.36ms
step:678/1680 train_time:59231ms step_avg:87.36ms
step:679/1680 train_time:59318ms step_avg:87.36ms
step:680/1680 train_time:59407ms step_avg:87.36ms
step:681/1680 train_time:59495ms step_avg:87.36ms
step:682/1680 train_time:59583ms step_avg:87.36ms
step:683/1680 train_time:59671ms step_avg:87.37ms
step:684/1680 train_time:59759ms step_avg:87.37ms
step:685/1680 train_time:59847ms step_avg:87.37ms
step:686/1680 train_time:59936ms step_avg:87.37ms
step:687/1680 train_time:60023ms step_avg:87.37ms
step:688/1680 train_time:60112ms step_avg:87.37ms
step:689/1680 train_time:60200ms step_avg:87.37ms
step:690/1680 train_time:60288ms step_avg:87.37ms
step:691/1680 train_time:60375ms step_avg:87.37ms
step:692/1680 train_time:60463ms step_avg:87.37ms
step:693/1680 train_time:60551ms step_avg:87.38ms
step:694/1680 train_time:60639ms step_avg:87.38ms
step:695/1680 train_time:60728ms step_avg:87.38ms
step:696/1680 train_time:60817ms step_avg:87.38ms
step:697/1680 train_time:60905ms step_avg:87.38ms
step:698/1680 train_time:60993ms step_avg:87.38ms
step:699/1680 train_time:61082ms step_avg:87.38ms
step:700/1680 train_time:61171ms step_avg:87.39ms
step:701/1680 train_time:61259ms step_avg:87.39ms
step:702/1680 train_time:61347ms step_avg:87.39ms
step:703/1680 train_time:61435ms step_avg:87.39ms
step:704/1680 train_time:61523ms step_avg:87.39ms
step:705/1680 train_time:61611ms step_avg:87.39ms
step:706/1680 train_time:61700ms step_avg:87.39ms
step:707/1680 train_time:61788ms step_avg:87.39ms
step:708/1680 train_time:61876ms step_avg:87.40ms
step:709/1680 train_time:61965ms step_avg:87.40ms
step:710/1680 train_time:62054ms step_avg:87.40ms
step:711/1680 train_time:62142ms step_avg:87.40ms
step:712/1680 train_time:62230ms step_avg:87.40ms
step:713/1680 train_time:62319ms step_avg:87.40ms
step:714/1680 train_time:62406ms step_avg:87.40ms
step:715/1680 train_time:62495ms step_avg:87.41ms
step:716/1680 train_time:62583ms step_avg:87.41ms
step:717/1680 train_time:62672ms step_avg:87.41ms
step:718/1680 train_time:62760ms step_avg:87.41ms
step:719/1680 train_time:62849ms step_avg:87.41ms
step:720/1680 train_time:62937ms step_avg:87.41ms
step:721/1680 train_time:63025ms step_avg:87.41ms
step:722/1680 train_time:63113ms step_avg:87.41ms
step:723/1680 train_time:63202ms step_avg:87.42ms
step:724/1680 train_time:63290ms step_avg:87.42ms
step:725/1680 train_time:63379ms step_avg:87.42ms
step:726/1680 train_time:63466ms step_avg:87.42ms
step:727/1680 train_time:63555ms step_avg:87.42ms
step:728/1680 train_time:63643ms step_avg:87.42ms
step:729/1680 train_time:63730ms step_avg:87.42ms
step:730/1680 train_time:63819ms step_avg:87.42ms
step:731/1680 train_time:63907ms step_avg:87.42ms
step:732/1680 train_time:63995ms step_avg:87.43ms
step:733/1680 train_time:64084ms step_avg:87.43ms
step:734/1680 train_time:64173ms step_avg:87.43ms
step:735/1680 train_time:64261ms step_avg:87.43ms
step:736/1680 train_time:64349ms step_avg:87.43ms
step:737/1680 train_time:64437ms step_avg:87.43ms
step:738/1680 train_time:64525ms step_avg:87.43ms
step:739/1680 train_time:64613ms step_avg:87.43ms
step:740/1680 train_time:64702ms step_avg:87.43ms
step:741/1680 train_time:64790ms step_avg:87.44ms
step:742/1680 train_time:64879ms step_avg:87.44ms
step:743/1680 train_time:64967ms step_avg:87.44ms
step:744/1680 train_time:65055ms step_avg:87.44ms
step:745/1680 train_time:65144ms step_avg:87.44ms
step:746/1680 train_time:65232ms step_avg:87.44ms
step:747/1680 train_time:65321ms step_avg:87.44ms
step:748/1680 train_time:65409ms step_avg:87.45ms
step:749/1680 train_time:65497ms step_avg:87.45ms
step:750/1680 train_time:65585ms step_avg:87.45ms
step:750/1680 val_loss:3.5671 train_time:65675ms step_avg:87.57ms
step:751/1680 train_time:65697ms step_avg:87.48ms
step:752/1680 train_time:65766ms step_avg:87.45ms
step:753/1680 train_time:65857ms step_avg:87.46ms
step:754/1680 train_time:65946ms step_avg:87.46ms
step:755/1680 train_time:66035ms step_avg:87.46ms
step:756/1680 train_time:66123ms step_avg:87.46ms
step:757/1680 train_time:66209ms step_avg:87.46ms
step:758/1680 train_time:66296ms step_avg:87.46ms
step:759/1680 train_time:66383ms step_avg:87.46ms
step:760/1680 train_time:66472ms step_avg:87.46ms
step:761/1680 train_time:66559ms step_avg:87.46ms
step:762/1680 train_time:66648ms step_avg:87.46ms
step:763/1680 train_time:66737ms step_avg:87.47ms
step:764/1680 train_time:66827ms step_avg:87.47ms
step:765/1680 train_time:66917ms step_avg:87.47ms
step:766/1680 train_time:67006ms step_avg:87.47ms
step:767/1680 train_time:67094ms step_avg:87.48ms
step:768/1680 train_time:67181ms step_avg:87.48ms
step:769/1680 train_time:67268ms step_avg:87.48ms
step:770/1680 train_time:67356ms step_avg:87.48ms
step:771/1680 train_time:67444ms step_avg:87.48ms
step:772/1680 train_time:67532ms step_avg:87.48ms
step:773/1680 train_time:67621ms step_avg:87.48ms
step:774/1680 train_time:67709ms step_avg:87.48ms
step:775/1680 train_time:67798ms step_avg:87.48ms
step:776/1680 train_time:67887ms step_avg:87.48ms
step:777/1680 train_time:67976ms step_avg:87.48ms
step:778/1680 train_time:68064ms step_avg:87.49ms
step:779/1680 train_time:68153ms step_avg:87.49ms
step:780/1680 train_time:68240ms step_avg:87.49ms
step:781/1680 train_time:68328ms step_avg:87.49ms
step:782/1680 train_time:68416ms step_avg:87.49ms
step:783/1680 train_time:68503ms step_avg:87.49ms
step:784/1680 train_time:68591ms step_avg:87.49ms
step:785/1680 train_time:68680ms step_avg:87.49ms
step:786/1680 train_time:68769ms step_avg:87.49ms
step:787/1680 train_time:68858ms step_avg:87.49ms
step:788/1680 train_time:68946ms step_avg:87.50ms
step:789/1680 train_time:69036ms step_avg:87.50ms
step:790/1680 train_time:69124ms step_avg:87.50ms
step:791/1680 train_time:69212ms step_avg:87.50ms
step:792/1680 train_time:69299ms step_avg:87.50ms
step:793/1680 train_time:69388ms step_avg:87.50ms
step:794/1680 train_time:69475ms step_avg:87.50ms
step:795/1680 train_time:69564ms step_avg:87.50ms
step:796/1680 train_time:69652ms step_avg:87.50ms
step:797/1680 train_time:69740ms step_avg:87.50ms
step:798/1680 train_time:69829ms step_avg:87.50ms
step:799/1680 train_time:69917ms step_avg:87.51ms
step:800/1680 train_time:70006ms step_avg:87.51ms
step:801/1680 train_time:70094ms step_avg:87.51ms
step:802/1680 train_time:70183ms step_avg:87.51ms
step:803/1680 train_time:70271ms step_avg:87.51ms
step:804/1680 train_time:70359ms step_avg:87.51ms
step:805/1680 train_time:70447ms step_avg:87.51ms
step:806/1680 train_time:70535ms step_avg:87.51ms
step:807/1680 train_time:70624ms step_avg:87.51ms
step:808/1680 train_time:70712ms step_avg:87.52ms
step:809/1680 train_time:70800ms step_avg:87.52ms
step:810/1680 train_time:70889ms step_avg:87.52ms
step:811/1680 train_time:70978ms step_avg:87.52ms
step:812/1680 train_time:71066ms step_avg:87.52ms
step:813/1680 train_time:71154ms step_avg:87.52ms
step:814/1680 train_time:71243ms step_avg:87.52ms
step:815/1680 train_time:71331ms step_avg:87.52ms
step:816/1680 train_time:71419ms step_avg:87.52ms
step:817/1680 train_time:71508ms step_avg:87.52ms
step:818/1680 train_time:71595ms step_avg:87.52ms
step:819/1680 train_time:71683ms step_avg:87.53ms
step:820/1680 train_time:71771ms step_avg:87.53ms
step:821/1680 train_time:71860ms step_avg:87.53ms
step:822/1680 train_time:71948ms step_avg:87.53ms
step:823/1680 train_time:72036ms step_avg:87.53ms
step:824/1680 train_time:72124ms step_avg:87.53ms
step:825/1680 train_time:72213ms step_avg:87.53ms
step:826/1680 train_time:72301ms step_avg:87.53ms
step:827/1680 train_time:72389ms step_avg:87.53ms
step:828/1680 train_time:72477ms step_avg:87.53ms
step:829/1680 train_time:72565ms step_avg:87.53ms
step:830/1680 train_time:72654ms step_avg:87.53ms
step:831/1680 train_time:72742ms step_avg:87.54ms
step:832/1680 train_time:72830ms step_avg:87.54ms
step:833/1680 train_time:72918ms step_avg:87.54ms
step:834/1680 train_time:73006ms step_avg:87.54ms
step:835/1680 train_time:73094ms step_avg:87.54ms
step:836/1680 train_time:73183ms step_avg:87.54ms
step:837/1680 train_time:73271ms step_avg:87.54ms
step:838/1680 train_time:73359ms step_avg:87.54ms
step:839/1680 train_time:73447ms step_avg:87.54ms
step:840/1680 train_time:73536ms step_avg:87.54ms
step:841/1680 train_time:73625ms step_avg:87.54ms
step:842/1680 train_time:73713ms step_avg:87.54ms
step:843/1680 train_time:73801ms step_avg:87.55ms
step:844/1680 train_time:73889ms step_avg:87.55ms
step:845/1680 train_time:73977ms step_avg:87.55ms
step:846/1680 train_time:74066ms step_avg:87.55ms
step:847/1680 train_time:74155ms step_avg:87.55ms
step:848/1680 train_time:74243ms step_avg:87.55ms
step:849/1680 train_time:74332ms step_avg:87.55ms
step:850/1680 train_time:74419ms step_avg:87.55ms
step:851/1680 train_time:74507ms step_avg:87.55ms
step:852/1680 train_time:74595ms step_avg:87.55ms
step:853/1680 train_time:74683ms step_avg:87.55ms
step:854/1680 train_time:74772ms step_avg:87.55ms
step:855/1680 train_time:74860ms step_avg:87.56ms
step:856/1680 train_time:74948ms step_avg:87.56ms
step:857/1680 train_time:75037ms step_avg:87.56ms
step:858/1680 train_time:75126ms step_avg:87.56ms
step:859/1680 train_time:75216ms step_avg:87.56ms
step:860/1680 train_time:75304ms step_avg:87.56ms
step:861/1680 train_time:75393ms step_avg:87.56ms
step:862/1680 train_time:75480ms step_avg:87.56ms
step:863/1680 train_time:75568ms step_avg:87.56ms
step:864/1680 train_time:75656ms step_avg:87.56ms
step:865/1680 train_time:75745ms step_avg:87.57ms
step:866/1680 train_time:75833ms step_avg:87.57ms
step:867/1680 train_time:75921ms step_avg:87.57ms
step:868/1680 train_time:76009ms step_avg:87.57ms
step:869/1680 train_time:76097ms step_avg:87.57ms
step:870/1680 train_time:76186ms step_avg:87.57ms
step:871/1680 train_time:76274ms step_avg:87.57ms
step:872/1680 train_time:76363ms step_avg:87.57ms
step:873/1680 train_time:76450ms step_avg:87.57ms
step:874/1680 train_time:76538ms step_avg:87.57ms
step:875/1680 train_time:76627ms step_avg:87.57ms
step:875/1680 val_loss:3.5195 train_time:76716ms step_avg:87.68ms
step:876/1680 train_time:76737ms step_avg:87.60ms
step:877/1680 train_time:76806ms step_avg:87.58ms
step:878/1680 train_time:76900ms step_avg:87.59ms
step:879/1680 train_time:76990ms step_avg:87.59ms
step:880/1680 train_time:77077ms step_avg:87.59ms
step:881/1680 train_time:77164ms step_avg:87.59ms
step:882/1680 train_time:77251ms step_avg:87.59ms
step:883/1680 train_time:77338ms step_avg:87.59ms
step:884/1680 train_time:77425ms step_avg:87.59ms
step:885/1680 train_time:77513ms step_avg:87.58ms
step:886/1680 train_time:77600ms step_avg:87.59ms
step:887/1680 train_time:77690ms step_avg:87.59ms
step:888/1680 train_time:77780ms step_avg:87.59ms
step:889/1680 train_time:77870ms step_avg:87.59ms
step:890/1680 train_time:77960ms step_avg:87.60ms
step:891/1680 train_time:78048ms step_avg:87.60ms
step:892/1680 train_time:78136ms step_avg:87.60ms
step:893/1680 train_time:78224ms step_avg:87.60ms
step:894/1680 train_time:78311ms step_avg:87.60ms
step:895/1680 train_time:78399ms step_avg:87.60ms
step:896/1680 train_time:78486ms step_avg:87.60ms
step:897/1680 train_time:78573ms step_avg:87.60ms
step:898/1680 train_time:78662ms step_avg:87.60ms
step:899/1680 train_time:78750ms step_avg:87.60ms
step:900/1680 train_time:78840ms step_avg:87.60ms
step:901/1680 train_time:78928ms step_avg:87.60ms
step:902/1680 train_time:79017ms step_avg:87.60ms
step:903/1680 train_time:79106ms step_avg:87.60ms
step:904/1680 train_time:79194ms step_avg:87.60ms
step:905/1680 train_time:79283ms step_avg:87.61ms
step:906/1680 train_time:79371ms step_avg:87.61ms
step:907/1680 train_time:79459ms step_avg:87.61ms
step:908/1680 train_time:79547ms step_avg:87.61ms
step:909/1680 train_time:79634ms step_avg:87.61ms
step:910/1680 train_time:79723ms step_avg:87.61ms
step:911/1680 train_time:79812ms step_avg:87.61ms
step:912/1680 train_time:79902ms step_avg:87.61ms
step:913/1680 train_time:79991ms step_avg:87.61ms
step:914/1680 train_time:80080ms step_avg:87.61ms
step:915/1680 train_time:80167ms step_avg:87.61ms
step:916/1680 train_time:80255ms step_avg:87.62ms
step:917/1680 train_time:80344ms step_avg:87.62ms
step:918/1680 train_time:80431ms step_avg:87.62ms
step:919/1680 train_time:80519ms step_avg:87.62ms
step:920/1680 train_time:80606ms step_avg:87.62ms
step:921/1680 train_time:80695ms step_avg:87.62ms
step:922/1680 train_time:80783ms step_avg:87.62ms
step:923/1680 train_time:80872ms step_avg:87.62ms
step:924/1680 train_time:80961ms step_avg:87.62ms
step:925/1680 train_time:81049ms step_avg:87.62ms
step:926/1680 train_time:81137ms step_avg:87.62ms
step:927/1680 train_time:81225ms step_avg:87.62ms
step:928/1680 train_time:81313ms step_avg:87.62ms
step:929/1680 train_time:81401ms step_avg:87.62ms
step:930/1680 train_time:81489ms step_avg:87.62ms
step:931/1680 train_time:81577ms step_avg:87.62ms
step:932/1680 train_time:81665ms step_avg:87.62ms
step:933/1680 train_time:81753ms step_avg:87.62ms
step:934/1680 train_time:81841ms step_avg:87.62ms
step:935/1680 train_time:81929ms step_avg:87.62ms
step:936/1680 train_time:82019ms step_avg:87.63ms
step:937/1680 train_time:82107ms step_avg:87.63ms
step:938/1680 train_time:82195ms step_avg:87.63ms
step:939/1680 train_time:82283ms step_avg:87.63ms
step:940/1680 train_time:82371ms step_avg:87.63ms
step:941/1680 train_time:82459ms step_avg:87.63ms
step:942/1680 train_time:82547ms step_avg:87.63ms
step:943/1680 train_time:82635ms step_avg:87.63ms
step:944/1680 train_time:82723ms step_avg:87.63ms
step:945/1680 train_time:82811ms step_avg:87.63ms
step:946/1680 train_time:82899ms step_avg:87.63ms
step:947/1680 train_time:82988ms step_avg:87.63ms
step:948/1680 train_time:83076ms step_avg:87.63ms
step:949/1680 train_time:83164ms step_avg:87.63ms
step:950/1680 train_time:83253ms step_avg:87.63ms
step:951/1680 train_time:83341ms step_avg:87.64ms
step:952/1680 train_time:83430ms step_avg:87.64ms
step:953/1680 train_time:83518ms step_avg:87.64ms
step:954/1680 train_time:83606ms step_avg:87.64ms
step:955/1680 train_time:83694ms step_avg:87.64ms
step:956/1680 train_time:83783ms step_avg:87.64ms
step:957/1680 train_time:83872ms step_avg:87.64ms
step:958/1680 train_time:83960ms step_avg:87.64ms
step:959/1680 train_time:84048ms step_avg:87.64ms
step:960/1680 train_time:84137ms step_avg:87.64ms
step:961/1680 train_time:84226ms step_avg:87.64ms
step:962/1680 train_time:84314ms step_avg:87.64ms
step:963/1680 train_time:84403ms step_avg:87.65ms
step:964/1680 train_time:84491ms step_avg:87.65ms
step:965/1680 train_time:84579ms step_avg:87.65ms
step:966/1680 train_time:84667ms step_avg:87.65ms
step:967/1680 train_time:84756ms step_avg:87.65ms
step:968/1680 train_time:84844ms step_avg:87.65ms
step:969/1680 train_time:84933ms step_avg:87.65ms
step:970/1680 train_time:85021ms step_avg:87.65ms
step:971/1680 train_time:85109ms step_avg:87.65ms
step:972/1680 train_time:85198ms step_avg:87.65ms
step:973/1680 train_time:85286ms step_avg:87.65ms
step:974/1680 train_time:85374ms step_avg:87.65ms
step:975/1680 train_time:85462ms step_avg:87.65ms
step:976/1680 train_time:85550ms step_avg:87.65ms
step:977/1680 train_time:85638ms step_avg:87.65ms
step:978/1680 train_time:85726ms step_avg:87.65ms
step:979/1680 train_time:85814ms step_avg:87.65ms
step:980/1680 train_time:85902ms step_avg:87.66ms
step:981/1680 train_time:85991ms step_avg:87.66ms
step:982/1680 train_time:86080ms step_avg:87.66ms
step:983/1680 train_time:86168ms step_avg:87.66ms
step:984/1680 train_time:86256ms step_avg:87.66ms
step:985/1680 train_time:86345ms step_avg:87.66ms
step:986/1680 train_time:86433ms step_avg:87.66ms
step:987/1680 train_time:86521ms step_avg:87.66ms
step:988/1680 train_time:86608ms step_avg:87.66ms
step:989/1680 train_time:86697ms step_avg:87.66ms
step:990/1680 train_time:86785ms step_avg:87.66ms
step:991/1680 train_time:86873ms step_avg:87.66ms
step:992/1680 train_time:86962ms step_avg:87.66ms
step:993/1680 train_time:87050ms step_avg:87.66ms
step:994/1680 train_time:87139ms step_avg:87.66ms
step:995/1680 train_time:87227ms step_avg:87.67ms
step:996/1680 train_time:87315ms step_avg:87.67ms
step:997/1680 train_time:87404ms step_avg:87.67ms
step:998/1680 train_time:87492ms step_avg:87.67ms
step:999/1680 train_time:87581ms step_avg:87.67ms
step:1000/1680 train_time:87668ms step_avg:87.67ms
step:1000/1680 val_loss:3.4690 train_time:87758ms step_avg:87.76ms
step:1001/1680 train_time:87778ms step_avg:87.69ms
step:1002/1680 train_time:87850ms step_avg:87.67ms
step:1003/1680 train_time:87944ms step_avg:87.68ms
step:1004/1680 train_time:88032ms step_avg:87.68ms
step:1005/1680 train_time:88120ms step_avg:87.68ms
step:1006/1680 train_time:88207ms step_avg:87.68ms
step:1007/1680 train_time:88295ms step_avg:87.68ms
step:1008/1680 train_time:88382ms step_avg:87.68ms
step:1009/1680 train_time:88469ms step_avg:87.68ms
step:1010/1680 train_time:88557ms step_avg:87.68ms
step:1011/1680 train_time:88644ms step_avg:87.68ms
step:1012/1680 train_time:88733ms step_avg:87.68ms
step:1013/1680 train_time:88823ms step_avg:87.68ms
step:1014/1680 train_time:88913ms step_avg:87.69ms
step:1015/1680 train_time:89002ms step_avg:87.69ms
step:1016/1680 train_time:89090ms step_avg:87.69ms
step:1017/1680 train_time:89179ms step_avg:87.69ms
step:1018/1680 train_time:89266ms step_avg:87.69ms
step:1019/1680 train_time:89353ms step_avg:87.69ms
step:1020/1680 train_time:89441ms step_avg:87.69ms
step:1021/1680 train_time:89529ms step_avg:87.69ms
step:1022/1680 train_time:89616ms step_avg:87.69ms
step:1023/1680 train_time:89706ms step_avg:87.69ms
step:1024/1680 train_time:89795ms step_avg:87.69ms
step:1025/1680 train_time:89884ms step_avg:87.69ms
step:1026/1680 train_time:89973ms step_avg:87.69ms
step:1027/1680 train_time:90061ms step_avg:87.69ms
step:1028/1680 train_time:90150ms step_avg:87.69ms
step:1029/1680 train_time:90238ms step_avg:87.70ms
step:1030/1680 train_time:90326ms step_avg:87.70ms
step:1031/1680 train_time:90413ms step_avg:87.69ms
step:1032/1680 train_time:90501ms step_avg:87.69ms
step:1033/1680 train_time:90589ms step_avg:87.69ms
step:1034/1680 train_time:90676ms step_avg:87.69ms
step:1035/1680 train_time:90765ms step_avg:87.70ms
step:1036/1680 train_time:90853ms step_avg:87.70ms
step:1037/1680 train_time:90943ms step_avg:87.70ms
step:1038/1680 train_time:91032ms step_avg:87.70ms
step:1039/1680 train_time:91120ms step_avg:87.70ms
step:1040/1680 train_time:91209ms step_avg:87.70ms
step:1041/1680 train_time:91297ms step_avg:87.70ms
step:1042/1680 train_time:91384ms step_avg:87.70ms
step:1043/1680 train_time:91472ms step_avg:87.70ms
step:1044/1680 train_time:91559ms step_avg:87.70ms
step:1045/1680 train_time:91647ms step_avg:87.70ms
step:1046/1680 train_time:91735ms step_avg:87.70ms
step:1047/1680 train_time:91824ms step_avg:87.70ms
step:1048/1680 train_time:91911ms step_avg:87.70ms
step:1049/1680 train_time:92001ms step_avg:87.70ms
step:1050/1680 train_time:92089ms step_avg:87.70ms
step:1051/1680 train_time:92178ms step_avg:87.71ms
step:1052/1680 train_time:92267ms step_avg:87.71ms
step:1053/1680 train_time:92355ms step_avg:87.71ms
step:1054/1680 train_time:92443ms step_avg:87.71ms
step:1055/1680 train_time:92531ms step_avg:87.71ms
step:1056/1680 train_time:92618ms step_avg:87.71ms
step:1057/1680 train_time:92706ms step_avg:87.71ms
step:1058/1680 train_time:92795ms step_avg:87.71ms
step:1059/1680 train_time:92883ms step_avg:87.71ms
step:1060/1680 train_time:92971ms step_avg:87.71ms
step:1061/1680 train_time:93060ms step_avg:87.71ms
step:1062/1680 train_time:93149ms step_avg:87.71ms
step:1063/1680 train_time:93236ms step_avg:87.71ms
step:1064/1680 train_time:93325ms step_avg:87.71ms
step:1065/1680 train_time:93413ms step_avg:87.71ms
step:1066/1680 train_time:93501ms step_avg:87.71ms
step:1067/1680 train_time:93589ms step_avg:87.71ms
step:1068/1680 train_time:93677ms step_avg:87.71ms
step:1069/1680 train_time:93765ms step_avg:87.71ms
step:1070/1680 train_time:93853ms step_avg:87.71ms
step:1071/1680 train_time:93942ms step_avg:87.71ms
step:1072/1680 train_time:94031ms step_avg:87.72ms
step:1073/1680 train_time:94120ms step_avg:87.72ms
step:1074/1680 train_time:94208ms step_avg:87.72ms
step:1075/1680 train_time:94295ms step_avg:87.72ms
step:1076/1680 train_time:94384ms step_avg:87.72ms
step:1077/1680 train_time:94472ms step_avg:87.72ms
step:1078/1680 train_time:94561ms step_avg:87.72ms
step:1079/1680 train_time:94649ms step_avg:87.72ms
step:1080/1680 train_time:94737ms step_avg:87.72ms
step:1081/1680 train_time:94825ms step_avg:87.72ms
step:1082/1680 train_time:94913ms step_avg:87.72ms
step:1083/1680 train_time:95002ms step_avg:87.72ms
step:1084/1680 train_time:95090ms step_avg:87.72ms
step:1085/1680 train_time:95179ms step_avg:87.72ms
step:1086/1680 train_time:95267ms step_avg:87.72ms
step:1087/1680 train_time:95356ms step_avg:87.72ms
step:1088/1680 train_time:95444ms step_avg:87.72ms
step:1089/1680 train_time:95532ms step_avg:87.72ms
step:1090/1680 train_time:95620ms step_avg:87.72ms
step:1091/1680 train_time:95708ms step_avg:87.72ms
step:1092/1680 train_time:95796ms step_avg:87.73ms
step:1093/1680 train_time:95885ms step_avg:87.73ms
step:1094/1680 train_time:95973ms step_avg:87.73ms
step:1095/1680 train_time:96062ms step_avg:87.73ms
step:1096/1680 train_time:96150ms step_avg:87.73ms
step:1097/1680 train_time:96239ms step_avg:87.73ms
step:1098/1680 train_time:96328ms step_avg:87.73ms
step:1099/1680 train_time:96417ms step_avg:87.73ms
step:1100/1680 train_time:96505ms step_avg:87.73ms
step:1101/1680 train_time:96594ms step_avg:87.73ms
step:1102/1680 train_time:96682ms step_avg:87.73ms
step:1103/1680 train_time:96771ms step_avg:87.73ms
step:1104/1680 train_time:96860ms step_avg:87.74ms
step:1105/1680 train_time:96949ms step_avg:87.74ms
step:1106/1680 train_time:97038ms step_avg:87.74ms
step:1107/1680 train_time:97127ms step_avg:87.74ms
step:1108/1680 train_time:97215ms step_avg:87.74ms
step:1109/1680 train_time:97304ms step_avg:87.74ms
step:1110/1680 train_time:97393ms step_avg:87.74ms
step:1111/1680 train_time:97482ms step_avg:87.74ms
step:1112/1680 train_time:97571ms step_avg:87.74ms
step:1113/1680 train_time:97660ms step_avg:87.74ms
step:1114/1680 train_time:97748ms step_avg:87.75ms
step:1115/1680 train_time:97837ms step_avg:87.75ms
step:1116/1680 train_time:97927ms step_avg:87.75ms
step:1117/1680 train_time:98016ms step_avg:87.75ms
step:1118/1680 train_time:98104ms step_avg:87.75ms
step:1119/1680 train_time:98193ms step_avg:87.75ms
step:1120/1680 train_time:98282ms step_avg:87.75ms
step:1121/1680 train_time:98371ms step_avg:87.75ms
step:1122/1680 train_time:98459ms step_avg:87.75ms
step:1123/1680 train_time:98548ms step_avg:87.75ms
step:1124/1680 train_time:98637ms step_avg:87.76ms
step:1125/1680 train_time:98727ms step_avg:87.76ms
step:1125/1680 val_loss:3.4152 train_time:98817ms step_avg:87.84ms
step:1126/1680 train_time:98837ms step_avg:87.78ms
step:1127/1680 train_time:98906ms step_avg:87.76ms
step:1128/1680 train_time:98998ms step_avg:87.76ms
step:1129/1680 train_time:99091ms step_avg:87.77ms
step:1130/1680 train_time:99178ms step_avg:87.77ms
step:1131/1680 train_time:99266ms step_avg:87.77ms
step:1132/1680 train_time:99355ms step_avg:87.77ms
step:1133/1680 train_time:99442ms step_avg:87.77ms
step:1134/1680 train_time:99530ms step_avg:87.77ms
step:1135/1680 train_time:99618ms step_avg:87.77ms
step:1136/1680 train_time:99708ms step_avg:87.77ms
step:1137/1680 train_time:99798ms step_avg:87.77ms
step:1138/1680 train_time:99889ms step_avg:87.78ms
step:1139/1680 train_time:99980ms step_avg:87.78ms
step:1140/1680 train_time:100071ms step_avg:87.78ms
step:1141/1680 train_time:100161ms step_avg:87.78ms
step:1142/1680 train_time:100248ms step_avg:87.78ms
step:1143/1680 train_time:100337ms step_avg:87.78ms
step:1144/1680 train_time:100426ms step_avg:87.78ms
step:1145/1680 train_time:100514ms step_avg:87.78ms
step:1146/1680 train_time:100602ms step_avg:87.79ms
step:1147/1680 train_time:100691ms step_avg:87.79ms
step:1148/1680 train_time:100780ms step_avg:87.79ms
step:1149/1680 train_time:100870ms step_avg:87.79ms
step:1150/1680 train_time:100960ms step_avg:87.79ms
step:1151/1680 train_time:101050ms step_avg:87.79ms
step:1152/1680 train_time:101140ms step_avg:87.79ms
step:1153/1680 train_time:101228ms step_avg:87.80ms
step:1154/1680 train_time:101317ms step_avg:87.80ms
step:1155/1680 train_time:101406ms step_avg:87.80ms
step:1156/1680 train_time:101495ms step_avg:87.80ms
step:1157/1680 train_time:101583ms step_avg:87.80ms
step:1158/1680 train_time:101671ms step_avg:87.80ms
step:1159/1680 train_time:101760ms step_avg:87.80ms
step:1160/1680 train_time:101849ms step_avg:87.80ms
step:1161/1680 train_time:101939ms step_avg:87.80ms
step:1162/1680 train_time:102029ms step_avg:87.80ms
step:1163/1680 train_time:102118ms step_avg:87.81ms
step:1164/1680 train_time:102208ms step_avg:87.81ms
step:1165/1680 train_time:102297ms step_avg:87.81ms
step:1166/1680 train_time:102386ms step_avg:87.81ms
step:1167/1680 train_time:102474ms step_avg:87.81ms
step:1168/1680 train_time:102562ms step_avg:87.81ms
step:1169/1680 train_time:102651ms step_avg:87.81ms
step:1170/1680 train_time:102739ms step_avg:87.81ms
step:1171/1680 train_time:102828ms step_avg:87.81ms
step:1172/1680 train_time:102918ms step_avg:87.81ms
step:1173/1680 train_time:103008ms step_avg:87.82ms
step:1174/1680 train_time:103098ms step_avg:87.82ms
step:1175/1680 train_time:103188ms step_avg:87.82ms
step:1176/1680 train_time:103276ms step_avg:87.82ms
step:1177/1680 train_time:103364ms step_avg:87.82ms
step:1178/1680 train_time:103452ms step_avg:87.82ms
step:1179/1680 train_time:103542ms step_avg:87.82ms
step:1180/1680 train_time:103630ms step_avg:87.82ms
step:1181/1680 train_time:103719ms step_avg:87.82ms
step:1182/1680 train_time:103807ms step_avg:87.82ms
step:1183/1680 train_time:103896ms step_avg:87.82ms
step:1184/1680 train_time:103984ms step_avg:87.82ms
step:1185/1680 train_time:104073ms step_avg:87.83ms
step:1186/1680 train_time:104163ms step_avg:87.83ms
step:1187/1680 train_time:104252ms step_avg:87.83ms
step:1188/1680 train_time:104341ms step_avg:87.83ms
step:1189/1680 train_time:104430ms step_avg:87.83ms
step:1190/1680 train_time:104519ms step_avg:87.83ms
step:1191/1680 train_time:104608ms step_avg:87.83ms
step:1192/1680 train_time:104697ms step_avg:87.83ms
step:1193/1680 train_time:104787ms step_avg:87.84ms
step:1194/1680 train_time:104876ms step_avg:87.84ms
step:1195/1680 train_time:104965ms step_avg:87.84ms
step:1196/1680 train_time:105054ms step_avg:87.84ms
step:1197/1680 train_time:105143ms step_avg:87.84ms
step:1198/1680 train_time:105232ms step_avg:87.84ms
step:1199/1680 train_time:105321ms step_avg:87.84ms
step:1200/1680 train_time:105410ms step_avg:87.84ms
step:1201/1680 train_time:105499ms step_avg:87.84ms
step:1202/1680 train_time:105588ms step_avg:87.84ms
step:1203/1680 train_time:105676ms step_avg:87.84ms
step:1204/1680 train_time:105765ms step_avg:87.85ms
step:1205/1680 train_time:105854ms step_avg:87.85ms
step:1206/1680 train_time:105944ms step_avg:87.85ms
step:1207/1680 train_time:106033ms step_avg:87.85ms
step:1208/1680 train_time:106122ms step_avg:87.85ms
step:1209/1680 train_time:106211ms step_avg:87.85ms
step:1210/1680 train_time:106300ms step_avg:87.85ms
step:1211/1680 train_time:106389ms step_avg:87.85ms
step:1212/1680 train_time:106478ms step_avg:87.85ms
step:1213/1680 train_time:106567ms step_avg:87.85ms
step:1214/1680 train_time:106655ms step_avg:87.85ms
step:1215/1680 train_time:106743ms step_avg:87.85ms
step:1216/1680 train_time:106833ms step_avg:87.86ms
step:1217/1680 train_time:106922ms step_avg:87.86ms
step:1218/1680 train_time:107011ms step_avg:87.86ms
step:1219/1680 train_time:107102ms step_avg:87.86ms
step:1220/1680 train_time:107192ms step_avg:87.86ms
step:1221/1680 train_time:107281ms step_avg:87.86ms
step:1222/1680 train_time:107370ms step_avg:87.86ms
step:1223/1680 train_time:107459ms step_avg:87.87ms
step:1224/1680 train_time:107548ms step_avg:87.87ms
step:1225/1680 train_time:107637ms step_avg:87.87ms
step:1226/1680 train_time:107726ms step_avg:87.87ms
step:1227/1680 train_time:107815ms step_avg:87.87ms
step:1228/1680 train_time:107904ms step_avg:87.87ms
step:1229/1680 train_time:107992ms step_avg:87.87ms
step:1230/1680 train_time:108081ms step_avg:87.87ms
step:1231/1680 train_time:108170ms step_avg:87.87ms
step:1232/1680 train_time:108259ms step_avg:87.87ms
step:1233/1680 train_time:108347ms step_avg:87.87ms
step:1234/1680 train_time:108436ms step_avg:87.87ms
step:1235/1680 train_time:108525ms step_avg:87.87ms
step:1236/1680 train_time:108614ms step_avg:87.88ms
step:1237/1680 train_time:108703ms step_avg:87.88ms
step:1238/1680 train_time:108793ms step_avg:87.88ms
step:1239/1680 train_time:108883ms step_avg:87.88ms
step:1240/1680 train_time:108972ms step_avg:87.88ms
step:1241/1680 train_time:109060ms step_avg:87.88ms
step:1242/1680 train_time:109149ms step_avg:87.88ms
step:1243/1680 train_time:109237ms step_avg:87.88ms
step:1244/1680 train_time:109327ms step_avg:87.88ms
step:1245/1680 train_time:109415ms step_avg:87.88ms
step:1246/1680 train_time:109504ms step_avg:87.88ms
step:1247/1680 train_time:109593ms step_avg:87.89ms
step:1248/1680 train_time:109682ms step_avg:87.89ms
step:1249/1680 train_time:109770ms step_avg:87.89ms
step:1250/1680 train_time:109859ms step_avg:87.89ms
step:1250/1680 val_loss:3.3769 train_time:109949ms step_avg:87.96ms
step:1251/1680 train_time:109968ms step_avg:87.90ms
step:1252/1680 train_time:110045ms step_avg:87.90ms
step:1253/1680 train_time:110138ms step_avg:87.90ms
step:1254/1680 train_time:110227ms step_avg:87.90ms
step:1255/1680 train_time:110315ms step_avg:87.90ms
step:1256/1680 train_time:110402ms step_avg:87.90ms
step:1257/1680 train_time:110490ms step_avg:87.90ms
step:1258/1680 train_time:110579ms step_avg:87.90ms
step:1259/1680 train_time:110667ms step_avg:87.90ms
step:1260/1680 train_time:110755ms step_avg:87.90ms
step:1261/1680 train_time:110843ms step_avg:87.90ms
step:1262/1680 train_time:110934ms step_avg:87.90ms
step:1263/1680 train_time:111027ms step_avg:87.91ms
step:1264/1680 train_time:111117ms step_avg:87.91ms
step:1265/1680 train_time:111207ms step_avg:87.91ms
step:1266/1680 train_time:111296ms step_avg:87.91ms
step:1267/1680 train_time:111384ms step_avg:87.91ms
step:1268/1680 train_time:111472ms step_avg:87.91ms
step:1269/1680 train_time:111560ms step_avg:87.91ms
step:1270/1680 train_time:111648ms step_avg:87.91ms
step:1271/1680 train_time:111736ms step_avg:87.91ms
step:1272/1680 train_time:111824ms step_avg:87.91ms
step:1273/1680 train_time:111914ms step_avg:87.91ms
step:1274/1680 train_time:112005ms step_avg:87.92ms
step:1275/1680 train_time:112094ms step_avg:87.92ms
step:1276/1680 train_time:112184ms step_avg:87.92ms
step:1277/1680 train_time:112273ms step_avg:87.92ms
step:1278/1680 train_time:112361ms step_avg:87.92ms
step:1279/1680 train_time:112450ms step_avg:87.92ms
step:1280/1680 train_time:112539ms step_avg:87.92ms
step:1281/1680 train_time:112627ms step_avg:87.92ms
step:1282/1680 train_time:112715ms step_avg:87.92ms
step:1283/1680 train_time:112804ms step_avg:87.92ms
step:1284/1680 train_time:112893ms step_avg:87.92ms
step:1285/1680 train_time:112982ms step_avg:87.92ms
step:1286/1680 train_time:113072ms step_avg:87.93ms
step:1287/1680 train_time:113163ms step_avg:87.93ms
step:1288/1680 train_time:113251ms step_avg:87.93ms
step:1289/1680 train_time:113340ms step_avg:87.93ms
step:1290/1680 train_time:113429ms step_avg:87.93ms
step:1291/1680 train_time:113518ms step_avg:87.93ms
step:1292/1680 train_time:113607ms step_avg:87.93ms
step:1293/1680 train_time:113696ms step_avg:87.93ms
step:1294/1680 train_time:113784ms step_avg:87.93ms
step:1295/1680 train_time:113873ms step_avg:87.93ms
step:1296/1680 train_time:113961ms step_avg:87.93ms
step:1297/1680 train_time:114051ms step_avg:87.93ms
step:1298/1680 train_time:114141ms step_avg:87.94ms
step:1299/1680 train_time:114230ms step_avg:87.94ms
step:1300/1680 train_time:114319ms step_avg:87.94ms
step:1301/1680 train_time:114408ms step_avg:87.94ms
step:1302/1680 train_time:114496ms step_avg:87.94ms
step:1303/1680 train_time:114585ms step_avg:87.94ms
step:1304/1680 train_time:114673ms step_avg:87.94ms
step:1305/1680 train_time:114762ms step_avg:87.94ms
step:1306/1680 train_time:114851ms step_avg:87.94ms
step:1307/1680 train_time:114940ms step_avg:87.94ms
step:1308/1680 train_time:115029ms step_avg:87.94ms
step:1309/1680 train_time:115119ms step_avg:87.94ms
step:1310/1680 train_time:115208ms step_avg:87.95ms
step:1311/1680 train_time:115297ms step_avg:87.95ms
step:1312/1680 train_time:115386ms step_avg:87.95ms
step:1313/1680 train_time:115475ms step_avg:87.95ms
step:1314/1680 train_time:115564ms step_avg:87.95ms
step:1315/1680 train_time:115652ms step_avg:87.95ms
step:1316/1680 train_time:115742ms step_avg:87.95ms
step:1317/1680 train_time:115830ms step_avg:87.95ms
step:1318/1680 train_time:115919ms step_avg:87.95ms
step:1319/1680 train_time:116008ms step_avg:87.95ms
step:1320/1680 train_time:116097ms step_avg:87.95ms
step:1321/1680 train_time:116187ms step_avg:87.95ms
step:1322/1680 train_time:116276ms step_avg:87.95ms
step:1323/1680 train_time:116366ms step_avg:87.96ms
step:1324/1680 train_time:116455ms step_avg:87.96ms
step:1325/1680 train_time:116544ms step_avg:87.96ms
step:1326/1680 train_time:116633ms step_avg:87.96ms
step:1327/1680 train_time:116721ms step_avg:87.96ms
step:1328/1680 train_time:116810ms step_avg:87.96ms
step:1329/1680 train_time:116899ms step_avg:87.96ms
step:1330/1680 train_time:116989ms step_avg:87.96ms
step:1331/1680 train_time:117079ms step_avg:87.96ms
step:1332/1680 train_time:117169ms step_avg:87.97ms
step:1333/1680 train_time:117258ms step_avg:87.97ms
step:1334/1680 train_time:117348ms step_avg:87.97ms
step:1335/1680 train_time:117437ms step_avg:87.97ms
step:1336/1680 train_time:117528ms step_avg:87.97ms
step:1337/1680 train_time:117616ms step_avg:87.97ms
step:1338/1680 train_time:117704ms step_avg:87.97ms
step:1339/1680 train_time:117793ms step_avg:87.97ms
step:1340/1680 train_time:117881ms step_avg:87.97ms
step:1341/1680 train_time:117970ms step_avg:87.97ms
step:1342/1680 train_time:118059ms step_avg:87.97ms
step:1343/1680 train_time:118149ms step_avg:87.97ms
step:1344/1680 train_time:118238ms step_avg:87.97ms
step:1345/1680 train_time:118328ms step_avg:87.98ms
step:1346/1680 train_time:118417ms step_avg:87.98ms
step:1347/1680 train_time:118507ms step_avg:87.98ms
step:1348/1680 train_time:118595ms step_avg:87.98ms
step:1349/1680 train_time:118684ms step_avg:87.98ms
step:1350/1680 train_time:118774ms step_avg:87.98ms
step:1351/1680 train_time:118862ms step_avg:87.98ms
step:1352/1680 train_time:118951ms step_avg:87.98ms
step:1353/1680 train_time:119042ms step_avg:87.98ms
step:1354/1680 train_time:119130ms step_avg:87.98ms
step:1355/1680 train_time:119220ms step_avg:87.98ms
step:1356/1680 train_time:119308ms step_avg:87.99ms
step:1357/1680 train_time:119397ms step_avg:87.99ms
step:1358/1680 train_time:119487ms step_avg:87.99ms
step:1359/1680 train_time:119576ms step_avg:87.99ms
step:1360/1680 train_time:119665ms step_avg:87.99ms
step:1361/1680 train_time:119753ms step_avg:87.99ms
step:1362/1680 train_time:119842ms step_avg:87.99ms
step:1363/1680 train_time:119930ms step_avg:87.99ms
step:1364/1680 train_time:120019ms step_avg:87.99ms
step:1365/1680 train_time:120108ms step_avg:87.99ms
step:1366/1680 train_time:120197ms step_avg:87.99ms
step:1367/1680 train_time:120285ms step_avg:87.99ms
step:1368/1680 train_time:120373ms step_avg:87.99ms
step:1369/1680 train_time:120463ms step_avg:87.99ms
step:1370/1680 train_time:120552ms step_avg:87.99ms
step:1371/1680 train_time:120641ms step_avg:88.00ms
step:1372/1680 train_time:120731ms step_avg:88.00ms
step:1373/1680 train_time:120820ms step_avg:88.00ms
step:1374/1680 train_time:120910ms step_avg:88.00ms
step:1375/1680 train_time:120999ms step_avg:88.00ms
step:1375/1680 val_loss:3.3420 train_time:121089ms step_avg:88.06ms
step:1376/1680 train_time:121108ms step_avg:88.01ms
step:1377/1680 train_time:121179ms step_avg:88.00ms
step:1378/1680 train_time:121271ms step_avg:88.01ms
step:1379/1680 train_time:121361ms step_avg:88.01ms
step:1380/1680 train_time:121449ms step_avg:88.01ms
step:1381/1680 train_time:121537ms step_avg:88.01ms
step:1382/1680 train_time:121625ms step_avg:88.01ms
step:1383/1680 train_time:121713ms step_avg:88.01ms
step:1384/1680 train_time:121801ms step_avg:88.01ms
step:1385/1680 train_time:121891ms step_avg:88.01ms
step:1386/1680 train_time:121979ms step_avg:88.01ms
step:1387/1680 train_time:122070ms step_avg:88.01ms
step:1388/1680 train_time:122160ms step_avg:88.01ms
step:1389/1680 train_time:122251ms step_avg:88.01ms
step:1390/1680 train_time:122340ms step_avg:88.01ms
step:1391/1680 train_time:122428ms step_avg:88.01ms
step:1392/1680 train_time:122517ms step_avg:88.01ms
step:1393/1680 train_time:122606ms step_avg:88.02ms
step:1394/1680 train_time:122694ms step_avg:88.02ms
step:1395/1680 train_time:122782ms step_avg:88.02ms
step:1396/1680 train_time:122870ms step_avg:88.02ms
step:1397/1680 train_time:122959ms step_avg:88.02ms
step:1398/1680 train_time:123048ms step_avg:88.02ms
step:1399/1680 train_time:123137ms step_avg:88.02ms
step:1400/1680 train_time:123227ms step_avg:88.02ms
step:1401/1680 train_time:123317ms step_avg:88.02ms
step:1402/1680 train_time:123406ms step_avg:88.02ms
step:1403/1680 train_time:123495ms step_avg:88.02ms
step:1404/1680 train_time:123584ms step_avg:88.02ms
step:1405/1680 train_time:123672ms step_avg:88.02ms
step:1406/1680 train_time:123760ms step_avg:88.02ms
step:1407/1680 train_time:123848ms step_avg:88.02ms
step:1408/1680 train_time:123937ms step_avg:88.02ms
step:1409/1680 train_time:124026ms step_avg:88.02ms
step:1410/1680 train_time:124116ms step_avg:88.03ms
step:1411/1680 train_time:124206ms step_avg:88.03ms
step:1412/1680 train_time:124296ms step_avg:88.03ms
step:1413/1680 train_time:124387ms step_avg:88.03ms
step:1414/1680 train_time:124475ms step_avg:88.03ms
step:1415/1680 train_time:124564ms step_avg:88.03ms
step:1416/1680 train_time:124652ms step_avg:88.03ms
step:1417/1680 train_time:124740ms step_avg:88.03ms
step:1418/1680 train_time:124829ms step_avg:88.03ms
step:1419/1680 train_time:124917ms step_avg:88.03ms
step:1420/1680 train_time:125006ms step_avg:88.03ms
step:1421/1680 train_time:125095ms step_avg:88.03ms
step:1422/1680 train_time:125184ms step_avg:88.03ms
step:1423/1680 train_time:125274ms step_avg:88.04ms
step:1424/1680 train_time:125365ms step_avg:88.04ms
step:1425/1680 train_time:125454ms step_avg:88.04ms
step:1426/1680 train_time:125543ms step_avg:88.04ms
step:1427/1680 train_time:125632ms step_avg:88.04ms
step:1428/1680 train_time:125720ms step_avg:88.04ms
step:1429/1680 train_time:125809ms step_avg:88.04ms
step:1430/1680 train_time:125899ms step_avg:88.04ms
step:1431/1680 train_time:125988ms step_avg:88.04ms
step:1432/1680 train_time:126076ms step_avg:88.04ms
step:1433/1680 train_time:126165ms step_avg:88.04ms
step:1434/1680 train_time:126255ms step_avg:88.04ms
step:1435/1680 train_time:126345ms step_avg:88.05ms
step:1436/1680 train_time:126434ms step_avg:88.05ms
step:1437/1680 train_time:126524ms step_avg:88.05ms
step:1438/1680 train_time:126613ms step_avg:88.05ms
step:1439/1680 train_time:126702ms step_avg:88.05ms
step:1440/1680 train_time:126790ms step_avg:88.05ms
step:1441/1680 train_time:126879ms step_avg:88.05ms
step:1442/1680 train_time:126968ms step_avg:88.05ms
step:1443/1680 train_time:127057ms step_avg:88.05ms
step:1444/1680 train_time:127146ms step_avg:88.05ms
step:1445/1680 train_time:127234ms step_avg:88.05ms
step:1446/1680 train_time:127324ms step_avg:88.05ms
step:1447/1680 train_time:127413ms step_avg:88.05ms
step:1448/1680 train_time:127502ms step_avg:88.05ms
step:1449/1680 train_time:127591ms step_avg:88.05ms
step:1450/1680 train_time:127679ms step_avg:88.05ms
step:1451/1680 train_time:127769ms step_avg:88.06ms
step:1452/1680 train_time:127858ms step_avg:88.06ms
step:1453/1680 train_time:127946ms step_avg:88.06ms
step:1454/1680 train_time:128034ms step_avg:88.06ms
step:1455/1680 train_time:128124ms step_avg:88.06ms
step:1456/1680 train_time:128213ms step_avg:88.06ms
step:1457/1680 train_time:128302ms step_avg:88.06ms
step:1458/1680 train_time:128392ms step_avg:88.06ms
step:1459/1680 train_time:128481ms step_avg:88.06ms
step:1460/1680 train_time:128571ms step_avg:88.06ms
step:1461/1680 train_time:128661ms step_avg:88.06ms
step:1462/1680 train_time:128749ms step_avg:88.06ms
step:1463/1680 train_time:128838ms step_avg:88.06ms
step:1464/1680 train_time:128927ms step_avg:88.06ms
step:1465/1680 train_time:129016ms step_avg:88.07ms
step:1466/1680 train_time:129105ms step_avg:88.07ms
step:1467/1680 train_time:129194ms step_avg:88.07ms
step:1468/1680 train_time:129282ms step_avg:88.07ms
step:1469/1680 train_time:129373ms step_avg:88.07ms
step:1470/1680 train_time:129462ms step_avg:88.07ms
step:1471/1680 train_time:129551ms step_avg:88.07ms
step:1472/1680 train_time:129640ms step_avg:88.07ms
step:1473/1680 train_time:129729ms step_avg:88.07ms
step:1474/1680 train_time:129818ms step_avg:88.07ms
step:1475/1680 train_time:129907ms step_avg:88.07ms
step:1476/1680 train_time:129996ms step_avg:88.07ms
step:1477/1680 train_time:130085ms step_avg:88.07ms
step:1478/1680 train_time:130175ms step_avg:88.07ms
step:1479/1680 train_time:130265ms step_avg:88.08ms
step:1480/1680 train_time:130354ms step_avg:88.08ms
step:1481/1680 train_time:130443ms step_avg:88.08ms
step:1482/1680 train_time:130532ms step_avg:88.08ms
step:1483/1680 train_time:130621ms step_avg:88.08ms
step:1484/1680 train_time:130710ms step_avg:88.08ms
step:1485/1680 train_time:130799ms step_avg:88.08ms
step:1486/1680 train_time:130888ms step_avg:88.08ms
step:1487/1680 train_time:130976ms step_avg:88.08ms
step:1488/1680 train_time:131065ms step_avg:88.08ms
step:1489/1680 train_time:131154ms step_avg:88.08ms
step:1490/1680 train_time:131243ms step_avg:88.08ms
step:1491/1680 train_time:131332ms step_avg:88.08ms
step:1492/1680 train_time:131422ms step_avg:88.08ms
step:1493/1680 train_time:131512ms step_avg:88.09ms
step:1494/1680 train_time:131600ms step_avg:88.09ms
step:1495/1680 train_time:131690ms step_avg:88.09ms
step:1496/1680 train_time:131779ms step_avg:88.09ms
step:1497/1680 train_time:131868ms step_avg:88.09ms
step:1498/1680 train_time:131956ms step_avg:88.09ms
step:1499/1680 train_time:132045ms step_avg:88.09ms
step:1500/1680 train_time:132133ms step_avg:88.09ms
step:1500/1680 val_loss:3.3122 train_time:132224ms step_avg:88.15ms
step:1501/1680 train_time:132243ms step_avg:88.10ms
step:1502/1680 train_time:132315ms step_avg:88.09ms
step:1503/1680 train_time:132409ms step_avg:88.10ms
step:1504/1680 train_time:132499ms step_avg:88.10ms
step:1505/1680 train_time:132587ms step_avg:88.10ms
step:1506/1680 train_time:132676ms step_avg:88.10ms
step:1507/1680 train_time:132764ms step_avg:88.10ms
step:1508/1680 train_time:132854ms step_avg:88.10ms
step:1509/1680 train_time:132941ms step_avg:88.10ms
step:1510/1680 train_time:133029ms step_avg:88.10ms
step:1511/1680 train_time:133117ms step_avg:88.10ms
step:1512/1680 train_time:133206ms step_avg:88.10ms
step:1513/1680 train_time:133296ms step_avg:88.10ms
step:1514/1680 train_time:133388ms step_avg:88.10ms
step:1515/1680 train_time:133478ms step_avg:88.10ms
step:1516/1680 train_time:133567ms step_avg:88.11ms
step:1517/1680 train_time:133657ms step_avg:88.11ms
step:1518/1680 train_time:133746ms step_avg:88.11ms
step:1519/1680 train_time:133835ms step_avg:88.11ms
step:1520/1680 train_time:133924ms step_avg:88.11ms
step:1521/1680 train_time:134012ms step_avg:88.11ms
step:1522/1680 train_time:134100ms step_avg:88.11ms
step:1523/1680 train_time:134188ms step_avg:88.11ms
step:1524/1680 train_time:134278ms step_avg:88.11ms
step:1525/1680 train_time:134367ms step_avg:88.11ms
step:1526/1680 train_time:134457ms step_avg:88.11ms
step:1527/1680 train_time:134547ms step_avg:88.11ms
step:1528/1680 train_time:134636ms step_avg:88.11ms
step:1529/1680 train_time:134726ms step_avg:88.11ms
step:1530/1680 train_time:134815ms step_avg:88.11ms
step:1531/1680 train_time:134904ms step_avg:88.11ms
step:1532/1680 train_time:134993ms step_avg:88.12ms
step:1533/1680 train_time:135081ms step_avg:88.12ms
step:1534/1680 train_time:135169ms step_avg:88.12ms
step:1535/1680 train_time:135259ms step_avg:88.12ms
step:1536/1680 train_time:135349ms step_avg:88.12ms
step:1537/1680 train_time:135439ms step_avg:88.12ms
step:1538/1680 train_time:135528ms step_avg:88.12ms
step:1539/1680 train_time:135618ms step_avg:88.12ms
step:1540/1680 train_time:135707ms step_avg:88.12ms
step:1541/1680 train_time:135796ms step_avg:88.12ms
step:1542/1680 train_time:135885ms step_avg:88.12ms
step:1543/1680 train_time:135973ms step_avg:88.12ms
step:1544/1680 train_time:136062ms step_avg:88.12ms
step:1545/1680 train_time:136151ms step_avg:88.12ms
step:1546/1680 train_time:136239ms step_avg:88.12ms
step:1547/1680 train_time:136329ms step_avg:88.12ms
step:1548/1680 train_time:136418ms step_avg:88.13ms
step:1549/1680 train_time:136506ms step_avg:88.13ms
step:1550/1680 train_time:136596ms step_avg:88.13ms
step:1551/1680 train_time:136685ms step_avg:88.13ms
step:1552/1680 train_time:136773ms step_avg:88.13ms
step:1553/1680 train_time:136862ms step_avg:88.13ms
step:1554/1680 train_time:136951ms step_avg:88.13ms
step:1555/1680 train_time:137041ms step_avg:88.13ms
step:1556/1680 train_time:137130ms step_avg:88.13ms
step:1557/1680 train_time:137218ms step_avg:88.13ms
step:1558/1680 train_time:137308ms step_avg:88.13ms
step:1559/1680 train_time:137397ms step_avg:88.13ms
step:1560/1680 train_time:137486ms step_avg:88.13ms
step:1561/1680 train_time:137575ms step_avg:88.13ms
step:1562/1680 train_time:137664ms step_avg:88.13ms
step:1563/1680 train_time:137754ms step_avg:88.13ms
step:1564/1680 train_time:137842ms step_avg:88.13ms
step:1565/1680 train_time:137931ms step_avg:88.13ms
step:1566/1680 train_time:138020ms step_avg:88.14ms
step:1567/1680 train_time:138110ms step_avg:88.14ms
step:1568/1680 train_time:138198ms step_avg:88.14ms
step:1569/1680 train_time:138286ms step_avg:88.14ms
step:1570/1680 train_time:138375ms step_avg:88.14ms
step:1571/1680 train_time:138464ms step_avg:88.14ms
step:1572/1680 train_time:138553ms step_avg:88.14ms
step:1573/1680 train_time:138642ms step_avg:88.14ms
step:1574/1680 train_time:138731ms step_avg:88.14ms
step:1575/1680 train_time:138819ms step_avg:88.14ms
step:1576/1680 train_time:138908ms step_avg:88.14ms
step:1577/1680 train_time:138997ms step_avg:88.14ms
step:1578/1680 train_time:139087ms step_avg:88.14ms
step:1579/1680 train_time:139176ms step_avg:88.14ms
step:1580/1680 train_time:139265ms step_avg:88.14ms
step:1581/1680 train_time:139354ms step_avg:88.14ms
step:1582/1680 train_time:139443ms step_avg:88.14ms
step:1583/1680 train_time:139531ms step_avg:88.14ms
step:1584/1680 train_time:139621ms step_avg:88.14ms
step:1585/1680 train_time:139710ms step_avg:88.14ms
step:1586/1680 train_time:139798ms step_avg:88.15ms
step:1587/1680 train_time:139888ms step_avg:88.15ms
step:1588/1680 train_time:139976ms step_avg:88.15ms
step:1589/1680 train_time:140065ms step_avg:88.15ms
step:1590/1680 train_time:140154ms step_avg:88.15ms
step:1591/1680 train_time:140244ms step_avg:88.15ms
step:1592/1680 train_time:140332ms step_avg:88.15ms
step:1593/1680 train_time:140421ms step_avg:88.15ms
step:1594/1680 train_time:140510ms step_avg:88.15ms
step:1595/1680 train_time:140599ms step_avg:88.15ms
step:1596/1680 train_time:140688ms step_avg:88.15ms
step:1597/1680 train_time:140777ms step_avg:88.15ms
step:1598/1680 train_time:140866ms step_avg:88.15ms
step:1599/1680 train_time:140955ms step_avg:88.15ms
step:1600/1680 train_time:141044ms step_avg:88.15ms
step:1601/1680 train_time:141133ms step_avg:88.15ms
step:1602/1680 train_time:141221ms step_avg:88.15ms
step:1603/1680 train_time:141310ms step_avg:88.15ms
step:1604/1680 train_time:141399ms step_avg:88.15ms
step:1605/1680 train_time:141487ms step_avg:88.15ms
step:1606/1680 train_time:141577ms step_avg:88.15ms
step:1607/1680 train_time:141665ms step_avg:88.16ms
step:1608/1680 train_time:141755ms step_avg:88.16ms
step:1609/1680 train_time:141844ms step_avg:88.16ms
step:1610/1680 train_time:141933ms step_avg:88.16ms
step:1611/1680 train_time:142022ms step_avg:88.16ms
step:1612/1680 train_time:142111ms step_avg:88.16ms
step:1613/1680 train_time:142200ms step_avg:88.16ms
step:1614/1680 train_time:142290ms step_avg:88.16ms
step:1615/1680 train_time:142378ms step_avg:88.16ms
step:1616/1680 train_time:142467ms step_avg:88.16ms
step:1617/1680 train_time:142556ms step_avg:88.16ms
step:1618/1680 train_time:142645ms step_avg:88.16ms
step:1619/1680 train_time:142735ms step_avg:88.16ms
step:1620/1680 train_time:142824ms step_avg:88.16ms
step:1621/1680 train_time:142915ms step_avg:88.16ms
step:1622/1680 train_time:143004ms step_avg:88.17ms
step:1623/1680 train_time:143094ms step_avg:88.17ms
step:1624/1680 train_time:143182ms step_avg:88.17ms
step:1625/1680 train_time:143272ms step_avg:88.17ms
step:1625/1680 val_loss:3.2882 train_time:143362ms step_avg:88.22ms
step:1626/1680 train_time:143380ms step_avg:88.18ms
step:1627/1680 train_time:143453ms step_avg:88.17ms
step:1628/1680 train_time:143545ms step_avg:88.17ms
step:1629/1680 train_time:143634ms step_avg:88.17ms
step:1630/1680 train_time:143722ms step_avg:88.17ms
step:1631/1680 train_time:143811ms step_avg:88.17ms
step:1632/1680 train_time:143898ms step_avg:88.17ms
step:1633/1680 train_time:143986ms step_avg:88.17ms
step:1634/1680 train_time:144074ms step_avg:88.17ms
step:1635/1680 train_time:144162ms step_avg:88.17ms
step:1636/1680 train_time:144251ms step_avg:88.17ms
step:1637/1680 train_time:144341ms step_avg:88.17ms
step:1638/1680 train_time:144433ms step_avg:88.18ms
step:1639/1680 train_time:144524ms step_avg:88.18ms
step:1640/1680 train_time:144613ms step_avg:88.18ms
step:1641/1680 train_time:144702ms step_avg:88.18ms
step:1642/1680 train_time:144791ms step_avg:88.18ms
step:1643/1680 train_time:144879ms step_avg:88.18ms
step:1644/1680 train_time:144967ms step_avg:88.18ms
step:1645/1680 train_time:145056ms step_avg:88.18ms
step:1646/1680 train_time:145144ms step_avg:88.18ms
step:1647/1680 train_time:145232ms step_avg:88.18ms
step:1648/1680 train_time:145321ms step_avg:88.18ms
step:1649/1680 train_time:145411ms step_avg:88.18ms
step:1650/1680 train_time:145501ms step_avg:88.18ms
step:1651/1680 train_time:145591ms step_avg:88.18ms
step:1652/1680 train_time:145680ms step_avg:88.18ms
step:1653/1680 train_time:145769ms step_avg:88.18ms
step:1654/1680 train_time:145858ms step_avg:88.18ms
step:1655/1680 train_time:145946ms step_avg:88.18ms
step:1656/1680 train_time:146034ms step_avg:88.18ms
step:1657/1680 train_time:146122ms step_avg:88.18ms
step:1658/1680 train_time:146211ms step_avg:88.19ms
step:1659/1680 train_time:146300ms step_avg:88.19ms
step:1660/1680 train_time:146390ms step_avg:88.19ms
step:1661/1680 train_time:146480ms step_avg:88.19ms
step:1662/1680 train_time:146569ms step_avg:88.19ms
step:1663/1680 train_time:146659ms step_avg:88.19ms
step:1664/1680 train_time:146748ms step_avg:88.19ms
step:1665/1680 train_time:146837ms step_avg:88.19ms
step:1666/1680 train_time:146926ms step_avg:88.19ms
step:1667/1680 train_time:147015ms step_avg:88.19ms
step:1668/1680 train_time:147103ms step_avg:88.19ms
step:1669/1680 train_time:147193ms step_avg:88.19ms
step:1670/1680 train_time:147281ms step_avg:88.19ms
step:1671/1680 train_time:147371ms step_avg:88.19ms
step:1672/1680 train_time:147460ms step_avg:88.19ms
step:1673/1680 train_time:147550ms step_avg:88.19ms
step:1674/1680 train_time:147639ms step_avg:88.20ms
step:1675/1680 train_time:147729ms step_avg:88.20ms
step:1676/1680 train_time:147817ms step_avg:88.20ms
step:1677/1680 train_time:147906ms step_avg:88.20ms
step:1678/1680 train_time:147995ms step_avg:88.20ms
step:1679/1680 train_time:148084ms step_avg:88.20ms
step:1680/1680 train_time:148172ms step_avg:88.20ms
step:1680/1680 val_loss:3.2774 train_time:148262ms step_avg:88.25ms
peak memory allocated: 30760 MiB reserved: 46214 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:37:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0            116W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            122W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0            123W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    160405      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    160406      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    160407      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    160408      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    160409      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    160410      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    160411      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    160412      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    160406      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    160407      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    160408      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    160409      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    160410      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    160411      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    160412      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:149ms step_avg:149.48ms
step:2/1680 train_time:169ms step_avg:84.48ms
step:3/1680 train_time:236ms step_avg:78.63ms
step:4/1680 train_time:321ms step_avg:80.22ms
step:5/1680 train_time:407ms step_avg:81.38ms
step:6/1680 train_time:493ms step_avg:82.16ms
step:7/1680 train_time:579ms step_avg:82.71ms
step:8/1680 train_time:665ms step_avg:83.17ms
step:9/1680 train_time:752ms step_avg:83.53ms
step:10/1680 train_time:838ms step_avg:83.82ms
step:11/1680 train_time:925ms step_avg:84.05ms
step:12/1680 train_time:1012ms step_avg:84.34ms
step:13/1680 train_time:1104ms step_avg:84.95ms
step:14/1680 train_time:1193ms step_avg:85.23ms
step:15/1680 train_time:1281ms step_avg:85.39ms
step:16/1680 train_time:1369ms step_avg:85.55ms
step:17/1680 train_time:1455ms step_avg:85.61ms
step:18/1680 train_time:1542ms step_avg:85.64ms
step:19/1680 train_time:1628ms step_avg:85.68ms
step:20/1680 train_time:1715ms step_avg:85.73ms
step:21/1680 train_time:1801ms step_avg:85.78ms
step:22/1680 train_time:1888ms step_avg:85.82ms
step:23/1680 train_time:1976ms step_avg:85.92ms
step:24/1680 train_time:2065ms step_avg:86.06ms
step:25/1680 train_time:2154ms step_avg:86.17ms
step:26/1680 train_time:2243ms step_avg:86.26ms
step:27/1680 train_time:2330ms step_avg:86.30ms
step:28/1680 train_time:2418ms step_avg:86.35ms
step:29/1680 train_time:2505ms step_avg:86.38ms
step:30/1680 train_time:2592ms step_avg:86.41ms
step:31/1680 train_time:2679ms step_avg:86.42ms
step:32/1680 train_time:2765ms step_avg:86.41ms
step:33/1680 train_time:2852ms step_avg:86.44ms
step:34/1680 train_time:2939ms step_avg:86.45ms
step:35/1680 train_time:3028ms step_avg:86.51ms
step:36/1680 train_time:3117ms step_avg:86.57ms
step:37/1680 train_time:3205ms step_avg:86.63ms
step:38/1680 train_time:3294ms step_avg:86.67ms
step:39/1680 train_time:3381ms step_avg:86.68ms
step:40/1680 train_time:3468ms step_avg:86.71ms
step:41/1680 train_time:3556ms step_avg:86.73ms
step:42/1680 train_time:3643ms step_avg:86.74ms
step:43/1680 train_time:3729ms step_avg:86.73ms
step:44/1680 train_time:3817ms step_avg:86.74ms
step:45/1680 train_time:3905ms step_avg:86.78ms
step:46/1680 train_time:3993ms step_avg:86.80ms
step:47/1680 train_time:4082ms step_avg:86.85ms
step:48/1680 train_time:4170ms step_avg:86.88ms
step:49/1680 train_time:4258ms step_avg:86.91ms
step:50/1680 train_time:4346ms step_avg:86.93ms
step:51/1680 train_time:4434ms step_avg:86.94ms
step:52/1680 train_time:4521ms step_avg:86.94ms
step:53/1680 train_time:4608ms step_avg:86.94ms
step:54/1680 train_time:4695ms step_avg:86.94ms
step:55/1680 train_time:4781ms step_avg:86.93ms
step:56/1680 train_time:4869ms step_avg:86.94ms
step:57/1680 train_time:4956ms step_avg:86.95ms
step:58/1680 train_time:5045ms step_avg:86.98ms
step:59/1680 train_time:5132ms step_avg:86.99ms
step:60/1680 train_time:5221ms step_avg:87.01ms
step:61/1680 train_time:5308ms step_avg:87.01ms
step:62/1680 train_time:5395ms step_avg:87.02ms
step:63/1680 train_time:5483ms step_avg:87.04ms
step:64/1680 train_time:5570ms step_avg:87.03ms
step:65/1680 train_time:5657ms step_avg:87.03ms
step:66/1680 train_time:5743ms step_avg:87.02ms
step:67/1680 train_time:5830ms step_avg:87.02ms
step:68/1680 train_time:5918ms step_avg:87.03ms
step:69/1680 train_time:6006ms step_avg:87.04ms
step:70/1680 train_time:6093ms step_avg:87.04ms
step:71/1680 train_time:6181ms step_avg:87.05ms
step:72/1680 train_time:6267ms step_avg:87.05ms
step:73/1680 train_time:6355ms step_avg:87.05ms
step:74/1680 train_time:6442ms step_avg:87.06ms
step:75/1680 train_time:6530ms step_avg:87.06ms
step:76/1680 train_time:6617ms step_avg:87.06ms
step:77/1680 train_time:6704ms step_avg:87.06ms
step:78/1680 train_time:6790ms step_avg:87.05ms
step:79/1680 train_time:6878ms step_avg:87.06ms
step:80/1680 train_time:6965ms step_avg:87.07ms
step:81/1680 train_time:7052ms step_avg:87.07ms
step:82/1680 train_time:7140ms step_avg:87.07ms
step:83/1680 train_time:7228ms step_avg:87.08ms
step:84/1680 train_time:7315ms step_avg:87.09ms
step:85/1680 train_time:7402ms step_avg:87.09ms
step:86/1680 train_time:7489ms step_avg:87.09ms
step:87/1680 train_time:7577ms step_avg:87.09ms
step:88/1680 train_time:7664ms step_avg:87.09ms
step:89/1680 train_time:7751ms step_avg:87.09ms
step:90/1680 train_time:7838ms step_avg:87.09ms
step:91/1680 train_time:7926ms step_avg:87.10ms
step:92/1680 train_time:8013ms step_avg:87.10ms
step:93/1680 train_time:8100ms step_avg:87.10ms
step:94/1680 train_time:8187ms step_avg:87.09ms
step:95/1680 train_time:8274ms step_avg:87.09ms
step:96/1680 train_time:8361ms step_avg:87.09ms
step:97/1680 train_time:8448ms step_avg:87.09ms
step:98/1680 train_time:8535ms step_avg:87.09ms
step:99/1680 train_time:8623ms step_avg:87.10ms
step:100/1680 train_time:8710ms step_avg:87.10ms
step:101/1680 train_time:8799ms step_avg:87.11ms
step:102/1680 train_time:8885ms step_avg:87.11ms
step:103/1680 train_time:8972ms step_avg:87.11ms
step:104/1680 train_time:9059ms step_avg:87.11ms
step:105/1680 train_time:9147ms step_avg:87.11ms
step:106/1680 train_time:9234ms step_avg:87.12ms
step:107/1680 train_time:9321ms step_avg:87.11ms
step:108/1680 train_time:9408ms step_avg:87.11ms
step:109/1680 train_time:9497ms step_avg:87.12ms
step:110/1680 train_time:9584ms step_avg:87.13ms
step:111/1680 train_time:9671ms step_avg:87.13ms
step:112/1680 train_time:9759ms step_avg:87.13ms
step:113/1680 train_time:9845ms step_avg:87.13ms
step:114/1680 train_time:9933ms step_avg:87.13ms
step:115/1680 train_time:10020ms step_avg:87.13ms
step:116/1680 train_time:10107ms step_avg:87.13ms
step:117/1680 train_time:10195ms step_avg:87.14ms
step:118/1680 train_time:10282ms step_avg:87.14ms
step:119/1680 train_time:10369ms step_avg:87.14ms
step:120/1680 train_time:10456ms step_avg:87.14ms
step:121/1680 train_time:10543ms step_avg:87.13ms
step:122/1680 train_time:10630ms step_avg:87.13ms
step:123/1680 train_time:10717ms step_avg:87.13ms
step:124/1680 train_time:10805ms step_avg:87.13ms
step:125/1680 train_time:10891ms step_avg:87.13ms
step:125/1680 val_loss:4.3060 train_time:10980ms step_avg:87.84ms
step:126/1680 train_time:11004ms step_avg:87.34ms
step:127/1680 train_time:11069ms step_avg:87.16ms
step:128/1680 train_time:11163ms step_avg:87.21ms
step:129/1680 train_time:11253ms step_avg:87.24ms
step:130/1680 train_time:11341ms step_avg:87.24ms
step:131/1680 train_time:11427ms step_avg:87.23ms
step:132/1680 train_time:11514ms step_avg:87.23ms
step:133/1680 train_time:11600ms step_avg:87.22ms
step:134/1680 train_time:11686ms step_avg:87.21ms
step:135/1680 train_time:11772ms step_avg:87.20ms
step:136/1680 train_time:11858ms step_avg:87.19ms
step:137/1680 train_time:11945ms step_avg:87.19ms
step:138/1680 train_time:12033ms step_avg:87.19ms
step:139/1680 train_time:12123ms step_avg:87.21ms
step:140/1680 train_time:12212ms step_avg:87.23ms
step:141/1680 train_time:12300ms step_avg:87.24ms
step:142/1680 train_time:12388ms step_avg:87.24ms
step:143/1680 train_time:12475ms step_avg:87.23ms
step:144/1680 train_time:12561ms step_avg:87.23ms
step:145/1680 train_time:12647ms step_avg:87.22ms
step:146/1680 train_time:12733ms step_avg:87.21ms
step:147/1680 train_time:12819ms step_avg:87.20ms
step:148/1680 train_time:12905ms step_avg:87.20ms
step:149/1680 train_time:12993ms step_avg:87.20ms
step:150/1680 train_time:13081ms step_avg:87.20ms
step:151/1680 train_time:13170ms step_avg:87.22ms
step:152/1680 train_time:13258ms step_avg:87.22ms
step:153/1680 train_time:13346ms step_avg:87.23ms
step:154/1680 train_time:13433ms step_avg:87.23ms
step:155/1680 train_time:13520ms step_avg:87.23ms
step:156/1680 train_time:13607ms step_avg:87.22ms
step:157/1680 train_time:13693ms step_avg:87.22ms
step:158/1680 train_time:13780ms step_avg:87.21ms
step:159/1680 train_time:13866ms step_avg:87.21ms
step:160/1680 train_time:13953ms step_avg:87.21ms
step:161/1680 train_time:14041ms step_avg:87.21ms
step:162/1680 train_time:14129ms step_avg:87.22ms
step:163/1680 train_time:14217ms step_avg:87.22ms
step:164/1680 train_time:14305ms step_avg:87.23ms
step:165/1680 train_time:14393ms step_avg:87.23ms
step:166/1680 train_time:14481ms step_avg:87.23ms
step:167/1680 train_time:14567ms step_avg:87.23ms
step:168/1680 train_time:14654ms step_avg:87.23ms
step:169/1680 train_time:14741ms step_avg:87.23ms
step:170/1680 train_time:14827ms step_avg:87.22ms
step:171/1680 train_time:14914ms step_avg:87.22ms
step:172/1680 train_time:15001ms step_avg:87.22ms
step:173/1680 train_time:15089ms step_avg:87.22ms
step:174/1680 train_time:15176ms step_avg:87.22ms
step:175/1680 train_time:15264ms step_avg:87.22ms
step:176/1680 train_time:15352ms step_avg:87.23ms
step:177/1680 train_time:15439ms step_avg:87.23ms
step:178/1680 train_time:15526ms step_avg:87.23ms
step:179/1680 train_time:15613ms step_avg:87.22ms
step:180/1680 train_time:15700ms step_avg:87.22ms
step:181/1680 train_time:15786ms step_avg:87.22ms
step:182/1680 train_time:15874ms step_avg:87.22ms
step:183/1680 train_time:15961ms step_avg:87.22ms
step:184/1680 train_time:16048ms step_avg:87.22ms
step:185/1680 train_time:16135ms step_avg:87.22ms
step:186/1680 train_time:16222ms step_avg:87.22ms
step:187/1680 train_time:16310ms step_avg:87.22ms
step:188/1680 train_time:16397ms step_avg:87.22ms
step:189/1680 train_time:16486ms step_avg:87.22ms
step:190/1680 train_time:16573ms step_avg:87.23ms
step:191/1680 train_time:16660ms step_avg:87.22ms
step:192/1680 train_time:16746ms step_avg:87.22ms
step:193/1680 train_time:16833ms step_avg:87.22ms
step:194/1680 train_time:16920ms step_avg:87.22ms
step:195/1680 train_time:17007ms step_avg:87.22ms
step:196/1680 train_time:17095ms step_avg:87.22ms
step:197/1680 train_time:17183ms step_avg:87.22ms
step:198/1680 train_time:17270ms step_avg:87.22ms
step:199/1680 train_time:17357ms step_avg:87.22ms
step:200/1680 train_time:17445ms step_avg:87.22ms
step:201/1680 train_time:17533ms step_avg:87.23ms
step:202/1680 train_time:17620ms step_avg:87.23ms
step:203/1680 train_time:17706ms step_avg:87.22ms
step:204/1680 train_time:17794ms step_avg:87.22ms
step:205/1680 train_time:17881ms step_avg:87.22ms
step:206/1680 train_time:17968ms step_avg:87.22ms
step:207/1680 train_time:18056ms step_avg:87.23ms
step:208/1680 train_time:18143ms step_avg:87.23ms
step:209/1680 train_time:18230ms step_avg:87.23ms
step:210/1680 train_time:18317ms step_avg:87.23ms
step:211/1680 train_time:18404ms step_avg:87.22ms
step:212/1680 train_time:18492ms step_avg:87.22ms
step:213/1680 train_time:18579ms step_avg:87.23ms
step:214/1680 train_time:18666ms step_avg:87.22ms
step:215/1680 train_time:18753ms step_avg:87.22ms
step:216/1680 train_time:18841ms step_avg:87.23ms
step:217/1680 train_time:18928ms step_avg:87.22ms
step:218/1680 train_time:19015ms step_avg:87.22ms
step:219/1680 train_time:19102ms step_avg:87.22ms
step:220/1680 train_time:19189ms step_avg:87.22ms
step:221/1680 train_time:19277ms step_avg:87.23ms
step:222/1680 train_time:19364ms step_avg:87.22ms
step:223/1680 train_time:19451ms step_avg:87.23ms
step:224/1680 train_time:19539ms step_avg:87.23ms
step:225/1680 train_time:19626ms step_avg:87.23ms
step:226/1680 train_time:19713ms step_avg:87.23ms
step:227/1680 train_time:19801ms step_avg:87.23ms
step:228/1680 train_time:19888ms step_avg:87.23ms
step:229/1680 train_time:19975ms step_avg:87.23ms
step:230/1680 train_time:20062ms step_avg:87.22ms
step:231/1680 train_time:20149ms step_avg:87.23ms
step:232/1680 train_time:20236ms step_avg:87.23ms
step:233/1680 train_time:20323ms step_avg:87.23ms
step:234/1680 train_time:20411ms step_avg:87.22ms
step:235/1680 train_time:20498ms step_avg:87.22ms
step:236/1680 train_time:20585ms step_avg:87.22ms
step:237/1680 train_time:20672ms step_avg:87.22ms
step:238/1680 train_time:20759ms step_avg:87.22ms
step:239/1680 train_time:20846ms step_avg:87.22ms
step:240/1680 train_time:20933ms step_avg:87.22ms
step:241/1680 train_time:21020ms step_avg:87.22ms
step:242/1680 train_time:21107ms step_avg:87.22ms
step:243/1680 train_time:21195ms step_avg:87.22ms
step:244/1680 train_time:21282ms step_avg:87.22ms
step:245/1680 train_time:21369ms step_avg:87.22ms
step:246/1680 train_time:21457ms step_avg:87.22ms
step:247/1680 train_time:21544ms step_avg:87.22ms
step:248/1680 train_time:21631ms step_avg:87.22ms
step:249/1680 train_time:21718ms step_avg:87.22ms
step:250/1680 train_time:21805ms step_avg:87.22ms
step:250/1680 val_loss:3.9809 train_time:21894ms step_avg:87.58ms
step:251/1680 train_time:21917ms step_avg:87.32ms
step:252/1680 train_time:21984ms step_avg:87.24ms
step:253/1680 train_time:22072ms step_avg:87.24ms
step:254/1680 train_time:22160ms step_avg:87.24ms
step:255/1680 train_time:22247ms step_avg:87.24ms
step:256/1680 train_time:22333ms step_avg:87.24ms
step:257/1680 train_time:22420ms step_avg:87.24ms
step:258/1680 train_time:22506ms step_avg:87.23ms
step:259/1680 train_time:22592ms step_avg:87.23ms
step:260/1680 train_time:22679ms step_avg:87.23ms
step:261/1680 train_time:22765ms step_avg:87.22ms
step:262/1680 train_time:22853ms step_avg:87.23ms
step:263/1680 train_time:22944ms step_avg:87.24ms
step:264/1680 train_time:23031ms step_avg:87.24ms
step:265/1680 train_time:23119ms step_avg:87.24ms
step:266/1680 train_time:23208ms step_avg:87.25ms
step:267/1680 train_time:23294ms step_avg:87.24ms
step:268/1680 train_time:23381ms step_avg:87.24ms
step:269/1680 train_time:23468ms step_avg:87.24ms
step:270/1680 train_time:23555ms step_avg:87.24ms
step:271/1680 train_time:23641ms step_avg:87.24ms
step:272/1680 train_time:23728ms step_avg:87.23ms
step:273/1680 train_time:23815ms step_avg:87.24ms
step:274/1680 train_time:23903ms step_avg:87.24ms
step:275/1680 train_time:23991ms step_avg:87.24ms
step:276/1680 train_time:24079ms step_avg:87.24ms
step:277/1680 train_time:24167ms step_avg:87.24ms
step:278/1680 train_time:24254ms step_avg:87.25ms
step:279/1680 train_time:24341ms step_avg:87.24ms
step:280/1680 train_time:24428ms step_avg:87.24ms
step:281/1680 train_time:24515ms step_avg:87.24ms
step:282/1680 train_time:24602ms step_avg:87.24ms
step:283/1680 train_time:24689ms step_avg:87.24ms
step:284/1680 train_time:24776ms step_avg:87.24ms
step:285/1680 train_time:24864ms step_avg:87.24ms
step:286/1680 train_time:24953ms step_avg:87.25ms
step:287/1680 train_time:25040ms step_avg:87.25ms
step:288/1680 train_time:25127ms step_avg:87.25ms
step:289/1680 train_time:25216ms step_avg:87.25ms
step:290/1680 train_time:25304ms step_avg:87.25ms
step:291/1680 train_time:25390ms step_avg:87.25ms
step:292/1680 train_time:25477ms step_avg:87.25ms
step:293/1680 train_time:25564ms step_avg:87.25ms
step:294/1680 train_time:25650ms step_avg:87.25ms
step:295/1680 train_time:25737ms step_avg:87.24ms
step:296/1680 train_time:25824ms step_avg:87.24ms
step:297/1680 train_time:25911ms step_avg:87.24ms
step:298/1680 train_time:25999ms step_avg:87.25ms
step:299/1680 train_time:26087ms step_avg:87.25ms
step:300/1680 train_time:26174ms step_avg:87.25ms
step:301/1680 train_time:26262ms step_avg:87.25ms
step:302/1680 train_time:26350ms step_avg:87.25ms
step:303/1680 train_time:26437ms step_avg:87.25ms
step:304/1680 train_time:26525ms step_avg:87.25ms
step:305/1680 train_time:26611ms step_avg:87.25ms
step:306/1680 train_time:26698ms step_avg:87.25ms
step:307/1680 train_time:26785ms step_avg:87.25ms
step:308/1680 train_time:26872ms step_avg:87.25ms
step:309/1680 train_time:26959ms step_avg:87.25ms
step:310/1680 train_time:27047ms step_avg:87.25ms
step:311/1680 train_time:27135ms step_avg:87.25ms
step:312/1680 train_time:27223ms step_avg:87.25ms
step:313/1680 train_time:27309ms step_avg:87.25ms
step:314/1680 train_time:27397ms step_avg:87.25ms
step:315/1680 train_time:27484ms step_avg:87.25ms
step:316/1680 train_time:27571ms step_avg:87.25ms
step:317/1680 train_time:27658ms step_avg:87.25ms
step:318/1680 train_time:27746ms step_avg:87.25ms
step:319/1680 train_time:27833ms step_avg:87.25ms
step:320/1680 train_time:27920ms step_avg:87.25ms
step:321/1680 train_time:28008ms step_avg:87.25ms
step:322/1680 train_time:28095ms step_avg:87.25ms
step:323/1680 train_time:28182ms step_avg:87.25ms
step:324/1680 train_time:28269ms step_avg:87.25ms
step:325/1680 train_time:28357ms step_avg:87.25ms
step:326/1680 train_time:28445ms step_avg:87.26ms
step:327/1680 train_time:28532ms step_avg:87.25ms
step:328/1680 train_time:28619ms step_avg:87.25ms
step:329/1680 train_time:28706ms step_avg:87.25ms
step:330/1680 train_time:28793ms step_avg:87.25ms
step:331/1680 train_time:28880ms step_avg:87.25ms
step:332/1680 train_time:28968ms step_avg:87.25ms
step:333/1680 train_time:29055ms step_avg:87.25ms
step:334/1680 train_time:29143ms step_avg:87.25ms
step:335/1680 train_time:29230ms step_avg:87.25ms
step:336/1680 train_time:29318ms step_avg:87.26ms
step:337/1680 train_time:29406ms step_avg:87.26ms
step:338/1680 train_time:29492ms step_avg:87.26ms
step:339/1680 train_time:29579ms step_avg:87.26ms
step:340/1680 train_time:29667ms step_avg:87.26ms
step:341/1680 train_time:29754ms step_avg:87.25ms
step:342/1680 train_time:29841ms step_avg:87.25ms
step:343/1680 train_time:29928ms step_avg:87.25ms
step:344/1680 train_time:30015ms step_avg:87.25ms
step:345/1680 train_time:30103ms step_avg:87.25ms
step:346/1680 train_time:30190ms step_avg:87.26ms
step:347/1680 train_time:30278ms step_avg:87.26ms
step:348/1680 train_time:30365ms step_avg:87.26ms
step:349/1680 train_time:30452ms step_avg:87.26ms
step:350/1680 train_time:30539ms step_avg:87.26ms
step:351/1680 train_time:30626ms step_avg:87.25ms
step:352/1680 train_time:30714ms step_avg:87.25ms
step:353/1680 train_time:30801ms step_avg:87.25ms
step:354/1680 train_time:30888ms step_avg:87.25ms
step:355/1680 train_time:30975ms step_avg:87.25ms
step:356/1680 train_time:31063ms step_avg:87.26ms
step:357/1680 train_time:31150ms step_avg:87.26ms
step:358/1680 train_time:31238ms step_avg:87.26ms
step:359/1680 train_time:31325ms step_avg:87.26ms
step:360/1680 train_time:31412ms step_avg:87.26ms
step:361/1680 train_time:31500ms step_avg:87.26ms
step:362/1680 train_time:31587ms step_avg:87.26ms
step:363/1680 train_time:31674ms step_avg:87.26ms
step:364/1680 train_time:31761ms step_avg:87.26ms
step:365/1680 train_time:31848ms step_avg:87.25ms
step:366/1680 train_time:31935ms step_avg:87.25ms
step:367/1680 train_time:32022ms step_avg:87.25ms
step:368/1680 train_time:32109ms step_avg:87.25ms
step:369/1680 train_time:32197ms step_avg:87.25ms
step:370/1680 train_time:32284ms step_avg:87.25ms
step:371/1680 train_time:32371ms step_avg:87.25ms
step:372/1680 train_time:32458ms step_avg:87.25ms
step:373/1680 train_time:32545ms step_avg:87.25ms
step:374/1680 train_time:32633ms step_avg:87.25ms
step:375/1680 train_time:32720ms step_avg:87.25ms
step:375/1680 val_loss:3.8227 train_time:32809ms step_avg:87.49ms
step:376/1680 train_time:32830ms step_avg:87.31ms
step:377/1680 train_time:32901ms step_avg:87.27ms
step:378/1680 train_time:32990ms step_avg:87.27ms
step:379/1680 train_time:33077ms step_avg:87.27ms
step:380/1680 train_time:33163ms step_avg:87.27ms
step:381/1680 train_time:33250ms step_avg:87.27ms
step:382/1680 train_time:33336ms step_avg:87.27ms
step:383/1680 train_time:33423ms step_avg:87.27ms
step:384/1680 train_time:33509ms step_avg:87.26ms
step:385/1680 train_time:33595ms step_avg:87.26ms
step:386/1680 train_time:33682ms step_avg:87.26ms
step:387/1680 train_time:33770ms step_avg:87.26ms
step:388/1680 train_time:33859ms step_avg:87.27ms
step:389/1680 train_time:33948ms step_avg:87.27ms
step:390/1680 train_time:34037ms step_avg:87.27ms
step:391/1680 train_time:34124ms step_avg:87.27ms
step:392/1680 train_time:34211ms step_avg:87.27ms
step:393/1680 train_time:34299ms step_avg:87.27ms
step:394/1680 train_time:34385ms step_avg:87.27ms
step:395/1680 train_time:34471ms step_avg:87.27ms
step:396/1680 train_time:34558ms step_avg:87.27ms
step:397/1680 train_time:34645ms step_avg:87.27ms
step:398/1680 train_time:34731ms step_avg:87.26ms
step:399/1680 train_time:34819ms step_avg:87.26ms
step:400/1680 train_time:34907ms step_avg:87.27ms
step:401/1680 train_time:34995ms step_avg:87.27ms
step:402/1680 train_time:35084ms step_avg:87.27ms
step:403/1680 train_time:35171ms step_avg:87.27ms
step:404/1680 train_time:35259ms step_avg:87.27ms
step:405/1680 train_time:35345ms step_avg:87.27ms
step:406/1680 train_time:35432ms step_avg:87.27ms
step:407/1680 train_time:35518ms step_avg:87.27ms
step:408/1680 train_time:35605ms step_avg:87.27ms
step:409/1680 train_time:35692ms step_avg:87.27ms
step:410/1680 train_time:35779ms step_avg:87.27ms
step:411/1680 train_time:35868ms step_avg:87.27ms
step:412/1680 train_time:35956ms step_avg:87.27ms
step:413/1680 train_time:36043ms step_avg:87.27ms
step:414/1680 train_time:36131ms step_avg:87.27ms
step:415/1680 train_time:36219ms step_avg:87.27ms
step:416/1680 train_time:36306ms step_avg:87.27ms
step:417/1680 train_time:36393ms step_avg:87.27ms
step:418/1680 train_time:36480ms step_avg:87.27ms
step:419/1680 train_time:36567ms step_avg:87.27ms
step:420/1680 train_time:36653ms step_avg:87.27ms
step:421/1680 train_time:36740ms step_avg:87.27ms
step:422/1680 train_time:36828ms step_avg:87.27ms
step:423/1680 train_time:36916ms step_avg:87.27ms
step:424/1680 train_time:37003ms step_avg:87.27ms
step:425/1680 train_time:37091ms step_avg:87.27ms
step:426/1680 train_time:37178ms step_avg:87.27ms
step:427/1680 train_time:37265ms step_avg:87.27ms
step:428/1680 train_time:37353ms step_avg:87.27ms
step:429/1680 train_time:37440ms step_avg:87.27ms
step:430/1680 train_time:37527ms step_avg:87.27ms
step:431/1680 train_time:37614ms step_avg:87.27ms
step:432/1680 train_time:37700ms step_avg:87.27ms
step:433/1680 train_time:37787ms step_avg:87.27ms
step:434/1680 train_time:37875ms step_avg:87.27ms
step:435/1680 train_time:37962ms step_avg:87.27ms
step:436/1680 train_time:38050ms step_avg:87.27ms
step:437/1680 train_time:38138ms step_avg:87.27ms
step:438/1680 train_time:38225ms step_avg:87.27ms
step:439/1680 train_time:38312ms step_avg:87.27ms
step:440/1680 train_time:38399ms step_avg:87.27ms
step:441/1680 train_time:38487ms step_avg:87.27ms
step:442/1680 train_time:38574ms step_avg:87.27ms
step:443/1680 train_time:38661ms step_avg:87.27ms
step:444/1680 train_time:38748ms step_avg:87.27ms
step:445/1680 train_time:38835ms step_avg:87.27ms
step:446/1680 train_time:38922ms step_avg:87.27ms
step:447/1680 train_time:39010ms step_avg:87.27ms
step:448/1680 train_time:39098ms step_avg:87.27ms
step:449/1680 train_time:39185ms step_avg:87.27ms
step:450/1680 train_time:39272ms step_avg:87.27ms
step:451/1680 train_time:39360ms step_avg:87.27ms
step:452/1680 train_time:39447ms step_avg:87.27ms
step:453/1680 train_time:39535ms step_avg:87.27ms
step:454/1680 train_time:39621ms step_avg:87.27ms
step:455/1680 train_time:39709ms step_avg:87.27ms
step:456/1680 train_time:39796ms step_avg:87.27ms
step:457/1680 train_time:39883ms step_avg:87.27ms
step:458/1680 train_time:39970ms step_avg:87.27ms
step:459/1680 train_time:40058ms step_avg:87.27ms
step:460/1680 train_time:40145ms step_avg:87.27ms
step:461/1680 train_time:40232ms step_avg:87.27ms
step:462/1680 train_time:40319ms step_avg:87.27ms
step:463/1680 train_time:40407ms step_avg:87.27ms
step:464/1680 train_time:40495ms step_avg:87.27ms
step:465/1680 train_time:40581ms step_avg:87.27ms
step:466/1680 train_time:40668ms step_avg:87.27ms
step:467/1680 train_time:40755ms step_avg:87.27ms
step:468/1680 train_time:40843ms step_avg:87.27ms
step:469/1680 train_time:40930ms step_avg:87.27ms
step:470/1680 train_time:41018ms step_avg:87.27ms
step:471/1680 train_time:41105ms step_avg:87.27ms
step:472/1680 train_time:41192ms step_avg:87.27ms
step:473/1680 train_time:41280ms step_avg:87.27ms
step:474/1680 train_time:41367ms step_avg:87.27ms
step:475/1680 train_time:41454ms step_avg:87.27ms
step:476/1680 train_time:41541ms step_avg:87.27ms
step:477/1680 train_time:41629ms step_avg:87.27ms
step:478/1680 train_time:41716ms step_avg:87.27ms
step:479/1680 train_time:41804ms step_avg:87.27ms
step:480/1680 train_time:41890ms step_avg:87.27ms
step:481/1680 train_time:41978ms step_avg:87.27ms
step:482/1680 train_time:42066ms step_avg:87.27ms
step:483/1680 train_time:42153ms step_avg:87.27ms
step:484/1680 train_time:42240ms step_avg:87.27ms
step:485/1680 train_time:42328ms step_avg:87.27ms
step:486/1680 train_time:42415ms step_avg:87.27ms
step:487/1680 train_time:42502ms step_avg:87.27ms
step:488/1680 train_time:42589ms step_avg:87.27ms
step:489/1680 train_time:42676ms step_avg:87.27ms
step:490/1680 train_time:42763ms step_avg:87.27ms
step:491/1680 train_time:42850ms step_avg:87.27ms
step:492/1680 train_time:42937ms step_avg:87.27ms
step:493/1680 train_time:43024ms step_avg:87.27ms
step:494/1680 train_time:43112ms step_avg:87.27ms
step:495/1680 train_time:43199ms step_avg:87.27ms
step:496/1680 train_time:43287ms step_avg:87.27ms
step:497/1680 train_time:43374ms step_avg:87.27ms
step:498/1680 train_time:43461ms step_avg:87.27ms
step:499/1680 train_time:43548ms step_avg:87.27ms
step:500/1680 train_time:43635ms step_avg:87.27ms
step:500/1680 val_loss:3.7186 train_time:43724ms step_avg:87.45ms
step:501/1680 train_time:43745ms step_avg:87.31ms
step:502/1680 train_time:43812ms step_avg:87.27ms
step:503/1680 train_time:43904ms step_avg:87.28ms
step:504/1680 train_time:43993ms step_avg:87.29ms
step:505/1680 train_time:44079ms step_avg:87.29ms
step:506/1680 train_time:44165ms step_avg:87.28ms
step:507/1680 train_time:44251ms step_avg:87.28ms
step:508/1680 train_time:44338ms step_avg:87.28ms
step:509/1680 train_time:44425ms step_avg:87.28ms
step:510/1680 train_time:44512ms step_avg:87.28ms
step:511/1680 train_time:44598ms step_avg:87.28ms
step:512/1680 train_time:44686ms step_avg:87.28ms
step:513/1680 train_time:44774ms step_avg:87.28ms
step:514/1680 train_time:44864ms step_avg:87.28ms
step:515/1680 train_time:44952ms step_avg:87.29ms
step:516/1680 train_time:45040ms step_avg:87.29ms
step:517/1680 train_time:45126ms step_avg:87.29ms
step:518/1680 train_time:45214ms step_avg:87.29ms
step:519/1680 train_time:45301ms step_avg:87.28ms
step:520/1680 train_time:45387ms step_avg:87.28ms
step:521/1680 train_time:45474ms step_avg:87.28ms
step:522/1680 train_time:45560ms step_avg:87.28ms
step:523/1680 train_time:45648ms step_avg:87.28ms
step:524/1680 train_time:45735ms step_avg:87.28ms
step:525/1680 train_time:45824ms step_avg:87.28ms
step:526/1680 train_time:45911ms step_avg:87.28ms
step:527/1680 train_time:45999ms step_avg:87.28ms
step:528/1680 train_time:46086ms step_avg:87.28ms
step:529/1680 train_time:46174ms step_avg:87.28ms
step:530/1680 train_time:46261ms step_avg:87.29ms
step:531/1680 train_time:46348ms step_avg:87.28ms
step:532/1680 train_time:46435ms step_avg:87.28ms
step:533/1680 train_time:46521ms step_avg:87.28ms
step:534/1680 train_time:46608ms step_avg:87.28ms
step:535/1680 train_time:46695ms step_avg:87.28ms
step:536/1680 train_time:46783ms step_avg:87.28ms
step:537/1680 train_time:46871ms step_avg:87.28ms
step:538/1680 train_time:46959ms step_avg:87.28ms
step:539/1680 train_time:47047ms step_avg:87.29ms
step:540/1680 train_time:47134ms step_avg:87.29ms
step:541/1680 train_time:47222ms step_avg:87.29ms
step:542/1680 train_time:47309ms step_avg:87.29ms
step:543/1680 train_time:47395ms step_avg:87.28ms
step:544/1680 train_time:47483ms step_avg:87.28ms
step:545/1680 train_time:47570ms step_avg:87.28ms
step:546/1680 train_time:47657ms step_avg:87.28ms
step:547/1680 train_time:47744ms step_avg:87.28ms
step:548/1680 train_time:47832ms step_avg:87.29ms
step:549/1680 train_time:47921ms step_avg:87.29ms
step:550/1680 train_time:48010ms step_avg:87.29ms
step:551/1680 train_time:48098ms step_avg:87.29ms
step:552/1680 train_time:48187ms step_avg:87.30ms
step:553/1680 train_time:48276ms step_avg:87.30ms
step:554/1680 train_time:48364ms step_avg:87.30ms
step:555/1680 train_time:48452ms step_avg:87.30ms
step:556/1680 train_time:48539ms step_avg:87.30ms
step:557/1680 train_time:48628ms step_avg:87.30ms
step:558/1680 train_time:48716ms step_avg:87.30ms
step:559/1680 train_time:48804ms step_avg:87.31ms
step:560/1680 train_time:48893ms step_avg:87.31ms
step:561/1680 train_time:48982ms step_avg:87.31ms
step:562/1680 train_time:49071ms step_avg:87.32ms
step:563/1680 train_time:49160ms step_avg:87.32ms
step:564/1680 train_time:49248ms step_avg:87.32ms
step:565/1680 train_time:49337ms step_avg:87.32ms
step:566/1680 train_time:49427ms step_avg:87.33ms
step:567/1680 train_time:49515ms step_avg:87.33ms
step:568/1680 train_time:49603ms step_avg:87.33ms
step:569/1680 train_time:49692ms step_avg:87.33ms
step:570/1680 train_time:49781ms step_avg:87.33ms
step:571/1680 train_time:49869ms step_avg:87.34ms
step:572/1680 train_time:49958ms step_avg:87.34ms
step:573/1680 train_time:50047ms step_avg:87.34ms
step:574/1680 train_time:50136ms step_avg:87.34ms
step:575/1680 train_time:50225ms step_avg:87.35ms
step:576/1680 train_time:50313ms step_avg:87.35ms
step:577/1680 train_time:50402ms step_avg:87.35ms
step:578/1680 train_time:50490ms step_avg:87.35ms
step:579/1680 train_time:50580ms step_avg:87.36ms
step:580/1680 train_time:50668ms step_avg:87.36ms
step:581/1680 train_time:50756ms step_avg:87.36ms
step:582/1680 train_time:50845ms step_avg:87.36ms
step:583/1680 train_time:50934ms step_avg:87.37ms
step:584/1680 train_time:51023ms step_avg:87.37ms
step:585/1680 train_time:51112ms step_avg:87.37ms
step:586/1680 train_time:51200ms step_avg:87.37ms
step:587/1680 train_time:51288ms step_avg:87.37ms
step:588/1680 train_time:51377ms step_avg:87.38ms
step:589/1680 train_time:51465ms step_avg:87.38ms
step:590/1680 train_time:51555ms step_avg:87.38ms
step:591/1680 train_time:51643ms step_avg:87.38ms
step:592/1680 train_time:51732ms step_avg:87.39ms
step:593/1680 train_time:51820ms step_avg:87.39ms
step:594/1680 train_time:51909ms step_avg:87.39ms
step:595/1680 train_time:51997ms step_avg:87.39ms
step:596/1680 train_time:52086ms step_avg:87.39ms
step:597/1680 train_time:52174ms step_avg:87.39ms
step:598/1680 train_time:52264ms step_avg:87.40ms
step:599/1680 train_time:52353ms step_avg:87.40ms
step:600/1680 train_time:52441ms step_avg:87.40ms
step:601/1680 train_time:52530ms step_avg:87.40ms
step:602/1680 train_time:52618ms step_avg:87.41ms
step:603/1680 train_time:52707ms step_avg:87.41ms
step:604/1680 train_time:52795ms step_avg:87.41ms
step:605/1680 train_time:52884ms step_avg:87.41ms
step:606/1680 train_time:52971ms step_avg:87.41ms
step:607/1680 train_time:53060ms step_avg:87.41ms
step:608/1680 train_time:53147ms step_avg:87.41ms
step:609/1680 train_time:53236ms step_avg:87.42ms
step:610/1680 train_time:53325ms step_avg:87.42ms
step:611/1680 train_time:53413ms step_avg:87.42ms
step:612/1680 train_time:53501ms step_avg:87.42ms
step:613/1680 train_time:53590ms step_avg:87.42ms
step:614/1680 train_time:53679ms step_avg:87.43ms
step:615/1680 train_time:53768ms step_avg:87.43ms
step:616/1680 train_time:53856ms step_avg:87.43ms
step:617/1680 train_time:53945ms step_avg:87.43ms
step:618/1680 train_time:54034ms step_avg:87.43ms
step:619/1680 train_time:54122ms step_avg:87.43ms
step:620/1680 train_time:54210ms step_avg:87.44ms
step:621/1680 train_time:54298ms step_avg:87.44ms
step:622/1680 train_time:54387ms step_avg:87.44ms
step:623/1680 train_time:54475ms step_avg:87.44ms
step:624/1680 train_time:54565ms step_avg:87.44ms
step:625/1680 train_time:54653ms step_avg:87.44ms
step:625/1680 val_loss:3.6169 train_time:54743ms step_avg:87.59ms
step:626/1680 train_time:54765ms step_avg:87.48ms
step:627/1680 train_time:54834ms step_avg:87.45ms
step:628/1680 train_time:54923ms step_avg:87.46ms
step:629/1680 train_time:55016ms step_avg:87.47ms
step:630/1680 train_time:55107ms step_avg:87.47ms
step:631/1680 train_time:55194ms step_avg:87.47ms
step:632/1680 train_time:55281ms step_avg:87.47ms
step:633/1680 train_time:55368ms step_avg:87.47ms
step:634/1680 train_time:55455ms step_avg:87.47ms
step:635/1680 train_time:55541ms step_avg:87.47ms
step:636/1680 train_time:55631ms step_avg:87.47ms
step:637/1680 train_time:55724ms step_avg:87.48ms
step:638/1680 train_time:55814ms step_avg:87.48ms
step:639/1680 train_time:55904ms step_avg:87.49ms
step:640/1680 train_time:55993ms step_avg:87.49ms
step:641/1680 train_time:56081ms step_avg:87.49ms
step:642/1680 train_time:56169ms step_avg:87.49ms
step:643/1680 train_time:56258ms step_avg:87.49ms
step:644/1680 train_time:56345ms step_avg:87.49ms
step:645/1680 train_time:56433ms step_avg:87.49ms
step:646/1680 train_time:56520ms step_avg:87.49ms
step:647/1680 train_time:56608ms step_avg:87.49ms
step:648/1680 train_time:56697ms step_avg:87.50ms
step:649/1680 train_time:56786ms step_avg:87.50ms
step:650/1680 train_time:56875ms step_avg:87.50ms
step:651/1680 train_time:56964ms step_avg:87.50ms
step:652/1680 train_time:57053ms step_avg:87.50ms
step:653/1680 train_time:57141ms step_avg:87.51ms
step:654/1680 train_time:57229ms step_avg:87.51ms
step:655/1680 train_time:57317ms step_avg:87.51ms
step:656/1680 train_time:57405ms step_avg:87.51ms
step:657/1680 train_time:57493ms step_avg:87.51ms
step:658/1680 train_time:57581ms step_avg:87.51ms
step:659/1680 train_time:57671ms step_avg:87.51ms
step:660/1680 train_time:57761ms step_avg:87.52ms
step:661/1680 train_time:57850ms step_avg:87.52ms
step:662/1680 train_time:57938ms step_avg:87.52ms
step:663/1680 train_time:58028ms step_avg:87.52ms
step:664/1680 train_time:58115ms step_avg:87.52ms
step:665/1680 train_time:58203ms step_avg:87.52ms
step:666/1680 train_time:58292ms step_avg:87.52ms
step:667/1680 train_time:58379ms step_avg:87.52ms
step:668/1680 train_time:58467ms step_avg:87.53ms
step:669/1680 train_time:58556ms step_avg:87.53ms
step:670/1680 train_time:58643ms step_avg:87.53ms
step:671/1680 train_time:58732ms step_avg:87.53ms
step:672/1680 train_time:58821ms step_avg:87.53ms
step:673/1680 train_time:58909ms step_avg:87.53ms
step:674/1680 train_time:58999ms step_avg:87.54ms
step:675/1680 train_time:59087ms step_avg:87.54ms
step:676/1680 train_time:59175ms step_avg:87.54ms
step:677/1680 train_time:59263ms step_avg:87.54ms
step:678/1680 train_time:59352ms step_avg:87.54ms
step:679/1680 train_time:59439ms step_avg:87.54ms
step:680/1680 train_time:59527ms step_avg:87.54ms
step:681/1680 train_time:59615ms step_avg:87.54ms
step:682/1680 train_time:59703ms step_avg:87.54ms
step:683/1680 train_time:59792ms step_avg:87.54ms
step:684/1680 train_time:59881ms step_avg:87.55ms
step:685/1680 train_time:59970ms step_avg:87.55ms
step:686/1680 train_time:60059ms step_avg:87.55ms
step:687/1680 train_time:60146ms step_avg:87.55ms
step:688/1680 train_time:60234ms step_avg:87.55ms
step:689/1680 train_time:60322ms step_avg:87.55ms
step:690/1680 train_time:60411ms step_avg:87.55ms
step:691/1680 train_time:60498ms step_avg:87.55ms
step:692/1680 train_time:60586ms step_avg:87.55ms
step:693/1680 train_time:60674ms step_avg:87.55ms
step:694/1680 train_time:60762ms step_avg:87.55ms
step:695/1680 train_time:60851ms step_avg:87.55ms
step:696/1680 train_time:60939ms step_avg:87.56ms
step:697/1680 train_time:61028ms step_avg:87.56ms
step:698/1680 train_time:61117ms step_avg:87.56ms
step:699/1680 train_time:61204ms step_avg:87.56ms
step:700/1680 train_time:61294ms step_avg:87.56ms
step:701/1680 train_time:61382ms step_avg:87.56ms
step:702/1680 train_time:61470ms step_avg:87.56ms
step:703/1680 train_time:61559ms step_avg:87.57ms
step:704/1680 train_time:61647ms step_avg:87.57ms
step:705/1680 train_time:61735ms step_avg:87.57ms
step:706/1680 train_time:61823ms step_avg:87.57ms
step:707/1680 train_time:61913ms step_avg:87.57ms
step:708/1680 train_time:62003ms step_avg:87.57ms
step:709/1680 train_time:62091ms step_avg:87.58ms
step:710/1680 train_time:62179ms step_avg:87.58ms
step:711/1680 train_time:62267ms step_avg:87.58ms
step:712/1680 train_time:62356ms step_avg:87.58ms
step:713/1680 train_time:62444ms step_avg:87.58ms
step:714/1680 train_time:62533ms step_avg:87.58ms
step:715/1680 train_time:62621ms step_avg:87.58ms
step:716/1680 train_time:62710ms step_avg:87.58ms
step:717/1680 train_time:62799ms step_avg:87.59ms
step:718/1680 train_time:62887ms step_avg:87.59ms
step:719/1680 train_time:62976ms step_avg:87.59ms
step:720/1680 train_time:63065ms step_avg:87.59ms
step:721/1680 train_time:63153ms step_avg:87.59ms
step:722/1680 train_time:63241ms step_avg:87.59ms
step:723/1680 train_time:63330ms step_avg:87.59ms
step:724/1680 train_time:63418ms step_avg:87.59ms
step:725/1680 train_time:63506ms step_avg:87.59ms
step:726/1680 train_time:63594ms step_avg:87.60ms
step:727/1680 train_time:63682ms step_avg:87.60ms
step:728/1680 train_time:63771ms step_avg:87.60ms
step:729/1680 train_time:63859ms step_avg:87.60ms
step:730/1680 train_time:63947ms step_avg:87.60ms
step:731/1680 train_time:64036ms step_avg:87.60ms
step:732/1680 train_time:64123ms step_avg:87.60ms
step:733/1680 train_time:64212ms step_avg:87.60ms
step:734/1680 train_time:64301ms step_avg:87.60ms
step:735/1680 train_time:64389ms step_avg:87.60ms
step:736/1680 train_time:64477ms step_avg:87.60ms
step:737/1680 train_time:64565ms step_avg:87.61ms
step:738/1680 train_time:64653ms step_avg:87.61ms
step:739/1680 train_time:64741ms step_avg:87.61ms
step:740/1680 train_time:64830ms step_avg:87.61ms
step:741/1680 train_time:64919ms step_avg:87.61ms
step:742/1680 train_time:65007ms step_avg:87.61ms
step:743/1680 train_time:65096ms step_avg:87.61ms
step:744/1680 train_time:65185ms step_avg:87.61ms
step:745/1680 train_time:65273ms step_avg:87.61ms
step:746/1680 train_time:65361ms step_avg:87.62ms
step:747/1680 train_time:65449ms step_avg:87.62ms
step:748/1680 train_time:65538ms step_avg:87.62ms
step:749/1680 train_time:65626ms step_avg:87.62ms
step:750/1680 train_time:65714ms step_avg:87.62ms
step:750/1680 val_loss:3.5644 train_time:65804ms step_avg:87.74ms
step:751/1680 train_time:65823ms step_avg:87.65ms
step:752/1680 train_time:65895ms step_avg:87.63ms
step:753/1680 train_time:65989ms step_avg:87.63ms
step:754/1680 train_time:66078ms step_avg:87.64ms
step:755/1680 train_time:66166ms step_avg:87.64ms
step:756/1680 train_time:66253ms step_avg:87.64ms
step:757/1680 train_time:66341ms step_avg:87.64ms
step:758/1680 train_time:66428ms step_avg:87.64ms
step:759/1680 train_time:66516ms step_avg:87.64ms
step:760/1680 train_time:66604ms step_avg:87.64ms
step:761/1680 train_time:66692ms step_avg:87.64ms
step:762/1680 train_time:66781ms step_avg:87.64ms
step:763/1680 train_time:66870ms step_avg:87.64ms
step:764/1680 train_time:66960ms step_avg:87.64ms
step:765/1680 train_time:67050ms step_avg:87.65ms
step:766/1680 train_time:67139ms step_avg:87.65ms
step:767/1680 train_time:67228ms step_avg:87.65ms
step:768/1680 train_time:67315ms step_avg:87.65ms
step:769/1680 train_time:67403ms step_avg:87.65ms
step:770/1680 train_time:67490ms step_avg:87.65ms
step:771/1680 train_time:67577ms step_avg:87.65ms
step:772/1680 train_time:67665ms step_avg:87.65ms
step:773/1680 train_time:67754ms step_avg:87.65ms
step:774/1680 train_time:67843ms step_avg:87.65ms
step:775/1680 train_time:67932ms step_avg:87.65ms
step:776/1680 train_time:68022ms step_avg:87.66ms
step:777/1680 train_time:68111ms step_avg:87.66ms
step:778/1680 train_time:68199ms step_avg:87.66ms
step:779/1680 train_time:68287ms step_avg:87.66ms
step:780/1680 train_time:68375ms step_avg:87.66ms
step:781/1680 train_time:68463ms step_avg:87.66ms
step:782/1680 train_time:68551ms step_avg:87.66ms
step:783/1680 train_time:68638ms step_avg:87.66ms
step:784/1680 train_time:68726ms step_avg:87.66ms
step:785/1680 train_time:68816ms step_avg:87.66ms
step:786/1680 train_time:68904ms step_avg:87.66ms
step:787/1680 train_time:68994ms step_avg:87.67ms
step:788/1680 train_time:69083ms step_avg:87.67ms
step:789/1680 train_time:69171ms step_avg:87.67ms
step:790/1680 train_time:69259ms step_avg:87.67ms
step:791/1680 train_time:69348ms step_avg:87.67ms
step:792/1680 train_time:69436ms step_avg:87.67ms
step:793/1680 train_time:69524ms step_avg:87.67ms
step:794/1680 train_time:69613ms step_avg:87.67ms
step:795/1680 train_time:69700ms step_avg:87.67ms
step:796/1680 train_time:69789ms step_avg:87.67ms
step:797/1680 train_time:69878ms step_avg:87.68ms
step:798/1680 train_time:69968ms step_avg:87.68ms
step:799/1680 train_time:70057ms step_avg:87.68ms
step:800/1680 train_time:70146ms step_avg:87.68ms
step:801/1680 train_time:70234ms step_avg:87.68ms
step:802/1680 train_time:70322ms step_avg:87.68ms
step:803/1680 train_time:70410ms step_avg:87.68ms
step:804/1680 train_time:70498ms step_avg:87.68ms
step:805/1680 train_time:70586ms step_avg:87.68ms
step:806/1680 train_time:70674ms step_avg:87.69ms
step:807/1680 train_time:70762ms step_avg:87.68ms
step:808/1680 train_time:70850ms step_avg:87.69ms
step:809/1680 train_time:70939ms step_avg:87.69ms
step:810/1680 train_time:71027ms step_avg:87.69ms
step:811/1680 train_time:71116ms step_avg:87.69ms
step:812/1680 train_time:71205ms step_avg:87.69ms
step:813/1680 train_time:71294ms step_avg:87.69ms
step:814/1680 train_time:71381ms step_avg:87.69ms
step:815/1680 train_time:71470ms step_avg:87.69ms
step:816/1680 train_time:71558ms step_avg:87.69ms
step:817/1680 train_time:71647ms step_avg:87.69ms
step:818/1680 train_time:71735ms step_avg:87.70ms
step:819/1680 train_time:71823ms step_avg:87.70ms
step:820/1680 train_time:71911ms step_avg:87.70ms
step:821/1680 train_time:72000ms step_avg:87.70ms
step:822/1680 train_time:72088ms step_avg:87.70ms
step:823/1680 train_time:72177ms step_avg:87.70ms
step:824/1680 train_time:72266ms step_avg:87.70ms
step:825/1680 train_time:72355ms step_avg:87.70ms
step:826/1680 train_time:72443ms step_avg:87.70ms
step:827/1680 train_time:72532ms step_avg:87.71ms
step:828/1680 train_time:72620ms step_avg:87.71ms
step:829/1680 train_time:72709ms step_avg:87.71ms
step:830/1680 train_time:72797ms step_avg:87.71ms
step:831/1680 train_time:72886ms step_avg:87.71ms
step:832/1680 train_time:72974ms step_avg:87.71ms
step:833/1680 train_time:73062ms step_avg:87.71ms
step:834/1680 train_time:73151ms step_avg:87.71ms
step:835/1680 train_time:73239ms step_avg:87.71ms
step:836/1680 train_time:73328ms step_avg:87.71ms
step:837/1680 train_time:73417ms step_avg:87.71ms
step:838/1680 train_time:73505ms step_avg:87.71ms
step:839/1680 train_time:73592ms step_avg:87.71ms
step:840/1680 train_time:73681ms step_avg:87.71ms
step:841/1680 train_time:73769ms step_avg:87.72ms
step:842/1680 train_time:73858ms step_avg:87.72ms
step:843/1680 train_time:73947ms step_avg:87.72ms
step:844/1680 train_time:74035ms step_avg:87.72ms
step:845/1680 train_time:74124ms step_avg:87.72ms
step:846/1680 train_time:74212ms step_avg:87.72ms
step:847/1680 train_time:74300ms step_avg:87.72ms
step:848/1680 train_time:74390ms step_avg:87.72ms
step:849/1680 train_time:74478ms step_avg:87.72ms
step:850/1680 train_time:74567ms step_avg:87.73ms
step:851/1680 train_time:74656ms step_avg:87.73ms
step:852/1680 train_time:74744ms step_avg:87.73ms
step:853/1680 train_time:74832ms step_avg:87.73ms
step:854/1680 train_time:74921ms step_avg:87.73ms
step:855/1680 train_time:75010ms step_avg:87.73ms
step:856/1680 train_time:75098ms step_avg:87.73ms
step:857/1680 train_time:75187ms step_avg:87.73ms
step:858/1680 train_time:75276ms step_avg:87.73ms
step:859/1680 train_time:75365ms step_avg:87.74ms
step:860/1680 train_time:75453ms step_avg:87.74ms
step:861/1680 train_time:75541ms step_avg:87.74ms
step:862/1680 train_time:75629ms step_avg:87.74ms
step:863/1680 train_time:75718ms step_avg:87.74ms
step:864/1680 train_time:75806ms step_avg:87.74ms
step:865/1680 train_time:75895ms step_avg:87.74ms
step:866/1680 train_time:75984ms step_avg:87.74ms
step:867/1680 train_time:76072ms step_avg:87.74ms
step:868/1680 train_time:76161ms step_avg:87.74ms
step:869/1680 train_time:76249ms step_avg:87.74ms
step:870/1680 train_time:76338ms step_avg:87.74ms
step:871/1680 train_time:76426ms step_avg:87.75ms
step:872/1680 train_time:76514ms step_avg:87.75ms
step:873/1680 train_time:76602ms step_avg:87.75ms
step:874/1680 train_time:76690ms step_avg:87.75ms
step:875/1680 train_time:76778ms step_avg:87.75ms
step:875/1680 val_loss:3.5198 train_time:76869ms step_avg:87.85ms
step:876/1680 train_time:76889ms step_avg:87.77ms
step:877/1680 train_time:76961ms step_avg:87.75ms
step:878/1680 train_time:77052ms step_avg:87.76ms
step:879/1680 train_time:77140ms step_avg:87.76ms
step:880/1680 train_time:77228ms step_avg:87.76ms
step:881/1680 train_time:77317ms step_avg:87.76ms
step:882/1680 train_time:77404ms step_avg:87.76ms
step:883/1680 train_time:77491ms step_avg:87.76ms
step:884/1680 train_time:77579ms step_avg:87.76ms
step:885/1680 train_time:77668ms step_avg:87.76ms
step:886/1680 train_time:77755ms step_avg:87.76ms
step:887/1680 train_time:77843ms step_avg:87.76ms
step:888/1680 train_time:77933ms step_avg:87.76ms
step:889/1680 train_time:78024ms step_avg:87.77ms
step:890/1680 train_time:78113ms step_avg:87.77ms
step:891/1680 train_time:78202ms step_avg:87.77ms
step:892/1680 train_time:78291ms step_avg:87.77ms
step:893/1680 train_time:78378ms step_avg:87.77ms
step:894/1680 train_time:78467ms step_avg:87.77ms
step:895/1680 train_time:78554ms step_avg:87.77ms
step:896/1680 train_time:78642ms step_avg:87.77ms
step:897/1680 train_time:78730ms step_avg:87.77ms
step:898/1680 train_time:78818ms step_avg:87.77ms
step:899/1680 train_time:78908ms step_avg:87.77ms
step:900/1680 train_time:78997ms step_avg:87.77ms
step:901/1680 train_time:79086ms step_avg:87.78ms
step:902/1680 train_time:79174ms step_avg:87.78ms
step:903/1680 train_time:79264ms step_avg:87.78ms
step:904/1680 train_time:79352ms step_avg:87.78ms
step:905/1680 train_time:79441ms step_avg:87.78ms
step:906/1680 train_time:79530ms step_avg:87.78ms
step:907/1680 train_time:79617ms step_avg:87.78ms
step:908/1680 train_time:79706ms step_avg:87.78ms
step:909/1680 train_time:79793ms step_avg:87.78ms
step:910/1680 train_time:79882ms step_avg:87.78ms
step:911/1680 train_time:79971ms step_avg:87.78ms
step:912/1680 train_time:80059ms step_avg:87.78ms
step:913/1680 train_time:80148ms step_avg:87.78ms
step:914/1680 train_time:80236ms step_avg:87.79ms
step:915/1680 train_time:80324ms step_avg:87.79ms
step:916/1680 train_time:80412ms step_avg:87.79ms
step:917/1680 train_time:80501ms step_avg:87.79ms
step:918/1680 train_time:80589ms step_avg:87.79ms
step:919/1680 train_time:80677ms step_avg:87.79ms
step:920/1680 train_time:80766ms step_avg:87.79ms
step:921/1680 train_time:80853ms step_avg:87.79ms
step:922/1680 train_time:80942ms step_avg:87.79ms
step:923/1680 train_time:81030ms step_avg:87.79ms
step:924/1680 train_time:81119ms step_avg:87.79ms
step:925/1680 train_time:81208ms step_avg:87.79ms
step:926/1680 train_time:81297ms step_avg:87.79ms
step:927/1680 train_time:81386ms step_avg:87.79ms
step:928/1680 train_time:81474ms step_avg:87.80ms
step:929/1680 train_time:81563ms step_avg:87.80ms
step:930/1680 train_time:81652ms step_avg:87.80ms
step:931/1680 train_time:81741ms step_avg:87.80ms
step:932/1680 train_time:81829ms step_avg:87.80ms
step:933/1680 train_time:81917ms step_avg:87.80ms
step:934/1680 train_time:82006ms step_avg:87.80ms
step:935/1680 train_time:82093ms step_avg:87.80ms
step:936/1680 train_time:82183ms step_avg:87.80ms
step:937/1680 train_time:82272ms step_avg:87.80ms
step:938/1680 train_time:82360ms step_avg:87.80ms
step:939/1680 train_time:82449ms step_avg:87.80ms
step:940/1680 train_time:82537ms step_avg:87.81ms
step:941/1680 train_time:82626ms step_avg:87.81ms
step:942/1680 train_time:82714ms step_avg:87.81ms
step:943/1680 train_time:82802ms step_avg:87.81ms
step:944/1680 train_time:82891ms step_avg:87.81ms
step:945/1680 train_time:82979ms step_avg:87.81ms
step:946/1680 train_time:83067ms step_avg:87.81ms
step:947/1680 train_time:83156ms step_avg:87.81ms
step:948/1680 train_time:83245ms step_avg:87.81ms
step:949/1680 train_time:83333ms step_avg:87.81ms
step:950/1680 train_time:83421ms step_avg:87.81ms
step:951/1680 train_time:83510ms step_avg:87.81ms
step:952/1680 train_time:83598ms step_avg:87.81ms
step:953/1680 train_time:83687ms step_avg:87.81ms
step:954/1680 train_time:83775ms step_avg:87.81ms
step:955/1680 train_time:83864ms step_avg:87.82ms
step:956/1680 train_time:83952ms step_avg:87.82ms
step:957/1680 train_time:84041ms step_avg:87.82ms
step:958/1680 train_time:84130ms step_avg:87.82ms
step:959/1680 train_time:84218ms step_avg:87.82ms
step:960/1680 train_time:84307ms step_avg:87.82ms
step:961/1680 train_time:84394ms step_avg:87.82ms
step:962/1680 train_time:84483ms step_avg:87.82ms
step:963/1680 train_time:84572ms step_avg:87.82ms
step:964/1680 train_time:84661ms step_avg:87.82ms
step:965/1680 train_time:84749ms step_avg:87.82ms
step:966/1680 train_time:84838ms step_avg:87.82ms
step:967/1680 train_time:84926ms step_avg:87.82ms
step:968/1680 train_time:85014ms step_avg:87.82ms
step:969/1680 train_time:85103ms step_avg:87.83ms
step:970/1680 train_time:85191ms step_avg:87.83ms
step:971/1680 train_time:85279ms step_avg:87.83ms
step:972/1680 train_time:85368ms step_avg:87.83ms
step:973/1680 train_time:85456ms step_avg:87.83ms
step:974/1680 train_time:85544ms step_avg:87.83ms
step:975/1680 train_time:85632ms step_avg:87.83ms
step:976/1680 train_time:85720ms step_avg:87.83ms
step:977/1680 train_time:85808ms step_avg:87.83ms
step:978/1680 train_time:85897ms step_avg:87.83ms
step:979/1680 train_time:85985ms step_avg:87.83ms
step:980/1680 train_time:86073ms step_avg:87.83ms
step:981/1680 train_time:86162ms step_avg:87.83ms
step:982/1680 train_time:86250ms step_avg:87.83ms
step:983/1680 train_time:86338ms step_avg:87.83ms
step:984/1680 train_time:86427ms step_avg:87.83ms
step:985/1680 train_time:86515ms step_avg:87.83ms
step:986/1680 train_time:86604ms step_avg:87.83ms
step:987/1680 train_time:86692ms step_avg:87.83ms
step:988/1680 train_time:86780ms step_avg:87.83ms
step:989/1680 train_time:86869ms step_avg:87.84ms
step:990/1680 train_time:86957ms step_avg:87.84ms
step:991/1680 train_time:87045ms step_avg:87.84ms
step:992/1680 train_time:87132ms step_avg:87.84ms
step:993/1680 train_time:87221ms step_avg:87.84ms
step:994/1680 train_time:87310ms step_avg:87.84ms
step:995/1680 train_time:87398ms step_avg:87.84ms
step:996/1680 train_time:87487ms step_avg:87.84ms
step:997/1680 train_time:87575ms step_avg:87.84ms
step:998/1680 train_time:87665ms step_avg:87.84ms
step:999/1680 train_time:87753ms step_avg:87.84ms
step:1000/1680 train_time:87841ms step_avg:87.84ms
step:1000/1680 val_loss:3.4705 train_time:87931ms step_avg:87.93ms
step:1001/1680 train_time:87950ms step_avg:87.86ms
step:1002/1680 train_time:88026ms step_avg:87.85ms
step:1003/1680 train_time:88115ms step_avg:87.85ms
step:1004/1680 train_time:88205ms step_avg:87.85ms
step:1005/1680 train_time:88292ms step_avg:87.85ms
step:1006/1680 train_time:88379ms step_avg:87.85ms
step:1007/1680 train_time:88466ms step_avg:87.85ms
step:1008/1680 train_time:88554ms step_avg:87.85ms
step:1009/1680 train_time:88641ms step_avg:87.85ms
step:1010/1680 train_time:88729ms step_avg:87.85ms
step:1011/1680 train_time:88816ms step_avg:87.85ms
step:1012/1680 train_time:88906ms step_avg:87.85ms
step:1013/1680 train_time:88996ms step_avg:87.85ms
step:1014/1680 train_time:89086ms step_avg:87.86ms
step:1015/1680 train_time:89175ms step_avg:87.86ms
step:1016/1680 train_time:89266ms step_avg:87.86ms
step:1017/1680 train_time:89354ms step_avg:87.86ms
step:1018/1680 train_time:89441ms step_avg:87.86ms
step:1019/1680 train_time:89529ms step_avg:87.86ms
step:1020/1680 train_time:89617ms step_avg:87.86ms
step:1021/1680 train_time:89705ms step_avg:87.86ms
step:1022/1680 train_time:89792ms step_avg:87.86ms
step:1023/1680 train_time:89881ms step_avg:87.86ms
step:1024/1680 train_time:89970ms step_avg:87.86ms
step:1025/1680 train_time:90059ms step_avg:87.86ms
step:1026/1680 train_time:90148ms step_avg:87.86ms
step:1027/1680 train_time:90237ms step_avg:87.86ms
step:1028/1680 train_time:90325ms step_avg:87.86ms
step:1029/1680 train_time:90413ms step_avg:87.86ms
step:1030/1680 train_time:90500ms step_avg:87.86ms
step:1031/1680 train_time:90588ms step_avg:87.86ms
step:1032/1680 train_time:90676ms step_avg:87.86ms
step:1033/1680 train_time:90764ms step_avg:87.86ms
step:1034/1680 train_time:90852ms step_avg:87.86ms
step:1035/1680 train_time:90941ms step_avg:87.87ms
step:1036/1680 train_time:91029ms step_avg:87.87ms
step:1037/1680 train_time:91118ms step_avg:87.87ms
step:1038/1680 train_time:91207ms step_avg:87.87ms
step:1039/1680 train_time:91296ms step_avg:87.87ms
step:1040/1680 train_time:91384ms step_avg:87.87ms
step:1041/1680 train_time:91472ms step_avg:87.87ms
step:1042/1680 train_time:91560ms step_avg:87.87ms
step:1043/1680 train_time:91649ms step_avg:87.87ms
step:1044/1680 train_time:91737ms step_avg:87.87ms
step:1045/1680 train_time:91825ms step_avg:87.87ms
step:1046/1680 train_time:91913ms step_avg:87.87ms
step:1047/1680 train_time:92002ms step_avg:87.87ms
step:1048/1680 train_time:92091ms step_avg:87.87ms
step:1049/1680 train_time:92180ms step_avg:87.87ms
step:1050/1680 train_time:92269ms step_avg:87.87ms
step:1051/1680 train_time:92357ms step_avg:87.88ms
step:1052/1680 train_time:92445ms step_avg:87.88ms
step:1053/1680 train_time:92533ms step_avg:87.88ms
step:1054/1680 train_time:92621ms step_avg:87.88ms
step:1055/1680 train_time:92709ms step_avg:87.88ms
step:1056/1680 train_time:92797ms step_avg:87.88ms
step:1057/1680 train_time:92885ms step_avg:87.88ms
step:1058/1680 train_time:92974ms step_avg:87.88ms
step:1059/1680 train_time:93062ms step_avg:87.88ms
step:1060/1680 train_time:93151ms step_avg:87.88ms
step:1061/1680 train_time:93241ms step_avg:87.88ms
step:1062/1680 train_time:93330ms step_avg:87.88ms
step:1063/1680 train_time:93418ms step_avg:87.88ms
step:1064/1680 train_time:93507ms step_avg:87.88ms
step:1065/1680 train_time:93595ms step_avg:87.88ms
step:1066/1680 train_time:93683ms step_avg:87.88ms
step:1067/1680 train_time:93771ms step_avg:87.88ms
step:1068/1680 train_time:93859ms step_avg:87.88ms
step:1069/1680 train_time:93948ms step_avg:87.88ms
step:1070/1680 train_time:94037ms step_avg:87.88ms
step:1071/1680 train_time:94126ms step_avg:87.89ms
step:1072/1680 train_time:94214ms step_avg:87.89ms
step:1073/1680 train_time:94303ms step_avg:87.89ms
step:1074/1680 train_time:94391ms step_avg:87.89ms
step:1075/1680 train_time:94480ms step_avg:87.89ms
step:1076/1680 train_time:94568ms step_avg:87.89ms
step:1077/1680 train_time:94657ms step_avg:87.89ms
step:1078/1680 train_time:94745ms step_avg:87.89ms
step:1079/1680 train_time:94833ms step_avg:87.89ms
step:1080/1680 train_time:94922ms step_avg:87.89ms
step:1081/1680 train_time:95010ms step_avg:87.89ms
step:1082/1680 train_time:95098ms step_avg:87.89ms
step:1083/1680 train_time:95187ms step_avg:87.89ms
step:1084/1680 train_time:95276ms step_avg:87.89ms
step:1085/1680 train_time:95365ms step_avg:87.89ms
step:1086/1680 train_time:95453ms step_avg:87.89ms
step:1087/1680 train_time:95541ms step_avg:87.89ms
step:1088/1680 train_time:95629ms step_avg:87.89ms
step:1089/1680 train_time:95717ms step_avg:87.89ms
step:1090/1680 train_time:95805ms step_avg:87.89ms
step:1091/1680 train_time:95894ms step_avg:87.90ms
step:1092/1680 train_time:95982ms step_avg:87.90ms
step:1093/1680 train_time:96070ms step_avg:87.90ms
step:1094/1680 train_time:96160ms step_avg:87.90ms
step:1095/1680 train_time:96249ms step_avg:87.90ms
step:1096/1680 train_time:96339ms step_avg:87.90ms
step:1097/1680 train_time:96427ms step_avg:87.90ms
step:1098/1680 train_time:96517ms step_avg:87.90ms
step:1099/1680 train_time:96606ms step_avg:87.90ms
step:1100/1680 train_time:96695ms step_avg:87.90ms
step:1101/1680 train_time:96783ms step_avg:87.91ms
step:1102/1680 train_time:96872ms step_avg:87.91ms
step:1103/1680 train_time:96961ms step_avg:87.91ms
step:1104/1680 train_time:97050ms step_avg:87.91ms
step:1105/1680 train_time:97139ms step_avg:87.91ms
step:1106/1680 train_time:97228ms step_avg:87.91ms
step:1107/1680 train_time:97318ms step_avg:87.91ms
step:1108/1680 train_time:97407ms step_avg:87.91ms
step:1109/1680 train_time:97497ms step_avg:87.91ms
step:1110/1680 train_time:97586ms step_avg:87.92ms
step:1111/1680 train_time:97675ms step_avg:87.92ms
step:1112/1680 train_time:97764ms step_avg:87.92ms
step:1113/1680 train_time:97853ms step_avg:87.92ms
step:1114/1680 train_time:97941ms step_avg:87.92ms
step:1115/1680 train_time:98030ms step_avg:87.92ms
step:1116/1680 train_time:98121ms step_avg:87.92ms
step:1117/1680 train_time:98210ms step_avg:87.92ms
step:1118/1680 train_time:98299ms step_avg:87.92ms
step:1119/1680 train_time:98388ms step_avg:87.92ms
step:1120/1680 train_time:98476ms step_avg:87.93ms
step:1121/1680 train_time:98566ms step_avg:87.93ms
step:1122/1680 train_time:98656ms step_avg:87.93ms
step:1123/1680 train_time:98745ms step_avg:87.93ms
step:1124/1680 train_time:98833ms step_avg:87.93ms
step:1125/1680 train_time:98922ms step_avg:87.93ms
step:1125/1680 val_loss:3.4164 train_time:99012ms step_avg:88.01ms
step:1126/1680 train_time:99032ms step_avg:87.95ms
step:1127/1680 train_time:99104ms step_avg:87.94ms
step:1128/1680 train_time:99194ms step_avg:87.94ms
step:1129/1680 train_time:99284ms step_avg:87.94ms
step:1130/1680 train_time:99373ms step_avg:87.94ms
step:1131/1680 train_time:99461ms step_avg:87.94ms
step:1132/1680 train_time:99549ms step_avg:87.94ms
step:1133/1680 train_time:99637ms step_avg:87.94ms
step:1134/1680 train_time:99725ms step_avg:87.94ms
step:1135/1680 train_time:99813ms step_avg:87.94ms
step:1136/1680 train_time:99903ms step_avg:87.94ms
step:1137/1680 train_time:99996ms step_avg:87.95ms
step:1138/1680 train_time:100086ms step_avg:87.95ms
step:1139/1680 train_time:100177ms step_avg:87.95ms
step:1140/1680 train_time:100267ms step_avg:87.95ms
step:1141/1680 train_time:100355ms step_avg:87.95ms
step:1142/1680 train_time:100443ms step_avg:87.95ms
step:1143/1680 train_time:100531ms step_avg:87.95ms
step:1144/1680 train_time:100619ms step_avg:87.95ms
step:1145/1680 train_time:100708ms step_avg:87.95ms
step:1146/1680 train_time:100796ms step_avg:87.95ms
step:1147/1680 train_time:100884ms step_avg:87.96ms
step:1148/1680 train_time:100975ms step_avg:87.96ms
step:1149/1680 train_time:101066ms step_avg:87.96ms
step:1150/1680 train_time:101156ms step_avg:87.96ms
step:1151/1680 train_time:101246ms step_avg:87.96ms
step:1152/1680 train_time:101335ms step_avg:87.96ms
step:1153/1680 train_time:101423ms step_avg:87.96ms
step:1154/1680 train_time:101512ms step_avg:87.97ms
step:1155/1680 train_time:101600ms step_avg:87.97ms
step:1156/1680 train_time:101688ms step_avg:87.97ms
step:1157/1680 train_time:101777ms step_avg:87.97ms
step:1158/1680 train_time:101867ms step_avg:87.97ms
step:1159/1680 train_time:101957ms step_avg:87.97ms
step:1160/1680 train_time:102048ms step_avg:87.97ms
step:1161/1680 train_time:102139ms step_avg:87.98ms
step:1162/1680 train_time:102228ms step_avg:87.98ms
step:1163/1680 train_time:102317ms step_avg:87.98ms
step:1164/1680 train_time:102407ms step_avg:87.98ms
step:1165/1680 train_time:102495ms step_avg:87.98ms
step:1166/1680 train_time:102584ms step_avg:87.98ms
step:1167/1680 train_time:102672ms step_avg:87.98ms
step:1168/1680 train_time:102761ms step_avg:87.98ms
step:1169/1680 train_time:102851ms step_avg:87.98ms
step:1170/1680 train_time:102940ms step_avg:87.98ms
step:1171/1680 train_time:103030ms step_avg:87.98ms
step:1172/1680 train_time:103120ms step_avg:87.99ms
step:1173/1680 train_time:103211ms step_avg:87.99ms
step:1174/1680 train_time:103300ms step_avg:87.99ms
step:1175/1680 train_time:103389ms step_avg:87.99ms
step:1176/1680 train_time:103478ms step_avg:87.99ms
step:1177/1680 train_time:103566ms step_avg:87.99ms
step:1178/1680 train_time:103654ms step_avg:87.99ms
step:1179/1680 train_time:103743ms step_avg:87.99ms
step:1180/1680 train_time:103832ms step_avg:87.99ms
step:1181/1680 train_time:103921ms step_avg:87.99ms
step:1182/1680 train_time:104011ms step_avg:88.00ms
step:1183/1680 train_time:104100ms step_avg:88.00ms
step:1184/1680 train_time:104190ms step_avg:88.00ms
step:1185/1680 train_time:104280ms step_avg:88.00ms
step:1186/1680 train_time:104369ms step_avg:88.00ms
step:1187/1680 train_time:104458ms step_avg:88.00ms
step:1188/1680 train_time:104547ms step_avg:88.00ms
step:1189/1680 train_time:104636ms step_avg:88.00ms
step:1190/1680 train_time:104726ms step_avg:88.00ms
step:1191/1680 train_time:104815ms step_avg:88.01ms
step:1192/1680 train_time:104905ms step_avg:88.01ms
step:1193/1680 train_time:104994ms step_avg:88.01ms
step:1194/1680 train_time:105083ms step_avg:88.01ms
step:1195/1680 train_time:105173ms step_avg:88.01ms
step:1196/1680 train_time:105262ms step_avg:88.01ms
step:1197/1680 train_time:105351ms step_avg:88.01ms
step:1198/1680 train_time:105440ms step_avg:88.01ms
step:1199/1680 train_time:105529ms step_avg:88.01ms
step:1200/1680 train_time:105618ms step_avg:88.01ms
step:1201/1680 train_time:105707ms step_avg:88.02ms
step:1202/1680 train_time:105796ms step_avg:88.02ms
step:1203/1680 train_time:105885ms step_avg:88.02ms
step:1204/1680 train_time:105974ms step_avg:88.02ms
step:1205/1680 train_time:106063ms step_avg:88.02ms
step:1206/1680 train_time:106152ms step_avg:88.02ms
step:1207/1680 train_time:106241ms step_avg:88.02ms
step:1208/1680 train_time:106331ms step_avg:88.02ms
step:1209/1680 train_time:106419ms step_avg:88.02ms
step:1210/1680 train_time:106508ms step_avg:88.02ms
step:1211/1680 train_time:106597ms step_avg:88.02ms
step:1212/1680 train_time:106686ms step_avg:88.02ms
step:1213/1680 train_time:106775ms step_avg:88.03ms
step:1214/1680 train_time:106864ms step_avg:88.03ms
step:1215/1680 train_time:106953ms step_avg:88.03ms
step:1216/1680 train_time:107042ms step_avg:88.03ms
step:1217/1680 train_time:107131ms step_avg:88.03ms
step:1218/1680 train_time:107221ms step_avg:88.03ms
step:1219/1680 train_time:107310ms step_avg:88.03ms
step:1220/1680 train_time:107398ms step_avg:88.03ms
step:1221/1680 train_time:107488ms step_avg:88.03ms
step:1222/1680 train_time:107576ms step_avg:88.03ms
step:1223/1680 train_time:107666ms step_avg:88.03ms
step:1224/1680 train_time:107756ms step_avg:88.04ms
step:1225/1680 train_time:107845ms step_avg:88.04ms
step:1226/1680 train_time:107934ms step_avg:88.04ms
step:1227/1680 train_time:108023ms step_avg:88.04ms
step:1228/1680 train_time:108112ms step_avg:88.04ms
step:1229/1680 train_time:108201ms step_avg:88.04ms
step:1230/1680 train_time:108290ms step_avg:88.04ms
step:1231/1680 train_time:108379ms step_avg:88.04ms
step:1232/1680 train_time:108468ms step_avg:88.04ms
step:1233/1680 train_time:108557ms step_avg:88.04ms
step:1234/1680 train_time:108646ms step_avg:88.04ms
step:1235/1680 train_time:108735ms step_avg:88.04ms
step:1236/1680 train_time:108824ms step_avg:88.05ms
step:1237/1680 train_time:108914ms step_avg:88.05ms
step:1238/1680 train_time:109003ms step_avg:88.05ms
step:1239/1680 train_time:109092ms step_avg:88.05ms
step:1240/1680 train_time:109181ms step_avg:88.05ms
step:1241/1680 train_time:109271ms step_avg:88.05ms
step:1242/1680 train_time:109360ms step_avg:88.05ms
step:1243/1680 train_time:109449ms step_avg:88.05ms
step:1244/1680 train_time:109539ms step_avg:88.05ms
step:1245/1680 train_time:109628ms step_avg:88.05ms
step:1246/1680 train_time:109717ms step_avg:88.06ms
step:1247/1680 train_time:109806ms step_avg:88.06ms
step:1248/1680 train_time:109896ms step_avg:88.06ms
step:1249/1680 train_time:109984ms step_avg:88.06ms
step:1250/1680 train_time:110073ms step_avg:88.06ms
step:1250/1680 val_loss:3.3778 train_time:110163ms step_avg:88.13ms
step:1251/1680 train_time:110183ms step_avg:88.08ms
step:1252/1680 train_time:110255ms step_avg:88.06ms
step:1253/1680 train_time:110348ms step_avg:88.07ms
step:1254/1680 train_time:110437ms step_avg:88.07ms
step:1255/1680 train_time:110526ms step_avg:88.07ms
step:1256/1680 train_time:110614ms step_avg:88.07ms
step:1257/1680 train_time:110702ms step_avg:88.07ms
step:1258/1680 train_time:110791ms step_avg:88.07ms
step:1259/1680 train_time:110879ms step_avg:88.07ms
step:1260/1680 train_time:110967ms step_avg:88.07ms
step:1261/1680 train_time:111056ms step_avg:88.07ms
step:1262/1680 train_time:111145ms step_avg:88.07ms
step:1263/1680 train_time:111236ms step_avg:88.07ms
step:1264/1680 train_time:111327ms step_avg:88.07ms
step:1265/1680 train_time:111417ms step_avg:88.08ms
step:1266/1680 train_time:111507ms step_avg:88.08ms
step:1267/1680 train_time:111597ms step_avg:88.08ms
step:1268/1680 train_time:111684ms step_avg:88.08ms
step:1269/1680 train_time:111773ms step_avg:88.08ms
step:1270/1680 train_time:111861ms step_avg:88.08ms
step:1271/1680 train_time:111949ms step_avg:88.08ms
step:1272/1680 train_time:112038ms step_avg:88.08ms
step:1273/1680 train_time:112127ms step_avg:88.08ms
step:1274/1680 train_time:112217ms step_avg:88.08ms
step:1275/1680 train_time:112307ms step_avg:88.08ms
step:1276/1680 train_time:112398ms step_avg:88.09ms
step:1277/1680 train_time:112487ms step_avg:88.09ms
step:1278/1680 train_time:112577ms step_avg:88.09ms
step:1279/1680 train_time:112666ms step_avg:88.09ms
step:1280/1680 train_time:112754ms step_avg:88.09ms
step:1281/1680 train_time:112843ms step_avg:88.09ms
step:1282/1680 train_time:112932ms step_avg:88.09ms
step:1283/1680 train_time:113021ms step_avg:88.09ms
step:1284/1680 train_time:113110ms step_avg:88.09ms
step:1285/1680 train_time:113200ms step_avg:88.09ms
step:1286/1680 train_time:113290ms step_avg:88.09ms
step:1287/1680 train_time:113379ms step_avg:88.10ms
step:1288/1680 train_time:113469ms step_avg:88.10ms
step:1289/1680 train_time:113558ms step_avg:88.10ms
step:1290/1680 train_time:113647ms step_avg:88.10ms
step:1291/1680 train_time:113736ms step_avg:88.10ms
step:1292/1680 train_time:113825ms step_avg:88.10ms
step:1293/1680 train_time:113914ms step_avg:88.10ms
step:1294/1680 train_time:114002ms step_avg:88.10ms
step:1295/1680 train_time:114091ms step_avg:88.10ms
step:1296/1680 train_time:114180ms step_avg:88.10ms
step:1297/1680 train_time:114269ms step_avg:88.10ms
step:1298/1680 train_time:114359ms step_avg:88.10ms
step:1299/1680 train_time:114448ms step_avg:88.10ms
step:1300/1680 train_time:114538ms step_avg:88.11ms
step:1301/1680 train_time:114626ms step_avg:88.11ms
step:1302/1680 train_time:114715ms step_avg:88.11ms
step:1303/1680 train_time:114804ms step_avg:88.11ms
step:1304/1680 train_time:114893ms step_avg:88.11ms
step:1305/1680 train_time:114983ms step_avg:88.11ms
step:1306/1680 train_time:115072ms step_avg:88.11ms
step:1307/1680 train_time:115161ms step_avg:88.11ms
step:1308/1680 train_time:115250ms step_avg:88.11ms
step:1309/1680 train_time:115339ms step_avg:88.11ms
step:1310/1680 train_time:115429ms step_avg:88.11ms
step:1311/1680 train_time:115519ms step_avg:88.11ms
step:1312/1680 train_time:115607ms step_avg:88.12ms
step:1313/1680 train_time:115696ms step_avg:88.12ms
step:1314/1680 train_time:115785ms step_avg:88.12ms
step:1315/1680 train_time:115873ms step_avg:88.12ms
step:1316/1680 train_time:115963ms step_avg:88.12ms
step:1317/1680 train_time:116052ms step_avg:88.12ms
step:1318/1680 train_time:116142ms step_avg:88.12ms
step:1319/1680 train_time:116231ms step_avg:88.12ms
step:1320/1680 train_time:116321ms step_avg:88.12ms
step:1321/1680 train_time:116410ms step_avg:88.12ms
step:1322/1680 train_time:116500ms step_avg:88.12ms
step:1323/1680 train_time:116589ms step_avg:88.12ms
step:1324/1680 train_time:116678ms step_avg:88.13ms
step:1325/1680 train_time:116767ms step_avg:88.13ms
step:1326/1680 train_time:116856ms step_avg:88.13ms
step:1327/1680 train_time:116944ms step_avg:88.13ms
step:1328/1680 train_time:117034ms step_avg:88.13ms
step:1329/1680 train_time:117124ms step_avg:88.13ms
step:1330/1680 train_time:117214ms step_avg:88.13ms
step:1331/1680 train_time:117303ms step_avg:88.13ms
step:1332/1680 train_time:117392ms step_avg:88.13ms
step:1333/1680 train_time:117482ms step_avg:88.13ms
step:1334/1680 train_time:117570ms step_avg:88.13ms
step:1335/1680 train_time:117660ms step_avg:88.13ms
step:1336/1680 train_time:117749ms step_avg:88.14ms
step:1337/1680 train_time:117837ms step_avg:88.14ms
step:1338/1680 train_time:117926ms step_avg:88.14ms
step:1339/1680 train_time:118016ms step_avg:88.14ms
step:1340/1680 train_time:118105ms step_avg:88.14ms
step:1341/1680 train_time:118193ms step_avg:88.14ms
step:1342/1680 train_time:118282ms step_avg:88.14ms
step:1343/1680 train_time:118372ms step_avg:88.14ms
step:1344/1680 train_time:118461ms step_avg:88.14ms
step:1345/1680 train_time:118550ms step_avg:88.14ms
step:1346/1680 train_time:118639ms step_avg:88.14ms
step:1347/1680 train_time:118728ms step_avg:88.14ms
step:1348/1680 train_time:118817ms step_avg:88.14ms
step:1349/1680 train_time:118905ms step_avg:88.14ms
step:1350/1680 train_time:118995ms step_avg:88.14ms
step:1351/1680 train_time:119085ms step_avg:88.15ms
step:1352/1680 train_time:119174ms step_avg:88.15ms
step:1353/1680 train_time:119264ms step_avg:88.15ms
step:1354/1680 train_time:119353ms step_avg:88.15ms
step:1355/1680 train_time:119442ms step_avg:88.15ms
step:1356/1680 train_time:119531ms step_avg:88.15ms
step:1357/1680 train_time:119620ms step_avg:88.15ms
step:1358/1680 train_time:119709ms step_avg:88.15ms
step:1359/1680 train_time:119799ms step_avg:88.15ms
step:1360/1680 train_time:119888ms step_avg:88.15ms
step:1361/1680 train_time:119977ms step_avg:88.15ms
step:1362/1680 train_time:120065ms step_avg:88.15ms
step:1363/1680 train_time:120155ms step_avg:88.15ms
step:1364/1680 train_time:120244ms step_avg:88.16ms
step:1365/1680 train_time:120333ms step_avg:88.16ms
step:1366/1680 train_time:120422ms step_avg:88.16ms
step:1367/1680 train_time:120512ms step_avg:88.16ms
step:1368/1680 train_time:120601ms step_avg:88.16ms
step:1369/1680 train_time:120691ms step_avg:88.16ms
step:1370/1680 train_time:120779ms step_avg:88.16ms
step:1371/1680 train_time:120868ms step_avg:88.16ms
step:1372/1680 train_time:120957ms step_avg:88.16ms
step:1373/1680 train_time:121046ms step_avg:88.16ms
step:1374/1680 train_time:121135ms step_avg:88.16ms
step:1375/1680 train_time:121224ms step_avg:88.16ms
step:1375/1680 val_loss:3.3432 train_time:121315ms step_avg:88.23ms
step:1376/1680 train_time:121333ms step_avg:88.18ms
step:1377/1680 train_time:121406ms step_avg:88.17ms
step:1378/1680 train_time:121500ms step_avg:88.17ms
step:1379/1680 train_time:121590ms step_avg:88.17ms
step:1380/1680 train_time:121679ms step_avg:88.17ms
step:1381/1680 train_time:121767ms step_avg:88.17ms
step:1382/1680 train_time:121856ms step_avg:88.17ms
step:1383/1680 train_time:121945ms step_avg:88.17ms
step:1384/1680 train_time:122033ms step_avg:88.17ms
step:1385/1680 train_time:122121ms step_avg:88.17ms
step:1386/1680 train_time:122210ms step_avg:88.17ms
step:1387/1680 train_time:122300ms step_avg:88.18ms
step:1388/1680 train_time:122390ms step_avg:88.18ms
step:1389/1680 train_time:122481ms step_avg:88.18ms
step:1390/1680 train_time:122571ms step_avg:88.18ms
step:1391/1680 train_time:122660ms step_avg:88.18ms
step:1392/1680 train_time:122748ms step_avg:88.18ms
step:1393/1680 train_time:122838ms step_avg:88.18ms
step:1394/1680 train_time:122928ms step_avg:88.18ms
step:1395/1680 train_time:123016ms step_avg:88.18ms
step:1396/1680 train_time:123105ms step_avg:88.18ms
step:1397/1680 train_time:123193ms step_avg:88.18ms
step:1398/1680 train_time:123282ms step_avg:88.18ms
step:1399/1680 train_time:123372ms step_avg:88.19ms
step:1400/1680 train_time:123463ms step_avg:88.19ms
step:1401/1680 train_time:123553ms step_avg:88.19ms
step:1402/1680 train_time:123642ms step_avg:88.19ms
step:1403/1680 train_time:123731ms step_avg:88.19ms
step:1404/1680 train_time:123821ms step_avg:88.19ms
step:1405/1680 train_time:123909ms step_avg:88.19ms
step:1406/1680 train_time:123997ms step_avg:88.19ms
step:1407/1680 train_time:124086ms step_avg:88.19ms
step:1408/1680 train_time:124176ms step_avg:88.19ms
step:1409/1680 train_time:124266ms step_avg:88.19ms
step:1410/1680 train_time:124355ms step_avg:88.20ms
step:1411/1680 train_time:124446ms step_avg:88.20ms
step:1412/1680 train_time:124535ms step_avg:88.20ms
step:1413/1680 train_time:124624ms step_avg:88.20ms
step:1414/1680 train_time:124713ms step_avg:88.20ms
step:1415/1680 train_time:124802ms step_avg:88.20ms
step:1416/1680 train_time:124891ms step_avg:88.20ms
step:1417/1680 train_time:124980ms step_avg:88.20ms
step:1418/1680 train_time:125068ms step_avg:88.20ms
step:1419/1680 train_time:125157ms step_avg:88.20ms
step:1420/1680 train_time:125246ms step_avg:88.20ms
step:1421/1680 train_time:125335ms step_avg:88.20ms
step:1422/1680 train_time:125426ms step_avg:88.20ms
step:1423/1680 train_time:125516ms step_avg:88.21ms
step:1424/1680 train_time:125605ms step_avg:88.21ms
step:1425/1680 train_time:125694ms step_avg:88.21ms
step:1426/1680 train_time:125784ms step_avg:88.21ms
step:1427/1680 train_time:125872ms step_avg:88.21ms
step:1428/1680 train_time:125962ms step_avg:88.21ms
step:1429/1680 train_time:126050ms step_avg:88.21ms
step:1430/1680 train_time:126140ms step_avg:88.21ms
step:1431/1680 train_time:126229ms step_avg:88.21ms
step:1432/1680 train_time:126318ms step_avg:88.21ms
step:1433/1680 train_time:126408ms step_avg:88.21ms
step:1434/1680 train_time:126498ms step_avg:88.21ms
step:1435/1680 train_time:126587ms step_avg:88.21ms
step:1436/1680 train_time:126676ms step_avg:88.21ms
step:1437/1680 train_time:126766ms step_avg:88.22ms
step:1438/1680 train_time:126855ms step_avg:88.22ms
step:1439/1680 train_time:126943ms step_avg:88.22ms
step:1440/1680 train_time:127032ms step_avg:88.22ms
step:1441/1680 train_time:127122ms step_avg:88.22ms
step:1442/1680 train_time:127211ms step_avg:88.22ms
step:1443/1680 train_time:127300ms step_avg:88.22ms
step:1444/1680 train_time:127388ms step_avg:88.22ms
step:1445/1680 train_time:127478ms step_avg:88.22ms
step:1446/1680 train_time:127568ms step_avg:88.22ms
step:1447/1680 train_time:127658ms step_avg:88.22ms
step:1448/1680 train_time:127747ms step_avg:88.22ms
step:1449/1680 train_time:127837ms step_avg:88.22ms
step:1450/1680 train_time:127926ms step_avg:88.22ms
step:1451/1680 train_time:128015ms step_avg:88.23ms
step:1452/1680 train_time:128104ms step_avg:88.23ms
step:1453/1680 train_time:128192ms step_avg:88.23ms
step:1454/1680 train_time:128282ms step_avg:88.23ms
step:1455/1680 train_time:128370ms step_avg:88.23ms
step:1456/1680 train_time:128460ms step_avg:88.23ms
step:1457/1680 train_time:128549ms step_avg:88.23ms
step:1458/1680 train_time:128639ms step_avg:88.23ms
step:1459/1680 train_time:128728ms step_avg:88.23ms
step:1460/1680 train_time:128818ms step_avg:88.23ms
step:1461/1680 train_time:128908ms step_avg:88.23ms
step:1462/1680 train_time:128997ms step_avg:88.23ms
step:1463/1680 train_time:129085ms step_avg:88.23ms
step:1464/1680 train_time:129175ms step_avg:88.23ms
step:1465/1680 train_time:129263ms step_avg:88.23ms
step:1466/1680 train_time:129353ms step_avg:88.24ms
step:1467/1680 train_time:129442ms step_avg:88.24ms
step:1468/1680 train_time:129531ms step_avg:88.24ms
step:1469/1680 train_time:129621ms step_avg:88.24ms
step:1470/1680 train_time:129710ms step_avg:88.24ms
step:1471/1680 train_time:129799ms step_avg:88.24ms
step:1472/1680 train_time:129888ms step_avg:88.24ms
step:1473/1680 train_time:129977ms step_avg:88.24ms
step:1474/1680 train_time:130066ms step_avg:88.24ms
step:1475/1680 train_time:130155ms step_avg:88.24ms
step:1476/1680 train_time:130245ms step_avg:88.24ms
step:1477/1680 train_time:130335ms step_avg:88.24ms
step:1478/1680 train_time:130424ms step_avg:88.24ms
step:1479/1680 train_time:130513ms step_avg:88.24ms
step:1480/1680 train_time:130603ms step_avg:88.25ms
step:1481/1680 train_time:130693ms step_avg:88.25ms
step:1482/1680 train_time:130782ms step_avg:88.25ms
step:1483/1680 train_time:130871ms step_avg:88.25ms
step:1484/1680 train_time:130960ms step_avg:88.25ms
step:1485/1680 train_time:131050ms step_avg:88.25ms
step:1486/1680 train_time:131139ms step_avg:88.25ms
step:1487/1680 train_time:131228ms step_avg:88.25ms
step:1488/1680 train_time:131317ms step_avg:88.25ms
step:1489/1680 train_time:131406ms step_avg:88.25ms
step:1490/1680 train_time:131494ms step_avg:88.25ms
step:1491/1680 train_time:131584ms step_avg:88.25ms
step:1492/1680 train_time:131673ms step_avg:88.25ms
step:1493/1680 train_time:131762ms step_avg:88.25ms
step:1494/1680 train_time:131852ms step_avg:88.25ms
step:1495/1680 train_time:131940ms step_avg:88.25ms
step:1496/1680 train_time:132030ms step_avg:88.26ms
step:1497/1680 train_time:132119ms step_avg:88.26ms
step:1498/1680 train_time:132208ms step_avg:88.26ms
step:1499/1680 train_time:132298ms step_avg:88.26ms
step:1500/1680 train_time:132387ms step_avg:88.26ms
step:1500/1680 val_loss:3.3141 train_time:132478ms step_avg:88.32ms
step:1501/1680 train_time:132497ms step_avg:88.27ms
step:1502/1680 train_time:132570ms step_avg:88.26ms
step:1503/1680 train_time:132661ms step_avg:88.26ms
step:1504/1680 train_time:132750ms step_avg:88.26ms
step:1505/1680 train_time:132839ms step_avg:88.27ms
step:1506/1680 train_time:132927ms step_avg:88.27ms
step:1507/1680 train_time:133015ms step_avg:88.27ms
step:1508/1680 train_time:133104ms step_avg:88.27ms
step:1509/1680 train_time:133191ms step_avg:88.26ms
step:1510/1680 train_time:133280ms step_avg:88.27ms
step:1511/1680 train_time:133369ms step_avg:88.27ms
step:1512/1680 train_time:133459ms step_avg:88.27ms
step:1513/1680 train_time:133550ms step_avg:88.27ms
step:1514/1680 train_time:133643ms step_avg:88.27ms
step:1515/1680 train_time:133732ms step_avg:88.27ms
step:1516/1680 train_time:133822ms step_avg:88.27ms
step:1517/1680 train_time:133911ms step_avg:88.27ms
step:1518/1680 train_time:133999ms step_avg:88.27ms
step:1519/1680 train_time:134088ms step_avg:88.27ms
step:1520/1680 train_time:134176ms step_avg:88.27ms
step:1521/1680 train_time:134264ms step_avg:88.27ms
step:1522/1680 train_time:134353ms step_avg:88.27ms
step:1523/1680 train_time:134442ms step_avg:88.27ms
step:1524/1680 train_time:134532ms step_avg:88.28ms
step:1525/1680 train_time:134623ms step_avg:88.28ms
step:1526/1680 train_time:134713ms step_avg:88.28ms
step:1527/1680 train_time:134803ms step_avg:88.28ms
step:1528/1680 train_time:134892ms step_avg:88.28ms
step:1529/1680 train_time:134981ms step_avg:88.28ms
step:1530/1680 train_time:135071ms step_avg:88.28ms
step:1531/1680 train_time:135160ms step_avg:88.28ms
step:1532/1680 train_time:135248ms step_avg:88.28ms
step:1533/1680 train_time:135337ms step_avg:88.28ms
step:1534/1680 train_time:135426ms step_avg:88.28ms
step:1535/1680 train_time:135515ms step_avg:88.28ms
step:1536/1680 train_time:135606ms step_avg:88.29ms
step:1537/1680 train_time:135696ms step_avg:88.29ms
step:1538/1680 train_time:135785ms step_avg:88.29ms
step:1539/1680 train_time:135875ms step_avg:88.29ms
step:1540/1680 train_time:135964ms step_avg:88.29ms
step:1541/1680 train_time:136053ms step_avg:88.29ms
step:1542/1680 train_time:136142ms step_avg:88.29ms
step:1543/1680 train_time:136231ms step_avg:88.29ms
step:1544/1680 train_time:136319ms step_avg:88.29ms
step:1545/1680 train_time:136407ms step_avg:88.29ms
step:1546/1680 train_time:136497ms step_avg:88.29ms
step:1547/1680 train_time:136586ms step_avg:88.29ms
step:1548/1680 train_time:136676ms step_avg:88.29ms
step:1549/1680 train_time:136766ms step_avg:88.29ms
step:1550/1680 train_time:136855ms step_avg:88.29ms
step:1551/1680 train_time:136945ms step_avg:88.29ms
step:1552/1680 train_time:137036ms step_avg:88.30ms
step:1553/1680 train_time:137124ms step_avg:88.30ms
step:1554/1680 train_time:137214ms step_avg:88.30ms
step:1555/1680 train_time:137303ms step_avg:88.30ms
step:1556/1680 train_time:137393ms step_avg:88.30ms
step:1557/1680 train_time:137483ms step_avg:88.30ms
step:1558/1680 train_time:137572ms step_avg:88.30ms
step:1559/1680 train_time:137662ms step_avg:88.30ms
step:1560/1680 train_time:137752ms step_avg:88.30ms
step:1561/1680 train_time:137841ms step_avg:88.30ms
step:1562/1680 train_time:137931ms step_avg:88.30ms
step:1563/1680 train_time:138022ms step_avg:88.31ms
step:1564/1680 train_time:138112ms step_avg:88.31ms
step:1565/1680 train_time:138201ms step_avg:88.31ms
step:1566/1680 train_time:138290ms step_avg:88.31ms
step:1567/1680 train_time:138379ms step_avg:88.31ms
step:1568/1680 train_time:138470ms step_avg:88.31ms
step:1569/1680 train_time:138559ms step_avg:88.31ms
step:1570/1680 train_time:138648ms step_avg:88.31ms
step:1571/1680 train_time:138736ms step_avg:88.31ms
step:1572/1680 train_time:138825ms step_avg:88.31ms
step:1573/1680 train_time:138914ms step_avg:88.31ms
step:1574/1680 train_time:139003ms step_avg:88.31ms
step:1575/1680 train_time:139092ms step_avg:88.31ms
step:1576/1680 train_time:139183ms step_avg:88.31ms
step:1577/1680 train_time:139272ms step_avg:88.31ms
step:1578/1680 train_time:139361ms step_avg:88.32ms
step:1579/1680 train_time:139451ms step_avg:88.32ms
step:1580/1680 train_time:139540ms step_avg:88.32ms
step:1581/1680 train_time:139629ms step_avg:88.32ms
step:1582/1680 train_time:139719ms step_avg:88.32ms
step:1583/1680 train_time:139808ms step_avg:88.32ms
step:1584/1680 train_time:139897ms step_avg:88.32ms
step:1585/1680 train_time:139987ms step_avg:88.32ms
step:1586/1680 train_time:140076ms step_avg:88.32ms
step:1587/1680 train_time:140166ms step_avg:88.32ms
step:1588/1680 train_time:140256ms step_avg:88.32ms
step:1589/1680 train_time:140345ms step_avg:88.32ms
step:1590/1680 train_time:140434ms step_avg:88.32ms
step:1591/1680 train_time:140524ms step_avg:88.32ms
step:1592/1680 train_time:140613ms step_avg:88.32ms
step:1593/1680 train_time:140702ms step_avg:88.33ms
step:1594/1680 train_time:140792ms step_avg:88.33ms
step:1595/1680 train_time:140881ms step_avg:88.33ms
step:1596/1680 train_time:140971ms step_avg:88.33ms
step:1597/1680 train_time:141061ms step_avg:88.33ms
step:1598/1680 train_time:141150ms step_avg:88.33ms
step:1599/1680 train_time:141239ms step_avg:88.33ms
step:1600/1680 train_time:141328ms step_avg:88.33ms
step:1601/1680 train_time:141418ms step_avg:88.33ms
step:1602/1680 train_time:141507ms step_avg:88.33ms
step:1603/1680 train_time:141595ms step_avg:88.33ms
step:1604/1680 train_time:141684ms step_avg:88.33ms
step:1605/1680 train_time:141774ms step_avg:88.33ms
step:1606/1680 train_time:141864ms step_avg:88.33ms
step:1607/1680 train_time:141952ms step_avg:88.33ms
step:1608/1680 train_time:142041ms step_avg:88.33ms
step:1609/1680 train_time:142131ms step_avg:88.33ms
step:1610/1680 train_time:142220ms step_avg:88.34ms
step:1611/1680 train_time:142310ms step_avg:88.34ms
step:1612/1680 train_time:142399ms step_avg:88.34ms
step:1613/1680 train_time:142489ms step_avg:88.34ms
step:1614/1680 train_time:142579ms step_avg:88.34ms
step:1615/1680 train_time:142668ms step_avg:88.34ms
step:1616/1680 train_time:142757ms step_avg:88.34ms
step:1617/1680 train_time:142846ms step_avg:88.34ms
step:1618/1680 train_time:142936ms step_avg:88.34ms
step:1619/1680 train_time:143025ms step_avg:88.34ms
step:1620/1680 train_time:143113ms step_avg:88.34ms
step:1621/1680 train_time:143202ms step_avg:88.34ms
step:1622/1680 train_time:143291ms step_avg:88.34ms
step:1623/1680 train_time:143381ms step_avg:88.34ms
step:1624/1680 train_time:143470ms step_avg:88.34ms
step:1625/1680 train_time:143560ms step_avg:88.34ms
step:1625/1680 val_loss:3.2901 train_time:143650ms step_avg:88.40ms
step:1626/1680 train_time:143668ms step_avg:88.36ms
step:1627/1680 train_time:143742ms step_avg:88.35ms
step:1628/1680 train_time:143834ms step_avg:88.35ms
step:1629/1680 train_time:143924ms step_avg:88.35ms
step:1630/1680 train_time:144012ms step_avg:88.35ms
step:1631/1680 train_time:144100ms step_avg:88.35ms
step:1632/1680 train_time:144188ms step_avg:88.35ms
step:1633/1680 train_time:144275ms step_avg:88.35ms
step:1634/1680 train_time:144364ms step_avg:88.35ms
step:1635/1680 train_time:144453ms step_avg:88.35ms
step:1636/1680 train_time:144542ms step_avg:88.35ms
step:1637/1680 train_time:144633ms step_avg:88.35ms
step:1638/1680 train_time:144725ms step_avg:88.35ms
step:1639/1680 train_time:144816ms step_avg:88.36ms
step:1640/1680 train_time:144906ms step_avg:88.36ms
step:1641/1680 train_time:144995ms step_avg:88.36ms
step:1642/1680 train_time:145084ms step_avg:88.36ms
step:1643/1680 train_time:145173ms step_avg:88.36ms
step:1644/1680 train_time:145261ms step_avg:88.36ms
step:1645/1680 train_time:145349ms step_avg:88.36ms
step:1646/1680 train_time:145437ms step_avg:88.36ms
step:1647/1680 train_time:145527ms step_avg:88.36ms
step:1648/1680 train_time:145617ms step_avg:88.36ms
step:1649/1680 train_time:145707ms step_avg:88.36ms
step:1650/1680 train_time:145796ms step_avg:88.36ms
step:1651/1680 train_time:145886ms step_avg:88.36ms
step:1652/1680 train_time:145975ms step_avg:88.36ms
step:1653/1680 train_time:146064ms step_avg:88.36ms
step:1654/1680 train_time:146153ms step_avg:88.36ms
step:1655/1680 train_time:146242ms step_avg:88.36ms
step:1656/1680 train_time:146331ms step_avg:88.36ms
step:1657/1680 train_time:146419ms step_avg:88.36ms
step:1658/1680 train_time:146507ms step_avg:88.36ms
step:1659/1680 train_time:146597ms step_avg:88.36ms
step:1660/1680 train_time:146688ms step_avg:88.37ms
step:1661/1680 train_time:146777ms step_avg:88.37ms
step:1662/1680 train_time:146867ms step_avg:88.37ms
step:1663/1680 train_time:146957ms step_avg:88.37ms
step:1664/1680 train_time:147046ms step_avg:88.37ms
step:1665/1680 train_time:147135ms step_avg:88.37ms
step:1666/1680 train_time:147224ms step_avg:88.37ms
step:1667/1680 train_time:147313ms step_avg:88.37ms
step:1668/1680 train_time:147401ms step_avg:88.37ms
step:1669/1680 train_time:147490ms step_avg:88.37ms
step:1670/1680 train_time:147579ms step_avg:88.37ms
step:1671/1680 train_time:147668ms step_avg:88.37ms
step:1672/1680 train_time:147757ms step_avg:88.37ms
step:1673/1680 train_time:147847ms step_avg:88.37ms
step:1674/1680 train_time:147937ms step_avg:88.37ms
step:1675/1680 train_time:148026ms step_avg:88.37ms
step:1676/1680 train_time:148115ms step_avg:88.37ms
step:1677/1680 train_time:148204ms step_avg:88.37ms
step:1678/1680 train_time:148292ms step_avg:88.37ms
step:1679/1680 train_time:148381ms step_avg:88.37ms
step:1680/1680 train_time:148470ms step_avg:88.37ms
step:1680/1680 val_loss:3.2796 train_time:148560ms step_avg:88.43ms
peak memory allocated: 30760 MiB reserved: 46034 MiB

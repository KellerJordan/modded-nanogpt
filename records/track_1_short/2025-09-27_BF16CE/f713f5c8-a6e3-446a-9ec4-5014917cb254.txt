import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:33:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            123W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0            117W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    159205      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    159206      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    159207      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    159208      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    159209      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    159210      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    159211      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    159212      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    159206      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    159207      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    159208      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    159209      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    159210      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    159211      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    159212      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:146ms step_avg:146.05ms
step:2/1680 train_time:166ms step_avg:82.84ms
step:3/1680 train_time:229ms step_avg:76.50ms
step:4/1680 train_time:315ms step_avg:78.70ms
step:5/1680 train_time:401ms step_avg:80.27ms
step:6/1680 train_time:487ms step_avg:81.20ms
step:7/1680 train_time:574ms step_avg:81.93ms
step:8/1680 train_time:660ms step_avg:82.47ms
step:9/1680 train_time:746ms step_avg:82.90ms
step:10/1680 train_time:833ms step_avg:83.26ms
step:11/1680 train_time:919ms step_avg:83.52ms
step:12/1680 train_time:1006ms step_avg:83.86ms
step:13/1680 train_time:1097ms step_avg:84.35ms
step:14/1680 train_time:1187ms step_avg:84.80ms
step:15/1680 train_time:1275ms step_avg:85.00ms
step:16/1680 train_time:1363ms step_avg:85.18ms
step:17/1680 train_time:1449ms step_avg:85.24ms
step:18/1680 train_time:1536ms step_avg:85.31ms
step:19/1680 train_time:1622ms step_avg:85.36ms
step:20/1680 train_time:1708ms step_avg:85.41ms
step:21/1680 train_time:1795ms step_avg:85.48ms
step:22/1680 train_time:1882ms step_avg:85.52ms
step:23/1680 train_time:1969ms step_avg:85.61ms
step:24/1680 train_time:2058ms step_avg:85.74ms
step:25/1680 train_time:2146ms step_avg:85.85ms
step:26/1680 train_time:2235ms step_avg:85.95ms
step:27/1680 train_time:2323ms step_avg:86.04ms
step:28/1680 train_time:2411ms step_avg:86.11ms
step:29/1680 train_time:2499ms step_avg:86.16ms
step:30/1680 train_time:2585ms step_avg:86.18ms
step:31/1680 train_time:2672ms step_avg:86.19ms
step:32/1680 train_time:2760ms step_avg:86.24ms
step:33/1680 train_time:2846ms step_avg:86.25ms
step:34/1680 train_time:2933ms step_avg:86.27ms
step:35/1680 train_time:3021ms step_avg:86.31ms
step:36/1680 train_time:3109ms step_avg:86.36ms
step:37/1680 train_time:3198ms step_avg:86.43ms
step:38/1680 train_time:3285ms step_avg:86.46ms
step:39/1680 train_time:3373ms step_avg:86.49ms
step:40/1680 train_time:3461ms step_avg:86.52ms
step:41/1680 train_time:3548ms step_avg:86.54ms
step:42/1680 train_time:3635ms step_avg:86.55ms
step:43/1680 train_time:3722ms step_avg:86.57ms
step:44/1680 train_time:3810ms step_avg:86.59ms
step:45/1680 train_time:3898ms step_avg:86.62ms
step:46/1680 train_time:3985ms step_avg:86.63ms
step:47/1680 train_time:4072ms step_avg:86.64ms
step:48/1680 train_time:4160ms step_avg:86.68ms
step:49/1680 train_time:4249ms step_avg:86.70ms
step:50/1680 train_time:4337ms step_avg:86.73ms
step:51/1680 train_time:4424ms step_avg:86.74ms
step:52/1680 train_time:4511ms step_avg:86.75ms
step:53/1680 train_time:4598ms step_avg:86.76ms
step:54/1680 train_time:4686ms step_avg:86.79ms
step:55/1680 train_time:4773ms step_avg:86.79ms
step:56/1680 train_time:4861ms step_avg:86.80ms
step:57/1680 train_time:4948ms step_avg:86.81ms
step:58/1680 train_time:5036ms step_avg:86.83ms
step:59/1680 train_time:5124ms step_avg:86.85ms
step:60/1680 train_time:5211ms step_avg:86.85ms
step:61/1680 train_time:5299ms step_avg:86.87ms
step:62/1680 train_time:5387ms step_avg:86.88ms
step:63/1680 train_time:5474ms step_avg:86.88ms
step:64/1680 train_time:5561ms step_avg:86.88ms
step:65/1680 train_time:5647ms step_avg:86.88ms
step:66/1680 train_time:5734ms step_avg:86.88ms
step:67/1680 train_time:5821ms step_avg:86.88ms
step:68/1680 train_time:5908ms step_avg:86.88ms
step:69/1680 train_time:5995ms step_avg:86.89ms
step:70/1680 train_time:6082ms step_avg:86.89ms
step:71/1680 train_time:6170ms step_avg:86.90ms
step:72/1680 train_time:6258ms step_avg:86.92ms
step:73/1680 train_time:6346ms step_avg:86.93ms
step:74/1680 train_time:6433ms step_avg:86.93ms
step:75/1680 train_time:6520ms step_avg:86.93ms
step:76/1680 train_time:6608ms step_avg:86.94ms
step:77/1680 train_time:6695ms step_avg:86.95ms
step:78/1680 train_time:6782ms step_avg:86.95ms
step:79/1680 train_time:6870ms step_avg:86.96ms
step:80/1680 train_time:6958ms step_avg:86.97ms
step:81/1680 train_time:7045ms step_avg:86.97ms
step:82/1680 train_time:7132ms step_avg:86.98ms
step:83/1680 train_time:7220ms step_avg:86.99ms
step:84/1680 train_time:7307ms step_avg:86.99ms
step:85/1680 train_time:7395ms step_avg:87.00ms
step:86/1680 train_time:7482ms step_avg:87.00ms
step:87/1680 train_time:7569ms step_avg:87.00ms
step:88/1680 train_time:7657ms step_avg:87.01ms
step:89/1680 train_time:7744ms step_avg:87.01ms
step:90/1680 train_time:7831ms step_avg:87.01ms
step:91/1680 train_time:7919ms step_avg:87.02ms
step:92/1680 train_time:8006ms step_avg:87.02ms
step:93/1680 train_time:8094ms step_avg:87.03ms
step:94/1680 train_time:8181ms step_avg:87.03ms
step:95/1680 train_time:8268ms step_avg:87.03ms
step:96/1680 train_time:8356ms step_avg:87.04ms
step:97/1680 train_time:8443ms step_avg:87.04ms
step:98/1680 train_time:8530ms step_avg:87.04ms
step:99/1680 train_time:8617ms step_avg:87.04ms
step:100/1680 train_time:8705ms step_avg:87.05ms
step:101/1680 train_time:8792ms step_avg:87.05ms
step:102/1680 train_time:8880ms step_avg:87.06ms
step:103/1680 train_time:8967ms step_avg:87.06ms
step:104/1680 train_time:9054ms step_avg:87.06ms
step:105/1680 train_time:9142ms step_avg:87.06ms
step:106/1680 train_time:9229ms step_avg:87.06ms
step:107/1680 train_time:9316ms step_avg:87.06ms
step:108/1680 train_time:9404ms step_avg:87.07ms
step:109/1680 train_time:9491ms step_avg:87.07ms
step:110/1680 train_time:9579ms step_avg:87.08ms
step:111/1680 train_time:9666ms step_avg:87.08ms
step:112/1680 train_time:9753ms step_avg:87.08ms
step:113/1680 train_time:9841ms step_avg:87.09ms
step:114/1680 train_time:9928ms step_avg:87.09ms
step:115/1680 train_time:10015ms step_avg:87.09ms
step:116/1680 train_time:10102ms step_avg:87.09ms
step:117/1680 train_time:10189ms step_avg:87.09ms
step:118/1680 train_time:10276ms step_avg:87.09ms
step:119/1680 train_time:10364ms step_avg:87.09ms
step:120/1680 train_time:10451ms step_avg:87.09ms
step:121/1680 train_time:10538ms step_avg:87.09ms
step:122/1680 train_time:10625ms step_avg:87.09ms
step:123/1680 train_time:10712ms step_avg:87.09ms
step:124/1680 train_time:10799ms step_avg:87.09ms
step:125/1680 train_time:10886ms step_avg:87.09ms
step:125/1680 val_loss:4.3356 train_time:10974ms step_avg:87.79ms
step:126/1680 train_time:10995ms step_avg:87.26ms
step:127/1680 train_time:11065ms step_avg:87.12ms
step:128/1680 train_time:11160ms step_avg:87.19ms
step:129/1680 train_time:11253ms step_avg:87.23ms
step:130/1680 train_time:11340ms step_avg:87.23ms
step:131/1680 train_time:11426ms step_avg:87.22ms
step:132/1680 train_time:11512ms step_avg:87.22ms
step:133/1680 train_time:11598ms step_avg:87.20ms
step:134/1680 train_time:11684ms step_avg:87.20ms
step:135/1680 train_time:11770ms step_avg:87.19ms
step:136/1680 train_time:11856ms step_avg:87.18ms
step:137/1680 train_time:11943ms step_avg:87.17ms
step:138/1680 train_time:12030ms step_avg:87.17ms
step:139/1680 train_time:12119ms step_avg:87.19ms
step:140/1680 train_time:12209ms step_avg:87.21ms
step:141/1680 train_time:12297ms step_avg:87.21ms
step:142/1680 train_time:12384ms step_avg:87.21ms
step:143/1680 train_time:12471ms step_avg:87.21ms
step:144/1680 train_time:12558ms step_avg:87.21ms
step:145/1680 train_time:12644ms step_avg:87.20ms
step:146/1680 train_time:12731ms step_avg:87.20ms
step:147/1680 train_time:12817ms step_avg:87.19ms
step:148/1680 train_time:12903ms step_avg:87.18ms
step:149/1680 train_time:12990ms step_avg:87.18ms
step:150/1680 train_time:13078ms step_avg:87.19ms
step:151/1680 train_time:13166ms step_avg:87.19ms
step:152/1680 train_time:13255ms step_avg:87.20ms
step:153/1680 train_time:13342ms step_avg:87.20ms
step:154/1680 train_time:13430ms step_avg:87.21ms
step:155/1680 train_time:13518ms step_avg:87.21ms
step:156/1680 train_time:13604ms step_avg:87.20ms
step:157/1680 train_time:13690ms step_avg:87.20ms
step:158/1680 train_time:13777ms step_avg:87.20ms
step:159/1680 train_time:13863ms step_avg:87.19ms
step:160/1680 train_time:13951ms step_avg:87.19ms
step:161/1680 train_time:14038ms step_avg:87.19ms
step:162/1680 train_time:14126ms step_avg:87.19ms
step:163/1680 train_time:14214ms step_avg:87.20ms
step:164/1680 train_time:14301ms step_avg:87.20ms
step:165/1680 train_time:14389ms step_avg:87.21ms
step:166/1680 train_time:14476ms step_avg:87.20ms
step:167/1680 train_time:14563ms step_avg:87.20ms
step:168/1680 train_time:14650ms step_avg:87.20ms
step:169/1680 train_time:14737ms step_avg:87.20ms
step:170/1680 train_time:14824ms step_avg:87.20ms
step:171/1680 train_time:14910ms step_avg:87.19ms
step:172/1680 train_time:14998ms step_avg:87.20ms
step:173/1680 train_time:15085ms step_avg:87.20ms
step:174/1680 train_time:15173ms step_avg:87.20ms
step:175/1680 train_time:15260ms step_avg:87.20ms
step:176/1680 train_time:15348ms step_avg:87.20ms
step:177/1680 train_time:15435ms step_avg:87.20ms
step:178/1680 train_time:15522ms step_avg:87.20ms
step:179/1680 train_time:15609ms step_avg:87.20ms
step:180/1680 train_time:15696ms step_avg:87.20ms
step:181/1680 train_time:15782ms step_avg:87.19ms
step:182/1680 train_time:15869ms step_avg:87.19ms
step:183/1680 train_time:15957ms step_avg:87.19ms
step:184/1680 train_time:16043ms step_avg:87.19ms
step:185/1680 train_time:16131ms step_avg:87.19ms
step:186/1680 train_time:16219ms step_avg:87.20ms
step:187/1680 train_time:16306ms step_avg:87.20ms
step:188/1680 train_time:16393ms step_avg:87.20ms
step:189/1680 train_time:16480ms step_avg:87.20ms
step:190/1680 train_time:16567ms step_avg:87.20ms
step:191/1680 train_time:16654ms step_avg:87.20ms
step:192/1680 train_time:16741ms step_avg:87.19ms
step:193/1680 train_time:16828ms step_avg:87.19ms
step:194/1680 train_time:16915ms step_avg:87.19ms
step:195/1680 train_time:17003ms step_avg:87.19ms
step:196/1680 train_time:17090ms step_avg:87.19ms
step:197/1680 train_time:17178ms step_avg:87.20ms
step:198/1680 train_time:17265ms step_avg:87.20ms
step:199/1680 train_time:17352ms step_avg:87.20ms
step:200/1680 train_time:17439ms step_avg:87.20ms
step:201/1680 train_time:17527ms step_avg:87.20ms
step:202/1680 train_time:17614ms step_avg:87.20ms
step:203/1680 train_time:17701ms step_avg:87.20ms
step:204/1680 train_time:17788ms step_avg:87.19ms
step:205/1680 train_time:17875ms step_avg:87.19ms
step:206/1680 train_time:17962ms step_avg:87.20ms
step:207/1680 train_time:18049ms step_avg:87.19ms
step:208/1680 train_time:18137ms step_avg:87.20ms
step:209/1680 train_time:18224ms step_avg:87.20ms
step:210/1680 train_time:18311ms step_avg:87.20ms
step:211/1680 train_time:18399ms step_avg:87.20ms
step:212/1680 train_time:18486ms step_avg:87.20ms
step:213/1680 train_time:18574ms step_avg:87.20ms
step:214/1680 train_time:18660ms step_avg:87.20ms
step:215/1680 train_time:18747ms step_avg:87.20ms
step:216/1680 train_time:18834ms step_avg:87.20ms
step:217/1680 train_time:18921ms step_avg:87.20ms
step:218/1680 train_time:19008ms step_avg:87.19ms
step:219/1680 train_time:19096ms step_avg:87.19ms
step:220/1680 train_time:19183ms step_avg:87.19ms
step:221/1680 train_time:19270ms step_avg:87.19ms
step:222/1680 train_time:19357ms step_avg:87.19ms
step:223/1680 train_time:19444ms step_avg:87.19ms
step:224/1680 train_time:19531ms step_avg:87.19ms
step:225/1680 train_time:19619ms step_avg:87.20ms
step:226/1680 train_time:19706ms step_avg:87.19ms
step:227/1680 train_time:19793ms step_avg:87.19ms
step:228/1680 train_time:19880ms step_avg:87.19ms
step:229/1680 train_time:19967ms step_avg:87.19ms
step:230/1680 train_time:20054ms step_avg:87.19ms
step:231/1680 train_time:20140ms step_avg:87.19ms
step:232/1680 train_time:20228ms step_avg:87.19ms
step:233/1680 train_time:20315ms step_avg:87.19ms
step:234/1680 train_time:20402ms step_avg:87.19ms
step:235/1680 train_time:20490ms step_avg:87.19ms
step:236/1680 train_time:20577ms step_avg:87.19ms
step:237/1680 train_time:20664ms step_avg:87.19ms
step:238/1680 train_time:20752ms step_avg:87.19ms
step:239/1680 train_time:20839ms step_avg:87.19ms
step:240/1680 train_time:20927ms step_avg:87.20ms
step:241/1680 train_time:21014ms step_avg:87.19ms
step:242/1680 train_time:21101ms step_avg:87.19ms
step:243/1680 train_time:21188ms step_avg:87.19ms
step:244/1680 train_time:21275ms step_avg:87.19ms
step:245/1680 train_time:21362ms step_avg:87.19ms
step:246/1680 train_time:21450ms step_avg:87.19ms
step:247/1680 train_time:21537ms step_avg:87.19ms
step:248/1680 train_time:21623ms step_avg:87.19ms
step:249/1680 train_time:21710ms step_avg:87.19ms
step:250/1680 train_time:21798ms step_avg:87.19ms
step:250/1680 val_loss:3.9780 train_time:21887ms step_avg:87.55ms
step:251/1680 train_time:21907ms step_avg:87.28ms
step:252/1680 train_time:21978ms step_avg:87.21ms
step:253/1680 train_time:22068ms step_avg:87.23ms
step:254/1680 train_time:22155ms step_avg:87.23ms
step:255/1680 train_time:22241ms step_avg:87.22ms
step:256/1680 train_time:22328ms step_avg:87.22ms
step:257/1680 train_time:22415ms step_avg:87.22ms
step:258/1680 train_time:22501ms step_avg:87.21ms
step:259/1680 train_time:22587ms step_avg:87.21ms
step:260/1680 train_time:22674ms step_avg:87.21ms
step:261/1680 train_time:22760ms step_avg:87.20ms
step:262/1680 train_time:22848ms step_avg:87.21ms
step:263/1680 train_time:22937ms step_avg:87.21ms
step:264/1680 train_time:23026ms step_avg:87.22ms
step:265/1680 train_time:23114ms step_avg:87.22ms
step:266/1680 train_time:23201ms step_avg:87.22ms
step:267/1680 train_time:23288ms step_avg:87.22ms
step:268/1680 train_time:23376ms step_avg:87.22ms
step:269/1680 train_time:23462ms step_avg:87.22ms
step:270/1680 train_time:23549ms step_avg:87.22ms
step:271/1680 train_time:23635ms step_avg:87.22ms
step:272/1680 train_time:23722ms step_avg:87.21ms
step:273/1680 train_time:23809ms step_avg:87.21ms
step:274/1680 train_time:23897ms step_avg:87.22ms
step:275/1680 train_time:23985ms step_avg:87.22ms
step:276/1680 train_time:24074ms step_avg:87.22ms
step:277/1680 train_time:24161ms step_avg:87.22ms
step:278/1680 train_time:24249ms step_avg:87.23ms
step:279/1680 train_time:24335ms step_avg:87.22ms
step:280/1680 train_time:24422ms step_avg:87.22ms
step:281/1680 train_time:24509ms step_avg:87.22ms
step:282/1680 train_time:24596ms step_avg:87.22ms
step:283/1680 train_time:24682ms step_avg:87.22ms
step:284/1680 train_time:24770ms step_avg:87.22ms
step:285/1680 train_time:24857ms step_avg:87.22ms
step:286/1680 train_time:24945ms step_avg:87.22ms
step:287/1680 train_time:25034ms step_avg:87.23ms
step:288/1680 train_time:25121ms step_avg:87.23ms
step:289/1680 train_time:25208ms step_avg:87.23ms
step:290/1680 train_time:25296ms step_avg:87.23ms
step:291/1680 train_time:25383ms step_avg:87.23ms
step:292/1680 train_time:25470ms step_avg:87.23ms
step:293/1680 train_time:25557ms step_avg:87.23ms
step:294/1680 train_time:25644ms step_avg:87.23ms
step:295/1680 train_time:25732ms step_avg:87.23ms
step:296/1680 train_time:25819ms step_avg:87.23ms
step:297/1680 train_time:25907ms step_avg:87.23ms
step:298/1680 train_time:25995ms step_avg:87.23ms
step:299/1680 train_time:26082ms step_avg:87.23ms
step:300/1680 train_time:26170ms step_avg:87.23ms
step:301/1680 train_time:26256ms step_avg:87.23ms
step:302/1680 train_time:26344ms step_avg:87.23ms
step:303/1680 train_time:26431ms step_avg:87.23ms
step:304/1680 train_time:26519ms step_avg:87.23ms
step:305/1680 train_time:26605ms step_avg:87.23ms
step:306/1680 train_time:26692ms step_avg:87.23ms
step:307/1680 train_time:26779ms step_avg:87.23ms
step:308/1680 train_time:26867ms step_avg:87.23ms
step:309/1680 train_time:26953ms step_avg:87.23ms
step:310/1680 train_time:27040ms step_avg:87.23ms
step:311/1680 train_time:27128ms step_avg:87.23ms
step:312/1680 train_time:27215ms step_avg:87.23ms
step:313/1680 train_time:27302ms step_avg:87.23ms
step:314/1680 train_time:27390ms step_avg:87.23ms
step:315/1680 train_time:27477ms step_avg:87.23ms
step:316/1680 train_time:27564ms step_avg:87.23ms
step:317/1680 train_time:27651ms step_avg:87.23ms
step:318/1680 train_time:27738ms step_avg:87.23ms
step:319/1680 train_time:27825ms step_avg:87.22ms
step:320/1680 train_time:27912ms step_avg:87.22ms
step:321/1680 train_time:27999ms step_avg:87.23ms
step:322/1680 train_time:28086ms step_avg:87.22ms
step:323/1680 train_time:28173ms step_avg:87.22ms
step:324/1680 train_time:28260ms step_avg:87.22ms
step:325/1680 train_time:28347ms step_avg:87.22ms
step:326/1680 train_time:28434ms step_avg:87.22ms
step:327/1680 train_time:28522ms step_avg:87.22ms
step:328/1680 train_time:28609ms step_avg:87.22ms
step:329/1680 train_time:28696ms step_avg:87.22ms
step:330/1680 train_time:28783ms step_avg:87.22ms
step:331/1680 train_time:28871ms step_avg:87.22ms
step:332/1680 train_time:28958ms step_avg:87.22ms
step:333/1680 train_time:29045ms step_avg:87.22ms
step:334/1680 train_time:29133ms step_avg:87.22ms
step:335/1680 train_time:29219ms step_avg:87.22ms
step:336/1680 train_time:29307ms step_avg:87.22ms
step:337/1680 train_time:29394ms step_avg:87.22ms
step:338/1680 train_time:29481ms step_avg:87.22ms
step:339/1680 train_time:29568ms step_avg:87.22ms
step:340/1680 train_time:29655ms step_avg:87.22ms
step:341/1680 train_time:29743ms step_avg:87.22ms
step:342/1680 train_time:29830ms step_avg:87.22ms
step:343/1680 train_time:29917ms step_avg:87.22ms
step:344/1680 train_time:30004ms step_avg:87.22ms
step:345/1680 train_time:30091ms step_avg:87.22ms
step:346/1680 train_time:30178ms step_avg:87.22ms
step:347/1680 train_time:30266ms step_avg:87.22ms
step:348/1680 train_time:30353ms step_avg:87.22ms
step:349/1680 train_time:30440ms step_avg:87.22ms
step:350/1680 train_time:30527ms step_avg:87.22ms
step:351/1680 train_time:30615ms step_avg:87.22ms
step:352/1680 train_time:30702ms step_avg:87.22ms
step:353/1680 train_time:30790ms step_avg:87.22ms
step:354/1680 train_time:30878ms step_avg:87.22ms
step:355/1680 train_time:30965ms step_avg:87.22ms
step:356/1680 train_time:31052ms step_avg:87.23ms
step:357/1680 train_time:31140ms step_avg:87.23ms
step:358/1680 train_time:31227ms step_avg:87.23ms
step:359/1680 train_time:31315ms step_avg:87.23ms
step:360/1680 train_time:31402ms step_avg:87.23ms
step:361/1680 train_time:31489ms step_avg:87.23ms
step:362/1680 train_time:31577ms step_avg:87.23ms
step:363/1680 train_time:31664ms step_avg:87.23ms
step:364/1680 train_time:31751ms step_avg:87.23ms
step:365/1680 train_time:31838ms step_avg:87.23ms
step:366/1680 train_time:31926ms step_avg:87.23ms
step:367/1680 train_time:32013ms step_avg:87.23ms
step:368/1680 train_time:32100ms step_avg:87.23ms
step:369/1680 train_time:32188ms step_avg:87.23ms
step:370/1680 train_time:32276ms step_avg:87.23ms
step:371/1680 train_time:32363ms step_avg:87.23ms
step:372/1680 train_time:32450ms step_avg:87.23ms
step:373/1680 train_time:32537ms step_avg:87.23ms
step:374/1680 train_time:32625ms step_avg:87.23ms
step:375/1680 train_time:32712ms step_avg:87.23ms
step:375/1680 val_loss:3.8205 train_time:32801ms step_avg:87.47ms
step:376/1680 train_time:32819ms step_avg:87.29ms
step:377/1680 train_time:32892ms step_avg:87.25ms
step:378/1680 train_time:32985ms step_avg:87.26ms
step:379/1680 train_time:33074ms step_avg:87.27ms
step:380/1680 train_time:33162ms step_avg:87.27ms
step:381/1680 train_time:33250ms step_avg:87.27ms
step:382/1680 train_time:33336ms step_avg:87.27ms
step:383/1680 train_time:33422ms step_avg:87.26ms
step:384/1680 train_time:33508ms step_avg:87.26ms
step:385/1680 train_time:33594ms step_avg:87.26ms
step:386/1680 train_time:33681ms step_avg:87.26ms
step:387/1680 train_time:33767ms step_avg:87.25ms
step:388/1680 train_time:33856ms step_avg:87.26ms
step:389/1680 train_time:33945ms step_avg:87.26ms
step:390/1680 train_time:34034ms step_avg:87.27ms
step:391/1680 train_time:34122ms step_avg:87.27ms
step:392/1680 train_time:34210ms step_avg:87.27ms
step:393/1680 train_time:34297ms step_avg:87.27ms
step:394/1680 train_time:34384ms step_avg:87.27ms
step:395/1680 train_time:34470ms step_avg:87.27ms
step:396/1680 train_time:34556ms step_avg:87.26ms
step:397/1680 train_time:34642ms step_avg:87.26ms
step:398/1680 train_time:34729ms step_avg:87.26ms
step:399/1680 train_time:34816ms step_avg:87.26ms
step:400/1680 train_time:34904ms step_avg:87.26ms
step:401/1680 train_time:34992ms step_avg:87.26ms
step:402/1680 train_time:35082ms step_avg:87.27ms
step:403/1680 train_time:35169ms step_avg:87.27ms
step:404/1680 train_time:35256ms step_avg:87.27ms
step:405/1680 train_time:35343ms step_avg:87.27ms
step:406/1680 train_time:35430ms step_avg:87.27ms
step:407/1680 train_time:35517ms step_avg:87.26ms
step:408/1680 train_time:35603ms step_avg:87.26ms
step:409/1680 train_time:35690ms step_avg:87.26ms
step:410/1680 train_time:35776ms step_avg:87.26ms
step:411/1680 train_time:35863ms step_avg:87.26ms
step:412/1680 train_time:35951ms step_avg:87.26ms
step:413/1680 train_time:36040ms step_avg:87.26ms
step:414/1680 train_time:36128ms step_avg:87.26ms
step:415/1680 train_time:36215ms step_avg:87.27ms
step:416/1680 train_time:36302ms step_avg:87.27ms
step:417/1680 train_time:36390ms step_avg:87.27ms
step:418/1680 train_time:36477ms step_avg:87.26ms
step:419/1680 train_time:36563ms step_avg:87.26ms
step:420/1680 train_time:36650ms step_avg:87.26ms
step:421/1680 train_time:36738ms step_avg:87.26ms
step:422/1680 train_time:36824ms step_avg:87.26ms
step:423/1680 train_time:36912ms step_avg:87.26ms
step:424/1680 train_time:37001ms step_avg:87.27ms
step:425/1680 train_time:37089ms step_avg:87.27ms
step:426/1680 train_time:37178ms step_avg:87.27ms
step:427/1680 train_time:37265ms step_avg:87.27ms
step:428/1680 train_time:37352ms step_avg:87.27ms
step:429/1680 train_time:37439ms step_avg:87.27ms
step:430/1680 train_time:37526ms step_avg:87.27ms
step:431/1680 train_time:37613ms step_avg:87.27ms
step:432/1680 train_time:37700ms step_avg:87.27ms
step:433/1680 train_time:37787ms step_avg:87.27ms
step:434/1680 train_time:37874ms step_avg:87.27ms
step:435/1680 train_time:37962ms step_avg:87.27ms
step:436/1680 train_time:38050ms step_avg:87.27ms
step:437/1680 train_time:38138ms step_avg:87.27ms
step:438/1680 train_time:38224ms step_avg:87.27ms
step:439/1680 train_time:38311ms step_avg:87.27ms
step:440/1680 train_time:38398ms step_avg:87.27ms
step:441/1680 train_time:38485ms step_avg:87.27ms
step:442/1680 train_time:38572ms step_avg:87.27ms
step:443/1680 train_time:38659ms step_avg:87.27ms
step:444/1680 train_time:38747ms step_avg:87.27ms
step:445/1680 train_time:38834ms step_avg:87.27ms
step:446/1680 train_time:38921ms step_avg:87.27ms
step:447/1680 train_time:39008ms step_avg:87.27ms
step:448/1680 train_time:39096ms step_avg:87.27ms
step:449/1680 train_time:39183ms step_avg:87.27ms
step:450/1680 train_time:39271ms step_avg:87.27ms
step:451/1680 train_time:39358ms step_avg:87.27ms
step:452/1680 train_time:39445ms step_avg:87.27ms
step:453/1680 train_time:39533ms step_avg:87.27ms
step:454/1680 train_time:39619ms step_avg:87.27ms
step:455/1680 train_time:39706ms step_avg:87.27ms
step:456/1680 train_time:39793ms step_avg:87.27ms
step:457/1680 train_time:39882ms step_avg:87.27ms
step:458/1680 train_time:39968ms step_avg:87.27ms
step:459/1680 train_time:40056ms step_avg:87.27ms
step:460/1680 train_time:40143ms step_avg:87.27ms
step:461/1680 train_time:40230ms step_avg:87.27ms
step:462/1680 train_time:40319ms step_avg:87.27ms
step:463/1680 train_time:40406ms step_avg:87.27ms
step:464/1680 train_time:40493ms step_avg:87.27ms
step:465/1680 train_time:40581ms step_avg:87.27ms
step:466/1680 train_time:40668ms step_avg:87.27ms
step:467/1680 train_time:40755ms step_avg:87.27ms
step:468/1680 train_time:40842ms step_avg:87.27ms
step:469/1680 train_time:40929ms step_avg:87.27ms
step:470/1680 train_time:41017ms step_avg:87.27ms
step:471/1680 train_time:41105ms step_avg:87.27ms
step:472/1680 train_time:41191ms step_avg:87.27ms
step:473/1680 train_time:41280ms step_avg:87.27ms
step:474/1680 train_time:41366ms step_avg:87.27ms
step:475/1680 train_time:41453ms step_avg:87.27ms
step:476/1680 train_time:41541ms step_avg:87.27ms
step:477/1680 train_time:41628ms step_avg:87.27ms
step:478/1680 train_time:41715ms step_avg:87.27ms
step:479/1680 train_time:41801ms step_avg:87.27ms
step:480/1680 train_time:41889ms step_avg:87.27ms
step:481/1680 train_time:41976ms step_avg:87.27ms
step:482/1680 train_time:42063ms step_avg:87.27ms
step:483/1680 train_time:42151ms step_avg:87.27ms
step:484/1680 train_time:42239ms step_avg:87.27ms
step:485/1680 train_time:42325ms step_avg:87.27ms
step:486/1680 train_time:42413ms step_avg:87.27ms
step:487/1680 train_time:42500ms step_avg:87.27ms
step:488/1680 train_time:42587ms step_avg:87.27ms
step:489/1680 train_time:42675ms step_avg:87.27ms
step:490/1680 train_time:42761ms step_avg:87.27ms
step:491/1680 train_time:42849ms step_avg:87.27ms
step:492/1680 train_time:42937ms step_avg:87.27ms
step:493/1680 train_time:43024ms step_avg:87.27ms
step:494/1680 train_time:43112ms step_avg:87.27ms
step:495/1680 train_time:43200ms step_avg:87.27ms
step:496/1680 train_time:43287ms step_avg:87.27ms
step:497/1680 train_time:43375ms step_avg:87.27ms
step:498/1680 train_time:43462ms step_avg:87.27ms
step:499/1680 train_time:43549ms step_avg:87.27ms
step:500/1680 train_time:43636ms step_avg:87.27ms
step:500/1680 val_loss:3.7191 train_time:43724ms step_avg:87.45ms
step:501/1680 train_time:43743ms step_avg:87.31ms
step:502/1680 train_time:43814ms step_avg:87.28ms
step:503/1680 train_time:43907ms step_avg:87.29ms
step:504/1680 train_time:43995ms step_avg:87.29ms
step:505/1680 train_time:44082ms step_avg:87.29ms
step:506/1680 train_time:44168ms step_avg:87.29ms
step:507/1680 train_time:44254ms step_avg:87.29ms
step:508/1680 train_time:44340ms step_avg:87.28ms
step:509/1680 train_time:44426ms step_avg:87.28ms
step:510/1680 train_time:44512ms step_avg:87.28ms
step:511/1680 train_time:44598ms step_avg:87.28ms
step:512/1680 train_time:44686ms step_avg:87.28ms
step:513/1680 train_time:44775ms step_avg:87.28ms
step:514/1680 train_time:44865ms step_avg:87.29ms
step:515/1680 train_time:44955ms step_avg:87.29ms
step:516/1680 train_time:45042ms step_avg:87.29ms
step:517/1680 train_time:45128ms step_avg:87.29ms
step:518/1680 train_time:45215ms step_avg:87.29ms
step:519/1680 train_time:45302ms step_avg:87.29ms
step:520/1680 train_time:45388ms step_avg:87.28ms
step:521/1680 train_time:45475ms step_avg:87.28ms
step:522/1680 train_time:45561ms step_avg:87.28ms
step:523/1680 train_time:45648ms step_avg:87.28ms
step:524/1680 train_time:45736ms step_avg:87.28ms
step:525/1680 train_time:45825ms step_avg:87.28ms
step:526/1680 train_time:45914ms step_avg:87.29ms
step:527/1680 train_time:46002ms step_avg:87.29ms
step:528/1680 train_time:46089ms step_avg:87.29ms
step:529/1680 train_time:46176ms step_avg:87.29ms
step:530/1680 train_time:46263ms step_avg:87.29ms
step:531/1680 train_time:46349ms step_avg:87.29ms
step:532/1680 train_time:46436ms step_avg:87.28ms
step:533/1680 train_time:46522ms step_avg:87.28ms
step:534/1680 train_time:46609ms step_avg:87.28ms
step:535/1680 train_time:46697ms step_avg:87.28ms
step:536/1680 train_time:46785ms step_avg:87.29ms
step:537/1680 train_time:46873ms step_avg:87.29ms
step:538/1680 train_time:46961ms step_avg:87.29ms
step:539/1680 train_time:47049ms step_avg:87.29ms
step:540/1680 train_time:47136ms step_avg:87.29ms
step:541/1680 train_time:47223ms step_avg:87.29ms
step:542/1680 train_time:47310ms step_avg:87.29ms
step:543/1680 train_time:47397ms step_avg:87.29ms
step:544/1680 train_time:47484ms step_avg:87.29ms
step:545/1680 train_time:47571ms step_avg:87.29ms
step:546/1680 train_time:47657ms step_avg:87.28ms
step:547/1680 train_time:47745ms step_avg:87.28ms
step:548/1680 train_time:47833ms step_avg:87.29ms
step:549/1680 train_time:47921ms step_avg:87.29ms
step:550/1680 train_time:48011ms step_avg:87.29ms
step:551/1680 train_time:48099ms step_avg:87.29ms
step:552/1680 train_time:48187ms step_avg:87.30ms
step:553/1680 train_time:48276ms step_avg:87.30ms
step:554/1680 train_time:48364ms step_avg:87.30ms
step:555/1680 train_time:48452ms step_avg:87.30ms
step:556/1680 train_time:48540ms step_avg:87.30ms
step:557/1680 train_time:48629ms step_avg:87.30ms
step:558/1680 train_time:48717ms step_avg:87.31ms
step:559/1680 train_time:48805ms step_avg:87.31ms
step:560/1680 train_time:48894ms step_avg:87.31ms
step:561/1680 train_time:48983ms step_avg:87.31ms
step:562/1680 train_time:49071ms step_avg:87.32ms
step:563/1680 train_time:49159ms step_avg:87.32ms
step:564/1680 train_time:49248ms step_avg:87.32ms
step:565/1680 train_time:49336ms step_avg:87.32ms
step:566/1680 train_time:49424ms step_avg:87.32ms
step:567/1680 train_time:49513ms step_avg:87.32ms
step:568/1680 train_time:49600ms step_avg:87.32ms
step:569/1680 train_time:49689ms step_avg:87.33ms
step:570/1680 train_time:49776ms step_avg:87.33ms
step:571/1680 train_time:49865ms step_avg:87.33ms
step:572/1680 train_time:49954ms step_avg:87.33ms
step:573/1680 train_time:50042ms step_avg:87.33ms
step:574/1680 train_time:50131ms step_avg:87.34ms
step:575/1680 train_time:50219ms step_avg:87.34ms
step:576/1680 train_time:50307ms step_avg:87.34ms
step:577/1680 train_time:50396ms step_avg:87.34ms
step:578/1680 train_time:50484ms step_avg:87.34ms
step:579/1680 train_time:50572ms step_avg:87.34ms
step:580/1680 train_time:50660ms step_avg:87.34ms
step:581/1680 train_time:50748ms step_avg:87.35ms
step:582/1680 train_time:50837ms step_avg:87.35ms
step:583/1680 train_time:50925ms step_avg:87.35ms
step:584/1680 train_time:51014ms step_avg:87.35ms
step:585/1680 train_time:51103ms step_avg:87.36ms
step:586/1680 train_time:51192ms step_avg:87.36ms
step:587/1680 train_time:51280ms step_avg:87.36ms
step:588/1680 train_time:51369ms step_avg:87.36ms
step:589/1680 train_time:51456ms step_avg:87.36ms
step:590/1680 train_time:51544ms step_avg:87.36ms
step:591/1680 train_time:51633ms step_avg:87.37ms
step:592/1680 train_time:51722ms step_avg:87.37ms
step:593/1680 train_time:51810ms step_avg:87.37ms
step:594/1680 train_time:51898ms step_avg:87.37ms
step:595/1680 train_time:51987ms step_avg:87.37ms
step:596/1680 train_time:52075ms step_avg:87.37ms
step:597/1680 train_time:52163ms step_avg:87.37ms
step:598/1680 train_time:52252ms step_avg:87.38ms
step:599/1680 train_time:52340ms step_avg:87.38ms
step:600/1680 train_time:52429ms step_avg:87.38ms
step:601/1680 train_time:52516ms step_avg:87.38ms
step:602/1680 train_time:52605ms step_avg:87.38ms
step:603/1680 train_time:52693ms step_avg:87.38ms
step:604/1680 train_time:52781ms step_avg:87.39ms
step:605/1680 train_time:52870ms step_avg:87.39ms
step:606/1680 train_time:52958ms step_avg:87.39ms
step:607/1680 train_time:53046ms step_avg:87.39ms
step:608/1680 train_time:53135ms step_avg:87.39ms
step:609/1680 train_time:53223ms step_avg:87.39ms
step:610/1680 train_time:53312ms step_avg:87.40ms
step:611/1680 train_time:53401ms step_avg:87.40ms
step:612/1680 train_time:53489ms step_avg:87.40ms
step:613/1680 train_time:53577ms step_avg:87.40ms
step:614/1680 train_time:53665ms step_avg:87.40ms
step:615/1680 train_time:53753ms step_avg:87.40ms
step:616/1680 train_time:53841ms step_avg:87.40ms
step:617/1680 train_time:53930ms step_avg:87.41ms
step:618/1680 train_time:54018ms step_avg:87.41ms
step:619/1680 train_time:54106ms step_avg:87.41ms
step:620/1680 train_time:54195ms step_avg:87.41ms
step:621/1680 train_time:54283ms step_avg:87.41ms
step:622/1680 train_time:54371ms step_avg:87.41ms
step:623/1680 train_time:54459ms step_avg:87.41ms
step:624/1680 train_time:54548ms step_avg:87.42ms
step:625/1680 train_time:54637ms step_avg:87.42ms
step:625/1680 val_loss:3.6179 train_time:54727ms step_avg:87.56ms
step:626/1680 train_time:54747ms step_avg:87.45ms
step:627/1680 train_time:54818ms step_avg:87.43ms
step:628/1680 train_time:54909ms step_avg:87.43ms
step:629/1680 train_time:54999ms step_avg:87.44ms
step:630/1680 train_time:55088ms step_avg:87.44ms
step:631/1680 train_time:55175ms step_avg:87.44ms
step:632/1680 train_time:55262ms step_avg:87.44ms
step:633/1680 train_time:55349ms step_avg:87.44ms
step:634/1680 train_time:55436ms step_avg:87.44ms
step:635/1680 train_time:55523ms step_avg:87.44ms
step:636/1680 train_time:55611ms step_avg:87.44ms
step:637/1680 train_time:55709ms step_avg:87.46ms
step:638/1680 train_time:55801ms step_avg:87.46ms
step:639/1680 train_time:55890ms step_avg:87.46ms
step:640/1680 train_time:55978ms step_avg:87.47ms
step:641/1680 train_time:56067ms step_avg:87.47ms
step:642/1680 train_time:56155ms step_avg:87.47ms
step:643/1680 train_time:56243ms step_avg:87.47ms
step:644/1680 train_time:56330ms step_avg:87.47ms
step:645/1680 train_time:56417ms step_avg:87.47ms
step:646/1680 train_time:56505ms step_avg:87.47ms
step:647/1680 train_time:56593ms step_avg:87.47ms
step:648/1680 train_time:56682ms step_avg:87.47ms
step:649/1680 train_time:56771ms step_avg:87.47ms
step:650/1680 train_time:56860ms step_avg:87.48ms
step:651/1680 train_time:56949ms step_avg:87.48ms
step:652/1680 train_time:57037ms step_avg:87.48ms
step:653/1680 train_time:57127ms step_avg:87.48ms
step:654/1680 train_time:57215ms step_avg:87.48ms
step:655/1680 train_time:57302ms step_avg:87.48ms
step:656/1680 train_time:57391ms step_avg:87.49ms
step:657/1680 train_time:57478ms step_avg:87.49ms
step:658/1680 train_time:57566ms step_avg:87.49ms
step:659/1680 train_time:57655ms step_avg:87.49ms
step:660/1680 train_time:57744ms step_avg:87.49ms
step:661/1680 train_time:57832ms step_avg:87.49ms
step:662/1680 train_time:57921ms step_avg:87.49ms
step:663/1680 train_time:58010ms step_avg:87.50ms
step:664/1680 train_time:58098ms step_avg:87.50ms
step:665/1680 train_time:58187ms step_avg:87.50ms
step:666/1680 train_time:58275ms step_avg:87.50ms
step:667/1680 train_time:58362ms step_avg:87.50ms
step:668/1680 train_time:58450ms step_avg:87.50ms
step:669/1680 train_time:58538ms step_avg:87.50ms
step:670/1680 train_time:58627ms step_avg:87.50ms
step:671/1680 train_time:58715ms step_avg:87.50ms
step:672/1680 train_time:58804ms step_avg:87.51ms
step:673/1680 train_time:58893ms step_avg:87.51ms
step:674/1680 train_time:58981ms step_avg:87.51ms
step:675/1680 train_time:59070ms step_avg:87.51ms
step:676/1680 train_time:59159ms step_avg:87.51ms
step:677/1680 train_time:59248ms step_avg:87.52ms
step:678/1680 train_time:59336ms step_avg:87.52ms
step:679/1680 train_time:59425ms step_avg:87.52ms
step:680/1680 train_time:59513ms step_avg:87.52ms
step:681/1680 train_time:59601ms step_avg:87.52ms
step:682/1680 train_time:59689ms step_avg:87.52ms
step:683/1680 train_time:59777ms step_avg:87.52ms
step:684/1680 train_time:59865ms step_avg:87.52ms
step:685/1680 train_time:59953ms step_avg:87.52ms
step:686/1680 train_time:60041ms step_avg:87.52ms
step:687/1680 train_time:60130ms step_avg:87.53ms
step:688/1680 train_time:60218ms step_avg:87.53ms
step:689/1680 train_time:60307ms step_avg:87.53ms
step:690/1680 train_time:60395ms step_avg:87.53ms
step:691/1680 train_time:60483ms step_avg:87.53ms
step:692/1680 train_time:60571ms step_avg:87.53ms
step:693/1680 train_time:60659ms step_avg:87.53ms
step:694/1680 train_time:60747ms step_avg:87.53ms
step:695/1680 train_time:60835ms step_avg:87.53ms
step:696/1680 train_time:60924ms step_avg:87.53ms
step:697/1680 train_time:61013ms step_avg:87.54ms
step:698/1680 train_time:61100ms step_avg:87.54ms
step:699/1680 train_time:61189ms step_avg:87.54ms
step:700/1680 train_time:61277ms step_avg:87.54ms
step:701/1680 train_time:61365ms step_avg:87.54ms
step:702/1680 train_time:61452ms step_avg:87.54ms
step:703/1680 train_time:61541ms step_avg:87.54ms
step:704/1680 train_time:61629ms step_avg:87.54ms
step:705/1680 train_time:61718ms step_avg:87.54ms
step:706/1680 train_time:61806ms step_avg:87.54ms
step:707/1680 train_time:61894ms step_avg:87.54ms
step:708/1680 train_time:61982ms step_avg:87.54ms
step:709/1680 train_time:62070ms step_avg:87.55ms
step:710/1680 train_time:62158ms step_avg:87.55ms
step:711/1680 train_time:62246ms step_avg:87.55ms
step:712/1680 train_time:62334ms step_avg:87.55ms
step:713/1680 train_time:62422ms step_avg:87.55ms
step:714/1680 train_time:62511ms step_avg:87.55ms
step:715/1680 train_time:62599ms step_avg:87.55ms
step:716/1680 train_time:62687ms step_avg:87.55ms
step:717/1680 train_time:62775ms step_avg:87.55ms
step:718/1680 train_time:62864ms step_avg:87.55ms
step:719/1680 train_time:62952ms step_avg:87.55ms
step:720/1680 train_time:63040ms step_avg:87.56ms
step:721/1680 train_time:63129ms step_avg:87.56ms
step:722/1680 train_time:63218ms step_avg:87.56ms
step:723/1680 train_time:63306ms step_avg:87.56ms
step:724/1680 train_time:63394ms step_avg:87.56ms
step:725/1680 train_time:63483ms step_avg:87.56ms
step:726/1680 train_time:63570ms step_avg:87.56ms
step:727/1680 train_time:63659ms step_avg:87.56ms
step:728/1680 train_time:63748ms step_avg:87.57ms
step:729/1680 train_time:63836ms step_avg:87.57ms
step:730/1680 train_time:63925ms step_avg:87.57ms
step:731/1680 train_time:64013ms step_avg:87.57ms
step:732/1680 train_time:64101ms step_avg:87.57ms
step:733/1680 train_time:64189ms step_avg:87.57ms
step:734/1680 train_time:64277ms step_avg:87.57ms
step:735/1680 train_time:64366ms step_avg:87.57ms
step:736/1680 train_time:64456ms step_avg:87.58ms
step:737/1680 train_time:64544ms step_avg:87.58ms
step:738/1680 train_time:64632ms step_avg:87.58ms
step:739/1680 train_time:64721ms step_avg:87.58ms
step:740/1680 train_time:64809ms step_avg:87.58ms
step:741/1680 train_time:64897ms step_avg:87.58ms
step:742/1680 train_time:64986ms step_avg:87.58ms
step:743/1680 train_time:65074ms step_avg:87.58ms
step:744/1680 train_time:65161ms step_avg:87.58ms
step:745/1680 train_time:65250ms step_avg:87.58ms
step:746/1680 train_time:65338ms step_avg:87.58ms
step:747/1680 train_time:65427ms step_avg:87.59ms
step:748/1680 train_time:65517ms step_avg:87.59ms
step:749/1680 train_time:65605ms step_avg:87.59ms
step:750/1680 train_time:65693ms step_avg:87.59ms
step:750/1680 val_loss:3.5674 train_time:65782ms step_avg:87.71ms
step:751/1680 train_time:65802ms step_avg:87.62ms
step:752/1680 train_time:65874ms step_avg:87.60ms
step:753/1680 train_time:65969ms step_avg:87.61ms
step:754/1680 train_time:66059ms step_avg:87.61ms
step:755/1680 train_time:66146ms step_avg:87.61ms
step:756/1680 train_time:66234ms step_avg:87.61ms
step:757/1680 train_time:66321ms step_avg:87.61ms
step:758/1680 train_time:66409ms step_avg:87.61ms
step:759/1680 train_time:66496ms step_avg:87.61ms
step:760/1680 train_time:66584ms step_avg:87.61ms
step:761/1680 train_time:66671ms step_avg:87.61ms
step:762/1680 train_time:66760ms step_avg:87.61ms
step:763/1680 train_time:66851ms step_avg:87.62ms
step:764/1680 train_time:66942ms step_avg:87.62ms
step:765/1680 train_time:67031ms step_avg:87.62ms
step:766/1680 train_time:67119ms step_avg:87.62ms
step:767/1680 train_time:67208ms step_avg:87.62ms
step:768/1680 train_time:67296ms step_avg:87.63ms
step:769/1680 train_time:67384ms step_avg:87.63ms
step:770/1680 train_time:67471ms step_avg:87.63ms
step:771/1680 train_time:67559ms step_avg:87.63ms
step:772/1680 train_time:67646ms step_avg:87.62ms
step:773/1680 train_time:67734ms step_avg:87.63ms
step:774/1680 train_time:67823ms step_avg:87.63ms
step:775/1680 train_time:67912ms step_avg:87.63ms
step:776/1680 train_time:68001ms step_avg:87.63ms
step:777/1680 train_time:68090ms step_avg:87.63ms
step:778/1680 train_time:68179ms step_avg:87.63ms
step:779/1680 train_time:68268ms step_avg:87.64ms
step:780/1680 train_time:68356ms step_avg:87.64ms
step:781/1680 train_time:68444ms step_avg:87.64ms
step:782/1680 train_time:68532ms step_avg:87.64ms
step:783/1680 train_time:68619ms step_avg:87.64ms
step:784/1680 train_time:68707ms step_avg:87.64ms
step:785/1680 train_time:68796ms step_avg:87.64ms
step:786/1680 train_time:68885ms step_avg:87.64ms
step:787/1680 train_time:68973ms step_avg:87.64ms
step:788/1680 train_time:69063ms step_avg:87.64ms
step:789/1680 train_time:69151ms step_avg:87.64ms
step:790/1680 train_time:69241ms step_avg:87.65ms
step:791/1680 train_time:69328ms step_avg:87.65ms
step:792/1680 train_time:69416ms step_avg:87.65ms
step:793/1680 train_time:69505ms step_avg:87.65ms
step:794/1680 train_time:69593ms step_avg:87.65ms
step:795/1680 train_time:69680ms step_avg:87.65ms
step:796/1680 train_time:69769ms step_avg:87.65ms
step:797/1680 train_time:69858ms step_avg:87.65ms
step:798/1680 train_time:69946ms step_avg:87.65ms
step:799/1680 train_time:70035ms step_avg:87.65ms
step:800/1680 train_time:70124ms step_avg:87.65ms
step:801/1680 train_time:70212ms step_avg:87.66ms
step:802/1680 train_time:70301ms step_avg:87.66ms
step:803/1680 train_time:70389ms step_avg:87.66ms
step:804/1680 train_time:70477ms step_avg:87.66ms
step:805/1680 train_time:70565ms step_avg:87.66ms
step:806/1680 train_time:70653ms step_avg:87.66ms
step:807/1680 train_time:70742ms step_avg:87.66ms
step:808/1680 train_time:70830ms step_avg:87.66ms
step:809/1680 train_time:70918ms step_avg:87.66ms
step:810/1680 train_time:71007ms step_avg:87.66ms
step:811/1680 train_time:71095ms step_avg:87.66ms
step:812/1680 train_time:71184ms step_avg:87.66ms
step:813/1680 train_time:71272ms step_avg:87.67ms
step:814/1680 train_time:71361ms step_avg:87.67ms
step:815/1680 train_time:71449ms step_avg:87.67ms
step:816/1680 train_time:71537ms step_avg:87.67ms
step:817/1680 train_time:71625ms step_avg:87.67ms
step:818/1680 train_time:71714ms step_avg:87.67ms
step:819/1680 train_time:71802ms step_avg:87.67ms
step:820/1680 train_time:71890ms step_avg:87.67ms
step:821/1680 train_time:71979ms step_avg:87.67ms
step:822/1680 train_time:72067ms step_avg:87.67ms
step:823/1680 train_time:72156ms step_avg:87.67ms
step:824/1680 train_time:72245ms step_avg:87.68ms
step:825/1680 train_time:72332ms step_avg:87.68ms
step:826/1680 train_time:72421ms step_avg:87.68ms
step:827/1680 train_time:72509ms step_avg:87.68ms
step:828/1680 train_time:72597ms step_avg:87.68ms
step:829/1680 train_time:72686ms step_avg:87.68ms
step:830/1680 train_time:72774ms step_avg:87.68ms
step:831/1680 train_time:72862ms step_avg:87.68ms
step:832/1680 train_time:72951ms step_avg:87.68ms
step:833/1680 train_time:73039ms step_avg:87.68ms
step:834/1680 train_time:73128ms step_avg:87.68ms
step:835/1680 train_time:73216ms step_avg:87.68ms
step:836/1680 train_time:73305ms step_avg:87.68ms
step:837/1680 train_time:73393ms step_avg:87.69ms
step:838/1680 train_time:73481ms step_avg:87.69ms
step:839/1680 train_time:73569ms step_avg:87.69ms
step:840/1680 train_time:73658ms step_avg:87.69ms
step:841/1680 train_time:73746ms step_avg:87.69ms
step:842/1680 train_time:73835ms step_avg:87.69ms
step:843/1680 train_time:73923ms step_avg:87.69ms
step:844/1680 train_time:74011ms step_avg:87.69ms
step:845/1680 train_time:74100ms step_avg:87.69ms
step:846/1680 train_time:74189ms step_avg:87.69ms
step:847/1680 train_time:74278ms step_avg:87.69ms
step:848/1680 train_time:74366ms step_avg:87.70ms
step:849/1680 train_time:74454ms step_avg:87.70ms
step:850/1680 train_time:74542ms step_avg:87.70ms
step:851/1680 train_time:74630ms step_avg:87.70ms
step:852/1680 train_time:74718ms step_avg:87.70ms
step:853/1680 train_time:74807ms step_avg:87.70ms
step:854/1680 train_time:74895ms step_avg:87.70ms
step:855/1680 train_time:74984ms step_avg:87.70ms
step:856/1680 train_time:75072ms step_avg:87.70ms
step:857/1680 train_time:75160ms step_avg:87.70ms
step:858/1680 train_time:75249ms step_avg:87.70ms
step:859/1680 train_time:75337ms step_avg:87.70ms
step:860/1680 train_time:75426ms step_avg:87.70ms
step:861/1680 train_time:75513ms step_avg:87.70ms
step:862/1680 train_time:75601ms step_avg:87.70ms
step:863/1680 train_time:75689ms step_avg:87.70ms
step:864/1680 train_time:75777ms step_avg:87.71ms
step:865/1680 train_time:75866ms step_avg:87.71ms
step:866/1680 train_time:75954ms step_avg:87.71ms
step:867/1680 train_time:76043ms step_avg:87.71ms
step:868/1680 train_time:76131ms step_avg:87.71ms
step:869/1680 train_time:76220ms step_avg:87.71ms
step:870/1680 train_time:76308ms step_avg:87.71ms
step:871/1680 train_time:76396ms step_avg:87.71ms
step:872/1680 train_time:76485ms step_avg:87.71ms
step:873/1680 train_time:76573ms step_avg:87.71ms
step:874/1680 train_time:76662ms step_avg:87.71ms
step:875/1680 train_time:76750ms step_avg:87.71ms
step:875/1680 val_loss:3.5193 train_time:76840ms step_avg:87.82ms
step:876/1680 train_time:76859ms step_avg:87.74ms
step:877/1680 train_time:76933ms step_avg:87.72ms
step:878/1680 train_time:77026ms step_avg:87.73ms
step:879/1680 train_time:77117ms step_avg:87.73ms
step:880/1680 train_time:77205ms step_avg:87.73ms
step:881/1680 train_time:77292ms step_avg:87.73ms
step:882/1680 train_time:77380ms step_avg:87.73ms
step:883/1680 train_time:77467ms step_avg:87.73ms
step:884/1680 train_time:77554ms step_avg:87.73ms
step:885/1680 train_time:77641ms step_avg:87.73ms
step:886/1680 train_time:77728ms step_avg:87.73ms
step:887/1680 train_time:77816ms step_avg:87.73ms
step:888/1680 train_time:77907ms step_avg:87.73ms
step:889/1680 train_time:77998ms step_avg:87.74ms
step:890/1680 train_time:78087ms step_avg:87.74ms
step:891/1680 train_time:78177ms step_avg:87.74ms
step:892/1680 train_time:78267ms step_avg:87.74ms
step:893/1680 train_time:78354ms step_avg:87.74ms
step:894/1680 train_time:78441ms step_avg:87.74ms
step:895/1680 train_time:78529ms step_avg:87.74ms
step:896/1680 train_time:78617ms step_avg:87.74ms
step:897/1680 train_time:78704ms step_avg:87.74ms
step:898/1680 train_time:78792ms step_avg:87.74ms
step:899/1680 train_time:78881ms step_avg:87.74ms
step:900/1680 train_time:78972ms step_avg:87.75ms
step:901/1680 train_time:79062ms step_avg:87.75ms
step:902/1680 train_time:79151ms step_avg:87.75ms
step:903/1680 train_time:79241ms step_avg:87.75ms
step:904/1680 train_time:79329ms step_avg:87.75ms
step:905/1680 train_time:79416ms step_avg:87.75ms
step:906/1680 train_time:79504ms step_avg:87.75ms
step:907/1680 train_time:79592ms step_avg:87.75ms
step:908/1680 train_time:79680ms step_avg:87.75ms
step:909/1680 train_time:79768ms step_avg:87.75ms
step:910/1680 train_time:79857ms step_avg:87.75ms
step:911/1680 train_time:79945ms step_avg:87.76ms
step:912/1680 train_time:80034ms step_avg:87.76ms
step:913/1680 train_time:80123ms step_avg:87.76ms
step:914/1680 train_time:80212ms step_avg:87.76ms
step:915/1680 train_time:80301ms step_avg:87.76ms
step:916/1680 train_time:80388ms step_avg:87.76ms
step:917/1680 train_time:80476ms step_avg:87.76ms
step:918/1680 train_time:80564ms step_avg:87.76ms
step:919/1680 train_time:80652ms step_avg:87.76ms
step:920/1680 train_time:80740ms step_avg:87.76ms
step:921/1680 train_time:80828ms step_avg:87.76ms
step:922/1680 train_time:80917ms step_avg:87.76ms
step:923/1680 train_time:81006ms step_avg:87.76ms
step:924/1680 train_time:81094ms step_avg:87.76ms
step:925/1680 train_time:81183ms step_avg:87.77ms
step:926/1680 train_time:81272ms step_avg:87.77ms
step:927/1680 train_time:81361ms step_avg:87.77ms
step:928/1680 train_time:81449ms step_avg:87.77ms
step:929/1680 train_time:81537ms step_avg:87.77ms
step:930/1680 train_time:81625ms step_avg:87.77ms
step:931/1680 train_time:81712ms step_avg:87.77ms
step:932/1680 train_time:81800ms step_avg:87.77ms
step:933/1680 train_time:81889ms step_avg:87.77ms
step:934/1680 train_time:81977ms step_avg:87.77ms
step:935/1680 train_time:82066ms step_avg:87.77ms
step:936/1680 train_time:82155ms step_avg:87.77ms
step:937/1680 train_time:82243ms step_avg:87.77ms
step:938/1680 train_time:82331ms step_avg:87.77ms
step:939/1680 train_time:82420ms step_avg:87.77ms
step:940/1680 train_time:82508ms step_avg:87.77ms
step:941/1680 train_time:82596ms step_avg:87.77ms
step:942/1680 train_time:82684ms step_avg:87.77ms
step:943/1680 train_time:82772ms step_avg:87.78ms
step:944/1680 train_time:82860ms step_avg:87.78ms
step:945/1680 train_time:82949ms step_avg:87.78ms
step:946/1680 train_time:83037ms step_avg:87.78ms
step:947/1680 train_time:83126ms step_avg:87.78ms
step:948/1680 train_time:83215ms step_avg:87.78ms
step:949/1680 train_time:83304ms step_avg:87.78ms
step:950/1680 train_time:83391ms step_avg:87.78ms
step:951/1680 train_time:83480ms step_avg:87.78ms
step:952/1680 train_time:83568ms step_avg:87.78ms
step:953/1680 train_time:83657ms step_avg:87.78ms
step:954/1680 train_time:83745ms step_avg:87.78ms
step:955/1680 train_time:83833ms step_avg:87.78ms
step:956/1680 train_time:83921ms step_avg:87.78ms
step:957/1680 train_time:84010ms step_avg:87.79ms
step:958/1680 train_time:84099ms step_avg:87.79ms
step:959/1680 train_time:84187ms step_avg:87.79ms
step:960/1680 train_time:84276ms step_avg:87.79ms
step:961/1680 train_time:84364ms step_avg:87.79ms
step:962/1680 train_time:84452ms step_avg:87.79ms
step:963/1680 train_time:84540ms step_avg:87.79ms
step:964/1680 train_time:84629ms step_avg:87.79ms
step:965/1680 train_time:84718ms step_avg:87.79ms
step:966/1680 train_time:84806ms step_avg:87.79ms
step:967/1680 train_time:84894ms step_avg:87.79ms
step:968/1680 train_time:84982ms step_avg:87.79ms
step:969/1680 train_time:85070ms step_avg:87.79ms
step:970/1680 train_time:85159ms step_avg:87.79ms
step:971/1680 train_time:85248ms step_avg:87.79ms
step:972/1680 train_time:85336ms step_avg:87.79ms
step:973/1680 train_time:85424ms step_avg:87.79ms
step:974/1680 train_time:85512ms step_avg:87.80ms
step:975/1680 train_time:85601ms step_avg:87.80ms
step:976/1680 train_time:85689ms step_avg:87.80ms
step:977/1680 train_time:85777ms step_avg:87.80ms
step:978/1680 train_time:85864ms step_avg:87.80ms
step:979/1680 train_time:85953ms step_avg:87.80ms
step:980/1680 train_time:86041ms step_avg:87.80ms
step:981/1680 train_time:86131ms step_avg:87.80ms
step:982/1680 train_time:86219ms step_avg:87.80ms
step:983/1680 train_time:86308ms step_avg:87.80ms
step:984/1680 train_time:86396ms step_avg:87.80ms
step:985/1680 train_time:86484ms step_avg:87.80ms
step:986/1680 train_time:86572ms step_avg:87.80ms
step:987/1680 train_time:86661ms step_avg:87.80ms
step:988/1680 train_time:86750ms step_avg:87.80ms
step:989/1680 train_time:86838ms step_avg:87.80ms
step:990/1680 train_time:86926ms step_avg:87.80ms
step:991/1680 train_time:87015ms step_avg:87.80ms
step:992/1680 train_time:87103ms step_avg:87.81ms
step:993/1680 train_time:87192ms step_avg:87.81ms
step:994/1680 train_time:87280ms step_avg:87.81ms
step:995/1680 train_time:87368ms step_avg:87.81ms
step:996/1680 train_time:87457ms step_avg:87.81ms
step:997/1680 train_time:87546ms step_avg:87.81ms
step:998/1680 train_time:87634ms step_avg:87.81ms
step:999/1680 train_time:87722ms step_avg:87.81ms
step:1000/1680 train_time:87811ms step_avg:87.81ms
step:1000/1680 val_loss:3.4694 train_time:87900ms step_avg:87.90ms
step:1001/1680 train_time:87919ms step_avg:87.83ms
step:1002/1680 train_time:87992ms step_avg:87.82ms
step:1003/1680 train_time:88086ms step_avg:87.82ms
step:1004/1680 train_time:88175ms step_avg:87.82ms
step:1005/1680 train_time:88263ms step_avg:87.82ms
step:1006/1680 train_time:88350ms step_avg:87.82ms
step:1007/1680 train_time:88437ms step_avg:87.82ms
step:1008/1680 train_time:88524ms step_avg:87.82ms
step:1009/1680 train_time:88611ms step_avg:87.82ms
step:1010/1680 train_time:88699ms step_avg:87.82ms
step:1011/1680 train_time:88785ms step_avg:87.82ms
step:1012/1680 train_time:88874ms step_avg:87.82ms
step:1013/1680 train_time:88965ms step_avg:87.82ms
step:1014/1680 train_time:89056ms step_avg:87.83ms
step:1015/1680 train_time:89146ms step_avg:87.83ms
step:1016/1680 train_time:89234ms step_avg:87.83ms
step:1017/1680 train_time:89323ms step_avg:87.83ms
step:1018/1680 train_time:89410ms step_avg:87.83ms
step:1019/1680 train_time:89498ms step_avg:87.83ms
step:1020/1680 train_time:89585ms step_avg:87.83ms
step:1021/1680 train_time:89672ms step_avg:87.83ms
step:1022/1680 train_time:89760ms step_avg:87.83ms
step:1023/1680 train_time:89848ms step_avg:87.83ms
step:1024/1680 train_time:89937ms step_avg:87.83ms
step:1025/1680 train_time:90026ms step_avg:87.83ms
step:1026/1680 train_time:90115ms step_avg:87.83ms
step:1027/1680 train_time:90203ms step_avg:87.83ms
step:1028/1680 train_time:90292ms step_avg:87.83ms
step:1029/1680 train_time:90380ms step_avg:87.83ms
step:1030/1680 train_time:90468ms step_avg:87.83ms
step:1031/1680 train_time:90555ms step_avg:87.83ms
step:1032/1680 train_time:90643ms step_avg:87.83ms
step:1033/1680 train_time:90730ms step_avg:87.83ms
step:1034/1680 train_time:90818ms step_avg:87.83ms
step:1035/1680 train_time:90907ms step_avg:87.83ms
step:1036/1680 train_time:90996ms step_avg:87.83ms
step:1037/1680 train_time:91086ms step_avg:87.84ms
step:1038/1680 train_time:91175ms step_avg:87.84ms
step:1039/1680 train_time:91263ms step_avg:87.84ms
step:1040/1680 train_time:91351ms step_avg:87.84ms
step:1041/1680 train_time:91440ms step_avg:87.84ms
step:1042/1680 train_time:91528ms step_avg:87.84ms
step:1043/1680 train_time:91616ms step_avg:87.84ms
step:1044/1680 train_time:91703ms step_avg:87.84ms
step:1045/1680 train_time:91791ms step_avg:87.84ms
step:1046/1680 train_time:91880ms step_avg:87.84ms
step:1047/1680 train_time:91969ms step_avg:87.84ms
step:1048/1680 train_time:92058ms step_avg:87.84ms
step:1049/1680 train_time:92148ms step_avg:87.84ms
step:1050/1680 train_time:92236ms step_avg:87.84ms
step:1051/1680 train_time:92325ms step_avg:87.84ms
step:1052/1680 train_time:92413ms step_avg:87.85ms
step:1053/1680 train_time:92501ms step_avg:87.84ms
step:1054/1680 train_time:92589ms step_avg:87.85ms
step:1055/1680 train_time:92677ms step_avg:87.85ms
step:1056/1680 train_time:92765ms step_avg:87.85ms
step:1057/1680 train_time:92854ms step_avg:87.85ms
step:1058/1680 train_time:92942ms step_avg:87.85ms
step:1059/1680 train_time:93031ms step_avg:87.85ms
step:1060/1680 train_time:93120ms step_avg:87.85ms
step:1061/1680 train_time:93208ms step_avg:87.85ms
step:1062/1680 train_time:93297ms step_avg:87.85ms
step:1063/1680 train_time:93385ms step_avg:87.85ms
step:1064/1680 train_time:93474ms step_avg:87.85ms
step:1065/1680 train_time:93562ms step_avg:87.85ms
step:1066/1680 train_time:93649ms step_avg:87.85ms
step:1067/1680 train_time:93738ms step_avg:87.85ms
step:1068/1680 train_time:93826ms step_avg:87.85ms
step:1069/1680 train_time:93914ms step_avg:87.85ms
step:1070/1680 train_time:94002ms step_avg:87.85ms
step:1071/1680 train_time:94091ms step_avg:87.85ms
step:1072/1680 train_time:94180ms step_avg:87.85ms
step:1073/1680 train_time:94269ms step_avg:87.86ms
step:1074/1680 train_time:94357ms step_avg:87.86ms
step:1075/1680 train_time:94445ms step_avg:87.86ms
step:1076/1680 train_time:94533ms step_avg:87.86ms
step:1077/1680 train_time:94621ms step_avg:87.86ms
step:1078/1680 train_time:94710ms step_avg:87.86ms
step:1079/1680 train_time:94798ms step_avg:87.86ms
step:1080/1680 train_time:94886ms step_avg:87.86ms
step:1081/1680 train_time:94974ms step_avg:87.86ms
step:1082/1680 train_time:95062ms step_avg:87.86ms
step:1083/1680 train_time:95151ms step_avg:87.86ms
step:1084/1680 train_time:95239ms step_avg:87.86ms
step:1085/1680 train_time:95327ms step_avg:87.86ms
step:1086/1680 train_time:95416ms step_avg:87.86ms
step:1087/1680 train_time:95504ms step_avg:87.86ms
step:1088/1680 train_time:95592ms step_avg:87.86ms
step:1089/1680 train_time:95680ms step_avg:87.86ms
step:1090/1680 train_time:95769ms step_avg:87.86ms
step:1091/1680 train_time:95857ms step_avg:87.86ms
step:1092/1680 train_time:95945ms step_avg:87.86ms
step:1093/1680 train_time:96033ms step_avg:87.86ms
step:1094/1680 train_time:96121ms step_avg:87.86ms
step:1095/1680 train_time:96210ms step_avg:87.86ms
step:1096/1680 train_time:96299ms step_avg:87.86ms
step:1097/1680 train_time:96389ms step_avg:87.87ms
step:1098/1680 train_time:96478ms step_avg:87.87ms
step:1099/1680 train_time:96568ms step_avg:87.87ms
step:1100/1680 train_time:96657ms step_avg:87.87ms
step:1101/1680 train_time:96746ms step_avg:87.87ms
step:1102/1680 train_time:96834ms step_avg:87.87ms
step:1103/1680 train_time:96923ms step_avg:87.87ms
step:1104/1680 train_time:97012ms step_avg:87.87ms
step:1105/1680 train_time:97101ms step_avg:87.87ms
step:1106/1680 train_time:97191ms step_avg:87.88ms
step:1107/1680 train_time:97281ms step_avg:87.88ms
step:1108/1680 train_time:97370ms step_avg:87.88ms
step:1109/1680 train_time:97459ms step_avg:87.88ms
step:1110/1680 train_time:97548ms step_avg:87.88ms
step:1111/1680 train_time:97637ms step_avg:87.88ms
step:1112/1680 train_time:97728ms step_avg:87.88ms
step:1113/1680 train_time:97817ms step_avg:87.89ms
step:1114/1680 train_time:97906ms step_avg:87.89ms
step:1115/1680 train_time:97995ms step_avg:87.89ms
step:1116/1680 train_time:98086ms step_avg:87.89ms
step:1117/1680 train_time:98175ms step_avg:87.89ms
step:1118/1680 train_time:98264ms step_avg:87.89ms
step:1119/1680 train_time:98354ms step_avg:87.89ms
step:1120/1680 train_time:98442ms step_avg:87.89ms
step:1121/1680 train_time:98531ms step_avg:87.90ms
step:1122/1680 train_time:98620ms step_avg:87.90ms
step:1123/1680 train_time:98710ms step_avg:87.90ms
step:1124/1680 train_time:98799ms step_avg:87.90ms
step:1125/1680 train_time:98888ms step_avg:87.90ms
step:1125/1680 val_loss:3.4160 train_time:98979ms step_avg:87.98ms
step:1126/1680 train_time:98998ms step_avg:87.92ms
step:1127/1680 train_time:99070ms step_avg:87.91ms
step:1128/1680 train_time:99160ms step_avg:87.91ms
step:1129/1680 train_time:99250ms step_avg:87.91ms
step:1130/1680 train_time:99339ms step_avg:87.91ms
step:1131/1680 train_time:99428ms step_avg:87.91ms
step:1132/1680 train_time:99516ms step_avg:87.91ms
step:1133/1680 train_time:99605ms step_avg:87.91ms
step:1134/1680 train_time:99692ms step_avg:87.91ms
step:1135/1680 train_time:99780ms step_avg:87.91ms
step:1136/1680 train_time:99870ms step_avg:87.91ms
step:1137/1680 train_time:99961ms step_avg:87.92ms
step:1138/1680 train_time:100052ms step_avg:87.92ms
step:1139/1680 train_time:100143ms step_avg:87.92ms
step:1140/1680 train_time:100234ms step_avg:87.92ms
step:1141/1680 train_time:100323ms step_avg:87.93ms
step:1142/1680 train_time:100411ms step_avg:87.93ms
step:1143/1680 train_time:100500ms step_avg:87.93ms
step:1144/1680 train_time:100589ms step_avg:87.93ms
step:1145/1680 train_time:100677ms step_avg:87.93ms
step:1146/1680 train_time:100765ms step_avg:87.93ms
step:1147/1680 train_time:100854ms step_avg:87.93ms
step:1148/1680 train_time:100943ms step_avg:87.93ms
step:1149/1680 train_time:101033ms step_avg:87.93ms
step:1150/1680 train_time:101122ms step_avg:87.93ms
step:1151/1680 train_time:101212ms step_avg:87.93ms
step:1152/1680 train_time:101302ms step_avg:87.94ms
step:1153/1680 train_time:101390ms step_avg:87.94ms
step:1154/1680 train_time:101479ms step_avg:87.94ms
step:1155/1680 train_time:101568ms step_avg:87.94ms
step:1156/1680 train_time:101656ms step_avg:87.94ms
step:1157/1680 train_time:101746ms step_avg:87.94ms
step:1158/1680 train_time:101834ms step_avg:87.94ms
step:1159/1680 train_time:101924ms step_avg:87.94ms
step:1160/1680 train_time:102014ms step_avg:87.94ms
step:1161/1680 train_time:102104ms step_avg:87.94ms
step:1162/1680 train_time:102193ms step_avg:87.95ms
step:1163/1680 train_time:102282ms step_avg:87.95ms
step:1164/1680 train_time:102372ms step_avg:87.95ms
step:1165/1680 train_time:102461ms step_avg:87.95ms
step:1166/1680 train_time:102550ms step_avg:87.95ms
step:1167/1680 train_time:102639ms step_avg:87.95ms
step:1168/1680 train_time:102729ms step_avg:87.95ms
step:1169/1680 train_time:102817ms step_avg:87.95ms
step:1170/1680 train_time:102906ms step_avg:87.95ms
step:1171/1680 train_time:102994ms step_avg:87.95ms
step:1172/1680 train_time:103084ms step_avg:87.96ms
step:1173/1680 train_time:103173ms step_avg:87.96ms
step:1174/1680 train_time:103262ms step_avg:87.96ms
step:1175/1680 train_time:103352ms step_avg:87.96ms
step:1176/1680 train_time:103441ms step_avg:87.96ms
step:1177/1680 train_time:103530ms step_avg:87.96ms
step:1178/1680 train_time:103618ms step_avg:87.96ms
step:1179/1680 train_time:103706ms step_avg:87.96ms
step:1180/1680 train_time:103795ms step_avg:87.96ms
step:1181/1680 train_time:103884ms step_avg:87.96ms
step:1182/1680 train_time:103973ms step_avg:87.96ms
step:1183/1680 train_time:104062ms step_avg:87.96ms
step:1184/1680 train_time:104151ms step_avg:87.97ms
step:1185/1680 train_time:104240ms step_avg:87.97ms
step:1186/1680 train_time:104329ms step_avg:87.97ms
step:1187/1680 train_time:104418ms step_avg:87.97ms
step:1188/1680 train_time:104507ms step_avg:87.97ms
step:1189/1680 train_time:104596ms step_avg:87.97ms
step:1190/1680 train_time:104685ms step_avg:87.97ms
step:1191/1680 train_time:104773ms step_avg:87.97ms
step:1192/1680 train_time:104863ms step_avg:87.97ms
step:1193/1680 train_time:104952ms step_avg:87.97ms
step:1194/1680 train_time:105042ms step_avg:87.98ms
step:1195/1680 train_time:105132ms step_avg:87.98ms
step:1196/1680 train_time:105221ms step_avg:87.98ms
step:1197/1680 train_time:105309ms step_avg:87.98ms
step:1198/1680 train_time:105398ms step_avg:87.98ms
step:1199/1680 train_time:105487ms step_avg:87.98ms
step:1200/1680 train_time:105576ms step_avg:87.98ms
step:1201/1680 train_time:105665ms step_avg:87.98ms
step:1202/1680 train_time:105754ms step_avg:87.98ms
step:1203/1680 train_time:105844ms step_avg:87.98ms
step:1204/1680 train_time:105934ms step_avg:87.99ms
step:1205/1680 train_time:106023ms step_avg:87.99ms
step:1206/1680 train_time:106112ms step_avg:87.99ms
step:1207/1680 train_time:106202ms step_avg:87.99ms
step:1208/1680 train_time:106290ms step_avg:87.99ms
step:1209/1680 train_time:106379ms step_avg:87.99ms
step:1210/1680 train_time:106468ms step_avg:87.99ms
step:1211/1680 train_time:106558ms step_avg:87.99ms
step:1212/1680 train_time:106646ms step_avg:87.99ms
step:1213/1680 train_time:106735ms step_avg:87.99ms
step:1214/1680 train_time:106825ms step_avg:87.99ms
step:1215/1680 train_time:106914ms step_avg:87.99ms
step:1216/1680 train_time:107003ms step_avg:88.00ms
step:1217/1680 train_time:107093ms step_avg:88.00ms
step:1218/1680 train_time:107182ms step_avg:88.00ms
step:1219/1680 train_time:107271ms step_avg:88.00ms
step:1220/1680 train_time:107361ms step_avg:88.00ms
step:1221/1680 train_time:107450ms step_avg:88.00ms
step:1222/1680 train_time:107539ms step_avg:88.00ms
step:1223/1680 train_time:107628ms step_avg:88.00ms
step:1224/1680 train_time:107717ms step_avg:88.00ms
step:1225/1680 train_time:107806ms step_avg:88.00ms
step:1226/1680 train_time:107895ms step_avg:88.01ms
step:1227/1680 train_time:107984ms step_avg:88.01ms
step:1228/1680 train_time:108074ms step_avg:88.01ms
step:1229/1680 train_time:108162ms step_avg:88.01ms
step:1230/1680 train_time:108252ms step_avg:88.01ms
step:1231/1680 train_time:108341ms step_avg:88.01ms
step:1232/1680 train_time:108431ms step_avg:88.01ms
step:1233/1680 train_time:108521ms step_avg:88.01ms
step:1234/1680 train_time:108610ms step_avg:88.01ms
step:1235/1680 train_time:108699ms step_avg:88.02ms
step:1236/1680 train_time:108789ms step_avg:88.02ms
step:1237/1680 train_time:108878ms step_avg:88.02ms
step:1238/1680 train_time:108967ms step_avg:88.02ms
step:1239/1680 train_time:109056ms step_avg:88.02ms
step:1240/1680 train_time:109145ms step_avg:88.02ms
step:1241/1680 train_time:109234ms step_avg:88.02ms
step:1242/1680 train_time:109323ms step_avg:88.02ms
step:1243/1680 train_time:109413ms step_avg:88.02ms
step:1244/1680 train_time:109504ms step_avg:88.03ms
step:1245/1680 train_time:109592ms step_avg:88.03ms
step:1246/1680 train_time:109681ms step_avg:88.03ms
step:1247/1680 train_time:109770ms step_avg:88.03ms
step:1248/1680 train_time:109859ms step_avg:88.03ms
step:1249/1680 train_time:109948ms step_avg:88.03ms
step:1250/1680 train_time:110037ms step_avg:88.03ms
step:1250/1680 val_loss:3.3779 train_time:110128ms step_avg:88.10ms
step:1251/1680 train_time:110147ms step_avg:88.05ms
step:1252/1680 train_time:110219ms step_avg:88.03ms
step:1253/1680 train_time:110314ms step_avg:88.04ms
step:1254/1680 train_time:110404ms step_avg:88.04ms
step:1255/1680 train_time:110492ms step_avg:88.04ms
step:1256/1680 train_time:110580ms step_avg:88.04ms
step:1257/1680 train_time:110669ms step_avg:88.04ms
step:1258/1680 train_time:110756ms step_avg:88.04ms
step:1259/1680 train_time:110844ms step_avg:88.04ms
step:1260/1680 train_time:110932ms step_avg:88.04ms
step:1261/1680 train_time:111020ms step_avg:88.04ms
step:1262/1680 train_time:111111ms step_avg:88.04ms
step:1263/1680 train_time:111202ms step_avg:88.05ms
step:1264/1680 train_time:111294ms step_avg:88.05ms
step:1265/1680 train_time:111385ms step_avg:88.05ms
step:1266/1680 train_time:111474ms step_avg:88.05ms
step:1267/1680 train_time:111562ms step_avg:88.05ms
step:1268/1680 train_time:111650ms step_avg:88.05ms
step:1269/1680 train_time:111739ms step_avg:88.05ms
step:1270/1680 train_time:111827ms step_avg:88.05ms
step:1271/1680 train_time:111915ms step_avg:88.05ms
step:1272/1680 train_time:112004ms step_avg:88.05ms
step:1273/1680 train_time:112093ms step_avg:88.05ms
step:1274/1680 train_time:112182ms step_avg:88.06ms
step:1275/1680 train_time:112273ms step_avg:88.06ms
step:1276/1680 train_time:112364ms step_avg:88.06ms
step:1277/1680 train_time:112453ms step_avg:88.06ms
step:1278/1680 train_time:112543ms step_avg:88.06ms
step:1279/1680 train_time:112632ms step_avg:88.06ms
step:1280/1680 train_time:112720ms step_avg:88.06ms
step:1281/1680 train_time:112808ms step_avg:88.06ms
step:1282/1680 train_time:112897ms step_avg:88.06ms
step:1283/1680 train_time:112986ms step_avg:88.06ms
step:1284/1680 train_time:113075ms step_avg:88.06ms
step:1285/1680 train_time:113165ms step_avg:88.07ms
step:1286/1680 train_time:113254ms step_avg:88.07ms
step:1287/1680 train_time:113344ms step_avg:88.07ms
step:1288/1680 train_time:113434ms step_avg:88.07ms
step:1289/1680 train_time:113524ms step_avg:88.07ms
step:1290/1680 train_time:113612ms step_avg:88.07ms
step:1291/1680 train_time:113702ms step_avg:88.07ms
step:1292/1680 train_time:113791ms step_avg:88.07ms
step:1293/1680 train_time:113879ms step_avg:88.07ms
step:1294/1680 train_time:113968ms step_avg:88.07ms
step:1295/1680 train_time:114057ms step_avg:88.07ms
step:1296/1680 train_time:114146ms step_avg:88.08ms
step:1297/1680 train_time:114235ms step_avg:88.08ms
step:1298/1680 train_time:114325ms step_avg:88.08ms
step:1299/1680 train_time:114415ms step_avg:88.08ms
step:1300/1680 train_time:114505ms step_avg:88.08ms
step:1301/1680 train_time:114594ms step_avg:88.08ms
step:1302/1680 train_time:114684ms step_avg:88.08ms
step:1303/1680 train_time:114773ms step_avg:88.08ms
step:1304/1680 train_time:114862ms step_avg:88.08ms
step:1305/1680 train_time:114951ms step_avg:88.09ms
step:1306/1680 train_time:115039ms step_avg:88.09ms
step:1307/1680 train_time:115128ms step_avg:88.09ms
step:1308/1680 train_time:115218ms step_avg:88.09ms
step:1309/1680 train_time:115307ms step_avg:88.09ms
step:1310/1680 train_time:115397ms step_avg:88.09ms
step:1311/1680 train_time:115486ms step_avg:88.09ms
step:1312/1680 train_time:115575ms step_avg:88.09ms
step:1313/1680 train_time:115663ms step_avg:88.09ms
step:1314/1680 train_time:115752ms step_avg:88.09ms
step:1315/1680 train_time:115841ms step_avg:88.09ms
step:1316/1680 train_time:115930ms step_avg:88.09ms
step:1317/1680 train_time:116018ms step_avg:88.09ms
step:1318/1680 train_time:116108ms step_avg:88.09ms
step:1319/1680 train_time:116196ms step_avg:88.09ms
step:1320/1680 train_time:116286ms step_avg:88.10ms
step:1321/1680 train_time:116376ms step_avg:88.10ms
step:1322/1680 train_time:116466ms step_avg:88.10ms
step:1323/1680 train_time:116556ms step_avg:88.10ms
step:1324/1680 train_time:116644ms step_avg:88.10ms
step:1325/1680 train_time:116734ms step_avg:88.10ms
step:1326/1680 train_time:116823ms step_avg:88.10ms
step:1327/1680 train_time:116911ms step_avg:88.10ms
step:1328/1680 train_time:117000ms step_avg:88.10ms
step:1329/1680 train_time:117091ms step_avg:88.10ms
step:1330/1680 train_time:117180ms step_avg:88.11ms
step:1331/1680 train_time:117270ms step_avg:88.11ms
step:1332/1680 train_time:117359ms step_avg:88.11ms
step:1333/1680 train_time:117448ms step_avg:88.11ms
step:1334/1680 train_time:117538ms step_avg:88.11ms
step:1335/1680 train_time:117627ms step_avg:88.11ms
step:1336/1680 train_time:117716ms step_avg:88.11ms
step:1337/1680 train_time:117805ms step_avg:88.11ms
step:1338/1680 train_time:117894ms step_avg:88.11ms
step:1339/1680 train_time:117984ms step_avg:88.11ms
step:1340/1680 train_time:118073ms step_avg:88.11ms
step:1341/1680 train_time:118162ms step_avg:88.11ms
step:1342/1680 train_time:118250ms step_avg:88.12ms
step:1343/1680 train_time:118339ms step_avg:88.12ms
step:1344/1680 train_time:118428ms step_avg:88.12ms
step:1345/1680 train_time:118517ms step_avg:88.12ms
step:1346/1680 train_time:118607ms step_avg:88.12ms
step:1347/1680 train_time:118696ms step_avg:88.12ms
step:1348/1680 train_time:118785ms step_avg:88.12ms
step:1349/1680 train_time:118875ms step_avg:88.12ms
step:1350/1680 train_time:118964ms step_avg:88.12ms
step:1351/1680 train_time:119053ms step_avg:88.12ms
step:1352/1680 train_time:119142ms step_avg:88.12ms
step:1353/1680 train_time:119231ms step_avg:88.12ms
step:1354/1680 train_time:119320ms step_avg:88.12ms
step:1355/1680 train_time:119410ms step_avg:88.13ms
step:1356/1680 train_time:119499ms step_avg:88.13ms
step:1357/1680 train_time:119589ms step_avg:88.13ms
step:1358/1680 train_time:119677ms step_avg:88.13ms
step:1359/1680 train_time:119767ms step_avg:88.13ms
step:1360/1680 train_time:119855ms step_avg:88.13ms
step:1361/1680 train_time:119944ms step_avg:88.13ms
step:1362/1680 train_time:120033ms step_avg:88.13ms
step:1363/1680 train_time:120122ms step_avg:88.13ms
step:1364/1680 train_time:120212ms step_avg:88.13ms
step:1365/1680 train_time:120301ms step_avg:88.13ms
step:1366/1680 train_time:120391ms step_avg:88.13ms
step:1367/1680 train_time:120480ms step_avg:88.13ms
step:1368/1680 train_time:120569ms step_avg:88.14ms
step:1369/1680 train_time:120659ms step_avg:88.14ms
step:1370/1680 train_time:120748ms step_avg:88.14ms
step:1371/1680 train_time:120837ms step_avg:88.14ms
step:1372/1680 train_time:120926ms step_avg:88.14ms
step:1373/1680 train_time:121016ms step_avg:88.14ms
step:1374/1680 train_time:121104ms step_avg:88.14ms
step:1375/1680 train_time:121194ms step_avg:88.14ms
step:1375/1680 val_loss:3.3434 train_time:121285ms step_avg:88.21ms
step:1376/1680 train_time:121303ms step_avg:88.16ms
step:1377/1680 train_time:121378ms step_avg:88.15ms
step:1378/1680 train_time:121473ms step_avg:88.15ms
step:1379/1680 train_time:121562ms step_avg:88.15ms
step:1380/1680 train_time:121650ms step_avg:88.15ms
step:1381/1680 train_time:121737ms step_avg:88.15ms
step:1382/1680 train_time:121825ms step_avg:88.15ms
step:1383/1680 train_time:121914ms step_avg:88.15ms
step:1384/1680 train_time:122001ms step_avg:88.15ms
step:1385/1680 train_time:122089ms step_avg:88.15ms
step:1386/1680 train_time:122177ms step_avg:88.15ms
step:1387/1680 train_time:122267ms step_avg:88.15ms
step:1388/1680 train_time:122358ms step_avg:88.15ms
step:1389/1680 train_time:122450ms step_avg:88.16ms
step:1390/1680 train_time:122542ms step_avg:88.16ms
step:1391/1680 train_time:122632ms step_avg:88.16ms
step:1392/1680 train_time:122720ms step_avg:88.16ms
step:1393/1680 train_time:122809ms step_avg:88.16ms
step:1394/1680 train_time:122897ms step_avg:88.16ms
step:1395/1680 train_time:122985ms step_avg:88.16ms
step:1396/1680 train_time:123073ms step_avg:88.16ms
step:1397/1680 train_time:123161ms step_avg:88.16ms
step:1398/1680 train_time:123250ms step_avg:88.16ms
step:1399/1680 train_time:123339ms step_avg:88.16ms
step:1400/1680 train_time:123430ms step_avg:88.16ms
step:1401/1680 train_time:123520ms step_avg:88.17ms
step:1402/1680 train_time:123610ms step_avg:88.17ms
step:1403/1680 train_time:123699ms step_avg:88.17ms
step:1404/1680 train_time:123788ms step_avg:88.17ms
step:1405/1680 train_time:123876ms step_avg:88.17ms
step:1406/1680 train_time:123965ms step_avg:88.17ms
step:1407/1680 train_time:124053ms step_avg:88.17ms
step:1408/1680 train_time:124141ms step_avg:88.17ms
step:1409/1680 train_time:124231ms step_avg:88.17ms
step:1410/1680 train_time:124320ms step_avg:88.17ms
step:1411/1680 train_time:124409ms step_avg:88.17ms
step:1412/1680 train_time:124499ms step_avg:88.17ms
step:1413/1680 train_time:124589ms step_avg:88.17ms
step:1414/1680 train_time:124678ms step_avg:88.17ms
step:1415/1680 train_time:124768ms step_avg:88.18ms
step:1416/1680 train_time:124857ms step_avg:88.18ms
step:1417/1680 train_time:124945ms step_avg:88.18ms
step:1418/1680 train_time:125034ms step_avg:88.18ms
step:1419/1680 train_time:125123ms step_avg:88.18ms
step:1420/1680 train_time:125213ms step_avg:88.18ms
step:1421/1680 train_time:125301ms step_avg:88.18ms
step:1422/1680 train_time:125390ms step_avg:88.18ms
step:1423/1680 train_time:125480ms step_avg:88.18ms
step:1424/1680 train_time:125570ms step_avg:88.18ms
step:1425/1680 train_time:125659ms step_avg:88.18ms
step:1426/1680 train_time:125750ms step_avg:88.18ms
step:1427/1680 train_time:125839ms step_avg:88.18ms
step:1428/1680 train_time:125929ms step_avg:88.19ms
step:1429/1680 train_time:126018ms step_avg:88.19ms
step:1430/1680 train_time:126106ms step_avg:88.19ms
step:1431/1680 train_time:126195ms step_avg:88.19ms
step:1432/1680 train_time:126283ms step_avg:88.19ms
step:1433/1680 train_time:126373ms step_avg:88.19ms
step:1434/1680 train_time:126462ms step_avg:88.19ms
step:1435/1680 train_time:126553ms step_avg:88.19ms
step:1436/1680 train_time:126642ms step_avg:88.19ms
step:1437/1680 train_time:126731ms step_avg:88.19ms
step:1438/1680 train_time:126820ms step_avg:88.19ms
step:1439/1680 train_time:126910ms step_avg:88.19ms
step:1440/1680 train_time:126999ms step_avg:88.19ms
step:1441/1680 train_time:127088ms step_avg:88.19ms
step:1442/1680 train_time:127177ms step_avg:88.19ms
step:1443/1680 train_time:127266ms step_avg:88.20ms
step:1444/1680 train_time:127355ms step_avg:88.20ms
step:1445/1680 train_time:127445ms step_avg:88.20ms
step:1446/1680 train_time:127536ms step_avg:88.20ms
step:1447/1680 train_time:127626ms step_avg:88.20ms
step:1448/1680 train_time:127716ms step_avg:88.20ms
step:1449/1680 train_time:127805ms step_avg:88.20ms
step:1450/1680 train_time:127895ms step_avg:88.20ms
step:1451/1680 train_time:127984ms step_avg:88.20ms
step:1452/1680 train_time:128072ms step_avg:88.20ms
step:1453/1680 train_time:128161ms step_avg:88.20ms
step:1454/1680 train_time:128251ms step_avg:88.21ms
step:1455/1680 train_time:128340ms step_avg:88.21ms
step:1456/1680 train_time:128429ms step_avg:88.21ms
step:1457/1680 train_time:128518ms step_avg:88.21ms
step:1458/1680 train_time:128607ms step_avg:88.21ms
step:1459/1680 train_time:128697ms step_avg:88.21ms
step:1460/1680 train_time:128787ms step_avg:88.21ms
step:1461/1680 train_time:128876ms step_avg:88.21ms
step:1462/1680 train_time:128965ms step_avg:88.21ms
step:1463/1680 train_time:129053ms step_avg:88.21ms
step:1464/1680 train_time:129143ms step_avg:88.21ms
step:1465/1680 train_time:129232ms step_avg:88.21ms
step:1466/1680 train_time:129321ms step_avg:88.21ms
step:1467/1680 train_time:129411ms step_avg:88.21ms
step:1468/1680 train_time:129499ms step_avg:88.21ms
step:1469/1680 train_time:129589ms step_avg:88.22ms
step:1470/1680 train_time:129678ms step_avg:88.22ms
step:1471/1680 train_time:129767ms step_avg:88.22ms
step:1472/1680 train_time:129856ms step_avg:88.22ms
step:1473/1680 train_time:129946ms step_avg:88.22ms
step:1474/1680 train_time:130035ms step_avg:88.22ms
step:1475/1680 train_time:130125ms step_avg:88.22ms
step:1476/1680 train_time:130215ms step_avg:88.22ms
step:1477/1680 train_time:130305ms step_avg:88.22ms
step:1478/1680 train_time:130395ms step_avg:88.22ms
step:1479/1680 train_time:130485ms step_avg:88.22ms
step:1480/1680 train_time:130575ms step_avg:88.23ms
step:1481/1680 train_time:130666ms step_avg:88.23ms
step:1482/1680 train_time:130755ms step_avg:88.23ms
step:1483/1680 train_time:130845ms step_avg:88.23ms
step:1484/1680 train_time:130934ms step_avg:88.23ms
step:1485/1680 train_time:131023ms step_avg:88.23ms
step:1486/1680 train_time:131113ms step_avg:88.23ms
step:1487/1680 train_time:131201ms step_avg:88.23ms
step:1488/1680 train_time:131290ms step_avg:88.23ms
step:1489/1680 train_time:131379ms step_avg:88.23ms
step:1490/1680 train_time:131468ms step_avg:88.23ms
step:1491/1680 train_time:131558ms step_avg:88.23ms
step:1492/1680 train_time:131647ms step_avg:88.24ms
step:1493/1680 train_time:131737ms step_avg:88.24ms
step:1494/1680 train_time:131827ms step_avg:88.24ms
step:1495/1680 train_time:131916ms step_avg:88.24ms
step:1496/1680 train_time:132006ms step_avg:88.24ms
step:1497/1680 train_time:132095ms step_avg:88.24ms
step:1498/1680 train_time:132184ms step_avg:88.24ms
step:1499/1680 train_time:132273ms step_avg:88.24ms
step:1500/1680 train_time:132361ms step_avg:88.24ms
step:1500/1680 val_loss:3.3132 train_time:132452ms step_avg:88.30ms
step:1501/1680 train_time:132472ms step_avg:88.26ms
step:1502/1680 train_time:132545ms step_avg:88.25ms
step:1503/1680 train_time:132637ms step_avg:88.25ms
step:1504/1680 train_time:132727ms step_avg:88.25ms
step:1505/1680 train_time:132815ms step_avg:88.25ms
step:1506/1680 train_time:132904ms step_avg:88.25ms
step:1507/1680 train_time:132991ms step_avg:88.25ms
step:1508/1680 train_time:133079ms step_avg:88.25ms
step:1509/1680 train_time:133167ms step_avg:88.25ms
step:1510/1680 train_time:133256ms step_avg:88.25ms
step:1511/1680 train_time:133345ms step_avg:88.25ms
step:1512/1680 train_time:133435ms step_avg:88.25ms
step:1513/1680 train_time:133526ms step_avg:88.25ms
step:1514/1680 train_time:133617ms step_avg:88.25ms
step:1515/1680 train_time:133707ms step_avg:88.26ms
step:1516/1680 train_time:133797ms step_avg:88.26ms
step:1517/1680 train_time:133885ms step_avg:88.26ms
step:1518/1680 train_time:133973ms step_avg:88.26ms
step:1519/1680 train_time:134062ms step_avg:88.26ms
step:1520/1680 train_time:134151ms step_avg:88.26ms
step:1521/1680 train_time:134239ms step_avg:88.26ms
step:1522/1680 train_time:134327ms step_avg:88.26ms
step:1523/1680 train_time:134417ms step_avg:88.26ms
step:1524/1680 train_time:134506ms step_avg:88.26ms
step:1525/1680 train_time:134597ms step_avg:88.26ms
step:1526/1680 train_time:134688ms step_avg:88.26ms
step:1527/1680 train_time:134777ms step_avg:88.26ms
step:1528/1680 train_time:134867ms step_avg:88.26ms
step:1529/1680 train_time:134957ms step_avg:88.26ms
step:1530/1680 train_time:135045ms step_avg:88.26ms
step:1531/1680 train_time:135133ms step_avg:88.26ms
step:1532/1680 train_time:135222ms step_avg:88.26ms
step:1533/1680 train_time:135310ms step_avg:88.26ms
step:1534/1680 train_time:135399ms step_avg:88.27ms
step:1535/1680 train_time:135489ms step_avg:88.27ms
step:1536/1680 train_time:135579ms step_avg:88.27ms
step:1537/1680 train_time:135669ms step_avg:88.27ms
step:1538/1680 train_time:135759ms step_avg:88.27ms
step:1539/1680 train_time:135848ms step_avg:88.27ms
step:1540/1680 train_time:135938ms step_avg:88.27ms
step:1541/1680 train_time:136027ms step_avg:88.27ms
step:1542/1680 train_time:136115ms step_avg:88.27ms
step:1543/1680 train_time:136204ms step_avg:88.27ms
step:1544/1680 train_time:136293ms step_avg:88.27ms
step:1545/1680 train_time:136382ms step_avg:88.27ms
step:1546/1680 train_time:136470ms step_avg:88.27ms
step:1547/1680 train_time:136560ms step_avg:88.27ms
step:1548/1680 train_time:136650ms step_avg:88.28ms
step:1549/1680 train_time:136739ms step_avg:88.28ms
step:1550/1680 train_time:136829ms step_avg:88.28ms
step:1551/1680 train_time:136917ms step_avg:88.28ms
step:1552/1680 train_time:137006ms step_avg:88.28ms
step:1553/1680 train_time:137095ms step_avg:88.28ms
step:1554/1680 train_time:137184ms step_avg:88.28ms
step:1555/1680 train_time:137274ms step_avg:88.28ms
step:1556/1680 train_time:137363ms step_avg:88.28ms
step:1557/1680 train_time:137452ms step_avg:88.28ms
step:1558/1680 train_time:137541ms step_avg:88.28ms
step:1559/1680 train_time:137631ms step_avg:88.28ms
step:1560/1680 train_time:137721ms step_avg:88.28ms
step:1561/1680 train_time:137811ms step_avg:88.28ms
step:1562/1680 train_time:137900ms step_avg:88.28ms
step:1563/1680 train_time:137989ms step_avg:88.28ms
step:1564/1680 train_time:138077ms step_avg:88.28ms
step:1565/1680 train_time:138166ms step_avg:88.28ms
step:1566/1680 train_time:138256ms step_avg:88.29ms
step:1567/1680 train_time:138345ms step_avg:88.29ms
step:1568/1680 train_time:138435ms step_avg:88.29ms
step:1569/1680 train_time:138525ms step_avg:88.29ms
step:1570/1680 train_time:138614ms step_avg:88.29ms
step:1571/1680 train_time:138703ms step_avg:88.29ms
step:1572/1680 train_time:138793ms step_avg:88.29ms
step:1573/1680 train_time:138882ms step_avg:88.29ms
step:1574/1680 train_time:138971ms step_avg:88.29ms
step:1575/1680 train_time:139060ms step_avg:88.29ms
step:1576/1680 train_time:139150ms step_avg:88.29ms
step:1577/1680 train_time:139238ms step_avg:88.29ms
step:1578/1680 train_time:139327ms step_avg:88.29ms
step:1579/1680 train_time:139416ms step_avg:88.29ms
step:1580/1680 train_time:139505ms step_avg:88.29ms
step:1581/1680 train_time:139595ms step_avg:88.30ms
step:1582/1680 train_time:139685ms step_avg:88.30ms
step:1583/1680 train_time:139774ms step_avg:88.30ms
step:1584/1680 train_time:139864ms step_avg:88.30ms
step:1585/1680 train_time:139953ms step_avg:88.30ms
step:1586/1680 train_time:140043ms step_avg:88.30ms
step:1587/1680 train_time:140131ms step_avg:88.30ms
step:1588/1680 train_time:140220ms step_avg:88.30ms
step:1589/1680 train_time:140310ms step_avg:88.30ms
step:1590/1680 train_time:140399ms step_avg:88.30ms
step:1591/1680 train_time:140488ms step_avg:88.30ms
step:1592/1680 train_time:140577ms step_avg:88.30ms
step:1593/1680 train_time:140666ms step_avg:88.30ms
step:1594/1680 train_time:140756ms step_avg:88.30ms
step:1595/1680 train_time:140845ms step_avg:88.30ms
step:1596/1680 train_time:140933ms step_avg:88.30ms
step:1597/1680 train_time:141022ms step_avg:88.30ms
step:1598/1680 train_time:141112ms step_avg:88.31ms
step:1599/1680 train_time:141200ms step_avg:88.31ms
step:1600/1680 train_time:141290ms step_avg:88.31ms
step:1601/1680 train_time:141379ms step_avg:88.31ms
step:1602/1680 train_time:141468ms step_avg:88.31ms
step:1603/1680 train_time:141557ms step_avg:88.31ms
step:1604/1680 train_time:141646ms step_avg:88.31ms
step:1605/1680 train_time:141735ms step_avg:88.31ms
step:1606/1680 train_time:141824ms step_avg:88.31ms
step:1607/1680 train_time:141914ms step_avg:88.31ms
step:1608/1680 train_time:142003ms step_avg:88.31ms
step:1609/1680 train_time:142092ms step_avg:88.31ms
step:1610/1680 train_time:142181ms step_avg:88.31ms
step:1611/1680 train_time:142271ms step_avg:88.31ms
step:1612/1680 train_time:142361ms step_avg:88.31ms
step:1613/1680 train_time:142451ms step_avg:88.31ms
step:1614/1680 train_time:142540ms step_avg:88.31ms
step:1615/1680 train_time:142629ms step_avg:88.32ms
step:1616/1680 train_time:142718ms step_avg:88.32ms
step:1617/1680 train_time:142808ms step_avg:88.32ms
step:1618/1680 train_time:142897ms step_avg:88.32ms
step:1619/1680 train_time:142986ms step_avg:88.32ms
step:1620/1680 train_time:143075ms step_avg:88.32ms
step:1621/1680 train_time:143166ms step_avg:88.32ms
step:1622/1680 train_time:143255ms step_avg:88.32ms
step:1623/1680 train_time:143344ms step_avg:88.32ms
step:1624/1680 train_time:143434ms step_avg:88.32ms
step:1625/1680 train_time:143522ms step_avg:88.32ms
step:1625/1680 val_loss:3.2899 train_time:143613ms step_avg:88.38ms
step:1626/1680 train_time:143632ms step_avg:88.33ms
step:1627/1680 train_time:143705ms step_avg:88.33ms
step:1628/1680 train_time:143799ms step_avg:88.33ms
step:1629/1680 train_time:143890ms step_avg:88.33ms
step:1630/1680 train_time:143978ms step_avg:88.33ms
step:1631/1680 train_time:144066ms step_avg:88.33ms
step:1632/1680 train_time:144154ms step_avg:88.33ms
step:1633/1680 train_time:144243ms step_avg:88.33ms
step:1634/1680 train_time:144332ms step_avg:88.33ms
step:1635/1680 train_time:144420ms step_avg:88.33ms
step:1636/1680 train_time:144509ms step_avg:88.33ms
step:1637/1680 train_time:144598ms step_avg:88.33ms
step:1638/1680 train_time:144689ms step_avg:88.33ms
step:1639/1680 train_time:144780ms step_avg:88.33ms
step:1640/1680 train_time:144870ms step_avg:88.34ms
step:1641/1680 train_time:144961ms step_avg:88.34ms
step:1642/1680 train_time:145050ms step_avg:88.34ms
step:1643/1680 train_time:145139ms step_avg:88.34ms
step:1644/1680 train_time:145227ms step_avg:88.34ms
step:1645/1680 train_time:145316ms step_avg:88.34ms
step:1646/1680 train_time:145405ms step_avg:88.34ms
step:1647/1680 train_time:145493ms step_avg:88.34ms
step:1648/1680 train_time:145582ms step_avg:88.34ms
step:1649/1680 train_time:145674ms step_avg:88.34ms
step:1650/1680 train_time:145764ms step_avg:88.34ms
step:1651/1680 train_time:145854ms step_avg:88.34ms
step:1652/1680 train_time:145943ms step_avg:88.34ms
step:1653/1680 train_time:146034ms step_avg:88.34ms
step:1654/1680 train_time:146122ms step_avg:88.34ms
step:1655/1680 train_time:146211ms step_avg:88.35ms
step:1656/1680 train_time:146300ms step_avg:88.35ms
step:1657/1680 train_time:146389ms step_avg:88.35ms
step:1658/1680 train_time:146477ms step_avg:88.35ms
step:1659/1680 train_time:146566ms step_avg:88.35ms
step:1660/1680 train_time:146656ms step_avg:88.35ms
step:1661/1680 train_time:146747ms step_avg:88.35ms
step:1662/1680 train_time:146837ms step_avg:88.35ms
step:1663/1680 train_time:146926ms step_avg:88.35ms
step:1664/1680 train_time:147016ms step_avg:88.35ms
step:1665/1680 train_time:147106ms step_avg:88.35ms
step:1666/1680 train_time:147197ms step_avg:88.35ms
step:1667/1680 train_time:147285ms step_avg:88.35ms
step:1668/1680 train_time:147374ms step_avg:88.35ms
step:1669/1680 train_time:147462ms step_avg:88.35ms
step:1670/1680 train_time:147551ms step_avg:88.35ms
step:1671/1680 train_time:147640ms step_avg:88.35ms
step:1672/1680 train_time:147730ms step_avg:88.36ms
step:1673/1680 train_time:147820ms step_avg:88.36ms
step:1674/1680 train_time:147910ms step_avg:88.36ms
step:1675/1680 train_time:148000ms step_avg:88.36ms
step:1676/1680 train_time:148090ms step_avg:88.36ms
step:1677/1680 train_time:148178ms step_avg:88.36ms
step:1678/1680 train_time:148267ms step_avg:88.36ms
step:1679/1680 train_time:148356ms step_avg:88.36ms
step:1680/1680 train_time:148445ms step_avg:88.36ms
step:1680/1680 val_loss:3.2789 train_time:148536ms step_avg:88.41ms
peak memory allocated: 30760 MiB reserved: 45914 MiB

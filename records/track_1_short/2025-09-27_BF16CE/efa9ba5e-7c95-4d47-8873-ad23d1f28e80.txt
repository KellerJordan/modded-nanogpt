import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:17:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    154404      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    154405      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    154406      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    154407      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    154408      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    154409      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    154410      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    154411      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    154405      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    154406      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    154407      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    154408      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    154409      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    154410      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    154411      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:138ms step_avg:137.66ms
step:2/1680 train_time:158ms step_avg:78.93ms
step:3/1680 train_time:222ms step_avg:74.06ms
step:4/1680 train_time:307ms step_avg:76.82ms
step:5/1680 train_time:393ms step_avg:78.65ms
step:6/1680 train_time:479ms step_avg:79.84ms
step:7/1680 train_time:565ms step_avg:80.70ms
step:8/1680 train_time:651ms step_avg:81.42ms
step:9/1680 train_time:737ms step_avg:81.93ms
step:10/1680 train_time:823ms step_avg:82.34ms
step:11/1680 train_time:910ms step_avg:82.71ms
step:12/1680 train_time:997ms step_avg:83.12ms
step:13/1680 train_time:1088ms step_avg:83.71ms
step:14/1680 train_time:1178ms step_avg:84.16ms
step:15/1680 train_time:1266ms step_avg:84.42ms
step:16/1680 train_time:1354ms step_avg:84.62ms
step:17/1680 train_time:1442ms step_avg:84.80ms
step:18/1680 train_time:1529ms step_avg:84.94ms
step:19/1680 train_time:1615ms step_avg:85.02ms
step:20/1680 train_time:1702ms step_avg:85.11ms
step:21/1680 train_time:1789ms step_avg:85.19ms
step:22/1680 train_time:1876ms step_avg:85.26ms
step:23/1680 train_time:1963ms step_avg:85.33ms
step:24/1680 train_time:2052ms step_avg:85.51ms
step:25/1680 train_time:2141ms step_avg:85.64ms
step:26/1680 train_time:2230ms step_avg:85.77ms
step:27/1680 train_time:2317ms step_avg:85.83ms
step:28/1680 train_time:2405ms step_avg:85.88ms
step:29/1680 train_time:2492ms step_avg:85.93ms
step:30/1680 train_time:2579ms step_avg:85.98ms
step:31/1680 train_time:2666ms step_avg:86.00ms
step:32/1680 train_time:2753ms step_avg:86.02ms
step:33/1680 train_time:2839ms step_avg:86.04ms
step:34/1680 train_time:2926ms step_avg:86.07ms
step:35/1680 train_time:3014ms step_avg:86.11ms
step:36/1680 train_time:3102ms step_avg:86.16ms
step:37/1680 train_time:3190ms step_avg:86.22ms
step:38/1680 train_time:3278ms step_avg:86.26ms
step:39/1680 train_time:3365ms step_avg:86.29ms
step:40/1680 train_time:3453ms step_avg:86.33ms
step:41/1680 train_time:3540ms step_avg:86.35ms
step:42/1680 train_time:3628ms step_avg:86.39ms
step:43/1680 train_time:3715ms step_avg:86.40ms
step:44/1680 train_time:3802ms step_avg:86.41ms
step:45/1680 train_time:3889ms step_avg:86.42ms
step:46/1680 train_time:3976ms step_avg:86.43ms
step:47/1680 train_time:4063ms step_avg:86.46ms
step:48/1680 train_time:4152ms step_avg:86.50ms
step:49/1680 train_time:4240ms step_avg:86.54ms
step:50/1680 train_time:4329ms step_avg:86.58ms
step:51/1680 train_time:4419ms step_avg:86.64ms
step:52/1680 train_time:4506ms step_avg:86.65ms
step:53/1680 train_time:4593ms step_avg:86.66ms
step:54/1680 train_time:4680ms step_avg:86.67ms
step:55/1680 train_time:4768ms step_avg:86.68ms
step:56/1680 train_time:4854ms step_avg:86.68ms
step:57/1680 train_time:4942ms step_avg:86.69ms
step:58/1680 train_time:5029ms step_avg:86.71ms
step:59/1680 train_time:5116ms step_avg:86.71ms
step:60/1680 train_time:5204ms step_avg:86.73ms
step:61/1680 train_time:5291ms step_avg:86.75ms
step:62/1680 train_time:5379ms step_avg:86.76ms
step:63/1680 train_time:5467ms step_avg:86.77ms
step:64/1680 train_time:5553ms step_avg:86.77ms
step:65/1680 train_time:5641ms step_avg:86.78ms
step:66/1680 train_time:5728ms step_avg:86.79ms
step:67/1680 train_time:5816ms step_avg:86.80ms
step:68/1680 train_time:5903ms step_avg:86.80ms
step:69/1680 train_time:5989ms step_avg:86.80ms
step:70/1680 train_time:6077ms step_avg:86.81ms
step:71/1680 train_time:6164ms step_avg:86.82ms
step:72/1680 train_time:6252ms step_avg:86.83ms
step:73/1680 train_time:6340ms step_avg:86.84ms
step:74/1680 train_time:6427ms step_avg:86.86ms
step:75/1680 train_time:6515ms step_avg:86.87ms
step:76/1680 train_time:6602ms step_avg:86.87ms
step:77/1680 train_time:6689ms step_avg:86.88ms
step:78/1680 train_time:6777ms step_avg:86.89ms
step:79/1680 train_time:6864ms step_avg:86.88ms
step:80/1680 train_time:6951ms step_avg:86.89ms
step:81/1680 train_time:7038ms step_avg:86.89ms
step:82/1680 train_time:7125ms step_avg:86.89ms
step:83/1680 train_time:7212ms step_avg:86.89ms
step:84/1680 train_time:7300ms step_avg:86.91ms
step:85/1680 train_time:7388ms step_avg:86.91ms
step:86/1680 train_time:7474ms step_avg:86.91ms
step:87/1680 train_time:7562ms step_avg:86.92ms
step:88/1680 train_time:7650ms step_avg:86.93ms
step:89/1680 train_time:7738ms step_avg:86.94ms
step:90/1680 train_time:7825ms step_avg:86.94ms
step:91/1680 train_time:7912ms step_avg:86.94ms
step:92/1680 train_time:7999ms step_avg:86.94ms
step:93/1680 train_time:8086ms step_avg:86.94ms
step:94/1680 train_time:8173ms step_avg:86.94ms
step:95/1680 train_time:8260ms step_avg:86.95ms
step:96/1680 train_time:8347ms step_avg:86.95ms
step:97/1680 train_time:8434ms step_avg:86.95ms
step:98/1680 train_time:8521ms step_avg:86.95ms
step:99/1680 train_time:8609ms step_avg:86.96ms
step:100/1680 train_time:8696ms step_avg:86.96ms
step:101/1680 train_time:8783ms step_avg:86.96ms
step:102/1680 train_time:8871ms step_avg:86.97ms
step:103/1680 train_time:8958ms step_avg:86.97ms
step:104/1680 train_time:9045ms step_avg:86.97ms
step:105/1680 train_time:9133ms step_avg:86.98ms
step:106/1680 train_time:9220ms step_avg:86.98ms
step:107/1680 train_time:9307ms step_avg:86.98ms
step:108/1680 train_time:9396ms step_avg:87.00ms
step:109/1680 train_time:9483ms step_avg:87.00ms
step:110/1680 train_time:9570ms step_avg:87.00ms
step:111/1680 train_time:9657ms step_avg:87.00ms
step:112/1680 train_time:9745ms step_avg:87.01ms
step:113/1680 train_time:9833ms step_avg:87.01ms
step:114/1680 train_time:9920ms step_avg:87.01ms
step:115/1680 train_time:10006ms step_avg:87.01ms
step:116/1680 train_time:10094ms step_avg:87.02ms
step:117/1680 train_time:10181ms step_avg:87.01ms
step:118/1680 train_time:10268ms step_avg:87.02ms
step:119/1680 train_time:10354ms step_avg:87.01ms
step:120/1680 train_time:10442ms step_avg:87.01ms
step:121/1680 train_time:10529ms step_avg:87.02ms
step:122/1680 train_time:10616ms step_avg:87.01ms
step:123/1680 train_time:10703ms step_avg:87.02ms
step:124/1680 train_time:10791ms step_avg:87.02ms
step:125/1680 train_time:10878ms step_avg:87.02ms
step:125/1680 val_loss:4.2984 train_time:10967ms step_avg:87.74ms
step:126/1680 train_time:10986ms step_avg:87.19ms
step:127/1680 train_time:11059ms step_avg:87.08ms
step:128/1680 train_time:11155ms step_avg:87.15ms
step:129/1680 train_time:11248ms step_avg:87.19ms
step:130/1680 train_time:11335ms step_avg:87.19ms
step:131/1680 train_time:11421ms step_avg:87.19ms
step:132/1680 train_time:11507ms step_avg:87.18ms
step:133/1680 train_time:11593ms step_avg:87.17ms
step:134/1680 train_time:11679ms step_avg:87.16ms
step:135/1680 train_time:11766ms step_avg:87.15ms
step:136/1680 train_time:11852ms step_avg:87.15ms
step:137/1680 train_time:11938ms step_avg:87.14ms
step:138/1680 train_time:12025ms step_avg:87.14ms
step:139/1680 train_time:12115ms step_avg:87.16ms
step:140/1680 train_time:12204ms step_avg:87.17ms
step:141/1680 train_time:12295ms step_avg:87.20ms
step:142/1680 train_time:12382ms step_avg:87.20ms
step:143/1680 train_time:12469ms step_avg:87.20ms
step:144/1680 train_time:12556ms step_avg:87.20ms
step:145/1680 train_time:12643ms step_avg:87.19ms
step:146/1680 train_time:12729ms step_avg:87.18ms
step:147/1680 train_time:12815ms step_avg:87.18ms
step:148/1680 train_time:12901ms step_avg:87.17ms
step:149/1680 train_time:12988ms step_avg:87.17ms
step:150/1680 train_time:13076ms step_avg:87.17ms
step:151/1680 train_time:13165ms step_avg:87.18ms
step:152/1680 train_time:13255ms step_avg:87.20ms
step:153/1680 train_time:13342ms step_avg:87.20ms
step:154/1680 train_time:13429ms step_avg:87.20ms
step:155/1680 train_time:13516ms step_avg:87.20ms
step:156/1680 train_time:13603ms step_avg:87.20ms
step:157/1680 train_time:13690ms step_avg:87.19ms
step:158/1680 train_time:13777ms step_avg:87.19ms
step:159/1680 train_time:13863ms step_avg:87.19ms
step:160/1680 train_time:13950ms step_avg:87.19ms
step:161/1680 train_time:14037ms step_avg:87.19ms
step:162/1680 train_time:14124ms step_avg:87.19ms
step:163/1680 train_time:14213ms step_avg:87.19ms
step:164/1680 train_time:14301ms step_avg:87.20ms
step:165/1680 train_time:14388ms step_avg:87.20ms
step:166/1680 train_time:14476ms step_avg:87.20ms
step:167/1680 train_time:14562ms step_avg:87.20ms
step:168/1680 train_time:14649ms step_avg:87.19ms
step:169/1680 train_time:14736ms step_avg:87.19ms
step:170/1680 train_time:14823ms step_avg:87.19ms
step:171/1680 train_time:14910ms step_avg:87.19ms
step:172/1680 train_time:14997ms step_avg:87.19ms
step:173/1680 train_time:15084ms step_avg:87.19ms
step:174/1680 train_time:15172ms step_avg:87.19ms
step:175/1680 train_time:15260ms step_avg:87.20ms
step:176/1680 train_time:15347ms step_avg:87.20ms
step:177/1680 train_time:15435ms step_avg:87.20ms
step:178/1680 train_time:15522ms step_avg:87.20ms
step:179/1680 train_time:15609ms step_avg:87.20ms
step:180/1680 train_time:15696ms step_avg:87.20ms
step:181/1680 train_time:15783ms step_avg:87.20ms
step:182/1680 train_time:15871ms step_avg:87.20ms
step:183/1680 train_time:15958ms step_avg:87.20ms
step:184/1680 train_time:16045ms step_avg:87.20ms
step:185/1680 train_time:16132ms step_avg:87.20ms
step:186/1680 train_time:16219ms step_avg:87.20ms
step:187/1680 train_time:16307ms step_avg:87.20ms
step:188/1680 train_time:16394ms step_avg:87.20ms
step:189/1680 train_time:16481ms step_avg:87.20ms
step:190/1680 train_time:16569ms step_avg:87.20ms
step:191/1680 train_time:16656ms step_avg:87.21ms
step:192/1680 train_time:16743ms step_avg:87.20ms
step:193/1680 train_time:16830ms step_avg:87.20ms
step:194/1680 train_time:16917ms step_avg:87.20ms
step:195/1680 train_time:17004ms step_avg:87.20ms
step:196/1680 train_time:17091ms step_avg:87.20ms
step:197/1680 train_time:17179ms step_avg:87.20ms
step:198/1680 train_time:17267ms step_avg:87.20ms
step:199/1680 train_time:17354ms step_avg:87.21ms
step:200/1680 train_time:17441ms step_avg:87.20ms
step:201/1680 train_time:17528ms step_avg:87.20ms
step:202/1680 train_time:17615ms step_avg:87.20ms
step:203/1680 train_time:17702ms step_avg:87.20ms
step:204/1680 train_time:17789ms step_avg:87.20ms
step:205/1680 train_time:17876ms step_avg:87.20ms
step:206/1680 train_time:17963ms step_avg:87.20ms
step:207/1680 train_time:18050ms step_avg:87.20ms
step:208/1680 train_time:18137ms step_avg:87.20ms
step:209/1680 train_time:18225ms step_avg:87.20ms
step:210/1680 train_time:18313ms step_avg:87.20ms
step:211/1680 train_time:18400ms step_avg:87.21ms
step:212/1680 train_time:18488ms step_avg:87.21ms
step:213/1680 train_time:18576ms step_avg:87.21ms
step:214/1680 train_time:18662ms step_avg:87.21ms
step:215/1680 train_time:18749ms step_avg:87.20ms
step:216/1680 train_time:18836ms step_avg:87.20ms
step:217/1680 train_time:18924ms step_avg:87.21ms
step:218/1680 train_time:19011ms step_avg:87.21ms
step:219/1680 train_time:19098ms step_avg:87.21ms
step:220/1680 train_time:19185ms step_avg:87.20ms
step:221/1680 train_time:19272ms step_avg:87.21ms
step:222/1680 train_time:19359ms step_avg:87.20ms
step:223/1680 train_time:19446ms step_avg:87.20ms
step:224/1680 train_time:19534ms step_avg:87.20ms
step:225/1680 train_time:19621ms step_avg:87.20ms
step:226/1680 train_time:19708ms step_avg:87.20ms
step:227/1680 train_time:19794ms step_avg:87.20ms
step:228/1680 train_time:19881ms step_avg:87.20ms
step:229/1680 train_time:19968ms step_avg:87.20ms
step:230/1680 train_time:20056ms step_avg:87.20ms
step:231/1680 train_time:20143ms step_avg:87.20ms
step:232/1680 train_time:20231ms step_avg:87.20ms
step:233/1680 train_time:20318ms step_avg:87.20ms
step:234/1680 train_time:20405ms step_avg:87.20ms
step:235/1680 train_time:20492ms step_avg:87.20ms
step:236/1680 train_time:20579ms step_avg:87.20ms
step:237/1680 train_time:20666ms step_avg:87.20ms
step:238/1680 train_time:20754ms step_avg:87.20ms
step:239/1680 train_time:20841ms step_avg:87.20ms
step:240/1680 train_time:20928ms step_avg:87.20ms
step:241/1680 train_time:21015ms step_avg:87.20ms
step:242/1680 train_time:21102ms step_avg:87.20ms
step:243/1680 train_time:21189ms step_avg:87.20ms
step:244/1680 train_time:21276ms step_avg:87.20ms
step:245/1680 train_time:21363ms step_avg:87.20ms
step:246/1680 train_time:21451ms step_avg:87.20ms
step:247/1680 train_time:21538ms step_avg:87.20ms
step:248/1680 train_time:21625ms step_avg:87.20ms
step:249/1680 train_time:21712ms step_avg:87.20ms
step:250/1680 train_time:21799ms step_avg:87.20ms
step:250/1680 val_loss:3.9788 train_time:21888ms step_avg:87.55ms
step:251/1680 train_time:21908ms step_avg:87.28ms
step:252/1680 train_time:21979ms step_avg:87.22ms
step:253/1680 train_time:22071ms step_avg:87.24ms
step:254/1680 train_time:22160ms step_avg:87.24ms
step:255/1680 train_time:22247ms step_avg:87.24ms
step:256/1680 train_time:22334ms step_avg:87.24ms
step:257/1680 train_time:22420ms step_avg:87.24ms
step:258/1680 train_time:22506ms step_avg:87.23ms
step:259/1680 train_time:22592ms step_avg:87.23ms
step:260/1680 train_time:22679ms step_avg:87.23ms
step:261/1680 train_time:22765ms step_avg:87.22ms
step:262/1680 train_time:22852ms step_avg:87.22ms
step:263/1680 train_time:22941ms step_avg:87.23ms
step:264/1680 train_time:23032ms step_avg:87.24ms
step:265/1680 train_time:23121ms step_avg:87.25ms
step:266/1680 train_time:23209ms step_avg:87.25ms
step:267/1680 train_time:23295ms step_avg:87.25ms
step:268/1680 train_time:23382ms step_avg:87.25ms
step:269/1680 train_time:23468ms step_avg:87.24ms
step:270/1680 train_time:23555ms step_avg:87.24ms
step:271/1680 train_time:23641ms step_avg:87.24ms
step:272/1680 train_time:23728ms step_avg:87.24ms
step:273/1680 train_time:23815ms step_avg:87.23ms
step:274/1680 train_time:23902ms step_avg:87.24ms
step:275/1680 train_time:23990ms step_avg:87.24ms
step:276/1680 train_time:24079ms step_avg:87.24ms
step:277/1680 train_time:24168ms step_avg:87.25ms
step:278/1680 train_time:24254ms step_avg:87.25ms
step:279/1680 train_time:24341ms step_avg:87.25ms
step:280/1680 train_time:24428ms step_avg:87.24ms
step:281/1680 train_time:24515ms step_avg:87.24ms
step:282/1680 train_time:24602ms step_avg:87.24ms
step:283/1680 train_time:24688ms step_avg:87.24ms
step:284/1680 train_time:24775ms step_avg:87.24ms
step:285/1680 train_time:24863ms step_avg:87.24ms
step:286/1680 train_time:24950ms step_avg:87.24ms
step:287/1680 train_time:25038ms step_avg:87.24ms
step:288/1680 train_time:25126ms step_avg:87.24ms
step:289/1680 train_time:25214ms step_avg:87.25ms
step:290/1680 train_time:25302ms step_avg:87.25ms
step:291/1680 train_time:25389ms step_avg:87.25ms
step:292/1680 train_time:25476ms step_avg:87.25ms
step:293/1680 train_time:25562ms step_avg:87.24ms
step:294/1680 train_time:25648ms step_avg:87.24ms
step:295/1680 train_time:25735ms step_avg:87.24ms
step:296/1680 train_time:25822ms step_avg:87.24ms
step:297/1680 train_time:25909ms step_avg:87.24ms
step:298/1680 train_time:25996ms step_avg:87.24ms
step:299/1680 train_time:26084ms step_avg:87.24ms
step:300/1680 train_time:26171ms step_avg:87.24ms
step:301/1680 train_time:26259ms step_avg:87.24ms
step:302/1680 train_time:26346ms step_avg:87.24ms
step:303/1680 train_time:26434ms step_avg:87.24ms
step:304/1680 train_time:26521ms step_avg:87.24ms
step:305/1680 train_time:26608ms step_avg:87.24ms
step:306/1680 train_time:26695ms step_avg:87.24ms
step:307/1680 train_time:26782ms step_avg:87.24ms
step:308/1680 train_time:26868ms step_avg:87.23ms
step:309/1680 train_time:26955ms step_avg:87.23ms
step:310/1680 train_time:27043ms step_avg:87.24ms
step:311/1680 train_time:27130ms step_avg:87.24ms
step:312/1680 train_time:27217ms step_avg:87.23ms
step:313/1680 train_time:27305ms step_avg:87.24ms
step:314/1680 train_time:27392ms step_avg:87.24ms
step:315/1680 train_time:27480ms step_avg:87.24ms
step:316/1680 train_time:27567ms step_avg:87.24ms
step:317/1680 train_time:27654ms step_avg:87.24ms
step:318/1680 train_time:27741ms step_avg:87.23ms
step:319/1680 train_time:27828ms step_avg:87.23ms
step:320/1680 train_time:27915ms step_avg:87.23ms
step:321/1680 train_time:28003ms step_avg:87.24ms
step:322/1680 train_time:28090ms step_avg:87.24ms
step:323/1680 train_time:28178ms step_avg:87.24ms
step:324/1680 train_time:28265ms step_avg:87.24ms
step:325/1680 train_time:28352ms step_avg:87.24ms
step:326/1680 train_time:28440ms step_avg:87.24ms
step:327/1680 train_time:28527ms step_avg:87.24ms
step:328/1680 train_time:28614ms step_avg:87.24ms
step:329/1680 train_time:28701ms step_avg:87.24ms
step:330/1680 train_time:28789ms step_avg:87.24ms
step:331/1680 train_time:28876ms step_avg:87.24ms
step:332/1680 train_time:28964ms step_avg:87.24ms
step:333/1680 train_time:29051ms step_avg:87.24ms
step:334/1680 train_time:29138ms step_avg:87.24ms
step:335/1680 train_time:29225ms step_avg:87.24ms
step:336/1680 train_time:29312ms step_avg:87.24ms
step:337/1680 train_time:29400ms step_avg:87.24ms
step:338/1680 train_time:29487ms step_avg:87.24ms
step:339/1680 train_time:29574ms step_avg:87.24ms
step:340/1680 train_time:29661ms step_avg:87.24ms
step:341/1680 train_time:29748ms step_avg:87.24ms
step:342/1680 train_time:29835ms step_avg:87.24ms
step:343/1680 train_time:29922ms step_avg:87.24ms
step:344/1680 train_time:30010ms step_avg:87.24ms
step:345/1680 train_time:30097ms step_avg:87.24ms
step:346/1680 train_time:30184ms step_avg:87.24ms
step:347/1680 train_time:30271ms step_avg:87.24ms
step:348/1680 train_time:30359ms step_avg:87.24ms
step:349/1680 train_time:30446ms step_avg:87.24ms
step:350/1680 train_time:30534ms step_avg:87.24ms
step:351/1680 train_time:30621ms step_avg:87.24ms
step:352/1680 train_time:30708ms step_avg:87.24ms
step:353/1680 train_time:30795ms step_avg:87.24ms
step:354/1680 train_time:30882ms step_avg:87.24ms
step:355/1680 train_time:30970ms step_avg:87.24ms
step:356/1680 train_time:31057ms step_avg:87.24ms
step:357/1680 train_time:31144ms step_avg:87.24ms
step:358/1680 train_time:31231ms step_avg:87.24ms
step:359/1680 train_time:31318ms step_avg:87.24ms
step:360/1680 train_time:31405ms step_avg:87.24ms
step:361/1680 train_time:31494ms step_avg:87.24ms
step:362/1680 train_time:31580ms step_avg:87.24ms
step:363/1680 train_time:31667ms step_avg:87.24ms
step:364/1680 train_time:31754ms step_avg:87.24ms
step:365/1680 train_time:31841ms step_avg:87.24ms
step:366/1680 train_time:31929ms step_avg:87.24ms
step:367/1680 train_time:32016ms step_avg:87.24ms
step:368/1680 train_time:32104ms step_avg:87.24ms
step:369/1680 train_time:32191ms step_avg:87.24ms
step:370/1680 train_time:32278ms step_avg:87.24ms
step:371/1680 train_time:32365ms step_avg:87.24ms
step:372/1680 train_time:32452ms step_avg:87.24ms
step:373/1680 train_time:32540ms step_avg:87.24ms
step:374/1680 train_time:32627ms step_avg:87.24ms
step:375/1680 train_time:32714ms step_avg:87.24ms
step:375/1680 val_loss:3.8188 train_time:32802ms step_avg:87.47ms
step:376/1680 train_time:32821ms step_avg:87.29ms
step:377/1680 train_time:32892ms step_avg:87.25ms
step:378/1680 train_time:32982ms step_avg:87.25ms
step:379/1680 train_time:33070ms step_avg:87.25ms
step:380/1680 train_time:33156ms step_avg:87.25ms
step:381/1680 train_time:33244ms step_avg:87.25ms
step:382/1680 train_time:33330ms step_avg:87.25ms
step:383/1680 train_time:33416ms step_avg:87.25ms
step:384/1680 train_time:33503ms step_avg:87.25ms
step:385/1680 train_time:33589ms step_avg:87.24ms
step:386/1680 train_time:33675ms step_avg:87.24ms
step:387/1680 train_time:33763ms step_avg:87.24ms
step:388/1680 train_time:33852ms step_avg:87.25ms
step:389/1680 train_time:33941ms step_avg:87.25ms
step:390/1680 train_time:34029ms step_avg:87.26ms
step:391/1680 train_time:34117ms step_avg:87.26ms
step:392/1680 train_time:34204ms step_avg:87.26ms
step:393/1680 train_time:34291ms step_avg:87.25ms
step:394/1680 train_time:34377ms step_avg:87.25ms
step:395/1680 train_time:34464ms step_avg:87.25ms
step:396/1680 train_time:34550ms step_avg:87.25ms
step:397/1680 train_time:34637ms step_avg:87.25ms
step:398/1680 train_time:34724ms step_avg:87.25ms
step:399/1680 train_time:34812ms step_avg:87.25ms
step:400/1680 train_time:34901ms step_avg:87.25ms
step:401/1680 train_time:34988ms step_avg:87.25ms
step:402/1680 train_time:35076ms step_avg:87.25ms
step:403/1680 train_time:35163ms step_avg:87.25ms
step:404/1680 train_time:35250ms step_avg:87.25ms
step:405/1680 train_time:35337ms step_avg:87.25ms
step:406/1680 train_time:35424ms step_avg:87.25ms
step:407/1680 train_time:35511ms step_avg:87.25ms
step:408/1680 train_time:35597ms step_avg:87.25ms
step:409/1680 train_time:35684ms step_avg:87.25ms
step:410/1680 train_time:35771ms step_avg:87.25ms
step:411/1680 train_time:35858ms step_avg:87.25ms
step:412/1680 train_time:35947ms step_avg:87.25ms
step:413/1680 train_time:36034ms step_avg:87.25ms
step:414/1680 train_time:36122ms step_avg:87.25ms
step:415/1680 train_time:36210ms step_avg:87.25ms
step:416/1680 train_time:36297ms step_avg:87.25ms
step:417/1680 train_time:36384ms step_avg:87.25ms
step:418/1680 train_time:36471ms step_avg:87.25ms
step:419/1680 train_time:36558ms step_avg:87.25ms
step:420/1680 train_time:36645ms step_avg:87.25ms
step:421/1680 train_time:36732ms step_avg:87.25ms
step:422/1680 train_time:36819ms step_avg:87.25ms
step:423/1680 train_time:36907ms step_avg:87.25ms
step:424/1680 train_time:36995ms step_avg:87.25ms
step:425/1680 train_time:37082ms step_avg:87.25ms
step:426/1680 train_time:37170ms step_avg:87.25ms
step:427/1680 train_time:37257ms step_avg:87.25ms
step:428/1680 train_time:37344ms step_avg:87.25ms
step:429/1680 train_time:37431ms step_avg:87.25ms
step:430/1680 train_time:37518ms step_avg:87.25ms
step:431/1680 train_time:37605ms step_avg:87.25ms
step:432/1680 train_time:37693ms step_avg:87.25ms
step:433/1680 train_time:37780ms step_avg:87.25ms
step:434/1680 train_time:37867ms step_avg:87.25ms
step:435/1680 train_time:37954ms step_avg:87.25ms
step:436/1680 train_time:38042ms step_avg:87.25ms
step:437/1680 train_time:38130ms step_avg:87.25ms
step:438/1680 train_time:38217ms step_avg:87.25ms
step:439/1680 train_time:38304ms step_avg:87.25ms
step:440/1680 train_time:38392ms step_avg:87.25ms
step:441/1680 train_time:38479ms step_avg:87.25ms
step:442/1680 train_time:38566ms step_avg:87.25ms
step:443/1680 train_time:38653ms step_avg:87.25ms
step:444/1680 train_time:38740ms step_avg:87.25ms
step:445/1680 train_time:38827ms step_avg:87.25ms
step:446/1680 train_time:38914ms step_avg:87.25ms
step:447/1680 train_time:39002ms step_avg:87.25ms
step:448/1680 train_time:39090ms step_avg:87.25ms
step:449/1680 train_time:39177ms step_avg:87.25ms
step:450/1680 train_time:39265ms step_avg:87.26ms
step:451/1680 train_time:39352ms step_avg:87.26ms
step:452/1680 train_time:39440ms step_avg:87.26ms
step:453/1680 train_time:39527ms step_avg:87.26ms
step:454/1680 train_time:39614ms step_avg:87.26ms
step:455/1680 train_time:39701ms step_avg:87.26ms
step:456/1680 train_time:39789ms step_avg:87.26ms
step:457/1680 train_time:39876ms step_avg:87.26ms
step:458/1680 train_time:39963ms step_avg:87.26ms
step:459/1680 train_time:40050ms step_avg:87.26ms
step:460/1680 train_time:40138ms step_avg:87.26ms
step:461/1680 train_time:40225ms step_avg:87.26ms
step:462/1680 train_time:40312ms step_avg:87.26ms
step:463/1680 train_time:40399ms step_avg:87.25ms
step:464/1680 train_time:40486ms step_avg:87.26ms
step:465/1680 train_time:40574ms step_avg:87.26ms
step:466/1680 train_time:40661ms step_avg:87.26ms
step:467/1680 train_time:40748ms step_avg:87.26ms
step:468/1680 train_time:40835ms step_avg:87.26ms
step:469/1680 train_time:40923ms step_avg:87.26ms
step:470/1680 train_time:41010ms step_avg:87.26ms
step:471/1680 train_time:41097ms step_avg:87.26ms
step:472/1680 train_time:41186ms step_avg:87.26ms
step:473/1680 train_time:41273ms step_avg:87.26ms
step:474/1680 train_time:41360ms step_avg:87.26ms
step:475/1680 train_time:41448ms step_avg:87.26ms
step:476/1680 train_time:41535ms step_avg:87.26ms
step:477/1680 train_time:41622ms step_avg:87.26ms
step:478/1680 train_time:41710ms step_avg:87.26ms
step:479/1680 train_time:41796ms step_avg:87.26ms
step:480/1680 train_time:41884ms step_avg:87.26ms
step:481/1680 train_time:41971ms step_avg:87.26ms
step:482/1680 train_time:42059ms step_avg:87.26ms
step:483/1680 train_time:42146ms step_avg:87.26ms
step:484/1680 train_time:42234ms step_avg:87.26ms
step:485/1680 train_time:42321ms step_avg:87.26ms
step:486/1680 train_time:42408ms step_avg:87.26ms
step:487/1680 train_time:42495ms step_avg:87.26ms
step:488/1680 train_time:42582ms step_avg:87.26ms
step:489/1680 train_time:42670ms step_avg:87.26ms
step:490/1680 train_time:42757ms step_avg:87.26ms
step:491/1680 train_time:42844ms step_avg:87.26ms
step:492/1680 train_time:42932ms step_avg:87.26ms
step:493/1680 train_time:43019ms step_avg:87.26ms
step:494/1680 train_time:43107ms step_avg:87.26ms
step:495/1680 train_time:43194ms step_avg:87.26ms
step:496/1680 train_time:43281ms step_avg:87.26ms
step:497/1680 train_time:43368ms step_avg:87.26ms
step:498/1680 train_time:43455ms step_avg:87.26ms
step:499/1680 train_time:43542ms step_avg:87.26ms
step:500/1680 train_time:43630ms step_avg:87.26ms
step:500/1680 val_loss:3.7174 train_time:43719ms step_avg:87.44ms
step:501/1680 train_time:43737ms step_avg:87.30ms
step:502/1680 train_time:43809ms step_avg:87.27ms
step:503/1680 train_time:43901ms step_avg:87.28ms
step:504/1680 train_time:43990ms step_avg:87.28ms
step:505/1680 train_time:44077ms step_avg:87.28ms
step:506/1680 train_time:44164ms step_avg:87.28ms
step:507/1680 train_time:44251ms step_avg:87.28ms
step:508/1680 train_time:44337ms step_avg:87.28ms
step:509/1680 train_time:44423ms step_avg:87.28ms
step:510/1680 train_time:44509ms step_avg:87.27ms
step:511/1680 train_time:44596ms step_avg:87.27ms
step:512/1680 train_time:44682ms step_avg:87.27ms
step:513/1680 train_time:44771ms step_avg:87.27ms
step:514/1680 train_time:44861ms step_avg:87.28ms
step:515/1680 train_time:44950ms step_avg:87.28ms
step:516/1680 train_time:45037ms step_avg:87.28ms
step:517/1680 train_time:45124ms step_avg:87.28ms
step:518/1680 train_time:45211ms step_avg:87.28ms
step:519/1680 train_time:45298ms step_avg:87.28ms
step:520/1680 train_time:45385ms step_avg:87.28ms
step:521/1680 train_time:45472ms step_avg:87.28ms
step:522/1680 train_time:45559ms step_avg:87.28ms
step:523/1680 train_time:45645ms step_avg:87.28ms
step:524/1680 train_time:45733ms step_avg:87.28ms
step:525/1680 train_time:45821ms step_avg:87.28ms
step:526/1680 train_time:45909ms step_avg:87.28ms
step:527/1680 train_time:45996ms step_avg:87.28ms
step:528/1680 train_time:46084ms step_avg:87.28ms
step:529/1680 train_time:46171ms step_avg:87.28ms
step:530/1680 train_time:46259ms step_avg:87.28ms
step:531/1680 train_time:46345ms step_avg:87.28ms
step:532/1680 train_time:46432ms step_avg:87.28ms
step:533/1680 train_time:46518ms step_avg:87.28ms
step:534/1680 train_time:46606ms step_avg:87.28ms
step:535/1680 train_time:46693ms step_avg:87.28ms
step:536/1680 train_time:46780ms step_avg:87.28ms
step:537/1680 train_time:46868ms step_avg:87.28ms
step:538/1680 train_time:46956ms step_avg:87.28ms
step:539/1680 train_time:47043ms step_avg:87.28ms
step:540/1680 train_time:47131ms step_avg:87.28ms
step:541/1680 train_time:47218ms step_avg:87.28ms
step:542/1680 train_time:47305ms step_avg:87.28ms
step:543/1680 train_time:47392ms step_avg:87.28ms
step:544/1680 train_time:47479ms step_avg:87.28ms
step:545/1680 train_time:47566ms step_avg:87.28ms
step:546/1680 train_time:47654ms step_avg:87.28ms
step:547/1680 train_time:47742ms step_avg:87.28ms
step:548/1680 train_time:47830ms step_avg:87.28ms
step:549/1680 train_time:47918ms step_avg:87.28ms
step:550/1680 train_time:48007ms step_avg:87.29ms
step:551/1680 train_time:48096ms step_avg:87.29ms
step:552/1680 train_time:48184ms step_avg:87.29ms
step:553/1680 train_time:48273ms step_avg:87.29ms
step:554/1680 train_time:48362ms step_avg:87.30ms
step:555/1680 train_time:48449ms step_avg:87.30ms
step:556/1680 train_time:48537ms step_avg:87.30ms
step:557/1680 train_time:48626ms step_avg:87.30ms
step:558/1680 train_time:48714ms step_avg:87.30ms
step:559/1680 train_time:48803ms step_avg:87.30ms
step:560/1680 train_time:48892ms step_avg:87.31ms
step:561/1680 train_time:48981ms step_avg:87.31ms
step:562/1680 train_time:49070ms step_avg:87.31ms
step:563/1680 train_time:49158ms step_avg:87.31ms
step:564/1680 train_time:49247ms step_avg:87.32ms
step:565/1680 train_time:49335ms step_avg:87.32ms
step:566/1680 train_time:49423ms step_avg:87.32ms
step:567/1680 train_time:49511ms step_avg:87.32ms
step:568/1680 train_time:49599ms step_avg:87.32ms
step:569/1680 train_time:49688ms step_avg:87.33ms
step:570/1680 train_time:49776ms step_avg:87.33ms
step:571/1680 train_time:49865ms step_avg:87.33ms
step:572/1680 train_time:49954ms step_avg:87.33ms
step:573/1680 train_time:50043ms step_avg:87.33ms
step:574/1680 train_time:50131ms step_avg:87.34ms
step:575/1680 train_time:50219ms step_avg:87.34ms
step:576/1680 train_time:50308ms step_avg:87.34ms
step:577/1680 train_time:50396ms step_avg:87.34ms
step:578/1680 train_time:50484ms step_avg:87.34ms
step:579/1680 train_time:50572ms step_avg:87.34ms
step:580/1680 train_time:50660ms step_avg:87.35ms
step:581/1680 train_time:50748ms step_avg:87.35ms
step:582/1680 train_time:50836ms step_avg:87.35ms
step:583/1680 train_time:50925ms step_avg:87.35ms
step:584/1680 train_time:51014ms step_avg:87.35ms
step:585/1680 train_time:51103ms step_avg:87.35ms
step:586/1680 train_time:51192ms step_avg:87.36ms
step:587/1680 train_time:51280ms step_avg:87.36ms
step:588/1680 train_time:51369ms step_avg:87.36ms
step:589/1680 train_time:51457ms step_avg:87.36ms
step:590/1680 train_time:51545ms step_avg:87.36ms
step:591/1680 train_time:51634ms step_avg:87.37ms
step:592/1680 train_time:51722ms step_avg:87.37ms
step:593/1680 train_time:51810ms step_avg:87.37ms
step:594/1680 train_time:51898ms step_avg:87.37ms
step:595/1680 train_time:51986ms step_avg:87.37ms
step:596/1680 train_time:52074ms step_avg:87.37ms
step:597/1680 train_time:52163ms step_avg:87.38ms
step:598/1680 train_time:52252ms step_avg:87.38ms
step:599/1680 train_time:52341ms step_avg:87.38ms
step:600/1680 train_time:52430ms step_avg:87.38ms
step:601/1680 train_time:52518ms step_avg:87.38ms
step:602/1680 train_time:52607ms step_avg:87.39ms
step:603/1680 train_time:52695ms step_avg:87.39ms
step:604/1680 train_time:52784ms step_avg:87.39ms
step:605/1680 train_time:52873ms step_avg:87.39ms
step:606/1680 train_time:52961ms step_avg:87.39ms
step:607/1680 train_time:53050ms step_avg:87.40ms
step:608/1680 train_time:53138ms step_avg:87.40ms
step:609/1680 train_time:53227ms step_avg:87.40ms
step:610/1680 train_time:53315ms step_avg:87.40ms
step:611/1680 train_time:53403ms step_avg:87.40ms
step:612/1680 train_time:53493ms step_avg:87.41ms
step:613/1680 train_time:53581ms step_avg:87.41ms
step:614/1680 train_time:53669ms step_avg:87.41ms
step:615/1680 train_time:53757ms step_avg:87.41ms
step:616/1680 train_time:53846ms step_avg:87.41ms
step:617/1680 train_time:53935ms step_avg:87.41ms
step:618/1680 train_time:54023ms step_avg:87.42ms
step:619/1680 train_time:54112ms step_avg:87.42ms
step:620/1680 train_time:54200ms step_avg:87.42ms
step:621/1680 train_time:54288ms step_avg:87.42ms
step:622/1680 train_time:54377ms step_avg:87.42ms
step:623/1680 train_time:54465ms step_avg:87.42ms
step:624/1680 train_time:54554ms step_avg:87.43ms
step:625/1680 train_time:54642ms step_avg:87.43ms
step:625/1680 val_loss:3.6182 train_time:54732ms step_avg:87.57ms
step:626/1680 train_time:54759ms step_avg:87.47ms
step:627/1680 train_time:54820ms step_avg:87.43ms
step:628/1680 train_time:54909ms step_avg:87.44ms
step:629/1680 train_time:55000ms step_avg:87.44ms
step:630/1680 train_time:55089ms step_avg:87.44ms
step:631/1680 train_time:55176ms step_avg:87.44ms
step:632/1680 train_time:55263ms step_avg:87.44ms
step:633/1680 train_time:55350ms step_avg:87.44ms
step:634/1680 train_time:55437ms step_avg:87.44ms
step:635/1680 train_time:55524ms step_avg:87.44ms
step:636/1680 train_time:55612ms step_avg:87.44ms
step:637/1680 train_time:55706ms step_avg:87.45ms
step:638/1680 train_time:55796ms step_avg:87.45ms
step:639/1680 train_time:55885ms step_avg:87.46ms
step:640/1680 train_time:55973ms step_avg:87.46ms
step:641/1680 train_time:56061ms step_avg:87.46ms
step:642/1680 train_time:56149ms step_avg:87.46ms
step:643/1680 train_time:56237ms step_avg:87.46ms
step:644/1680 train_time:56325ms step_avg:87.46ms
step:645/1680 train_time:56412ms step_avg:87.46ms
step:646/1680 train_time:56500ms step_avg:87.46ms
step:647/1680 train_time:56589ms step_avg:87.46ms
step:648/1680 train_time:56679ms step_avg:87.47ms
step:649/1680 train_time:56767ms step_avg:87.47ms
step:650/1680 train_time:56856ms step_avg:87.47ms
step:651/1680 train_time:56945ms step_avg:87.47ms
step:652/1680 train_time:57033ms step_avg:87.47ms
step:653/1680 train_time:57121ms step_avg:87.47ms
step:654/1680 train_time:57209ms step_avg:87.48ms
step:655/1680 train_time:57297ms step_avg:87.48ms
step:656/1680 train_time:57384ms step_avg:87.48ms
step:657/1680 train_time:57472ms step_avg:87.48ms
step:658/1680 train_time:57561ms step_avg:87.48ms
step:659/1680 train_time:57649ms step_avg:87.48ms
step:660/1680 train_time:57738ms step_avg:87.48ms
step:661/1680 train_time:57826ms step_avg:87.48ms
step:662/1680 train_time:57915ms step_avg:87.48ms
step:663/1680 train_time:58004ms step_avg:87.49ms
step:664/1680 train_time:58092ms step_avg:87.49ms
step:665/1680 train_time:58180ms step_avg:87.49ms
step:666/1680 train_time:58268ms step_avg:87.49ms
step:667/1680 train_time:58355ms step_avg:87.49ms
step:668/1680 train_time:58444ms step_avg:87.49ms
step:669/1680 train_time:58532ms step_avg:87.49ms
step:670/1680 train_time:58620ms step_avg:87.49ms
step:671/1680 train_time:58709ms step_avg:87.49ms
step:672/1680 train_time:58797ms step_avg:87.50ms
step:673/1680 train_time:58887ms step_avg:87.50ms
step:674/1680 train_time:58976ms step_avg:87.50ms
step:675/1680 train_time:59064ms step_avg:87.50ms
step:676/1680 train_time:59152ms step_avg:87.50ms
step:677/1680 train_time:59241ms step_avg:87.50ms
step:678/1680 train_time:59329ms step_avg:87.51ms
step:679/1680 train_time:59417ms step_avg:87.51ms
step:680/1680 train_time:59506ms step_avg:87.51ms
step:681/1680 train_time:59594ms step_avg:87.51ms
step:682/1680 train_time:59682ms step_avg:87.51ms
step:683/1680 train_time:59770ms step_avg:87.51ms
step:684/1680 train_time:59860ms step_avg:87.51ms
step:685/1680 train_time:59949ms step_avg:87.52ms
step:686/1680 train_time:60038ms step_avg:87.52ms
step:687/1680 train_time:60126ms step_avg:87.52ms
step:688/1680 train_time:60213ms step_avg:87.52ms
step:689/1680 train_time:60302ms step_avg:87.52ms
step:690/1680 train_time:60390ms step_avg:87.52ms
step:691/1680 train_time:60478ms step_avg:87.52ms
step:692/1680 train_time:60567ms step_avg:87.52ms
step:693/1680 train_time:60656ms step_avg:87.53ms
step:694/1680 train_time:60743ms step_avg:87.53ms
step:695/1680 train_time:60832ms step_avg:87.53ms
step:696/1680 train_time:60920ms step_avg:87.53ms
step:697/1680 train_time:61009ms step_avg:87.53ms
step:698/1680 train_time:61097ms step_avg:87.53ms
step:699/1680 train_time:61185ms step_avg:87.53ms
step:700/1680 train_time:61274ms step_avg:87.53ms
step:701/1680 train_time:61362ms step_avg:87.53ms
step:702/1680 train_time:61450ms step_avg:87.54ms
step:703/1680 train_time:61538ms step_avg:87.54ms
step:704/1680 train_time:61626ms step_avg:87.54ms
step:705/1680 train_time:61714ms step_avg:87.54ms
step:706/1680 train_time:61803ms step_avg:87.54ms
step:707/1680 train_time:61891ms step_avg:87.54ms
step:708/1680 train_time:61980ms step_avg:87.54ms
step:709/1680 train_time:62068ms step_avg:87.54ms
step:710/1680 train_time:62157ms step_avg:87.55ms
step:711/1680 train_time:62246ms step_avg:87.55ms
step:712/1680 train_time:62334ms step_avg:87.55ms
step:713/1680 train_time:62422ms step_avg:87.55ms
step:714/1680 train_time:62510ms step_avg:87.55ms
step:715/1680 train_time:62599ms step_avg:87.55ms
step:716/1680 train_time:62687ms step_avg:87.55ms
step:717/1680 train_time:62776ms step_avg:87.55ms
step:718/1680 train_time:62865ms step_avg:87.56ms
step:719/1680 train_time:62952ms step_avg:87.56ms
step:720/1680 train_time:63041ms step_avg:87.56ms
step:721/1680 train_time:63130ms step_avg:87.56ms
step:722/1680 train_time:63218ms step_avg:87.56ms
step:723/1680 train_time:63306ms step_avg:87.56ms
step:724/1680 train_time:63394ms step_avg:87.56ms
step:725/1680 train_time:63482ms step_avg:87.56ms
step:726/1680 train_time:63570ms step_avg:87.56ms
step:727/1680 train_time:63659ms step_avg:87.56ms
step:728/1680 train_time:63748ms step_avg:87.57ms
step:729/1680 train_time:63837ms step_avg:87.57ms
step:730/1680 train_time:63925ms step_avg:87.57ms
step:731/1680 train_time:64013ms step_avg:87.57ms
step:732/1680 train_time:64102ms step_avg:87.57ms
step:733/1680 train_time:64190ms step_avg:87.57ms
step:734/1680 train_time:64278ms step_avg:87.57ms
step:735/1680 train_time:64366ms step_avg:87.57ms
step:736/1680 train_time:64454ms step_avg:87.57ms
step:737/1680 train_time:64542ms step_avg:87.57ms
step:738/1680 train_time:64630ms step_avg:87.57ms
step:739/1680 train_time:64718ms step_avg:87.58ms
step:740/1680 train_time:64807ms step_avg:87.58ms
step:741/1680 train_time:64895ms step_avg:87.58ms
step:742/1680 train_time:64984ms step_avg:87.58ms
step:743/1680 train_time:65072ms step_avg:87.58ms
step:744/1680 train_time:65161ms step_avg:87.58ms
step:745/1680 train_time:65249ms step_avg:87.58ms
step:746/1680 train_time:65338ms step_avg:87.58ms
step:747/1680 train_time:65427ms step_avg:87.59ms
step:748/1680 train_time:65515ms step_avg:87.59ms
step:749/1680 train_time:65604ms step_avg:87.59ms
step:750/1680 train_time:65692ms step_avg:87.59ms
step:750/1680 val_loss:3.5644 train_time:65782ms step_avg:87.71ms
step:751/1680 train_time:65805ms step_avg:87.62ms
step:752/1680 train_time:65872ms step_avg:87.60ms
step:753/1680 train_time:65964ms step_avg:87.60ms
step:754/1680 train_time:66054ms step_avg:87.61ms
step:755/1680 train_time:66143ms step_avg:87.61ms
step:756/1680 train_time:66230ms step_avg:87.61ms
step:757/1680 train_time:66318ms step_avg:87.61ms
step:758/1680 train_time:66405ms step_avg:87.61ms
step:759/1680 train_time:66492ms step_avg:87.60ms
step:760/1680 train_time:66580ms step_avg:87.60ms
step:761/1680 train_time:66667ms step_avg:87.60ms
step:762/1680 train_time:66756ms step_avg:87.61ms
step:763/1680 train_time:66846ms step_avg:87.61ms
step:764/1680 train_time:66937ms step_avg:87.61ms
step:765/1680 train_time:67028ms step_avg:87.62ms
step:766/1680 train_time:67117ms step_avg:87.62ms
step:767/1680 train_time:67206ms step_avg:87.62ms
step:768/1680 train_time:67294ms step_avg:87.62ms
step:769/1680 train_time:67381ms step_avg:87.62ms
step:770/1680 train_time:67469ms step_avg:87.62ms
step:771/1680 train_time:67556ms step_avg:87.62ms
step:772/1680 train_time:67643ms step_avg:87.62ms
step:773/1680 train_time:67731ms step_avg:87.62ms
step:774/1680 train_time:67821ms step_avg:87.62ms
step:775/1680 train_time:67910ms step_avg:87.63ms
step:776/1680 train_time:68000ms step_avg:87.63ms
step:777/1680 train_time:68088ms step_avg:87.63ms
step:778/1680 train_time:68177ms step_avg:87.63ms
step:779/1680 train_time:68264ms step_avg:87.63ms
step:780/1680 train_time:68352ms step_avg:87.63ms
step:781/1680 train_time:68440ms step_avg:87.63ms
step:782/1680 train_time:68527ms step_avg:87.63ms
step:783/1680 train_time:68616ms step_avg:87.63ms
step:784/1680 train_time:68704ms step_avg:87.63ms
step:785/1680 train_time:68792ms step_avg:87.63ms
step:786/1680 train_time:68881ms step_avg:87.64ms
step:787/1680 train_time:68970ms step_avg:87.64ms
step:788/1680 train_time:69059ms step_avg:87.64ms
step:789/1680 train_time:69147ms step_avg:87.64ms
step:790/1680 train_time:69236ms step_avg:87.64ms
step:791/1680 train_time:69324ms step_avg:87.64ms
step:792/1680 train_time:69412ms step_avg:87.64ms
step:793/1680 train_time:69501ms step_avg:87.64ms
step:794/1680 train_time:69588ms step_avg:87.64ms
step:795/1680 train_time:69676ms step_avg:87.64ms
step:796/1680 train_time:69765ms step_avg:87.64ms
step:797/1680 train_time:69854ms step_avg:87.65ms
step:798/1680 train_time:69943ms step_avg:87.65ms
step:799/1680 train_time:70031ms step_avg:87.65ms
step:800/1680 train_time:70120ms step_avg:87.65ms
step:801/1680 train_time:70208ms step_avg:87.65ms
step:802/1680 train_time:70297ms step_avg:87.65ms
step:803/1680 train_time:70385ms step_avg:87.65ms
step:804/1680 train_time:70473ms step_avg:87.65ms
step:805/1680 train_time:70562ms step_avg:87.65ms
step:806/1680 train_time:70650ms step_avg:87.65ms
step:807/1680 train_time:70739ms step_avg:87.66ms
step:808/1680 train_time:70827ms step_avg:87.66ms
step:809/1680 train_time:70915ms step_avg:87.66ms
step:810/1680 train_time:71004ms step_avg:87.66ms
step:811/1680 train_time:71093ms step_avg:87.66ms
step:812/1680 train_time:71181ms step_avg:87.66ms
step:813/1680 train_time:71270ms step_avg:87.66ms
step:814/1680 train_time:71358ms step_avg:87.66ms
step:815/1680 train_time:71446ms step_avg:87.66ms
step:816/1680 train_time:71535ms step_avg:87.66ms
step:817/1680 train_time:71623ms step_avg:87.67ms
step:818/1680 train_time:71711ms step_avg:87.67ms
step:819/1680 train_time:71800ms step_avg:87.67ms
step:820/1680 train_time:71888ms step_avg:87.67ms
step:821/1680 train_time:71977ms step_avg:87.67ms
step:822/1680 train_time:72066ms step_avg:87.67ms
step:823/1680 train_time:72155ms step_avg:87.67ms
step:824/1680 train_time:72244ms step_avg:87.67ms
step:825/1680 train_time:72332ms step_avg:87.68ms
step:826/1680 train_time:72420ms step_avg:87.68ms
step:827/1680 train_time:72508ms step_avg:87.68ms
step:828/1680 train_time:72596ms step_avg:87.68ms
step:829/1680 train_time:72684ms step_avg:87.68ms
step:830/1680 train_time:72772ms step_avg:87.68ms
step:831/1680 train_time:72860ms step_avg:87.68ms
step:832/1680 train_time:72948ms step_avg:87.68ms
step:833/1680 train_time:73037ms step_avg:87.68ms
step:834/1680 train_time:73126ms step_avg:87.68ms
step:835/1680 train_time:73214ms step_avg:87.68ms
step:836/1680 train_time:73304ms step_avg:87.68ms
step:837/1680 train_time:73392ms step_avg:87.68ms
step:838/1680 train_time:73480ms step_avg:87.69ms
step:839/1680 train_time:73568ms step_avg:87.69ms
step:840/1680 train_time:73656ms step_avg:87.69ms
step:841/1680 train_time:73745ms step_avg:87.69ms
step:842/1680 train_time:73833ms step_avg:87.69ms
step:843/1680 train_time:73921ms step_avg:87.69ms
step:844/1680 train_time:74009ms step_avg:87.69ms
step:845/1680 train_time:74098ms step_avg:87.69ms
step:846/1680 train_time:74186ms step_avg:87.69ms
step:847/1680 train_time:74274ms step_avg:87.69ms
step:848/1680 train_time:74364ms step_avg:87.69ms
step:849/1680 train_time:74452ms step_avg:87.69ms
step:850/1680 train_time:74541ms step_avg:87.70ms
step:851/1680 train_time:74628ms step_avg:87.69ms
step:852/1680 train_time:74716ms step_avg:87.70ms
step:853/1680 train_time:74805ms step_avg:87.70ms
step:854/1680 train_time:74894ms step_avg:87.70ms
step:855/1680 train_time:74982ms step_avg:87.70ms
step:856/1680 train_time:75070ms step_avg:87.70ms
step:857/1680 train_time:75158ms step_avg:87.70ms
step:858/1680 train_time:75246ms step_avg:87.70ms
step:859/1680 train_time:75335ms step_avg:87.70ms
step:860/1680 train_time:75423ms step_avg:87.70ms
step:861/1680 train_time:75511ms step_avg:87.70ms
step:862/1680 train_time:75600ms step_avg:87.70ms
step:863/1680 train_time:75688ms step_avg:87.70ms
step:864/1680 train_time:75776ms step_avg:87.70ms
step:865/1680 train_time:75864ms step_avg:87.70ms
step:866/1680 train_time:75952ms step_avg:87.70ms
step:867/1680 train_time:76041ms step_avg:87.71ms
step:868/1680 train_time:76129ms step_avg:87.71ms
step:869/1680 train_time:76217ms step_avg:87.71ms
step:870/1680 train_time:76306ms step_avg:87.71ms
step:871/1680 train_time:76395ms step_avg:87.71ms
step:872/1680 train_time:76483ms step_avg:87.71ms
step:873/1680 train_time:76572ms step_avg:87.71ms
step:874/1680 train_time:76660ms step_avg:87.71ms
step:875/1680 train_time:76748ms step_avg:87.71ms
step:875/1680 val_loss:3.5182 train_time:76838ms step_avg:87.81ms
step:876/1680 train_time:76856ms step_avg:87.74ms
step:877/1680 train_time:76931ms step_avg:87.72ms
step:878/1680 train_time:77022ms step_avg:87.72ms
step:879/1680 train_time:77113ms step_avg:87.73ms
step:880/1680 train_time:77201ms step_avg:87.73ms
step:881/1680 train_time:77289ms step_avg:87.73ms
step:882/1680 train_time:77375ms step_avg:87.73ms
step:883/1680 train_time:77463ms step_avg:87.73ms
step:884/1680 train_time:77551ms step_avg:87.73ms
step:885/1680 train_time:77639ms step_avg:87.73ms
step:886/1680 train_time:77727ms step_avg:87.73ms
step:887/1680 train_time:77817ms step_avg:87.73ms
step:888/1680 train_time:77906ms step_avg:87.73ms
step:889/1680 train_time:77997ms step_avg:87.74ms
step:890/1680 train_time:78087ms step_avg:87.74ms
step:891/1680 train_time:78177ms step_avg:87.74ms
step:892/1680 train_time:78264ms step_avg:87.74ms
step:893/1680 train_time:78352ms step_avg:87.74ms
step:894/1680 train_time:78439ms step_avg:87.74ms
step:895/1680 train_time:78527ms step_avg:87.74ms
step:896/1680 train_time:78614ms step_avg:87.74ms
step:897/1680 train_time:78702ms step_avg:87.74ms
step:898/1680 train_time:78791ms step_avg:87.74ms
step:899/1680 train_time:78879ms step_avg:87.74ms
step:900/1680 train_time:78970ms step_avg:87.74ms
step:901/1680 train_time:79060ms step_avg:87.75ms
step:902/1680 train_time:79149ms step_avg:87.75ms
step:903/1680 train_time:79238ms step_avg:87.75ms
step:904/1680 train_time:79326ms step_avg:87.75ms
step:905/1680 train_time:79414ms step_avg:87.75ms
step:906/1680 train_time:79501ms step_avg:87.75ms
step:907/1680 train_time:79590ms step_avg:87.75ms
step:908/1680 train_time:79678ms step_avg:87.75ms
step:909/1680 train_time:79766ms step_avg:87.75ms
step:910/1680 train_time:79854ms step_avg:87.75ms
step:911/1680 train_time:79943ms step_avg:87.75ms
step:912/1680 train_time:80032ms step_avg:87.75ms
step:913/1680 train_time:80121ms step_avg:87.76ms
step:914/1680 train_time:80209ms step_avg:87.76ms
step:915/1680 train_time:80299ms step_avg:87.76ms
step:916/1680 train_time:80387ms step_avg:87.76ms
step:917/1680 train_time:80475ms step_avg:87.76ms
step:918/1680 train_time:80563ms step_avg:87.76ms
step:919/1680 train_time:80652ms step_avg:87.76ms
step:920/1680 train_time:80740ms step_avg:87.76ms
step:921/1680 train_time:80828ms step_avg:87.76ms
step:922/1680 train_time:80917ms step_avg:87.76ms
step:923/1680 train_time:81005ms step_avg:87.76ms
step:924/1680 train_time:81094ms step_avg:87.76ms
step:925/1680 train_time:81182ms step_avg:87.76ms
step:926/1680 train_time:81272ms step_avg:87.77ms
step:927/1680 train_time:81361ms step_avg:87.77ms
step:928/1680 train_time:81449ms step_avg:87.77ms
step:929/1680 train_time:81537ms step_avg:87.77ms
step:930/1680 train_time:81626ms step_avg:87.77ms
step:931/1680 train_time:81715ms step_avg:87.77ms
step:932/1680 train_time:81803ms step_avg:87.77ms
step:933/1680 train_time:81893ms step_avg:87.77ms
step:934/1680 train_time:81982ms step_avg:87.77ms
step:935/1680 train_time:82070ms step_avg:87.78ms
step:936/1680 train_time:82160ms step_avg:87.78ms
step:937/1680 train_time:82248ms step_avg:87.78ms
step:938/1680 train_time:82336ms step_avg:87.78ms
step:939/1680 train_time:82425ms step_avg:87.78ms
step:940/1680 train_time:82513ms step_avg:87.78ms
step:941/1680 train_time:82601ms step_avg:87.78ms
step:942/1680 train_time:82689ms step_avg:87.78ms
step:943/1680 train_time:82778ms step_avg:87.78ms
step:944/1680 train_time:82866ms step_avg:87.78ms
step:945/1680 train_time:82956ms step_avg:87.78ms
step:946/1680 train_time:83044ms step_avg:87.78ms
step:947/1680 train_time:83133ms step_avg:87.79ms
step:948/1680 train_time:83221ms step_avg:87.79ms
step:949/1680 train_time:83311ms step_avg:87.79ms
step:950/1680 train_time:83399ms step_avg:87.79ms
step:951/1680 train_time:83488ms step_avg:87.79ms
step:952/1680 train_time:83577ms step_avg:87.79ms
step:953/1680 train_time:83665ms step_avg:87.79ms
step:954/1680 train_time:83754ms step_avg:87.79ms
step:955/1680 train_time:83842ms step_avg:87.79ms
step:956/1680 train_time:83931ms step_avg:87.79ms
step:957/1680 train_time:84020ms step_avg:87.79ms
step:958/1680 train_time:84108ms step_avg:87.80ms
step:959/1680 train_time:84197ms step_avg:87.80ms
step:960/1680 train_time:84286ms step_avg:87.80ms
step:961/1680 train_time:84374ms step_avg:87.80ms
step:962/1680 train_time:84462ms step_avg:87.80ms
step:963/1680 train_time:84551ms step_avg:87.80ms
step:964/1680 train_time:84639ms step_avg:87.80ms
step:965/1680 train_time:84728ms step_avg:87.80ms
step:966/1680 train_time:84816ms step_avg:87.80ms
step:967/1680 train_time:84905ms step_avg:87.80ms
step:968/1680 train_time:84994ms step_avg:87.80ms
step:969/1680 train_time:85083ms step_avg:87.80ms
step:970/1680 train_time:85172ms step_avg:87.81ms
step:971/1680 train_time:85261ms step_avg:87.81ms
step:972/1680 train_time:85351ms step_avg:87.81ms
step:973/1680 train_time:85439ms step_avg:87.81ms
step:974/1680 train_time:85528ms step_avg:87.81ms
step:975/1680 train_time:85616ms step_avg:87.81ms
step:976/1680 train_time:85704ms step_avg:87.81ms
step:977/1680 train_time:85792ms step_avg:87.81ms
step:978/1680 train_time:85880ms step_avg:87.81ms
step:979/1680 train_time:85969ms step_avg:87.81ms
step:980/1680 train_time:86058ms step_avg:87.81ms
step:981/1680 train_time:86147ms step_avg:87.82ms
step:982/1680 train_time:86236ms step_avg:87.82ms
step:983/1680 train_time:86324ms step_avg:87.82ms
step:984/1680 train_time:86413ms step_avg:87.82ms
step:985/1680 train_time:86501ms step_avg:87.82ms
step:986/1680 train_time:86591ms step_avg:87.82ms
step:987/1680 train_time:86679ms step_avg:87.82ms
step:988/1680 train_time:86768ms step_avg:87.82ms
step:989/1680 train_time:86858ms step_avg:87.82ms
step:990/1680 train_time:86946ms step_avg:87.82ms
step:991/1680 train_time:87034ms step_avg:87.82ms
step:992/1680 train_time:87123ms step_avg:87.83ms
step:993/1680 train_time:87212ms step_avg:87.83ms
step:994/1680 train_time:87301ms step_avg:87.83ms
step:995/1680 train_time:87389ms step_avg:87.83ms
step:996/1680 train_time:87477ms step_avg:87.83ms
step:997/1680 train_time:87566ms step_avg:87.83ms
step:998/1680 train_time:87653ms step_avg:87.83ms
step:999/1680 train_time:87742ms step_avg:87.83ms
step:1000/1680 train_time:87830ms step_avg:87.83ms
step:1000/1680 val_loss:3.4685 train_time:87920ms step_avg:87.92ms
step:1001/1680 train_time:87942ms step_avg:87.85ms
step:1002/1680 train_time:88013ms step_avg:87.84ms
step:1003/1680 train_time:88104ms step_avg:87.84ms
step:1004/1680 train_time:88192ms step_avg:87.84ms
step:1005/1680 train_time:88279ms step_avg:87.84ms
step:1006/1680 train_time:88367ms step_avg:87.84ms
step:1007/1680 train_time:88454ms step_avg:87.84ms
step:1008/1680 train_time:88542ms step_avg:87.84ms
step:1009/1680 train_time:88630ms step_avg:87.84ms
step:1010/1680 train_time:88717ms step_avg:87.84ms
step:1011/1680 train_time:88805ms step_avg:87.84ms
step:1012/1680 train_time:88894ms step_avg:87.84ms
step:1013/1680 train_time:88985ms step_avg:87.84ms
step:1014/1680 train_time:89074ms step_avg:87.84ms
step:1015/1680 train_time:89164ms step_avg:87.85ms
step:1016/1680 train_time:89252ms step_avg:87.85ms
step:1017/1680 train_time:89340ms step_avg:87.85ms
step:1018/1680 train_time:89428ms step_avg:87.85ms
step:1019/1680 train_time:89516ms step_avg:87.85ms
step:1020/1680 train_time:89604ms step_avg:87.85ms
step:1021/1680 train_time:89692ms step_avg:87.85ms
step:1022/1680 train_time:89781ms step_avg:87.85ms
step:1023/1680 train_time:89870ms step_avg:87.85ms
step:1024/1680 train_time:89959ms step_avg:87.85ms
step:1025/1680 train_time:90050ms step_avg:87.85ms
step:1026/1680 train_time:90139ms step_avg:87.85ms
step:1027/1680 train_time:90228ms step_avg:87.86ms
step:1028/1680 train_time:90317ms step_avg:87.86ms
step:1029/1680 train_time:90405ms step_avg:87.86ms
step:1030/1680 train_time:90493ms step_avg:87.86ms
step:1031/1680 train_time:90580ms step_avg:87.86ms
step:1032/1680 train_time:90668ms step_avg:87.86ms
step:1033/1680 train_time:90756ms step_avg:87.86ms
step:1034/1680 train_time:90845ms step_avg:87.86ms
step:1035/1680 train_time:90933ms step_avg:87.86ms
step:1036/1680 train_time:91023ms step_avg:87.86ms
step:1037/1680 train_time:91112ms step_avg:87.86ms
step:1038/1680 train_time:91200ms step_avg:87.86ms
step:1039/1680 train_time:91289ms step_avg:87.86ms
step:1040/1680 train_time:91377ms step_avg:87.86ms
step:1041/1680 train_time:91465ms step_avg:87.86ms
step:1042/1680 train_time:91554ms step_avg:87.86ms
step:1043/1680 train_time:91642ms step_avg:87.86ms
step:1044/1680 train_time:91732ms step_avg:87.87ms
step:1045/1680 train_time:91820ms step_avg:87.87ms
step:1046/1680 train_time:91909ms step_avg:87.87ms
step:1047/1680 train_time:91997ms step_avg:87.87ms
step:1048/1680 train_time:92087ms step_avg:87.87ms
step:1049/1680 train_time:92175ms step_avg:87.87ms
step:1050/1680 train_time:92264ms step_avg:87.87ms
step:1051/1680 train_time:92352ms step_avg:87.87ms
step:1052/1680 train_time:92442ms step_avg:87.87ms
step:1053/1680 train_time:92529ms step_avg:87.87ms
step:1054/1680 train_time:92617ms step_avg:87.87ms
step:1055/1680 train_time:92706ms step_avg:87.87ms
step:1056/1680 train_time:92794ms step_avg:87.87ms
step:1057/1680 train_time:92882ms step_avg:87.87ms
step:1058/1680 train_time:92971ms step_avg:87.87ms
step:1059/1680 train_time:93061ms step_avg:87.88ms
step:1060/1680 train_time:93150ms step_avg:87.88ms
step:1061/1680 train_time:93239ms step_avg:87.88ms
step:1062/1680 train_time:93328ms step_avg:87.88ms
step:1063/1680 train_time:93416ms step_avg:87.88ms
step:1064/1680 train_time:93504ms step_avg:87.88ms
step:1065/1680 train_time:93593ms step_avg:87.88ms
step:1066/1680 train_time:93682ms step_avg:87.88ms
step:1067/1680 train_time:93770ms step_avg:87.88ms
step:1068/1680 train_time:93858ms step_avg:87.88ms
step:1069/1680 train_time:93947ms step_avg:87.88ms
step:1070/1680 train_time:94036ms step_avg:87.88ms
step:1071/1680 train_time:94124ms step_avg:87.88ms
step:1072/1680 train_time:94214ms step_avg:87.89ms
step:1073/1680 train_time:94302ms step_avg:87.89ms
step:1074/1680 train_time:94391ms step_avg:87.89ms
step:1075/1680 train_time:94480ms step_avg:87.89ms
step:1076/1680 train_time:94568ms step_avg:87.89ms
step:1077/1680 train_time:94657ms step_avg:87.89ms
step:1078/1680 train_time:94744ms step_avg:87.89ms
step:1079/1680 train_time:94833ms step_avg:87.89ms
step:1080/1680 train_time:94922ms step_avg:87.89ms
step:1081/1680 train_time:95011ms step_avg:87.89ms
step:1082/1680 train_time:95100ms step_avg:87.89ms
step:1083/1680 train_time:95188ms step_avg:87.89ms
step:1084/1680 train_time:95277ms step_avg:87.89ms
step:1085/1680 train_time:95366ms step_avg:87.89ms
step:1086/1680 train_time:95454ms step_avg:87.89ms
step:1087/1680 train_time:95543ms step_avg:87.90ms
step:1088/1680 train_time:95631ms step_avg:87.90ms
step:1089/1680 train_time:95720ms step_avg:87.90ms
step:1090/1680 train_time:95808ms step_avg:87.90ms
step:1091/1680 train_time:95896ms step_avg:87.90ms
step:1092/1680 train_time:95985ms step_avg:87.90ms
step:1093/1680 train_time:96073ms step_avg:87.90ms
step:1094/1680 train_time:96163ms step_avg:87.90ms
step:1095/1680 train_time:96251ms step_avg:87.90ms
step:1096/1680 train_time:96341ms step_avg:87.90ms
step:1097/1680 train_time:96430ms step_avg:87.90ms
step:1098/1680 train_time:96520ms step_avg:87.91ms
step:1099/1680 train_time:96609ms step_avg:87.91ms
step:1100/1680 train_time:96697ms step_avg:87.91ms
step:1101/1680 train_time:96786ms step_avg:87.91ms
step:1102/1680 train_time:96875ms step_avg:87.91ms
step:1103/1680 train_time:96964ms step_avg:87.91ms
step:1104/1680 train_time:97053ms step_avg:87.91ms
step:1105/1680 train_time:97142ms step_avg:87.91ms
step:1106/1680 train_time:97232ms step_avg:87.91ms
step:1107/1680 train_time:97322ms step_avg:87.92ms
step:1108/1680 train_time:97413ms step_avg:87.92ms
step:1109/1680 train_time:97503ms step_avg:87.92ms
step:1110/1680 train_time:97592ms step_avg:87.92ms
step:1111/1680 train_time:97682ms step_avg:87.92ms
step:1112/1680 train_time:97770ms step_avg:87.92ms
step:1113/1680 train_time:97859ms step_avg:87.92ms
step:1114/1680 train_time:97948ms step_avg:87.92ms
step:1115/1680 train_time:98037ms step_avg:87.93ms
step:1116/1680 train_time:98126ms step_avg:87.93ms
step:1117/1680 train_time:98215ms step_avg:87.93ms
step:1118/1680 train_time:98304ms step_avg:87.93ms
step:1119/1680 train_time:98393ms step_avg:87.93ms
step:1120/1680 train_time:98482ms step_avg:87.93ms
step:1121/1680 train_time:98571ms step_avg:87.93ms
step:1122/1680 train_time:98661ms step_avg:87.93ms
step:1123/1680 train_time:98750ms step_avg:87.93ms
step:1124/1680 train_time:98839ms step_avg:87.93ms
step:1125/1680 train_time:98929ms step_avg:87.94ms
step:1125/1680 val_loss:3.4150 train_time:99019ms step_avg:88.02ms
step:1126/1680 train_time:99041ms step_avg:87.96ms
step:1127/1680 train_time:99111ms step_avg:87.94ms
step:1128/1680 train_time:99203ms step_avg:87.95ms
step:1129/1680 train_time:99295ms step_avg:87.95ms
step:1130/1680 train_time:99384ms step_avg:87.95ms
step:1131/1680 train_time:99472ms step_avg:87.95ms
step:1132/1680 train_time:99560ms step_avg:87.95ms
step:1133/1680 train_time:99648ms step_avg:87.95ms
step:1134/1680 train_time:99736ms step_avg:87.95ms
step:1135/1680 train_time:99825ms step_avg:87.95ms
step:1136/1680 train_time:99914ms step_avg:87.95ms
step:1137/1680 train_time:100005ms step_avg:87.95ms
step:1138/1680 train_time:100096ms step_avg:87.96ms
step:1139/1680 train_time:100187ms step_avg:87.96ms
step:1140/1680 train_time:100279ms step_avg:87.96ms
step:1141/1680 train_time:100368ms step_avg:87.97ms
step:1142/1680 train_time:100457ms step_avg:87.97ms
step:1143/1680 train_time:100545ms step_avg:87.97ms
step:1144/1680 train_time:100633ms step_avg:87.97ms
step:1145/1680 train_time:100722ms step_avg:87.97ms
step:1146/1680 train_time:100810ms step_avg:87.97ms
step:1147/1680 train_time:100898ms step_avg:87.97ms
step:1148/1680 train_time:100988ms step_avg:87.97ms
step:1149/1680 train_time:101078ms step_avg:87.97ms
step:1150/1680 train_time:101168ms step_avg:87.97ms
step:1151/1680 train_time:101258ms step_avg:87.97ms
step:1152/1680 train_time:101347ms step_avg:87.98ms
step:1153/1680 train_time:101437ms step_avg:87.98ms
step:1154/1680 train_time:101526ms step_avg:87.98ms
step:1155/1680 train_time:101615ms step_avg:87.98ms
step:1156/1680 train_time:101703ms step_avg:87.98ms
step:1157/1680 train_time:101792ms step_avg:87.98ms
step:1158/1680 train_time:101881ms step_avg:87.98ms
step:1159/1680 train_time:101969ms step_avg:87.98ms
step:1160/1680 train_time:102059ms step_avg:87.98ms
step:1161/1680 train_time:102149ms step_avg:87.98ms
step:1162/1680 train_time:102238ms step_avg:87.98ms
step:1163/1680 train_time:102328ms step_avg:87.99ms
step:1164/1680 train_time:102417ms step_avg:87.99ms
step:1165/1680 train_time:102506ms step_avg:87.99ms
step:1166/1680 train_time:102595ms step_avg:87.99ms
step:1167/1680 train_time:102684ms step_avg:87.99ms
step:1168/1680 train_time:102773ms step_avg:87.99ms
step:1169/1680 train_time:102862ms step_avg:87.99ms
step:1170/1680 train_time:102950ms step_avg:87.99ms
step:1171/1680 train_time:103040ms step_avg:87.99ms
step:1172/1680 train_time:103130ms step_avg:87.99ms
step:1173/1680 train_time:103219ms step_avg:88.00ms
step:1174/1680 train_time:103309ms step_avg:88.00ms
step:1175/1680 train_time:103399ms step_avg:88.00ms
step:1176/1680 train_time:103487ms step_avg:88.00ms
step:1177/1680 train_time:103577ms step_avg:88.00ms
step:1178/1680 train_time:103666ms step_avg:88.00ms
step:1179/1680 train_time:103754ms step_avg:88.00ms
step:1180/1680 train_time:103843ms step_avg:88.00ms
step:1181/1680 train_time:103932ms step_avg:88.00ms
step:1182/1680 train_time:104021ms step_avg:88.00ms
step:1183/1680 train_time:104110ms step_avg:88.00ms
step:1184/1680 train_time:104199ms step_avg:88.01ms
step:1185/1680 train_time:104288ms step_avg:88.01ms
step:1186/1680 train_time:104378ms step_avg:88.01ms
step:1187/1680 train_time:104468ms step_avg:88.01ms
step:1188/1680 train_time:104557ms step_avg:88.01ms
step:1189/1680 train_time:104646ms step_avg:88.01ms
step:1190/1680 train_time:104735ms step_avg:88.01ms
step:1191/1680 train_time:104824ms step_avg:88.01ms
step:1192/1680 train_time:104913ms step_avg:88.01ms
step:1193/1680 train_time:105002ms step_avg:88.02ms
step:1194/1680 train_time:105091ms step_avg:88.02ms
step:1195/1680 train_time:105180ms step_avg:88.02ms
step:1196/1680 train_time:105269ms step_avg:88.02ms
step:1197/1680 train_time:105359ms step_avg:88.02ms
step:1198/1680 train_time:105448ms step_avg:88.02ms
step:1199/1680 train_time:105537ms step_avg:88.02ms
step:1200/1680 train_time:105625ms step_avg:88.02ms
step:1201/1680 train_time:105715ms step_avg:88.02ms
step:1202/1680 train_time:105804ms step_avg:88.02ms
step:1203/1680 train_time:105894ms step_avg:88.02ms
step:1204/1680 train_time:105983ms step_avg:88.03ms
step:1205/1680 train_time:106072ms step_avg:88.03ms
step:1206/1680 train_time:106161ms step_avg:88.03ms
step:1207/1680 train_time:106250ms step_avg:88.03ms
step:1208/1680 train_time:106340ms step_avg:88.03ms
step:1209/1680 train_time:106428ms step_avg:88.03ms
step:1210/1680 train_time:106517ms step_avg:88.03ms
step:1211/1680 train_time:106607ms step_avg:88.03ms
step:1212/1680 train_time:106696ms step_avg:88.03ms
step:1213/1680 train_time:106786ms step_avg:88.03ms
step:1214/1680 train_time:106875ms step_avg:88.04ms
step:1215/1680 train_time:106965ms step_avg:88.04ms
step:1216/1680 train_time:107053ms step_avg:88.04ms
step:1217/1680 train_time:107142ms step_avg:88.04ms
step:1218/1680 train_time:107231ms step_avg:88.04ms
step:1219/1680 train_time:107321ms step_avg:88.04ms
step:1220/1680 train_time:107411ms step_avg:88.04ms
step:1221/1680 train_time:107500ms step_avg:88.04ms
step:1222/1680 train_time:107590ms step_avg:88.04ms
step:1223/1680 train_time:107678ms step_avg:88.04ms
step:1224/1680 train_time:107767ms step_avg:88.05ms
step:1225/1680 train_time:107857ms step_avg:88.05ms
step:1226/1680 train_time:107946ms step_avg:88.05ms
step:1227/1680 train_time:108035ms step_avg:88.05ms
step:1228/1680 train_time:108124ms step_avg:88.05ms
step:1229/1680 train_time:108213ms step_avg:88.05ms
step:1230/1680 train_time:108302ms step_avg:88.05ms
step:1231/1680 train_time:108392ms step_avg:88.05ms
step:1232/1680 train_time:108481ms step_avg:88.05ms
step:1233/1680 train_time:108570ms step_avg:88.05ms
step:1234/1680 train_time:108659ms step_avg:88.05ms
step:1235/1680 train_time:108748ms step_avg:88.06ms
step:1236/1680 train_time:108838ms step_avg:88.06ms
step:1237/1680 train_time:108927ms step_avg:88.06ms
step:1238/1680 train_time:109016ms step_avg:88.06ms
step:1239/1680 train_time:109105ms step_avg:88.06ms
step:1240/1680 train_time:109194ms step_avg:88.06ms
step:1241/1680 train_time:109283ms step_avg:88.06ms
step:1242/1680 train_time:109372ms step_avg:88.06ms
step:1243/1680 train_time:109463ms step_avg:88.06ms
step:1244/1680 train_time:109552ms step_avg:88.06ms
step:1245/1680 train_time:109642ms step_avg:88.07ms
step:1246/1680 train_time:109731ms step_avg:88.07ms
step:1247/1680 train_time:109820ms step_avg:88.07ms
step:1248/1680 train_time:109909ms step_avg:88.07ms
step:1249/1680 train_time:109999ms step_avg:88.07ms
step:1250/1680 train_time:110089ms step_avg:88.07ms
step:1250/1680 val_loss:3.3770 train_time:110179ms step_avg:88.14ms
step:1251/1680 train_time:110200ms step_avg:88.09ms
step:1252/1680 train_time:110271ms step_avg:88.08ms
step:1253/1680 train_time:110361ms step_avg:88.08ms
step:1254/1680 train_time:110452ms step_avg:88.08ms
step:1255/1680 train_time:110540ms step_avg:88.08ms
step:1256/1680 train_time:110628ms step_avg:88.08ms
step:1257/1680 train_time:110716ms step_avg:88.08ms
step:1258/1680 train_time:110805ms step_avg:88.08ms
step:1259/1680 train_time:110893ms step_avg:88.08ms
step:1260/1680 train_time:110983ms step_avg:88.08ms
step:1261/1680 train_time:111072ms step_avg:88.08ms
step:1262/1680 train_time:111163ms step_avg:88.08ms
step:1263/1680 train_time:111253ms step_avg:88.09ms
step:1264/1680 train_time:111343ms step_avg:88.09ms
step:1265/1680 train_time:111433ms step_avg:88.09ms
step:1266/1680 train_time:111522ms step_avg:88.09ms
step:1267/1680 train_time:111611ms step_avg:88.09ms
step:1268/1680 train_time:111700ms step_avg:88.09ms
step:1269/1680 train_time:111789ms step_avg:88.09ms
step:1270/1680 train_time:111878ms step_avg:88.09ms
step:1271/1680 train_time:111967ms step_avg:88.09ms
step:1272/1680 train_time:112056ms step_avg:88.09ms
step:1273/1680 train_time:112146ms step_avg:88.10ms
step:1274/1680 train_time:112237ms step_avg:88.10ms
step:1275/1680 train_time:112326ms step_avg:88.10ms
step:1276/1680 train_time:112416ms step_avg:88.10ms
step:1277/1680 train_time:112505ms step_avg:88.10ms
step:1278/1680 train_time:112594ms step_avg:88.10ms
step:1279/1680 train_time:112683ms step_avg:88.10ms
step:1280/1680 train_time:112771ms step_avg:88.10ms
step:1281/1680 train_time:112860ms step_avg:88.10ms
step:1282/1680 train_time:112949ms step_avg:88.10ms
step:1283/1680 train_time:113038ms step_avg:88.10ms
step:1284/1680 train_time:113128ms step_avg:88.11ms
step:1285/1680 train_time:113218ms step_avg:88.11ms
step:1286/1680 train_time:113308ms step_avg:88.11ms
step:1287/1680 train_time:113398ms step_avg:88.11ms
step:1288/1680 train_time:113487ms step_avg:88.11ms
step:1289/1680 train_time:113576ms step_avg:88.11ms
step:1290/1680 train_time:113665ms step_avg:88.11ms
step:1291/1680 train_time:113754ms step_avg:88.11ms
step:1292/1680 train_time:113844ms step_avg:88.11ms
step:1293/1680 train_time:113933ms step_avg:88.12ms
step:1294/1680 train_time:114023ms step_avg:88.12ms
step:1295/1680 train_time:114112ms step_avg:88.12ms
step:1296/1680 train_time:114203ms step_avg:88.12ms
step:1297/1680 train_time:114291ms step_avg:88.12ms
step:1298/1680 train_time:114381ms step_avg:88.12ms
step:1299/1680 train_time:114471ms step_avg:88.12ms
step:1300/1680 train_time:114560ms step_avg:88.12ms
step:1301/1680 train_time:114650ms step_avg:88.12ms
step:1302/1680 train_time:114738ms step_avg:88.12ms
step:1303/1680 train_time:114827ms step_avg:88.13ms
step:1304/1680 train_time:114918ms step_avg:88.13ms
step:1305/1680 train_time:115007ms step_avg:88.13ms
step:1306/1680 train_time:115096ms step_avg:88.13ms
step:1307/1680 train_time:115185ms step_avg:88.13ms
step:1308/1680 train_time:115274ms step_avg:88.13ms
step:1309/1680 train_time:115364ms step_avg:88.13ms
step:1310/1680 train_time:115453ms step_avg:88.13ms
step:1311/1680 train_time:115543ms step_avg:88.13ms
step:1312/1680 train_time:115632ms step_avg:88.13ms
step:1313/1680 train_time:115722ms step_avg:88.14ms
step:1314/1680 train_time:115811ms step_avg:88.14ms
step:1315/1680 train_time:115900ms step_avg:88.14ms
step:1316/1680 train_time:115989ms step_avg:88.14ms
step:1317/1680 train_time:116078ms step_avg:88.14ms
step:1318/1680 train_time:116168ms step_avg:88.14ms
step:1319/1680 train_time:116258ms step_avg:88.14ms
step:1320/1680 train_time:116347ms step_avg:88.14ms
step:1321/1680 train_time:116437ms step_avg:88.14ms
step:1322/1680 train_time:116526ms step_avg:88.14ms
step:1323/1680 train_time:116616ms step_avg:88.15ms
step:1324/1680 train_time:116705ms step_avg:88.15ms
step:1325/1680 train_time:116795ms step_avg:88.15ms
step:1326/1680 train_time:116884ms step_avg:88.15ms
step:1327/1680 train_time:116973ms step_avg:88.15ms
step:1328/1680 train_time:117061ms step_avg:88.15ms
step:1329/1680 train_time:117150ms step_avg:88.15ms
step:1330/1680 train_time:117240ms step_avg:88.15ms
step:1331/1680 train_time:117330ms step_avg:88.15ms
step:1332/1680 train_time:117420ms step_avg:88.15ms
step:1333/1680 train_time:117509ms step_avg:88.15ms
step:1334/1680 train_time:117598ms step_avg:88.15ms
step:1335/1680 train_time:117687ms step_avg:88.16ms
step:1336/1680 train_time:117777ms step_avg:88.16ms
step:1337/1680 train_time:117867ms step_avg:88.16ms
step:1338/1680 train_time:117956ms step_avg:88.16ms
step:1339/1680 train_time:118045ms step_avg:88.16ms
step:1340/1680 train_time:118134ms step_avg:88.16ms
step:1341/1680 train_time:118223ms step_avg:88.16ms
step:1342/1680 train_time:118313ms step_avg:88.16ms
step:1343/1680 train_time:118402ms step_avg:88.16ms
step:1344/1680 train_time:118492ms step_avg:88.16ms
step:1345/1680 train_time:118581ms step_avg:88.16ms
step:1346/1680 train_time:118669ms step_avg:88.16ms
step:1347/1680 train_time:118759ms step_avg:88.17ms
step:1348/1680 train_time:118848ms step_avg:88.17ms
step:1349/1680 train_time:118938ms step_avg:88.17ms
step:1350/1680 train_time:119027ms step_avg:88.17ms
step:1351/1680 train_time:119116ms step_avg:88.17ms
step:1352/1680 train_time:119206ms step_avg:88.17ms
step:1353/1680 train_time:119296ms step_avg:88.17ms
step:1354/1680 train_time:119385ms step_avg:88.17ms
step:1355/1680 train_time:119474ms step_avg:88.17ms
step:1356/1680 train_time:119563ms step_avg:88.17ms
step:1357/1680 train_time:119652ms step_avg:88.17ms
step:1358/1680 train_time:119741ms step_avg:88.17ms
step:1359/1680 train_time:119830ms step_avg:88.18ms
step:1360/1680 train_time:119919ms step_avg:88.18ms
step:1361/1680 train_time:120008ms step_avg:88.18ms
step:1362/1680 train_time:120097ms step_avg:88.18ms
step:1363/1680 train_time:120186ms step_avg:88.18ms
step:1364/1680 train_time:120276ms step_avg:88.18ms
step:1365/1680 train_time:120365ms step_avg:88.18ms
step:1366/1680 train_time:120454ms step_avg:88.18ms
step:1367/1680 train_time:120543ms step_avg:88.18ms
step:1368/1680 train_time:120633ms step_avg:88.18ms
step:1369/1680 train_time:120723ms step_avg:88.18ms
step:1370/1680 train_time:120813ms step_avg:88.18ms
step:1371/1680 train_time:120902ms step_avg:88.19ms
step:1372/1680 train_time:120992ms step_avg:88.19ms
step:1373/1680 train_time:121080ms step_avg:88.19ms
step:1374/1680 train_time:121170ms step_avg:88.19ms
step:1375/1680 train_time:121258ms step_avg:88.19ms
step:1375/1680 val_loss:3.3421 train_time:121349ms step_avg:88.25ms
step:1376/1680 train_time:121369ms step_avg:88.20ms
step:1377/1680 train_time:121444ms step_avg:88.19ms
step:1378/1680 train_time:121534ms step_avg:88.20ms
step:1379/1680 train_time:121623ms step_avg:88.20ms
step:1380/1680 train_time:121711ms step_avg:88.20ms
step:1381/1680 train_time:121800ms step_avg:88.20ms
step:1382/1680 train_time:121888ms step_avg:88.20ms
step:1383/1680 train_time:121976ms step_avg:88.20ms
step:1384/1680 train_time:122065ms step_avg:88.20ms
step:1385/1680 train_time:122154ms step_avg:88.20ms
step:1386/1680 train_time:122242ms step_avg:88.20ms
step:1387/1680 train_time:122332ms step_avg:88.20ms
step:1388/1680 train_time:122424ms step_avg:88.20ms
step:1389/1680 train_time:122514ms step_avg:88.20ms
step:1390/1680 train_time:122604ms step_avg:88.20ms
step:1391/1680 train_time:122693ms step_avg:88.20ms
step:1392/1680 train_time:122782ms step_avg:88.21ms
step:1393/1680 train_time:122871ms step_avg:88.21ms
step:1394/1680 train_time:122960ms step_avg:88.21ms
step:1395/1680 train_time:123048ms step_avg:88.21ms
step:1396/1680 train_time:123137ms step_avg:88.21ms
step:1397/1680 train_time:123226ms step_avg:88.21ms
step:1398/1680 train_time:123315ms step_avg:88.21ms
step:1399/1680 train_time:123405ms step_avg:88.21ms
step:1400/1680 train_time:123495ms step_avg:88.21ms
step:1401/1680 train_time:123585ms step_avg:88.21ms
step:1402/1680 train_time:123674ms step_avg:88.21ms
step:1403/1680 train_time:123765ms step_avg:88.21ms
step:1404/1680 train_time:123854ms step_avg:88.21ms
step:1405/1680 train_time:123942ms step_avg:88.22ms
step:1406/1680 train_time:124031ms step_avg:88.22ms
step:1407/1680 train_time:124119ms step_avg:88.22ms
step:1408/1680 train_time:124208ms step_avg:88.22ms
step:1409/1680 train_time:124298ms step_avg:88.22ms
step:1410/1680 train_time:124387ms step_avg:88.22ms
step:1411/1680 train_time:124477ms step_avg:88.22ms
step:1412/1680 train_time:124567ms step_avg:88.22ms
step:1413/1680 train_time:124657ms step_avg:88.22ms
step:1414/1680 train_time:124747ms step_avg:88.22ms
step:1415/1680 train_time:124836ms step_avg:88.22ms
step:1416/1680 train_time:124925ms step_avg:88.22ms
step:1417/1680 train_time:125014ms step_avg:88.22ms
step:1418/1680 train_time:125103ms step_avg:88.22ms
step:1419/1680 train_time:125191ms step_avg:88.22ms
step:1420/1680 train_time:125280ms step_avg:88.23ms
step:1421/1680 train_time:125370ms step_avg:88.23ms
step:1422/1680 train_time:125460ms step_avg:88.23ms
step:1423/1680 train_time:125549ms step_avg:88.23ms
step:1424/1680 train_time:125638ms step_avg:88.23ms
step:1425/1680 train_time:125728ms step_avg:88.23ms
step:1426/1680 train_time:125817ms step_avg:88.23ms
step:1427/1680 train_time:125906ms step_avg:88.23ms
step:1428/1680 train_time:125995ms step_avg:88.23ms
step:1429/1680 train_time:126085ms step_avg:88.23ms
step:1430/1680 train_time:126174ms step_avg:88.23ms
step:1431/1680 train_time:126262ms step_avg:88.23ms
step:1432/1680 train_time:126351ms step_avg:88.23ms
step:1433/1680 train_time:126441ms step_avg:88.24ms
step:1434/1680 train_time:126531ms step_avg:88.24ms
step:1435/1680 train_time:126620ms step_avg:88.24ms
step:1436/1680 train_time:126709ms step_avg:88.24ms
step:1437/1680 train_time:126799ms step_avg:88.24ms
step:1438/1680 train_time:126887ms step_avg:88.24ms
step:1439/1680 train_time:126976ms step_avg:88.24ms
step:1440/1680 train_time:127065ms step_avg:88.24ms
step:1441/1680 train_time:127155ms step_avg:88.24ms
step:1442/1680 train_time:127245ms step_avg:88.24ms
step:1443/1680 train_time:127333ms step_avg:88.24ms
step:1444/1680 train_time:127423ms step_avg:88.24ms
step:1445/1680 train_time:127513ms step_avg:88.24ms
step:1446/1680 train_time:127602ms step_avg:88.24ms
step:1447/1680 train_time:127692ms step_avg:88.25ms
step:1448/1680 train_time:127781ms step_avg:88.25ms
step:1449/1680 train_time:127871ms step_avg:88.25ms
step:1450/1680 train_time:127960ms step_avg:88.25ms
step:1451/1680 train_time:128049ms step_avg:88.25ms
step:1452/1680 train_time:128138ms step_avg:88.25ms
step:1453/1680 train_time:128227ms step_avg:88.25ms
step:1454/1680 train_time:128317ms step_avg:88.25ms
step:1455/1680 train_time:128406ms step_avg:88.25ms
step:1456/1680 train_time:128497ms step_avg:88.25ms
step:1457/1680 train_time:128586ms step_avg:88.25ms
step:1458/1680 train_time:128674ms step_avg:88.25ms
step:1459/1680 train_time:128763ms step_avg:88.25ms
step:1460/1680 train_time:128853ms step_avg:88.26ms
step:1461/1680 train_time:128944ms step_avg:88.26ms
step:1462/1680 train_time:129033ms step_avg:88.26ms
step:1463/1680 train_time:129121ms step_avg:88.26ms
step:1464/1680 train_time:129211ms step_avg:88.26ms
step:1465/1680 train_time:129300ms step_avg:88.26ms
step:1466/1680 train_time:129389ms step_avg:88.26ms
step:1467/1680 train_time:129479ms step_avg:88.26ms
step:1468/1680 train_time:129568ms step_avg:88.26ms
step:1469/1680 train_time:129658ms step_avg:88.26ms
step:1470/1680 train_time:129746ms step_avg:88.26ms
step:1471/1680 train_time:129835ms step_avg:88.26ms
step:1472/1680 train_time:129924ms step_avg:88.26ms
step:1473/1680 train_time:130013ms step_avg:88.26ms
step:1474/1680 train_time:130102ms step_avg:88.26ms
step:1475/1680 train_time:130191ms step_avg:88.27ms
step:1476/1680 train_time:130280ms step_avg:88.27ms
step:1477/1680 train_time:130370ms step_avg:88.27ms
step:1478/1680 train_time:130460ms step_avg:88.27ms
step:1479/1680 train_time:130549ms step_avg:88.27ms
step:1480/1680 train_time:130638ms step_avg:88.27ms
step:1481/1680 train_time:130728ms step_avg:88.27ms
step:1482/1680 train_time:130818ms step_avg:88.27ms
step:1483/1680 train_time:130906ms step_avg:88.27ms
step:1484/1680 train_time:130996ms step_avg:88.27ms
step:1485/1680 train_time:131085ms step_avg:88.27ms
step:1486/1680 train_time:131174ms step_avg:88.27ms
step:1487/1680 train_time:131262ms step_avg:88.27ms
step:1488/1680 train_time:131352ms step_avg:88.27ms
step:1489/1680 train_time:131441ms step_avg:88.27ms
step:1490/1680 train_time:131531ms step_avg:88.28ms
step:1491/1680 train_time:131620ms step_avg:88.28ms
step:1492/1680 train_time:131709ms step_avg:88.28ms
step:1493/1680 train_time:131800ms step_avg:88.28ms
step:1494/1680 train_time:131889ms step_avg:88.28ms
step:1495/1680 train_time:131978ms step_avg:88.28ms
step:1496/1680 train_time:132067ms step_avg:88.28ms
step:1497/1680 train_time:132157ms step_avg:88.28ms
step:1498/1680 train_time:132246ms step_avg:88.28ms
step:1499/1680 train_time:132335ms step_avg:88.28ms
step:1500/1680 train_time:132424ms step_avg:88.28ms
step:1500/1680 val_loss:3.3125 train_time:132514ms step_avg:88.34ms
step:1501/1680 train_time:132534ms step_avg:88.30ms
step:1502/1680 train_time:132606ms step_avg:88.29ms
step:1503/1680 train_time:132699ms step_avg:88.29ms
step:1504/1680 train_time:132788ms step_avg:88.29ms
step:1505/1680 train_time:132877ms step_avg:88.29ms
step:1506/1680 train_time:132965ms step_avg:88.29ms
step:1507/1680 train_time:133053ms step_avg:88.29ms
step:1508/1680 train_time:133141ms step_avg:88.29ms
step:1509/1680 train_time:133229ms step_avg:88.29ms
step:1510/1680 train_time:133318ms step_avg:88.29ms
step:1511/1680 train_time:133406ms step_avg:88.29ms
step:1512/1680 train_time:133497ms step_avg:88.29ms
step:1513/1680 train_time:133589ms step_avg:88.29ms
step:1514/1680 train_time:133681ms step_avg:88.30ms
step:1515/1680 train_time:133771ms step_avg:88.30ms
step:1516/1680 train_time:133860ms step_avg:88.30ms
step:1517/1680 train_time:133949ms step_avg:88.30ms
step:1518/1680 train_time:134037ms step_avg:88.30ms
step:1519/1680 train_time:134125ms step_avg:88.30ms
step:1520/1680 train_time:134213ms step_avg:88.30ms
step:1521/1680 train_time:134302ms step_avg:88.30ms
step:1522/1680 train_time:134391ms step_avg:88.30ms
step:1523/1680 train_time:134480ms step_avg:88.30ms
step:1524/1680 train_time:134571ms step_avg:88.30ms
step:1525/1680 train_time:134661ms step_avg:88.30ms
step:1526/1680 train_time:134751ms step_avg:88.30ms
step:1527/1680 train_time:134841ms step_avg:88.30ms
step:1528/1680 train_time:134931ms step_avg:88.31ms
step:1529/1680 train_time:135020ms step_avg:88.31ms
step:1530/1680 train_time:135108ms step_avg:88.31ms
step:1531/1680 train_time:135196ms step_avg:88.31ms
step:1532/1680 train_time:135284ms step_avg:88.31ms
step:1533/1680 train_time:135373ms step_avg:88.31ms
step:1534/1680 train_time:135462ms step_avg:88.31ms
step:1535/1680 train_time:135552ms step_avg:88.31ms
step:1536/1680 train_time:135642ms step_avg:88.31ms
step:1537/1680 train_time:135732ms step_avg:88.31ms
step:1538/1680 train_time:135823ms step_avg:88.31ms
step:1539/1680 train_time:135912ms step_avg:88.31ms
step:1540/1680 train_time:136002ms step_avg:88.31ms
step:1541/1680 train_time:136090ms step_avg:88.31ms
step:1542/1680 train_time:136180ms step_avg:88.31ms
step:1543/1680 train_time:136269ms step_avg:88.31ms
step:1544/1680 train_time:136357ms step_avg:88.31ms
step:1545/1680 train_time:136446ms step_avg:88.31ms
step:1546/1680 train_time:136535ms step_avg:88.32ms
step:1547/1680 train_time:136625ms step_avg:88.32ms
step:1548/1680 train_time:136714ms step_avg:88.32ms
step:1549/1680 train_time:136804ms step_avg:88.32ms
step:1550/1680 train_time:136893ms step_avg:88.32ms
step:1551/1680 train_time:136982ms step_avg:88.32ms
step:1552/1680 train_time:137071ms step_avg:88.32ms
step:1553/1680 train_time:137160ms step_avg:88.32ms
step:1554/1680 train_time:137249ms step_avg:88.32ms
step:1555/1680 train_time:137338ms step_avg:88.32ms
step:1556/1680 train_time:137427ms step_avg:88.32ms
step:1557/1680 train_time:137517ms step_avg:88.32ms
step:1558/1680 train_time:137606ms step_avg:88.32ms
step:1559/1680 train_time:137696ms step_avg:88.32ms
step:1560/1680 train_time:137786ms step_avg:88.32ms
step:1561/1680 train_time:137875ms step_avg:88.32ms
step:1562/1680 train_time:137964ms step_avg:88.33ms
step:1563/1680 train_time:138053ms step_avg:88.33ms
step:1564/1680 train_time:138142ms step_avg:88.33ms
step:1565/1680 train_time:138231ms step_avg:88.33ms
step:1566/1680 train_time:138321ms step_avg:88.33ms
step:1567/1680 train_time:138410ms step_avg:88.33ms
step:1568/1680 train_time:138499ms step_avg:88.33ms
step:1569/1680 train_time:138588ms step_avg:88.33ms
step:1570/1680 train_time:138678ms step_avg:88.33ms
step:1571/1680 train_time:138767ms step_avg:88.33ms
step:1572/1680 train_time:138856ms step_avg:88.33ms
step:1573/1680 train_time:138945ms step_avg:88.33ms
step:1574/1680 train_time:139034ms step_avg:88.33ms
step:1575/1680 train_time:139122ms step_avg:88.33ms
step:1576/1680 train_time:139211ms step_avg:88.33ms
step:1577/1680 train_time:139299ms step_avg:88.33ms
step:1578/1680 train_time:139389ms step_avg:88.33ms
step:1579/1680 train_time:139479ms step_avg:88.33ms
step:1580/1680 train_time:139569ms step_avg:88.33ms
step:1581/1680 train_time:139658ms step_avg:88.34ms
step:1582/1680 train_time:139747ms step_avg:88.34ms
step:1583/1680 train_time:139836ms step_avg:88.34ms
step:1584/1680 train_time:139926ms step_avg:88.34ms
step:1585/1680 train_time:140016ms step_avg:88.34ms
step:1586/1680 train_time:140105ms step_avg:88.34ms
step:1587/1680 train_time:140195ms step_avg:88.34ms
step:1588/1680 train_time:140283ms step_avg:88.34ms
step:1589/1680 train_time:140372ms step_avg:88.34ms
step:1590/1680 train_time:140461ms step_avg:88.34ms
step:1591/1680 train_time:140550ms step_avg:88.34ms
step:1592/1680 train_time:140639ms step_avg:88.34ms
step:1593/1680 train_time:140728ms step_avg:88.34ms
step:1594/1680 train_time:140817ms step_avg:88.34ms
step:1595/1680 train_time:140906ms step_avg:88.34ms
step:1596/1680 train_time:140995ms step_avg:88.34ms
step:1597/1680 train_time:141085ms step_avg:88.34ms
step:1598/1680 train_time:141174ms step_avg:88.34ms
step:1599/1680 train_time:141263ms step_avg:88.34ms
step:1600/1680 train_time:141351ms step_avg:88.34ms
step:1601/1680 train_time:141440ms step_avg:88.34ms
step:1602/1680 train_time:141530ms step_avg:88.35ms
step:1603/1680 train_time:141619ms step_avg:88.35ms
step:1604/1680 train_time:141708ms step_avg:88.35ms
step:1605/1680 train_time:141798ms step_avg:88.35ms
step:1606/1680 train_time:141887ms step_avg:88.35ms
step:1607/1680 train_time:141975ms step_avg:88.35ms
step:1608/1680 train_time:142064ms step_avg:88.35ms
step:1609/1680 train_time:142153ms step_avg:88.35ms
step:1610/1680 train_time:142243ms step_avg:88.35ms
step:1611/1680 train_time:142332ms step_avg:88.35ms
step:1612/1680 train_time:142421ms step_avg:88.35ms
step:1613/1680 train_time:142510ms step_avg:88.35ms
step:1614/1680 train_time:142600ms step_avg:88.35ms
step:1615/1680 train_time:142689ms step_avg:88.35ms
step:1616/1680 train_time:142778ms step_avg:88.35ms
step:1617/1680 train_time:142868ms step_avg:88.35ms
step:1618/1680 train_time:142957ms step_avg:88.35ms
step:1619/1680 train_time:143046ms step_avg:88.35ms
step:1620/1680 train_time:143136ms step_avg:88.36ms
step:1621/1680 train_time:143226ms step_avg:88.36ms
step:1622/1680 train_time:143315ms step_avg:88.36ms
step:1623/1680 train_time:143405ms step_avg:88.36ms
step:1624/1680 train_time:143494ms step_avg:88.36ms
step:1625/1680 train_time:143583ms step_avg:88.36ms
step:1625/1680 val_loss:3.2891 train_time:143674ms step_avg:88.41ms
step:1626/1680 train_time:143692ms step_avg:88.37ms
step:1627/1680 train_time:143768ms step_avg:88.36ms
step:1628/1680 train_time:143862ms step_avg:88.37ms
step:1629/1680 train_time:143951ms step_avg:88.37ms
step:1630/1680 train_time:144039ms step_avg:88.37ms
step:1631/1680 train_time:144127ms step_avg:88.37ms
step:1632/1680 train_time:144215ms step_avg:88.37ms
step:1633/1680 train_time:144304ms step_avg:88.37ms
step:1634/1680 train_time:144392ms step_avg:88.37ms
step:1635/1680 train_time:144480ms step_avg:88.37ms
step:1636/1680 train_time:144571ms step_avg:88.37ms
step:1637/1680 train_time:144661ms step_avg:88.37ms
step:1638/1680 train_time:144752ms step_avg:88.37ms
step:1639/1680 train_time:144845ms step_avg:88.37ms
step:1640/1680 train_time:144934ms step_avg:88.37ms
step:1641/1680 train_time:145023ms step_avg:88.37ms
step:1642/1680 train_time:145111ms step_avg:88.37ms
step:1643/1680 train_time:145200ms step_avg:88.37ms
step:1644/1680 train_time:145288ms step_avg:88.37ms
step:1645/1680 train_time:145377ms step_avg:88.37ms
step:1646/1680 train_time:145466ms step_avg:88.38ms
step:1647/1680 train_time:145554ms step_avg:88.38ms
step:1648/1680 train_time:145645ms step_avg:88.38ms
step:1649/1680 train_time:145736ms step_avg:88.38ms
step:1650/1680 train_time:145827ms step_avg:88.38ms
step:1651/1680 train_time:145916ms step_avg:88.38ms
step:1652/1680 train_time:146006ms step_avg:88.38ms
step:1653/1680 train_time:146095ms step_avg:88.38ms
step:1654/1680 train_time:146184ms step_avg:88.38ms
step:1655/1680 train_time:146273ms step_avg:88.38ms
step:1656/1680 train_time:146361ms step_avg:88.38ms
step:1657/1680 train_time:146451ms step_avg:88.38ms
step:1658/1680 train_time:146539ms step_avg:88.38ms
step:1659/1680 train_time:146629ms step_avg:88.38ms
step:1660/1680 train_time:146719ms step_avg:88.38ms
step:1661/1680 train_time:146809ms step_avg:88.39ms
step:1662/1680 train_time:146899ms step_avg:88.39ms
step:1663/1680 train_time:146988ms step_avg:88.39ms
step:1664/1680 train_time:147077ms step_avg:88.39ms
step:1665/1680 train_time:147166ms step_avg:88.39ms
step:1666/1680 train_time:147255ms step_avg:88.39ms
step:1667/1680 train_time:147344ms step_avg:88.39ms
step:1668/1680 train_time:147432ms step_avg:88.39ms
step:1669/1680 train_time:147521ms step_avg:88.39ms
step:1670/1680 train_time:147610ms step_avg:88.39ms
step:1671/1680 train_time:147699ms step_avg:88.39ms
step:1672/1680 train_time:147789ms step_avg:88.39ms
step:1673/1680 train_time:147878ms step_avg:88.39ms
step:1674/1680 train_time:147968ms step_avg:88.39ms
step:1675/1680 train_time:148056ms step_avg:88.39ms
step:1676/1680 train_time:148146ms step_avg:88.39ms
step:1677/1680 train_time:148236ms step_avg:88.39ms
step:1678/1680 train_time:148325ms step_avg:88.39ms
step:1679/1680 train_time:148413ms step_avg:88.39ms
step:1680/1680 train_time:148502ms step_avg:88.39ms
step:1680/1680 val_loss:3.2782 train_time:148593ms step_avg:88.45ms
peak memory allocated: 30760 MiB reserved: 46254 MiB

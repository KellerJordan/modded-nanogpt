import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 13:32:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            116W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    176192      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    176193      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    176194      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    176195      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    176196      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    176197      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    176198      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    176199      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    176193      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    176194      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    176195      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    176196      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    176197      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    176198      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    176199      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:146ms step_avg:145.82ms
step:2/1680 train_time:165ms step_avg:82.52ms
step:3/1680 train_time:230ms step_avg:76.64ms
step:4/1680 train_time:315ms step_avg:78.70ms
step:5/1680 train_time:401ms step_avg:80.25ms
step:6/1680 train_time:487ms step_avg:81.18ms
step:7/1680 train_time:574ms step_avg:81.94ms
step:8/1680 train_time:660ms step_avg:82.46ms
step:9/1680 train_time:746ms step_avg:82.86ms
step:10/1680 train_time:832ms step_avg:83.17ms
step:11/1680 train_time:918ms step_avg:83.42ms
step:12/1680 train_time:1005ms step_avg:83.76ms
step:13/1680 train_time:1094ms step_avg:84.18ms
step:14/1680 train_time:1185ms step_avg:84.61ms
step:15/1680 train_time:1272ms step_avg:84.83ms
step:16/1680 train_time:1360ms step_avg:85.02ms
step:17/1680 train_time:1447ms step_avg:85.13ms
step:18/1680 train_time:1535ms step_avg:85.26ms
step:19/1680 train_time:1621ms step_avg:85.34ms
step:20/1680 train_time:1708ms step_avg:85.39ms
step:21/1680 train_time:1794ms step_avg:85.42ms
step:22/1680 train_time:1880ms step_avg:85.47ms
step:23/1680 train_time:1967ms step_avg:85.52ms
step:24/1680 train_time:2055ms step_avg:85.62ms
step:25/1680 train_time:2144ms step_avg:85.76ms
step:26/1680 train_time:2232ms step_avg:85.86ms
step:27/1680 train_time:2320ms step_avg:85.92ms
step:28/1680 train_time:2408ms step_avg:86.00ms
step:29/1680 train_time:2495ms step_avg:86.04ms
step:30/1680 train_time:2583ms step_avg:86.09ms
step:31/1680 train_time:2670ms step_avg:86.11ms
step:32/1680 train_time:2756ms step_avg:86.11ms
step:33/1680 train_time:2842ms step_avg:86.12ms
step:34/1680 train_time:2928ms step_avg:86.13ms
step:35/1680 train_time:3015ms step_avg:86.15ms
step:36/1680 train_time:3103ms step_avg:86.20ms
step:37/1680 train_time:3192ms step_avg:86.26ms
step:38/1680 train_time:3280ms step_avg:86.32ms
step:39/1680 train_time:3368ms step_avg:86.36ms
step:40/1680 train_time:3455ms step_avg:86.38ms
step:41/1680 train_time:3543ms step_avg:86.41ms
step:42/1680 train_time:3629ms step_avg:86.41ms
step:43/1680 train_time:3715ms step_avg:86.41ms
step:44/1680 train_time:3802ms step_avg:86.42ms
step:45/1680 train_time:3889ms step_avg:86.43ms
step:46/1680 train_time:3975ms step_avg:86.42ms
step:47/1680 train_time:4063ms step_avg:86.44ms
step:48/1680 train_time:4150ms step_avg:86.47ms
step:49/1680 train_time:4238ms step_avg:86.48ms
step:50/1680 train_time:4325ms step_avg:86.50ms
step:51/1680 train_time:4413ms step_avg:86.52ms
step:52/1680 train_time:4500ms step_avg:86.54ms
step:53/1680 train_time:4588ms step_avg:86.57ms
step:54/1680 train_time:4675ms step_avg:86.58ms
step:55/1680 train_time:4762ms step_avg:86.59ms
step:56/1680 train_time:4849ms step_avg:86.58ms
step:57/1680 train_time:4936ms step_avg:86.59ms
step:58/1680 train_time:5023ms step_avg:86.61ms
step:59/1680 train_time:5110ms step_avg:86.61ms
step:60/1680 train_time:5198ms step_avg:86.63ms
step:61/1680 train_time:5285ms step_avg:86.64ms
step:62/1680 train_time:5372ms step_avg:86.65ms
step:63/1680 train_time:5460ms step_avg:86.67ms
step:64/1680 train_time:5548ms step_avg:86.68ms
step:65/1680 train_time:5634ms step_avg:86.68ms
step:66/1680 train_time:5721ms step_avg:86.69ms
step:67/1680 train_time:5808ms step_avg:86.69ms
step:68/1680 train_time:5895ms step_avg:86.69ms
step:69/1680 train_time:5982ms step_avg:86.69ms
step:70/1680 train_time:6069ms step_avg:86.70ms
step:71/1680 train_time:6157ms step_avg:86.71ms
step:72/1680 train_time:6244ms step_avg:86.73ms
step:73/1680 train_time:6331ms step_avg:86.73ms
step:74/1680 train_time:6419ms step_avg:86.74ms
step:75/1680 train_time:6505ms step_avg:86.74ms
step:76/1680 train_time:6592ms step_avg:86.74ms
step:77/1680 train_time:6680ms step_avg:86.76ms
step:78/1680 train_time:6767ms step_avg:86.76ms
step:79/1680 train_time:6854ms step_avg:86.76ms
step:80/1680 train_time:6941ms step_avg:86.77ms
step:81/1680 train_time:7028ms step_avg:86.76ms
step:82/1680 train_time:7115ms step_avg:86.77ms
step:83/1680 train_time:7203ms step_avg:86.78ms
step:84/1680 train_time:7290ms step_avg:86.78ms
step:85/1680 train_time:7377ms step_avg:86.79ms
step:86/1680 train_time:7465ms step_avg:86.80ms
step:87/1680 train_time:7552ms step_avg:86.80ms
step:88/1680 train_time:7640ms step_avg:86.82ms
step:89/1680 train_time:7727ms step_avg:86.82ms
step:90/1680 train_time:7814ms step_avg:86.82ms
step:91/1680 train_time:7901ms step_avg:86.83ms
step:92/1680 train_time:7988ms step_avg:86.83ms
step:93/1680 train_time:8075ms step_avg:86.83ms
step:94/1680 train_time:8162ms step_avg:86.83ms
step:95/1680 train_time:8250ms step_avg:86.84ms
step:96/1680 train_time:8337ms step_avg:86.85ms
step:97/1680 train_time:8425ms step_avg:86.85ms
step:98/1680 train_time:8512ms step_avg:86.85ms
step:99/1680 train_time:8600ms step_avg:86.86ms
step:100/1680 train_time:8686ms step_avg:86.86ms
step:101/1680 train_time:8774ms step_avg:86.87ms
step:102/1680 train_time:8861ms step_avg:86.87ms
step:103/1680 train_time:8947ms step_avg:86.87ms
step:104/1680 train_time:9034ms step_avg:86.87ms
step:105/1680 train_time:9122ms step_avg:86.87ms
step:106/1680 train_time:9208ms step_avg:86.87ms
step:107/1680 train_time:9295ms step_avg:86.87ms
step:108/1680 train_time:9383ms step_avg:86.88ms
step:109/1680 train_time:9470ms step_avg:86.88ms
step:110/1680 train_time:9556ms step_avg:86.88ms
step:111/1680 train_time:9644ms step_avg:86.88ms
step:112/1680 train_time:9730ms step_avg:86.88ms
step:113/1680 train_time:9818ms step_avg:86.89ms
step:114/1680 train_time:9905ms step_avg:86.89ms
step:115/1680 train_time:9992ms step_avg:86.89ms
step:116/1680 train_time:10079ms step_avg:86.89ms
step:117/1680 train_time:10166ms step_avg:86.89ms
step:118/1680 train_time:10253ms step_avg:86.89ms
step:119/1680 train_time:10340ms step_avg:86.89ms
step:120/1680 train_time:10426ms step_avg:86.88ms
step:121/1680 train_time:10513ms step_avg:86.89ms
step:122/1680 train_time:10600ms step_avg:86.89ms
step:123/1680 train_time:10687ms step_avg:86.89ms
step:124/1680 train_time:10774ms step_avg:86.89ms
step:125/1680 train_time:10862ms step_avg:86.89ms
step:125/1680 val_loss:4.3222 train_time:10950ms step_avg:87.60ms
step:126/1680 train_time:10969ms step_avg:87.06ms
step:127/1680 train_time:11038ms step_avg:86.91ms
step:128/1680 train_time:11133ms step_avg:86.97ms
step:129/1680 train_time:11225ms step_avg:87.02ms
step:130/1680 train_time:11312ms step_avg:87.01ms
step:131/1680 train_time:11397ms step_avg:87.00ms
step:132/1680 train_time:11483ms step_avg:86.99ms
step:133/1680 train_time:11569ms step_avg:86.98ms
step:134/1680 train_time:11655ms step_avg:86.98ms
step:135/1680 train_time:11740ms step_avg:86.96ms
step:136/1680 train_time:11826ms step_avg:86.96ms
step:137/1680 train_time:11913ms step_avg:86.95ms
step:138/1680 train_time:12000ms step_avg:86.95ms
step:139/1680 train_time:12089ms step_avg:86.97ms
step:140/1680 train_time:12178ms step_avg:86.98ms
step:141/1680 train_time:12267ms step_avg:87.00ms
step:142/1680 train_time:12354ms step_avg:87.00ms
step:143/1680 train_time:12441ms step_avg:87.00ms
step:144/1680 train_time:12528ms step_avg:87.00ms
step:145/1680 train_time:12614ms step_avg:86.99ms
step:146/1680 train_time:12700ms step_avg:86.99ms
step:147/1680 train_time:12787ms step_avg:86.98ms
step:148/1680 train_time:12873ms step_avg:86.98ms
step:149/1680 train_time:12959ms step_avg:86.98ms
step:150/1680 train_time:13047ms step_avg:86.98ms
step:151/1680 train_time:13135ms step_avg:86.98ms
step:152/1680 train_time:13222ms step_avg:86.99ms
step:153/1680 train_time:13310ms step_avg:86.99ms
step:154/1680 train_time:13397ms step_avg:86.99ms
step:155/1680 train_time:13484ms step_avg:86.99ms
step:156/1680 train_time:13571ms step_avg:87.00ms
step:157/1680 train_time:13658ms step_avg:86.99ms
step:158/1680 train_time:13745ms step_avg:86.99ms
step:159/1680 train_time:13832ms step_avg:86.99ms
step:160/1680 train_time:13919ms step_avg:86.99ms
step:161/1680 train_time:14006ms step_avg:86.99ms
step:162/1680 train_time:14094ms step_avg:87.00ms
step:163/1680 train_time:14180ms step_avg:87.00ms
step:164/1680 train_time:14268ms step_avg:87.00ms
step:165/1680 train_time:14355ms step_avg:87.00ms
step:166/1680 train_time:14442ms step_avg:87.00ms
step:167/1680 train_time:14529ms step_avg:87.00ms
step:168/1680 train_time:14616ms step_avg:87.00ms
step:169/1680 train_time:14703ms step_avg:87.00ms
step:170/1680 train_time:14791ms step_avg:87.00ms
step:171/1680 train_time:14877ms step_avg:87.00ms
step:172/1680 train_time:14964ms step_avg:87.00ms
step:173/1680 train_time:15052ms step_avg:87.01ms
step:174/1680 train_time:15139ms step_avg:87.00ms
step:175/1680 train_time:15226ms step_avg:87.01ms
step:176/1680 train_time:15313ms step_avg:87.01ms
step:177/1680 train_time:15400ms step_avg:87.01ms
step:178/1680 train_time:15487ms step_avg:87.01ms
step:179/1680 train_time:15574ms step_avg:87.01ms
step:180/1680 train_time:15661ms step_avg:87.01ms
step:181/1680 train_time:15747ms step_avg:87.00ms
step:182/1680 train_time:15834ms step_avg:87.00ms
step:183/1680 train_time:15920ms step_avg:87.00ms
step:184/1680 train_time:16007ms step_avg:87.00ms
step:185/1680 train_time:16094ms step_avg:87.00ms
step:186/1680 train_time:16181ms step_avg:86.99ms
step:187/1680 train_time:16268ms step_avg:87.00ms
step:188/1680 train_time:16355ms step_avg:87.00ms
step:189/1680 train_time:16442ms step_avg:87.00ms
step:190/1680 train_time:16529ms step_avg:87.00ms
step:191/1680 train_time:16616ms step_avg:86.99ms
step:192/1680 train_time:16703ms step_avg:87.00ms
step:193/1680 train_time:16791ms step_avg:87.00ms
step:194/1680 train_time:16877ms step_avg:86.99ms
step:195/1680 train_time:16964ms step_avg:87.00ms
step:196/1680 train_time:17051ms step_avg:87.00ms
step:197/1680 train_time:17138ms step_avg:87.00ms
step:198/1680 train_time:17226ms step_avg:87.00ms
step:199/1680 train_time:17313ms step_avg:87.00ms
step:200/1680 train_time:17400ms step_avg:87.00ms
step:201/1680 train_time:17487ms step_avg:87.00ms
step:202/1680 train_time:17574ms step_avg:87.00ms
step:203/1680 train_time:17661ms step_avg:87.00ms
step:204/1680 train_time:17748ms step_avg:87.00ms
step:205/1680 train_time:17835ms step_avg:87.00ms
step:206/1680 train_time:17922ms step_avg:87.00ms
step:207/1680 train_time:18009ms step_avg:87.00ms
step:208/1680 train_time:18096ms step_avg:87.00ms
step:209/1680 train_time:18183ms step_avg:87.00ms
step:210/1680 train_time:18271ms step_avg:87.01ms
step:211/1680 train_time:18358ms step_avg:87.00ms
step:212/1680 train_time:18445ms step_avg:87.00ms
step:213/1680 train_time:18532ms step_avg:87.01ms
step:214/1680 train_time:18619ms step_avg:87.00ms
step:215/1680 train_time:18706ms step_avg:87.01ms
step:216/1680 train_time:18794ms step_avg:87.01ms
step:217/1680 train_time:18880ms step_avg:87.00ms
step:218/1680 train_time:18967ms step_avg:87.00ms
step:219/1680 train_time:19054ms step_avg:87.01ms
step:220/1680 train_time:19141ms step_avg:87.00ms
step:221/1680 train_time:19228ms step_avg:87.00ms
step:222/1680 train_time:19315ms step_avg:87.01ms
step:223/1680 train_time:19402ms step_avg:87.01ms
step:224/1680 train_time:19490ms step_avg:87.01ms
step:225/1680 train_time:19576ms step_avg:87.01ms
step:226/1680 train_time:19663ms step_avg:87.01ms
step:227/1680 train_time:19751ms step_avg:87.01ms
step:228/1680 train_time:19837ms step_avg:87.01ms
step:229/1680 train_time:19925ms step_avg:87.01ms
step:230/1680 train_time:20012ms step_avg:87.01ms
step:231/1680 train_time:20099ms step_avg:87.01ms
step:232/1680 train_time:20185ms step_avg:87.01ms
step:233/1680 train_time:20272ms step_avg:87.00ms
step:234/1680 train_time:20358ms step_avg:87.00ms
step:235/1680 train_time:20446ms step_avg:87.00ms
step:236/1680 train_time:20534ms step_avg:87.01ms
step:237/1680 train_time:20620ms step_avg:87.01ms
step:238/1680 train_time:20708ms step_avg:87.01ms
step:239/1680 train_time:20794ms step_avg:87.01ms
step:240/1680 train_time:20881ms step_avg:87.00ms
step:241/1680 train_time:20968ms step_avg:87.00ms
step:242/1680 train_time:21054ms step_avg:87.00ms
step:243/1680 train_time:21141ms step_avg:87.00ms
step:244/1680 train_time:21227ms step_avg:87.00ms
step:245/1680 train_time:21314ms step_avg:87.00ms
step:246/1680 train_time:21401ms step_avg:87.00ms
step:247/1680 train_time:21489ms step_avg:87.00ms
step:248/1680 train_time:21576ms step_avg:87.00ms
step:249/1680 train_time:21663ms step_avg:87.00ms
step:250/1680 train_time:21751ms step_avg:87.00ms
step:250/1680 val_loss:3.9704 train_time:21839ms step_avg:87.35ms
step:251/1680 train_time:21857ms step_avg:87.08ms
step:252/1680 train_time:21929ms step_avg:87.02ms
step:253/1680 train_time:22019ms step_avg:87.03ms
step:254/1680 train_time:22106ms step_avg:87.03ms
step:255/1680 train_time:22194ms step_avg:87.03ms
step:256/1680 train_time:22279ms step_avg:87.03ms
step:257/1680 train_time:22365ms step_avg:87.02ms
step:258/1680 train_time:22452ms step_avg:87.02ms
step:259/1680 train_time:22538ms step_avg:87.02ms
step:260/1680 train_time:22625ms step_avg:87.02ms
step:261/1680 train_time:22711ms step_avg:87.02ms
step:262/1680 train_time:22798ms step_avg:87.02ms
step:263/1680 train_time:22887ms step_avg:87.02ms
step:264/1680 train_time:22976ms step_avg:87.03ms
step:265/1680 train_time:23064ms step_avg:87.03ms
step:266/1680 train_time:23151ms step_avg:87.04ms
step:267/1680 train_time:23238ms step_avg:87.03ms
step:268/1680 train_time:23325ms step_avg:87.03ms
step:269/1680 train_time:23411ms step_avg:87.03ms
step:270/1680 train_time:23497ms step_avg:87.03ms
step:271/1680 train_time:23584ms step_avg:87.02ms
step:272/1680 train_time:23671ms step_avg:87.02ms
step:273/1680 train_time:23758ms step_avg:87.02ms
step:274/1680 train_time:23845ms step_avg:87.03ms
step:275/1680 train_time:23932ms step_avg:87.03ms
step:276/1680 train_time:24020ms step_avg:87.03ms
step:277/1680 train_time:24107ms step_avg:87.03ms
step:278/1680 train_time:24195ms step_avg:87.03ms
step:279/1680 train_time:24281ms step_avg:87.03ms
step:280/1680 train_time:24368ms step_avg:87.03ms
step:281/1680 train_time:24454ms step_avg:87.03ms
step:282/1680 train_time:24540ms step_avg:87.02ms
step:283/1680 train_time:24627ms step_avg:87.02ms
step:284/1680 train_time:24713ms step_avg:87.02ms
step:285/1680 train_time:24800ms step_avg:87.02ms
step:286/1680 train_time:24888ms step_avg:87.02ms
step:287/1680 train_time:24975ms step_avg:87.02ms
step:288/1680 train_time:25063ms step_avg:87.02ms
step:289/1680 train_time:25150ms step_avg:87.03ms
step:290/1680 train_time:25237ms step_avg:87.02ms
step:291/1680 train_time:25324ms step_avg:87.03ms
step:292/1680 train_time:25411ms step_avg:87.03ms
step:293/1680 train_time:25498ms step_avg:87.02ms
step:294/1680 train_time:25585ms step_avg:87.02ms
step:295/1680 train_time:25673ms step_avg:87.03ms
step:296/1680 train_time:25759ms step_avg:87.02ms
step:297/1680 train_time:25847ms step_avg:87.03ms
step:298/1680 train_time:25934ms step_avg:87.03ms
step:299/1680 train_time:26021ms step_avg:87.03ms
step:300/1680 train_time:26108ms step_avg:87.03ms
step:301/1680 train_time:26195ms step_avg:87.03ms
step:302/1680 train_time:26282ms step_avg:87.03ms
step:303/1680 train_time:26369ms step_avg:87.03ms
step:304/1680 train_time:26456ms step_avg:87.02ms
step:305/1680 train_time:26542ms step_avg:87.02ms
step:306/1680 train_time:26628ms step_avg:87.02ms
step:307/1680 train_time:26715ms step_avg:87.02ms
step:308/1680 train_time:26802ms step_avg:87.02ms
step:309/1680 train_time:26889ms step_avg:87.02ms
step:310/1680 train_time:26976ms step_avg:87.02ms
step:311/1680 train_time:27063ms step_avg:87.02ms
step:312/1680 train_time:27151ms step_avg:87.02ms
step:313/1680 train_time:27238ms step_avg:87.02ms
step:314/1680 train_time:27325ms step_avg:87.02ms
step:315/1680 train_time:27412ms step_avg:87.02ms
step:316/1680 train_time:27499ms step_avg:87.02ms
step:317/1680 train_time:27585ms step_avg:87.02ms
step:318/1680 train_time:27672ms step_avg:87.02ms
step:319/1680 train_time:27759ms step_avg:87.02ms
step:320/1680 train_time:27846ms step_avg:87.02ms
step:321/1680 train_time:27933ms step_avg:87.02ms
step:322/1680 train_time:28019ms step_avg:87.02ms
step:323/1680 train_time:28107ms step_avg:87.02ms
step:324/1680 train_time:28194ms step_avg:87.02ms
step:325/1680 train_time:28281ms step_avg:87.02ms
step:326/1680 train_time:28368ms step_avg:87.02ms
step:327/1680 train_time:28455ms step_avg:87.02ms
step:328/1680 train_time:28541ms step_avg:87.01ms
step:329/1680 train_time:28628ms step_avg:87.01ms
step:330/1680 train_time:28715ms step_avg:87.02ms
step:331/1680 train_time:28802ms step_avg:87.01ms
step:332/1680 train_time:28889ms step_avg:87.01ms
step:333/1680 train_time:28976ms step_avg:87.01ms
step:334/1680 train_time:29063ms step_avg:87.01ms
step:335/1680 train_time:29150ms step_avg:87.01ms
step:336/1680 train_time:29237ms step_avg:87.01ms
step:337/1680 train_time:29324ms step_avg:87.01ms
step:338/1680 train_time:29410ms step_avg:87.01ms
step:339/1680 train_time:29497ms step_avg:87.01ms
step:340/1680 train_time:29584ms step_avg:87.01ms
step:341/1680 train_time:29672ms step_avg:87.01ms
step:342/1680 train_time:29758ms step_avg:87.01ms
step:343/1680 train_time:29845ms step_avg:87.01ms
step:344/1680 train_time:29933ms step_avg:87.01ms
step:345/1680 train_time:30020ms step_avg:87.01ms
step:346/1680 train_time:30107ms step_avg:87.01ms
step:347/1680 train_time:30194ms step_avg:87.01ms
step:348/1680 train_time:30281ms step_avg:87.01ms
step:349/1680 train_time:30368ms step_avg:87.01ms
step:350/1680 train_time:30455ms step_avg:87.01ms
step:351/1680 train_time:30541ms step_avg:87.01ms
step:352/1680 train_time:30628ms step_avg:87.01ms
step:353/1680 train_time:30715ms step_avg:87.01ms
step:354/1680 train_time:30802ms step_avg:87.01ms
step:355/1680 train_time:30889ms step_avg:87.01ms
step:356/1680 train_time:30977ms step_avg:87.01ms
step:357/1680 train_time:31064ms step_avg:87.01ms
step:358/1680 train_time:31152ms step_avg:87.02ms
step:359/1680 train_time:31238ms step_avg:87.01ms
step:360/1680 train_time:31325ms step_avg:87.01ms
step:361/1680 train_time:31412ms step_avg:87.01ms
step:362/1680 train_time:31499ms step_avg:87.01ms
step:363/1680 train_time:31586ms step_avg:87.01ms
step:364/1680 train_time:31674ms step_avg:87.02ms
step:365/1680 train_time:31760ms step_avg:87.01ms
step:366/1680 train_time:31847ms step_avg:87.01ms
step:367/1680 train_time:31933ms step_avg:87.01ms
step:368/1680 train_time:32020ms step_avg:87.01ms
step:369/1680 train_time:32107ms step_avg:87.01ms
step:370/1680 train_time:32195ms step_avg:87.01ms
step:371/1680 train_time:32281ms step_avg:87.01ms
step:372/1680 train_time:32368ms step_avg:87.01ms
step:373/1680 train_time:32455ms step_avg:87.01ms
step:374/1680 train_time:32541ms step_avg:87.01ms
step:375/1680 train_time:32628ms step_avg:87.01ms
step:375/1680 val_loss:3.8212 train_time:32716ms step_avg:87.24ms
step:376/1680 train_time:32736ms step_avg:87.06ms
step:377/1680 train_time:32805ms step_avg:87.02ms
step:378/1680 train_time:32895ms step_avg:87.02ms
step:379/1680 train_time:32983ms step_avg:87.03ms
step:380/1680 train_time:33070ms step_avg:87.03ms
step:381/1680 train_time:33157ms step_avg:87.03ms
step:382/1680 train_time:33245ms step_avg:87.03ms
step:383/1680 train_time:33331ms step_avg:87.03ms
step:384/1680 train_time:33417ms step_avg:87.02ms
step:385/1680 train_time:33504ms step_avg:87.02ms
step:386/1680 train_time:33590ms step_avg:87.02ms
step:387/1680 train_time:33677ms step_avg:87.02ms
step:388/1680 train_time:33764ms step_avg:87.02ms
step:389/1680 train_time:33853ms step_avg:87.03ms
step:390/1680 train_time:33941ms step_avg:87.03ms
step:391/1680 train_time:34028ms step_avg:87.03ms
step:392/1680 train_time:34115ms step_avg:87.03ms
step:393/1680 train_time:34202ms step_avg:87.03ms
step:394/1680 train_time:34289ms step_avg:87.03ms
step:395/1680 train_time:34375ms step_avg:87.03ms
step:396/1680 train_time:34462ms step_avg:87.03ms
step:397/1680 train_time:34548ms step_avg:87.02ms
step:398/1680 train_time:34635ms step_avg:87.02ms
step:399/1680 train_time:34722ms step_avg:87.02ms
step:400/1680 train_time:34811ms step_avg:87.03ms
step:401/1680 train_time:34899ms step_avg:87.03ms
step:402/1680 train_time:34987ms step_avg:87.03ms
step:403/1680 train_time:35074ms step_avg:87.03ms
step:404/1680 train_time:35161ms step_avg:87.03ms
step:405/1680 train_time:35249ms step_avg:87.03ms
step:406/1680 train_time:35336ms step_avg:87.03ms
step:407/1680 train_time:35423ms step_avg:87.03ms
step:408/1680 train_time:35509ms step_avg:87.03ms
step:409/1680 train_time:35596ms step_avg:87.03ms
step:410/1680 train_time:35682ms step_avg:87.03ms
step:411/1680 train_time:35769ms step_avg:87.03ms
step:412/1680 train_time:35857ms step_avg:87.03ms
step:413/1680 train_time:35944ms step_avg:87.03ms
step:414/1680 train_time:36031ms step_avg:87.03ms
step:415/1680 train_time:36118ms step_avg:87.03ms
step:416/1680 train_time:36206ms step_avg:87.03ms
step:417/1680 train_time:36293ms step_avg:87.03ms
step:418/1680 train_time:36380ms step_avg:87.03ms
step:419/1680 train_time:36466ms step_avg:87.03ms
step:420/1680 train_time:36553ms step_avg:87.03ms
step:421/1680 train_time:36639ms step_avg:87.03ms
step:422/1680 train_time:36726ms step_avg:87.03ms
step:423/1680 train_time:36814ms step_avg:87.03ms
step:424/1680 train_time:36900ms step_avg:87.03ms
step:425/1680 train_time:36988ms step_avg:87.03ms
step:426/1680 train_time:37075ms step_avg:87.03ms
step:427/1680 train_time:37162ms step_avg:87.03ms
step:428/1680 train_time:37250ms step_avg:87.03ms
step:429/1680 train_time:37336ms step_avg:87.03ms
step:430/1680 train_time:37423ms step_avg:87.03ms
step:431/1680 train_time:37510ms step_avg:87.03ms
step:432/1680 train_time:37597ms step_avg:87.03ms
step:433/1680 train_time:37684ms step_avg:87.03ms
step:434/1680 train_time:37770ms step_avg:87.03ms
step:435/1680 train_time:37857ms step_avg:87.03ms
step:436/1680 train_time:37945ms step_avg:87.03ms
step:437/1680 train_time:38033ms step_avg:87.03ms
step:438/1680 train_time:38120ms step_avg:87.03ms
step:439/1680 train_time:38207ms step_avg:87.03ms
step:440/1680 train_time:38294ms step_avg:87.03ms
step:441/1680 train_time:38381ms step_avg:87.03ms
step:442/1680 train_time:38468ms step_avg:87.03ms
step:443/1680 train_time:38556ms step_avg:87.03ms
step:444/1680 train_time:38643ms step_avg:87.03ms
step:445/1680 train_time:38729ms step_avg:87.03ms
step:446/1680 train_time:38815ms step_avg:87.03ms
step:447/1680 train_time:38902ms step_avg:87.03ms
step:448/1680 train_time:38990ms step_avg:87.03ms
step:449/1680 train_time:39078ms step_avg:87.03ms
step:450/1680 train_time:39165ms step_avg:87.03ms
step:451/1680 train_time:39252ms step_avg:87.03ms
step:452/1680 train_time:39338ms step_avg:87.03ms
step:453/1680 train_time:39425ms step_avg:87.03ms
step:454/1680 train_time:39513ms step_avg:87.03ms
step:455/1680 train_time:39599ms step_avg:87.03ms
step:456/1680 train_time:39686ms step_avg:87.03ms
step:457/1680 train_time:39773ms step_avg:87.03ms
step:458/1680 train_time:39860ms step_avg:87.03ms
step:459/1680 train_time:39948ms step_avg:87.03ms
step:460/1680 train_time:40036ms step_avg:87.03ms
step:461/1680 train_time:40123ms step_avg:87.03ms
step:462/1680 train_time:40209ms step_avg:87.03ms
step:463/1680 train_time:40295ms step_avg:87.03ms
step:464/1680 train_time:40382ms step_avg:87.03ms
step:465/1680 train_time:40470ms step_avg:87.03ms
step:466/1680 train_time:40557ms step_avg:87.03ms
step:467/1680 train_time:40643ms step_avg:87.03ms
step:468/1680 train_time:40730ms step_avg:87.03ms
step:469/1680 train_time:40817ms step_avg:87.03ms
step:470/1680 train_time:40904ms step_avg:87.03ms
step:471/1680 train_time:40990ms step_avg:87.03ms
step:472/1680 train_time:41077ms step_avg:87.03ms
step:473/1680 train_time:41165ms step_avg:87.03ms
step:474/1680 train_time:41252ms step_avg:87.03ms
step:475/1680 train_time:41339ms step_avg:87.03ms
step:476/1680 train_time:41426ms step_avg:87.03ms
step:477/1680 train_time:41514ms step_avg:87.03ms
step:478/1680 train_time:41601ms step_avg:87.03ms
step:479/1680 train_time:41688ms step_avg:87.03ms
step:480/1680 train_time:41776ms step_avg:87.03ms
step:481/1680 train_time:41862ms step_avg:87.03ms
step:482/1680 train_time:41949ms step_avg:87.03ms
step:483/1680 train_time:42036ms step_avg:87.03ms
step:484/1680 train_time:42123ms step_avg:87.03ms
step:485/1680 train_time:42211ms step_avg:87.03ms
step:486/1680 train_time:42298ms step_avg:87.03ms
step:487/1680 train_time:42385ms step_avg:87.03ms
step:488/1680 train_time:42472ms step_avg:87.03ms
step:489/1680 train_time:42559ms step_avg:87.03ms
step:490/1680 train_time:42646ms step_avg:87.03ms
step:491/1680 train_time:42733ms step_avg:87.03ms
step:492/1680 train_time:42819ms step_avg:87.03ms
step:493/1680 train_time:42906ms step_avg:87.03ms
step:494/1680 train_time:42993ms step_avg:87.03ms
step:495/1680 train_time:43080ms step_avg:87.03ms
step:496/1680 train_time:43167ms step_avg:87.03ms
step:497/1680 train_time:43255ms step_avg:87.03ms
step:498/1680 train_time:43341ms step_avg:87.03ms
step:499/1680 train_time:43429ms step_avg:87.03ms
step:500/1680 train_time:43516ms step_avg:87.03ms
step:500/1680 val_loss:3.7171 train_time:43604ms step_avg:87.21ms
step:501/1680 train_time:43622ms step_avg:87.07ms
step:502/1680 train_time:43694ms step_avg:87.04ms
step:503/1680 train_time:43784ms step_avg:87.04ms
step:504/1680 train_time:43871ms step_avg:87.05ms
step:505/1680 train_time:43958ms step_avg:87.05ms
step:506/1680 train_time:44043ms step_avg:87.04ms
step:507/1680 train_time:44130ms step_avg:87.04ms
step:508/1680 train_time:44216ms step_avg:87.04ms
step:509/1680 train_time:44302ms step_avg:87.04ms
step:510/1680 train_time:44389ms step_avg:87.04ms
step:511/1680 train_time:44475ms step_avg:87.04ms
step:512/1680 train_time:44562ms step_avg:87.04ms
step:513/1680 train_time:44651ms step_avg:87.04ms
step:514/1680 train_time:44739ms step_avg:87.04ms
step:515/1680 train_time:44828ms step_avg:87.04ms
step:516/1680 train_time:44916ms step_avg:87.05ms
step:517/1680 train_time:45002ms step_avg:87.04ms
step:518/1680 train_time:45089ms step_avg:87.04ms
step:519/1680 train_time:45175ms step_avg:87.04ms
step:520/1680 train_time:45261ms step_avg:87.04ms
step:521/1680 train_time:45347ms step_avg:87.04ms
step:522/1680 train_time:45434ms step_avg:87.04ms
step:523/1680 train_time:45521ms step_avg:87.04ms
step:524/1680 train_time:45608ms step_avg:87.04ms
step:525/1680 train_time:45696ms step_avg:87.04ms
step:526/1680 train_time:45784ms step_avg:87.04ms
step:527/1680 train_time:45871ms step_avg:87.04ms
step:528/1680 train_time:45958ms step_avg:87.04ms
step:529/1680 train_time:46045ms step_avg:87.04ms
step:530/1680 train_time:46132ms step_avg:87.04ms
step:531/1680 train_time:46219ms step_avg:87.04ms
step:532/1680 train_time:46306ms step_avg:87.04ms
step:533/1680 train_time:46393ms step_avg:87.04ms
step:534/1680 train_time:46480ms step_avg:87.04ms
step:535/1680 train_time:46567ms step_avg:87.04ms
step:536/1680 train_time:46654ms step_avg:87.04ms
step:537/1680 train_time:46742ms step_avg:87.04ms
step:538/1680 train_time:46830ms step_avg:87.04ms
step:539/1680 train_time:46917ms step_avg:87.05ms
step:540/1680 train_time:47004ms step_avg:87.04ms
step:541/1680 train_time:47091ms step_avg:87.04ms
step:542/1680 train_time:47178ms step_avg:87.04ms
step:543/1680 train_time:47264ms step_avg:87.04ms
step:544/1680 train_time:47351ms step_avg:87.04ms
step:545/1680 train_time:47438ms step_avg:87.04ms
step:546/1680 train_time:47525ms step_avg:87.04ms
step:547/1680 train_time:47612ms step_avg:87.04ms
step:548/1680 train_time:47699ms step_avg:87.04ms
step:549/1680 train_time:47788ms step_avg:87.04ms
step:550/1680 train_time:47877ms step_avg:87.05ms
step:551/1680 train_time:47965ms step_avg:87.05ms
step:552/1680 train_time:48053ms step_avg:87.05ms
step:553/1680 train_time:48141ms step_avg:87.05ms
step:554/1680 train_time:48229ms step_avg:87.06ms
step:555/1680 train_time:48318ms step_avg:87.06ms
step:556/1680 train_time:48405ms step_avg:87.06ms
step:557/1680 train_time:48493ms step_avg:87.06ms
step:558/1680 train_time:48581ms step_avg:87.06ms
step:559/1680 train_time:48670ms step_avg:87.07ms
step:560/1680 train_time:48759ms step_avg:87.07ms
step:561/1680 train_time:48847ms step_avg:87.07ms
step:562/1680 train_time:48937ms step_avg:87.08ms
step:563/1680 train_time:49025ms step_avg:87.08ms
step:564/1680 train_time:49113ms step_avg:87.08ms
step:565/1680 train_time:49200ms step_avg:87.08ms
step:566/1680 train_time:49288ms step_avg:87.08ms
step:567/1680 train_time:49377ms step_avg:87.08ms
step:568/1680 train_time:49464ms step_avg:87.08ms
step:569/1680 train_time:49552ms step_avg:87.09ms
step:570/1680 train_time:49640ms step_avg:87.09ms
step:571/1680 train_time:49729ms step_avg:87.09ms
step:572/1680 train_time:49817ms step_avg:87.09ms
step:573/1680 train_time:49905ms step_avg:87.09ms
step:574/1680 train_time:49993ms step_avg:87.10ms
step:575/1680 train_time:50082ms step_avg:87.10ms
step:576/1680 train_time:50170ms step_avg:87.10ms
step:577/1680 train_time:50258ms step_avg:87.10ms
step:578/1680 train_time:50346ms step_avg:87.10ms
step:579/1680 train_time:50434ms step_avg:87.11ms
step:580/1680 train_time:50522ms step_avg:87.11ms
step:581/1680 train_time:50611ms step_avg:87.11ms
step:582/1680 train_time:50700ms step_avg:87.11ms
step:583/1680 train_time:50788ms step_avg:87.11ms
step:584/1680 train_time:50877ms step_avg:87.12ms
step:585/1680 train_time:50964ms step_avg:87.12ms
step:586/1680 train_time:51052ms step_avg:87.12ms
step:587/1680 train_time:51140ms step_avg:87.12ms
step:588/1680 train_time:51228ms step_avg:87.12ms
step:589/1680 train_time:51316ms step_avg:87.12ms
step:590/1680 train_time:51405ms step_avg:87.13ms
step:591/1680 train_time:51493ms step_avg:87.13ms
step:592/1680 train_time:51581ms step_avg:87.13ms
step:593/1680 train_time:51669ms step_avg:87.13ms
step:594/1680 train_time:51757ms step_avg:87.13ms
step:595/1680 train_time:51845ms step_avg:87.13ms
step:596/1680 train_time:51933ms step_avg:87.14ms
step:597/1680 train_time:52021ms step_avg:87.14ms
step:598/1680 train_time:52109ms step_avg:87.14ms
step:599/1680 train_time:52198ms step_avg:87.14ms
step:600/1680 train_time:52286ms step_avg:87.14ms
step:601/1680 train_time:52374ms step_avg:87.15ms
step:602/1680 train_time:52462ms step_avg:87.15ms
step:603/1680 train_time:52551ms step_avg:87.15ms
step:604/1680 train_time:52639ms step_avg:87.15ms
step:605/1680 train_time:52727ms step_avg:87.15ms
step:606/1680 train_time:52815ms step_avg:87.15ms
step:607/1680 train_time:52903ms step_avg:87.16ms
step:608/1680 train_time:52991ms step_avg:87.16ms
step:609/1680 train_time:53079ms step_avg:87.16ms
step:610/1680 train_time:53167ms step_avg:87.16ms
step:611/1680 train_time:53255ms step_avg:87.16ms
step:612/1680 train_time:53344ms step_avg:87.16ms
step:613/1680 train_time:53432ms step_avg:87.16ms
step:614/1680 train_time:53520ms step_avg:87.17ms
step:615/1680 train_time:53608ms step_avg:87.17ms
step:616/1680 train_time:53696ms step_avg:87.17ms
step:617/1680 train_time:53784ms step_avg:87.17ms
step:618/1680 train_time:53872ms step_avg:87.17ms
step:619/1680 train_time:53960ms step_avg:87.17ms
step:620/1680 train_time:54047ms step_avg:87.17ms
step:621/1680 train_time:54135ms step_avg:87.17ms
step:622/1680 train_time:54223ms step_avg:87.18ms
step:623/1680 train_time:54312ms step_avg:87.18ms
step:624/1680 train_time:54399ms step_avg:87.18ms
step:625/1680 train_time:54488ms step_avg:87.18ms
step:625/1680 val_loss:3.6172 train_time:54578ms step_avg:87.33ms
step:626/1680 train_time:54598ms step_avg:87.22ms
step:627/1680 train_time:54667ms step_avg:87.19ms
step:628/1680 train_time:54757ms step_avg:87.19ms
step:629/1680 train_time:54848ms step_avg:87.20ms
step:630/1680 train_time:54937ms step_avg:87.20ms
step:631/1680 train_time:55025ms step_avg:87.20ms
step:632/1680 train_time:55112ms step_avg:87.20ms
step:633/1680 train_time:55198ms step_avg:87.20ms
step:634/1680 train_time:55285ms step_avg:87.20ms
step:635/1680 train_time:55373ms step_avg:87.20ms
step:636/1680 train_time:55462ms step_avg:87.20ms
step:637/1680 train_time:55554ms step_avg:87.21ms
step:638/1680 train_time:55644ms step_avg:87.22ms
step:639/1680 train_time:55733ms step_avg:87.22ms
step:640/1680 train_time:55823ms step_avg:87.22ms
step:641/1680 train_time:55910ms step_avg:87.22ms
step:642/1680 train_time:55998ms step_avg:87.22ms
step:643/1680 train_time:56086ms step_avg:87.23ms
step:644/1680 train_time:56173ms step_avg:87.23ms
step:645/1680 train_time:56261ms step_avg:87.23ms
step:646/1680 train_time:56348ms step_avg:87.23ms
step:647/1680 train_time:56435ms step_avg:87.23ms
step:648/1680 train_time:56524ms step_avg:87.23ms
step:649/1680 train_time:56612ms step_avg:87.23ms
step:650/1680 train_time:56701ms step_avg:87.23ms
step:651/1680 train_time:56789ms step_avg:87.23ms
step:652/1680 train_time:56878ms step_avg:87.24ms
step:653/1680 train_time:56966ms step_avg:87.24ms
step:654/1680 train_time:57055ms step_avg:87.24ms
step:655/1680 train_time:57143ms step_avg:87.24ms
step:656/1680 train_time:57230ms step_avg:87.24ms
step:657/1680 train_time:57317ms step_avg:87.24ms
step:658/1680 train_time:57405ms step_avg:87.24ms
step:659/1680 train_time:57493ms step_avg:87.24ms
step:660/1680 train_time:57581ms step_avg:87.24ms
step:661/1680 train_time:57670ms step_avg:87.25ms
step:662/1680 train_time:57759ms step_avg:87.25ms
step:663/1680 train_time:57848ms step_avg:87.25ms
step:664/1680 train_time:57936ms step_avg:87.25ms
step:665/1680 train_time:58026ms step_avg:87.26ms
step:666/1680 train_time:58113ms step_avg:87.26ms
step:667/1680 train_time:58201ms step_avg:87.26ms
step:668/1680 train_time:58289ms step_avg:87.26ms
step:669/1680 train_time:58376ms step_avg:87.26ms
step:670/1680 train_time:58464ms step_avg:87.26ms
step:671/1680 train_time:58552ms step_avg:87.26ms
step:672/1680 train_time:58640ms step_avg:87.26ms
step:673/1680 train_time:58729ms step_avg:87.26ms
step:674/1680 train_time:58817ms step_avg:87.27ms
step:675/1680 train_time:58905ms step_avg:87.27ms
step:676/1680 train_time:58993ms step_avg:87.27ms
step:677/1680 train_time:59082ms step_avg:87.27ms
step:678/1680 train_time:59170ms step_avg:87.27ms
step:679/1680 train_time:59257ms step_avg:87.27ms
step:680/1680 train_time:59345ms step_avg:87.27ms
step:681/1680 train_time:59433ms step_avg:87.27ms
step:682/1680 train_time:59522ms step_avg:87.27ms
step:683/1680 train_time:59609ms step_avg:87.28ms
step:684/1680 train_time:59697ms step_avg:87.28ms
step:685/1680 train_time:59786ms step_avg:87.28ms
step:686/1680 train_time:59874ms step_avg:87.28ms
step:687/1680 train_time:59964ms step_avg:87.28ms
step:688/1680 train_time:60052ms step_avg:87.28ms
step:689/1680 train_time:60139ms step_avg:87.28ms
step:690/1680 train_time:60227ms step_avg:87.29ms
step:691/1680 train_time:60314ms step_avg:87.29ms
step:692/1680 train_time:60403ms step_avg:87.29ms
step:693/1680 train_time:60491ms step_avg:87.29ms
step:694/1680 train_time:60579ms step_avg:87.29ms
step:695/1680 train_time:60667ms step_avg:87.29ms
step:696/1680 train_time:60755ms step_avg:87.29ms
step:697/1680 train_time:60843ms step_avg:87.29ms
step:698/1680 train_time:60932ms step_avg:87.29ms
step:699/1680 train_time:61020ms step_avg:87.30ms
step:700/1680 train_time:61108ms step_avg:87.30ms
step:701/1680 train_time:61196ms step_avg:87.30ms
step:702/1680 train_time:61284ms step_avg:87.30ms
step:703/1680 train_time:61372ms step_avg:87.30ms
step:704/1680 train_time:61460ms step_avg:87.30ms
step:705/1680 train_time:61548ms step_avg:87.30ms
step:706/1680 train_time:61636ms step_avg:87.30ms
step:707/1680 train_time:61725ms step_avg:87.30ms
step:708/1680 train_time:61813ms step_avg:87.31ms
step:709/1680 train_time:61901ms step_avg:87.31ms
step:710/1680 train_time:61989ms step_avg:87.31ms
step:711/1680 train_time:62078ms step_avg:87.31ms
step:712/1680 train_time:62167ms step_avg:87.31ms
step:713/1680 train_time:62255ms step_avg:87.31ms
step:714/1680 train_time:62344ms step_avg:87.32ms
step:715/1680 train_time:62431ms step_avg:87.32ms
step:716/1680 train_time:62519ms step_avg:87.32ms
step:717/1680 train_time:62606ms step_avg:87.32ms
step:718/1680 train_time:62694ms step_avg:87.32ms
step:719/1680 train_time:62782ms step_avg:87.32ms
step:720/1680 train_time:62871ms step_avg:87.32ms
step:721/1680 train_time:62959ms step_avg:87.32ms
step:722/1680 train_time:63047ms step_avg:87.32ms
step:723/1680 train_time:63135ms step_avg:87.32ms
step:724/1680 train_time:63223ms step_avg:87.32ms
step:725/1680 train_time:63311ms step_avg:87.33ms
step:726/1680 train_time:63399ms step_avg:87.33ms
step:727/1680 train_time:63487ms step_avg:87.33ms
step:728/1680 train_time:63575ms step_avg:87.33ms
step:729/1680 train_time:63664ms step_avg:87.33ms
step:730/1680 train_time:63752ms step_avg:87.33ms
step:731/1680 train_time:63840ms step_avg:87.33ms
step:732/1680 train_time:63928ms step_avg:87.33ms
step:733/1680 train_time:64017ms step_avg:87.34ms
step:734/1680 train_time:64105ms step_avg:87.34ms
step:735/1680 train_time:64193ms step_avg:87.34ms
step:736/1680 train_time:64280ms step_avg:87.34ms
step:737/1680 train_time:64368ms step_avg:87.34ms
step:738/1680 train_time:64456ms step_avg:87.34ms
step:739/1680 train_time:64545ms step_avg:87.34ms
step:740/1680 train_time:64632ms step_avg:87.34ms
step:741/1680 train_time:64721ms step_avg:87.34ms
step:742/1680 train_time:64808ms step_avg:87.34ms
step:743/1680 train_time:64896ms step_avg:87.34ms
step:744/1680 train_time:64985ms step_avg:87.35ms
step:745/1680 train_time:65074ms step_avg:87.35ms
step:746/1680 train_time:65163ms step_avg:87.35ms
step:747/1680 train_time:65251ms step_avg:87.35ms
step:748/1680 train_time:65339ms step_avg:87.35ms
step:749/1680 train_time:65427ms step_avg:87.35ms
step:750/1680 train_time:65515ms step_avg:87.35ms
step:750/1680 val_loss:3.5643 train_time:65605ms step_avg:87.47ms
step:751/1680 train_time:65623ms step_avg:87.38ms
step:752/1680 train_time:65695ms step_avg:87.36ms
step:753/1680 train_time:65789ms step_avg:87.37ms
step:754/1680 train_time:65878ms step_avg:87.37ms
step:755/1680 train_time:65966ms step_avg:87.37ms
step:756/1680 train_time:66053ms step_avg:87.37ms
step:757/1680 train_time:66141ms step_avg:87.37ms
step:758/1680 train_time:66228ms step_avg:87.37ms
step:759/1680 train_time:66315ms step_avg:87.37ms
step:760/1680 train_time:66402ms step_avg:87.37ms
step:761/1680 train_time:66490ms step_avg:87.37ms
step:762/1680 train_time:66578ms step_avg:87.37ms
step:763/1680 train_time:66668ms step_avg:87.38ms
step:764/1680 train_time:66759ms step_avg:87.38ms
step:765/1680 train_time:66849ms step_avg:87.38ms
step:766/1680 train_time:66937ms step_avg:87.39ms
step:767/1680 train_time:67025ms step_avg:87.39ms
step:768/1680 train_time:67113ms step_avg:87.39ms
step:769/1680 train_time:67200ms step_avg:87.39ms
step:770/1680 train_time:67287ms step_avg:87.39ms
step:771/1680 train_time:67374ms step_avg:87.39ms
step:772/1680 train_time:67462ms step_avg:87.39ms
step:773/1680 train_time:67550ms step_avg:87.39ms
step:774/1680 train_time:67638ms step_avg:87.39ms
step:775/1680 train_time:67727ms step_avg:87.39ms
step:776/1680 train_time:67817ms step_avg:87.39ms
step:777/1680 train_time:67906ms step_avg:87.40ms
step:778/1680 train_time:67994ms step_avg:87.40ms
step:779/1680 train_time:68082ms step_avg:87.40ms
step:780/1680 train_time:68170ms step_avg:87.40ms
step:781/1680 train_time:68257ms step_avg:87.40ms
step:782/1680 train_time:68345ms step_avg:87.40ms
step:783/1680 train_time:68432ms step_avg:87.40ms
step:784/1680 train_time:68520ms step_avg:87.40ms
step:785/1680 train_time:68608ms step_avg:87.40ms
step:786/1680 train_time:68696ms step_avg:87.40ms
step:787/1680 train_time:68786ms step_avg:87.40ms
step:788/1680 train_time:68876ms step_avg:87.41ms
step:789/1680 train_time:68964ms step_avg:87.41ms
step:790/1680 train_time:69052ms step_avg:87.41ms
step:791/1680 train_time:69140ms step_avg:87.41ms
step:792/1680 train_time:69227ms step_avg:87.41ms
step:793/1680 train_time:69315ms step_avg:87.41ms
step:794/1680 train_time:69403ms step_avg:87.41ms
step:795/1680 train_time:69491ms step_avg:87.41ms
step:796/1680 train_time:69579ms step_avg:87.41ms
step:797/1680 train_time:69668ms step_avg:87.41ms
step:798/1680 train_time:69757ms step_avg:87.41ms
step:799/1680 train_time:69845ms step_avg:87.42ms
step:800/1680 train_time:69934ms step_avg:87.42ms
step:801/1680 train_time:70022ms step_avg:87.42ms
step:802/1680 train_time:70110ms step_avg:87.42ms
step:803/1680 train_time:70198ms step_avg:87.42ms
step:804/1680 train_time:70286ms step_avg:87.42ms
step:805/1680 train_time:70374ms step_avg:87.42ms
step:806/1680 train_time:70461ms step_avg:87.42ms
step:807/1680 train_time:70550ms step_avg:87.42ms
step:808/1680 train_time:70638ms step_avg:87.42ms
step:809/1680 train_time:70726ms step_avg:87.42ms
step:810/1680 train_time:70815ms step_avg:87.43ms
step:811/1680 train_time:70904ms step_avg:87.43ms
step:812/1680 train_time:70992ms step_avg:87.43ms
step:813/1680 train_time:71081ms step_avg:87.43ms
step:814/1680 train_time:71168ms step_avg:87.43ms
step:815/1680 train_time:71256ms step_avg:87.43ms
step:816/1680 train_time:71344ms step_avg:87.43ms
step:817/1680 train_time:71432ms step_avg:87.43ms
step:818/1680 train_time:71520ms step_avg:87.43ms
step:819/1680 train_time:71607ms step_avg:87.43ms
step:820/1680 train_time:71696ms step_avg:87.43ms
step:821/1680 train_time:71785ms step_avg:87.44ms
step:822/1680 train_time:71874ms step_avg:87.44ms
step:823/1680 train_time:71962ms step_avg:87.44ms
step:824/1680 train_time:72050ms step_avg:87.44ms
step:825/1680 train_time:72138ms step_avg:87.44ms
step:826/1680 train_time:72226ms step_avg:87.44ms
step:827/1680 train_time:72315ms step_avg:87.44ms
step:828/1680 train_time:72403ms step_avg:87.44ms
step:829/1680 train_time:72491ms step_avg:87.44ms
step:830/1680 train_time:72578ms step_avg:87.44ms
step:831/1680 train_time:72667ms step_avg:87.45ms
step:832/1680 train_time:72755ms step_avg:87.45ms
step:833/1680 train_time:72843ms step_avg:87.45ms
step:834/1680 train_time:72932ms step_avg:87.45ms
step:835/1680 train_time:73021ms step_avg:87.45ms
step:836/1680 train_time:73109ms step_avg:87.45ms
step:837/1680 train_time:73197ms step_avg:87.45ms
step:838/1680 train_time:73285ms step_avg:87.45ms
step:839/1680 train_time:73374ms step_avg:87.45ms
step:840/1680 train_time:73461ms step_avg:87.45ms
step:841/1680 train_time:73549ms step_avg:87.45ms
step:842/1680 train_time:73637ms step_avg:87.45ms
step:843/1680 train_time:73724ms step_avg:87.45ms
step:844/1680 train_time:73812ms step_avg:87.45ms
step:845/1680 train_time:73900ms step_avg:87.46ms
step:846/1680 train_time:73988ms step_avg:87.46ms
step:847/1680 train_time:74077ms step_avg:87.46ms
step:848/1680 train_time:74165ms step_avg:87.46ms
step:849/1680 train_time:74253ms step_avg:87.46ms
step:850/1680 train_time:74341ms step_avg:87.46ms
step:851/1680 train_time:74428ms step_avg:87.46ms
step:852/1680 train_time:74517ms step_avg:87.46ms
step:853/1680 train_time:74605ms step_avg:87.46ms
step:854/1680 train_time:74693ms step_avg:87.46ms
step:855/1680 train_time:74781ms step_avg:87.46ms
step:856/1680 train_time:74869ms step_avg:87.46ms
step:857/1680 train_time:74957ms step_avg:87.46ms
step:858/1680 train_time:75046ms step_avg:87.47ms
step:859/1680 train_time:75135ms step_avg:87.47ms
step:860/1680 train_time:75223ms step_avg:87.47ms
step:861/1680 train_time:75312ms step_avg:87.47ms
step:862/1680 train_time:75400ms step_avg:87.47ms
step:863/1680 train_time:75488ms step_avg:87.47ms
step:864/1680 train_time:75576ms step_avg:87.47ms
step:865/1680 train_time:75664ms step_avg:87.47ms
step:866/1680 train_time:75753ms step_avg:87.47ms
step:867/1680 train_time:75841ms step_avg:87.47ms
step:868/1680 train_time:75928ms step_avg:87.48ms
step:869/1680 train_time:76016ms step_avg:87.48ms
step:870/1680 train_time:76105ms step_avg:87.48ms
step:871/1680 train_time:76194ms step_avg:87.48ms
step:872/1680 train_time:76282ms step_avg:87.48ms
step:873/1680 train_time:76370ms step_avg:87.48ms
step:874/1680 train_time:76458ms step_avg:87.48ms
step:875/1680 train_time:76546ms step_avg:87.48ms
step:875/1680 val_loss:3.5179 train_time:76635ms step_avg:87.58ms
step:876/1680 train_time:76654ms step_avg:87.50ms
step:877/1680 train_time:76725ms step_avg:87.49ms
step:878/1680 train_time:76816ms step_avg:87.49ms
step:879/1680 train_time:76904ms step_avg:87.49ms
step:880/1680 train_time:76991ms step_avg:87.49ms
step:881/1680 train_time:77078ms step_avg:87.49ms
step:882/1680 train_time:77165ms step_avg:87.49ms
step:883/1680 train_time:77252ms step_avg:87.49ms
step:884/1680 train_time:77340ms step_avg:87.49ms
step:885/1680 train_time:77429ms step_avg:87.49ms
step:886/1680 train_time:77516ms step_avg:87.49ms
step:887/1680 train_time:77605ms step_avg:87.49ms
step:888/1680 train_time:77695ms step_avg:87.49ms
step:889/1680 train_time:77785ms step_avg:87.50ms
step:890/1680 train_time:77873ms step_avg:87.50ms
step:891/1680 train_time:77961ms step_avg:87.50ms
step:892/1680 train_time:78049ms step_avg:87.50ms
step:893/1680 train_time:78136ms step_avg:87.50ms
step:894/1680 train_time:78223ms step_avg:87.50ms
step:895/1680 train_time:78311ms step_avg:87.50ms
step:896/1680 train_time:78399ms step_avg:87.50ms
step:897/1680 train_time:78487ms step_avg:87.50ms
step:898/1680 train_time:78575ms step_avg:87.50ms
step:899/1680 train_time:78664ms step_avg:87.50ms
step:900/1680 train_time:78753ms step_avg:87.50ms
step:901/1680 train_time:78842ms step_avg:87.50ms
step:902/1680 train_time:78930ms step_avg:87.51ms
step:903/1680 train_time:79018ms step_avg:87.51ms
step:904/1680 train_time:79106ms step_avg:87.51ms
step:905/1680 train_time:79193ms step_avg:87.51ms
step:906/1680 train_time:79281ms step_avg:87.51ms
step:907/1680 train_time:79368ms step_avg:87.51ms
step:908/1680 train_time:79456ms step_avg:87.51ms
step:909/1680 train_time:79544ms step_avg:87.51ms
step:910/1680 train_time:79633ms step_avg:87.51ms
step:911/1680 train_time:79722ms step_avg:87.51ms
step:912/1680 train_time:79810ms step_avg:87.51ms
step:913/1680 train_time:79899ms step_avg:87.51ms
step:914/1680 train_time:79987ms step_avg:87.51ms
step:915/1680 train_time:80075ms step_avg:87.51ms
step:916/1680 train_time:80162ms step_avg:87.51ms
step:917/1680 train_time:80251ms step_avg:87.51ms
step:918/1680 train_time:80339ms step_avg:87.51ms
step:919/1680 train_time:80428ms step_avg:87.52ms
step:920/1680 train_time:80516ms step_avg:87.52ms
step:921/1680 train_time:80604ms step_avg:87.52ms
step:922/1680 train_time:80693ms step_avg:87.52ms
step:923/1680 train_time:80782ms step_avg:87.52ms
step:924/1680 train_time:80870ms step_avg:87.52ms
step:925/1680 train_time:80959ms step_avg:87.52ms
step:926/1680 train_time:81047ms step_avg:87.52ms
step:927/1680 train_time:81134ms step_avg:87.52ms
step:928/1680 train_time:81222ms step_avg:87.52ms
step:929/1680 train_time:81310ms step_avg:87.52ms
step:930/1680 train_time:81398ms step_avg:87.52ms
step:931/1680 train_time:81486ms step_avg:87.53ms
step:932/1680 train_time:81574ms step_avg:87.53ms
step:933/1680 train_time:81663ms step_avg:87.53ms
step:934/1680 train_time:81752ms step_avg:87.53ms
step:935/1680 train_time:81840ms step_avg:87.53ms
step:936/1680 train_time:81928ms step_avg:87.53ms
step:937/1680 train_time:82017ms step_avg:87.53ms
step:938/1680 train_time:82104ms step_avg:87.53ms
step:939/1680 train_time:82192ms step_avg:87.53ms
step:940/1680 train_time:82280ms step_avg:87.53ms
step:941/1680 train_time:82369ms step_avg:87.53ms
step:942/1680 train_time:82457ms step_avg:87.53ms
step:943/1680 train_time:82545ms step_avg:87.53ms
step:944/1680 train_time:82632ms step_avg:87.53ms
step:945/1680 train_time:82721ms step_avg:87.54ms
step:946/1680 train_time:82809ms step_avg:87.54ms
step:947/1680 train_time:82898ms step_avg:87.54ms
step:948/1680 train_time:82986ms step_avg:87.54ms
step:949/1680 train_time:83075ms step_avg:87.54ms
step:950/1680 train_time:83163ms step_avg:87.54ms
step:951/1680 train_time:83252ms step_avg:87.54ms
step:952/1680 train_time:83340ms step_avg:87.54ms
step:953/1680 train_time:83428ms step_avg:87.54ms
step:954/1680 train_time:83516ms step_avg:87.54ms
step:955/1680 train_time:83604ms step_avg:87.54ms
step:956/1680 train_time:83691ms step_avg:87.54ms
step:957/1680 train_time:83779ms step_avg:87.54ms
step:958/1680 train_time:83868ms step_avg:87.54ms
step:959/1680 train_time:83956ms step_avg:87.55ms
step:960/1680 train_time:84045ms step_avg:87.55ms
step:961/1680 train_time:84133ms step_avg:87.55ms
step:962/1680 train_time:84221ms step_avg:87.55ms
step:963/1680 train_time:84309ms step_avg:87.55ms
step:964/1680 train_time:84397ms step_avg:87.55ms
step:965/1680 train_time:84486ms step_avg:87.55ms
step:966/1680 train_time:84573ms step_avg:87.55ms
step:967/1680 train_time:84662ms step_avg:87.55ms
step:968/1680 train_time:84750ms step_avg:87.55ms
step:969/1680 train_time:84838ms step_avg:87.55ms
step:970/1680 train_time:84926ms step_avg:87.55ms
step:971/1680 train_time:85014ms step_avg:87.55ms
step:972/1680 train_time:85102ms step_avg:87.55ms
step:973/1680 train_time:85191ms step_avg:87.55ms
step:974/1680 train_time:85279ms step_avg:87.56ms
step:975/1680 train_time:85367ms step_avg:87.56ms
step:976/1680 train_time:85455ms step_avg:87.56ms
step:977/1680 train_time:85543ms step_avg:87.56ms
step:978/1680 train_time:85631ms step_avg:87.56ms
step:979/1680 train_time:85720ms step_avg:87.56ms
step:980/1680 train_time:85808ms step_avg:87.56ms
step:981/1680 train_time:85896ms step_avg:87.56ms
step:982/1680 train_time:85984ms step_avg:87.56ms
step:983/1680 train_time:86073ms step_avg:87.56ms
step:984/1680 train_time:86161ms step_avg:87.56ms
step:985/1680 train_time:86250ms step_avg:87.56ms
step:986/1680 train_time:86338ms step_avg:87.56ms
step:987/1680 train_time:86427ms step_avg:87.57ms
step:988/1680 train_time:86515ms step_avg:87.57ms
step:989/1680 train_time:86603ms step_avg:87.57ms
step:990/1680 train_time:86691ms step_avg:87.57ms
step:991/1680 train_time:86779ms step_avg:87.57ms
step:992/1680 train_time:86866ms step_avg:87.57ms
step:993/1680 train_time:86955ms step_avg:87.57ms
step:994/1680 train_time:87043ms step_avg:87.57ms
step:995/1680 train_time:87132ms step_avg:87.57ms
step:996/1680 train_time:87220ms step_avg:87.57ms
step:997/1680 train_time:87307ms step_avg:87.57ms
step:998/1680 train_time:87395ms step_avg:87.57ms
step:999/1680 train_time:87483ms step_avg:87.57ms
step:1000/1680 train_time:87571ms step_avg:87.57ms
step:1000/1680 val_loss:3.4696 train_time:87661ms step_avg:87.66ms
step:1001/1680 train_time:87679ms step_avg:87.59ms
step:1002/1680 train_time:87752ms step_avg:87.58ms
step:1003/1680 train_time:87845ms step_avg:87.58ms
step:1004/1680 train_time:87936ms step_avg:87.59ms
step:1005/1680 train_time:88024ms step_avg:87.59ms
step:1006/1680 train_time:88111ms step_avg:87.59ms
step:1007/1680 train_time:88198ms step_avg:87.59ms
step:1008/1680 train_time:88285ms step_avg:87.58ms
step:1009/1680 train_time:88372ms step_avg:87.58ms
step:1010/1680 train_time:88459ms step_avg:87.58ms
step:1011/1680 train_time:88546ms step_avg:87.58ms
step:1012/1680 train_time:88635ms step_avg:87.58ms
step:1013/1680 train_time:88725ms step_avg:87.59ms
step:1014/1680 train_time:88814ms step_avg:87.59ms
step:1015/1680 train_time:88904ms step_avg:87.59ms
step:1016/1680 train_time:88993ms step_avg:87.59ms
step:1017/1680 train_time:89080ms step_avg:87.59ms
step:1018/1680 train_time:89167ms step_avg:87.59ms
step:1019/1680 train_time:89255ms step_avg:87.59ms
step:1020/1680 train_time:89342ms step_avg:87.59ms
step:1021/1680 train_time:89429ms step_avg:87.59ms
step:1022/1680 train_time:89517ms step_avg:87.59ms
step:1023/1680 train_time:89605ms step_avg:87.59ms
step:1024/1680 train_time:89694ms step_avg:87.59ms
step:1025/1680 train_time:89783ms step_avg:87.59ms
step:1026/1680 train_time:89872ms step_avg:87.59ms
step:1027/1680 train_time:89961ms step_avg:87.60ms
step:1028/1680 train_time:90049ms step_avg:87.60ms
step:1029/1680 train_time:90137ms step_avg:87.60ms
step:1030/1680 train_time:90224ms step_avg:87.60ms
step:1031/1680 train_time:90312ms step_avg:87.60ms
step:1032/1680 train_time:90399ms step_avg:87.60ms
step:1033/1680 train_time:90487ms step_avg:87.60ms
step:1034/1680 train_time:90575ms step_avg:87.60ms
step:1035/1680 train_time:90665ms step_avg:87.60ms
step:1036/1680 train_time:90753ms step_avg:87.60ms
step:1037/1680 train_time:90842ms step_avg:87.60ms
step:1038/1680 train_time:90930ms step_avg:87.60ms
step:1039/1680 train_time:91020ms step_avg:87.60ms
step:1040/1680 train_time:91108ms step_avg:87.60ms
step:1041/1680 train_time:91195ms step_avg:87.60ms
step:1042/1680 train_time:91284ms step_avg:87.60ms
step:1043/1680 train_time:91372ms step_avg:87.60ms
step:1044/1680 train_time:91459ms step_avg:87.60ms
step:1045/1680 train_time:91546ms step_avg:87.60ms
step:1046/1680 train_time:91635ms step_avg:87.60ms
step:1047/1680 train_time:91724ms step_avg:87.61ms
step:1048/1680 train_time:91813ms step_avg:87.61ms
step:1049/1680 train_time:91902ms step_avg:87.61ms
step:1050/1680 train_time:91990ms step_avg:87.61ms
step:1051/1680 train_time:92079ms step_avg:87.61ms
step:1052/1680 train_time:92167ms step_avg:87.61ms
step:1053/1680 train_time:92255ms step_avg:87.61ms
step:1054/1680 train_time:92343ms step_avg:87.61ms
step:1055/1680 train_time:92430ms step_avg:87.61ms
step:1056/1680 train_time:92518ms step_avg:87.61ms
step:1057/1680 train_time:92607ms step_avg:87.61ms
step:1058/1680 train_time:92695ms step_avg:87.61ms
step:1059/1680 train_time:92784ms step_avg:87.61ms
step:1060/1680 train_time:92872ms step_avg:87.62ms
step:1061/1680 train_time:92961ms step_avg:87.62ms
step:1062/1680 train_time:93049ms step_avg:87.62ms
step:1063/1680 train_time:93137ms step_avg:87.62ms
step:1064/1680 train_time:93225ms step_avg:87.62ms
step:1065/1680 train_time:93313ms step_avg:87.62ms
step:1066/1680 train_time:93401ms step_avg:87.62ms
step:1067/1680 train_time:93489ms step_avg:87.62ms
step:1068/1680 train_time:93577ms step_avg:87.62ms
step:1069/1680 train_time:93665ms step_avg:87.62ms
step:1070/1680 train_time:93753ms step_avg:87.62ms
step:1071/1680 train_time:93841ms step_avg:87.62ms
step:1072/1680 train_time:93929ms step_avg:87.62ms
step:1073/1680 train_time:94017ms step_avg:87.62ms
step:1074/1680 train_time:94105ms step_avg:87.62ms
step:1075/1680 train_time:94193ms step_avg:87.62ms
step:1076/1680 train_time:94282ms step_avg:87.62ms
step:1077/1680 train_time:94370ms step_avg:87.62ms
step:1078/1680 train_time:94458ms step_avg:87.62ms
step:1079/1680 train_time:94546ms step_avg:87.62ms
step:1080/1680 train_time:94633ms step_avg:87.62ms
step:1081/1680 train_time:94721ms step_avg:87.62ms
step:1082/1680 train_time:94810ms step_avg:87.63ms
step:1083/1680 train_time:94899ms step_avg:87.63ms
step:1084/1680 train_time:94987ms step_avg:87.63ms
step:1085/1680 train_time:95076ms step_avg:87.63ms
step:1086/1680 train_time:95164ms step_avg:87.63ms
step:1087/1680 train_time:95252ms step_avg:87.63ms
step:1088/1680 train_time:95340ms step_avg:87.63ms
step:1089/1680 train_time:95428ms step_avg:87.63ms
step:1090/1680 train_time:95516ms step_avg:87.63ms
step:1091/1680 train_time:95604ms step_avg:87.63ms
step:1092/1680 train_time:95693ms step_avg:87.63ms
step:1093/1680 train_time:95781ms step_avg:87.63ms
step:1094/1680 train_time:95869ms step_avg:87.63ms
step:1095/1680 train_time:95958ms step_avg:87.63ms
step:1096/1680 train_time:96046ms step_avg:87.63ms
step:1097/1680 train_time:96134ms step_avg:87.63ms
step:1098/1680 train_time:96223ms step_avg:87.63ms
step:1099/1680 train_time:96311ms step_avg:87.64ms
step:1100/1680 train_time:96400ms step_avg:87.64ms
step:1101/1680 train_time:96489ms step_avg:87.64ms
step:1102/1680 train_time:96578ms step_avg:87.64ms
step:1103/1680 train_time:96666ms step_avg:87.64ms
step:1104/1680 train_time:96755ms step_avg:87.64ms
step:1105/1680 train_time:96844ms step_avg:87.64ms
step:1106/1680 train_time:96934ms step_avg:87.64ms
step:1107/1680 train_time:97023ms step_avg:87.64ms
step:1108/1680 train_time:97111ms step_avg:87.65ms
step:1109/1680 train_time:97200ms step_avg:87.65ms
step:1110/1680 train_time:97290ms step_avg:87.65ms
step:1111/1680 train_time:97379ms step_avg:87.65ms
step:1112/1680 train_time:97468ms step_avg:87.65ms
step:1113/1680 train_time:97556ms step_avg:87.65ms
step:1114/1680 train_time:97645ms step_avg:87.65ms
step:1115/1680 train_time:97735ms step_avg:87.65ms
step:1116/1680 train_time:97824ms step_avg:87.66ms
step:1117/1680 train_time:97913ms step_avg:87.66ms
step:1118/1680 train_time:98002ms step_avg:87.66ms
step:1119/1680 train_time:98092ms step_avg:87.66ms
step:1120/1680 train_time:98181ms step_avg:87.66ms
step:1121/1680 train_time:98270ms step_avg:87.66ms
step:1122/1680 train_time:98359ms step_avg:87.66ms
step:1123/1680 train_time:98448ms step_avg:87.67ms
step:1124/1680 train_time:98537ms step_avg:87.67ms
step:1125/1680 train_time:98626ms step_avg:87.67ms
step:1125/1680 val_loss:3.4152 train_time:98716ms step_avg:87.75ms
step:1126/1680 train_time:98735ms step_avg:87.69ms
step:1127/1680 train_time:98807ms step_avg:87.67ms
step:1128/1680 train_time:98899ms step_avg:87.68ms
step:1129/1680 train_time:98989ms step_avg:87.68ms
step:1130/1680 train_time:99078ms step_avg:87.68ms
step:1131/1680 train_time:99166ms step_avg:87.68ms
step:1132/1680 train_time:99254ms step_avg:87.68ms
step:1133/1680 train_time:99342ms step_avg:87.68ms
step:1134/1680 train_time:99430ms step_avg:87.68ms
step:1135/1680 train_time:99518ms step_avg:87.68ms
step:1136/1680 train_time:99605ms step_avg:87.68ms
step:1137/1680 train_time:99695ms step_avg:87.68ms
step:1138/1680 train_time:99785ms step_avg:87.68ms
step:1139/1680 train_time:99876ms step_avg:87.69ms
step:1140/1680 train_time:99967ms step_avg:87.69ms
step:1141/1680 train_time:100056ms step_avg:87.69ms
step:1142/1680 train_time:100144ms step_avg:87.69ms
step:1143/1680 train_time:100233ms step_avg:87.69ms
step:1144/1680 train_time:100321ms step_avg:87.69ms
step:1145/1680 train_time:100409ms step_avg:87.69ms
step:1146/1680 train_time:100497ms step_avg:87.69ms
step:1147/1680 train_time:100586ms step_avg:87.69ms
step:1148/1680 train_time:100674ms step_avg:87.70ms
step:1149/1680 train_time:100764ms step_avg:87.70ms
step:1150/1680 train_time:100853ms step_avg:87.70ms
step:1151/1680 train_time:100943ms step_avg:87.70ms
step:1152/1680 train_time:101032ms step_avg:87.70ms
step:1153/1680 train_time:101122ms step_avg:87.70ms
step:1154/1680 train_time:101210ms step_avg:87.70ms
step:1155/1680 train_time:101300ms step_avg:87.71ms
step:1156/1680 train_time:101388ms step_avg:87.71ms
step:1157/1680 train_time:101476ms step_avg:87.71ms
step:1158/1680 train_time:101565ms step_avg:87.71ms
step:1159/1680 train_time:101654ms step_avg:87.71ms
step:1160/1680 train_time:101743ms step_avg:87.71ms
step:1161/1680 train_time:101832ms step_avg:87.71ms
step:1162/1680 train_time:101922ms step_avg:87.71ms
step:1163/1680 train_time:102011ms step_avg:87.71ms
step:1164/1680 train_time:102100ms step_avg:87.71ms
step:1165/1680 train_time:102190ms step_avg:87.72ms
step:1166/1680 train_time:102278ms step_avg:87.72ms
step:1167/1680 train_time:102367ms step_avg:87.72ms
step:1168/1680 train_time:102456ms step_avg:87.72ms
step:1169/1680 train_time:102544ms step_avg:87.72ms
step:1170/1680 train_time:102633ms step_avg:87.72ms
step:1171/1680 train_time:102721ms step_avg:87.72ms
step:1172/1680 train_time:102811ms step_avg:87.72ms
step:1173/1680 train_time:102902ms step_avg:87.73ms
step:1174/1680 train_time:102990ms step_avg:87.73ms
step:1175/1680 train_time:103080ms step_avg:87.73ms
step:1176/1680 train_time:103169ms step_avg:87.73ms
step:1177/1680 train_time:103258ms step_avg:87.73ms
step:1178/1680 train_time:103347ms step_avg:87.73ms
step:1179/1680 train_time:103436ms step_avg:87.73ms
step:1180/1680 train_time:103525ms step_avg:87.73ms
step:1181/1680 train_time:103613ms step_avg:87.73ms
step:1182/1680 train_time:103702ms step_avg:87.73ms
step:1183/1680 train_time:103790ms step_avg:87.73ms
step:1184/1680 train_time:103879ms step_avg:87.74ms
step:1185/1680 train_time:103969ms step_avg:87.74ms
step:1186/1680 train_time:104058ms step_avg:87.74ms
step:1187/1680 train_time:104148ms step_avg:87.74ms
step:1188/1680 train_time:104237ms step_avg:87.74ms
step:1189/1680 train_time:104326ms step_avg:87.74ms
step:1190/1680 train_time:104415ms step_avg:87.74ms
step:1191/1680 train_time:104504ms step_avg:87.74ms
step:1192/1680 train_time:104593ms step_avg:87.75ms
step:1193/1680 train_time:104681ms step_avg:87.75ms
step:1194/1680 train_time:104771ms step_avg:87.75ms
step:1195/1680 train_time:104859ms step_avg:87.75ms
step:1196/1680 train_time:104948ms step_avg:87.75ms
step:1197/1680 train_time:105037ms step_avg:87.75ms
step:1198/1680 train_time:105127ms step_avg:87.75ms
step:1199/1680 train_time:105215ms step_avg:87.75ms
step:1200/1680 train_time:105303ms step_avg:87.75ms
step:1201/1680 train_time:105392ms step_avg:87.75ms
step:1202/1680 train_time:105482ms step_avg:87.76ms
step:1203/1680 train_time:105570ms step_avg:87.76ms
step:1204/1680 train_time:105659ms step_avg:87.76ms
step:1205/1680 train_time:105748ms step_avg:87.76ms
step:1206/1680 train_time:105836ms step_avg:87.76ms
step:1207/1680 train_time:105926ms step_avg:87.76ms
step:1208/1680 train_time:106015ms step_avg:87.76ms
step:1209/1680 train_time:106104ms step_avg:87.76ms
step:1210/1680 train_time:106193ms step_avg:87.76ms
step:1211/1680 train_time:106282ms step_avg:87.76ms
step:1212/1680 train_time:106372ms step_avg:87.77ms
step:1213/1680 train_time:106461ms step_avg:87.77ms
step:1214/1680 train_time:106550ms step_avg:87.77ms
step:1215/1680 train_time:106639ms step_avg:87.77ms
step:1216/1680 train_time:106729ms step_avg:87.77ms
step:1217/1680 train_time:106818ms step_avg:87.77ms
step:1218/1680 train_time:106908ms step_avg:87.77ms
step:1219/1680 train_time:106998ms step_avg:87.78ms
step:1220/1680 train_time:107088ms step_avg:87.78ms
step:1221/1680 train_time:107177ms step_avg:87.78ms
step:1222/1680 train_time:107266ms step_avg:87.78ms
step:1223/1680 train_time:107354ms step_avg:87.78ms
step:1224/1680 train_time:107445ms step_avg:87.78ms
step:1225/1680 train_time:107533ms step_avg:87.78ms
step:1226/1680 train_time:107622ms step_avg:87.78ms
step:1227/1680 train_time:107710ms step_avg:87.78ms
step:1228/1680 train_time:107799ms step_avg:87.78ms
step:1229/1680 train_time:107887ms step_avg:87.78ms
step:1230/1680 train_time:107976ms step_avg:87.79ms
step:1231/1680 train_time:108064ms step_avg:87.79ms
step:1232/1680 train_time:108153ms step_avg:87.79ms
step:1233/1680 train_time:108242ms step_avg:87.79ms
step:1234/1680 train_time:108331ms step_avg:87.79ms
step:1235/1680 train_time:108419ms step_avg:87.79ms
step:1236/1680 train_time:108508ms step_avg:87.79ms
step:1237/1680 train_time:108597ms step_avg:87.79ms
step:1238/1680 train_time:108686ms step_avg:87.79ms
step:1239/1680 train_time:108775ms step_avg:87.79ms
step:1240/1680 train_time:108865ms step_avg:87.79ms
step:1241/1680 train_time:108954ms step_avg:87.80ms
step:1242/1680 train_time:109043ms step_avg:87.80ms
step:1243/1680 train_time:109132ms step_avg:87.80ms
step:1244/1680 train_time:109222ms step_avg:87.80ms
step:1245/1680 train_time:109310ms step_avg:87.80ms
step:1246/1680 train_time:109399ms step_avg:87.80ms
step:1247/1680 train_time:109487ms step_avg:87.80ms
step:1248/1680 train_time:109577ms step_avg:87.80ms
step:1249/1680 train_time:109665ms step_avg:87.80ms
step:1250/1680 train_time:109755ms step_avg:87.80ms
step:1250/1680 val_loss:3.3772 train_time:109845ms step_avg:87.88ms
step:1251/1680 train_time:109864ms step_avg:87.82ms
step:1252/1680 train_time:109937ms step_avg:87.81ms
step:1253/1680 train_time:110031ms step_avg:87.81ms
step:1254/1680 train_time:110121ms step_avg:87.82ms
step:1255/1680 train_time:110209ms step_avg:87.82ms
step:1256/1680 train_time:110298ms step_avg:87.82ms
step:1257/1680 train_time:110386ms step_avg:87.82ms
step:1258/1680 train_time:110474ms step_avg:87.82ms
step:1259/1680 train_time:110561ms step_avg:87.82ms
step:1260/1680 train_time:110650ms step_avg:87.82ms
step:1261/1680 train_time:110738ms step_avg:87.82ms
step:1262/1680 train_time:110828ms step_avg:87.82ms
step:1263/1680 train_time:110919ms step_avg:87.82ms
step:1264/1680 train_time:111010ms step_avg:87.82ms
step:1265/1680 train_time:111101ms step_avg:87.83ms
step:1266/1680 train_time:111190ms step_avg:87.83ms
step:1267/1680 train_time:111279ms step_avg:87.83ms
step:1268/1680 train_time:111368ms step_avg:87.83ms
step:1269/1680 train_time:111455ms step_avg:87.83ms
step:1270/1680 train_time:111543ms step_avg:87.83ms
step:1271/1680 train_time:111631ms step_avg:87.83ms
step:1272/1680 train_time:111719ms step_avg:87.83ms
step:1273/1680 train_time:111808ms step_avg:87.83ms
step:1274/1680 train_time:111898ms step_avg:87.83ms
step:1275/1680 train_time:111988ms step_avg:87.83ms
step:1276/1680 train_time:112077ms step_avg:87.84ms
step:1277/1680 train_time:112168ms step_avg:87.84ms
step:1278/1680 train_time:112256ms step_avg:87.84ms
step:1279/1680 train_time:112345ms step_avg:87.84ms
step:1280/1680 train_time:112433ms step_avg:87.84ms
step:1281/1680 train_time:112522ms step_avg:87.84ms
step:1282/1680 train_time:112611ms step_avg:87.84ms
step:1283/1680 train_time:112699ms step_avg:87.84ms
step:1284/1680 train_time:112788ms step_avg:87.84ms
step:1285/1680 train_time:112878ms step_avg:87.84ms
step:1286/1680 train_time:112969ms step_avg:87.85ms
step:1287/1680 train_time:113059ms step_avg:87.85ms
step:1288/1680 train_time:113149ms step_avg:87.85ms
step:1289/1680 train_time:113238ms step_avg:87.85ms
step:1290/1680 train_time:113327ms step_avg:87.85ms
step:1291/1680 train_time:113415ms step_avg:87.85ms
step:1292/1680 train_time:113504ms step_avg:87.85ms
step:1293/1680 train_time:113592ms step_avg:87.85ms
step:1294/1680 train_time:113681ms step_avg:87.85ms
step:1295/1680 train_time:113770ms step_avg:87.85ms
step:1296/1680 train_time:113859ms step_avg:87.85ms
step:1297/1680 train_time:113949ms step_avg:87.86ms
step:1298/1680 train_time:114038ms step_avg:87.86ms
step:1299/1680 train_time:114129ms step_avg:87.86ms
step:1300/1680 train_time:114218ms step_avg:87.86ms
step:1301/1680 train_time:114307ms step_avg:87.86ms
step:1302/1680 train_time:114395ms step_avg:87.86ms
step:1303/1680 train_time:114484ms step_avg:87.86ms
step:1304/1680 train_time:114573ms step_avg:87.86ms
step:1305/1680 train_time:114662ms step_avg:87.86ms
step:1306/1680 train_time:114752ms step_avg:87.87ms
step:1307/1680 train_time:114842ms step_avg:87.87ms
step:1308/1680 train_time:114931ms step_avg:87.87ms
step:1309/1680 train_time:115020ms step_avg:87.87ms
step:1310/1680 train_time:115110ms step_avg:87.87ms
step:1311/1680 train_time:115199ms step_avg:87.87ms
step:1312/1680 train_time:115288ms step_avg:87.87ms
step:1313/1680 train_time:115379ms step_avg:87.87ms
step:1314/1680 train_time:115467ms step_avg:87.87ms
step:1315/1680 train_time:115556ms step_avg:87.87ms
step:1316/1680 train_time:115644ms step_avg:87.88ms
step:1317/1680 train_time:115733ms step_avg:87.88ms
step:1318/1680 train_time:115823ms step_avg:87.88ms
step:1319/1680 train_time:115913ms step_avg:87.88ms
step:1320/1680 train_time:116001ms step_avg:87.88ms
step:1321/1680 train_time:116091ms step_avg:87.88ms
step:1322/1680 train_time:116181ms step_avg:87.88ms
step:1323/1680 train_time:116271ms step_avg:87.88ms
step:1324/1680 train_time:116361ms step_avg:87.89ms
step:1325/1680 train_time:116450ms step_avg:87.89ms
step:1326/1680 train_time:116539ms step_avg:87.89ms
step:1327/1680 train_time:116627ms step_avg:87.89ms
step:1328/1680 train_time:116716ms step_avg:87.89ms
step:1329/1680 train_time:116806ms step_avg:87.89ms
step:1330/1680 train_time:116895ms step_avg:87.89ms
step:1331/1680 train_time:116985ms step_avg:87.89ms
step:1332/1680 train_time:117075ms step_avg:87.89ms
step:1333/1680 train_time:117164ms step_avg:87.89ms
step:1334/1680 train_time:117252ms step_avg:87.90ms
step:1335/1680 train_time:117342ms step_avg:87.90ms
step:1336/1680 train_time:117432ms step_avg:87.90ms
step:1337/1680 train_time:117522ms step_avg:87.90ms
step:1338/1680 train_time:117611ms step_avg:87.90ms
step:1339/1680 train_time:117700ms step_avg:87.90ms
step:1340/1680 train_time:117789ms step_avg:87.90ms
step:1341/1680 train_time:117878ms step_avg:87.90ms
step:1342/1680 train_time:117968ms step_avg:87.90ms
step:1343/1680 train_time:118057ms step_avg:87.91ms
step:1344/1680 train_time:118146ms step_avg:87.91ms
step:1345/1680 train_time:118235ms step_avg:87.91ms
step:1346/1680 train_time:118324ms step_avg:87.91ms
step:1347/1680 train_time:118413ms step_avg:87.91ms
step:1348/1680 train_time:118502ms step_avg:87.91ms
step:1349/1680 train_time:118591ms step_avg:87.91ms
step:1350/1680 train_time:118681ms step_avg:87.91ms
step:1351/1680 train_time:118770ms step_avg:87.91ms
step:1352/1680 train_time:118859ms step_avg:87.91ms
step:1353/1680 train_time:118949ms step_avg:87.91ms
step:1354/1680 train_time:119037ms step_avg:87.92ms
step:1355/1680 train_time:119127ms step_avg:87.92ms
step:1356/1680 train_time:119215ms step_avg:87.92ms
step:1357/1680 train_time:119305ms step_avg:87.92ms
step:1358/1680 train_time:119394ms step_avg:87.92ms
step:1359/1680 train_time:119483ms step_avg:87.92ms
step:1360/1680 train_time:119572ms step_avg:87.92ms
step:1361/1680 train_time:119661ms step_avg:87.92ms
step:1362/1680 train_time:119750ms step_avg:87.92ms
step:1363/1680 train_time:119839ms step_avg:87.92ms
step:1364/1680 train_time:119929ms step_avg:87.92ms
step:1365/1680 train_time:120017ms step_avg:87.92ms
step:1366/1680 train_time:120107ms step_avg:87.93ms
step:1367/1680 train_time:120197ms step_avg:87.93ms
step:1368/1680 train_time:120287ms step_avg:87.93ms
step:1369/1680 train_time:120375ms step_avg:87.93ms
step:1370/1680 train_time:120464ms step_avg:87.93ms
step:1371/1680 train_time:120553ms step_avg:87.93ms
step:1372/1680 train_time:120642ms step_avg:87.93ms
step:1373/1680 train_time:120731ms step_avg:87.93ms
step:1374/1680 train_time:120821ms step_avg:87.93ms
step:1375/1680 train_time:120910ms step_avg:87.93ms
step:1375/1680 val_loss:3.3431 train_time:121000ms step_avg:88.00ms
step:1376/1680 train_time:121019ms step_avg:87.95ms
step:1377/1680 train_time:121093ms step_avg:87.94ms
step:1378/1680 train_time:121189ms step_avg:87.95ms
step:1379/1680 train_time:121280ms step_avg:87.95ms
step:1380/1680 train_time:121368ms step_avg:87.95ms
step:1381/1680 train_time:121456ms step_avg:87.95ms
step:1382/1680 train_time:121544ms step_avg:87.95ms
step:1383/1680 train_time:121631ms step_avg:87.95ms
step:1384/1680 train_time:121719ms step_avg:87.95ms
step:1385/1680 train_time:121807ms step_avg:87.95ms
step:1386/1680 train_time:121895ms step_avg:87.95ms
step:1387/1680 train_time:121985ms step_avg:87.95ms
step:1388/1680 train_time:122076ms step_avg:87.95ms
step:1389/1680 train_time:122168ms step_avg:87.95ms
step:1390/1680 train_time:122258ms step_avg:87.96ms
step:1391/1680 train_time:122348ms step_avg:87.96ms
step:1392/1680 train_time:122437ms step_avg:87.96ms
step:1393/1680 train_time:122525ms step_avg:87.96ms
step:1394/1680 train_time:122613ms step_avg:87.96ms
step:1395/1680 train_time:122701ms step_avg:87.96ms
step:1396/1680 train_time:122790ms step_avg:87.96ms
step:1397/1680 train_time:122877ms step_avg:87.96ms
step:1398/1680 train_time:122966ms step_avg:87.96ms
step:1399/1680 train_time:123055ms step_avg:87.96ms
step:1400/1680 train_time:123146ms step_avg:87.96ms
step:1401/1680 train_time:123236ms step_avg:87.96ms
step:1402/1680 train_time:123325ms step_avg:87.96ms
step:1403/1680 train_time:123415ms step_avg:87.96ms
step:1404/1680 train_time:123503ms step_avg:87.97ms
step:1405/1680 train_time:123592ms step_avg:87.97ms
step:1406/1680 train_time:123680ms step_avg:87.97ms
step:1407/1680 train_time:123768ms step_avg:87.97ms
step:1408/1680 train_time:123857ms step_avg:87.97ms
step:1409/1680 train_time:123946ms step_avg:87.97ms
step:1410/1680 train_time:124035ms step_avg:87.97ms
step:1411/1680 train_time:124124ms step_avg:87.97ms
step:1412/1680 train_time:124214ms step_avg:87.97ms
step:1413/1680 train_time:124303ms step_avg:87.97ms
step:1414/1680 train_time:124393ms step_avg:87.97ms
step:1415/1680 train_time:124482ms step_avg:87.97ms
step:1416/1680 train_time:124570ms step_avg:87.97ms
step:1417/1680 train_time:124658ms step_avg:87.97ms
step:1418/1680 train_time:124746ms step_avg:87.97ms
step:1419/1680 train_time:124836ms step_avg:87.97ms
step:1420/1680 train_time:124925ms step_avg:87.98ms
step:1421/1680 train_time:125014ms step_avg:87.98ms
step:1422/1680 train_time:125103ms step_avg:87.98ms
step:1423/1680 train_time:125193ms step_avg:87.98ms
step:1424/1680 train_time:125283ms step_avg:87.98ms
step:1425/1680 train_time:125372ms step_avg:87.98ms
step:1426/1680 train_time:125462ms step_avg:87.98ms
step:1427/1680 train_time:125551ms step_avg:87.98ms
step:1428/1680 train_time:125640ms step_avg:87.98ms
step:1429/1680 train_time:125729ms step_avg:87.98ms
step:1430/1680 train_time:125817ms step_avg:87.98ms
step:1431/1680 train_time:125906ms step_avg:87.98ms
step:1432/1680 train_time:125995ms step_avg:87.99ms
step:1433/1680 train_time:126084ms step_avg:87.99ms
step:1434/1680 train_time:126173ms step_avg:87.99ms
step:1435/1680 train_time:126263ms step_avg:87.99ms
step:1436/1680 train_time:126352ms step_avg:87.99ms
step:1437/1680 train_time:126441ms step_avg:87.99ms
step:1438/1680 train_time:126530ms step_avg:87.99ms
step:1439/1680 train_time:126618ms step_avg:87.99ms
step:1440/1680 train_time:126706ms step_avg:87.99ms
step:1441/1680 train_time:126795ms step_avg:87.99ms
step:1442/1680 train_time:126884ms step_avg:87.99ms
step:1443/1680 train_time:126973ms step_avg:87.99ms
step:1444/1680 train_time:127062ms step_avg:87.99ms
step:1445/1680 train_time:127151ms step_avg:87.99ms
step:1446/1680 train_time:127240ms step_avg:87.99ms
step:1447/1680 train_time:127331ms step_avg:88.00ms
step:1448/1680 train_time:127421ms step_avg:88.00ms
step:1449/1680 train_time:127511ms step_avg:88.00ms
step:1450/1680 train_time:127601ms step_avg:88.00ms
step:1451/1680 train_time:127689ms step_avg:88.00ms
step:1452/1680 train_time:127778ms step_avg:88.00ms
step:1453/1680 train_time:127866ms step_avg:88.00ms
step:1454/1680 train_time:127955ms step_avg:88.00ms
step:1455/1680 train_time:128044ms step_avg:88.00ms
step:1456/1680 train_time:128133ms step_avg:88.00ms
step:1457/1680 train_time:128222ms step_avg:88.00ms
step:1458/1680 train_time:128311ms step_avg:88.01ms
step:1459/1680 train_time:128400ms step_avg:88.01ms
step:1460/1680 train_time:128489ms step_avg:88.01ms
step:1461/1680 train_time:128578ms step_avg:88.01ms
step:1462/1680 train_time:128667ms step_avg:88.01ms
step:1463/1680 train_time:128756ms step_avg:88.01ms
step:1464/1680 train_time:128844ms step_avg:88.01ms
step:1465/1680 train_time:128933ms step_avg:88.01ms
step:1466/1680 train_time:129022ms step_avg:88.01ms
step:1467/1680 train_time:129111ms step_avg:88.01ms
step:1468/1680 train_time:129201ms step_avg:88.01ms
step:1469/1680 train_time:129290ms step_avg:88.01ms
step:1470/1680 train_time:129378ms step_avg:88.01ms
step:1471/1680 train_time:129467ms step_avg:88.01ms
step:1472/1680 train_time:129558ms step_avg:88.01ms
step:1473/1680 train_time:129647ms step_avg:88.02ms
step:1474/1680 train_time:129736ms step_avg:88.02ms
step:1475/1680 train_time:129826ms step_avg:88.02ms
step:1476/1680 train_time:129915ms step_avg:88.02ms
step:1477/1680 train_time:130005ms step_avg:88.02ms
step:1478/1680 train_time:130093ms step_avg:88.02ms
step:1479/1680 train_time:130183ms step_avg:88.02ms
step:1480/1680 train_time:130272ms step_avg:88.02ms
step:1481/1680 train_time:130361ms step_avg:88.02ms
step:1482/1680 train_time:130450ms step_avg:88.02ms
step:1483/1680 train_time:130539ms step_avg:88.02ms
step:1484/1680 train_time:130628ms step_avg:88.02ms
step:1485/1680 train_time:130718ms step_avg:88.03ms
step:1486/1680 train_time:130807ms step_avg:88.03ms
step:1487/1680 train_time:130895ms step_avg:88.03ms
step:1488/1680 train_time:130984ms step_avg:88.03ms
step:1489/1680 train_time:131073ms step_avg:88.03ms
step:1490/1680 train_time:131161ms step_avg:88.03ms
step:1491/1680 train_time:131251ms step_avg:88.03ms
step:1492/1680 train_time:131340ms step_avg:88.03ms
step:1493/1680 train_time:131430ms step_avg:88.03ms
step:1494/1680 train_time:131519ms step_avg:88.03ms
step:1495/1680 train_time:131609ms step_avg:88.03ms
step:1496/1680 train_time:131697ms step_avg:88.03ms
step:1497/1680 train_time:131786ms step_avg:88.03ms
step:1498/1680 train_time:131875ms step_avg:88.03ms
step:1499/1680 train_time:131964ms step_avg:88.03ms
step:1500/1680 train_time:132052ms step_avg:88.03ms
step:1500/1680 val_loss:3.3132 train_time:132143ms step_avg:88.10ms
step:1501/1680 train_time:132163ms step_avg:88.05ms
step:1502/1680 train_time:132235ms step_avg:88.04ms
step:1503/1680 train_time:132330ms step_avg:88.04ms
step:1504/1680 train_time:132418ms step_avg:88.04ms
step:1505/1680 train_time:132506ms step_avg:88.04ms
step:1506/1680 train_time:132594ms step_avg:88.04ms
step:1507/1680 train_time:132682ms step_avg:88.04ms
step:1508/1680 train_time:132770ms step_avg:88.04ms
step:1509/1680 train_time:132858ms step_avg:88.04ms
step:1510/1680 train_time:132947ms step_avg:88.04ms
step:1511/1680 train_time:133034ms step_avg:88.04ms
step:1512/1680 train_time:133124ms step_avg:88.05ms
step:1513/1680 train_time:133214ms step_avg:88.05ms
step:1514/1680 train_time:133305ms step_avg:88.05ms
step:1515/1680 train_time:133395ms step_avg:88.05ms
step:1516/1680 train_time:133484ms step_avg:88.05ms
step:1517/1680 train_time:133573ms step_avg:88.05ms
step:1518/1680 train_time:133661ms step_avg:88.05ms
step:1519/1680 train_time:133750ms step_avg:88.05ms
step:1520/1680 train_time:133838ms step_avg:88.05ms
step:1521/1680 train_time:133926ms step_avg:88.05ms
step:1522/1680 train_time:134015ms step_avg:88.05ms
step:1523/1680 train_time:134104ms step_avg:88.05ms
step:1524/1680 train_time:134194ms step_avg:88.05ms
step:1525/1680 train_time:134285ms step_avg:88.06ms
step:1526/1680 train_time:134374ms step_avg:88.06ms
step:1527/1680 train_time:134463ms step_avg:88.06ms
step:1528/1680 train_time:134552ms step_avg:88.06ms
step:1529/1680 train_time:134641ms step_avg:88.06ms
step:1530/1680 train_time:134729ms step_avg:88.06ms
step:1531/1680 train_time:134817ms step_avg:88.06ms
step:1532/1680 train_time:134907ms step_avg:88.06ms
step:1533/1680 train_time:134996ms step_avg:88.06ms
step:1534/1680 train_time:135085ms step_avg:88.06ms
step:1535/1680 train_time:135174ms step_avg:88.06ms
step:1536/1680 train_time:135264ms step_avg:88.06ms
step:1537/1680 train_time:135353ms step_avg:88.06ms
step:1538/1680 train_time:135443ms step_avg:88.06ms
step:1539/1680 train_time:135532ms step_avg:88.06ms
step:1540/1680 train_time:135621ms step_avg:88.07ms
step:1541/1680 train_time:135709ms step_avg:88.07ms
step:1542/1680 train_time:135798ms step_avg:88.07ms
step:1543/1680 train_time:135887ms step_avg:88.07ms
step:1544/1680 train_time:135975ms step_avg:88.07ms
step:1545/1680 train_time:136064ms step_avg:88.07ms
step:1546/1680 train_time:136154ms step_avg:88.07ms
step:1547/1680 train_time:136242ms step_avg:88.07ms
step:1548/1680 train_time:136332ms step_avg:88.07ms
step:1549/1680 train_time:136421ms step_avg:88.07ms
step:1550/1680 train_time:136509ms step_avg:88.07ms
step:1551/1680 train_time:136598ms step_avg:88.07ms
step:1552/1680 train_time:136686ms step_avg:88.07ms
step:1553/1680 train_time:136775ms step_avg:88.07ms
step:1554/1680 train_time:136864ms step_avg:88.07ms
step:1555/1680 train_time:136953ms step_avg:88.07ms
step:1556/1680 train_time:137043ms step_avg:88.07ms
step:1557/1680 train_time:137132ms step_avg:88.07ms
step:1558/1680 train_time:137221ms step_avg:88.07ms
step:1559/1680 train_time:137310ms step_avg:88.08ms
step:1560/1680 train_time:137399ms step_avg:88.08ms
step:1561/1680 train_time:137490ms step_avg:88.08ms
step:1562/1680 train_time:137578ms step_avg:88.08ms
step:1563/1680 train_time:137668ms step_avg:88.08ms
step:1564/1680 train_time:137758ms step_avg:88.08ms
step:1565/1680 train_time:137847ms step_avg:88.08ms
step:1566/1680 train_time:137935ms step_avg:88.08ms
step:1567/1680 train_time:138023ms step_avg:88.08ms
step:1568/1680 train_time:138112ms step_avg:88.08ms
step:1569/1680 train_time:138200ms step_avg:88.08ms
step:1570/1680 train_time:138290ms step_avg:88.08ms
step:1571/1680 train_time:138380ms step_avg:88.08ms
step:1572/1680 train_time:138470ms step_avg:88.09ms
step:1573/1680 train_time:138559ms step_avg:88.09ms
step:1574/1680 train_time:138649ms step_avg:88.09ms
step:1575/1680 train_time:138738ms step_avg:88.09ms
step:1576/1680 train_time:138826ms step_avg:88.09ms
step:1577/1680 train_time:138914ms step_avg:88.09ms
step:1578/1680 train_time:139004ms step_avg:88.09ms
step:1579/1680 train_time:139093ms step_avg:88.09ms
step:1580/1680 train_time:139182ms step_avg:88.09ms
step:1581/1680 train_time:139271ms step_avg:88.09ms
step:1582/1680 train_time:139360ms step_avg:88.09ms
step:1583/1680 train_time:139449ms step_avg:88.09ms
step:1584/1680 train_time:139538ms step_avg:88.09ms
step:1585/1680 train_time:139627ms step_avg:88.09ms
step:1586/1680 train_time:139717ms step_avg:88.09ms
step:1587/1680 train_time:139805ms step_avg:88.09ms
step:1588/1680 train_time:139894ms step_avg:88.09ms
step:1589/1680 train_time:139983ms step_avg:88.10ms
step:1590/1680 train_time:140072ms step_avg:88.10ms
step:1591/1680 train_time:140161ms step_avg:88.10ms
step:1592/1680 train_time:140250ms step_avg:88.10ms
step:1593/1680 train_time:140339ms step_avg:88.10ms
step:1594/1680 train_time:140428ms step_avg:88.10ms
step:1595/1680 train_time:140517ms step_avg:88.10ms
step:1596/1680 train_time:140607ms step_avg:88.10ms
step:1597/1680 train_time:140696ms step_avg:88.10ms
step:1598/1680 train_time:140785ms step_avg:88.10ms
step:1599/1680 train_time:140874ms step_avg:88.10ms
step:1600/1680 train_time:140963ms step_avg:88.10ms
step:1601/1680 train_time:141052ms step_avg:88.10ms
step:1602/1680 train_time:141140ms step_avg:88.10ms
step:1603/1680 train_time:141229ms step_avg:88.10ms
step:1604/1680 train_time:141318ms step_avg:88.10ms
step:1605/1680 train_time:141406ms step_avg:88.10ms
step:1606/1680 train_time:141494ms step_avg:88.10ms
step:1607/1680 train_time:141584ms step_avg:88.10ms
step:1608/1680 train_time:141673ms step_avg:88.11ms
step:1609/1680 train_time:141762ms step_avg:88.11ms
step:1610/1680 train_time:141853ms step_avg:88.11ms
step:1611/1680 train_time:141943ms step_avg:88.11ms
step:1612/1680 train_time:142032ms step_avg:88.11ms
step:1613/1680 train_time:142121ms step_avg:88.11ms
step:1614/1680 train_time:142210ms step_avg:88.11ms
step:1615/1680 train_time:142299ms step_avg:88.11ms
step:1616/1680 train_time:142389ms step_avg:88.11ms
step:1617/1680 train_time:142478ms step_avg:88.11ms
step:1618/1680 train_time:142568ms step_avg:88.11ms
step:1619/1680 train_time:142658ms step_avg:88.12ms
step:1620/1680 train_time:142747ms step_avg:88.12ms
step:1621/1680 train_time:142836ms step_avg:88.12ms
step:1622/1680 train_time:142925ms step_avg:88.12ms
step:1623/1680 train_time:143015ms step_avg:88.12ms
step:1624/1680 train_time:143104ms step_avg:88.12ms
step:1625/1680 train_time:143192ms step_avg:88.12ms
step:1625/1680 val_loss:3.2893 train_time:143282ms step_avg:88.17ms
step:1626/1680 train_time:143300ms step_avg:88.13ms
step:1627/1680 train_time:143374ms step_avg:88.12ms
step:1628/1680 train_time:143471ms step_avg:88.13ms
step:1629/1680 train_time:143560ms step_avg:88.13ms
step:1630/1680 train_time:143648ms step_avg:88.13ms
step:1631/1680 train_time:143737ms step_avg:88.13ms
step:1632/1680 train_time:143825ms step_avg:88.13ms
step:1633/1680 train_time:143913ms step_avg:88.13ms
step:1634/1680 train_time:144001ms step_avg:88.13ms
step:1635/1680 train_time:144089ms step_avg:88.13ms
step:1636/1680 train_time:144179ms step_avg:88.13ms
step:1637/1680 train_time:144270ms step_avg:88.13ms
step:1638/1680 train_time:144363ms step_avg:88.13ms
step:1639/1680 train_time:144455ms step_avg:88.14ms
step:1640/1680 train_time:144546ms step_avg:88.14ms
step:1641/1680 train_time:144635ms step_avg:88.14ms
step:1642/1680 train_time:144723ms step_avg:88.14ms
step:1643/1680 train_time:144811ms step_avg:88.14ms
step:1644/1680 train_time:144899ms step_avg:88.14ms
step:1645/1680 train_time:144987ms step_avg:88.14ms
step:1646/1680 train_time:145075ms step_avg:88.14ms
step:1647/1680 train_time:145164ms step_avg:88.14ms
step:1648/1680 train_time:145253ms step_avg:88.14ms
step:1649/1680 train_time:145345ms step_avg:88.14ms
step:1650/1680 train_time:145435ms step_avg:88.14ms
step:1651/1680 train_time:145525ms step_avg:88.14ms
step:1652/1680 train_time:145614ms step_avg:88.14ms
step:1653/1680 train_time:145703ms step_avg:88.14ms
step:1654/1680 train_time:145791ms step_avg:88.14ms
step:1655/1680 train_time:145880ms step_avg:88.15ms
step:1656/1680 train_time:145969ms step_avg:88.15ms
step:1657/1680 train_time:146057ms step_avg:88.15ms
step:1658/1680 train_time:146146ms step_avg:88.15ms
step:1659/1680 train_time:146234ms step_avg:88.15ms
step:1660/1680 train_time:146325ms step_avg:88.15ms
step:1661/1680 train_time:146414ms step_avg:88.15ms
step:1662/1680 train_time:146503ms step_avg:88.15ms
step:1663/1680 train_time:146593ms step_avg:88.15ms
step:1664/1680 train_time:146683ms step_avg:88.15ms
step:1665/1680 train_time:146771ms step_avg:88.15ms
step:1666/1680 train_time:146860ms step_avg:88.15ms
step:1667/1680 train_time:146948ms step_avg:88.15ms
step:1668/1680 train_time:147037ms step_avg:88.15ms
step:1669/1680 train_time:147125ms step_avg:88.15ms
step:1670/1680 train_time:147214ms step_avg:88.15ms
step:1671/1680 train_time:147303ms step_avg:88.15ms
step:1672/1680 train_time:147393ms step_avg:88.15ms
step:1673/1680 train_time:147483ms step_avg:88.15ms
step:1674/1680 train_time:147572ms step_avg:88.16ms
step:1675/1680 train_time:147660ms step_avg:88.16ms
step:1676/1680 train_time:147749ms step_avg:88.16ms
step:1677/1680 train_time:147838ms step_avg:88.16ms
step:1678/1680 train_time:147927ms step_avg:88.16ms
step:1679/1680 train_time:148016ms step_avg:88.16ms
step:1680/1680 train_time:148105ms step_avg:88.16ms
step:1680/1680 val_loss:3.2783 train_time:148196ms step_avg:88.21ms
peak memory allocated: 30760 MiB reserved: 45914 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 13:07:16 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   27C    P0            122W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            114W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    168906      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    168907      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    168908      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    168909      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    168910      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    168911      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    168912      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    168913      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    168907      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    168908      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    168909      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    168910      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    168911      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    168912      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    168913      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:145ms step_avg:145.22ms
step:2/1680 train_time:166ms step_avg:82.87ms
step:3/1680 train_time:229ms step_avg:76.30ms
step:4/1680 train_time:314ms step_avg:78.41ms
step:5/1680 train_time:400ms step_avg:79.90ms
step:6/1680 train_time:486ms step_avg:80.96ms
step:7/1680 train_time:572ms step_avg:81.69ms
step:8/1680 train_time:659ms step_avg:82.43ms
step:9/1680 train_time:746ms step_avg:82.88ms
step:10/1680 train_time:833ms step_avg:83.25ms
step:11/1680 train_time:919ms step_avg:83.53ms
step:12/1680 train_time:1006ms step_avg:83.85ms
step:13/1680 train_time:1096ms step_avg:84.33ms
step:14/1680 train_time:1187ms step_avg:84.82ms
step:15/1680 train_time:1275ms step_avg:85.02ms
step:16/1680 train_time:1362ms step_avg:85.14ms
step:17/1680 train_time:1450ms step_avg:85.27ms
step:18/1680 train_time:1536ms step_avg:85.33ms
step:19/1680 train_time:1622ms step_avg:85.35ms
step:20/1680 train_time:1709ms step_avg:85.43ms
step:21/1680 train_time:1795ms step_avg:85.48ms
step:22/1680 train_time:1882ms step_avg:85.54ms
step:23/1680 train_time:1969ms step_avg:85.62ms
step:24/1680 train_time:2057ms step_avg:85.72ms
step:25/1680 train_time:2145ms step_avg:85.81ms
step:26/1680 train_time:2233ms step_avg:85.89ms
step:27/1680 train_time:2321ms step_avg:85.95ms
step:28/1680 train_time:2408ms step_avg:85.99ms
step:29/1680 train_time:2494ms step_avg:86.01ms
step:30/1680 train_time:2583ms step_avg:86.09ms
step:31/1680 train_time:2670ms step_avg:86.12ms
step:32/1680 train_time:2756ms step_avg:86.13ms
step:33/1680 train_time:2843ms step_avg:86.15ms
step:34/1680 train_time:2931ms step_avg:86.20ms
step:35/1680 train_time:3018ms step_avg:86.24ms
step:36/1680 train_time:3107ms step_avg:86.30ms
step:37/1680 train_time:3194ms step_avg:86.33ms
step:38/1680 train_time:3282ms step_avg:86.36ms
step:39/1680 train_time:3370ms step_avg:86.41ms
step:40/1680 train_time:3457ms step_avg:86.42ms
step:41/1680 train_time:3544ms step_avg:86.44ms
step:42/1680 train_time:3631ms step_avg:86.46ms
step:43/1680 train_time:3718ms step_avg:86.46ms
step:44/1680 train_time:3805ms step_avg:86.47ms
step:45/1680 train_time:3892ms step_avg:86.50ms
step:46/1680 train_time:3979ms step_avg:86.51ms
step:47/1680 train_time:4067ms step_avg:86.53ms
step:48/1680 train_time:4155ms step_avg:86.56ms
step:49/1680 train_time:4242ms step_avg:86.58ms
step:50/1680 train_time:4331ms step_avg:86.61ms
step:51/1680 train_time:4417ms step_avg:86.61ms
step:52/1680 train_time:4505ms step_avg:86.63ms
step:53/1680 train_time:4592ms step_avg:86.64ms
step:54/1680 train_time:4679ms step_avg:86.65ms
step:55/1680 train_time:4765ms step_avg:86.64ms
step:56/1680 train_time:4853ms step_avg:86.66ms
step:57/1680 train_time:4940ms step_avg:86.66ms
step:58/1680 train_time:5027ms step_avg:86.67ms
step:59/1680 train_time:5114ms step_avg:86.68ms
step:60/1680 train_time:5202ms step_avg:86.70ms
step:61/1680 train_time:5290ms step_avg:86.72ms
step:62/1680 train_time:5377ms step_avg:86.72ms
step:63/1680 train_time:5464ms step_avg:86.72ms
step:64/1680 train_time:5550ms step_avg:86.72ms
step:65/1680 train_time:5637ms step_avg:86.72ms
step:66/1680 train_time:5724ms step_avg:86.73ms
step:67/1680 train_time:5812ms step_avg:86.74ms
step:68/1680 train_time:5899ms step_avg:86.75ms
step:69/1680 train_time:5986ms step_avg:86.75ms
step:70/1680 train_time:6074ms step_avg:86.77ms
step:71/1680 train_time:6161ms step_avg:86.77ms
step:72/1680 train_time:6248ms step_avg:86.77ms
step:73/1680 train_time:6335ms step_avg:86.78ms
step:74/1680 train_time:6422ms step_avg:86.79ms
step:75/1680 train_time:6509ms step_avg:86.79ms
step:76/1680 train_time:6596ms step_avg:86.79ms
step:77/1680 train_time:6683ms step_avg:86.80ms
step:78/1680 train_time:6771ms step_avg:86.81ms
step:79/1680 train_time:6858ms step_avg:86.81ms
step:80/1680 train_time:6945ms step_avg:86.82ms
step:81/1680 train_time:7033ms step_avg:86.82ms
step:82/1680 train_time:7120ms step_avg:86.83ms
step:83/1680 train_time:7207ms step_avg:86.83ms
step:84/1680 train_time:7295ms step_avg:86.84ms
step:85/1680 train_time:7382ms step_avg:86.85ms
step:86/1680 train_time:7469ms step_avg:86.85ms
step:87/1680 train_time:7556ms step_avg:86.85ms
step:88/1680 train_time:7643ms step_avg:86.86ms
step:89/1680 train_time:7732ms step_avg:86.87ms
step:90/1680 train_time:7819ms step_avg:86.88ms
step:91/1680 train_time:7906ms step_avg:86.88ms
step:92/1680 train_time:7993ms step_avg:86.88ms
step:93/1680 train_time:8080ms step_avg:86.89ms
step:94/1680 train_time:8170ms step_avg:86.91ms
step:95/1680 train_time:8255ms step_avg:86.90ms
step:96/1680 train_time:8342ms step_avg:86.90ms
step:97/1680 train_time:8430ms step_avg:86.91ms
step:98/1680 train_time:8517ms step_avg:86.91ms
step:99/1680 train_time:8604ms step_avg:86.91ms
step:100/1680 train_time:8692ms step_avg:86.92ms
step:101/1680 train_time:8779ms step_avg:86.92ms
step:102/1680 train_time:8866ms step_avg:86.92ms
step:103/1680 train_time:8953ms step_avg:86.92ms
step:104/1680 train_time:9040ms step_avg:86.93ms
step:105/1680 train_time:9128ms step_avg:86.94ms
step:106/1680 train_time:9215ms step_avg:86.94ms
step:107/1680 train_time:9302ms step_avg:86.93ms
step:108/1680 train_time:9390ms step_avg:86.94ms
step:109/1680 train_time:9477ms step_avg:86.94ms
step:110/1680 train_time:9564ms step_avg:86.95ms
step:111/1680 train_time:9651ms step_avg:86.95ms
step:112/1680 train_time:9738ms step_avg:86.95ms
step:113/1680 train_time:9825ms step_avg:86.95ms
step:114/1680 train_time:9912ms step_avg:86.95ms
step:115/1680 train_time:9999ms step_avg:86.95ms
step:116/1680 train_time:10086ms step_avg:86.95ms
step:117/1680 train_time:10173ms step_avg:86.95ms
step:118/1680 train_time:10260ms step_avg:86.95ms
step:119/1680 train_time:10346ms step_avg:86.95ms
step:120/1680 train_time:10434ms step_avg:86.95ms
step:121/1680 train_time:10521ms step_avg:86.95ms
step:122/1680 train_time:10609ms step_avg:86.96ms
step:123/1680 train_time:10696ms step_avg:86.96ms
step:124/1680 train_time:10783ms step_avg:86.96ms
step:125/1680 train_time:10871ms step_avg:86.97ms
step:125/1680 val_loss:4.3173 train_time:10959ms step_avg:87.67ms
step:126/1680 train_time:10982ms step_avg:87.16ms
step:127/1680 train_time:11049ms step_avg:87.00ms
step:128/1680 train_time:11143ms step_avg:87.06ms
step:129/1680 train_time:11233ms step_avg:87.07ms
step:130/1680 train_time:11320ms step_avg:87.08ms
step:131/1680 train_time:11407ms step_avg:87.08ms
step:132/1680 train_time:11493ms step_avg:87.07ms
step:133/1680 train_time:11579ms step_avg:87.06ms
step:134/1680 train_time:11665ms step_avg:87.05ms
step:135/1680 train_time:11751ms step_avg:87.04ms
step:136/1680 train_time:11836ms step_avg:87.03ms
step:137/1680 train_time:11922ms step_avg:87.02ms
step:138/1680 train_time:12009ms step_avg:87.02ms
step:139/1680 train_time:12099ms step_avg:87.04ms
step:140/1680 train_time:12188ms step_avg:87.06ms
step:141/1680 train_time:12276ms step_avg:87.06ms
step:142/1680 train_time:12363ms step_avg:87.07ms
step:143/1680 train_time:12450ms step_avg:87.07ms
step:144/1680 train_time:12537ms step_avg:87.06ms
step:145/1680 train_time:12623ms step_avg:87.06ms
step:146/1680 train_time:12710ms step_avg:87.05ms
step:147/1680 train_time:12796ms step_avg:87.05ms
step:148/1680 train_time:12882ms step_avg:87.04ms
step:149/1680 train_time:12968ms step_avg:87.04ms
step:150/1680 train_time:13056ms step_avg:87.04ms
step:151/1680 train_time:13144ms step_avg:87.05ms
step:152/1680 train_time:13232ms step_avg:87.05ms
step:153/1680 train_time:13320ms step_avg:87.06ms
step:154/1680 train_time:13408ms step_avg:87.06ms
step:155/1680 train_time:13495ms step_avg:87.06ms
step:156/1680 train_time:13582ms step_avg:87.07ms
step:157/1680 train_time:13669ms step_avg:87.06ms
step:158/1680 train_time:13755ms step_avg:87.06ms
step:159/1680 train_time:13842ms step_avg:87.06ms
step:160/1680 train_time:13928ms step_avg:87.05ms
step:161/1680 train_time:14014ms step_avg:87.04ms
step:162/1680 train_time:14102ms step_avg:87.05ms
step:163/1680 train_time:14189ms step_avg:87.05ms
step:164/1680 train_time:14277ms step_avg:87.05ms
step:165/1680 train_time:14364ms step_avg:87.05ms
step:166/1680 train_time:14451ms step_avg:87.06ms
step:167/1680 train_time:14538ms step_avg:87.05ms
step:168/1680 train_time:14625ms step_avg:87.05ms
step:169/1680 train_time:14712ms step_avg:87.05ms
step:170/1680 train_time:14799ms step_avg:87.05ms
step:171/1680 train_time:14886ms step_avg:87.05ms
step:172/1680 train_time:14972ms step_avg:87.05ms
step:173/1680 train_time:15060ms step_avg:87.05ms
step:174/1680 train_time:15147ms step_avg:87.05ms
step:175/1680 train_time:15234ms step_avg:87.05ms
step:176/1680 train_time:15322ms step_avg:87.06ms
step:177/1680 train_time:15409ms step_avg:87.06ms
step:178/1680 train_time:15497ms step_avg:87.06ms
step:179/1680 train_time:15584ms step_avg:87.06ms
step:180/1680 train_time:15670ms step_avg:87.06ms
step:181/1680 train_time:15757ms step_avg:87.05ms
step:182/1680 train_time:15844ms step_avg:87.05ms
step:183/1680 train_time:15931ms step_avg:87.06ms
step:184/1680 train_time:16020ms step_avg:87.07ms
step:185/1680 train_time:16106ms step_avg:87.06ms
step:186/1680 train_time:16193ms step_avg:87.06ms
step:187/1680 train_time:16280ms step_avg:87.06ms
step:188/1680 train_time:16367ms step_avg:87.06ms
step:189/1680 train_time:16454ms step_avg:87.06ms
step:190/1680 train_time:16541ms step_avg:87.06ms
step:191/1680 train_time:16628ms step_avg:87.06ms
step:192/1680 train_time:16715ms step_avg:87.06ms
step:193/1680 train_time:16802ms step_avg:87.06ms
step:194/1680 train_time:16888ms step_avg:87.05ms
step:195/1680 train_time:16976ms step_avg:87.05ms
step:196/1680 train_time:17063ms step_avg:87.05ms
step:197/1680 train_time:17150ms step_avg:87.06ms
step:198/1680 train_time:17237ms step_avg:87.06ms
step:199/1680 train_time:17324ms step_avg:87.06ms
step:200/1680 train_time:17411ms step_avg:87.06ms
step:201/1680 train_time:17498ms step_avg:87.06ms
step:202/1680 train_time:17586ms step_avg:87.06ms
step:203/1680 train_time:17672ms step_avg:87.05ms
step:204/1680 train_time:17759ms step_avg:87.05ms
step:205/1680 train_time:17846ms step_avg:87.05ms
step:206/1680 train_time:17933ms step_avg:87.05ms
step:207/1680 train_time:18020ms step_avg:87.05ms
step:208/1680 train_time:18108ms step_avg:87.06ms
step:209/1680 train_time:18195ms step_avg:87.06ms
step:210/1680 train_time:18282ms step_avg:87.06ms
step:211/1680 train_time:18369ms step_avg:87.06ms
step:212/1680 train_time:18457ms step_avg:87.06ms
step:213/1680 train_time:18544ms step_avg:87.06ms
step:214/1680 train_time:18631ms step_avg:87.06ms
step:215/1680 train_time:18718ms step_avg:87.06ms
step:216/1680 train_time:18805ms step_avg:87.06ms
step:217/1680 train_time:18893ms step_avg:87.06ms
step:218/1680 train_time:18981ms step_avg:87.07ms
step:219/1680 train_time:19068ms step_avg:87.07ms
step:220/1680 train_time:19155ms step_avg:87.07ms
step:221/1680 train_time:19241ms step_avg:87.06ms
step:222/1680 train_time:19328ms step_avg:87.06ms
step:223/1680 train_time:19416ms step_avg:87.07ms
step:224/1680 train_time:19503ms step_avg:87.07ms
step:225/1680 train_time:19590ms step_avg:87.07ms
step:226/1680 train_time:19677ms step_avg:87.07ms
step:227/1680 train_time:19764ms step_avg:87.07ms
step:228/1680 train_time:19851ms step_avg:87.07ms
step:229/1680 train_time:19938ms step_avg:87.07ms
step:230/1680 train_time:20025ms step_avg:87.07ms
step:231/1680 train_time:20112ms step_avg:87.07ms
step:232/1680 train_time:20199ms step_avg:87.07ms
step:233/1680 train_time:20286ms step_avg:87.06ms
step:234/1680 train_time:20373ms step_avg:87.06ms
step:235/1680 train_time:20460ms step_avg:87.06ms
step:236/1680 train_time:20547ms step_avg:87.06ms
step:237/1680 train_time:20635ms step_avg:87.07ms
step:238/1680 train_time:20722ms step_avg:87.07ms
step:239/1680 train_time:20808ms step_avg:87.06ms
step:240/1680 train_time:20897ms step_avg:87.07ms
step:241/1680 train_time:20984ms step_avg:87.07ms
step:242/1680 train_time:21071ms step_avg:87.07ms
step:243/1680 train_time:21157ms step_avg:87.07ms
step:244/1680 train_time:21244ms step_avg:87.07ms
step:245/1680 train_time:21331ms step_avg:87.06ms
step:246/1680 train_time:21418ms step_avg:87.06ms
step:247/1680 train_time:21505ms step_avg:87.06ms
step:248/1680 train_time:21592ms step_avg:87.07ms
step:249/1680 train_time:21680ms step_avg:87.07ms
step:250/1680 train_time:21768ms step_avg:87.07ms
step:250/1680 val_loss:3.9760 train_time:21856ms step_avg:87.42ms
step:251/1680 train_time:21878ms step_avg:87.16ms
step:252/1680 train_time:21944ms step_avg:87.08ms
step:253/1680 train_time:22034ms step_avg:87.09ms
step:254/1680 train_time:22123ms step_avg:87.10ms
step:255/1680 train_time:22209ms step_avg:87.09ms
step:256/1680 train_time:22295ms step_avg:87.09ms
step:257/1680 train_time:22381ms step_avg:87.09ms
step:258/1680 train_time:22468ms step_avg:87.08ms
step:259/1680 train_time:22554ms step_avg:87.08ms
step:260/1680 train_time:22639ms step_avg:87.07ms
step:261/1680 train_time:22726ms step_avg:87.07ms
step:262/1680 train_time:22815ms step_avg:87.08ms
step:263/1680 train_time:22903ms step_avg:87.08ms
step:264/1680 train_time:22992ms step_avg:87.09ms
step:265/1680 train_time:23080ms step_avg:87.09ms
step:266/1680 train_time:23167ms step_avg:87.09ms
step:267/1680 train_time:23253ms step_avg:87.09ms
step:268/1680 train_time:23341ms step_avg:87.09ms
step:269/1680 train_time:23427ms step_avg:87.09ms
step:270/1680 train_time:23513ms step_avg:87.08ms
step:271/1680 train_time:23599ms step_avg:87.08ms
step:272/1680 train_time:23686ms step_avg:87.08ms
step:273/1680 train_time:23773ms step_avg:87.08ms
step:274/1680 train_time:23860ms step_avg:87.08ms
step:275/1680 train_time:23948ms step_avg:87.09ms
step:276/1680 train_time:24036ms step_avg:87.09ms
step:277/1680 train_time:24123ms step_avg:87.09ms
step:278/1680 train_time:24210ms step_avg:87.09ms
step:279/1680 train_time:24297ms step_avg:87.09ms
step:280/1680 train_time:24384ms step_avg:87.09ms
step:281/1680 train_time:24470ms step_avg:87.08ms
step:282/1680 train_time:24557ms step_avg:87.08ms
step:283/1680 train_time:24644ms step_avg:87.08ms
step:284/1680 train_time:24730ms step_avg:87.08ms
step:285/1680 train_time:24817ms step_avg:87.08ms
step:286/1680 train_time:24905ms step_avg:87.08ms
step:287/1680 train_time:24993ms step_avg:87.08ms
step:288/1680 train_time:25079ms step_avg:87.08ms
step:289/1680 train_time:25167ms step_avg:87.08ms
step:290/1680 train_time:25254ms step_avg:87.08ms
step:291/1680 train_time:25341ms step_avg:87.08ms
step:292/1680 train_time:25429ms step_avg:87.08ms
step:293/1680 train_time:25516ms step_avg:87.09ms
step:294/1680 train_time:25602ms step_avg:87.08ms
step:295/1680 train_time:25689ms step_avg:87.08ms
step:296/1680 train_time:25777ms step_avg:87.08ms
step:297/1680 train_time:25864ms step_avg:87.08ms
step:298/1680 train_time:25951ms step_avg:87.09ms
step:299/1680 train_time:26038ms step_avg:87.09ms
step:300/1680 train_time:26126ms step_avg:87.09ms
step:301/1680 train_time:26212ms step_avg:87.08ms
step:302/1680 train_time:26300ms step_avg:87.09ms
step:303/1680 train_time:26387ms step_avg:87.09ms
step:304/1680 train_time:26474ms step_avg:87.09ms
step:305/1680 train_time:26561ms step_avg:87.08ms
step:306/1680 train_time:26648ms step_avg:87.08ms
step:307/1680 train_time:26734ms step_avg:87.08ms
step:308/1680 train_time:26822ms step_avg:87.08ms
step:309/1680 train_time:26908ms step_avg:87.08ms
step:310/1680 train_time:26995ms step_avg:87.08ms
step:311/1680 train_time:27082ms step_avg:87.08ms
step:312/1680 train_time:27169ms step_avg:87.08ms
step:313/1680 train_time:27256ms step_avg:87.08ms
step:314/1680 train_time:27344ms step_avg:87.08ms
step:315/1680 train_time:27431ms step_avg:87.08ms
step:316/1680 train_time:27518ms step_avg:87.08ms
step:317/1680 train_time:27604ms step_avg:87.08ms
step:318/1680 train_time:27692ms step_avg:87.08ms
step:319/1680 train_time:27778ms step_avg:87.08ms
step:320/1680 train_time:27866ms step_avg:87.08ms
step:321/1680 train_time:27952ms step_avg:87.08ms
step:322/1680 train_time:28040ms step_avg:87.08ms
step:323/1680 train_time:28127ms step_avg:87.08ms
step:324/1680 train_time:28214ms step_avg:87.08ms
step:325/1680 train_time:28301ms step_avg:87.08ms
step:326/1680 train_time:28388ms step_avg:87.08ms
step:327/1680 train_time:28476ms step_avg:87.08ms
step:328/1680 train_time:28562ms step_avg:87.08ms
step:329/1680 train_time:28649ms step_avg:87.08ms
step:330/1680 train_time:28735ms step_avg:87.08ms
step:331/1680 train_time:28822ms step_avg:87.08ms
step:332/1680 train_time:28910ms step_avg:87.08ms
step:333/1680 train_time:28997ms step_avg:87.08ms
step:334/1680 train_time:29084ms step_avg:87.08ms
step:335/1680 train_time:29171ms step_avg:87.08ms
step:336/1680 train_time:29258ms step_avg:87.08ms
step:337/1680 train_time:29346ms step_avg:87.08ms
step:338/1680 train_time:29433ms step_avg:87.08ms
step:339/1680 train_time:29520ms step_avg:87.08ms
step:340/1680 train_time:29607ms step_avg:87.08ms
step:341/1680 train_time:29694ms step_avg:87.08ms
step:342/1680 train_time:29780ms step_avg:87.08ms
step:343/1680 train_time:29868ms step_avg:87.08ms
step:344/1680 train_time:29955ms step_avg:87.08ms
step:345/1680 train_time:30042ms step_avg:87.08ms
step:346/1680 train_time:30128ms step_avg:87.08ms
step:347/1680 train_time:30216ms step_avg:87.08ms
step:348/1680 train_time:30302ms step_avg:87.08ms
step:349/1680 train_time:30389ms step_avg:87.08ms
step:350/1680 train_time:30476ms step_avg:87.07ms
step:351/1680 train_time:30563ms step_avg:87.07ms
step:352/1680 train_time:30649ms step_avg:87.07ms
step:353/1680 train_time:30736ms step_avg:87.07ms
step:354/1680 train_time:30824ms step_avg:87.07ms
step:355/1680 train_time:30910ms step_avg:87.07ms
step:356/1680 train_time:30997ms step_avg:87.07ms
step:357/1680 train_time:31084ms step_avg:87.07ms
step:358/1680 train_time:31172ms step_avg:87.07ms
step:359/1680 train_time:31259ms step_avg:87.07ms
step:360/1680 train_time:31346ms step_avg:87.07ms
step:361/1680 train_time:31432ms step_avg:87.07ms
step:362/1680 train_time:31519ms step_avg:87.07ms
step:363/1680 train_time:31606ms step_avg:87.07ms
step:364/1680 train_time:31693ms step_avg:87.07ms
step:365/1680 train_time:31780ms step_avg:87.07ms
step:366/1680 train_time:31867ms step_avg:87.07ms
step:367/1680 train_time:31953ms step_avg:87.07ms
step:368/1680 train_time:32041ms step_avg:87.07ms
step:369/1680 train_time:32128ms step_avg:87.07ms
step:370/1680 train_time:32215ms step_avg:87.07ms
step:371/1680 train_time:32302ms step_avg:87.07ms
step:372/1680 train_time:32389ms step_avg:87.07ms
step:373/1680 train_time:32476ms step_avg:87.07ms
step:374/1680 train_time:32563ms step_avg:87.07ms
step:375/1680 train_time:32650ms step_avg:87.07ms
step:375/1680 val_loss:3.8203 train_time:32739ms step_avg:87.30ms
step:376/1680 train_time:32757ms step_avg:87.12ms
step:377/1680 train_time:32831ms step_avg:87.09ms
step:378/1680 train_time:32921ms step_avg:87.09ms
step:379/1680 train_time:33010ms step_avg:87.10ms
step:380/1680 train_time:33097ms step_avg:87.10ms
step:381/1680 train_time:33183ms step_avg:87.10ms
step:382/1680 train_time:33269ms step_avg:87.09ms
step:383/1680 train_time:33355ms step_avg:87.09ms
step:384/1680 train_time:33441ms step_avg:87.09ms
step:385/1680 train_time:33527ms step_avg:87.08ms
step:386/1680 train_time:33613ms step_avg:87.08ms
step:387/1680 train_time:33701ms step_avg:87.08ms
step:388/1680 train_time:33789ms step_avg:87.09ms
step:389/1680 train_time:33879ms step_avg:87.09ms
step:390/1680 train_time:33967ms step_avg:87.10ms
step:391/1680 train_time:34055ms step_avg:87.10ms
step:392/1680 train_time:34142ms step_avg:87.10ms
step:393/1680 train_time:34230ms step_avg:87.10ms
step:394/1680 train_time:34315ms step_avg:87.09ms
step:395/1680 train_time:34401ms step_avg:87.09ms
step:396/1680 train_time:34488ms step_avg:87.09ms
step:397/1680 train_time:34574ms step_avg:87.09ms
step:398/1680 train_time:34660ms step_avg:87.09ms
step:399/1680 train_time:34749ms step_avg:87.09ms
step:400/1680 train_time:34837ms step_avg:87.09ms
step:401/1680 train_time:34926ms step_avg:87.10ms
step:402/1680 train_time:35014ms step_avg:87.10ms
step:403/1680 train_time:35101ms step_avg:87.10ms
step:404/1680 train_time:35187ms step_avg:87.10ms
step:405/1680 train_time:35274ms step_avg:87.10ms
step:406/1680 train_time:35360ms step_avg:87.09ms
step:407/1680 train_time:35446ms step_avg:87.09ms
step:408/1680 train_time:35533ms step_avg:87.09ms
step:409/1680 train_time:35619ms step_avg:87.09ms
step:410/1680 train_time:35706ms step_avg:87.09ms
step:411/1680 train_time:35794ms step_avg:87.09ms
step:412/1680 train_time:35882ms step_avg:87.09ms
step:413/1680 train_time:35969ms step_avg:87.09ms
step:414/1680 train_time:36057ms step_avg:87.09ms
step:415/1680 train_time:36144ms step_avg:87.10ms
step:416/1680 train_time:36231ms step_avg:87.09ms
step:417/1680 train_time:36318ms step_avg:87.09ms
step:418/1680 train_time:36405ms step_avg:87.09ms
step:419/1680 train_time:36492ms step_avg:87.09ms
step:420/1680 train_time:36578ms step_avg:87.09ms
step:421/1680 train_time:36665ms step_avg:87.09ms
step:422/1680 train_time:36752ms step_avg:87.09ms
step:423/1680 train_time:36840ms step_avg:87.09ms
step:424/1680 train_time:36928ms step_avg:87.09ms
step:425/1680 train_time:37016ms step_avg:87.10ms
step:426/1680 train_time:37103ms step_avg:87.10ms
step:427/1680 train_time:37190ms step_avg:87.10ms
step:428/1680 train_time:37277ms step_avg:87.10ms
step:429/1680 train_time:37364ms step_avg:87.10ms
step:430/1680 train_time:37450ms step_avg:87.09ms
step:431/1680 train_time:37537ms step_avg:87.09ms
step:432/1680 train_time:37623ms step_avg:87.09ms
step:433/1680 train_time:37710ms step_avg:87.09ms
step:434/1680 train_time:37798ms step_avg:87.09ms
step:435/1680 train_time:37885ms step_avg:87.09ms
step:436/1680 train_time:37972ms step_avg:87.09ms
step:437/1680 train_time:38060ms step_avg:87.09ms
step:438/1680 train_time:38148ms step_avg:87.10ms
step:439/1680 train_time:38235ms step_avg:87.10ms
step:440/1680 train_time:38322ms step_avg:87.10ms
step:441/1680 train_time:38409ms step_avg:87.10ms
step:442/1680 train_time:38496ms step_avg:87.10ms
step:443/1680 train_time:38584ms step_avg:87.10ms
step:444/1680 train_time:38670ms step_avg:87.09ms
step:445/1680 train_time:38757ms step_avg:87.09ms
step:446/1680 train_time:38844ms step_avg:87.09ms
step:447/1680 train_time:38931ms step_avg:87.09ms
step:448/1680 train_time:39019ms step_avg:87.10ms
step:449/1680 train_time:39106ms step_avg:87.10ms
step:450/1680 train_time:39193ms step_avg:87.10ms
step:451/1680 train_time:39280ms step_avg:87.09ms
step:452/1680 train_time:39367ms step_avg:87.09ms
step:453/1680 train_time:39453ms step_avg:87.09ms
step:454/1680 train_time:39541ms step_avg:87.09ms
step:455/1680 train_time:39628ms step_avg:87.09ms
step:456/1680 train_time:39714ms step_avg:87.09ms
step:457/1680 train_time:39802ms step_avg:87.09ms
step:458/1680 train_time:39889ms step_avg:87.09ms
step:459/1680 train_time:39976ms step_avg:87.09ms
step:460/1680 train_time:40063ms step_avg:87.09ms
step:461/1680 train_time:40150ms step_avg:87.09ms
step:462/1680 train_time:40237ms step_avg:87.09ms
step:463/1680 train_time:40324ms step_avg:87.09ms
step:464/1680 train_time:40411ms step_avg:87.09ms
step:465/1680 train_time:40499ms step_avg:87.09ms
step:466/1680 train_time:40585ms step_avg:87.09ms
step:467/1680 train_time:40672ms step_avg:87.09ms
step:468/1680 train_time:40759ms step_avg:87.09ms
step:469/1680 train_time:40847ms step_avg:87.09ms
step:470/1680 train_time:40934ms step_avg:87.09ms
step:471/1680 train_time:41021ms step_avg:87.09ms
step:472/1680 train_time:41108ms step_avg:87.09ms
step:473/1680 train_time:41195ms step_avg:87.09ms
step:474/1680 train_time:41283ms step_avg:87.09ms
step:475/1680 train_time:41371ms step_avg:87.10ms
step:476/1680 train_time:41458ms step_avg:87.10ms
step:477/1680 train_time:41545ms step_avg:87.10ms
step:478/1680 train_time:41632ms step_avg:87.10ms
step:479/1680 train_time:41719ms step_avg:87.10ms
step:480/1680 train_time:41807ms step_avg:87.10ms
step:481/1680 train_time:41894ms step_avg:87.10ms
step:482/1680 train_time:41981ms step_avg:87.10ms
step:483/1680 train_time:42069ms step_avg:87.10ms
step:484/1680 train_time:42155ms step_avg:87.10ms
step:485/1680 train_time:42242ms step_avg:87.10ms
step:486/1680 train_time:42329ms step_avg:87.10ms
step:487/1680 train_time:42416ms step_avg:87.10ms
step:488/1680 train_time:42504ms step_avg:87.10ms
step:489/1680 train_time:42590ms step_avg:87.10ms
step:490/1680 train_time:42677ms step_avg:87.10ms
step:491/1680 train_time:42764ms step_avg:87.10ms
step:492/1680 train_time:42851ms step_avg:87.10ms
step:493/1680 train_time:42939ms step_avg:87.10ms
step:494/1680 train_time:43025ms step_avg:87.10ms
step:495/1680 train_time:43112ms step_avg:87.10ms
step:496/1680 train_time:43199ms step_avg:87.09ms
step:497/1680 train_time:43286ms step_avg:87.09ms
step:498/1680 train_time:43373ms step_avg:87.09ms
step:499/1680 train_time:43460ms step_avg:87.09ms
step:500/1680 train_time:43547ms step_avg:87.09ms
step:500/1680 val_loss:3.7188 train_time:43635ms step_avg:87.27ms
step:501/1680 train_time:43654ms step_avg:87.13ms
step:502/1680 train_time:43723ms step_avg:87.10ms
step:503/1680 train_time:43817ms step_avg:87.11ms
step:504/1680 train_time:43906ms step_avg:87.12ms
step:505/1680 train_time:43993ms step_avg:87.12ms
step:506/1680 train_time:44080ms step_avg:87.11ms
step:507/1680 train_time:44166ms step_avg:87.11ms
step:508/1680 train_time:44252ms step_avg:87.11ms
step:509/1680 train_time:44338ms step_avg:87.11ms
step:510/1680 train_time:44424ms step_avg:87.11ms
step:511/1680 train_time:44511ms step_avg:87.11ms
step:512/1680 train_time:44599ms step_avg:87.11ms
step:513/1680 train_time:44686ms step_avg:87.11ms
step:514/1680 train_time:44775ms step_avg:87.11ms
step:515/1680 train_time:44864ms step_avg:87.12ms
step:516/1680 train_time:44952ms step_avg:87.12ms
step:517/1680 train_time:45039ms step_avg:87.12ms
step:518/1680 train_time:45126ms step_avg:87.12ms
step:519/1680 train_time:45212ms step_avg:87.11ms
step:520/1680 train_time:45298ms step_avg:87.11ms
step:521/1680 train_time:45385ms step_avg:87.11ms
step:522/1680 train_time:45472ms step_avg:87.11ms
step:523/1680 train_time:45559ms step_avg:87.11ms
step:524/1680 train_time:45646ms step_avg:87.11ms
step:525/1680 train_time:45734ms step_avg:87.11ms
step:526/1680 train_time:45822ms step_avg:87.11ms
step:527/1680 train_time:45911ms step_avg:87.12ms
step:528/1680 train_time:45998ms step_avg:87.12ms
step:529/1680 train_time:46085ms step_avg:87.12ms
step:530/1680 train_time:46172ms step_avg:87.12ms
step:531/1680 train_time:46258ms step_avg:87.12ms
step:532/1680 train_time:46345ms step_avg:87.12ms
step:533/1680 train_time:46432ms step_avg:87.11ms
step:534/1680 train_time:46518ms step_avg:87.11ms
step:535/1680 train_time:46606ms step_avg:87.11ms
step:536/1680 train_time:46694ms step_avg:87.12ms
step:537/1680 train_time:46782ms step_avg:87.12ms
step:538/1680 train_time:46871ms step_avg:87.12ms
step:539/1680 train_time:46958ms step_avg:87.12ms
step:540/1680 train_time:47045ms step_avg:87.12ms
step:541/1680 train_time:47132ms step_avg:87.12ms
step:542/1680 train_time:47218ms step_avg:87.12ms
step:543/1680 train_time:47305ms step_avg:87.12ms
step:544/1680 train_time:47392ms step_avg:87.12ms
step:545/1680 train_time:47479ms step_avg:87.12ms
step:546/1680 train_time:47565ms step_avg:87.12ms
step:547/1680 train_time:47652ms step_avg:87.12ms
step:548/1680 train_time:47739ms step_avg:87.12ms
step:549/1680 train_time:47828ms step_avg:87.12ms
step:550/1680 train_time:47917ms step_avg:87.12ms
step:551/1680 train_time:48005ms step_avg:87.12ms
step:552/1680 train_time:48094ms step_avg:87.13ms
step:553/1680 train_time:48183ms step_avg:87.13ms
step:554/1680 train_time:48272ms step_avg:87.13ms
step:555/1680 train_time:48359ms step_avg:87.13ms
step:556/1680 train_time:48446ms step_avg:87.13ms
step:557/1680 train_time:48534ms step_avg:87.13ms
step:558/1680 train_time:48623ms step_avg:87.14ms
step:559/1680 train_time:48711ms step_avg:87.14ms
step:560/1680 train_time:48799ms step_avg:87.14ms
step:561/1680 train_time:48888ms step_avg:87.14ms
step:562/1680 train_time:48976ms step_avg:87.15ms
step:563/1680 train_time:49064ms step_avg:87.15ms
step:564/1680 train_time:49152ms step_avg:87.15ms
step:565/1680 train_time:49240ms step_avg:87.15ms
step:566/1680 train_time:49328ms step_avg:87.15ms
step:567/1680 train_time:49416ms step_avg:87.15ms
step:568/1680 train_time:49504ms step_avg:87.16ms
step:569/1680 train_time:49592ms step_avg:87.16ms
step:570/1680 train_time:49680ms step_avg:87.16ms
step:571/1680 train_time:49768ms step_avg:87.16ms
step:572/1680 train_time:49856ms step_avg:87.16ms
step:573/1680 train_time:49944ms step_avg:87.16ms
step:574/1680 train_time:50032ms step_avg:87.16ms
step:575/1680 train_time:50120ms step_avg:87.17ms
step:576/1680 train_time:50209ms step_avg:87.17ms
step:577/1680 train_time:50297ms step_avg:87.17ms
step:578/1680 train_time:50385ms step_avg:87.17ms
step:579/1680 train_time:50474ms step_avg:87.17ms
step:580/1680 train_time:50563ms step_avg:87.18ms
step:581/1680 train_time:50651ms step_avg:87.18ms
step:582/1680 train_time:50739ms step_avg:87.18ms
step:583/1680 train_time:50828ms step_avg:87.18ms
step:584/1680 train_time:50916ms step_avg:87.18ms
step:585/1680 train_time:51004ms step_avg:87.19ms
step:586/1680 train_time:51093ms step_avg:87.19ms
step:587/1680 train_time:51181ms step_avg:87.19ms
step:588/1680 train_time:51269ms step_avg:87.19ms
step:589/1680 train_time:51357ms step_avg:87.19ms
step:590/1680 train_time:51446ms step_avg:87.20ms
step:591/1680 train_time:51534ms step_avg:87.20ms
step:592/1680 train_time:51624ms step_avg:87.20ms
step:593/1680 train_time:51711ms step_avg:87.20ms
step:594/1680 train_time:51799ms step_avg:87.20ms
step:595/1680 train_time:51888ms step_avg:87.21ms
step:596/1680 train_time:51976ms step_avg:87.21ms
step:597/1680 train_time:52064ms step_avg:87.21ms
step:598/1680 train_time:52153ms step_avg:87.21ms
step:599/1680 train_time:52241ms step_avg:87.21ms
step:600/1680 train_time:52329ms step_avg:87.22ms
step:601/1680 train_time:52417ms step_avg:87.22ms
step:602/1680 train_time:52505ms step_avg:87.22ms
step:603/1680 train_time:52594ms step_avg:87.22ms
step:604/1680 train_time:52682ms step_avg:87.22ms
step:605/1680 train_time:52771ms step_avg:87.22ms
step:606/1680 train_time:52859ms step_avg:87.23ms
step:607/1680 train_time:52947ms step_avg:87.23ms
step:608/1680 train_time:53035ms step_avg:87.23ms
step:609/1680 train_time:53123ms step_avg:87.23ms
step:610/1680 train_time:53211ms step_avg:87.23ms
step:611/1680 train_time:53299ms step_avg:87.23ms
step:612/1680 train_time:53387ms step_avg:87.23ms
step:613/1680 train_time:53475ms step_avg:87.24ms
step:614/1680 train_time:53564ms step_avg:87.24ms
step:615/1680 train_time:53652ms step_avg:87.24ms
step:616/1680 train_time:53741ms step_avg:87.24ms
step:617/1680 train_time:53829ms step_avg:87.24ms
step:618/1680 train_time:53917ms step_avg:87.24ms
step:619/1680 train_time:54005ms step_avg:87.25ms
step:620/1680 train_time:54093ms step_avg:87.25ms
step:621/1680 train_time:54182ms step_avg:87.25ms
step:622/1680 train_time:54270ms step_avg:87.25ms
step:623/1680 train_time:54358ms step_avg:87.25ms
step:624/1680 train_time:54446ms step_avg:87.25ms
step:625/1680 train_time:54534ms step_avg:87.25ms
step:625/1680 val_loss:3.6184 train_time:54624ms step_avg:87.40ms
step:626/1680 train_time:54643ms step_avg:87.29ms
step:627/1680 train_time:54712ms step_avg:87.26ms
step:628/1680 train_time:54801ms step_avg:87.26ms
step:629/1680 train_time:54892ms step_avg:87.27ms
step:630/1680 train_time:54983ms step_avg:87.28ms
step:631/1680 train_time:55070ms step_avg:87.27ms
step:632/1680 train_time:55157ms step_avg:87.27ms
step:633/1680 train_time:55244ms step_avg:87.27ms
step:634/1680 train_time:55331ms step_avg:87.27ms
step:635/1680 train_time:55418ms step_avg:87.27ms
step:636/1680 train_time:55506ms step_avg:87.27ms
step:637/1680 train_time:55597ms step_avg:87.28ms
step:638/1680 train_time:55687ms step_avg:87.28ms
step:639/1680 train_time:55776ms step_avg:87.29ms
step:640/1680 train_time:55865ms step_avg:87.29ms
step:641/1680 train_time:55953ms step_avg:87.29ms
step:642/1680 train_time:56042ms step_avg:87.29ms
step:643/1680 train_time:56131ms step_avg:87.29ms
step:644/1680 train_time:56218ms step_avg:87.29ms
step:645/1680 train_time:56305ms step_avg:87.30ms
step:646/1680 train_time:56393ms step_avg:87.30ms
step:647/1680 train_time:56480ms step_avg:87.30ms
step:648/1680 train_time:56569ms step_avg:87.30ms
step:649/1680 train_time:56658ms step_avg:87.30ms
step:650/1680 train_time:56747ms step_avg:87.30ms
step:651/1680 train_time:56836ms step_avg:87.30ms
step:652/1680 train_time:56924ms step_avg:87.31ms
step:653/1680 train_time:57013ms step_avg:87.31ms
step:654/1680 train_time:57100ms step_avg:87.31ms
step:655/1680 train_time:57188ms step_avg:87.31ms
step:656/1680 train_time:57275ms step_avg:87.31ms
step:657/1680 train_time:57363ms step_avg:87.31ms
step:658/1680 train_time:57450ms step_avg:87.31ms
step:659/1680 train_time:57538ms step_avg:87.31ms
step:660/1680 train_time:57626ms step_avg:87.31ms
step:661/1680 train_time:57715ms step_avg:87.31ms
step:662/1680 train_time:57803ms step_avg:87.32ms
step:663/1680 train_time:57892ms step_avg:87.32ms
step:664/1680 train_time:57981ms step_avg:87.32ms
step:665/1680 train_time:58069ms step_avg:87.32ms
step:666/1680 train_time:58157ms step_avg:87.32ms
step:667/1680 train_time:58244ms step_avg:87.32ms
step:668/1680 train_time:58331ms step_avg:87.32ms
step:669/1680 train_time:58418ms step_avg:87.32ms
step:670/1680 train_time:58507ms step_avg:87.32ms
step:671/1680 train_time:58595ms step_avg:87.32ms
step:672/1680 train_time:58683ms step_avg:87.33ms
step:673/1680 train_time:58771ms step_avg:87.33ms
step:674/1680 train_time:58860ms step_avg:87.33ms
step:675/1680 train_time:58948ms step_avg:87.33ms
step:676/1680 train_time:59035ms step_avg:87.33ms
step:677/1680 train_time:59124ms step_avg:87.33ms
step:678/1680 train_time:59212ms step_avg:87.33ms
step:679/1680 train_time:59300ms step_avg:87.33ms
step:680/1680 train_time:59388ms step_avg:87.33ms
step:681/1680 train_time:59475ms step_avg:87.34ms
step:682/1680 train_time:59563ms step_avg:87.34ms
step:683/1680 train_time:59652ms step_avg:87.34ms
step:684/1680 train_time:59740ms step_avg:87.34ms
step:685/1680 train_time:59829ms step_avg:87.34ms
step:686/1680 train_time:59917ms step_avg:87.34ms
step:687/1680 train_time:60005ms step_avg:87.34ms
step:688/1680 train_time:60093ms step_avg:87.34ms
step:689/1680 train_time:60181ms step_avg:87.35ms
step:690/1680 train_time:60270ms step_avg:87.35ms
step:691/1680 train_time:60358ms step_avg:87.35ms
step:692/1680 train_time:60445ms step_avg:87.35ms
step:693/1680 train_time:60533ms step_avg:87.35ms
step:694/1680 train_time:60621ms step_avg:87.35ms
step:695/1680 train_time:60710ms step_avg:87.35ms
step:696/1680 train_time:60798ms step_avg:87.35ms
step:697/1680 train_time:60885ms step_avg:87.35ms
step:698/1680 train_time:60974ms step_avg:87.35ms
step:699/1680 train_time:61061ms step_avg:87.36ms
step:700/1680 train_time:61150ms step_avg:87.36ms
step:701/1680 train_time:61238ms step_avg:87.36ms
step:702/1680 train_time:61327ms step_avg:87.36ms
step:703/1680 train_time:61415ms step_avg:87.36ms
step:704/1680 train_time:61502ms step_avg:87.36ms
step:705/1680 train_time:61591ms step_avg:87.36ms
step:706/1680 train_time:61679ms step_avg:87.36ms
step:707/1680 train_time:61768ms step_avg:87.37ms
step:708/1680 train_time:61855ms step_avg:87.37ms
step:709/1680 train_time:61944ms step_avg:87.37ms
step:710/1680 train_time:62032ms step_avg:87.37ms
step:711/1680 train_time:62120ms step_avg:87.37ms
step:712/1680 train_time:62207ms step_avg:87.37ms
step:713/1680 train_time:62295ms step_avg:87.37ms
step:714/1680 train_time:62384ms step_avg:87.37ms
step:715/1680 train_time:62472ms step_avg:87.37ms
step:716/1680 train_time:62561ms step_avg:87.38ms
step:717/1680 train_time:62649ms step_avg:87.38ms
step:718/1680 train_time:62737ms step_avg:87.38ms
step:719/1680 train_time:62825ms step_avg:87.38ms
step:720/1680 train_time:62913ms step_avg:87.38ms
step:721/1680 train_time:63001ms step_avg:87.38ms
step:722/1680 train_time:63089ms step_avg:87.38ms
step:723/1680 train_time:63177ms step_avg:87.38ms
step:724/1680 train_time:63265ms step_avg:87.38ms
step:725/1680 train_time:63353ms step_avg:87.38ms
step:726/1680 train_time:63442ms step_avg:87.39ms
step:727/1680 train_time:63530ms step_avg:87.39ms
step:728/1680 train_time:63618ms step_avg:87.39ms
step:729/1680 train_time:63706ms step_avg:87.39ms
step:730/1680 train_time:63794ms step_avg:87.39ms
step:731/1680 train_time:63882ms step_avg:87.39ms
step:732/1680 train_time:63970ms step_avg:87.39ms
step:733/1680 train_time:64059ms step_avg:87.39ms
step:734/1680 train_time:64147ms step_avg:87.39ms
step:735/1680 train_time:64235ms step_avg:87.39ms
step:736/1680 train_time:64323ms step_avg:87.40ms
step:737/1680 train_time:64411ms step_avg:87.40ms
step:738/1680 train_time:64499ms step_avg:87.40ms
step:739/1680 train_time:64587ms step_avg:87.40ms
step:740/1680 train_time:64675ms step_avg:87.40ms
step:741/1680 train_time:64763ms step_avg:87.40ms
step:742/1680 train_time:64852ms step_avg:87.40ms
step:743/1680 train_time:64941ms step_avg:87.40ms
step:744/1680 train_time:65029ms step_avg:87.40ms
step:745/1680 train_time:65117ms step_avg:87.41ms
step:746/1680 train_time:65205ms step_avg:87.41ms
step:747/1680 train_time:65293ms step_avg:87.41ms
step:748/1680 train_time:65381ms step_avg:87.41ms
step:749/1680 train_time:65469ms step_avg:87.41ms
step:750/1680 train_time:65558ms step_avg:87.41ms
step:750/1680 val_loss:3.5686 train_time:65647ms step_avg:87.53ms
step:751/1680 train_time:65665ms step_avg:87.44ms
step:752/1680 train_time:65737ms step_avg:87.42ms
step:753/1680 train_time:65829ms step_avg:87.42ms
step:754/1680 train_time:65918ms step_avg:87.42ms
step:755/1680 train_time:66006ms step_avg:87.42ms
step:756/1680 train_time:66093ms step_avg:87.43ms
step:757/1680 train_time:66181ms step_avg:87.42ms
step:758/1680 train_time:66267ms step_avg:87.42ms
step:759/1680 train_time:66354ms step_avg:87.42ms
step:760/1680 train_time:66442ms step_avg:87.42ms
step:761/1680 train_time:66530ms step_avg:87.42ms
step:762/1680 train_time:66618ms step_avg:87.42ms
step:763/1680 train_time:66708ms step_avg:87.43ms
step:764/1680 train_time:66798ms step_avg:87.43ms
step:765/1680 train_time:66887ms step_avg:87.43ms
step:766/1680 train_time:66975ms step_avg:87.43ms
step:767/1680 train_time:67063ms step_avg:87.44ms
step:768/1680 train_time:67151ms step_avg:87.44ms
step:769/1680 train_time:67239ms step_avg:87.44ms
step:770/1680 train_time:67327ms step_avg:87.44ms
step:771/1680 train_time:67414ms step_avg:87.44ms
step:772/1680 train_time:67501ms step_avg:87.44ms
step:773/1680 train_time:67589ms step_avg:87.44ms
step:774/1680 train_time:67677ms step_avg:87.44ms
step:775/1680 train_time:67766ms step_avg:87.44ms
step:776/1680 train_time:67856ms step_avg:87.44ms
step:777/1680 train_time:67944ms step_avg:87.44ms
step:778/1680 train_time:68033ms step_avg:87.45ms
step:779/1680 train_time:68121ms step_avg:87.45ms
step:780/1680 train_time:68209ms step_avg:87.45ms
step:781/1680 train_time:68296ms step_avg:87.45ms
step:782/1680 train_time:68385ms step_avg:87.45ms
step:783/1680 train_time:68472ms step_avg:87.45ms
step:784/1680 train_time:68560ms step_avg:87.45ms
step:785/1680 train_time:68648ms step_avg:87.45ms
step:786/1680 train_time:68736ms step_avg:87.45ms
step:787/1680 train_time:68827ms step_avg:87.45ms
step:788/1680 train_time:68917ms step_avg:87.46ms
step:789/1680 train_time:69005ms step_avg:87.46ms
step:790/1680 train_time:69094ms step_avg:87.46ms
step:791/1680 train_time:69182ms step_avg:87.46ms
step:792/1680 train_time:69270ms step_avg:87.46ms
step:793/1680 train_time:69357ms step_avg:87.46ms
step:794/1680 train_time:69445ms step_avg:87.46ms
step:795/1680 train_time:69533ms step_avg:87.46ms
step:796/1680 train_time:69621ms step_avg:87.46ms
step:797/1680 train_time:69708ms step_avg:87.46ms
step:798/1680 train_time:69797ms step_avg:87.46ms
step:799/1680 train_time:69886ms step_avg:87.47ms
step:800/1680 train_time:69974ms step_avg:87.47ms
step:801/1680 train_time:70062ms step_avg:87.47ms
step:802/1680 train_time:70150ms step_avg:87.47ms
step:803/1680 train_time:70238ms step_avg:87.47ms
step:804/1680 train_time:70326ms step_avg:87.47ms
step:805/1680 train_time:70414ms step_avg:87.47ms
step:806/1680 train_time:70501ms step_avg:87.47ms
step:807/1680 train_time:70589ms step_avg:87.47ms
step:808/1680 train_time:70677ms step_avg:87.47ms
step:809/1680 train_time:70765ms step_avg:87.47ms
step:810/1680 train_time:70853ms step_avg:87.47ms
step:811/1680 train_time:70942ms step_avg:87.47ms
step:812/1680 train_time:71030ms step_avg:87.48ms
step:813/1680 train_time:71118ms step_avg:87.48ms
step:814/1680 train_time:71206ms step_avg:87.48ms
step:815/1680 train_time:71294ms step_avg:87.48ms
step:816/1680 train_time:71382ms step_avg:87.48ms
step:817/1680 train_time:71470ms step_avg:87.48ms
step:818/1680 train_time:71559ms step_avg:87.48ms
step:819/1680 train_time:71646ms step_avg:87.48ms
step:820/1680 train_time:71735ms step_avg:87.48ms
step:821/1680 train_time:71824ms step_avg:87.48ms
step:822/1680 train_time:71911ms step_avg:87.48ms
step:823/1680 train_time:72000ms step_avg:87.48ms
step:824/1680 train_time:72088ms step_avg:87.49ms
step:825/1680 train_time:72177ms step_avg:87.49ms
step:826/1680 train_time:72264ms step_avg:87.49ms
step:827/1680 train_time:72352ms step_avg:87.49ms
step:828/1680 train_time:72439ms step_avg:87.49ms
step:829/1680 train_time:72528ms step_avg:87.49ms
step:830/1680 train_time:72616ms step_avg:87.49ms
step:831/1680 train_time:72703ms step_avg:87.49ms
step:832/1680 train_time:72791ms step_avg:87.49ms
step:833/1680 train_time:72880ms step_avg:87.49ms
step:834/1680 train_time:72968ms step_avg:87.49ms
step:835/1680 train_time:73056ms step_avg:87.49ms
step:836/1680 train_time:73144ms step_avg:87.49ms
step:837/1680 train_time:73232ms step_avg:87.49ms
step:838/1680 train_time:73320ms step_avg:87.49ms
step:839/1680 train_time:73408ms step_avg:87.50ms
step:840/1680 train_time:73496ms step_avg:87.50ms
step:841/1680 train_time:73585ms step_avg:87.50ms
step:842/1680 train_time:73673ms step_avg:87.50ms
step:843/1680 train_time:73761ms step_avg:87.50ms
step:844/1680 train_time:73849ms step_avg:87.50ms
step:845/1680 train_time:73937ms step_avg:87.50ms
step:846/1680 train_time:74026ms step_avg:87.50ms
step:847/1680 train_time:74114ms step_avg:87.50ms
step:848/1680 train_time:74202ms step_avg:87.50ms
step:849/1680 train_time:74290ms step_avg:87.50ms
step:850/1680 train_time:74378ms step_avg:87.50ms
step:851/1680 train_time:74466ms step_avg:87.50ms
step:852/1680 train_time:74554ms step_avg:87.51ms
step:853/1680 train_time:74643ms step_avg:87.51ms
step:854/1680 train_time:74730ms step_avg:87.51ms
step:855/1680 train_time:74818ms step_avg:87.51ms
step:856/1680 train_time:74907ms step_avg:87.51ms
step:857/1680 train_time:74996ms step_avg:87.51ms
step:858/1680 train_time:75085ms step_avg:87.51ms
step:859/1680 train_time:75173ms step_avg:87.51ms
step:860/1680 train_time:75261ms step_avg:87.51ms
step:861/1680 train_time:75349ms step_avg:87.51ms
step:862/1680 train_time:75438ms step_avg:87.51ms
step:863/1680 train_time:75526ms step_avg:87.52ms
step:864/1680 train_time:75614ms step_avg:87.52ms
step:865/1680 train_time:75701ms step_avg:87.52ms
step:866/1680 train_time:75790ms step_avg:87.52ms
step:867/1680 train_time:75878ms step_avg:87.52ms
step:868/1680 train_time:75966ms step_avg:87.52ms
step:869/1680 train_time:76055ms step_avg:87.52ms
step:870/1680 train_time:76143ms step_avg:87.52ms
step:871/1680 train_time:76231ms step_avg:87.52ms
step:872/1680 train_time:76318ms step_avg:87.52ms
step:873/1680 train_time:76407ms step_avg:87.52ms
step:874/1680 train_time:76495ms step_avg:87.52ms
step:875/1680 train_time:76583ms step_avg:87.52ms
step:875/1680 val_loss:3.5223 train_time:76672ms step_avg:87.63ms
step:876/1680 train_time:76691ms step_avg:87.55ms
step:877/1680 train_time:76763ms step_avg:87.53ms
step:878/1680 train_time:76855ms step_avg:87.53ms
step:879/1680 train_time:76943ms step_avg:87.54ms
step:880/1680 train_time:77031ms step_avg:87.53ms
step:881/1680 train_time:77117ms step_avg:87.53ms
step:882/1680 train_time:77204ms step_avg:87.53ms
step:883/1680 train_time:77291ms step_avg:87.53ms
step:884/1680 train_time:77378ms step_avg:87.53ms
step:885/1680 train_time:77466ms step_avg:87.53ms
step:886/1680 train_time:77554ms step_avg:87.53ms
step:887/1680 train_time:77643ms step_avg:87.53ms
step:888/1680 train_time:77733ms step_avg:87.54ms
step:889/1680 train_time:77823ms step_avg:87.54ms
step:890/1680 train_time:77911ms step_avg:87.54ms
step:891/1680 train_time:77999ms step_avg:87.54ms
step:892/1680 train_time:78087ms step_avg:87.54ms
step:893/1680 train_time:78174ms step_avg:87.54ms
step:894/1680 train_time:78261ms step_avg:87.54ms
step:895/1680 train_time:78349ms step_avg:87.54ms
step:896/1680 train_time:78437ms step_avg:87.54ms
step:897/1680 train_time:78525ms step_avg:87.54ms
step:898/1680 train_time:78614ms step_avg:87.54ms
step:899/1680 train_time:78703ms step_avg:87.54ms
step:900/1680 train_time:78791ms step_avg:87.55ms
step:901/1680 train_time:78880ms step_avg:87.55ms
step:902/1680 train_time:78969ms step_avg:87.55ms
step:903/1680 train_time:79057ms step_avg:87.55ms
step:904/1680 train_time:79145ms step_avg:87.55ms
step:905/1680 train_time:79232ms step_avg:87.55ms
step:906/1680 train_time:79319ms step_avg:87.55ms
step:907/1680 train_time:79406ms step_avg:87.55ms
step:908/1680 train_time:79495ms step_avg:87.55ms
step:909/1680 train_time:79582ms step_avg:87.55ms
step:910/1680 train_time:79672ms step_avg:87.55ms
step:911/1680 train_time:79760ms step_avg:87.55ms
step:912/1680 train_time:79849ms step_avg:87.55ms
step:913/1680 train_time:79937ms step_avg:87.55ms
step:914/1680 train_time:80025ms step_avg:87.56ms
step:915/1680 train_time:80113ms step_avg:87.56ms
step:916/1680 train_time:80201ms step_avg:87.56ms
step:917/1680 train_time:80289ms step_avg:87.56ms
step:918/1680 train_time:80376ms step_avg:87.56ms
step:919/1680 train_time:80464ms step_avg:87.56ms
step:920/1680 train_time:80553ms step_avg:87.56ms
step:921/1680 train_time:80641ms step_avg:87.56ms
step:922/1680 train_time:80730ms step_avg:87.56ms
step:923/1680 train_time:80819ms step_avg:87.56ms
step:924/1680 train_time:80907ms step_avg:87.56ms
step:925/1680 train_time:80996ms step_avg:87.56ms
step:926/1680 train_time:81084ms step_avg:87.56ms
step:927/1680 train_time:81172ms step_avg:87.56ms
step:928/1680 train_time:81260ms step_avg:87.56ms
step:929/1680 train_time:81348ms step_avg:87.56ms
step:930/1680 train_time:81436ms step_avg:87.57ms
step:931/1680 train_time:81524ms step_avg:87.57ms
step:932/1680 train_time:81612ms step_avg:87.57ms
step:933/1680 train_time:81701ms step_avg:87.57ms
step:934/1680 train_time:81789ms step_avg:87.57ms
step:935/1680 train_time:81877ms step_avg:87.57ms
step:936/1680 train_time:81966ms step_avg:87.57ms
step:937/1680 train_time:82054ms step_avg:87.57ms
step:938/1680 train_time:82142ms step_avg:87.57ms
step:939/1680 train_time:82230ms step_avg:87.57ms
step:940/1680 train_time:82318ms step_avg:87.57ms
step:941/1680 train_time:82406ms step_avg:87.57ms
step:942/1680 train_time:82494ms step_avg:87.57ms
step:943/1680 train_time:82582ms step_avg:87.57ms
step:944/1680 train_time:82670ms step_avg:87.57ms
step:945/1680 train_time:82758ms step_avg:87.57ms
step:946/1680 train_time:82846ms step_avg:87.57ms
step:947/1680 train_time:82935ms step_avg:87.58ms
step:948/1680 train_time:83023ms step_avg:87.58ms
step:949/1680 train_time:83111ms step_avg:87.58ms
step:950/1680 train_time:83198ms step_avg:87.58ms
step:951/1680 train_time:83287ms step_avg:87.58ms
step:952/1680 train_time:83375ms step_avg:87.58ms
step:953/1680 train_time:83462ms step_avg:87.58ms
step:954/1680 train_time:83550ms step_avg:87.58ms
step:955/1680 train_time:83638ms step_avg:87.58ms
step:956/1680 train_time:83726ms step_avg:87.58ms
step:957/1680 train_time:83815ms step_avg:87.58ms
step:958/1680 train_time:83904ms step_avg:87.58ms
step:959/1680 train_time:83991ms step_avg:87.58ms
step:960/1680 train_time:84079ms step_avg:87.58ms
step:961/1680 train_time:84167ms step_avg:87.58ms
step:962/1680 train_time:84255ms step_avg:87.58ms
step:963/1680 train_time:84343ms step_avg:87.58ms
step:964/1680 train_time:84431ms step_avg:87.58ms
step:965/1680 train_time:84519ms step_avg:87.58ms
step:966/1680 train_time:84607ms step_avg:87.59ms
step:967/1680 train_time:84696ms step_avg:87.59ms
step:968/1680 train_time:84784ms step_avg:87.59ms
step:969/1680 train_time:84873ms step_avg:87.59ms
step:970/1680 train_time:84961ms step_avg:87.59ms
step:971/1680 train_time:85049ms step_avg:87.59ms
step:972/1680 train_time:85137ms step_avg:87.59ms
step:973/1680 train_time:85225ms step_avg:87.59ms
step:974/1680 train_time:85313ms step_avg:87.59ms
step:975/1680 train_time:85401ms step_avg:87.59ms
step:976/1680 train_time:85489ms step_avg:87.59ms
step:977/1680 train_time:85577ms step_avg:87.59ms
step:978/1680 train_time:85665ms step_avg:87.59ms
step:979/1680 train_time:85754ms step_avg:87.59ms
step:980/1680 train_time:85842ms step_avg:87.59ms
step:981/1680 train_time:85931ms step_avg:87.60ms
step:982/1680 train_time:86019ms step_avg:87.60ms
step:983/1680 train_time:86108ms step_avg:87.60ms
step:984/1680 train_time:86196ms step_avg:87.60ms
step:985/1680 train_time:86285ms step_avg:87.60ms
step:986/1680 train_time:86373ms step_avg:87.60ms
step:987/1680 train_time:86461ms step_avg:87.60ms
step:988/1680 train_time:86548ms step_avg:87.60ms
step:989/1680 train_time:86636ms step_avg:87.60ms
step:990/1680 train_time:86724ms step_avg:87.60ms
step:991/1680 train_time:86812ms step_avg:87.60ms
step:992/1680 train_time:86901ms step_avg:87.60ms
step:993/1680 train_time:86989ms step_avg:87.60ms
step:994/1680 train_time:87077ms step_avg:87.60ms
step:995/1680 train_time:87166ms step_avg:87.60ms
step:996/1680 train_time:87254ms step_avg:87.60ms
step:997/1680 train_time:87342ms step_avg:87.60ms
step:998/1680 train_time:87430ms step_avg:87.61ms
step:999/1680 train_time:87518ms step_avg:87.61ms
step:1000/1680 train_time:87605ms step_avg:87.61ms
step:1000/1680 val_loss:3.4714 train_time:87695ms step_avg:87.69ms
step:1001/1680 train_time:87713ms step_avg:87.63ms
step:1002/1680 train_time:87784ms step_avg:87.61ms
step:1003/1680 train_time:87878ms step_avg:87.61ms
step:1004/1680 train_time:87967ms step_avg:87.62ms
step:1005/1680 train_time:88054ms step_avg:87.62ms
step:1006/1680 train_time:88142ms step_avg:87.62ms
step:1007/1680 train_time:88229ms step_avg:87.62ms
step:1008/1680 train_time:88317ms step_avg:87.62ms
step:1009/1680 train_time:88405ms step_avg:87.62ms
step:1010/1680 train_time:88492ms step_avg:87.62ms
step:1011/1680 train_time:88579ms step_avg:87.62ms
step:1012/1680 train_time:88668ms step_avg:87.62ms
step:1013/1680 train_time:88757ms step_avg:87.62ms
step:1014/1680 train_time:88848ms step_avg:87.62ms
step:1015/1680 train_time:88938ms step_avg:87.62ms
step:1016/1680 train_time:89026ms step_avg:87.62ms
step:1017/1680 train_time:89115ms step_avg:87.63ms
step:1018/1680 train_time:89202ms step_avg:87.62ms
step:1019/1680 train_time:89290ms step_avg:87.62ms
step:1020/1680 train_time:89377ms step_avg:87.62ms
step:1021/1680 train_time:89465ms step_avg:87.63ms
step:1022/1680 train_time:89553ms step_avg:87.62ms
step:1023/1680 train_time:89641ms step_avg:87.63ms
step:1024/1680 train_time:89730ms step_avg:87.63ms
step:1025/1680 train_time:89819ms step_avg:87.63ms
step:1026/1680 train_time:89909ms step_avg:87.63ms
step:1027/1680 train_time:89998ms step_avg:87.63ms
step:1028/1680 train_time:90087ms step_avg:87.63ms
step:1029/1680 train_time:90175ms step_avg:87.63ms
step:1030/1680 train_time:90262ms step_avg:87.63ms
step:1031/1680 train_time:90349ms step_avg:87.63ms
step:1032/1680 train_time:90438ms step_avg:87.63ms
step:1033/1680 train_time:90525ms step_avg:87.63ms
step:1034/1680 train_time:90613ms step_avg:87.63ms
step:1035/1680 train_time:90701ms step_avg:87.63ms
step:1036/1680 train_time:90790ms step_avg:87.64ms
step:1037/1680 train_time:90879ms step_avg:87.64ms
step:1038/1680 train_time:90967ms step_avg:87.64ms
step:1039/1680 train_time:91056ms step_avg:87.64ms
step:1040/1680 train_time:91144ms step_avg:87.64ms
step:1041/1680 train_time:91232ms step_avg:87.64ms
step:1042/1680 train_time:91320ms step_avg:87.64ms
step:1043/1680 train_time:91408ms step_avg:87.64ms
step:1044/1680 train_time:91496ms step_avg:87.64ms
step:1045/1680 train_time:91584ms step_avg:87.64ms
step:1046/1680 train_time:91672ms step_avg:87.64ms
step:1047/1680 train_time:91761ms step_avg:87.64ms
step:1048/1680 train_time:91849ms step_avg:87.64ms
step:1049/1680 train_time:91937ms step_avg:87.64ms
step:1050/1680 train_time:92026ms step_avg:87.64ms
step:1051/1680 train_time:92114ms step_avg:87.64ms
step:1052/1680 train_time:92202ms step_avg:87.64ms
step:1053/1680 train_time:92290ms step_avg:87.64ms
step:1054/1680 train_time:92378ms step_avg:87.65ms
step:1055/1680 train_time:92466ms step_avg:87.65ms
step:1056/1680 train_time:92554ms step_avg:87.65ms
step:1057/1680 train_time:92643ms step_avg:87.65ms
step:1058/1680 train_time:92731ms step_avg:87.65ms
step:1059/1680 train_time:92819ms step_avg:87.65ms
step:1060/1680 train_time:92908ms step_avg:87.65ms
step:1061/1680 train_time:92996ms step_avg:87.65ms
step:1062/1680 train_time:93085ms step_avg:87.65ms
step:1063/1680 train_time:93173ms step_avg:87.65ms
step:1064/1680 train_time:93261ms step_avg:87.65ms
step:1065/1680 train_time:93349ms step_avg:87.65ms
step:1066/1680 train_time:93438ms step_avg:87.65ms
step:1067/1680 train_time:93526ms step_avg:87.65ms
step:1068/1680 train_time:93613ms step_avg:87.65ms
step:1069/1680 train_time:93702ms step_avg:87.65ms
step:1070/1680 train_time:93790ms step_avg:87.65ms
step:1071/1680 train_time:93878ms step_avg:87.65ms
step:1072/1680 train_time:93966ms step_avg:87.66ms
step:1073/1680 train_time:94056ms step_avg:87.66ms
step:1074/1680 train_time:94143ms step_avg:87.66ms
step:1075/1680 train_time:94231ms step_avg:87.66ms
step:1076/1680 train_time:94320ms step_avg:87.66ms
step:1077/1680 train_time:94409ms step_avg:87.66ms
step:1078/1680 train_time:94497ms step_avg:87.66ms
step:1079/1680 train_time:94586ms step_avg:87.66ms
step:1080/1680 train_time:94674ms step_avg:87.66ms
step:1081/1680 train_time:94762ms step_avg:87.66ms
step:1082/1680 train_time:94851ms step_avg:87.66ms
step:1083/1680 train_time:94939ms step_avg:87.66ms
step:1084/1680 train_time:95028ms step_avg:87.66ms
step:1085/1680 train_time:95116ms step_avg:87.66ms
step:1086/1680 train_time:95204ms step_avg:87.67ms
step:1087/1680 train_time:95293ms step_avg:87.67ms
step:1088/1680 train_time:95381ms step_avg:87.67ms
step:1089/1680 train_time:95470ms step_avg:87.67ms
step:1090/1680 train_time:95558ms step_avg:87.67ms
step:1091/1680 train_time:95646ms step_avg:87.67ms
step:1092/1680 train_time:95734ms step_avg:87.67ms
step:1093/1680 train_time:95822ms step_avg:87.67ms
step:1094/1680 train_time:95910ms step_avg:87.67ms
step:1095/1680 train_time:95999ms step_avg:87.67ms
step:1096/1680 train_time:96088ms step_avg:87.67ms
step:1097/1680 train_time:96176ms step_avg:87.67ms
step:1098/1680 train_time:96265ms step_avg:87.67ms
step:1099/1680 train_time:96355ms step_avg:87.67ms
step:1100/1680 train_time:96443ms step_avg:87.68ms
step:1101/1680 train_time:96533ms step_avg:87.68ms
step:1102/1680 train_time:96622ms step_avg:87.68ms
step:1103/1680 train_time:96710ms step_avg:87.68ms
step:1104/1680 train_time:96799ms step_avg:87.68ms
step:1105/1680 train_time:96889ms step_avg:87.68ms
step:1106/1680 train_time:96977ms step_avg:87.68ms
step:1107/1680 train_time:97066ms step_avg:87.68ms
step:1108/1680 train_time:97154ms step_avg:87.68ms
step:1109/1680 train_time:97244ms step_avg:87.69ms
step:1110/1680 train_time:97332ms step_avg:87.69ms
step:1111/1680 train_time:97422ms step_avg:87.69ms
step:1112/1680 train_time:97511ms step_avg:87.69ms
step:1113/1680 train_time:97600ms step_avg:87.69ms
step:1114/1680 train_time:97690ms step_avg:87.69ms
step:1115/1680 train_time:97778ms step_avg:87.69ms
step:1116/1680 train_time:97867ms step_avg:87.69ms
step:1117/1680 train_time:97956ms step_avg:87.70ms
step:1118/1680 train_time:98044ms step_avg:87.70ms
step:1119/1680 train_time:98133ms step_avg:87.70ms
step:1120/1680 train_time:98222ms step_avg:87.70ms
step:1121/1680 train_time:98311ms step_avg:87.70ms
step:1122/1680 train_time:98401ms step_avg:87.70ms
step:1123/1680 train_time:98490ms step_avg:87.70ms
step:1124/1680 train_time:98579ms step_avg:87.70ms
step:1125/1680 train_time:98668ms step_avg:87.70ms
step:1125/1680 val_loss:3.4178 train_time:98758ms step_avg:87.78ms
step:1126/1680 train_time:98777ms step_avg:87.72ms
step:1127/1680 train_time:98849ms step_avg:87.71ms
step:1128/1680 train_time:98939ms step_avg:87.71ms
step:1129/1680 train_time:99031ms step_avg:87.72ms
step:1130/1680 train_time:99119ms step_avg:87.72ms
step:1131/1680 train_time:99207ms step_avg:87.72ms
step:1132/1680 train_time:99294ms step_avg:87.72ms
step:1133/1680 train_time:99382ms step_avg:87.72ms
step:1134/1680 train_time:99469ms step_avg:87.72ms
step:1135/1680 train_time:99557ms step_avg:87.72ms
step:1136/1680 train_time:99647ms step_avg:87.72ms
step:1137/1680 train_time:99739ms step_avg:87.72ms
step:1138/1680 train_time:99829ms step_avg:87.72ms
step:1139/1680 train_time:99920ms step_avg:87.73ms
step:1140/1680 train_time:100010ms step_avg:87.73ms
step:1141/1680 train_time:100099ms step_avg:87.73ms
step:1142/1680 train_time:100187ms step_avg:87.73ms
step:1143/1680 train_time:100276ms step_avg:87.73ms
step:1144/1680 train_time:100364ms step_avg:87.73ms
step:1145/1680 train_time:100453ms step_avg:87.73ms
step:1146/1680 train_time:100541ms step_avg:87.73ms
step:1147/1680 train_time:100630ms step_avg:87.73ms
step:1148/1680 train_time:100719ms step_avg:87.73ms
step:1149/1680 train_time:100809ms step_avg:87.74ms
step:1150/1680 train_time:100898ms step_avg:87.74ms
step:1151/1680 train_time:100988ms step_avg:87.74ms
step:1152/1680 train_time:101077ms step_avg:87.74ms
step:1153/1680 train_time:101166ms step_avg:87.74ms
step:1154/1680 train_time:101255ms step_avg:87.74ms
step:1155/1680 train_time:101344ms step_avg:87.74ms
step:1156/1680 train_time:101432ms step_avg:87.74ms
step:1157/1680 train_time:101520ms step_avg:87.74ms
step:1158/1680 train_time:101608ms step_avg:87.74ms
step:1159/1680 train_time:101697ms step_avg:87.75ms
step:1160/1680 train_time:101787ms step_avg:87.75ms
step:1161/1680 train_time:101876ms step_avg:87.75ms
step:1162/1680 train_time:101966ms step_avg:87.75ms
step:1163/1680 train_time:102055ms step_avg:87.75ms
step:1164/1680 train_time:102145ms step_avg:87.75ms
step:1165/1680 train_time:102234ms step_avg:87.75ms
step:1166/1680 train_time:102323ms step_avg:87.76ms
step:1167/1680 train_time:102411ms step_avg:87.76ms
step:1168/1680 train_time:102500ms step_avg:87.76ms
step:1169/1680 train_time:102589ms step_avg:87.76ms
step:1170/1680 train_time:102677ms step_avg:87.76ms
step:1171/1680 train_time:102766ms step_avg:87.76ms
step:1172/1680 train_time:102854ms step_avg:87.76ms
step:1173/1680 train_time:102945ms step_avg:87.76ms
step:1174/1680 train_time:103033ms step_avg:87.76ms
step:1175/1680 train_time:103122ms step_avg:87.76ms
step:1176/1680 train_time:103211ms step_avg:87.76ms
step:1177/1680 train_time:103300ms step_avg:87.77ms
step:1178/1680 train_time:103388ms step_avg:87.77ms
step:1179/1680 train_time:103476ms step_avg:87.77ms
step:1180/1680 train_time:103565ms step_avg:87.77ms
step:1181/1680 train_time:103654ms step_avg:87.77ms
step:1182/1680 train_time:103744ms step_avg:87.77ms
step:1183/1680 train_time:103834ms step_avg:87.77ms
step:1184/1680 train_time:103923ms step_avg:87.77ms
step:1185/1680 train_time:104012ms step_avg:87.77ms
step:1186/1680 train_time:104102ms step_avg:87.78ms
step:1187/1680 train_time:104190ms step_avg:87.78ms
step:1188/1680 train_time:104279ms step_avg:87.78ms
step:1189/1680 train_time:104368ms step_avg:87.78ms
step:1190/1680 train_time:104457ms step_avg:87.78ms
step:1191/1680 train_time:104545ms step_avg:87.78ms
step:1192/1680 train_time:104634ms step_avg:87.78ms
step:1193/1680 train_time:104722ms step_avg:87.78ms
step:1194/1680 train_time:104812ms step_avg:87.78ms
step:1195/1680 train_time:104901ms step_avg:87.78ms
step:1196/1680 train_time:104990ms step_avg:87.78ms
step:1197/1680 train_time:105079ms step_avg:87.78ms
step:1198/1680 train_time:105168ms step_avg:87.79ms
step:1199/1680 train_time:105257ms step_avg:87.79ms
step:1200/1680 train_time:105345ms step_avg:87.79ms
step:1201/1680 train_time:105434ms step_avg:87.79ms
step:1202/1680 train_time:105523ms step_avg:87.79ms
step:1203/1680 train_time:105612ms step_avg:87.79ms
step:1204/1680 train_time:105702ms step_avg:87.79ms
step:1205/1680 train_time:105790ms step_avg:87.79ms
step:1206/1680 train_time:105879ms step_avg:87.79ms
step:1207/1680 train_time:105968ms step_avg:87.79ms
step:1208/1680 train_time:106057ms step_avg:87.80ms
step:1209/1680 train_time:106146ms step_avg:87.80ms
step:1210/1680 train_time:106235ms step_avg:87.80ms
step:1211/1680 train_time:106324ms step_avg:87.80ms
step:1212/1680 train_time:106413ms step_avg:87.80ms
step:1213/1680 train_time:106502ms step_avg:87.80ms
step:1214/1680 train_time:106591ms step_avg:87.80ms
step:1215/1680 train_time:106680ms step_avg:87.80ms
step:1216/1680 train_time:106769ms step_avg:87.80ms
step:1217/1680 train_time:106858ms step_avg:87.80ms
step:1218/1680 train_time:106948ms step_avg:87.81ms
step:1219/1680 train_time:107036ms step_avg:87.81ms
step:1220/1680 train_time:107125ms step_avg:87.81ms
step:1221/1680 train_time:107213ms step_avg:87.81ms
step:1222/1680 train_time:107302ms step_avg:87.81ms
step:1223/1680 train_time:107391ms step_avg:87.81ms
step:1224/1680 train_time:107480ms step_avg:87.81ms
step:1225/1680 train_time:107569ms step_avg:87.81ms
step:1226/1680 train_time:107658ms step_avg:87.81ms
step:1227/1680 train_time:107747ms step_avg:87.81ms
step:1228/1680 train_time:107836ms step_avg:87.81ms
step:1229/1680 train_time:107925ms step_avg:87.81ms
step:1230/1680 train_time:108013ms step_avg:87.82ms
step:1231/1680 train_time:108102ms step_avg:87.82ms
step:1232/1680 train_time:108190ms step_avg:87.82ms
step:1233/1680 train_time:108280ms step_avg:87.82ms
step:1234/1680 train_time:108368ms step_avg:87.82ms
step:1235/1680 train_time:108457ms step_avg:87.82ms
step:1236/1680 train_time:108546ms step_avg:87.82ms
step:1237/1680 train_time:108635ms step_avg:87.82ms
step:1238/1680 train_time:108725ms step_avg:87.82ms
step:1239/1680 train_time:108814ms step_avg:87.82ms
step:1240/1680 train_time:108903ms step_avg:87.82ms
step:1241/1680 train_time:108991ms step_avg:87.82ms
step:1242/1680 train_time:109079ms step_avg:87.83ms
step:1243/1680 train_time:109168ms step_avg:87.83ms
step:1244/1680 train_time:109257ms step_avg:87.83ms
step:1245/1680 train_time:109345ms step_avg:87.83ms
step:1246/1680 train_time:109434ms step_avg:87.83ms
step:1247/1680 train_time:109523ms step_avg:87.83ms
step:1248/1680 train_time:109612ms step_avg:87.83ms
step:1249/1680 train_time:109701ms step_avg:87.83ms
step:1250/1680 train_time:109790ms step_avg:87.83ms
step:1250/1680 val_loss:3.3795 train_time:109880ms step_avg:87.90ms
step:1251/1680 train_time:109899ms step_avg:87.85ms
step:1252/1680 train_time:109971ms step_avg:87.84ms
step:1253/1680 train_time:110063ms step_avg:87.84ms
step:1254/1680 train_time:110153ms step_avg:87.84ms
step:1255/1680 train_time:110241ms step_avg:87.84ms
step:1256/1680 train_time:110329ms step_avg:87.84ms
step:1257/1680 train_time:110417ms step_avg:87.84ms
step:1258/1680 train_time:110504ms step_avg:87.84ms
step:1259/1680 train_time:110592ms step_avg:87.84ms
step:1260/1680 train_time:110680ms step_avg:87.84ms
step:1261/1680 train_time:110768ms step_avg:87.84ms
step:1262/1680 train_time:110858ms step_avg:87.84ms
step:1263/1680 train_time:110949ms step_avg:87.85ms
step:1264/1680 train_time:111039ms step_avg:87.85ms
step:1265/1680 train_time:111129ms step_avg:87.85ms
step:1266/1680 train_time:111218ms step_avg:87.85ms
step:1267/1680 train_time:111307ms step_avg:87.85ms
step:1268/1680 train_time:111395ms step_avg:87.85ms
step:1269/1680 train_time:111485ms step_avg:87.85ms
step:1270/1680 train_time:111572ms step_avg:87.85ms
step:1271/1680 train_time:111660ms step_avg:87.85ms
step:1272/1680 train_time:111748ms step_avg:87.85ms
step:1273/1680 train_time:111838ms step_avg:87.85ms
step:1274/1680 train_time:111927ms step_avg:87.86ms
step:1275/1680 train_time:112018ms step_avg:87.86ms
step:1276/1680 train_time:112107ms step_avg:87.86ms
step:1277/1680 train_time:112196ms step_avg:87.86ms
step:1278/1680 train_time:112285ms step_avg:87.86ms
step:1279/1680 train_time:112373ms step_avg:87.86ms
step:1280/1680 train_time:112461ms step_avg:87.86ms
step:1281/1680 train_time:112549ms step_avg:87.86ms
step:1282/1680 train_time:112638ms step_avg:87.86ms
step:1283/1680 train_time:112728ms step_avg:87.86ms
step:1284/1680 train_time:112817ms step_avg:87.86ms
step:1285/1680 train_time:112906ms step_avg:87.86ms
step:1286/1680 train_time:112996ms step_avg:87.87ms
step:1287/1680 train_time:113086ms step_avg:87.87ms
step:1288/1680 train_time:113175ms step_avg:87.87ms
step:1289/1680 train_time:113264ms step_avg:87.87ms
step:1290/1680 train_time:113353ms step_avg:87.87ms
step:1291/1680 train_time:113442ms step_avg:87.87ms
step:1292/1680 train_time:113530ms step_avg:87.87ms
step:1293/1680 train_time:113618ms step_avg:87.87ms
step:1294/1680 train_time:113707ms step_avg:87.87ms
step:1295/1680 train_time:113796ms step_avg:87.87ms
step:1296/1680 train_time:113885ms step_avg:87.87ms
step:1297/1680 train_time:113975ms step_avg:87.88ms
step:1298/1680 train_time:114066ms step_avg:87.88ms
step:1299/1680 train_time:114155ms step_avg:87.88ms
step:1300/1680 train_time:114243ms step_avg:87.88ms
step:1301/1680 train_time:114332ms step_avg:87.88ms
step:1302/1680 train_time:114421ms step_avg:87.88ms
step:1303/1680 train_time:114510ms step_avg:87.88ms
step:1304/1680 train_time:114599ms step_avg:87.88ms
step:1305/1680 train_time:114689ms step_avg:87.88ms
step:1306/1680 train_time:114777ms step_avg:87.88ms
step:1307/1680 train_time:114866ms step_avg:87.89ms
step:1308/1680 train_time:114955ms step_avg:87.89ms
step:1309/1680 train_time:115045ms step_avg:87.89ms
step:1310/1680 train_time:115134ms step_avg:87.89ms
step:1311/1680 train_time:115224ms step_avg:87.89ms
step:1312/1680 train_time:115314ms step_avg:87.89ms
step:1313/1680 train_time:115403ms step_avg:87.89ms
step:1314/1680 train_time:115492ms step_avg:87.89ms
step:1315/1680 train_time:115581ms step_avg:87.89ms
step:1316/1680 train_time:115670ms step_avg:87.90ms
step:1317/1680 train_time:115759ms step_avg:87.90ms
step:1318/1680 train_time:115847ms step_avg:87.90ms
step:1319/1680 train_time:115936ms step_avg:87.90ms
step:1320/1680 train_time:116025ms step_avg:87.90ms
step:1321/1680 train_time:116114ms step_avg:87.90ms
step:1322/1680 train_time:116204ms step_avg:87.90ms
step:1323/1680 train_time:116294ms step_avg:87.90ms
step:1324/1680 train_time:116382ms step_avg:87.90ms
step:1325/1680 train_time:116471ms step_avg:87.90ms
step:1326/1680 train_time:116560ms step_avg:87.90ms
step:1327/1680 train_time:116649ms step_avg:87.90ms
step:1328/1680 train_time:116737ms step_avg:87.90ms
step:1329/1680 train_time:116826ms step_avg:87.91ms
step:1330/1680 train_time:116915ms step_avg:87.91ms
step:1331/1680 train_time:117004ms step_avg:87.91ms
step:1332/1680 train_time:117094ms step_avg:87.91ms
step:1333/1680 train_time:117183ms step_avg:87.91ms
step:1334/1680 train_time:117272ms step_avg:87.91ms
step:1335/1680 train_time:117360ms step_avg:87.91ms
step:1336/1680 train_time:117450ms step_avg:87.91ms
step:1337/1680 train_time:117539ms step_avg:87.91ms
step:1338/1680 train_time:117628ms step_avg:87.91ms
step:1339/1680 train_time:117716ms step_avg:87.91ms
step:1340/1680 train_time:117806ms step_avg:87.91ms
step:1341/1680 train_time:117894ms step_avg:87.92ms
step:1342/1680 train_time:117982ms step_avg:87.92ms
step:1343/1680 train_time:118071ms step_avg:87.92ms
step:1344/1680 train_time:118161ms step_avg:87.92ms
step:1345/1680 train_time:118250ms step_avg:87.92ms
step:1346/1680 train_time:118338ms step_avg:87.92ms
step:1347/1680 train_time:118428ms step_avg:87.92ms
step:1348/1680 train_time:118517ms step_avg:87.92ms
step:1349/1680 train_time:118606ms step_avg:87.92ms
step:1350/1680 train_time:118695ms step_avg:87.92ms
step:1351/1680 train_time:118785ms step_avg:87.92ms
step:1352/1680 train_time:118874ms step_avg:87.92ms
step:1353/1680 train_time:118964ms step_avg:87.93ms
step:1354/1680 train_time:119053ms step_avg:87.93ms
step:1355/1680 train_time:119142ms step_avg:87.93ms
step:1356/1680 train_time:119232ms step_avg:87.93ms
step:1357/1680 train_time:119321ms step_avg:87.93ms
step:1358/1680 train_time:119410ms step_avg:87.93ms
step:1359/1680 train_time:119498ms step_avg:87.93ms
step:1360/1680 train_time:119588ms step_avg:87.93ms
step:1361/1680 train_time:119676ms step_avg:87.93ms
step:1362/1680 train_time:119765ms step_avg:87.93ms
step:1363/1680 train_time:119854ms step_avg:87.93ms
step:1364/1680 train_time:119942ms step_avg:87.93ms
step:1365/1680 train_time:120031ms step_avg:87.93ms
step:1366/1680 train_time:120120ms step_avg:87.94ms
step:1367/1680 train_time:120209ms step_avg:87.94ms
step:1368/1680 train_time:120298ms step_avg:87.94ms
step:1369/1680 train_time:120388ms step_avg:87.94ms
step:1370/1680 train_time:120476ms step_avg:87.94ms
step:1371/1680 train_time:120565ms step_avg:87.94ms
step:1372/1680 train_time:120654ms step_avg:87.94ms
step:1373/1680 train_time:120744ms step_avg:87.94ms
step:1374/1680 train_time:120833ms step_avg:87.94ms
step:1375/1680 train_time:120923ms step_avg:87.94ms
step:1375/1680 val_loss:3.3446 train_time:121013ms step_avg:88.01ms
step:1376/1680 train_time:121032ms step_avg:87.96ms
step:1377/1680 train_time:121105ms step_avg:87.95ms
step:1378/1680 train_time:121196ms step_avg:87.95ms
step:1379/1680 train_time:121285ms step_avg:87.95ms
step:1380/1680 train_time:121373ms step_avg:87.95ms
step:1381/1680 train_time:121461ms step_avg:87.95ms
step:1382/1680 train_time:121548ms step_avg:87.95ms
step:1383/1680 train_time:121636ms step_avg:87.95ms
step:1384/1680 train_time:121724ms step_avg:87.95ms
step:1385/1680 train_time:121812ms step_avg:87.95ms
step:1386/1680 train_time:121901ms step_avg:87.95ms
step:1387/1680 train_time:121990ms step_avg:87.95ms
step:1388/1680 train_time:122081ms step_avg:87.95ms
step:1389/1680 train_time:122172ms step_avg:87.96ms
step:1390/1680 train_time:122261ms step_avg:87.96ms
step:1391/1680 train_time:122350ms step_avg:87.96ms
step:1392/1680 train_time:122439ms step_avg:87.96ms
step:1393/1680 train_time:122527ms step_avg:87.96ms
step:1394/1680 train_time:122615ms step_avg:87.96ms
step:1395/1680 train_time:122703ms step_avg:87.96ms
step:1396/1680 train_time:122790ms step_avg:87.96ms
step:1397/1680 train_time:122878ms step_avg:87.96ms
step:1398/1680 train_time:122968ms step_avg:87.96ms
step:1399/1680 train_time:123058ms step_avg:87.96ms
step:1400/1680 train_time:123148ms step_avg:87.96ms
step:1401/1680 train_time:123238ms step_avg:87.96ms
step:1402/1680 train_time:123327ms step_avg:87.96ms
step:1403/1680 train_time:123417ms step_avg:87.97ms
step:1404/1680 train_time:123506ms step_avg:87.97ms
step:1405/1680 train_time:123594ms step_avg:87.97ms
step:1406/1680 train_time:123682ms step_avg:87.97ms
step:1407/1680 train_time:123770ms step_avg:87.97ms
step:1408/1680 train_time:123859ms step_avg:87.97ms
step:1409/1680 train_time:123948ms step_avg:87.97ms
step:1410/1680 train_time:124037ms step_avg:87.97ms
step:1411/1680 train_time:124126ms step_avg:87.97ms
step:1412/1680 train_time:124216ms step_avg:87.97ms
step:1413/1680 train_time:124306ms step_avg:87.97ms
step:1414/1680 train_time:124395ms step_avg:87.97ms
step:1415/1680 train_time:124484ms step_avg:87.97ms
step:1416/1680 train_time:124573ms step_avg:87.98ms
step:1417/1680 train_time:124663ms step_avg:87.98ms
step:1418/1680 train_time:124751ms step_avg:87.98ms
step:1419/1680 train_time:124839ms step_avg:87.98ms
step:1420/1680 train_time:124928ms step_avg:87.98ms
step:1421/1680 train_time:125017ms step_avg:87.98ms
step:1422/1680 train_time:125105ms step_avg:87.98ms
step:1423/1680 train_time:125195ms step_avg:87.98ms
step:1424/1680 train_time:125284ms step_avg:87.98ms
step:1425/1680 train_time:125373ms step_avg:87.98ms
step:1426/1680 train_time:125462ms step_avg:87.98ms
step:1427/1680 train_time:125551ms step_avg:87.98ms
step:1428/1680 train_time:125640ms step_avg:87.98ms
step:1429/1680 train_time:125729ms step_avg:87.98ms
step:1430/1680 train_time:125818ms step_avg:87.98ms
step:1431/1680 train_time:125907ms step_avg:87.99ms
step:1432/1680 train_time:125995ms step_avg:87.99ms
step:1433/1680 train_time:126085ms step_avg:87.99ms
step:1434/1680 train_time:126174ms step_avg:87.99ms
step:1435/1680 train_time:126264ms step_avg:87.99ms
step:1436/1680 train_time:126354ms step_avg:87.99ms
step:1437/1680 train_time:126444ms step_avg:87.99ms
step:1438/1680 train_time:126534ms step_avg:87.99ms
step:1439/1680 train_time:126623ms step_avg:87.99ms
step:1440/1680 train_time:126712ms step_avg:87.99ms
step:1441/1680 train_time:126800ms step_avg:87.99ms
step:1442/1680 train_time:126889ms step_avg:88.00ms
step:1443/1680 train_time:126977ms step_avg:88.00ms
step:1444/1680 train_time:127067ms step_avg:88.00ms
step:1445/1680 train_time:127156ms step_avg:88.00ms
step:1446/1680 train_time:127246ms step_avg:88.00ms
step:1447/1680 train_time:127335ms step_avg:88.00ms
step:1448/1680 train_time:127424ms step_avg:88.00ms
step:1449/1680 train_time:127514ms step_avg:88.00ms
step:1450/1680 train_time:127603ms step_avg:88.00ms
step:1451/1680 train_time:127692ms step_avg:88.00ms
step:1452/1680 train_time:127780ms step_avg:88.00ms
step:1453/1680 train_time:127869ms step_avg:88.00ms
step:1454/1680 train_time:127958ms step_avg:88.00ms
step:1455/1680 train_time:128046ms step_avg:88.00ms
step:1456/1680 train_time:128136ms step_avg:88.01ms
step:1457/1680 train_time:128225ms step_avg:88.01ms
step:1458/1680 train_time:128314ms step_avg:88.01ms
step:1459/1680 train_time:128404ms step_avg:88.01ms
step:1460/1680 train_time:128493ms step_avg:88.01ms
step:1461/1680 train_time:128582ms step_avg:88.01ms
step:1462/1680 train_time:128671ms step_avg:88.01ms
step:1463/1680 train_time:128759ms step_avg:88.01ms
step:1464/1680 train_time:128848ms step_avg:88.01ms
step:1465/1680 train_time:128936ms step_avg:88.01ms
step:1466/1680 train_time:129026ms step_avg:88.01ms
step:1467/1680 train_time:129114ms step_avg:88.01ms
step:1468/1680 train_time:129203ms step_avg:88.01ms
step:1469/1680 train_time:129292ms step_avg:88.01ms
step:1470/1680 train_time:129381ms step_avg:88.01ms
step:1471/1680 train_time:129470ms step_avg:88.01ms
step:1472/1680 train_time:129559ms step_avg:88.02ms
step:1473/1680 train_time:129648ms step_avg:88.02ms
step:1474/1680 train_time:129737ms step_avg:88.02ms
step:1475/1680 train_time:129826ms step_avg:88.02ms
step:1476/1680 train_time:129915ms step_avg:88.02ms
step:1477/1680 train_time:130003ms step_avg:88.02ms
step:1478/1680 train_time:130093ms step_avg:88.02ms
step:1479/1680 train_time:130182ms step_avg:88.02ms
step:1480/1680 train_time:130272ms step_avg:88.02ms
step:1481/1680 train_time:130361ms step_avg:88.02ms
step:1482/1680 train_time:130451ms step_avg:88.02ms
step:1483/1680 train_time:130540ms step_avg:88.02ms
step:1484/1680 train_time:130629ms step_avg:88.02ms
step:1485/1680 train_time:130718ms step_avg:88.03ms
step:1486/1680 train_time:130807ms step_avg:88.03ms
step:1487/1680 train_time:130896ms step_avg:88.03ms
step:1488/1680 train_time:130984ms step_avg:88.03ms
step:1489/1680 train_time:131073ms step_avg:88.03ms
step:1490/1680 train_time:131162ms step_avg:88.03ms
step:1491/1680 train_time:131252ms step_avg:88.03ms
step:1492/1680 train_time:131341ms step_avg:88.03ms
step:1493/1680 train_time:131430ms step_avg:88.03ms
step:1494/1680 train_time:131519ms step_avg:88.03ms
step:1495/1680 train_time:131608ms step_avg:88.03ms
step:1496/1680 train_time:131697ms step_avg:88.03ms
step:1497/1680 train_time:131786ms step_avg:88.03ms
step:1498/1680 train_time:131875ms step_avg:88.03ms
step:1499/1680 train_time:131963ms step_avg:88.03ms
step:1500/1680 train_time:132052ms step_avg:88.03ms
step:1500/1680 val_loss:3.3147 train_time:132142ms step_avg:88.09ms
step:1501/1680 train_time:132162ms step_avg:88.05ms
step:1502/1680 train_time:132235ms step_avg:88.04ms
step:1503/1680 train_time:132326ms step_avg:88.04ms
step:1504/1680 train_time:132417ms step_avg:88.04ms
step:1505/1680 train_time:132505ms step_avg:88.04ms
step:1506/1680 train_time:132593ms step_avg:88.04ms
step:1507/1680 train_time:132681ms step_avg:88.04ms
step:1508/1680 train_time:132769ms step_avg:88.04ms
step:1509/1680 train_time:132856ms step_avg:88.04ms
step:1510/1680 train_time:132944ms step_avg:88.04ms
step:1511/1680 train_time:133033ms step_avg:88.04ms
step:1512/1680 train_time:133123ms step_avg:88.04ms
step:1513/1680 train_time:133214ms step_avg:88.05ms
step:1514/1680 train_time:133305ms step_avg:88.05ms
step:1515/1680 train_time:133395ms step_avg:88.05ms
step:1516/1680 train_time:133485ms step_avg:88.05ms
step:1517/1680 train_time:133574ms step_avg:88.05ms
step:1518/1680 train_time:133662ms step_avg:88.05ms
step:1519/1680 train_time:133750ms step_avg:88.05ms
step:1520/1680 train_time:133838ms step_avg:88.05ms
step:1521/1680 train_time:133927ms step_avg:88.05ms
step:1522/1680 train_time:134015ms step_avg:88.05ms
step:1523/1680 train_time:134105ms step_avg:88.05ms
step:1524/1680 train_time:134194ms step_avg:88.05ms
step:1525/1680 train_time:134284ms step_avg:88.06ms
step:1526/1680 train_time:134375ms step_avg:88.06ms
step:1527/1680 train_time:134464ms step_avg:88.06ms
step:1528/1680 train_time:134553ms step_avg:88.06ms
step:1529/1680 train_time:134641ms step_avg:88.06ms
step:1530/1680 train_time:134730ms step_avg:88.06ms
step:1531/1680 train_time:134819ms step_avg:88.06ms
step:1532/1680 train_time:134908ms step_avg:88.06ms
step:1533/1680 train_time:134996ms step_avg:88.06ms
step:1534/1680 train_time:135084ms step_avg:88.06ms
step:1535/1680 train_time:135174ms step_avg:88.06ms
step:1536/1680 train_time:135263ms step_avg:88.06ms
step:1537/1680 train_time:135353ms step_avg:88.06ms
step:1538/1680 train_time:135443ms step_avg:88.06ms
step:1539/1680 train_time:135532ms step_avg:88.06ms
step:1540/1680 train_time:135621ms step_avg:88.07ms
step:1541/1680 train_time:135710ms step_avg:88.07ms
step:1542/1680 train_time:135798ms step_avg:88.07ms
step:1543/1680 train_time:135887ms step_avg:88.07ms
step:1544/1680 train_time:135976ms step_avg:88.07ms
step:1545/1680 train_time:136064ms step_avg:88.07ms
step:1546/1680 train_time:136153ms step_avg:88.07ms
step:1547/1680 train_time:136243ms step_avg:88.07ms
step:1548/1680 train_time:136332ms step_avg:88.07ms
step:1549/1680 train_time:136421ms step_avg:88.07ms
step:1550/1680 train_time:136511ms step_avg:88.07ms
step:1551/1680 train_time:136599ms step_avg:88.07ms
step:1552/1680 train_time:136688ms step_avg:88.07ms
step:1553/1680 train_time:136777ms step_avg:88.07ms
step:1554/1680 train_time:136865ms step_avg:88.07ms
step:1555/1680 train_time:136954ms step_avg:88.07ms
step:1556/1680 train_time:137043ms step_avg:88.07ms
step:1557/1680 train_time:137133ms step_avg:88.07ms
step:1558/1680 train_time:137222ms step_avg:88.08ms
step:1559/1680 train_time:137311ms step_avg:88.08ms
step:1560/1680 train_time:137401ms step_avg:88.08ms
step:1561/1680 train_time:137493ms step_avg:88.08ms
step:1562/1680 train_time:137582ms step_avg:88.08ms
step:1563/1680 train_time:137670ms step_avg:88.08ms
step:1564/1680 train_time:137759ms step_avg:88.08ms
step:1565/1680 train_time:137848ms step_avg:88.08ms
step:1566/1680 train_time:137936ms step_avg:88.08ms
step:1567/1680 train_time:138025ms step_avg:88.08ms
step:1568/1680 train_time:138114ms step_avg:88.08ms
step:1569/1680 train_time:138203ms step_avg:88.08ms
step:1570/1680 train_time:138292ms step_avg:88.08ms
step:1571/1680 train_time:138381ms step_avg:88.08ms
step:1572/1680 train_time:138470ms step_avg:88.09ms
step:1573/1680 train_time:138559ms step_avg:88.09ms
step:1574/1680 train_time:138648ms step_avg:88.09ms
step:1575/1680 train_time:138737ms step_avg:88.09ms
step:1576/1680 train_time:138826ms step_avg:88.09ms
step:1577/1680 train_time:138914ms step_avg:88.09ms
step:1578/1680 train_time:139003ms step_avg:88.09ms
step:1579/1680 train_time:139092ms step_avg:88.09ms
step:1580/1680 train_time:139181ms step_avg:88.09ms
step:1581/1680 train_time:139270ms step_avg:88.09ms
step:1582/1680 train_time:139360ms step_avg:88.09ms
step:1583/1680 train_time:139450ms step_avg:88.09ms
step:1584/1680 train_time:139539ms step_avg:88.09ms
step:1585/1680 train_time:139627ms step_avg:88.09ms
step:1586/1680 train_time:139717ms step_avg:88.09ms
step:1587/1680 train_time:139805ms step_avg:88.09ms
step:1588/1680 train_time:139894ms step_avg:88.09ms
step:1589/1680 train_time:139984ms step_avg:88.10ms
step:1590/1680 train_time:140073ms step_avg:88.10ms
step:1591/1680 train_time:140162ms step_avg:88.10ms
step:1592/1680 train_time:140250ms step_avg:88.10ms
step:1593/1680 train_time:140339ms step_avg:88.10ms
step:1594/1680 train_time:140429ms step_avg:88.10ms
step:1595/1680 train_time:140518ms step_avg:88.10ms
step:1596/1680 train_time:140607ms step_avg:88.10ms
step:1597/1680 train_time:140695ms step_avg:88.10ms
step:1598/1680 train_time:140784ms step_avg:88.10ms
step:1599/1680 train_time:140873ms step_avg:88.10ms
step:1600/1680 train_time:140961ms step_avg:88.10ms
step:1601/1680 train_time:141051ms step_avg:88.10ms
step:1602/1680 train_time:141140ms step_avg:88.10ms
step:1603/1680 train_time:141228ms step_avg:88.10ms
step:1604/1680 train_time:141318ms step_avg:88.10ms
step:1605/1680 train_time:141407ms step_avg:88.10ms
step:1606/1680 train_time:141496ms step_avg:88.10ms
step:1607/1680 train_time:141585ms step_avg:88.11ms
step:1608/1680 train_time:141674ms step_avg:88.11ms
step:1609/1680 train_time:141763ms step_avg:88.11ms
step:1610/1680 train_time:141852ms step_avg:88.11ms
step:1611/1680 train_time:141941ms step_avg:88.11ms
step:1612/1680 train_time:142030ms step_avg:88.11ms
step:1613/1680 train_time:142120ms step_avg:88.11ms
step:1614/1680 train_time:142209ms step_avg:88.11ms
step:1615/1680 train_time:142297ms step_avg:88.11ms
step:1616/1680 train_time:142386ms step_avg:88.11ms
step:1617/1680 train_time:142475ms step_avg:88.11ms
step:1618/1680 train_time:142564ms step_avg:88.11ms
step:1619/1680 train_time:142653ms step_avg:88.11ms
step:1620/1680 train_time:142742ms step_avg:88.11ms
step:1621/1680 train_time:142832ms step_avg:88.11ms
step:1622/1680 train_time:142921ms step_avg:88.11ms
step:1623/1680 train_time:143010ms step_avg:88.11ms
step:1624/1680 train_time:143100ms step_avg:88.12ms
step:1625/1680 train_time:143189ms step_avg:88.12ms
step:1625/1680 val_loss:3.2910 train_time:143279ms step_avg:88.17ms
step:1626/1680 train_time:143299ms step_avg:88.13ms
step:1627/1680 train_time:143370ms step_avg:88.12ms
step:1628/1680 train_time:143464ms step_avg:88.12ms
step:1629/1680 train_time:143555ms step_avg:88.12ms
step:1630/1680 train_time:143643ms step_avg:88.12ms
step:1631/1680 train_time:143732ms step_avg:88.13ms
step:1632/1680 train_time:143820ms step_avg:88.13ms
step:1633/1680 train_time:143908ms step_avg:88.12ms
step:1634/1680 train_time:143996ms step_avg:88.12ms
step:1635/1680 train_time:144084ms step_avg:88.12ms
step:1636/1680 train_time:144171ms step_avg:88.12ms
step:1637/1680 train_time:144262ms step_avg:88.13ms
step:1638/1680 train_time:144353ms step_avg:88.13ms
step:1639/1680 train_time:144444ms step_avg:88.13ms
step:1640/1680 train_time:144534ms step_avg:88.13ms
step:1641/1680 train_time:144624ms step_avg:88.13ms
step:1642/1680 train_time:144714ms step_avg:88.13ms
step:1643/1680 train_time:144803ms step_avg:88.13ms
step:1644/1680 train_time:144891ms step_avg:88.13ms
step:1645/1680 train_time:144980ms step_avg:88.13ms
step:1646/1680 train_time:145068ms step_avg:88.13ms
step:1647/1680 train_time:145156ms step_avg:88.13ms
step:1648/1680 train_time:145246ms step_avg:88.13ms
step:1649/1680 train_time:145336ms step_avg:88.14ms
step:1650/1680 train_time:145426ms step_avg:88.14ms
step:1651/1680 train_time:145515ms step_avg:88.14ms
step:1652/1680 train_time:145605ms step_avg:88.14ms
step:1653/1680 train_time:145694ms step_avg:88.14ms
step:1654/1680 train_time:145783ms step_avg:88.14ms
step:1655/1680 train_time:145871ms step_avg:88.14ms
step:1656/1680 train_time:145960ms step_avg:88.14ms
step:1657/1680 train_time:146048ms step_avg:88.14ms
step:1658/1680 train_time:146136ms step_avg:88.14ms
step:1659/1680 train_time:146225ms step_avg:88.14ms
step:1660/1680 train_time:146315ms step_avg:88.14ms
step:1661/1680 train_time:146404ms step_avg:88.14ms
step:1662/1680 train_time:146494ms step_avg:88.14ms
step:1663/1680 train_time:146583ms step_avg:88.14ms
step:1664/1680 train_time:146673ms step_avg:88.14ms
step:1665/1680 train_time:146762ms step_avg:88.15ms
step:1666/1680 train_time:146851ms step_avg:88.15ms
step:1667/1680 train_time:146939ms step_avg:88.15ms
step:1668/1680 train_time:147027ms step_avg:88.15ms
step:1669/1680 train_time:147116ms step_avg:88.15ms
step:1670/1680 train_time:147205ms step_avg:88.15ms
step:1671/1680 train_time:147294ms step_avg:88.15ms
step:1672/1680 train_time:147383ms step_avg:88.15ms
step:1673/1680 train_time:147473ms step_avg:88.15ms
step:1674/1680 train_time:147563ms step_avg:88.15ms
step:1675/1680 train_time:147652ms step_avg:88.15ms
step:1676/1680 train_time:147743ms step_avg:88.15ms
step:1677/1680 train_time:147832ms step_avg:88.15ms
step:1678/1680 train_time:147922ms step_avg:88.15ms
step:1679/1680 train_time:148011ms step_avg:88.15ms
step:1680/1680 train_time:148100ms step_avg:88.15ms
step:1680/1680 val_loss:3.2799 train_time:148190ms step_avg:88.21ms
peak memory allocated: 30760 MiB reserved: 45934 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:25:28 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            123W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            117W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    156805      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    156806      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    156807      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    156808      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    156809      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    156810      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    156811      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    156812      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    156806      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    156807      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    156808      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    156809      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    156810      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    156811      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    156812      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:151ms step_avg:151.29ms
step:2/1680 train_time:172ms step_avg:85.76ms
step:3/1680 train_time:235ms step_avg:78.29ms
step:4/1680 train_time:320ms step_avg:80.05ms
step:5/1680 train_time:406ms step_avg:81.23ms
step:6/1680 train_time:493ms step_avg:82.17ms
step:7/1680 train_time:580ms step_avg:82.80ms
step:8/1680 train_time:666ms step_avg:83.28ms
step:9/1680 train_time:752ms step_avg:83.61ms
step:10/1680 train_time:839ms step_avg:83.88ms
step:11/1680 train_time:926ms step_avg:84.19ms
step:12/1680 train_time:1012ms step_avg:84.34ms
step:13/1680 train_time:1101ms step_avg:84.70ms
step:14/1680 train_time:1193ms step_avg:85.22ms
step:15/1680 train_time:1282ms step_avg:85.49ms
step:16/1680 train_time:1370ms step_avg:85.62ms
step:17/1680 train_time:1457ms step_avg:85.71ms
step:18/1680 train_time:1545ms step_avg:85.81ms
step:19/1680 train_time:1632ms step_avg:85.87ms
step:20/1680 train_time:1718ms step_avg:85.90ms
step:21/1680 train_time:1804ms step_avg:85.91ms
step:22/1680 train_time:1891ms step_avg:85.95ms
step:23/1680 train_time:1978ms step_avg:86.00ms
step:24/1680 train_time:2065ms step_avg:86.05ms
step:25/1680 train_time:2154ms step_avg:86.16ms
step:26/1680 train_time:2243ms step_avg:86.26ms
step:27/1680 train_time:2331ms step_avg:86.32ms
step:28/1680 train_time:2418ms step_avg:86.35ms
step:29/1680 train_time:2505ms step_avg:86.38ms
step:30/1680 train_time:2592ms step_avg:86.41ms
step:31/1680 train_time:2680ms step_avg:86.44ms
step:32/1680 train_time:2767ms step_avg:86.45ms
step:33/1680 train_time:2853ms step_avg:86.46ms
step:34/1680 train_time:2940ms step_avg:86.47ms
step:35/1680 train_time:3027ms step_avg:86.50ms
step:36/1680 train_time:3115ms step_avg:86.53ms
step:37/1680 train_time:3203ms step_avg:86.57ms
step:38/1680 train_time:3292ms step_avg:86.62ms
step:39/1680 train_time:3379ms step_avg:86.65ms
step:40/1680 train_time:3467ms step_avg:86.68ms
step:41/1680 train_time:3555ms step_avg:86.70ms
step:42/1680 train_time:3643ms step_avg:86.73ms
step:43/1680 train_time:3729ms step_avg:86.73ms
step:44/1680 train_time:3817ms step_avg:86.74ms
step:45/1680 train_time:3904ms step_avg:86.75ms
step:46/1680 train_time:3991ms step_avg:86.76ms
step:47/1680 train_time:4078ms step_avg:86.76ms
step:48/1680 train_time:4166ms step_avg:86.79ms
step:49/1680 train_time:4254ms step_avg:86.82ms
step:50/1680 train_time:4342ms step_avg:86.85ms
step:51/1680 train_time:4430ms step_avg:86.87ms
step:52/1680 train_time:4517ms step_avg:86.87ms
step:53/1680 train_time:4604ms step_avg:86.88ms
step:54/1680 train_time:4691ms step_avg:86.88ms
step:55/1680 train_time:4778ms step_avg:86.88ms
step:56/1680 train_time:4866ms step_avg:86.88ms
step:57/1680 train_time:4953ms step_avg:86.90ms
step:58/1680 train_time:5040ms step_avg:86.90ms
step:59/1680 train_time:5127ms step_avg:86.90ms
step:60/1680 train_time:5214ms step_avg:86.90ms
step:61/1680 train_time:5302ms step_avg:86.93ms
step:62/1680 train_time:5391ms step_avg:86.95ms
step:63/1680 train_time:5478ms step_avg:86.95ms
step:64/1680 train_time:5566ms step_avg:86.96ms
step:65/1680 train_time:5653ms step_avg:86.97ms
step:66/1680 train_time:5741ms step_avg:86.99ms
step:67/1680 train_time:5828ms step_avg:86.99ms
step:68/1680 train_time:5915ms step_avg:86.98ms
step:69/1680 train_time:6002ms step_avg:86.99ms
step:70/1680 train_time:6090ms step_avg:87.00ms
step:71/1680 train_time:6177ms step_avg:87.00ms
step:72/1680 train_time:6264ms step_avg:87.00ms
step:73/1680 train_time:6351ms step_avg:87.01ms
step:74/1680 train_time:6439ms step_avg:87.01ms
step:75/1680 train_time:6527ms step_avg:87.02ms
step:76/1680 train_time:6614ms step_avg:87.02ms
step:77/1680 train_time:6701ms step_avg:87.02ms
step:78/1680 train_time:6788ms step_avg:87.03ms
step:79/1680 train_time:6875ms step_avg:87.03ms
step:80/1680 train_time:6962ms step_avg:87.03ms
step:81/1680 train_time:7050ms step_avg:87.04ms
step:82/1680 train_time:7137ms step_avg:87.04ms
step:83/1680 train_time:7225ms step_avg:87.05ms
step:84/1680 train_time:7312ms step_avg:87.05ms
step:85/1680 train_time:7399ms step_avg:87.05ms
step:86/1680 train_time:7487ms step_avg:87.06ms
step:87/1680 train_time:7574ms step_avg:87.06ms
step:88/1680 train_time:7661ms step_avg:87.06ms
step:89/1680 train_time:7749ms step_avg:87.06ms
step:90/1680 train_time:7836ms step_avg:87.06ms
step:91/1680 train_time:7923ms step_avg:87.07ms
step:92/1680 train_time:8011ms step_avg:87.07ms
step:93/1680 train_time:8098ms step_avg:87.07ms
step:94/1680 train_time:8186ms step_avg:87.09ms
step:95/1680 train_time:8273ms step_avg:87.09ms
step:96/1680 train_time:8361ms step_avg:87.09ms
step:97/1680 train_time:8448ms step_avg:87.09ms
step:98/1680 train_time:8535ms step_avg:87.09ms
step:99/1680 train_time:8622ms step_avg:87.09ms
step:100/1680 train_time:8709ms step_avg:87.09ms
step:101/1680 train_time:8797ms step_avg:87.10ms
step:102/1680 train_time:8885ms step_avg:87.11ms
step:103/1680 train_time:8972ms step_avg:87.11ms
step:104/1680 train_time:9059ms step_avg:87.11ms
step:105/1680 train_time:9147ms step_avg:87.11ms
step:106/1680 train_time:9234ms step_avg:87.11ms
step:107/1680 train_time:9322ms step_avg:87.12ms
step:108/1680 train_time:9409ms step_avg:87.12ms
step:109/1680 train_time:9496ms step_avg:87.12ms
step:110/1680 train_time:9584ms step_avg:87.12ms
step:111/1680 train_time:9671ms step_avg:87.13ms
step:112/1680 train_time:9758ms step_avg:87.13ms
step:113/1680 train_time:9846ms step_avg:87.13ms
step:114/1680 train_time:9933ms step_avg:87.13ms
step:115/1680 train_time:10021ms step_avg:87.14ms
step:116/1680 train_time:10109ms step_avg:87.15ms
step:117/1680 train_time:10196ms step_avg:87.14ms
step:118/1680 train_time:10284ms step_avg:87.15ms
step:119/1680 train_time:10371ms step_avg:87.15ms
step:120/1680 train_time:10460ms step_avg:87.16ms
step:121/1680 train_time:10546ms step_avg:87.16ms
step:122/1680 train_time:10633ms step_avg:87.16ms
step:123/1680 train_time:10721ms step_avg:87.16ms
step:124/1680 train_time:10808ms step_avg:87.16ms
step:125/1680 train_time:10895ms step_avg:87.16ms
step:125/1680 val_loss:4.3079 train_time:10983ms step_avg:87.86ms
step:126/1680 train_time:11013ms step_avg:87.40ms
step:127/1680 train_time:11072ms step_avg:87.18ms
step:128/1680 train_time:11167ms step_avg:87.25ms
step:129/1680 train_time:11256ms step_avg:87.26ms
step:130/1680 train_time:11344ms step_avg:87.26ms
step:131/1680 train_time:11430ms step_avg:87.25ms
step:132/1680 train_time:11516ms step_avg:87.24ms
step:133/1680 train_time:11602ms step_avg:87.24ms
step:134/1680 train_time:11688ms step_avg:87.23ms
step:135/1680 train_time:11774ms step_avg:87.22ms
step:136/1680 train_time:11860ms step_avg:87.21ms
step:137/1680 train_time:11947ms step_avg:87.20ms
step:138/1680 train_time:12034ms step_avg:87.20ms
step:139/1680 train_time:12124ms step_avg:87.22ms
step:140/1680 train_time:12214ms step_avg:87.24ms
step:141/1680 train_time:12303ms step_avg:87.25ms
step:142/1680 train_time:12390ms step_avg:87.26ms
step:143/1680 train_time:12477ms step_avg:87.25ms
step:144/1680 train_time:12564ms step_avg:87.25ms
step:145/1680 train_time:12650ms step_avg:87.24ms
step:146/1680 train_time:12736ms step_avg:87.24ms
step:147/1680 train_time:12822ms step_avg:87.23ms
step:148/1680 train_time:12909ms step_avg:87.22ms
step:149/1680 train_time:12996ms step_avg:87.22ms
step:150/1680 train_time:13085ms step_avg:87.23ms
step:151/1680 train_time:13173ms step_avg:87.24ms
step:152/1680 train_time:13262ms step_avg:87.25ms
step:153/1680 train_time:13350ms step_avg:87.26ms
step:154/1680 train_time:13437ms step_avg:87.25ms
step:155/1680 train_time:13524ms step_avg:87.25ms
step:156/1680 train_time:13610ms step_avg:87.25ms
step:157/1680 train_time:13697ms step_avg:87.24ms
step:158/1680 train_time:13784ms step_avg:87.24ms
step:159/1680 train_time:13871ms step_avg:87.24ms
step:160/1680 train_time:13957ms step_avg:87.23ms
step:161/1680 train_time:14045ms step_avg:87.24ms
step:162/1680 train_time:14133ms step_avg:87.24ms
step:163/1680 train_time:14221ms step_avg:87.24ms
step:164/1680 train_time:14309ms step_avg:87.25ms
step:165/1680 train_time:14396ms step_avg:87.25ms
step:166/1680 train_time:14484ms step_avg:87.25ms
step:167/1680 train_time:14571ms step_avg:87.25ms
step:168/1680 train_time:14658ms step_avg:87.25ms
step:169/1680 train_time:14745ms step_avg:87.25ms
step:170/1680 train_time:14831ms step_avg:87.24ms
step:171/1680 train_time:14918ms step_avg:87.24ms
step:172/1680 train_time:15005ms step_avg:87.24ms
step:173/1680 train_time:15092ms step_avg:87.24ms
step:174/1680 train_time:15180ms step_avg:87.24ms
step:175/1680 train_time:15269ms step_avg:87.25ms
step:176/1680 train_time:15356ms step_avg:87.25ms
step:177/1680 train_time:15444ms step_avg:87.25ms
step:178/1680 train_time:15531ms step_avg:87.25ms
step:179/1680 train_time:15618ms step_avg:87.25ms
step:180/1680 train_time:15705ms step_avg:87.25ms
step:181/1680 train_time:15791ms step_avg:87.24ms
step:182/1680 train_time:15879ms step_avg:87.25ms
step:183/1680 train_time:15965ms step_avg:87.24ms
step:184/1680 train_time:16051ms step_avg:87.23ms
step:185/1680 train_time:16139ms step_avg:87.24ms
step:186/1680 train_time:16226ms step_avg:87.24ms
step:187/1680 train_time:16314ms step_avg:87.24ms
step:188/1680 train_time:16401ms step_avg:87.24ms
step:189/1680 train_time:16489ms step_avg:87.25ms
step:190/1680 train_time:16576ms step_avg:87.24ms
step:191/1680 train_time:16663ms step_avg:87.24ms
step:192/1680 train_time:16751ms step_avg:87.24ms
step:193/1680 train_time:16838ms step_avg:87.24ms
step:194/1680 train_time:16925ms step_avg:87.24ms
step:195/1680 train_time:17012ms step_avg:87.24ms
step:196/1680 train_time:17099ms step_avg:87.24ms
step:197/1680 train_time:17187ms step_avg:87.24ms
step:198/1680 train_time:17274ms step_avg:87.24ms
step:199/1680 train_time:17362ms step_avg:87.25ms
step:200/1680 train_time:17449ms step_avg:87.25ms
step:201/1680 train_time:17537ms step_avg:87.25ms
step:202/1680 train_time:17624ms step_avg:87.25ms
step:203/1680 train_time:17710ms step_avg:87.24ms
step:204/1680 train_time:17797ms step_avg:87.24ms
step:205/1680 train_time:17884ms step_avg:87.24ms
step:206/1680 train_time:17972ms step_avg:87.24ms
step:207/1680 train_time:18058ms step_avg:87.24ms
step:208/1680 train_time:18146ms step_avg:87.24ms
step:209/1680 train_time:18233ms step_avg:87.24ms
step:210/1680 train_time:18321ms step_avg:87.24ms
step:211/1680 train_time:18408ms step_avg:87.24ms
step:212/1680 train_time:18496ms step_avg:87.24ms
step:213/1680 train_time:18583ms step_avg:87.24ms
step:214/1680 train_time:18670ms step_avg:87.24ms
step:215/1680 train_time:18757ms step_avg:87.24ms
step:216/1680 train_time:18844ms step_avg:87.24ms
step:217/1680 train_time:18931ms step_avg:87.24ms
step:218/1680 train_time:19017ms step_avg:87.24ms
step:219/1680 train_time:19105ms step_avg:87.24ms
step:220/1680 train_time:19192ms step_avg:87.24ms
step:221/1680 train_time:19279ms step_avg:87.24ms
step:222/1680 train_time:19367ms step_avg:87.24ms
step:223/1680 train_time:19454ms step_avg:87.24ms
step:224/1680 train_time:19541ms step_avg:87.24ms
step:225/1680 train_time:19629ms step_avg:87.24ms
step:226/1680 train_time:19716ms step_avg:87.24ms
step:227/1680 train_time:19803ms step_avg:87.24ms
step:228/1680 train_time:19891ms step_avg:87.24ms
step:229/1680 train_time:19978ms step_avg:87.24ms
step:230/1680 train_time:20065ms step_avg:87.24ms
step:231/1680 train_time:20152ms step_avg:87.24ms
step:232/1680 train_time:20239ms step_avg:87.24ms
step:233/1680 train_time:20326ms step_avg:87.24ms
step:234/1680 train_time:20413ms step_avg:87.23ms
step:235/1680 train_time:20500ms step_avg:87.23ms
step:236/1680 train_time:20588ms step_avg:87.24ms
step:237/1680 train_time:20675ms step_avg:87.24ms
step:238/1680 train_time:20763ms step_avg:87.24ms
step:239/1680 train_time:20850ms step_avg:87.24ms
step:240/1680 train_time:20937ms step_avg:87.24ms
step:241/1680 train_time:21024ms step_avg:87.24ms
step:242/1680 train_time:21111ms step_avg:87.23ms
step:243/1680 train_time:21197ms step_avg:87.23ms
step:244/1680 train_time:21284ms step_avg:87.23ms
step:245/1680 train_time:21371ms step_avg:87.23ms
step:246/1680 train_time:21458ms step_avg:87.23ms
step:247/1680 train_time:21546ms step_avg:87.23ms
step:248/1680 train_time:21633ms step_avg:87.23ms
step:249/1680 train_time:21720ms step_avg:87.23ms
step:250/1680 train_time:21808ms step_avg:87.23ms
step:250/1680 val_loss:3.9612 train_time:21896ms step_avg:87.58ms
step:251/1680 train_time:21921ms step_avg:87.34ms
step:252/1680 train_time:21990ms step_avg:87.26ms
step:253/1680 train_time:22082ms step_avg:87.28ms
step:254/1680 train_time:22169ms step_avg:87.28ms
step:255/1680 train_time:22255ms step_avg:87.28ms
step:256/1680 train_time:22341ms step_avg:87.27ms
step:257/1680 train_time:22428ms step_avg:87.27ms
step:258/1680 train_time:22514ms step_avg:87.26ms
step:259/1680 train_time:22601ms step_avg:87.26ms
step:260/1680 train_time:22687ms step_avg:87.26ms
step:261/1680 train_time:22774ms step_avg:87.25ms
step:262/1680 train_time:22861ms step_avg:87.26ms
step:263/1680 train_time:22950ms step_avg:87.26ms
step:264/1680 train_time:23039ms step_avg:87.27ms
step:265/1680 train_time:23127ms step_avg:87.27ms
step:266/1680 train_time:23215ms step_avg:87.28ms
step:267/1680 train_time:23302ms step_avg:87.27ms
step:268/1680 train_time:23388ms step_avg:87.27ms
step:269/1680 train_time:23475ms step_avg:87.27ms
step:270/1680 train_time:23562ms step_avg:87.26ms
step:271/1680 train_time:23648ms step_avg:87.26ms
step:272/1680 train_time:23735ms step_avg:87.26ms
step:273/1680 train_time:23821ms step_avg:87.26ms
step:274/1680 train_time:23909ms step_avg:87.26ms
step:275/1680 train_time:23997ms step_avg:87.26ms
step:276/1680 train_time:24086ms step_avg:87.27ms
step:277/1680 train_time:24173ms step_avg:87.27ms
step:278/1680 train_time:24261ms step_avg:87.27ms
step:279/1680 train_time:24348ms step_avg:87.27ms
step:280/1680 train_time:24435ms step_avg:87.27ms
step:281/1680 train_time:24522ms step_avg:87.27ms
step:282/1680 train_time:24609ms step_avg:87.27ms
step:283/1680 train_time:24696ms step_avg:87.26ms
step:284/1680 train_time:24782ms step_avg:87.26ms
step:285/1680 train_time:24869ms step_avg:87.26ms
step:286/1680 train_time:24957ms step_avg:87.26ms
step:287/1680 train_time:25045ms step_avg:87.26ms
step:288/1680 train_time:25132ms step_avg:87.26ms
step:289/1680 train_time:25220ms step_avg:87.27ms
step:290/1680 train_time:25307ms step_avg:87.27ms
step:291/1680 train_time:25395ms step_avg:87.27ms
step:292/1680 train_time:25481ms step_avg:87.26ms
step:293/1680 train_time:25568ms step_avg:87.26ms
step:294/1680 train_time:25655ms step_avg:87.26ms
step:295/1680 train_time:25742ms step_avg:87.26ms
step:296/1680 train_time:25828ms step_avg:87.26ms
step:297/1680 train_time:25916ms step_avg:87.26ms
step:298/1680 train_time:26003ms step_avg:87.26ms
step:299/1680 train_time:26090ms step_avg:87.26ms
step:300/1680 train_time:26179ms step_avg:87.26ms
step:301/1680 train_time:26266ms step_avg:87.26ms
step:302/1680 train_time:26353ms step_avg:87.26ms
step:303/1680 train_time:26441ms step_avg:87.26ms
step:304/1680 train_time:26528ms step_avg:87.26ms
step:305/1680 train_time:26615ms step_avg:87.26ms
step:306/1680 train_time:26703ms step_avg:87.26ms
step:307/1680 train_time:26789ms step_avg:87.26ms
step:308/1680 train_time:26876ms step_avg:87.26ms
step:309/1680 train_time:26964ms step_avg:87.26ms
step:310/1680 train_time:27052ms step_avg:87.26ms
step:311/1680 train_time:27139ms step_avg:87.26ms
step:312/1680 train_time:27226ms step_avg:87.26ms
step:313/1680 train_time:27314ms step_avg:87.27ms
step:314/1680 train_time:27402ms step_avg:87.27ms
step:315/1680 train_time:27489ms step_avg:87.27ms
step:316/1680 train_time:27577ms step_avg:87.27ms
step:317/1680 train_time:27664ms step_avg:87.27ms
step:318/1680 train_time:27751ms step_avg:87.27ms
step:319/1680 train_time:27837ms step_avg:87.26ms
step:320/1680 train_time:27925ms step_avg:87.27ms
step:321/1680 train_time:28012ms step_avg:87.27ms
step:322/1680 train_time:28100ms step_avg:87.27ms
step:323/1680 train_time:28187ms step_avg:87.27ms
step:324/1680 train_time:28274ms step_avg:87.27ms
step:325/1680 train_time:28361ms step_avg:87.27ms
step:326/1680 train_time:28449ms step_avg:87.27ms
step:327/1680 train_time:28537ms step_avg:87.27ms
step:328/1680 train_time:28624ms step_avg:87.27ms
step:329/1680 train_time:28711ms step_avg:87.27ms
step:330/1680 train_time:28798ms step_avg:87.27ms
step:331/1680 train_time:28886ms step_avg:87.27ms
step:332/1680 train_time:28972ms step_avg:87.27ms
step:333/1680 train_time:29059ms step_avg:87.27ms
step:334/1680 train_time:29147ms step_avg:87.27ms
step:335/1680 train_time:29234ms step_avg:87.26ms
step:336/1680 train_time:29321ms step_avg:87.27ms
step:337/1680 train_time:29408ms step_avg:87.27ms
step:338/1680 train_time:29496ms step_avg:87.27ms
step:339/1680 train_time:29583ms step_avg:87.27ms
step:340/1680 train_time:29670ms step_avg:87.26ms
step:341/1680 train_time:29757ms step_avg:87.26ms
step:342/1680 train_time:29844ms step_avg:87.26ms
step:343/1680 train_time:29931ms step_avg:87.26ms
step:344/1680 train_time:30019ms step_avg:87.26ms
step:345/1680 train_time:30106ms step_avg:87.26ms
step:346/1680 train_time:30193ms step_avg:87.26ms
step:347/1680 train_time:30281ms step_avg:87.26ms
step:348/1680 train_time:30367ms step_avg:87.26ms
step:349/1680 train_time:30455ms step_avg:87.26ms
step:350/1680 train_time:30543ms step_avg:87.26ms
step:351/1680 train_time:30630ms step_avg:87.26ms
step:352/1680 train_time:30717ms step_avg:87.26ms
step:353/1680 train_time:30804ms step_avg:87.26ms
step:354/1680 train_time:30891ms step_avg:87.26ms
step:355/1680 train_time:30979ms step_avg:87.26ms
step:356/1680 train_time:31066ms step_avg:87.26ms
step:357/1680 train_time:31153ms step_avg:87.26ms
step:358/1680 train_time:31240ms step_avg:87.26ms
step:359/1680 train_time:31327ms step_avg:87.26ms
step:360/1680 train_time:31414ms step_avg:87.26ms
step:361/1680 train_time:31503ms step_avg:87.27ms
step:362/1680 train_time:31590ms step_avg:87.26ms
step:363/1680 train_time:31678ms step_avg:87.27ms
step:364/1680 train_time:31765ms step_avg:87.27ms
step:365/1680 train_time:31852ms step_avg:87.26ms
step:366/1680 train_time:31939ms step_avg:87.26ms
step:367/1680 train_time:32026ms step_avg:87.26ms
step:368/1680 train_time:32113ms step_avg:87.26ms
step:369/1680 train_time:32201ms step_avg:87.27ms
step:370/1680 train_time:32288ms step_avg:87.26ms
step:371/1680 train_time:32375ms step_avg:87.26ms
step:372/1680 train_time:32463ms step_avg:87.27ms
step:373/1680 train_time:32550ms step_avg:87.27ms
step:374/1680 train_time:32637ms step_avg:87.27ms
step:375/1680 train_time:32725ms step_avg:87.27ms
step:375/1680 val_loss:3.8140 train_time:32813ms step_avg:87.50ms
step:376/1680 train_time:32837ms step_avg:87.33ms
step:377/1680 train_time:32903ms step_avg:87.28ms
step:378/1680 train_time:32993ms step_avg:87.28ms
step:379/1680 train_time:33083ms step_avg:87.29ms
step:380/1680 train_time:33170ms step_avg:87.29ms
step:381/1680 train_time:33256ms step_avg:87.29ms
step:382/1680 train_time:33342ms step_avg:87.28ms
step:383/1680 train_time:33428ms step_avg:87.28ms
step:384/1680 train_time:33514ms step_avg:87.28ms
step:385/1680 train_time:33601ms step_avg:87.28ms
step:386/1680 train_time:33688ms step_avg:87.27ms
step:387/1680 train_time:33775ms step_avg:87.27ms
step:388/1680 train_time:33863ms step_avg:87.28ms
step:389/1680 train_time:33951ms step_avg:87.28ms
step:390/1680 train_time:34040ms step_avg:87.28ms
step:391/1680 train_time:34128ms step_avg:87.28ms
step:392/1680 train_time:34216ms step_avg:87.28ms
step:393/1680 train_time:34302ms step_avg:87.28ms
step:394/1680 train_time:34389ms step_avg:87.28ms
step:395/1680 train_time:34475ms step_avg:87.28ms
step:396/1680 train_time:34561ms step_avg:87.28ms
step:397/1680 train_time:34648ms step_avg:87.27ms
step:398/1680 train_time:34734ms step_avg:87.27ms
step:399/1680 train_time:34821ms step_avg:87.27ms
step:400/1680 train_time:34909ms step_avg:87.27ms
step:401/1680 train_time:34997ms step_avg:87.27ms
step:402/1680 train_time:35085ms step_avg:87.28ms
step:403/1680 train_time:35173ms step_avg:87.28ms
step:404/1680 train_time:35260ms step_avg:87.28ms
step:405/1680 train_time:35347ms step_avg:87.28ms
step:406/1680 train_time:35434ms step_avg:87.27ms
step:407/1680 train_time:35520ms step_avg:87.27ms
step:408/1680 train_time:35607ms step_avg:87.27ms
step:409/1680 train_time:35693ms step_avg:87.27ms
step:410/1680 train_time:35780ms step_avg:87.27ms
step:411/1680 train_time:35868ms step_avg:87.27ms
step:412/1680 train_time:35956ms step_avg:87.27ms
step:413/1680 train_time:36043ms step_avg:87.27ms
step:414/1680 train_time:36131ms step_avg:87.27ms
step:415/1680 train_time:36219ms step_avg:87.27ms
step:416/1680 train_time:36306ms step_avg:87.27ms
step:417/1680 train_time:36392ms step_avg:87.27ms
step:418/1680 train_time:36480ms step_avg:87.27ms
step:419/1680 train_time:36567ms step_avg:87.27ms
step:420/1680 train_time:36654ms step_avg:87.27ms
step:421/1680 train_time:36741ms step_avg:87.27ms
step:422/1680 train_time:36828ms step_avg:87.27ms
step:423/1680 train_time:36915ms step_avg:87.27ms
step:424/1680 train_time:37002ms step_avg:87.27ms
step:425/1680 train_time:37089ms step_avg:87.27ms
step:426/1680 train_time:37176ms step_avg:87.27ms
step:427/1680 train_time:37264ms step_avg:87.27ms
step:428/1680 train_time:37351ms step_avg:87.27ms
step:429/1680 train_time:37439ms step_avg:87.27ms
step:430/1680 train_time:37526ms step_avg:87.27ms
step:431/1680 train_time:37614ms step_avg:87.27ms
step:432/1680 train_time:37701ms step_avg:87.27ms
step:433/1680 train_time:37788ms step_avg:87.27ms
step:434/1680 train_time:37875ms step_avg:87.27ms
step:435/1680 train_time:37962ms step_avg:87.27ms
step:436/1680 train_time:38049ms step_avg:87.27ms
step:437/1680 train_time:38137ms step_avg:87.27ms
step:438/1680 train_time:38224ms step_avg:87.27ms
step:439/1680 train_time:38312ms step_avg:87.27ms
step:440/1680 train_time:38399ms step_avg:87.27ms
step:441/1680 train_time:38486ms step_avg:87.27ms
step:442/1680 train_time:38573ms step_avg:87.27ms
step:443/1680 train_time:38661ms step_avg:87.27ms
step:444/1680 train_time:38748ms step_avg:87.27ms
step:445/1680 train_time:38837ms step_avg:87.27ms
step:446/1680 train_time:38923ms step_avg:87.27ms
step:447/1680 train_time:39010ms step_avg:87.27ms
step:448/1680 train_time:39098ms step_avg:87.27ms
step:449/1680 train_time:39186ms step_avg:87.27ms
step:450/1680 train_time:39272ms step_avg:87.27ms
step:451/1680 train_time:39360ms step_avg:87.27ms
step:452/1680 train_time:39446ms step_avg:87.27ms
step:453/1680 train_time:39534ms step_avg:87.27ms
step:454/1680 train_time:39622ms step_avg:87.27ms
step:455/1680 train_time:39709ms step_avg:87.27ms
step:456/1680 train_time:39796ms step_avg:87.27ms
step:457/1680 train_time:39883ms step_avg:87.27ms
step:458/1680 train_time:39970ms step_avg:87.27ms
step:459/1680 train_time:40057ms step_avg:87.27ms
step:460/1680 train_time:40144ms step_avg:87.27ms
step:461/1680 train_time:40232ms step_avg:87.27ms
step:462/1680 train_time:40319ms step_avg:87.27ms
step:463/1680 train_time:40407ms step_avg:87.27ms
step:464/1680 train_time:40495ms step_avg:87.27ms
step:465/1680 train_time:40582ms step_avg:87.27ms
step:466/1680 train_time:40669ms step_avg:87.27ms
step:467/1680 train_time:40756ms step_avg:87.27ms
step:468/1680 train_time:40843ms step_avg:87.27ms
step:469/1680 train_time:40930ms step_avg:87.27ms
step:470/1680 train_time:41018ms step_avg:87.27ms
step:471/1680 train_time:41105ms step_avg:87.27ms
step:472/1680 train_time:41192ms step_avg:87.27ms
step:473/1680 train_time:41280ms step_avg:87.27ms
step:474/1680 train_time:41367ms step_avg:87.27ms
step:475/1680 train_time:41454ms step_avg:87.27ms
step:476/1680 train_time:41541ms step_avg:87.27ms
step:477/1680 train_time:41628ms step_avg:87.27ms
step:478/1680 train_time:41716ms step_avg:87.27ms
step:479/1680 train_time:41803ms step_avg:87.27ms
step:480/1680 train_time:41891ms step_avg:87.27ms
step:481/1680 train_time:41978ms step_avg:87.27ms
step:482/1680 train_time:42065ms step_avg:87.27ms
step:483/1680 train_time:42152ms step_avg:87.27ms
step:484/1680 train_time:42240ms step_avg:87.27ms
step:485/1680 train_time:42327ms step_avg:87.27ms
step:486/1680 train_time:42414ms step_avg:87.27ms
step:487/1680 train_time:42502ms step_avg:87.27ms
step:488/1680 train_time:42589ms step_avg:87.27ms
step:489/1680 train_time:42676ms step_avg:87.27ms
step:490/1680 train_time:42764ms step_avg:87.27ms
step:491/1680 train_time:42851ms step_avg:87.27ms
step:492/1680 train_time:42938ms step_avg:87.27ms
step:493/1680 train_time:43026ms step_avg:87.27ms
step:494/1680 train_time:43113ms step_avg:87.27ms
step:495/1680 train_time:43200ms step_avg:87.27ms
step:496/1680 train_time:43287ms step_avg:87.27ms
step:497/1680 train_time:43375ms step_avg:87.27ms
step:498/1680 train_time:43462ms step_avg:87.27ms
step:499/1680 train_time:43549ms step_avg:87.27ms
step:500/1680 train_time:43636ms step_avg:87.27ms
step:500/1680 val_loss:3.7139 train_time:43726ms step_avg:87.45ms
step:501/1680 train_time:43747ms step_avg:87.32ms
step:502/1680 train_time:43815ms step_avg:87.28ms
step:503/1680 train_time:43905ms step_avg:87.29ms
step:504/1680 train_time:43993ms step_avg:87.29ms
step:505/1680 train_time:44081ms step_avg:87.29ms
step:506/1680 train_time:44168ms step_avg:87.29ms
step:507/1680 train_time:44254ms step_avg:87.29ms
step:508/1680 train_time:44340ms step_avg:87.28ms
step:509/1680 train_time:44426ms step_avg:87.28ms
step:510/1680 train_time:44512ms step_avg:87.28ms
step:511/1680 train_time:44598ms step_avg:87.28ms
step:512/1680 train_time:44687ms step_avg:87.28ms
step:513/1680 train_time:44775ms step_avg:87.28ms
step:514/1680 train_time:44865ms step_avg:87.29ms
step:515/1680 train_time:44953ms step_avg:87.29ms
step:516/1680 train_time:45041ms step_avg:87.29ms
step:517/1680 train_time:45127ms step_avg:87.29ms
step:518/1680 train_time:45214ms step_avg:87.29ms
step:519/1680 train_time:45301ms step_avg:87.29ms
step:520/1680 train_time:45387ms step_avg:87.28ms
step:521/1680 train_time:45474ms step_avg:87.28ms
step:522/1680 train_time:45561ms step_avg:87.28ms
step:523/1680 train_time:45648ms step_avg:87.28ms
step:524/1680 train_time:45736ms step_avg:87.28ms
step:525/1680 train_time:45825ms step_avg:87.28ms
step:526/1680 train_time:45914ms step_avg:87.29ms
step:527/1680 train_time:46001ms step_avg:87.29ms
step:528/1680 train_time:46088ms step_avg:87.29ms
step:529/1680 train_time:46175ms step_avg:87.29ms
step:530/1680 train_time:46262ms step_avg:87.29ms
step:531/1680 train_time:46349ms step_avg:87.29ms
step:532/1680 train_time:46436ms step_avg:87.29ms
step:533/1680 train_time:46523ms step_avg:87.29ms
step:534/1680 train_time:46610ms step_avg:87.28ms
step:535/1680 train_time:46697ms step_avg:87.28ms
step:536/1680 train_time:46785ms step_avg:87.29ms
step:537/1680 train_time:46873ms step_avg:87.29ms
step:538/1680 train_time:46961ms step_avg:87.29ms
step:539/1680 train_time:47049ms step_avg:87.29ms
step:540/1680 train_time:47136ms step_avg:87.29ms
step:541/1680 train_time:47223ms step_avg:87.29ms
step:542/1680 train_time:47310ms step_avg:87.29ms
step:543/1680 train_time:47397ms step_avg:87.29ms
step:544/1680 train_time:47484ms step_avg:87.29ms
step:545/1680 train_time:47571ms step_avg:87.29ms
step:546/1680 train_time:47658ms step_avg:87.29ms
step:547/1680 train_time:47745ms step_avg:87.29ms
step:548/1680 train_time:47833ms step_avg:87.29ms
step:549/1680 train_time:47922ms step_avg:87.29ms
step:550/1680 train_time:48011ms step_avg:87.29ms
step:551/1680 train_time:48100ms step_avg:87.30ms
step:552/1680 train_time:48188ms step_avg:87.30ms
step:553/1680 train_time:48276ms step_avg:87.30ms
step:554/1680 train_time:48365ms step_avg:87.30ms
step:555/1680 train_time:48453ms step_avg:87.30ms
step:556/1680 train_time:48542ms step_avg:87.31ms
step:557/1680 train_time:48630ms step_avg:87.31ms
step:558/1680 train_time:48718ms step_avg:87.31ms
step:559/1680 train_time:48807ms step_avg:87.31ms
step:560/1680 train_time:48896ms step_avg:87.31ms
step:561/1680 train_time:48985ms step_avg:87.32ms
step:562/1680 train_time:49074ms step_avg:87.32ms
step:563/1680 train_time:49163ms step_avg:87.32ms
step:564/1680 train_time:49251ms step_avg:87.32ms
step:565/1680 train_time:49340ms step_avg:87.33ms
step:566/1680 train_time:49428ms step_avg:87.33ms
step:567/1680 train_time:49517ms step_avg:87.33ms
step:568/1680 train_time:49604ms step_avg:87.33ms
step:569/1680 train_time:49692ms step_avg:87.33ms
step:570/1680 train_time:49782ms step_avg:87.34ms
step:571/1680 train_time:49870ms step_avg:87.34ms
step:572/1680 train_time:49959ms step_avg:87.34ms
step:573/1680 train_time:50048ms step_avg:87.34ms
step:574/1680 train_time:50136ms step_avg:87.35ms
step:575/1680 train_time:50225ms step_avg:87.35ms
step:576/1680 train_time:50313ms step_avg:87.35ms
step:577/1680 train_time:50402ms step_avg:87.35ms
step:578/1680 train_time:50490ms step_avg:87.35ms
step:579/1680 train_time:50578ms step_avg:87.35ms
step:580/1680 train_time:50666ms step_avg:87.35ms
step:581/1680 train_time:50755ms step_avg:87.36ms
step:582/1680 train_time:50844ms step_avg:87.36ms
step:583/1680 train_time:50933ms step_avg:87.36ms
step:584/1680 train_time:51021ms step_avg:87.36ms
step:585/1680 train_time:51110ms step_avg:87.37ms
step:586/1680 train_time:51199ms step_avg:87.37ms
step:587/1680 train_time:51288ms step_avg:87.37ms
step:588/1680 train_time:51376ms step_avg:87.37ms
step:589/1680 train_time:51465ms step_avg:87.38ms
step:590/1680 train_time:51553ms step_avg:87.38ms
step:591/1680 train_time:51641ms step_avg:87.38ms
step:592/1680 train_time:51729ms step_avg:87.38ms
step:593/1680 train_time:51817ms step_avg:87.38ms
step:594/1680 train_time:51906ms step_avg:87.38ms
step:595/1680 train_time:51994ms step_avg:87.38ms
step:596/1680 train_time:52083ms step_avg:87.39ms
step:597/1680 train_time:52171ms step_avg:87.39ms
step:598/1680 train_time:52260ms step_avg:87.39ms
step:599/1680 train_time:52349ms step_avg:87.39ms
step:600/1680 train_time:52437ms step_avg:87.39ms
step:601/1680 train_time:52525ms step_avg:87.40ms
step:602/1680 train_time:52613ms step_avg:87.40ms
step:603/1680 train_time:52702ms step_avg:87.40ms
step:604/1680 train_time:52790ms step_avg:87.40ms
step:605/1680 train_time:52879ms step_avg:87.40ms
step:606/1680 train_time:52967ms step_avg:87.40ms
step:607/1680 train_time:53056ms step_avg:87.41ms
step:608/1680 train_time:53146ms step_avg:87.41ms
step:609/1680 train_time:53234ms step_avg:87.41ms
step:610/1680 train_time:53323ms step_avg:87.41ms
step:611/1680 train_time:53411ms step_avg:87.42ms
step:612/1680 train_time:53499ms step_avg:87.42ms
step:613/1680 train_time:53587ms step_avg:87.42ms
step:614/1680 train_time:53676ms step_avg:87.42ms
step:615/1680 train_time:53764ms step_avg:87.42ms
step:616/1680 train_time:53853ms step_avg:87.42ms
step:617/1680 train_time:53942ms step_avg:87.43ms
step:618/1680 train_time:54030ms step_avg:87.43ms
step:619/1680 train_time:54118ms step_avg:87.43ms
step:620/1680 train_time:54207ms step_avg:87.43ms
step:621/1680 train_time:54295ms step_avg:87.43ms
step:622/1680 train_time:54384ms step_avg:87.43ms
step:623/1680 train_time:54472ms step_avg:87.44ms
step:624/1680 train_time:54561ms step_avg:87.44ms
step:625/1680 train_time:54649ms step_avg:87.44ms
step:625/1680 val_loss:3.6126 train_time:54739ms step_avg:87.58ms
step:626/1680 train_time:54763ms step_avg:87.48ms
step:627/1680 train_time:54829ms step_avg:87.45ms
step:628/1680 train_time:54919ms step_avg:87.45ms
step:629/1680 train_time:55009ms step_avg:87.46ms
step:630/1680 train_time:55099ms step_avg:87.46ms
step:631/1680 train_time:55186ms step_avg:87.46ms
step:632/1680 train_time:55274ms step_avg:87.46ms
step:633/1680 train_time:55361ms step_avg:87.46ms
step:634/1680 train_time:55448ms step_avg:87.46ms
step:635/1680 train_time:55535ms step_avg:87.46ms
step:636/1680 train_time:55625ms step_avg:87.46ms
step:637/1680 train_time:55718ms step_avg:87.47ms
step:638/1680 train_time:55809ms step_avg:87.48ms
step:639/1680 train_time:55898ms step_avg:87.48ms
step:640/1680 train_time:55986ms step_avg:87.48ms
step:641/1680 train_time:56075ms step_avg:87.48ms
step:642/1680 train_time:56164ms step_avg:87.48ms
step:643/1680 train_time:56251ms step_avg:87.48ms
step:644/1680 train_time:56339ms step_avg:87.48ms
step:645/1680 train_time:56426ms step_avg:87.48ms
step:646/1680 train_time:56515ms step_avg:87.48ms
step:647/1680 train_time:56604ms step_avg:87.49ms
step:648/1680 train_time:56693ms step_avg:87.49ms
step:649/1680 train_time:56783ms step_avg:87.49ms
step:650/1680 train_time:56872ms step_avg:87.50ms
step:651/1680 train_time:56960ms step_avg:87.50ms
step:652/1680 train_time:57048ms step_avg:87.50ms
step:653/1680 train_time:57137ms step_avg:87.50ms
step:654/1680 train_time:57224ms step_avg:87.50ms
step:655/1680 train_time:57312ms step_avg:87.50ms
step:656/1680 train_time:57399ms step_avg:87.50ms
step:657/1680 train_time:57487ms step_avg:87.50ms
step:658/1680 train_time:57576ms step_avg:87.50ms
step:659/1680 train_time:57665ms step_avg:87.50ms
step:660/1680 train_time:57753ms step_avg:87.51ms
step:661/1680 train_time:57842ms step_avg:87.51ms
step:662/1680 train_time:57930ms step_avg:87.51ms
step:663/1680 train_time:58019ms step_avg:87.51ms
step:664/1680 train_time:58107ms step_avg:87.51ms
step:665/1680 train_time:58196ms step_avg:87.51ms
step:666/1680 train_time:58284ms step_avg:87.51ms
step:667/1680 train_time:58371ms step_avg:87.51ms
step:668/1680 train_time:58460ms step_avg:87.52ms
step:669/1680 train_time:58548ms step_avg:87.52ms
step:670/1680 train_time:58637ms step_avg:87.52ms
step:671/1680 train_time:58725ms step_avg:87.52ms
step:672/1680 train_time:58813ms step_avg:87.52ms
step:673/1680 train_time:58903ms step_avg:87.52ms
step:674/1680 train_time:58991ms step_avg:87.52ms
step:675/1680 train_time:59079ms step_avg:87.52ms
step:676/1680 train_time:59167ms step_avg:87.53ms
step:677/1680 train_time:59256ms step_avg:87.53ms
step:678/1680 train_time:59343ms step_avg:87.53ms
step:679/1680 train_time:59431ms step_avg:87.53ms
step:680/1680 train_time:59519ms step_avg:87.53ms
step:681/1680 train_time:59607ms step_avg:87.53ms
step:682/1680 train_time:59696ms step_avg:87.53ms
step:683/1680 train_time:59785ms step_avg:87.53ms
step:684/1680 train_time:59873ms step_avg:87.53ms
step:685/1680 train_time:59962ms step_avg:87.54ms
step:686/1680 train_time:60051ms step_avg:87.54ms
step:687/1680 train_time:60139ms step_avg:87.54ms
step:688/1680 train_time:60227ms step_avg:87.54ms
step:689/1680 train_time:60315ms step_avg:87.54ms
step:690/1680 train_time:60403ms step_avg:87.54ms
step:691/1680 train_time:60492ms step_avg:87.54ms
step:692/1680 train_time:60581ms step_avg:87.54ms
step:693/1680 train_time:60669ms step_avg:87.55ms
step:694/1680 train_time:60757ms step_avg:87.55ms
step:695/1680 train_time:60845ms step_avg:87.55ms
step:696/1680 train_time:60933ms step_avg:87.55ms
step:697/1680 train_time:61023ms step_avg:87.55ms
step:698/1680 train_time:61111ms step_avg:87.55ms
step:699/1680 train_time:61200ms step_avg:87.55ms
step:700/1680 train_time:61289ms step_avg:87.56ms
step:701/1680 train_time:61378ms step_avg:87.56ms
step:702/1680 train_time:61467ms step_avg:87.56ms
step:703/1680 train_time:61555ms step_avg:87.56ms
step:704/1680 train_time:61643ms step_avg:87.56ms
step:705/1680 train_time:61732ms step_avg:87.56ms
step:706/1680 train_time:61820ms step_avg:87.56ms
step:707/1680 train_time:61908ms step_avg:87.56ms
step:708/1680 train_time:61996ms step_avg:87.57ms
step:709/1680 train_time:62084ms step_avg:87.57ms
step:710/1680 train_time:62172ms step_avg:87.57ms
step:711/1680 train_time:62260ms step_avg:87.57ms
step:712/1680 train_time:62349ms step_avg:87.57ms
step:713/1680 train_time:62437ms step_avg:87.57ms
step:714/1680 train_time:62524ms step_avg:87.57ms
step:715/1680 train_time:62612ms step_avg:87.57ms
step:716/1680 train_time:62700ms step_avg:87.57ms
step:717/1680 train_time:62789ms step_avg:87.57ms
step:718/1680 train_time:62878ms step_avg:87.57ms
step:719/1680 train_time:62965ms step_avg:87.57ms
step:720/1680 train_time:63053ms step_avg:87.57ms
step:721/1680 train_time:63142ms step_avg:87.58ms
step:722/1680 train_time:63230ms step_avg:87.58ms
step:723/1680 train_time:63318ms step_avg:87.58ms
step:724/1680 train_time:63406ms step_avg:87.58ms
step:725/1680 train_time:63495ms step_avg:87.58ms
step:726/1680 train_time:63583ms step_avg:87.58ms
step:727/1680 train_time:63670ms step_avg:87.58ms
step:728/1680 train_time:63759ms step_avg:87.58ms
step:729/1680 train_time:63848ms step_avg:87.58ms
step:730/1680 train_time:63936ms step_avg:87.58ms
step:731/1680 train_time:64024ms step_avg:87.58ms
step:732/1680 train_time:64113ms step_avg:87.59ms
step:733/1680 train_time:64202ms step_avg:87.59ms
step:734/1680 train_time:64290ms step_avg:87.59ms
step:735/1680 train_time:64379ms step_avg:87.59ms
step:736/1680 train_time:64468ms step_avg:87.59ms
step:737/1680 train_time:64556ms step_avg:87.59ms
step:738/1680 train_time:64644ms step_avg:87.59ms
step:739/1680 train_time:64732ms step_avg:87.59ms
step:740/1680 train_time:64820ms step_avg:87.60ms
step:741/1680 train_time:64910ms step_avg:87.60ms
step:742/1680 train_time:64998ms step_avg:87.60ms
step:743/1680 train_time:65086ms step_avg:87.60ms
step:744/1680 train_time:65175ms step_avg:87.60ms
step:745/1680 train_time:65263ms step_avg:87.60ms
step:746/1680 train_time:65351ms step_avg:87.60ms
step:747/1680 train_time:65440ms step_avg:87.60ms
step:748/1680 train_time:65528ms step_avg:87.60ms
step:749/1680 train_time:65616ms step_avg:87.61ms
step:750/1680 train_time:65705ms step_avg:87.61ms
step:750/1680 val_loss:3.5614 train_time:65795ms step_avg:87.73ms
step:751/1680 train_time:65818ms step_avg:87.64ms
step:752/1680 train_time:65885ms step_avg:87.61ms
step:753/1680 train_time:65977ms step_avg:87.62ms
step:754/1680 train_time:66066ms step_avg:87.62ms
step:755/1680 train_time:66154ms step_avg:87.62ms
step:756/1680 train_time:66243ms step_avg:87.62ms
step:757/1680 train_time:66331ms step_avg:87.62ms
step:758/1680 train_time:66418ms step_avg:87.62ms
step:759/1680 train_time:66506ms step_avg:87.62ms
step:760/1680 train_time:66593ms step_avg:87.62ms
step:761/1680 train_time:66681ms step_avg:87.62ms
step:762/1680 train_time:66769ms step_avg:87.62ms
step:763/1680 train_time:66860ms step_avg:87.63ms
step:764/1680 train_time:66949ms step_avg:87.63ms
step:765/1680 train_time:67038ms step_avg:87.63ms
step:766/1680 train_time:67127ms step_avg:87.63ms
step:767/1680 train_time:67217ms step_avg:87.64ms
step:768/1680 train_time:67305ms step_avg:87.64ms
step:769/1680 train_time:67392ms step_avg:87.64ms
step:770/1680 train_time:67480ms step_avg:87.64ms
step:771/1680 train_time:67568ms step_avg:87.64ms
step:772/1680 train_time:67656ms step_avg:87.64ms
step:773/1680 train_time:67744ms step_avg:87.64ms
step:774/1680 train_time:67833ms step_avg:87.64ms
step:775/1680 train_time:67922ms step_avg:87.64ms
step:776/1680 train_time:68011ms step_avg:87.64ms
step:777/1680 train_time:68101ms step_avg:87.65ms
step:778/1680 train_time:68189ms step_avg:87.65ms
step:779/1680 train_time:68278ms step_avg:87.65ms
step:780/1680 train_time:68366ms step_avg:87.65ms
step:781/1680 train_time:68454ms step_avg:87.65ms
step:782/1680 train_time:68542ms step_avg:87.65ms
step:783/1680 train_time:68630ms step_avg:87.65ms
step:784/1680 train_time:68718ms step_avg:87.65ms
step:785/1680 train_time:68807ms step_avg:87.65ms
step:786/1680 train_time:68896ms step_avg:87.65ms
step:787/1680 train_time:68985ms step_avg:87.66ms
step:788/1680 train_time:69075ms step_avg:87.66ms
step:789/1680 train_time:69163ms step_avg:87.66ms
step:790/1680 train_time:69252ms step_avg:87.66ms
step:791/1680 train_time:69340ms step_avg:87.66ms
step:792/1680 train_time:69428ms step_avg:87.66ms
step:793/1680 train_time:69517ms step_avg:87.66ms
step:794/1680 train_time:69605ms step_avg:87.66ms
step:795/1680 train_time:69694ms step_avg:87.66ms
step:796/1680 train_time:69782ms step_avg:87.67ms
step:797/1680 train_time:69870ms step_avg:87.67ms
step:798/1680 train_time:69958ms step_avg:87.67ms
step:799/1680 train_time:70047ms step_avg:87.67ms
step:800/1680 train_time:70137ms step_avg:87.67ms
step:801/1680 train_time:70225ms step_avg:87.67ms
step:802/1680 train_time:70314ms step_avg:87.67ms
step:803/1680 train_time:70402ms step_avg:87.67ms
step:804/1680 train_time:70489ms step_avg:87.67ms
step:805/1680 train_time:70577ms step_avg:87.67ms
step:806/1680 train_time:70665ms step_avg:87.67ms
step:807/1680 train_time:70754ms step_avg:87.68ms
step:808/1680 train_time:70843ms step_avg:87.68ms
step:809/1680 train_time:70931ms step_avg:87.68ms
step:810/1680 train_time:71019ms step_avg:87.68ms
step:811/1680 train_time:71108ms step_avg:87.68ms
step:812/1680 train_time:71196ms step_avg:87.68ms
step:813/1680 train_time:71285ms step_avg:87.68ms
step:814/1680 train_time:71374ms step_avg:87.68ms
step:815/1680 train_time:71462ms step_avg:87.68ms
step:816/1680 train_time:71550ms step_avg:87.68ms
step:817/1680 train_time:71638ms step_avg:87.68ms
step:818/1680 train_time:71727ms step_avg:87.69ms
step:819/1680 train_time:71815ms step_avg:87.69ms
step:820/1680 train_time:71903ms step_avg:87.69ms
step:821/1680 train_time:71992ms step_avg:87.69ms
step:822/1680 train_time:72080ms step_avg:87.69ms
step:823/1680 train_time:72168ms step_avg:87.69ms
step:824/1680 train_time:72257ms step_avg:87.69ms
step:825/1680 train_time:72345ms step_avg:87.69ms
step:826/1680 train_time:72434ms step_avg:87.69ms
step:827/1680 train_time:72522ms step_avg:87.69ms
step:828/1680 train_time:72610ms step_avg:87.69ms
step:829/1680 train_time:72698ms step_avg:87.69ms
step:830/1680 train_time:72786ms step_avg:87.69ms
step:831/1680 train_time:72874ms step_avg:87.69ms
step:832/1680 train_time:72962ms step_avg:87.69ms
step:833/1680 train_time:73050ms step_avg:87.70ms
step:834/1680 train_time:73139ms step_avg:87.70ms
step:835/1680 train_time:73228ms step_avg:87.70ms
step:836/1680 train_time:73317ms step_avg:87.70ms
step:837/1680 train_time:73405ms step_avg:87.70ms
step:838/1680 train_time:73494ms step_avg:87.70ms
step:839/1680 train_time:73582ms step_avg:87.70ms
step:840/1680 train_time:73671ms step_avg:87.70ms
step:841/1680 train_time:73759ms step_avg:87.70ms
step:842/1680 train_time:73847ms step_avg:87.70ms
step:843/1680 train_time:73936ms step_avg:87.71ms
step:844/1680 train_time:74024ms step_avg:87.71ms
step:845/1680 train_time:74112ms step_avg:87.71ms
step:846/1680 train_time:74200ms step_avg:87.71ms
step:847/1680 train_time:74289ms step_avg:87.71ms
step:848/1680 train_time:74378ms step_avg:87.71ms
step:849/1680 train_time:74466ms step_avg:87.71ms
step:850/1680 train_time:74554ms step_avg:87.71ms
step:851/1680 train_time:74643ms step_avg:87.71ms
step:852/1680 train_time:74731ms step_avg:87.71ms
step:853/1680 train_time:74819ms step_avg:87.71ms
step:854/1680 train_time:74908ms step_avg:87.71ms
step:855/1680 train_time:74995ms step_avg:87.71ms
step:856/1680 train_time:75083ms step_avg:87.71ms
step:857/1680 train_time:75172ms step_avg:87.72ms
step:858/1680 train_time:75260ms step_avg:87.72ms
step:859/1680 train_time:75349ms step_avg:87.72ms
step:860/1680 train_time:75437ms step_avg:87.72ms
step:861/1680 train_time:75526ms step_avg:87.72ms
step:862/1680 train_time:75614ms step_avg:87.72ms
step:863/1680 train_time:75702ms step_avg:87.72ms
step:864/1680 train_time:75790ms step_avg:87.72ms
step:865/1680 train_time:75878ms step_avg:87.72ms
step:866/1680 train_time:75967ms step_avg:87.72ms
step:867/1680 train_time:76055ms step_avg:87.72ms
step:868/1680 train_time:76144ms step_avg:87.72ms
step:869/1680 train_time:76233ms step_avg:87.72ms
step:870/1680 train_time:76321ms step_avg:87.73ms
step:871/1680 train_time:76410ms step_avg:87.73ms
step:872/1680 train_time:76498ms step_avg:87.73ms
step:873/1680 train_time:76587ms step_avg:87.73ms
step:874/1680 train_time:76675ms step_avg:87.73ms
step:875/1680 train_time:76763ms step_avg:87.73ms
step:875/1680 val_loss:3.5152 train_time:76853ms step_avg:87.83ms
step:876/1680 train_time:76872ms step_avg:87.75ms
step:877/1680 train_time:76947ms step_avg:87.74ms
step:878/1680 train_time:77042ms step_avg:87.75ms
step:879/1680 train_time:77131ms step_avg:87.75ms
step:880/1680 train_time:77219ms step_avg:87.75ms
step:881/1680 train_time:77308ms step_avg:87.75ms
step:882/1680 train_time:77395ms step_avg:87.75ms
step:883/1680 train_time:77483ms step_avg:87.75ms
step:884/1680 train_time:77571ms step_avg:87.75ms
step:885/1680 train_time:77659ms step_avg:87.75ms
step:886/1680 train_time:77746ms step_avg:87.75ms
step:887/1680 train_time:77835ms step_avg:87.75ms
step:888/1680 train_time:77925ms step_avg:87.75ms
step:889/1680 train_time:78015ms step_avg:87.76ms
step:890/1680 train_time:78104ms step_avg:87.76ms
step:891/1680 train_time:78193ms step_avg:87.76ms
step:892/1680 train_time:78282ms step_avg:87.76ms
step:893/1680 train_time:78370ms step_avg:87.76ms
step:894/1680 train_time:78457ms step_avg:87.76ms
step:895/1680 train_time:78545ms step_avg:87.76ms
step:896/1680 train_time:78632ms step_avg:87.76ms
step:897/1680 train_time:78720ms step_avg:87.76ms
step:898/1680 train_time:78809ms step_avg:87.76ms
step:899/1680 train_time:78897ms step_avg:87.76ms
step:900/1680 train_time:78986ms step_avg:87.76ms
step:901/1680 train_time:79075ms step_avg:87.76ms
step:902/1680 train_time:79164ms step_avg:87.76ms
step:903/1680 train_time:79254ms step_avg:87.77ms
step:904/1680 train_time:79342ms step_avg:87.77ms
step:905/1680 train_time:79430ms step_avg:87.77ms
step:906/1680 train_time:79519ms step_avg:87.77ms
step:907/1680 train_time:79606ms step_avg:87.77ms
step:908/1680 train_time:79694ms step_avg:87.77ms
step:909/1680 train_time:79783ms step_avg:87.77ms
step:910/1680 train_time:79871ms step_avg:87.77ms
step:911/1680 train_time:79960ms step_avg:87.77ms
step:912/1680 train_time:80049ms step_avg:87.77ms
step:913/1680 train_time:80138ms step_avg:87.77ms
step:914/1680 train_time:80227ms step_avg:87.78ms
step:915/1680 train_time:80315ms step_avg:87.78ms
step:916/1680 train_time:80403ms step_avg:87.78ms
step:917/1680 train_time:80491ms step_avg:87.78ms
step:918/1680 train_time:80580ms step_avg:87.78ms
step:919/1680 train_time:80668ms step_avg:87.78ms
step:920/1680 train_time:80757ms step_avg:87.78ms
step:921/1680 train_time:80845ms step_avg:87.78ms
step:922/1680 train_time:80933ms step_avg:87.78ms
step:923/1680 train_time:81022ms step_avg:87.78ms
step:924/1680 train_time:81112ms step_avg:87.78ms
step:925/1680 train_time:81202ms step_avg:87.79ms
step:926/1680 train_time:81290ms step_avg:87.79ms
step:927/1680 train_time:81379ms step_avg:87.79ms
step:928/1680 train_time:81467ms step_avg:87.79ms
step:929/1680 train_time:81556ms step_avg:87.79ms
step:930/1680 train_time:81644ms step_avg:87.79ms
step:931/1680 train_time:81733ms step_avg:87.79ms
step:932/1680 train_time:81821ms step_avg:87.79ms
step:933/1680 train_time:81909ms step_avg:87.79ms
step:934/1680 train_time:81998ms step_avg:87.79ms
step:935/1680 train_time:82086ms step_avg:87.79ms
step:936/1680 train_time:82175ms step_avg:87.79ms
step:937/1680 train_time:82264ms step_avg:87.79ms
step:938/1680 train_time:82351ms step_avg:87.79ms
step:939/1680 train_time:82440ms step_avg:87.80ms
step:940/1680 train_time:82528ms step_avg:87.80ms
step:941/1680 train_time:82617ms step_avg:87.80ms
step:942/1680 train_time:82705ms step_avg:87.80ms
step:943/1680 train_time:82792ms step_avg:87.80ms
step:944/1680 train_time:82882ms step_avg:87.80ms
step:945/1680 train_time:82970ms step_avg:87.80ms
step:946/1680 train_time:83059ms step_avg:87.80ms
step:947/1680 train_time:83147ms step_avg:87.80ms
step:948/1680 train_time:83236ms step_avg:87.80ms
step:949/1680 train_time:83324ms step_avg:87.80ms
step:950/1680 train_time:83412ms step_avg:87.80ms
step:951/1680 train_time:83500ms step_avg:87.80ms
step:952/1680 train_time:83589ms step_avg:87.80ms
step:953/1680 train_time:83678ms step_avg:87.80ms
step:954/1680 train_time:83766ms step_avg:87.80ms
step:955/1680 train_time:83854ms step_avg:87.81ms
step:956/1680 train_time:83943ms step_avg:87.81ms
step:957/1680 train_time:84032ms step_avg:87.81ms
step:958/1680 train_time:84121ms step_avg:87.81ms
step:959/1680 train_time:84210ms step_avg:87.81ms
step:960/1680 train_time:84299ms step_avg:87.81ms
step:961/1680 train_time:84387ms step_avg:87.81ms
step:962/1680 train_time:84475ms step_avg:87.81ms
step:963/1680 train_time:84563ms step_avg:87.81ms
step:964/1680 train_time:84652ms step_avg:87.81ms
step:965/1680 train_time:84739ms step_avg:87.81ms
step:966/1680 train_time:84828ms step_avg:87.81ms
step:967/1680 train_time:84916ms step_avg:87.81ms
step:968/1680 train_time:85005ms step_avg:87.82ms
step:969/1680 train_time:85094ms step_avg:87.82ms
step:970/1680 train_time:85183ms step_avg:87.82ms
step:971/1680 train_time:85271ms step_avg:87.82ms
step:972/1680 train_time:85359ms step_avg:87.82ms
step:973/1680 train_time:85447ms step_avg:87.82ms
step:974/1680 train_time:85535ms step_avg:87.82ms
step:975/1680 train_time:85623ms step_avg:87.82ms
step:976/1680 train_time:85711ms step_avg:87.82ms
step:977/1680 train_time:85799ms step_avg:87.82ms
step:978/1680 train_time:85887ms step_avg:87.82ms
step:979/1680 train_time:85975ms step_avg:87.82ms
step:980/1680 train_time:86065ms step_avg:87.82ms
step:981/1680 train_time:86154ms step_avg:87.82ms
step:982/1680 train_time:86242ms step_avg:87.82ms
step:983/1680 train_time:86332ms step_avg:87.83ms
step:984/1680 train_time:86421ms step_avg:87.83ms
step:985/1680 train_time:86510ms step_avg:87.83ms
step:986/1680 train_time:86599ms step_avg:87.83ms
step:987/1680 train_time:86688ms step_avg:87.83ms
step:988/1680 train_time:86776ms step_avg:87.83ms
step:989/1680 train_time:86864ms step_avg:87.83ms
step:990/1680 train_time:86952ms step_avg:87.83ms
step:991/1680 train_time:87040ms step_avg:87.83ms
step:992/1680 train_time:87129ms step_avg:87.83ms
step:993/1680 train_time:87219ms step_avg:87.83ms
step:994/1680 train_time:87307ms step_avg:87.83ms
step:995/1680 train_time:87394ms step_avg:87.83ms
step:996/1680 train_time:87483ms step_avg:87.83ms
step:997/1680 train_time:87572ms step_avg:87.84ms
step:998/1680 train_time:87661ms step_avg:87.84ms
step:999/1680 train_time:87749ms step_avg:87.84ms
step:1000/1680 train_time:87837ms step_avg:87.84ms
step:1000/1680 val_loss:3.4656 train_time:87926ms step_avg:87.93ms
step:1001/1680 train_time:87947ms step_avg:87.86ms
step:1002/1680 train_time:88017ms step_avg:87.84ms
step:1003/1680 train_time:88109ms step_avg:87.85ms
step:1004/1680 train_time:88198ms step_avg:87.85ms
step:1005/1680 train_time:88285ms step_avg:87.85ms
step:1006/1680 train_time:88373ms step_avg:87.85ms
step:1007/1680 train_time:88460ms step_avg:87.85ms
step:1008/1680 train_time:88549ms step_avg:87.85ms
step:1009/1680 train_time:88637ms step_avg:87.85ms
step:1010/1680 train_time:88726ms step_avg:87.85ms
step:1011/1680 train_time:88814ms step_avg:87.85ms
step:1012/1680 train_time:88903ms step_avg:87.85ms
step:1013/1680 train_time:88992ms step_avg:87.85ms
step:1014/1680 train_time:89081ms step_avg:87.85ms
step:1015/1680 train_time:89170ms step_avg:87.85ms
step:1016/1680 train_time:89259ms step_avg:87.85ms
step:1017/1680 train_time:89348ms step_avg:87.85ms
step:1018/1680 train_time:89436ms step_avg:87.85ms
step:1019/1680 train_time:89524ms step_avg:87.85ms
step:1020/1680 train_time:89611ms step_avg:87.85ms
step:1021/1680 train_time:89699ms step_avg:87.85ms
step:1022/1680 train_time:89788ms step_avg:87.85ms
step:1023/1680 train_time:89876ms step_avg:87.85ms
step:1024/1680 train_time:89965ms step_avg:87.86ms
step:1025/1680 train_time:90054ms step_avg:87.86ms
step:1026/1680 train_time:90143ms step_avg:87.86ms
step:1027/1680 train_time:90231ms step_avg:87.86ms
step:1028/1680 train_time:90319ms step_avg:87.86ms
step:1029/1680 train_time:90407ms step_avg:87.86ms
step:1030/1680 train_time:90495ms step_avg:87.86ms
step:1031/1680 train_time:90582ms step_avg:87.86ms
step:1032/1680 train_time:90670ms step_avg:87.86ms
step:1033/1680 train_time:90758ms step_avg:87.86ms
step:1034/1680 train_time:90847ms step_avg:87.86ms
step:1035/1680 train_time:90935ms step_avg:87.86ms
step:1036/1680 train_time:91024ms step_avg:87.86ms
step:1037/1680 train_time:91112ms step_avg:87.86ms
step:1038/1680 train_time:91202ms step_avg:87.86ms
step:1039/1680 train_time:91290ms step_avg:87.86ms
step:1040/1680 train_time:91378ms step_avg:87.86ms
step:1041/1680 train_time:91467ms step_avg:87.86ms
step:1042/1680 train_time:91556ms step_avg:87.87ms
step:1043/1680 train_time:91645ms step_avg:87.87ms
step:1044/1680 train_time:91733ms step_avg:87.87ms
step:1045/1680 train_time:91821ms step_avg:87.87ms
step:1046/1680 train_time:91910ms step_avg:87.87ms
step:1047/1680 train_time:91999ms step_avg:87.87ms
step:1048/1680 train_time:92087ms step_avg:87.87ms
step:1049/1680 train_time:92175ms step_avg:87.87ms
step:1050/1680 train_time:92264ms step_avg:87.87ms
step:1051/1680 train_time:92353ms step_avg:87.87ms
step:1052/1680 train_time:92441ms step_avg:87.87ms
step:1053/1680 train_time:92529ms step_avg:87.87ms
step:1054/1680 train_time:92617ms step_avg:87.87ms
step:1055/1680 train_time:92705ms step_avg:87.87ms
step:1056/1680 train_time:92794ms step_avg:87.87ms
step:1057/1680 train_time:92882ms step_avg:87.87ms
step:1058/1680 train_time:92971ms step_avg:87.87ms
step:1059/1680 train_time:93059ms step_avg:87.87ms
step:1060/1680 train_time:93149ms step_avg:87.88ms
step:1061/1680 train_time:93238ms step_avg:87.88ms
step:1062/1680 train_time:93326ms step_avg:87.88ms
step:1063/1680 train_time:93415ms step_avg:87.88ms
step:1064/1680 train_time:93503ms step_avg:87.88ms
step:1065/1680 train_time:93591ms step_avg:87.88ms
step:1066/1680 train_time:93679ms step_avg:87.88ms
step:1067/1680 train_time:93768ms step_avg:87.88ms
step:1068/1680 train_time:93856ms step_avg:87.88ms
step:1069/1680 train_time:93945ms step_avg:87.88ms
step:1070/1680 train_time:94034ms step_avg:87.88ms
step:1071/1680 train_time:94121ms step_avg:87.88ms
step:1072/1680 train_time:94211ms step_avg:87.88ms
step:1073/1680 train_time:94300ms step_avg:87.88ms
step:1074/1680 train_time:94388ms step_avg:87.88ms
step:1075/1680 train_time:94475ms step_avg:87.88ms
step:1076/1680 train_time:94564ms step_avg:87.88ms
step:1077/1680 train_time:94653ms step_avg:87.89ms
step:1078/1680 train_time:94741ms step_avg:87.89ms
step:1079/1680 train_time:94829ms step_avg:87.89ms
step:1080/1680 train_time:94918ms step_avg:87.89ms
step:1081/1680 train_time:95007ms step_avg:87.89ms
step:1082/1680 train_time:95096ms step_avg:87.89ms
step:1083/1680 train_time:95184ms step_avg:87.89ms
step:1084/1680 train_time:95273ms step_avg:87.89ms
step:1085/1680 train_time:95361ms step_avg:87.89ms
step:1086/1680 train_time:95450ms step_avg:87.89ms
step:1087/1680 train_time:95539ms step_avg:87.89ms
step:1088/1680 train_time:95627ms step_avg:87.89ms
step:1089/1680 train_time:95715ms step_avg:87.89ms
step:1090/1680 train_time:95803ms step_avg:87.89ms
step:1091/1680 train_time:95891ms step_avg:87.89ms
step:1092/1680 train_time:95980ms step_avg:87.89ms
step:1093/1680 train_time:96069ms step_avg:87.89ms
step:1094/1680 train_time:96158ms step_avg:87.90ms
step:1095/1680 train_time:96247ms step_avg:87.90ms
step:1096/1680 train_time:96336ms step_avg:87.90ms
step:1097/1680 train_time:96425ms step_avg:87.90ms
step:1098/1680 train_time:96515ms step_avg:87.90ms
step:1099/1680 train_time:96604ms step_avg:87.90ms
step:1100/1680 train_time:96692ms step_avg:87.90ms
step:1101/1680 train_time:96781ms step_avg:87.90ms
step:1102/1680 train_time:96870ms step_avg:87.90ms
step:1103/1680 train_time:96960ms step_avg:87.91ms
step:1104/1680 train_time:97049ms step_avg:87.91ms
step:1105/1680 train_time:97138ms step_avg:87.91ms
step:1106/1680 train_time:97228ms step_avg:87.91ms
step:1107/1680 train_time:97317ms step_avg:87.91ms
step:1108/1680 train_time:97406ms step_avg:87.91ms
step:1109/1680 train_time:97495ms step_avg:87.91ms
step:1110/1680 train_time:97585ms step_avg:87.91ms
step:1111/1680 train_time:97674ms step_avg:87.92ms
step:1112/1680 train_time:97763ms step_avg:87.92ms
step:1113/1680 train_time:97852ms step_avg:87.92ms
step:1114/1680 train_time:97941ms step_avg:87.92ms
step:1115/1680 train_time:98031ms step_avg:87.92ms
step:1116/1680 train_time:98121ms step_avg:87.92ms
step:1117/1680 train_time:98211ms step_avg:87.92ms
step:1118/1680 train_time:98300ms step_avg:87.93ms
step:1119/1680 train_time:98390ms step_avg:87.93ms
step:1120/1680 train_time:98479ms step_avg:87.93ms
step:1121/1680 train_time:98567ms step_avg:87.93ms
step:1122/1680 train_time:98656ms step_avg:87.93ms
step:1123/1680 train_time:98745ms step_avg:87.93ms
step:1124/1680 train_time:98836ms step_avg:87.93ms
step:1125/1680 train_time:98925ms step_avg:87.93ms
step:1125/1680 val_loss:3.4121 train_time:99016ms step_avg:88.01ms
step:1126/1680 train_time:99036ms step_avg:87.95ms
step:1127/1680 train_time:99106ms step_avg:87.94ms
step:1128/1680 train_time:99197ms step_avg:87.94ms
step:1129/1680 train_time:99289ms step_avg:87.94ms
step:1130/1680 train_time:99378ms step_avg:87.94ms
step:1131/1680 train_time:99466ms step_avg:87.95ms
step:1132/1680 train_time:99555ms step_avg:87.95ms
step:1133/1680 train_time:99644ms step_avg:87.95ms
step:1134/1680 train_time:99732ms step_avg:87.95ms
step:1135/1680 train_time:99821ms step_avg:87.95ms
step:1136/1680 train_time:99910ms step_avg:87.95ms
step:1137/1680 train_time:100001ms step_avg:87.95ms
step:1138/1680 train_time:100092ms step_avg:87.95ms
step:1139/1680 train_time:100184ms step_avg:87.96ms
step:1140/1680 train_time:100274ms step_avg:87.96ms
step:1141/1680 train_time:100363ms step_avg:87.96ms
step:1142/1680 train_time:100452ms step_avg:87.96ms
step:1143/1680 train_time:100541ms step_avg:87.96ms
step:1144/1680 train_time:100629ms step_avg:87.96ms
step:1145/1680 train_time:100717ms step_avg:87.96ms
step:1146/1680 train_time:100806ms step_avg:87.96ms
step:1147/1680 train_time:100895ms step_avg:87.96ms
step:1148/1680 train_time:100985ms step_avg:87.97ms
step:1149/1680 train_time:101075ms step_avg:87.97ms
step:1150/1680 train_time:101164ms step_avg:87.97ms
step:1151/1680 train_time:101255ms step_avg:87.97ms
step:1152/1680 train_time:101345ms step_avg:87.97ms
step:1153/1680 train_time:101434ms step_avg:87.97ms
step:1154/1680 train_time:101523ms step_avg:87.97ms
step:1155/1680 train_time:101612ms step_avg:87.98ms
step:1156/1680 train_time:101700ms step_avg:87.98ms
step:1157/1680 train_time:101788ms step_avg:87.98ms
step:1158/1680 train_time:101878ms step_avg:87.98ms
step:1159/1680 train_time:101967ms step_avg:87.98ms
step:1160/1680 train_time:102056ms step_avg:87.98ms
step:1161/1680 train_time:102145ms step_avg:87.98ms
step:1162/1680 train_time:102234ms step_avg:87.98ms
step:1163/1680 train_time:102323ms step_avg:87.98ms
step:1164/1680 train_time:102414ms step_avg:87.98ms
step:1165/1680 train_time:102503ms step_avg:87.99ms
step:1166/1680 train_time:102592ms step_avg:87.99ms
step:1167/1680 train_time:102682ms step_avg:87.99ms
step:1168/1680 train_time:102770ms step_avg:87.99ms
step:1169/1680 train_time:102859ms step_avg:87.99ms
step:1170/1680 train_time:102948ms step_avg:87.99ms
step:1171/1680 train_time:103038ms step_avg:87.99ms
step:1172/1680 train_time:103126ms step_avg:87.99ms
step:1173/1680 train_time:103216ms step_avg:87.99ms
step:1174/1680 train_time:103305ms step_avg:87.99ms
step:1175/1680 train_time:103395ms step_avg:88.00ms
step:1176/1680 train_time:103484ms step_avg:88.00ms
step:1177/1680 train_time:103573ms step_avg:88.00ms
step:1178/1680 train_time:103663ms step_avg:88.00ms
step:1179/1680 train_time:103751ms step_avg:88.00ms
step:1180/1680 train_time:103841ms step_avg:88.00ms
step:1181/1680 train_time:103929ms step_avg:88.00ms
step:1182/1680 train_time:104019ms step_avg:88.00ms
step:1183/1680 train_time:104108ms step_avg:88.00ms
step:1184/1680 train_time:104198ms step_avg:88.01ms
step:1185/1680 train_time:104287ms step_avg:88.01ms
step:1186/1680 train_time:104377ms step_avg:88.01ms
step:1187/1680 train_time:104466ms step_avg:88.01ms
step:1188/1680 train_time:104555ms step_avg:88.01ms
step:1189/1680 train_time:104645ms step_avg:88.01ms
step:1190/1680 train_time:104734ms step_avg:88.01ms
step:1191/1680 train_time:104824ms step_avg:88.01ms
step:1192/1680 train_time:104913ms step_avg:88.01ms
step:1193/1680 train_time:105002ms step_avg:88.02ms
step:1194/1680 train_time:105092ms step_avg:88.02ms
step:1195/1680 train_time:105182ms step_avg:88.02ms
step:1196/1680 train_time:105271ms step_avg:88.02ms
step:1197/1680 train_time:105360ms step_avg:88.02ms
step:1198/1680 train_time:105449ms step_avg:88.02ms
step:1199/1680 train_time:105538ms step_avg:88.02ms
step:1200/1680 train_time:105627ms step_avg:88.02ms
step:1201/1680 train_time:105715ms step_avg:88.02ms
step:1202/1680 train_time:105805ms step_avg:88.02ms
step:1203/1680 train_time:105894ms step_avg:88.02ms
step:1204/1680 train_time:105984ms step_avg:88.03ms
step:1205/1680 train_time:106074ms step_avg:88.03ms
step:1206/1680 train_time:106163ms step_avg:88.03ms
step:1207/1680 train_time:106253ms step_avg:88.03ms
step:1208/1680 train_time:106342ms step_avg:88.03ms
step:1209/1680 train_time:106431ms step_avg:88.03ms
step:1210/1680 train_time:106521ms step_avg:88.03ms
step:1211/1680 train_time:106610ms step_avg:88.03ms
step:1212/1680 train_time:106699ms step_avg:88.04ms
step:1213/1680 train_time:106788ms step_avg:88.04ms
step:1214/1680 train_time:106878ms step_avg:88.04ms
step:1215/1680 train_time:106967ms step_avg:88.04ms
step:1216/1680 train_time:107055ms step_avg:88.04ms
step:1217/1680 train_time:107144ms step_avg:88.04ms
step:1218/1680 train_time:107233ms step_avg:88.04ms
step:1219/1680 train_time:107324ms step_avg:88.04ms
step:1220/1680 train_time:107414ms step_avg:88.04ms
step:1221/1680 train_time:107502ms step_avg:88.04ms
step:1222/1680 train_time:107591ms step_avg:88.05ms
step:1223/1680 train_time:107681ms step_avg:88.05ms
step:1224/1680 train_time:107769ms step_avg:88.05ms
step:1225/1680 train_time:107858ms step_avg:88.05ms
step:1226/1680 train_time:107948ms step_avg:88.05ms
step:1227/1680 train_time:108037ms step_avg:88.05ms
step:1228/1680 train_time:108126ms step_avg:88.05ms
step:1229/1680 train_time:108216ms step_avg:88.05ms
step:1230/1680 train_time:108305ms step_avg:88.05ms
step:1231/1680 train_time:108395ms step_avg:88.05ms
step:1232/1680 train_time:108484ms step_avg:88.06ms
step:1233/1680 train_time:108574ms step_avg:88.06ms
step:1234/1680 train_time:108662ms step_avg:88.06ms
step:1235/1680 train_time:108751ms step_avg:88.06ms
step:1236/1680 train_time:108840ms step_avg:88.06ms
step:1237/1680 train_time:108929ms step_avg:88.06ms
step:1238/1680 train_time:109018ms step_avg:88.06ms
step:1239/1680 train_time:109107ms step_avg:88.06ms
step:1240/1680 train_time:109196ms step_avg:88.06ms
step:1241/1680 train_time:109285ms step_avg:88.06ms
step:1242/1680 train_time:109374ms step_avg:88.06ms
step:1243/1680 train_time:109463ms step_avg:88.06ms
step:1244/1680 train_time:109552ms step_avg:88.06ms
step:1245/1680 train_time:109643ms step_avg:88.07ms
step:1246/1680 train_time:109733ms step_avg:88.07ms
step:1247/1680 train_time:109821ms step_avg:88.07ms
step:1248/1680 train_time:109910ms step_avg:88.07ms
step:1249/1680 train_time:109999ms step_avg:88.07ms
step:1250/1680 train_time:110089ms step_avg:88.07ms
step:1250/1680 val_loss:3.3741 train_time:110180ms step_avg:88.14ms
step:1251/1680 train_time:110199ms step_avg:88.09ms
step:1252/1680 train_time:110274ms step_avg:88.08ms
step:1253/1680 train_time:110368ms step_avg:88.08ms
step:1254/1680 train_time:110459ms step_avg:88.09ms
step:1255/1680 train_time:110547ms step_avg:88.09ms
step:1256/1680 train_time:110635ms step_avg:88.09ms
step:1257/1680 train_time:110723ms step_avg:88.09ms
step:1258/1680 train_time:110811ms step_avg:88.08ms
step:1259/1680 train_time:110899ms step_avg:88.08ms
step:1260/1680 train_time:110987ms step_avg:88.08ms
step:1261/1680 train_time:111075ms step_avg:88.08ms
step:1262/1680 train_time:111165ms step_avg:88.09ms
step:1263/1680 train_time:111255ms step_avg:88.09ms
step:1264/1680 train_time:111347ms step_avg:88.09ms
step:1265/1680 train_time:111437ms step_avg:88.09ms
step:1266/1680 train_time:111526ms step_avg:88.09ms
step:1267/1680 train_time:111615ms step_avg:88.09ms
step:1268/1680 train_time:111703ms step_avg:88.09ms
step:1269/1680 train_time:111792ms step_avg:88.09ms
step:1270/1680 train_time:111880ms step_avg:88.09ms
step:1271/1680 train_time:111969ms step_avg:88.09ms
step:1272/1680 train_time:112057ms step_avg:88.09ms
step:1273/1680 train_time:112146ms step_avg:88.10ms
step:1274/1680 train_time:112237ms step_avg:88.10ms
step:1275/1680 train_time:112329ms step_avg:88.10ms
step:1276/1680 train_time:112419ms step_avg:88.10ms
step:1277/1680 train_time:112508ms step_avg:88.10ms
step:1278/1680 train_time:112597ms step_avg:88.10ms
step:1279/1680 train_time:112686ms step_avg:88.10ms
step:1280/1680 train_time:112775ms step_avg:88.11ms
step:1281/1680 train_time:112863ms step_avg:88.11ms
step:1282/1680 train_time:112951ms step_avg:88.11ms
step:1283/1680 train_time:113041ms step_avg:88.11ms
step:1284/1680 train_time:113131ms step_avg:88.11ms
step:1285/1680 train_time:113221ms step_avg:88.11ms
step:1286/1680 train_time:113312ms step_avg:88.11ms
step:1287/1680 train_time:113403ms step_avg:88.11ms
step:1288/1680 train_time:113491ms step_avg:88.11ms
step:1289/1680 train_time:113581ms step_avg:88.12ms
step:1290/1680 train_time:113670ms step_avg:88.12ms
step:1291/1680 train_time:113759ms step_avg:88.12ms
step:1292/1680 train_time:113847ms step_avg:88.12ms
step:1293/1680 train_time:113936ms step_avg:88.12ms
step:1294/1680 train_time:114024ms step_avg:88.12ms
step:1295/1680 train_time:114113ms step_avg:88.12ms
step:1296/1680 train_time:114202ms step_avg:88.12ms
step:1297/1680 train_time:114292ms step_avg:88.12ms
step:1298/1680 train_time:114381ms step_avg:88.12ms
step:1299/1680 train_time:114470ms step_avg:88.12ms
step:1300/1680 train_time:114560ms step_avg:88.12ms
step:1301/1680 train_time:114649ms step_avg:88.12ms
step:1302/1680 train_time:114739ms step_avg:88.13ms
step:1303/1680 train_time:114829ms step_avg:88.13ms
step:1304/1680 train_time:114918ms step_avg:88.13ms
step:1305/1680 train_time:115006ms step_avg:88.13ms
step:1306/1680 train_time:115095ms step_avg:88.13ms
step:1307/1680 train_time:115184ms step_avg:88.13ms
step:1308/1680 train_time:115275ms step_avg:88.13ms
step:1309/1680 train_time:115364ms step_avg:88.13ms
step:1310/1680 train_time:115454ms step_avg:88.13ms
step:1311/1680 train_time:115543ms step_avg:88.13ms
step:1312/1680 train_time:115632ms step_avg:88.13ms
step:1313/1680 train_time:115721ms step_avg:88.13ms
step:1314/1680 train_time:115810ms step_avg:88.14ms
step:1315/1680 train_time:115899ms step_avg:88.14ms
step:1316/1680 train_time:115988ms step_avg:88.14ms
step:1317/1680 train_time:116078ms step_avg:88.14ms
step:1318/1680 train_time:116167ms step_avg:88.14ms
step:1319/1680 train_time:116256ms step_avg:88.14ms
step:1320/1680 train_time:116345ms step_avg:88.14ms
step:1321/1680 train_time:116435ms step_avg:88.14ms
step:1322/1680 train_time:116525ms step_avg:88.14ms
step:1323/1680 train_time:116616ms step_avg:88.14ms
step:1324/1680 train_time:116705ms step_avg:88.15ms
step:1325/1680 train_time:116795ms step_avg:88.15ms
step:1326/1680 train_time:116882ms step_avg:88.15ms
step:1327/1680 train_time:116971ms step_avg:88.15ms
step:1328/1680 train_time:117060ms step_avg:88.15ms
step:1329/1680 train_time:117150ms step_avg:88.15ms
step:1330/1680 train_time:117239ms step_avg:88.15ms
step:1331/1680 train_time:117329ms step_avg:88.15ms
step:1332/1680 train_time:117419ms step_avg:88.15ms
step:1333/1680 train_time:117508ms step_avg:88.15ms
step:1334/1680 train_time:117598ms step_avg:88.15ms
step:1335/1680 train_time:117687ms step_avg:88.16ms
step:1336/1680 train_time:117776ms step_avg:88.16ms
step:1337/1680 train_time:117865ms step_avg:88.16ms
step:1338/1680 train_time:117955ms step_avg:88.16ms
step:1339/1680 train_time:118044ms step_avg:88.16ms
step:1340/1680 train_time:118133ms step_avg:88.16ms
step:1341/1680 train_time:118222ms step_avg:88.16ms
step:1342/1680 train_time:118311ms step_avg:88.16ms
step:1343/1680 train_time:118400ms step_avg:88.16ms
step:1344/1680 train_time:118490ms step_avg:88.16ms
step:1345/1680 train_time:118580ms step_avg:88.16ms
step:1346/1680 train_time:118670ms step_avg:88.17ms
step:1347/1680 train_time:118760ms step_avg:88.17ms
step:1348/1680 train_time:118848ms step_avg:88.17ms
step:1349/1680 train_time:118938ms step_avg:88.17ms
step:1350/1680 train_time:119028ms step_avg:88.17ms
step:1351/1680 train_time:119119ms step_avg:88.17ms
step:1352/1680 train_time:119208ms step_avg:88.17ms
step:1353/1680 train_time:119296ms step_avg:88.17ms
step:1354/1680 train_time:119387ms step_avg:88.17ms
step:1355/1680 train_time:119475ms step_avg:88.17ms
step:1356/1680 train_time:119565ms step_avg:88.17ms
step:1357/1680 train_time:119655ms step_avg:88.18ms
step:1358/1680 train_time:119744ms step_avg:88.18ms
step:1359/1680 train_time:119834ms step_avg:88.18ms
step:1360/1680 train_time:119923ms step_avg:88.18ms
step:1361/1680 train_time:120012ms step_avg:88.18ms
step:1362/1680 train_time:120101ms step_avg:88.18ms
step:1363/1680 train_time:120190ms step_avg:88.18ms
step:1364/1680 train_time:120279ms step_avg:88.18ms
step:1365/1680 train_time:120368ms step_avg:88.18ms
step:1366/1680 train_time:120457ms step_avg:88.18ms
step:1367/1680 train_time:120547ms step_avg:88.18ms
step:1368/1680 train_time:120636ms step_avg:88.18ms
step:1369/1680 train_time:120726ms step_avg:88.19ms
step:1370/1680 train_time:120815ms step_avg:88.19ms
step:1371/1680 train_time:120904ms step_avg:88.19ms
step:1372/1680 train_time:120993ms step_avg:88.19ms
step:1373/1680 train_time:121083ms step_avg:88.19ms
step:1374/1680 train_time:121172ms step_avg:88.19ms
step:1375/1680 train_time:121262ms step_avg:88.19ms
step:1375/1680 val_loss:3.3393 train_time:121352ms step_avg:88.26ms
step:1376/1680 train_time:121371ms step_avg:88.21ms
step:1377/1680 train_time:121443ms step_avg:88.19ms
step:1378/1680 train_time:121535ms step_avg:88.20ms
step:1379/1680 train_time:121624ms step_avg:88.20ms
step:1380/1680 train_time:121712ms step_avg:88.20ms
step:1381/1680 train_time:121801ms step_avg:88.20ms
step:1382/1680 train_time:121889ms step_avg:88.20ms
step:1383/1680 train_time:121977ms step_avg:88.20ms
step:1384/1680 train_time:122065ms step_avg:88.20ms
step:1385/1680 train_time:122156ms step_avg:88.20ms
step:1386/1680 train_time:122245ms step_avg:88.20ms
step:1387/1680 train_time:122335ms step_avg:88.20ms
step:1388/1680 train_time:122426ms step_avg:88.20ms
step:1389/1680 train_time:122516ms step_avg:88.20ms
step:1390/1680 train_time:122608ms step_avg:88.21ms
step:1391/1680 train_time:122697ms step_avg:88.21ms
step:1392/1680 train_time:122786ms step_avg:88.21ms
step:1393/1680 train_time:122874ms step_avg:88.21ms
step:1394/1680 train_time:122963ms step_avg:88.21ms
step:1395/1680 train_time:123051ms step_avg:88.21ms
step:1396/1680 train_time:123140ms step_avg:88.21ms
step:1397/1680 train_time:123230ms step_avg:88.21ms
step:1398/1680 train_time:123319ms step_avg:88.21ms
step:1399/1680 train_time:123408ms step_avg:88.21ms
step:1400/1680 train_time:123498ms step_avg:88.21ms
step:1401/1680 train_time:123589ms step_avg:88.21ms
step:1402/1680 train_time:123678ms step_avg:88.22ms
step:1403/1680 train_time:123768ms step_avg:88.22ms
step:1404/1680 train_time:123856ms step_avg:88.22ms
step:1405/1680 train_time:123944ms step_avg:88.22ms
step:1406/1680 train_time:124033ms step_avg:88.22ms
step:1407/1680 train_time:124121ms step_avg:88.22ms
step:1408/1680 train_time:124210ms step_avg:88.22ms
step:1409/1680 train_time:124299ms step_avg:88.22ms
step:1410/1680 train_time:124390ms step_avg:88.22ms
step:1411/1680 train_time:124481ms step_avg:88.22ms
step:1412/1680 train_time:124573ms step_avg:88.22ms
step:1413/1680 train_time:124663ms step_avg:88.23ms
step:1414/1680 train_time:124753ms step_avg:88.23ms
step:1415/1680 train_time:124842ms step_avg:88.23ms
step:1416/1680 train_time:124931ms step_avg:88.23ms
step:1417/1680 train_time:125020ms step_avg:88.23ms
step:1418/1680 train_time:125109ms step_avg:88.23ms
step:1419/1680 train_time:125198ms step_avg:88.23ms
step:1420/1680 train_time:125288ms step_avg:88.23ms
step:1421/1680 train_time:125378ms step_avg:88.23ms
step:1422/1680 train_time:125468ms step_avg:88.23ms
step:1423/1680 train_time:125557ms step_avg:88.23ms
step:1424/1680 train_time:125647ms step_avg:88.24ms
step:1425/1680 train_time:125736ms step_avg:88.24ms
step:1426/1680 train_time:125825ms step_avg:88.24ms
step:1427/1680 train_time:125914ms step_avg:88.24ms
step:1428/1680 train_time:126004ms step_avg:88.24ms
step:1429/1680 train_time:126094ms step_avg:88.24ms
step:1430/1680 train_time:126183ms step_avg:88.24ms
step:1431/1680 train_time:126273ms step_avg:88.24ms
step:1432/1680 train_time:126362ms step_avg:88.24ms
step:1433/1680 train_time:126451ms step_avg:88.24ms
step:1434/1680 train_time:126540ms step_avg:88.24ms
step:1435/1680 train_time:126630ms step_avg:88.24ms
step:1436/1680 train_time:126720ms step_avg:88.24ms
step:1437/1680 train_time:126809ms step_avg:88.25ms
step:1438/1680 train_time:126898ms step_avg:88.25ms
step:1439/1680 train_time:126988ms step_avg:88.25ms
step:1440/1680 train_time:127077ms step_avg:88.25ms
step:1441/1680 train_time:127167ms step_avg:88.25ms
step:1442/1680 train_time:127256ms step_avg:88.25ms
step:1443/1680 train_time:127344ms step_avg:88.25ms
step:1444/1680 train_time:127434ms step_avg:88.25ms
step:1445/1680 train_time:127523ms step_avg:88.25ms
step:1446/1680 train_time:127613ms step_avg:88.25ms
step:1447/1680 train_time:127703ms step_avg:88.25ms
step:1448/1680 train_time:127793ms step_avg:88.26ms
step:1449/1680 train_time:127883ms step_avg:88.26ms
step:1450/1680 train_time:127972ms step_avg:88.26ms
step:1451/1680 train_time:128060ms step_avg:88.26ms
step:1452/1680 train_time:128150ms step_avg:88.26ms
step:1453/1680 train_time:128238ms step_avg:88.26ms
step:1454/1680 train_time:128327ms step_avg:88.26ms
step:1455/1680 train_time:128417ms step_avg:88.26ms
step:1456/1680 train_time:128506ms step_avg:88.26ms
step:1457/1680 train_time:128595ms step_avg:88.26ms
step:1458/1680 train_time:128685ms step_avg:88.26ms
step:1459/1680 train_time:128775ms step_avg:88.26ms
step:1460/1680 train_time:128864ms step_avg:88.26ms
step:1461/1680 train_time:128954ms step_avg:88.26ms
step:1462/1680 train_time:129044ms step_avg:88.27ms
step:1463/1680 train_time:129133ms step_avg:88.27ms
step:1464/1680 train_time:129222ms step_avg:88.27ms
step:1465/1680 train_time:129312ms step_avg:88.27ms
step:1466/1680 train_time:129401ms step_avg:88.27ms
step:1467/1680 train_time:129490ms step_avg:88.27ms
step:1468/1680 train_time:129580ms step_avg:88.27ms
step:1469/1680 train_time:129670ms step_avg:88.27ms
step:1470/1680 train_time:129758ms step_avg:88.27ms
step:1471/1680 train_time:129847ms step_avg:88.27ms
step:1472/1680 train_time:129936ms step_avg:88.27ms
step:1473/1680 train_time:130025ms step_avg:88.27ms
step:1474/1680 train_time:130114ms step_avg:88.27ms
step:1475/1680 train_time:130204ms step_avg:88.27ms
step:1476/1680 train_time:130293ms step_avg:88.27ms
step:1477/1680 train_time:130383ms step_avg:88.28ms
step:1478/1680 train_time:130472ms step_avg:88.28ms
step:1479/1680 train_time:130562ms step_avg:88.28ms
step:1480/1680 train_time:130651ms step_avg:88.28ms
step:1481/1680 train_time:130740ms step_avg:88.28ms
step:1482/1680 train_time:130828ms step_avg:88.28ms
step:1483/1680 train_time:130918ms step_avg:88.28ms
step:1484/1680 train_time:131007ms step_avg:88.28ms
step:1485/1680 train_time:131096ms step_avg:88.28ms
step:1486/1680 train_time:131185ms step_avg:88.28ms
step:1487/1680 train_time:131274ms step_avg:88.28ms
step:1488/1680 train_time:131364ms step_avg:88.28ms
step:1489/1680 train_time:131454ms step_avg:88.28ms
step:1490/1680 train_time:131543ms step_avg:88.28ms
step:1491/1680 train_time:131633ms step_avg:88.29ms
step:1492/1680 train_time:131723ms step_avg:88.29ms
step:1493/1680 train_time:131813ms step_avg:88.29ms
step:1494/1680 train_time:131903ms step_avg:88.29ms
step:1495/1680 train_time:131992ms step_avg:88.29ms
step:1496/1680 train_time:132080ms step_avg:88.29ms
step:1497/1680 train_time:132169ms step_avg:88.29ms
step:1498/1680 train_time:132258ms step_avg:88.29ms
step:1499/1680 train_time:132348ms step_avg:88.29ms
step:1500/1680 train_time:132437ms step_avg:88.29ms
step:1500/1680 val_loss:3.3097 train_time:132529ms step_avg:88.35ms
step:1501/1680 train_time:132548ms step_avg:88.31ms
step:1502/1680 train_time:132623ms step_avg:88.30ms
step:1503/1680 train_time:132717ms step_avg:88.30ms
step:1504/1680 train_time:132807ms step_avg:88.30ms
step:1505/1680 train_time:132895ms step_avg:88.30ms
step:1506/1680 train_time:132983ms step_avg:88.30ms
step:1507/1680 train_time:133071ms step_avg:88.30ms
step:1508/1680 train_time:133159ms step_avg:88.30ms
step:1509/1680 train_time:133247ms step_avg:88.30ms
step:1510/1680 train_time:133335ms step_avg:88.30ms
step:1511/1680 train_time:133424ms step_avg:88.30ms
step:1512/1680 train_time:133515ms step_avg:88.30ms
step:1513/1680 train_time:133605ms step_avg:88.30ms
step:1514/1680 train_time:133698ms step_avg:88.31ms
step:1515/1680 train_time:133788ms step_avg:88.31ms
step:1516/1680 train_time:133877ms step_avg:88.31ms
step:1517/1680 train_time:133966ms step_avg:88.31ms
step:1518/1680 train_time:134055ms step_avg:88.31ms
step:1519/1680 train_time:134143ms step_avg:88.31ms
step:1520/1680 train_time:134231ms step_avg:88.31ms
step:1521/1680 train_time:134319ms step_avg:88.31ms
step:1522/1680 train_time:134408ms step_avg:88.31ms
step:1523/1680 train_time:134498ms step_avg:88.31ms
step:1524/1680 train_time:134588ms step_avg:88.31ms
step:1525/1680 train_time:134680ms step_avg:88.31ms
step:1526/1680 train_time:134770ms step_avg:88.32ms
step:1527/1680 train_time:134859ms step_avg:88.32ms
step:1528/1680 train_time:134948ms step_avg:88.32ms
step:1529/1680 train_time:135037ms step_avg:88.32ms
step:1530/1680 train_time:135125ms step_avg:88.32ms
step:1531/1680 train_time:135214ms step_avg:88.32ms
step:1532/1680 train_time:135302ms step_avg:88.32ms
step:1533/1680 train_time:135391ms step_avg:88.32ms
step:1534/1680 train_time:135481ms step_avg:88.32ms
step:1535/1680 train_time:135571ms step_avg:88.32ms
step:1536/1680 train_time:135661ms step_avg:88.32ms
step:1537/1680 train_time:135752ms step_avg:88.32ms
step:1538/1680 train_time:135842ms step_avg:88.32ms
step:1539/1680 train_time:135932ms step_avg:88.32ms
step:1540/1680 train_time:136021ms step_avg:88.33ms
step:1541/1680 train_time:136110ms step_avg:88.33ms
step:1542/1680 train_time:136198ms step_avg:88.33ms
step:1543/1680 train_time:136286ms step_avg:88.33ms
step:1544/1680 train_time:136375ms step_avg:88.33ms
step:1545/1680 train_time:136464ms step_avg:88.33ms
step:1546/1680 train_time:136554ms step_avg:88.33ms
step:1547/1680 train_time:136643ms step_avg:88.33ms
step:1548/1680 train_time:136733ms step_avg:88.33ms
step:1549/1680 train_time:136822ms step_avg:88.33ms
step:1550/1680 train_time:136911ms step_avg:88.33ms
step:1551/1680 train_time:137000ms step_avg:88.33ms
step:1552/1680 train_time:137088ms step_avg:88.33ms
step:1553/1680 train_time:137177ms step_avg:88.33ms
step:1554/1680 train_time:137265ms step_avg:88.33ms
step:1555/1680 train_time:137354ms step_avg:88.33ms
step:1556/1680 train_time:137443ms step_avg:88.33ms
step:1557/1680 train_time:137533ms step_avg:88.33ms
step:1558/1680 train_time:137623ms step_avg:88.33ms
step:1559/1680 train_time:137712ms step_avg:88.33ms
step:1560/1680 train_time:137801ms step_avg:88.33ms
step:1561/1680 train_time:137892ms step_avg:88.34ms
step:1562/1680 train_time:137981ms step_avg:88.34ms
step:1563/1680 train_time:138069ms step_avg:88.34ms
step:1564/1680 train_time:138158ms step_avg:88.34ms
step:1565/1680 train_time:138247ms step_avg:88.34ms
step:1566/1680 train_time:138336ms step_avg:88.34ms
step:1567/1680 train_time:138425ms step_avg:88.34ms
step:1568/1680 train_time:138514ms step_avg:88.34ms
step:1569/1680 train_time:138603ms step_avg:88.34ms
step:1570/1680 train_time:138692ms step_avg:88.34ms
step:1571/1680 train_time:138782ms step_avg:88.34ms
step:1572/1680 train_time:138870ms step_avg:88.34ms
step:1573/1680 train_time:138960ms step_avg:88.34ms
step:1574/1680 train_time:139049ms step_avg:88.34ms
step:1575/1680 train_time:139139ms step_avg:88.34ms
step:1576/1680 train_time:139227ms step_avg:88.34ms
step:1577/1680 train_time:139317ms step_avg:88.34ms
step:1578/1680 train_time:139405ms step_avg:88.34ms
step:1579/1680 train_time:139494ms step_avg:88.34ms
step:1580/1680 train_time:139584ms step_avg:88.34ms
step:1581/1680 train_time:139674ms step_avg:88.35ms
step:1582/1680 train_time:139763ms step_avg:88.35ms
step:1583/1680 train_time:139852ms step_avg:88.35ms
step:1584/1680 train_time:139942ms step_avg:88.35ms
step:1585/1680 train_time:140032ms step_avg:88.35ms
step:1586/1680 train_time:140121ms step_avg:88.35ms
step:1587/1680 train_time:140210ms step_avg:88.35ms
step:1588/1680 train_time:140299ms step_avg:88.35ms
step:1589/1680 train_time:140388ms step_avg:88.35ms
step:1590/1680 train_time:140477ms step_avg:88.35ms
step:1591/1680 train_time:140567ms step_avg:88.35ms
step:1592/1680 train_time:140657ms step_avg:88.35ms
step:1593/1680 train_time:140746ms step_avg:88.35ms
step:1594/1680 train_time:140836ms step_avg:88.35ms
step:1595/1680 train_time:140926ms step_avg:88.35ms
step:1596/1680 train_time:141015ms step_avg:88.35ms
step:1597/1680 train_time:141104ms step_avg:88.36ms
step:1598/1680 train_time:141193ms step_avg:88.36ms
step:1599/1680 train_time:141282ms step_avg:88.36ms
step:1600/1680 train_time:141371ms step_avg:88.36ms
step:1601/1680 train_time:141461ms step_avg:88.36ms
step:1602/1680 train_time:141550ms step_avg:88.36ms
step:1603/1680 train_time:141640ms step_avg:88.36ms
step:1604/1680 train_time:141729ms step_avg:88.36ms
step:1605/1680 train_time:141819ms step_avg:88.36ms
step:1606/1680 train_time:141909ms step_avg:88.36ms
step:1607/1680 train_time:141998ms step_avg:88.36ms
step:1608/1680 train_time:142087ms step_avg:88.36ms
step:1609/1680 train_time:142177ms step_avg:88.36ms
step:1610/1680 train_time:142266ms step_avg:88.36ms
step:1611/1680 train_time:142355ms step_avg:88.36ms
step:1612/1680 train_time:142444ms step_avg:88.36ms
step:1613/1680 train_time:142534ms step_avg:88.37ms
step:1614/1680 train_time:142624ms step_avg:88.37ms
step:1615/1680 train_time:142714ms step_avg:88.37ms
step:1616/1680 train_time:142802ms step_avg:88.37ms
step:1617/1680 train_time:142892ms step_avg:88.37ms
step:1618/1680 train_time:142983ms step_avg:88.37ms
step:1619/1680 train_time:143072ms step_avg:88.37ms
step:1620/1680 train_time:143161ms step_avg:88.37ms
step:1621/1680 train_time:143251ms step_avg:88.37ms
step:1622/1680 train_time:143340ms step_avg:88.37ms
step:1623/1680 train_time:143430ms step_avg:88.37ms
step:1624/1680 train_time:143520ms step_avg:88.37ms
step:1625/1680 train_time:143609ms step_avg:88.38ms
step:1625/1680 val_loss:3.2859 train_time:143700ms step_avg:88.43ms
step:1626/1680 train_time:143719ms step_avg:88.39ms
step:1627/1680 train_time:143792ms step_avg:88.38ms
step:1628/1680 train_time:143884ms step_avg:88.38ms
step:1629/1680 train_time:143975ms step_avg:88.38ms
step:1630/1680 train_time:144064ms step_avg:88.38ms
step:1631/1680 train_time:144152ms step_avg:88.38ms
step:1632/1680 train_time:144240ms step_avg:88.38ms
step:1633/1680 train_time:144328ms step_avg:88.38ms
step:1634/1680 train_time:144416ms step_avg:88.38ms
step:1635/1680 train_time:144504ms step_avg:88.38ms
step:1636/1680 train_time:144593ms step_avg:88.38ms
step:1637/1680 train_time:144684ms step_avg:88.38ms
step:1638/1680 train_time:144776ms step_avg:88.39ms
step:1639/1680 train_time:144867ms step_avg:88.39ms
step:1640/1680 train_time:144958ms step_avg:88.39ms
step:1641/1680 train_time:145048ms step_avg:88.39ms
step:1642/1680 train_time:145137ms step_avg:88.39ms
step:1643/1680 train_time:145226ms step_avg:88.39ms
step:1644/1680 train_time:145314ms step_avg:88.39ms
step:1645/1680 train_time:145402ms step_avg:88.39ms
step:1646/1680 train_time:145491ms step_avg:88.39ms
step:1647/1680 train_time:145580ms step_avg:88.39ms
step:1648/1680 train_time:145670ms step_avg:88.39ms
step:1649/1680 train_time:145760ms step_avg:88.39ms
step:1650/1680 train_time:145850ms step_avg:88.39ms
step:1651/1680 train_time:145942ms step_avg:88.40ms
step:1652/1680 train_time:146031ms step_avg:88.40ms
step:1653/1680 train_time:146120ms step_avg:88.40ms
step:1654/1680 train_time:146208ms step_avg:88.40ms
step:1655/1680 train_time:146297ms step_avg:88.40ms
step:1656/1680 train_time:146386ms step_avg:88.40ms
step:1657/1680 train_time:146476ms step_avg:88.40ms
step:1658/1680 train_time:146565ms step_avg:88.40ms
step:1659/1680 train_time:146655ms step_avg:88.40ms
step:1660/1680 train_time:146744ms step_avg:88.40ms
step:1661/1680 train_time:146834ms step_avg:88.40ms
step:1662/1680 train_time:146924ms step_avg:88.40ms
step:1663/1680 train_time:147014ms step_avg:88.40ms
step:1664/1680 train_time:147103ms step_avg:88.40ms
step:1665/1680 train_time:147192ms step_avg:88.40ms
step:1666/1680 train_time:147281ms step_avg:88.40ms
step:1667/1680 train_time:147369ms step_avg:88.40ms
step:1668/1680 train_time:147458ms step_avg:88.40ms
step:1669/1680 train_time:147547ms step_avg:88.40ms
step:1670/1680 train_time:147636ms step_avg:88.40ms
step:1671/1680 train_time:147726ms step_avg:88.41ms
step:1672/1680 train_time:147816ms step_avg:88.41ms
step:1673/1680 train_time:147906ms step_avg:88.41ms
step:1674/1680 train_time:147996ms step_avg:88.41ms
step:1675/1680 train_time:148086ms step_avg:88.41ms
step:1676/1680 train_time:148176ms step_avg:88.41ms
step:1677/1680 train_time:148265ms step_avg:88.41ms
step:1678/1680 train_time:148354ms step_avg:88.41ms
step:1679/1680 train_time:148442ms step_avg:88.41ms
step:1680/1680 train_time:148530ms step_avg:88.41ms
step:1680/1680 val_loss:3.2752 train_time:148621ms step_avg:88.47ms
peak memory allocated: 30760 MiB reserved: 45994 MiB

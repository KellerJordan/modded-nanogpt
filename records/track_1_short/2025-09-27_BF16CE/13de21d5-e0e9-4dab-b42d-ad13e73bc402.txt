import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:12:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0            117W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    153204      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    153205      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    153206      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    153207      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    153208      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    153209      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    153210      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    153211      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    153205      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    153206      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    153207      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    153208      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    153209      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    153210      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    153211      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:150ms step_avg:149.83ms
step:2/1680 train_time:170ms step_avg:85.10ms
step:3/1680 train_time:233ms step_avg:77.80ms
step:4/1680 train_time:319ms step_avg:79.70ms
step:5/1680 train_time:405ms step_avg:80.94ms
step:6/1680 train_time:491ms step_avg:81.78ms
step:7/1680 train_time:577ms step_avg:82.44ms
step:8/1680 train_time:664ms step_avg:82.96ms
step:9/1680 train_time:750ms step_avg:83.35ms
step:10/1680 train_time:836ms step_avg:83.64ms
step:11/1680 train_time:923ms step_avg:83.87ms
step:12/1680 train_time:1011ms step_avg:84.22ms
step:13/1680 train_time:1101ms step_avg:84.71ms
step:14/1680 train_time:1192ms step_avg:85.13ms
step:15/1680 train_time:1280ms step_avg:85.33ms
step:16/1680 train_time:1368ms step_avg:85.49ms
step:17/1680 train_time:1454ms step_avg:85.54ms
step:18/1680 train_time:1541ms step_avg:85.63ms
step:19/1680 train_time:1628ms step_avg:85.70ms
step:20/1680 train_time:1715ms step_avg:85.76ms
step:21/1680 train_time:1802ms step_avg:85.80ms
step:22/1680 train_time:1889ms step_avg:85.88ms
step:23/1680 train_time:1976ms step_avg:85.92ms
step:24/1680 train_time:2064ms step_avg:86.01ms
step:25/1680 train_time:2154ms step_avg:86.14ms
step:26/1680 train_time:2242ms step_avg:86.24ms
step:27/1680 train_time:2331ms step_avg:86.34ms
step:28/1680 train_time:2419ms step_avg:86.38ms
step:29/1680 train_time:2506ms step_avg:86.40ms
step:30/1680 train_time:2593ms step_avg:86.42ms
step:31/1680 train_time:2680ms step_avg:86.46ms
step:32/1680 train_time:2767ms step_avg:86.46ms
step:33/1680 train_time:2853ms step_avg:86.46ms
step:34/1680 train_time:2941ms step_avg:86.50ms
step:35/1680 train_time:3030ms step_avg:86.56ms
step:36/1680 train_time:3118ms step_avg:86.61ms
step:37/1680 train_time:3206ms step_avg:86.66ms
step:38/1680 train_time:3294ms step_avg:86.70ms
step:39/1680 train_time:3382ms step_avg:86.73ms
step:40/1680 train_time:3469ms step_avg:86.74ms
step:41/1680 train_time:3556ms step_avg:86.74ms
step:42/1680 train_time:3643ms step_avg:86.74ms
step:43/1680 train_time:3730ms step_avg:86.75ms
step:44/1680 train_time:3817ms step_avg:86.75ms
step:45/1680 train_time:3904ms step_avg:86.75ms
step:46/1680 train_time:3991ms step_avg:86.77ms
step:47/1680 train_time:4080ms step_avg:86.80ms
step:48/1680 train_time:4168ms step_avg:86.83ms
step:49/1680 train_time:4256ms step_avg:86.85ms
step:50/1680 train_time:4344ms step_avg:86.87ms
step:51/1680 train_time:4431ms step_avg:86.88ms
step:52/1680 train_time:4518ms step_avg:86.88ms
step:53/1680 train_time:4605ms step_avg:86.90ms
step:54/1680 train_time:4693ms step_avg:86.90ms
step:55/1680 train_time:4779ms step_avg:86.90ms
step:56/1680 train_time:4866ms step_avg:86.89ms
step:57/1680 train_time:4953ms step_avg:86.90ms
step:58/1680 train_time:5041ms step_avg:86.91ms
step:59/1680 train_time:5129ms step_avg:86.94ms
step:60/1680 train_time:5217ms step_avg:86.95ms
step:61/1680 train_time:5305ms step_avg:86.96ms
step:62/1680 train_time:5393ms step_avg:86.98ms
step:63/1680 train_time:5480ms step_avg:86.99ms
step:64/1680 train_time:5567ms step_avg:86.98ms
step:65/1680 train_time:5654ms step_avg:86.99ms
step:66/1680 train_time:5742ms step_avg:87.00ms
step:67/1680 train_time:5829ms step_avg:87.00ms
step:68/1680 train_time:5916ms step_avg:87.00ms
step:69/1680 train_time:6003ms step_avg:87.00ms
step:70/1680 train_time:6091ms step_avg:87.01ms
step:71/1680 train_time:6179ms step_avg:87.03ms
step:72/1680 train_time:6267ms step_avg:87.04ms
step:73/1680 train_time:6355ms step_avg:87.05ms
step:74/1680 train_time:6442ms step_avg:87.06ms
step:75/1680 train_time:6529ms step_avg:87.06ms
step:76/1680 train_time:6616ms step_avg:87.06ms
step:77/1680 train_time:6703ms step_avg:87.06ms
step:78/1680 train_time:6791ms step_avg:87.06ms
step:79/1680 train_time:6878ms step_avg:87.07ms
step:80/1680 train_time:6966ms step_avg:87.07ms
step:81/1680 train_time:7053ms step_avg:87.08ms
step:82/1680 train_time:7141ms step_avg:87.08ms
step:83/1680 train_time:7228ms step_avg:87.09ms
step:84/1680 train_time:7315ms step_avg:87.09ms
step:85/1680 train_time:7403ms step_avg:87.09ms
step:86/1680 train_time:7490ms step_avg:87.09ms
step:87/1680 train_time:7577ms step_avg:87.09ms
step:88/1680 train_time:7665ms step_avg:87.10ms
step:89/1680 train_time:7752ms step_avg:87.10ms
step:90/1680 train_time:7840ms step_avg:87.11ms
step:91/1680 train_time:7927ms step_avg:87.11ms
step:92/1680 train_time:8014ms step_avg:87.11ms
step:93/1680 train_time:8101ms step_avg:87.11ms
step:94/1680 train_time:8189ms step_avg:87.12ms
step:95/1680 train_time:8276ms step_avg:87.12ms
step:96/1680 train_time:8364ms step_avg:87.12ms
step:97/1680 train_time:8452ms step_avg:87.13ms
step:98/1680 train_time:8539ms step_avg:87.13ms
step:99/1680 train_time:8626ms step_avg:87.13ms
step:100/1680 train_time:8713ms step_avg:87.13ms
step:101/1680 train_time:8800ms step_avg:87.13ms
step:102/1680 train_time:8888ms step_avg:87.14ms
step:103/1680 train_time:8974ms step_avg:87.13ms
step:104/1680 train_time:9062ms step_avg:87.14ms
step:105/1680 train_time:9151ms step_avg:87.15ms
step:106/1680 train_time:9238ms step_avg:87.15ms
step:107/1680 train_time:9326ms step_avg:87.16ms
step:108/1680 train_time:9413ms step_avg:87.15ms
step:109/1680 train_time:9500ms step_avg:87.16ms
step:110/1680 train_time:9588ms step_avg:87.17ms
step:111/1680 train_time:9675ms step_avg:87.16ms
step:112/1680 train_time:9762ms step_avg:87.16ms
step:113/1680 train_time:9849ms step_avg:87.16ms
step:114/1680 train_time:9936ms step_avg:87.16ms
step:115/1680 train_time:10023ms step_avg:87.16ms
step:116/1680 train_time:10110ms step_avg:87.16ms
step:117/1680 train_time:10197ms step_avg:87.16ms
step:118/1680 train_time:10285ms step_avg:87.16ms
step:119/1680 train_time:10373ms step_avg:87.17ms
step:120/1680 train_time:10460ms step_avg:87.17ms
step:121/1680 train_time:10548ms step_avg:87.17ms
step:122/1680 train_time:10635ms step_avg:87.17ms
step:123/1680 train_time:10722ms step_avg:87.17ms
step:124/1680 train_time:10809ms step_avg:87.17ms
step:125/1680 train_time:10896ms step_avg:87.17ms
step:125/1680 val_loss:4.3221 train_time:10985ms step_avg:87.88ms
step:126/1680 train_time:11004ms step_avg:87.34ms
step:127/1680 train_time:11075ms step_avg:87.21ms
step:128/1680 train_time:11172ms step_avg:87.28ms
step:129/1680 train_time:11263ms step_avg:87.31ms
step:130/1680 train_time:11350ms step_avg:87.31ms
step:131/1680 train_time:11437ms step_avg:87.30ms
step:132/1680 train_time:11523ms step_avg:87.29ms
step:133/1680 train_time:11609ms step_avg:87.29ms
step:134/1680 train_time:11695ms step_avg:87.28ms
step:135/1680 train_time:11781ms step_avg:87.27ms
step:136/1680 train_time:11867ms step_avg:87.26ms
step:137/1680 train_time:11955ms step_avg:87.26ms
step:138/1680 train_time:12043ms step_avg:87.27ms
step:139/1680 train_time:12132ms step_avg:87.28ms
step:140/1680 train_time:12222ms step_avg:87.30ms
step:141/1680 train_time:12310ms step_avg:87.30ms
step:142/1680 train_time:12397ms step_avg:87.30ms
step:143/1680 train_time:12484ms step_avg:87.30ms
step:144/1680 train_time:12571ms step_avg:87.30ms
step:145/1680 train_time:12657ms step_avg:87.29ms
step:146/1680 train_time:12744ms step_avg:87.29ms
step:147/1680 train_time:12830ms step_avg:87.28ms
step:148/1680 train_time:12917ms step_avg:87.27ms
step:149/1680 train_time:13004ms step_avg:87.27ms
step:150/1680 train_time:13092ms step_avg:87.28ms
step:151/1680 train_time:13181ms step_avg:87.29ms
step:152/1680 train_time:13269ms step_avg:87.30ms
step:153/1680 train_time:13357ms step_avg:87.30ms
step:154/1680 train_time:13445ms step_avg:87.31ms
step:155/1680 train_time:13532ms step_avg:87.30ms
step:156/1680 train_time:13620ms step_avg:87.30ms
step:157/1680 train_time:13707ms step_avg:87.30ms
step:158/1680 train_time:13793ms step_avg:87.30ms
step:159/1680 train_time:13880ms step_avg:87.30ms
step:160/1680 train_time:13967ms step_avg:87.29ms
step:161/1680 train_time:14055ms step_avg:87.30ms
step:162/1680 train_time:14142ms step_avg:87.30ms
step:163/1680 train_time:14230ms step_avg:87.30ms
step:164/1680 train_time:14318ms step_avg:87.30ms
step:165/1680 train_time:14405ms step_avg:87.30ms
step:166/1680 train_time:14492ms step_avg:87.30ms
step:167/1680 train_time:14579ms step_avg:87.30ms
step:168/1680 train_time:14666ms step_avg:87.30ms
step:169/1680 train_time:14753ms step_avg:87.29ms
step:170/1680 train_time:14840ms step_avg:87.29ms
step:171/1680 train_time:14927ms step_avg:87.29ms
step:172/1680 train_time:15014ms step_avg:87.29ms
step:173/1680 train_time:15101ms step_avg:87.29ms
step:174/1680 train_time:15189ms step_avg:87.29ms
step:175/1680 train_time:15276ms step_avg:87.29ms
step:176/1680 train_time:15364ms step_avg:87.30ms
step:177/1680 train_time:15452ms step_avg:87.30ms
step:178/1680 train_time:15539ms step_avg:87.30ms
step:179/1680 train_time:15626ms step_avg:87.30ms
step:180/1680 train_time:15713ms step_avg:87.29ms
step:181/1680 train_time:15800ms step_avg:87.29ms
step:182/1680 train_time:15887ms step_avg:87.29ms
step:183/1680 train_time:15973ms step_avg:87.29ms
step:184/1680 train_time:16060ms step_avg:87.28ms
step:185/1680 train_time:16148ms step_avg:87.29ms
step:186/1680 train_time:16235ms step_avg:87.28ms
step:187/1680 train_time:16322ms step_avg:87.28ms
step:188/1680 train_time:16409ms step_avg:87.28ms
step:189/1680 train_time:16497ms step_avg:87.29ms
step:190/1680 train_time:16585ms step_avg:87.29ms
step:191/1680 train_time:16672ms step_avg:87.29ms
step:192/1680 train_time:16760ms step_avg:87.29ms
step:193/1680 train_time:16847ms step_avg:87.29ms
step:194/1680 train_time:16934ms step_avg:87.29ms
step:195/1680 train_time:17021ms step_avg:87.29ms
step:196/1680 train_time:17108ms step_avg:87.28ms
step:197/1680 train_time:17195ms step_avg:87.29ms
step:198/1680 train_time:17282ms step_avg:87.28ms
step:199/1680 train_time:17369ms step_avg:87.28ms
step:200/1680 train_time:17457ms step_avg:87.28ms
step:201/1680 train_time:17544ms step_avg:87.29ms
step:202/1680 train_time:17631ms step_avg:87.28ms
step:203/1680 train_time:17719ms step_avg:87.29ms
step:204/1680 train_time:17807ms step_avg:87.29ms
step:205/1680 train_time:17894ms step_avg:87.29ms
step:206/1680 train_time:17981ms step_avg:87.29ms
step:207/1680 train_time:18068ms step_avg:87.28ms
step:208/1680 train_time:18155ms step_avg:87.28ms
step:209/1680 train_time:18242ms step_avg:87.28ms
step:210/1680 train_time:18330ms step_avg:87.28ms
step:211/1680 train_time:18417ms step_avg:87.28ms
step:212/1680 train_time:18505ms step_avg:87.29ms
step:213/1680 train_time:18593ms step_avg:87.29ms
step:214/1680 train_time:18681ms step_avg:87.29ms
step:215/1680 train_time:18768ms step_avg:87.29ms
step:216/1680 train_time:18854ms step_avg:87.29ms
step:217/1680 train_time:18942ms step_avg:87.29ms
step:218/1680 train_time:19029ms step_avg:87.29ms
step:219/1680 train_time:19116ms step_avg:87.29ms
step:220/1680 train_time:19204ms step_avg:87.29ms
step:221/1680 train_time:19291ms step_avg:87.29ms
step:222/1680 train_time:19378ms step_avg:87.29ms
step:223/1680 train_time:19465ms step_avg:87.29ms
step:224/1680 train_time:19553ms step_avg:87.29ms
step:225/1680 train_time:19641ms step_avg:87.29ms
step:226/1680 train_time:19728ms step_avg:87.29ms
step:227/1680 train_time:19815ms step_avg:87.29ms
step:228/1680 train_time:19902ms step_avg:87.29ms
step:229/1680 train_time:19989ms step_avg:87.29ms
step:230/1680 train_time:20077ms step_avg:87.29ms
step:231/1680 train_time:20164ms step_avg:87.29ms
step:232/1680 train_time:20251ms step_avg:87.29ms
step:233/1680 train_time:20338ms step_avg:87.29ms
step:234/1680 train_time:20425ms step_avg:87.29ms
step:235/1680 train_time:20512ms step_avg:87.28ms
step:236/1680 train_time:20599ms step_avg:87.29ms
step:237/1680 train_time:20687ms step_avg:87.29ms
step:238/1680 train_time:20774ms step_avg:87.29ms
step:239/1680 train_time:20861ms step_avg:87.28ms
step:240/1680 train_time:20948ms step_avg:87.28ms
step:241/1680 train_time:21035ms step_avg:87.28ms
step:242/1680 train_time:21121ms step_avg:87.28ms
step:243/1680 train_time:21208ms step_avg:87.28ms
step:244/1680 train_time:21295ms step_avg:87.28ms
step:245/1680 train_time:21382ms step_avg:87.27ms
step:246/1680 train_time:21469ms step_avg:87.27ms
step:247/1680 train_time:21557ms step_avg:87.27ms
step:248/1680 train_time:21645ms step_avg:87.28ms
step:249/1680 train_time:21732ms step_avg:87.28ms
step:250/1680 train_time:21819ms step_avg:87.28ms
step:250/1680 val_loss:3.9705 train_time:21908ms step_avg:87.63ms
step:251/1680 train_time:21927ms step_avg:87.36ms
step:252/1680 train_time:21998ms step_avg:87.29ms
step:253/1680 train_time:22090ms step_avg:87.31ms
step:254/1680 train_time:22179ms step_avg:87.32ms
step:255/1680 train_time:22266ms step_avg:87.32ms
step:256/1680 train_time:22352ms step_avg:87.31ms
step:257/1680 train_time:22440ms step_avg:87.31ms
step:258/1680 train_time:22526ms step_avg:87.31ms
step:259/1680 train_time:22612ms step_avg:87.30ms
step:260/1680 train_time:22698ms step_avg:87.30ms
step:261/1680 train_time:22784ms step_avg:87.29ms
step:262/1680 train_time:22870ms step_avg:87.29ms
step:263/1680 train_time:22958ms step_avg:87.29ms
step:264/1680 train_time:23048ms step_avg:87.30ms
step:265/1680 train_time:23136ms step_avg:87.31ms
step:266/1680 train_time:23226ms step_avg:87.31ms
step:267/1680 train_time:23313ms step_avg:87.32ms
step:268/1680 train_time:23401ms step_avg:87.32ms
step:269/1680 train_time:23488ms step_avg:87.32ms
step:270/1680 train_time:23575ms step_avg:87.31ms
step:271/1680 train_time:23662ms step_avg:87.31ms
step:272/1680 train_time:23748ms step_avg:87.31ms
step:273/1680 train_time:23835ms step_avg:87.31ms
step:274/1680 train_time:23923ms step_avg:87.31ms
step:275/1680 train_time:24010ms step_avg:87.31ms
step:276/1680 train_time:24099ms step_avg:87.31ms
step:277/1680 train_time:24186ms step_avg:87.32ms
step:278/1680 train_time:24274ms step_avg:87.32ms
step:279/1680 train_time:24362ms step_avg:87.32ms
step:280/1680 train_time:24449ms step_avg:87.32ms
step:281/1680 train_time:24536ms step_avg:87.32ms
step:282/1680 train_time:24623ms step_avg:87.32ms
step:283/1680 train_time:24709ms step_avg:87.31ms
step:284/1680 train_time:24796ms step_avg:87.31ms
step:285/1680 train_time:24883ms step_avg:87.31ms
step:286/1680 train_time:24970ms step_avg:87.31ms
step:287/1680 train_time:25058ms step_avg:87.31ms
step:288/1680 train_time:25145ms step_avg:87.31ms
step:289/1680 train_time:25233ms step_avg:87.31ms
step:290/1680 train_time:25320ms step_avg:87.31ms
step:291/1680 train_time:25407ms step_avg:87.31ms
step:292/1680 train_time:25494ms step_avg:87.31ms
step:293/1680 train_time:25581ms step_avg:87.31ms
step:294/1680 train_time:25668ms step_avg:87.31ms
step:295/1680 train_time:25754ms step_avg:87.30ms
step:296/1680 train_time:25842ms step_avg:87.30ms
step:297/1680 train_time:25928ms step_avg:87.30ms
step:298/1680 train_time:26016ms step_avg:87.30ms
step:299/1680 train_time:26104ms step_avg:87.30ms
step:300/1680 train_time:26191ms step_avg:87.30ms
step:301/1680 train_time:26279ms step_avg:87.31ms
step:302/1680 train_time:26366ms step_avg:87.31ms
step:303/1680 train_time:26454ms step_avg:87.31ms
step:304/1680 train_time:26541ms step_avg:87.31ms
step:305/1680 train_time:26628ms step_avg:87.30ms
step:306/1680 train_time:26715ms step_avg:87.30ms
step:307/1680 train_time:26802ms step_avg:87.30ms
step:308/1680 train_time:26889ms step_avg:87.30ms
step:309/1680 train_time:26976ms step_avg:87.30ms
step:310/1680 train_time:27063ms step_avg:87.30ms
step:311/1680 train_time:27151ms step_avg:87.30ms
step:312/1680 train_time:27239ms step_avg:87.31ms
step:313/1680 train_time:27327ms step_avg:87.31ms
step:314/1680 train_time:27414ms step_avg:87.30ms
step:315/1680 train_time:27501ms step_avg:87.30ms
step:316/1680 train_time:27587ms step_avg:87.30ms
step:317/1680 train_time:27675ms step_avg:87.30ms
step:318/1680 train_time:27762ms step_avg:87.30ms
step:319/1680 train_time:27849ms step_avg:87.30ms
step:320/1680 train_time:27936ms step_avg:87.30ms
step:321/1680 train_time:28024ms step_avg:87.30ms
step:322/1680 train_time:28111ms step_avg:87.30ms
step:323/1680 train_time:28199ms step_avg:87.30ms
step:324/1680 train_time:28287ms step_avg:87.30ms
step:325/1680 train_time:28375ms step_avg:87.31ms
step:326/1680 train_time:28462ms step_avg:87.31ms
step:327/1680 train_time:28550ms step_avg:87.31ms
step:328/1680 train_time:28637ms step_avg:87.31ms
step:329/1680 train_time:28724ms step_avg:87.31ms
step:330/1680 train_time:28812ms step_avg:87.31ms
step:331/1680 train_time:28899ms step_avg:87.31ms
step:332/1680 train_time:28986ms step_avg:87.31ms
step:333/1680 train_time:29073ms step_avg:87.31ms
step:334/1680 train_time:29160ms step_avg:87.31ms
step:335/1680 train_time:29248ms step_avg:87.31ms
step:336/1680 train_time:29335ms step_avg:87.31ms
step:337/1680 train_time:29423ms step_avg:87.31ms
step:338/1680 train_time:29510ms step_avg:87.31ms
step:339/1680 train_time:29597ms step_avg:87.31ms
step:340/1680 train_time:29685ms step_avg:87.31ms
step:341/1680 train_time:29771ms step_avg:87.31ms
step:342/1680 train_time:29858ms step_avg:87.30ms
step:343/1680 train_time:29945ms step_avg:87.30ms
step:344/1680 train_time:30033ms step_avg:87.30ms
step:345/1680 train_time:30120ms step_avg:87.30ms
step:346/1680 train_time:30208ms step_avg:87.30ms
step:347/1680 train_time:30295ms step_avg:87.31ms
step:348/1680 train_time:30383ms step_avg:87.31ms
step:349/1680 train_time:30471ms step_avg:87.31ms
step:350/1680 train_time:30558ms step_avg:87.31ms
step:351/1680 train_time:30646ms step_avg:87.31ms
step:352/1680 train_time:30733ms step_avg:87.31ms
step:353/1680 train_time:30819ms step_avg:87.31ms
step:354/1680 train_time:30906ms step_avg:87.31ms
step:355/1680 train_time:30993ms step_avg:87.31ms
step:356/1680 train_time:31080ms step_avg:87.30ms
step:357/1680 train_time:31167ms step_avg:87.30ms
step:358/1680 train_time:31254ms step_avg:87.30ms
step:359/1680 train_time:31343ms step_avg:87.31ms
step:360/1680 train_time:31429ms step_avg:87.30ms
step:361/1680 train_time:31517ms step_avg:87.30ms
step:362/1680 train_time:31605ms step_avg:87.31ms
step:363/1680 train_time:31692ms step_avg:87.31ms
step:364/1680 train_time:31780ms step_avg:87.31ms
step:365/1680 train_time:31867ms step_avg:87.31ms
step:366/1680 train_time:31955ms step_avg:87.31ms
step:367/1680 train_time:32042ms step_avg:87.31ms
step:368/1680 train_time:32129ms step_avg:87.31ms
step:369/1680 train_time:32218ms step_avg:87.31ms
step:370/1680 train_time:32305ms step_avg:87.31ms
step:371/1680 train_time:32392ms step_avg:87.31ms
step:372/1680 train_time:32479ms step_avg:87.31ms
step:373/1680 train_time:32566ms step_avg:87.31ms
step:374/1680 train_time:32653ms step_avg:87.31ms
step:375/1680 train_time:32739ms step_avg:87.30ms
step:375/1680 val_loss:3.8197 train_time:32828ms step_avg:87.54ms
step:376/1680 train_time:32846ms step_avg:87.36ms
step:377/1680 train_time:32918ms step_avg:87.31ms
step:378/1680 train_time:33010ms step_avg:87.33ms
step:379/1680 train_time:33099ms step_avg:87.33ms
step:380/1680 train_time:33188ms step_avg:87.34ms
step:381/1680 train_time:33275ms step_avg:87.34ms
step:382/1680 train_time:33361ms step_avg:87.33ms
step:383/1680 train_time:33447ms step_avg:87.33ms
step:384/1680 train_time:33534ms step_avg:87.33ms
step:385/1680 train_time:33621ms step_avg:87.33ms
step:386/1680 train_time:33707ms step_avg:87.32ms
step:387/1680 train_time:33795ms step_avg:87.32ms
step:388/1680 train_time:33882ms step_avg:87.33ms
step:389/1680 train_time:33971ms step_avg:87.33ms
step:390/1680 train_time:34060ms step_avg:87.33ms
step:391/1680 train_time:34148ms step_avg:87.33ms
step:392/1680 train_time:34235ms step_avg:87.33ms
step:393/1680 train_time:34322ms step_avg:87.33ms
step:394/1680 train_time:34409ms step_avg:87.33ms
step:395/1680 train_time:34496ms step_avg:87.33ms
step:396/1680 train_time:34583ms step_avg:87.33ms
step:397/1680 train_time:34670ms step_avg:87.33ms
step:398/1680 train_time:34757ms step_avg:87.33ms
step:399/1680 train_time:34844ms step_avg:87.33ms
step:400/1680 train_time:34931ms step_avg:87.33ms
step:401/1680 train_time:35020ms step_avg:87.33ms
step:402/1680 train_time:35107ms step_avg:87.33ms
step:403/1680 train_time:35194ms step_avg:87.33ms
step:404/1680 train_time:35281ms step_avg:87.33ms
step:405/1680 train_time:35368ms step_avg:87.33ms
step:406/1680 train_time:35455ms step_avg:87.33ms
step:407/1680 train_time:35542ms step_avg:87.33ms
step:408/1680 train_time:35629ms step_avg:87.33ms
step:409/1680 train_time:35716ms step_avg:87.33ms
step:410/1680 train_time:35803ms step_avg:87.32ms
step:411/1680 train_time:35891ms step_avg:87.33ms
step:412/1680 train_time:35980ms step_avg:87.33ms
step:413/1680 train_time:36067ms step_avg:87.33ms
step:414/1680 train_time:36155ms step_avg:87.33ms
step:415/1680 train_time:36242ms step_avg:87.33ms
step:416/1680 train_time:36329ms step_avg:87.33ms
step:417/1680 train_time:36417ms step_avg:87.33ms
step:418/1680 train_time:36504ms step_avg:87.33ms
step:419/1680 train_time:36591ms step_avg:87.33ms
step:420/1680 train_time:36679ms step_avg:87.33ms
step:421/1680 train_time:36766ms step_avg:87.33ms
step:422/1680 train_time:36854ms step_avg:87.33ms
step:423/1680 train_time:36941ms step_avg:87.33ms
step:424/1680 train_time:37029ms step_avg:87.33ms
step:425/1680 train_time:37117ms step_avg:87.33ms
step:426/1680 train_time:37204ms step_avg:87.33ms
step:427/1680 train_time:37291ms step_avg:87.33ms
step:428/1680 train_time:37378ms step_avg:87.33ms
step:429/1680 train_time:37465ms step_avg:87.33ms
step:430/1680 train_time:37552ms step_avg:87.33ms
step:431/1680 train_time:37639ms step_avg:87.33ms
step:432/1680 train_time:37725ms step_avg:87.33ms
step:433/1680 train_time:37813ms step_avg:87.33ms
step:434/1680 train_time:37900ms step_avg:87.33ms
step:435/1680 train_time:37988ms step_avg:87.33ms
step:436/1680 train_time:38075ms step_avg:87.33ms
step:437/1680 train_time:38162ms step_avg:87.33ms
step:438/1680 train_time:38249ms step_avg:87.33ms
step:439/1680 train_time:38337ms step_avg:87.33ms
step:440/1680 train_time:38424ms step_avg:87.33ms
step:441/1680 train_time:38511ms step_avg:87.33ms
step:442/1680 train_time:38598ms step_avg:87.33ms
step:443/1680 train_time:38685ms step_avg:87.32ms
step:444/1680 train_time:38773ms step_avg:87.33ms
step:445/1680 train_time:38860ms step_avg:87.33ms
step:446/1680 train_time:38947ms step_avg:87.32ms
step:447/1680 train_time:39034ms step_avg:87.32ms
step:448/1680 train_time:39122ms step_avg:87.32ms
step:449/1680 train_time:39209ms step_avg:87.32ms
step:450/1680 train_time:39296ms step_avg:87.32ms
step:451/1680 train_time:39383ms step_avg:87.32ms
step:452/1680 train_time:39470ms step_avg:87.32ms
step:453/1680 train_time:39558ms step_avg:87.32ms
step:454/1680 train_time:39644ms step_avg:87.32ms
step:455/1680 train_time:39732ms step_avg:87.32ms
step:456/1680 train_time:39820ms step_avg:87.32ms
step:457/1680 train_time:39908ms step_avg:87.33ms
step:458/1680 train_time:39995ms step_avg:87.33ms
step:459/1680 train_time:40083ms step_avg:87.33ms
step:460/1680 train_time:40170ms step_avg:87.33ms
step:461/1680 train_time:40257ms step_avg:87.33ms
step:462/1680 train_time:40344ms step_avg:87.33ms
step:463/1680 train_time:40432ms step_avg:87.33ms
step:464/1680 train_time:40519ms step_avg:87.33ms
step:465/1680 train_time:40606ms step_avg:87.32ms
step:466/1680 train_time:40693ms step_avg:87.32ms
step:467/1680 train_time:40780ms step_avg:87.32ms
step:468/1680 train_time:40867ms step_avg:87.32ms
step:469/1680 train_time:40955ms step_avg:87.32ms
step:470/1680 train_time:41042ms step_avg:87.32ms
step:471/1680 train_time:41129ms step_avg:87.32ms
step:472/1680 train_time:41217ms step_avg:87.32ms
step:473/1680 train_time:41304ms step_avg:87.32ms
step:474/1680 train_time:41392ms step_avg:87.33ms
step:475/1680 train_time:41480ms step_avg:87.33ms
step:476/1680 train_time:41566ms step_avg:87.32ms
step:477/1680 train_time:41654ms step_avg:87.32ms
step:478/1680 train_time:41741ms step_avg:87.32ms
step:479/1680 train_time:41828ms step_avg:87.32ms
step:480/1680 train_time:41915ms step_avg:87.32ms
step:481/1680 train_time:42002ms step_avg:87.32ms
step:482/1680 train_time:42090ms step_avg:87.32ms
step:483/1680 train_time:42177ms step_avg:87.32ms
step:484/1680 train_time:42264ms step_avg:87.32ms
step:485/1680 train_time:42352ms step_avg:87.32ms
step:486/1680 train_time:42439ms step_avg:87.32ms
step:487/1680 train_time:42525ms step_avg:87.32ms
step:488/1680 train_time:42613ms step_avg:87.32ms
step:489/1680 train_time:42701ms step_avg:87.32ms
step:490/1680 train_time:42788ms step_avg:87.32ms
step:491/1680 train_time:42875ms step_avg:87.32ms
step:492/1680 train_time:42962ms step_avg:87.32ms
step:493/1680 train_time:43049ms step_avg:87.32ms
step:494/1680 train_time:43137ms step_avg:87.32ms
step:495/1680 train_time:43224ms step_avg:87.32ms
step:496/1680 train_time:43311ms step_avg:87.32ms
step:497/1680 train_time:43399ms step_avg:87.32ms
step:498/1680 train_time:43486ms step_avg:87.32ms
step:499/1680 train_time:43574ms step_avg:87.32ms
step:500/1680 train_time:43661ms step_avg:87.32ms
step:500/1680 val_loss:3.7186 train_time:43750ms step_avg:87.50ms
step:501/1680 train_time:43768ms step_avg:87.36ms
step:502/1680 train_time:43838ms step_avg:87.33ms
step:503/1680 train_time:43932ms step_avg:87.34ms
step:504/1680 train_time:44024ms step_avg:87.35ms
step:505/1680 train_time:44110ms step_avg:87.35ms
step:506/1680 train_time:44198ms step_avg:87.35ms
step:507/1680 train_time:44283ms step_avg:87.34ms
step:508/1680 train_time:44370ms step_avg:87.34ms
step:509/1680 train_time:44456ms step_avg:87.34ms
step:510/1680 train_time:44542ms step_avg:87.34ms
step:511/1680 train_time:44628ms step_avg:87.33ms
step:512/1680 train_time:44716ms step_avg:87.34ms
step:513/1680 train_time:44805ms step_avg:87.34ms
step:514/1680 train_time:44894ms step_avg:87.34ms
step:515/1680 train_time:44982ms step_avg:87.34ms
step:516/1680 train_time:45070ms step_avg:87.35ms
step:517/1680 train_time:45158ms step_avg:87.35ms
step:518/1680 train_time:45245ms step_avg:87.34ms
step:519/1680 train_time:45331ms step_avg:87.34ms
step:520/1680 train_time:45417ms step_avg:87.34ms
step:521/1680 train_time:45504ms step_avg:87.34ms
step:522/1680 train_time:45590ms step_avg:87.34ms
step:523/1680 train_time:45677ms step_avg:87.34ms
step:524/1680 train_time:45765ms step_avg:87.34ms
step:525/1680 train_time:45855ms step_avg:87.34ms
step:526/1680 train_time:45943ms step_avg:87.34ms
step:527/1680 train_time:46032ms step_avg:87.35ms
step:528/1680 train_time:46119ms step_avg:87.35ms
step:529/1680 train_time:46206ms step_avg:87.35ms
step:530/1680 train_time:46293ms step_avg:87.35ms
step:531/1680 train_time:46380ms step_avg:87.34ms
step:532/1680 train_time:46467ms step_avg:87.34ms
step:533/1680 train_time:46553ms step_avg:87.34ms
step:534/1680 train_time:46640ms step_avg:87.34ms
step:535/1680 train_time:46727ms step_avg:87.34ms
step:536/1680 train_time:46815ms step_avg:87.34ms
step:537/1680 train_time:46903ms step_avg:87.34ms
step:538/1680 train_time:46991ms step_avg:87.34ms
step:539/1680 train_time:47079ms step_avg:87.34ms
step:540/1680 train_time:47166ms step_avg:87.34ms
step:541/1680 train_time:47254ms step_avg:87.35ms
step:542/1680 train_time:47341ms step_avg:87.34ms
step:543/1680 train_time:47428ms step_avg:87.34ms
step:544/1680 train_time:47514ms step_avg:87.34ms
step:545/1680 train_time:47601ms step_avg:87.34ms
step:546/1680 train_time:47688ms step_avg:87.34ms
step:547/1680 train_time:47775ms step_avg:87.34ms
step:548/1680 train_time:47863ms step_avg:87.34ms
step:549/1680 train_time:47953ms step_avg:87.35ms
step:550/1680 train_time:48041ms step_avg:87.35ms
step:551/1680 train_time:48130ms step_avg:87.35ms
step:552/1680 train_time:48218ms step_avg:87.35ms
step:553/1680 train_time:48307ms step_avg:87.35ms
step:554/1680 train_time:48395ms step_avg:87.36ms
step:555/1680 train_time:48483ms step_avg:87.36ms
step:556/1680 train_time:48571ms step_avg:87.36ms
step:557/1680 train_time:48660ms step_avg:87.36ms
step:558/1680 train_time:48749ms step_avg:87.36ms
step:559/1680 train_time:48838ms step_avg:87.37ms
step:560/1680 train_time:48927ms step_avg:87.37ms
step:561/1680 train_time:49016ms step_avg:87.37ms
step:562/1680 train_time:49105ms step_avg:87.38ms
step:563/1680 train_time:49193ms step_avg:87.38ms
step:564/1680 train_time:49281ms step_avg:87.38ms
step:565/1680 train_time:49369ms step_avg:87.38ms
step:566/1680 train_time:49457ms step_avg:87.38ms
step:567/1680 train_time:49545ms step_avg:87.38ms
step:568/1680 train_time:49634ms step_avg:87.38ms
step:569/1680 train_time:49723ms step_avg:87.39ms
step:570/1680 train_time:49811ms step_avg:87.39ms
step:571/1680 train_time:49899ms step_avg:87.39ms
step:572/1680 train_time:49989ms step_avg:87.39ms
step:573/1680 train_time:50077ms step_avg:87.39ms
step:574/1680 train_time:50166ms step_avg:87.40ms
step:575/1680 train_time:50255ms step_avg:87.40ms
step:576/1680 train_time:50342ms step_avg:87.40ms
step:577/1680 train_time:50431ms step_avg:87.40ms
step:578/1680 train_time:50519ms step_avg:87.40ms
step:579/1680 train_time:50608ms step_avg:87.41ms
step:580/1680 train_time:50696ms step_avg:87.41ms
step:581/1680 train_time:50785ms step_avg:87.41ms
step:582/1680 train_time:50874ms step_avg:87.41ms
step:583/1680 train_time:50962ms step_avg:87.41ms
step:584/1680 train_time:51050ms step_avg:87.41ms
step:585/1680 train_time:51138ms step_avg:87.42ms
step:586/1680 train_time:51227ms step_avg:87.42ms
step:587/1680 train_time:51315ms step_avg:87.42ms
step:588/1680 train_time:51404ms step_avg:87.42ms
step:589/1680 train_time:51492ms step_avg:87.42ms
step:590/1680 train_time:51580ms step_avg:87.42ms
step:591/1680 train_time:51668ms step_avg:87.43ms
step:592/1680 train_time:51757ms step_avg:87.43ms
step:593/1680 train_time:51845ms step_avg:87.43ms
step:594/1680 train_time:51934ms step_avg:87.43ms
step:595/1680 train_time:52022ms step_avg:87.43ms
step:596/1680 train_time:52111ms step_avg:87.44ms
step:597/1680 train_time:52199ms step_avg:87.44ms
step:598/1680 train_time:52288ms step_avg:87.44ms
step:599/1680 train_time:52376ms step_avg:87.44ms
step:600/1680 train_time:52464ms step_avg:87.44ms
step:601/1680 train_time:52553ms step_avg:87.44ms
step:602/1680 train_time:52641ms step_avg:87.44ms
step:603/1680 train_time:52729ms step_avg:87.44ms
step:604/1680 train_time:52818ms step_avg:87.45ms
step:605/1680 train_time:52906ms step_avg:87.45ms
step:606/1680 train_time:52995ms step_avg:87.45ms
step:607/1680 train_time:53083ms step_avg:87.45ms
step:608/1680 train_time:53172ms step_avg:87.45ms
step:609/1680 train_time:53260ms step_avg:87.46ms
step:610/1680 train_time:53348ms step_avg:87.46ms
step:611/1680 train_time:53436ms step_avg:87.46ms
step:612/1680 train_time:53525ms step_avg:87.46ms
step:613/1680 train_time:53614ms step_avg:87.46ms
step:614/1680 train_time:53702ms step_avg:87.46ms
step:615/1680 train_time:53790ms step_avg:87.46ms
step:616/1680 train_time:53878ms step_avg:87.46ms
step:617/1680 train_time:53966ms step_avg:87.47ms
step:618/1680 train_time:54054ms step_avg:87.47ms
step:619/1680 train_time:54143ms step_avg:87.47ms
step:620/1680 train_time:54232ms step_avg:87.47ms
step:621/1680 train_time:54320ms step_avg:87.47ms
step:622/1680 train_time:54409ms step_avg:87.47ms
step:623/1680 train_time:54497ms step_avg:87.47ms
step:624/1680 train_time:54585ms step_avg:87.48ms
step:625/1680 train_time:54673ms step_avg:87.48ms
step:625/1680 val_loss:3.6190 train_time:54763ms step_avg:87.62ms
step:626/1680 train_time:54783ms step_avg:87.51ms
step:627/1680 train_time:54851ms step_avg:87.48ms
step:628/1680 train_time:54940ms step_avg:87.48ms
step:629/1680 train_time:55032ms step_avg:87.49ms
step:630/1680 train_time:55121ms step_avg:87.49ms
step:631/1680 train_time:55207ms step_avg:87.49ms
step:632/1680 train_time:55295ms step_avg:87.49ms
step:633/1680 train_time:55382ms step_avg:87.49ms
step:634/1680 train_time:55469ms step_avg:87.49ms
step:635/1680 train_time:55557ms step_avg:87.49ms
step:636/1680 train_time:55645ms step_avg:87.49ms
step:637/1680 train_time:55737ms step_avg:87.50ms
step:638/1680 train_time:55827ms step_avg:87.50ms
step:639/1680 train_time:55916ms step_avg:87.51ms
step:640/1680 train_time:56005ms step_avg:87.51ms
step:641/1680 train_time:56092ms step_avg:87.51ms
step:642/1680 train_time:56180ms step_avg:87.51ms
step:643/1680 train_time:56268ms step_avg:87.51ms
step:644/1680 train_time:56356ms step_avg:87.51ms
step:645/1680 train_time:56443ms step_avg:87.51ms
step:646/1680 train_time:56529ms step_avg:87.51ms
step:647/1680 train_time:56618ms step_avg:87.51ms
step:648/1680 train_time:56709ms step_avg:87.51ms
step:649/1680 train_time:56799ms step_avg:87.52ms
step:650/1680 train_time:56888ms step_avg:87.52ms
step:651/1680 train_time:56977ms step_avg:87.52ms
step:652/1680 train_time:57066ms step_avg:87.52ms
step:653/1680 train_time:57154ms step_avg:87.53ms
step:654/1680 train_time:57242ms step_avg:87.53ms
step:655/1680 train_time:57330ms step_avg:87.53ms
step:656/1680 train_time:57417ms step_avg:87.53ms
step:657/1680 train_time:57505ms step_avg:87.53ms
step:658/1680 train_time:57593ms step_avg:87.53ms
step:659/1680 train_time:57681ms step_avg:87.53ms
step:660/1680 train_time:57771ms step_avg:87.53ms
step:661/1680 train_time:57861ms step_avg:87.53ms
step:662/1680 train_time:57950ms step_avg:87.54ms
step:663/1680 train_time:58040ms step_avg:87.54ms
step:664/1680 train_time:58128ms step_avg:87.54ms
step:665/1680 train_time:58217ms step_avg:87.54ms
step:666/1680 train_time:58305ms step_avg:87.54ms
step:667/1680 train_time:58392ms step_avg:87.54ms
step:668/1680 train_time:58480ms step_avg:87.55ms
step:669/1680 train_time:58568ms step_avg:87.55ms
step:670/1680 train_time:58657ms step_avg:87.55ms
step:671/1680 train_time:58745ms step_avg:87.55ms
step:672/1680 train_time:58834ms step_avg:87.55ms
step:673/1680 train_time:58922ms step_avg:87.55ms
step:674/1680 train_time:59010ms step_avg:87.55ms
step:675/1680 train_time:59098ms step_avg:87.55ms
step:676/1680 train_time:59187ms step_avg:87.55ms
step:677/1680 train_time:59275ms step_avg:87.56ms
step:678/1680 train_time:59364ms step_avg:87.56ms
step:679/1680 train_time:59451ms step_avg:87.56ms
step:680/1680 train_time:59540ms step_avg:87.56ms
step:681/1680 train_time:59628ms step_avg:87.56ms
step:682/1680 train_time:59716ms step_avg:87.56ms
step:683/1680 train_time:59805ms step_avg:87.56ms
step:684/1680 train_time:59893ms step_avg:87.56ms
step:685/1680 train_time:59982ms step_avg:87.56ms
step:686/1680 train_time:60070ms step_avg:87.57ms
step:687/1680 train_time:60159ms step_avg:87.57ms
step:688/1680 train_time:60247ms step_avg:87.57ms
step:689/1680 train_time:60336ms step_avg:87.57ms
step:690/1680 train_time:60424ms step_avg:87.57ms
step:691/1680 train_time:60512ms step_avg:87.57ms
step:692/1680 train_time:60601ms step_avg:87.57ms
step:693/1680 train_time:60689ms step_avg:87.57ms
step:694/1680 train_time:60778ms step_avg:87.58ms
step:695/1680 train_time:60867ms step_avg:87.58ms
step:696/1680 train_time:60955ms step_avg:87.58ms
step:697/1680 train_time:61044ms step_avg:87.58ms
step:698/1680 train_time:61132ms step_avg:87.58ms
step:699/1680 train_time:61221ms step_avg:87.58ms
step:700/1680 train_time:61309ms step_avg:87.58ms
step:701/1680 train_time:61399ms step_avg:87.59ms
step:702/1680 train_time:61487ms step_avg:87.59ms
step:703/1680 train_time:61575ms step_avg:87.59ms
step:704/1680 train_time:61663ms step_avg:87.59ms
step:705/1680 train_time:61750ms step_avg:87.59ms
step:706/1680 train_time:61839ms step_avg:87.59ms
step:707/1680 train_time:61927ms step_avg:87.59ms
step:708/1680 train_time:62016ms step_avg:87.59ms
step:709/1680 train_time:62106ms step_avg:87.60ms
step:710/1680 train_time:62193ms step_avg:87.60ms
step:711/1680 train_time:62282ms step_avg:87.60ms
step:712/1680 train_time:62370ms step_avg:87.60ms
step:713/1680 train_time:62459ms step_avg:87.60ms
step:714/1680 train_time:62547ms step_avg:87.60ms
step:715/1680 train_time:62635ms step_avg:87.60ms
step:716/1680 train_time:62724ms step_avg:87.60ms
step:717/1680 train_time:62812ms step_avg:87.60ms
step:718/1680 train_time:62901ms step_avg:87.61ms
step:719/1680 train_time:62989ms step_avg:87.61ms
step:720/1680 train_time:63078ms step_avg:87.61ms
step:721/1680 train_time:63166ms step_avg:87.61ms
step:722/1680 train_time:63254ms step_avg:87.61ms
step:723/1680 train_time:63343ms step_avg:87.61ms
step:724/1680 train_time:63431ms step_avg:87.61ms
step:725/1680 train_time:63520ms step_avg:87.61ms
step:726/1680 train_time:63609ms step_avg:87.62ms
step:727/1680 train_time:63698ms step_avg:87.62ms
step:728/1680 train_time:63786ms step_avg:87.62ms
step:729/1680 train_time:63874ms step_avg:87.62ms
step:730/1680 train_time:63963ms step_avg:87.62ms
step:731/1680 train_time:64051ms step_avg:87.62ms
step:732/1680 train_time:64140ms step_avg:87.62ms
step:733/1680 train_time:64228ms step_avg:87.62ms
step:734/1680 train_time:64316ms step_avg:87.62ms
step:735/1680 train_time:64405ms step_avg:87.63ms
step:736/1680 train_time:64493ms step_avg:87.63ms
step:737/1680 train_time:64582ms step_avg:87.63ms
step:738/1680 train_time:64670ms step_avg:87.63ms
step:739/1680 train_time:64759ms step_avg:87.63ms
step:740/1680 train_time:64847ms step_avg:87.63ms
step:741/1680 train_time:64936ms step_avg:87.63ms
step:742/1680 train_time:65024ms step_avg:87.63ms
step:743/1680 train_time:65113ms step_avg:87.63ms
step:744/1680 train_time:65201ms step_avg:87.64ms
step:745/1680 train_time:65290ms step_avg:87.64ms
step:746/1680 train_time:65379ms step_avg:87.64ms
step:747/1680 train_time:65467ms step_avg:87.64ms
step:748/1680 train_time:65556ms step_avg:87.64ms
step:749/1680 train_time:65645ms step_avg:87.64ms
step:750/1680 train_time:65733ms step_avg:87.64ms
step:750/1680 val_loss:3.5680 train_time:65823ms step_avg:87.76ms
step:751/1680 train_time:65841ms step_avg:87.67ms
step:752/1680 train_time:65917ms step_avg:87.66ms
step:753/1680 train_time:66009ms step_avg:87.66ms
step:754/1680 train_time:66098ms step_avg:87.66ms
step:755/1680 train_time:66186ms step_avg:87.66ms
step:756/1680 train_time:66273ms step_avg:87.66ms
step:757/1680 train_time:66361ms step_avg:87.66ms
step:758/1680 train_time:66448ms step_avg:87.66ms
step:759/1680 train_time:66536ms step_avg:87.66ms
step:760/1680 train_time:66624ms step_avg:87.66ms
step:761/1680 train_time:66712ms step_avg:87.66ms
step:762/1680 train_time:66801ms step_avg:87.67ms
step:763/1680 train_time:66891ms step_avg:87.67ms
step:764/1680 train_time:66980ms step_avg:87.67ms
step:765/1680 train_time:67070ms step_avg:87.67ms
step:766/1680 train_time:67158ms step_avg:87.67ms
step:767/1680 train_time:67247ms step_avg:87.68ms
step:768/1680 train_time:67335ms step_avg:87.68ms
step:769/1680 train_time:67422ms step_avg:87.68ms
step:770/1680 train_time:67511ms step_avg:87.68ms
step:771/1680 train_time:67598ms step_avg:87.68ms
step:772/1680 train_time:67686ms step_avg:87.68ms
step:773/1680 train_time:67774ms step_avg:87.68ms
step:774/1680 train_time:67864ms step_avg:87.68ms
step:775/1680 train_time:67954ms step_avg:87.68ms
step:776/1680 train_time:68043ms step_avg:87.68ms
step:777/1680 train_time:68132ms step_avg:87.69ms
step:778/1680 train_time:68220ms step_avg:87.69ms
step:779/1680 train_time:68309ms step_avg:87.69ms
step:780/1680 train_time:68397ms step_avg:87.69ms
step:781/1680 train_time:68485ms step_avg:87.69ms
step:782/1680 train_time:68573ms step_avg:87.69ms
step:783/1680 train_time:68661ms step_avg:87.69ms
step:784/1680 train_time:68749ms step_avg:87.69ms
step:785/1680 train_time:68838ms step_avg:87.69ms
step:786/1680 train_time:68927ms step_avg:87.69ms
step:787/1680 train_time:69017ms step_avg:87.70ms
step:788/1680 train_time:69107ms step_avg:87.70ms
step:789/1680 train_time:69195ms step_avg:87.70ms
step:790/1680 train_time:69284ms step_avg:87.70ms
step:791/1680 train_time:69372ms step_avg:87.70ms
step:792/1680 train_time:69460ms step_avg:87.70ms
step:793/1680 train_time:69548ms step_avg:87.70ms
step:794/1680 train_time:69636ms step_avg:87.70ms
step:795/1680 train_time:69724ms step_avg:87.70ms
step:796/1680 train_time:69813ms step_avg:87.71ms
step:797/1680 train_time:69902ms step_avg:87.71ms
step:798/1680 train_time:69991ms step_avg:87.71ms
step:799/1680 train_time:70079ms step_avg:87.71ms
step:800/1680 train_time:70169ms step_avg:87.71ms
step:801/1680 train_time:70257ms step_avg:87.71ms
step:802/1680 train_time:70345ms step_avg:87.71ms
step:803/1680 train_time:70434ms step_avg:87.71ms
step:804/1680 train_time:70522ms step_avg:87.71ms
step:805/1680 train_time:70610ms step_avg:87.71ms
step:806/1680 train_time:70699ms step_avg:87.72ms
step:807/1680 train_time:70787ms step_avg:87.72ms
step:808/1680 train_time:70875ms step_avg:87.72ms
step:809/1680 train_time:70963ms step_avg:87.72ms
step:810/1680 train_time:71053ms step_avg:87.72ms
step:811/1680 train_time:71142ms step_avg:87.72ms
step:812/1680 train_time:71231ms step_avg:87.72ms
step:813/1680 train_time:71320ms step_avg:87.72ms
step:814/1680 train_time:71408ms step_avg:87.72ms
step:815/1680 train_time:71496ms step_avg:87.73ms
step:816/1680 train_time:71585ms step_avg:87.73ms
step:817/1680 train_time:71673ms step_avg:87.73ms
step:818/1680 train_time:71761ms step_avg:87.73ms
step:819/1680 train_time:71849ms step_avg:87.73ms
step:820/1680 train_time:71937ms step_avg:87.73ms
step:821/1680 train_time:72026ms step_avg:87.73ms
step:822/1680 train_time:72114ms step_avg:87.73ms
step:823/1680 train_time:72203ms step_avg:87.73ms
step:824/1680 train_time:72291ms step_avg:87.73ms
step:825/1680 train_time:72379ms step_avg:87.73ms
step:826/1680 train_time:72467ms step_avg:87.73ms
step:827/1680 train_time:72556ms step_avg:87.73ms
step:828/1680 train_time:72644ms step_avg:87.73ms
step:829/1680 train_time:72732ms step_avg:87.73ms
step:830/1680 train_time:72820ms step_avg:87.73ms
step:831/1680 train_time:72908ms step_avg:87.74ms
step:832/1680 train_time:72997ms step_avg:87.74ms
step:833/1680 train_time:73086ms step_avg:87.74ms
step:834/1680 train_time:73174ms step_avg:87.74ms
step:835/1680 train_time:73262ms step_avg:87.74ms
step:836/1680 train_time:73350ms step_avg:87.74ms
step:837/1680 train_time:73439ms step_avg:87.74ms
step:838/1680 train_time:73528ms step_avg:87.74ms
step:839/1680 train_time:73616ms step_avg:87.74ms
step:840/1680 train_time:73703ms step_avg:87.74ms
step:841/1680 train_time:73792ms step_avg:87.74ms
step:842/1680 train_time:73880ms step_avg:87.74ms
step:843/1680 train_time:73969ms step_avg:87.74ms
step:844/1680 train_time:74058ms step_avg:87.75ms
step:845/1680 train_time:74146ms step_avg:87.75ms
step:846/1680 train_time:74235ms step_avg:87.75ms
step:847/1680 train_time:74324ms step_avg:87.75ms
step:848/1680 train_time:74413ms step_avg:87.75ms
step:849/1680 train_time:74500ms step_avg:87.75ms
step:850/1680 train_time:74588ms step_avg:87.75ms
step:851/1680 train_time:74676ms step_avg:87.75ms
step:852/1680 train_time:74765ms step_avg:87.75ms
step:853/1680 train_time:74853ms step_avg:87.75ms
step:854/1680 train_time:74942ms step_avg:87.75ms
step:855/1680 train_time:75031ms step_avg:87.76ms
step:856/1680 train_time:75119ms step_avg:87.76ms
step:857/1680 train_time:75208ms step_avg:87.76ms
step:858/1680 train_time:75296ms step_avg:87.76ms
step:859/1680 train_time:75385ms step_avg:87.76ms
step:860/1680 train_time:75474ms step_avg:87.76ms
step:861/1680 train_time:75561ms step_avg:87.76ms
step:862/1680 train_time:75649ms step_avg:87.76ms
step:863/1680 train_time:75738ms step_avg:87.76ms
step:864/1680 train_time:75827ms step_avg:87.76ms
step:865/1680 train_time:75915ms step_avg:87.76ms
step:866/1680 train_time:76004ms step_avg:87.76ms
step:867/1680 train_time:76092ms step_avg:87.77ms
step:868/1680 train_time:76181ms step_avg:87.77ms
step:869/1680 train_time:76269ms step_avg:87.77ms
step:870/1680 train_time:76358ms step_avg:87.77ms
step:871/1680 train_time:76447ms step_avg:87.77ms
step:872/1680 train_time:76536ms step_avg:87.77ms
step:873/1680 train_time:76624ms step_avg:87.77ms
step:874/1680 train_time:76713ms step_avg:87.77ms
step:875/1680 train_time:76801ms step_avg:87.77ms
step:875/1680 val_loss:3.5210 train_time:76890ms step_avg:87.87ms
step:876/1680 train_time:76910ms step_avg:87.80ms
step:877/1680 train_time:76982ms step_avg:87.78ms
step:878/1680 train_time:77077ms step_avg:87.79ms
step:879/1680 train_time:77167ms step_avg:87.79ms
step:880/1680 train_time:77254ms step_avg:87.79ms
step:881/1680 train_time:77342ms step_avg:87.79ms
step:882/1680 train_time:77429ms step_avg:87.79ms
step:883/1680 train_time:77516ms step_avg:87.79ms
step:884/1680 train_time:77603ms step_avg:87.79ms
step:885/1680 train_time:77691ms step_avg:87.79ms
step:886/1680 train_time:77779ms step_avg:87.79ms
step:887/1680 train_time:77869ms step_avg:87.79ms
step:888/1680 train_time:77960ms step_avg:87.79ms
step:889/1680 train_time:78049ms step_avg:87.79ms
step:890/1680 train_time:78139ms step_avg:87.80ms
step:891/1680 train_time:78227ms step_avg:87.80ms
step:892/1680 train_time:78315ms step_avg:87.80ms
step:893/1680 train_time:78403ms step_avg:87.80ms
step:894/1680 train_time:78491ms step_avg:87.80ms
step:895/1680 train_time:78578ms step_avg:87.80ms
step:896/1680 train_time:78665ms step_avg:87.80ms
step:897/1680 train_time:78754ms step_avg:87.80ms
step:898/1680 train_time:78841ms step_avg:87.80ms
step:899/1680 train_time:78932ms step_avg:87.80ms
step:900/1680 train_time:79021ms step_avg:87.80ms
step:901/1680 train_time:79111ms step_avg:87.80ms
step:902/1680 train_time:79200ms step_avg:87.80ms
step:903/1680 train_time:79288ms step_avg:87.81ms
step:904/1680 train_time:79376ms step_avg:87.81ms
step:905/1680 train_time:79464ms step_avg:87.81ms
step:906/1680 train_time:79551ms step_avg:87.81ms
step:907/1680 train_time:79639ms step_avg:87.81ms
step:908/1680 train_time:79728ms step_avg:87.81ms
step:909/1680 train_time:79816ms step_avg:87.81ms
step:910/1680 train_time:79904ms step_avg:87.81ms
step:911/1680 train_time:79995ms step_avg:87.81ms
step:912/1680 train_time:80084ms step_avg:87.81ms
step:913/1680 train_time:80173ms step_avg:87.81ms
step:914/1680 train_time:80262ms step_avg:87.81ms
step:915/1680 train_time:80351ms step_avg:87.82ms
step:916/1680 train_time:80439ms step_avg:87.82ms
step:917/1680 train_time:80527ms step_avg:87.82ms
step:918/1680 train_time:80614ms step_avg:87.82ms
step:919/1680 train_time:80703ms step_avg:87.82ms
step:920/1680 train_time:80791ms step_avg:87.82ms
step:921/1680 train_time:80878ms step_avg:87.82ms
step:922/1680 train_time:80968ms step_avg:87.82ms
step:923/1680 train_time:81056ms step_avg:87.82ms
step:924/1680 train_time:81145ms step_avg:87.82ms
step:925/1680 train_time:81235ms step_avg:87.82ms
step:926/1680 train_time:81323ms step_avg:87.82ms
step:927/1680 train_time:81411ms step_avg:87.82ms
step:928/1680 train_time:81499ms step_avg:87.82ms
step:929/1680 train_time:81588ms step_avg:87.82ms
step:930/1680 train_time:81676ms step_avg:87.82ms
step:931/1680 train_time:81764ms step_avg:87.82ms
step:932/1680 train_time:81852ms step_avg:87.82ms
step:933/1680 train_time:81940ms step_avg:87.82ms
step:934/1680 train_time:82028ms step_avg:87.82ms
step:935/1680 train_time:82117ms step_avg:87.83ms
step:936/1680 train_time:82206ms step_avg:87.83ms
step:937/1680 train_time:82295ms step_avg:87.83ms
step:938/1680 train_time:82384ms step_avg:87.83ms
step:939/1680 train_time:82473ms step_avg:87.83ms
step:940/1680 train_time:82562ms step_avg:87.83ms
step:941/1680 train_time:82650ms step_avg:87.83ms
step:942/1680 train_time:82738ms step_avg:87.83ms
step:943/1680 train_time:82826ms step_avg:87.83ms
step:944/1680 train_time:82915ms step_avg:87.83ms
step:945/1680 train_time:83004ms step_avg:87.83ms
step:946/1680 train_time:83092ms step_avg:87.84ms
step:947/1680 train_time:83181ms step_avg:87.84ms
step:948/1680 train_time:83270ms step_avg:87.84ms
step:949/1680 train_time:83359ms step_avg:87.84ms
step:950/1680 train_time:83447ms step_avg:87.84ms
step:951/1680 train_time:83535ms step_avg:87.84ms
step:952/1680 train_time:83623ms step_avg:87.84ms
step:953/1680 train_time:83711ms step_avg:87.84ms
step:954/1680 train_time:83800ms step_avg:87.84ms
step:955/1680 train_time:83889ms step_avg:87.84ms
step:956/1680 train_time:83977ms step_avg:87.84ms
step:957/1680 train_time:84065ms step_avg:87.84ms
step:958/1680 train_time:84154ms step_avg:87.84ms
step:959/1680 train_time:84242ms step_avg:87.84ms
step:960/1680 train_time:84331ms step_avg:87.84ms
step:961/1680 train_time:84419ms step_avg:87.85ms
step:962/1680 train_time:84508ms step_avg:87.85ms
step:963/1680 train_time:84596ms step_avg:87.85ms
step:964/1680 train_time:84684ms step_avg:87.85ms
step:965/1680 train_time:84773ms step_avg:87.85ms
step:966/1680 train_time:84862ms step_avg:87.85ms
step:967/1680 train_time:84951ms step_avg:87.85ms
step:968/1680 train_time:85039ms step_avg:87.85ms
step:969/1680 train_time:85127ms step_avg:87.85ms
step:970/1680 train_time:85215ms step_avg:87.85ms
step:971/1680 train_time:85304ms step_avg:87.85ms
step:972/1680 train_time:85393ms step_avg:87.85ms
step:973/1680 train_time:85482ms step_avg:87.85ms
step:974/1680 train_time:85571ms step_avg:87.86ms
step:975/1680 train_time:85659ms step_avg:87.86ms
step:976/1680 train_time:85748ms step_avg:87.86ms
step:977/1680 train_time:85836ms step_avg:87.86ms
step:978/1680 train_time:85925ms step_avg:87.86ms
step:979/1680 train_time:86014ms step_avg:87.86ms
step:980/1680 train_time:86102ms step_avg:87.86ms
step:981/1680 train_time:86191ms step_avg:87.86ms
step:982/1680 train_time:86279ms step_avg:87.86ms
step:983/1680 train_time:86367ms step_avg:87.86ms
step:984/1680 train_time:86456ms step_avg:87.86ms
step:985/1680 train_time:86544ms step_avg:87.86ms
step:986/1680 train_time:86633ms step_avg:87.86ms
step:987/1680 train_time:86721ms step_avg:87.86ms
step:988/1680 train_time:86810ms step_avg:87.86ms
step:989/1680 train_time:86900ms step_avg:87.87ms
step:990/1680 train_time:86988ms step_avg:87.87ms
step:991/1680 train_time:87077ms step_avg:87.87ms
step:992/1680 train_time:87164ms step_avg:87.87ms
step:993/1680 train_time:87253ms step_avg:87.87ms
step:994/1680 train_time:87341ms step_avg:87.87ms
step:995/1680 train_time:87430ms step_avg:87.87ms
step:996/1680 train_time:87518ms step_avg:87.87ms
step:997/1680 train_time:87607ms step_avg:87.87ms
step:998/1680 train_time:87695ms step_avg:87.87ms
step:999/1680 train_time:87784ms step_avg:87.87ms
step:1000/1680 train_time:87873ms step_avg:87.87ms
step:1000/1680 val_loss:3.4706 train_time:87964ms step_avg:87.96ms
step:1001/1680 train_time:87982ms step_avg:87.89ms
step:1002/1680 train_time:88055ms step_avg:87.88ms
step:1003/1680 train_time:88150ms step_avg:87.89ms
step:1004/1680 train_time:88240ms step_avg:87.89ms
step:1005/1680 train_time:88328ms step_avg:87.89ms
step:1006/1680 train_time:88415ms step_avg:87.89ms
step:1007/1680 train_time:88503ms step_avg:87.89ms
step:1008/1680 train_time:88590ms step_avg:87.89ms
step:1009/1680 train_time:88678ms step_avg:87.89ms
step:1010/1680 train_time:88766ms step_avg:87.89ms
step:1011/1680 train_time:88854ms step_avg:87.89ms
step:1012/1680 train_time:88943ms step_avg:87.89ms
step:1013/1680 train_time:89033ms step_avg:87.89ms
step:1014/1680 train_time:89124ms step_avg:87.89ms
step:1015/1680 train_time:89213ms step_avg:87.89ms
step:1016/1680 train_time:89302ms step_avg:87.90ms
step:1017/1680 train_time:89390ms step_avg:87.90ms
step:1018/1680 train_time:89478ms step_avg:87.90ms
step:1019/1680 train_time:89566ms step_avg:87.90ms
step:1020/1680 train_time:89654ms step_avg:87.90ms
step:1021/1680 train_time:89743ms step_avg:87.90ms
step:1022/1680 train_time:89830ms step_avg:87.90ms
step:1023/1680 train_time:89920ms step_avg:87.90ms
step:1024/1680 train_time:90009ms step_avg:87.90ms
step:1025/1680 train_time:90099ms step_avg:87.90ms
step:1026/1680 train_time:90188ms step_avg:87.90ms
step:1027/1680 train_time:90276ms step_avg:87.90ms
step:1028/1680 train_time:90364ms step_avg:87.90ms
step:1029/1680 train_time:90452ms step_avg:87.90ms
step:1030/1680 train_time:90541ms step_avg:87.90ms
step:1031/1680 train_time:90628ms step_avg:87.90ms
step:1032/1680 train_time:90717ms step_avg:87.90ms
step:1033/1680 train_time:90805ms step_avg:87.90ms
step:1034/1680 train_time:90894ms step_avg:87.90ms
step:1035/1680 train_time:90983ms step_avg:87.91ms
step:1036/1680 train_time:91071ms step_avg:87.91ms
step:1037/1680 train_time:91160ms step_avg:87.91ms
step:1038/1680 train_time:91249ms step_avg:87.91ms
step:1039/1680 train_time:91338ms step_avg:87.91ms
step:1040/1680 train_time:91426ms step_avg:87.91ms
step:1041/1680 train_time:91514ms step_avg:87.91ms
step:1042/1680 train_time:91602ms step_avg:87.91ms
step:1043/1680 train_time:91690ms step_avg:87.91ms
step:1044/1680 train_time:91779ms step_avg:87.91ms
step:1045/1680 train_time:91867ms step_avg:87.91ms
step:1046/1680 train_time:91957ms step_avg:87.91ms
step:1047/1680 train_time:92046ms step_avg:87.91ms
step:1048/1680 train_time:92135ms step_avg:87.92ms
step:1049/1680 train_time:92224ms step_avg:87.92ms
step:1050/1680 train_time:92313ms step_avg:87.92ms
step:1051/1680 train_time:92401ms step_avg:87.92ms
step:1052/1680 train_time:92489ms step_avg:87.92ms
step:1053/1680 train_time:92577ms step_avg:87.92ms
step:1054/1680 train_time:92665ms step_avg:87.92ms
step:1055/1680 train_time:92752ms step_avg:87.92ms
step:1056/1680 train_time:92840ms step_avg:87.92ms
step:1057/1680 train_time:92930ms step_avg:87.92ms
step:1058/1680 train_time:93019ms step_avg:87.92ms
step:1059/1680 train_time:93108ms step_avg:87.92ms
step:1060/1680 train_time:93197ms step_avg:87.92ms
step:1061/1680 train_time:93286ms step_avg:87.92ms
step:1062/1680 train_time:93374ms step_avg:87.92ms
step:1063/1680 train_time:93463ms step_avg:87.92ms
step:1064/1680 train_time:93551ms step_avg:87.92ms
step:1065/1680 train_time:93640ms step_avg:87.92ms
step:1066/1680 train_time:93729ms step_avg:87.93ms
step:1067/1680 train_time:93817ms step_avg:87.93ms
step:1068/1680 train_time:93906ms step_avg:87.93ms
step:1069/1680 train_time:93995ms step_avg:87.93ms
step:1070/1680 train_time:94084ms step_avg:87.93ms
step:1071/1680 train_time:94172ms step_avg:87.93ms
step:1072/1680 train_time:94261ms step_avg:87.93ms
step:1073/1680 train_time:94350ms step_avg:87.93ms
step:1074/1680 train_time:94438ms step_avg:87.93ms
step:1075/1680 train_time:94526ms step_avg:87.93ms
step:1076/1680 train_time:94614ms step_avg:87.93ms
step:1077/1680 train_time:94703ms step_avg:87.93ms
step:1078/1680 train_time:94792ms step_avg:87.93ms
step:1079/1680 train_time:94879ms step_avg:87.93ms
step:1080/1680 train_time:94967ms step_avg:87.93ms
step:1081/1680 train_time:95057ms step_avg:87.93ms
step:1082/1680 train_time:95146ms step_avg:87.94ms
step:1083/1680 train_time:95235ms step_avg:87.94ms
step:1084/1680 train_time:95323ms step_avg:87.94ms
step:1085/1680 train_time:95413ms step_avg:87.94ms
step:1086/1680 train_time:95501ms step_avg:87.94ms
step:1087/1680 train_time:95589ms step_avg:87.94ms
step:1088/1680 train_time:95676ms step_avg:87.94ms
step:1089/1680 train_time:95765ms step_avg:87.94ms
step:1090/1680 train_time:95853ms step_avg:87.94ms
step:1091/1680 train_time:95941ms step_avg:87.94ms
step:1092/1680 train_time:96029ms step_avg:87.94ms
step:1093/1680 train_time:96119ms step_avg:87.94ms
step:1094/1680 train_time:96207ms step_avg:87.94ms
step:1095/1680 train_time:96296ms step_avg:87.94ms
step:1096/1680 train_time:96384ms step_avg:87.94ms
step:1097/1680 train_time:96474ms step_avg:87.94ms
step:1098/1680 train_time:96563ms step_avg:87.94ms
step:1099/1680 train_time:96652ms step_avg:87.95ms
step:1100/1680 train_time:96742ms step_avg:87.95ms
step:1101/1680 train_time:96830ms step_avg:87.95ms
step:1102/1680 train_time:96920ms step_avg:87.95ms
step:1103/1680 train_time:97009ms step_avg:87.95ms
step:1104/1680 train_time:97099ms step_avg:87.95ms
step:1105/1680 train_time:97188ms step_avg:87.95ms
step:1106/1680 train_time:97277ms step_avg:87.95ms
step:1107/1680 train_time:97366ms step_avg:87.95ms
step:1108/1680 train_time:97456ms step_avg:87.96ms
step:1109/1680 train_time:97546ms step_avg:87.96ms
step:1110/1680 train_time:97635ms step_avg:87.96ms
step:1111/1680 train_time:97724ms step_avg:87.96ms
step:1112/1680 train_time:97814ms step_avg:87.96ms
step:1113/1680 train_time:97904ms step_avg:87.96ms
step:1114/1680 train_time:97992ms step_avg:87.96ms
step:1115/1680 train_time:98081ms step_avg:87.96ms
step:1116/1680 train_time:98169ms step_avg:87.97ms
step:1117/1680 train_time:98260ms step_avg:87.97ms
step:1118/1680 train_time:98349ms step_avg:87.97ms
step:1119/1680 train_time:98438ms step_avg:87.97ms
step:1120/1680 train_time:98528ms step_avg:87.97ms
step:1121/1680 train_time:98618ms step_avg:87.97ms
step:1122/1680 train_time:98707ms step_avg:87.97ms
step:1123/1680 train_time:98796ms step_avg:87.97ms
step:1124/1680 train_time:98884ms step_avg:87.98ms
step:1125/1680 train_time:98973ms step_avg:87.98ms
step:1125/1680 val_loss:3.4164 train_time:99064ms step_avg:88.06ms
step:1126/1680 train_time:99083ms step_avg:88.00ms
step:1127/1680 train_time:99154ms step_avg:87.98ms
step:1128/1680 train_time:99244ms step_avg:87.98ms
step:1129/1680 train_time:99336ms step_avg:87.99ms
step:1130/1680 train_time:99427ms step_avg:87.99ms
step:1131/1680 train_time:99514ms step_avg:87.99ms
step:1132/1680 train_time:99602ms step_avg:87.99ms
step:1133/1680 train_time:99690ms step_avg:87.99ms
step:1134/1680 train_time:99779ms step_avg:87.99ms
step:1135/1680 train_time:99869ms step_avg:87.99ms
step:1136/1680 train_time:99959ms step_avg:87.99ms
step:1137/1680 train_time:100049ms step_avg:87.99ms
step:1138/1680 train_time:100141ms step_avg:88.00ms
step:1139/1680 train_time:100231ms step_avg:88.00ms
step:1140/1680 train_time:100322ms step_avg:88.00ms
step:1141/1680 train_time:100411ms step_avg:88.00ms
step:1142/1680 train_time:100501ms step_avg:88.00ms
step:1143/1680 train_time:100589ms step_avg:88.00ms
step:1144/1680 train_time:100677ms step_avg:88.00ms
step:1145/1680 train_time:100766ms step_avg:88.01ms
step:1146/1680 train_time:100855ms step_avg:88.01ms
step:1147/1680 train_time:100943ms step_avg:88.01ms
step:1148/1680 train_time:101033ms step_avg:88.01ms
step:1149/1680 train_time:101123ms step_avg:88.01ms
step:1150/1680 train_time:101212ms step_avg:88.01ms
step:1151/1680 train_time:101302ms step_avg:88.01ms
step:1152/1680 train_time:101391ms step_avg:88.01ms
step:1153/1680 train_time:101481ms step_avg:88.01ms
step:1154/1680 train_time:101570ms step_avg:88.02ms
step:1155/1680 train_time:101659ms step_avg:88.02ms
step:1156/1680 train_time:101748ms step_avg:88.02ms
step:1157/1680 train_time:101837ms step_avg:88.02ms
step:1158/1680 train_time:101926ms step_avg:88.02ms
step:1159/1680 train_time:102015ms step_avg:88.02ms
step:1160/1680 train_time:102106ms step_avg:88.02ms
step:1161/1680 train_time:102197ms step_avg:88.03ms
step:1162/1680 train_time:102286ms step_avg:88.03ms
step:1163/1680 train_time:102377ms step_avg:88.03ms
step:1164/1680 train_time:102466ms step_avg:88.03ms
step:1165/1680 train_time:102556ms step_avg:88.03ms
step:1166/1680 train_time:102644ms step_avg:88.03ms
step:1167/1680 train_time:102733ms step_avg:88.03ms
step:1168/1680 train_time:102821ms step_avg:88.03ms
step:1169/1680 train_time:102910ms step_avg:88.03ms
step:1170/1680 train_time:102999ms step_avg:88.03ms
step:1171/1680 train_time:103089ms step_avg:88.03ms
step:1172/1680 train_time:103180ms step_avg:88.04ms
step:1173/1680 train_time:103270ms step_avg:88.04ms
step:1174/1680 train_time:103359ms step_avg:88.04ms
step:1175/1680 train_time:103449ms step_avg:88.04ms
step:1176/1680 train_time:103537ms step_avg:88.04ms
step:1177/1680 train_time:103626ms step_avg:88.04ms
step:1178/1680 train_time:103715ms step_avg:88.04ms
step:1179/1680 train_time:103804ms step_avg:88.04ms
step:1180/1680 train_time:103893ms step_avg:88.04ms
step:1181/1680 train_time:103982ms step_avg:88.05ms
step:1182/1680 train_time:104071ms step_avg:88.05ms
step:1183/1680 train_time:104160ms step_avg:88.05ms
step:1184/1680 train_time:104249ms step_avg:88.05ms
step:1185/1680 train_time:104338ms step_avg:88.05ms
step:1186/1680 train_time:104427ms step_avg:88.05ms
step:1187/1680 train_time:104516ms step_avg:88.05ms
step:1188/1680 train_time:104605ms step_avg:88.05ms
step:1189/1680 train_time:104695ms step_avg:88.05ms
step:1190/1680 train_time:104784ms step_avg:88.05ms
step:1191/1680 train_time:104873ms step_avg:88.05ms
step:1192/1680 train_time:104962ms step_avg:88.06ms
step:1193/1680 train_time:105051ms step_avg:88.06ms
step:1194/1680 train_time:105140ms step_avg:88.06ms
step:1195/1680 train_time:105229ms step_avg:88.06ms
step:1196/1680 train_time:105319ms step_avg:88.06ms
step:1197/1680 train_time:105407ms step_avg:88.06ms
step:1198/1680 train_time:105497ms step_avg:88.06ms
step:1199/1680 train_time:105587ms step_avg:88.06ms
step:1200/1680 train_time:105677ms step_avg:88.06ms
step:1201/1680 train_time:105766ms step_avg:88.06ms
step:1202/1680 train_time:105856ms step_avg:88.07ms
step:1203/1680 train_time:105945ms step_avg:88.07ms
step:1204/1680 train_time:106035ms step_avg:88.07ms
step:1205/1680 train_time:106123ms step_avg:88.07ms
step:1206/1680 train_time:106213ms step_avg:88.07ms
step:1207/1680 train_time:106302ms step_avg:88.07ms
step:1208/1680 train_time:106391ms step_avg:88.07ms
step:1209/1680 train_time:106480ms step_avg:88.07ms
step:1210/1680 train_time:106570ms step_avg:88.07ms
step:1211/1680 train_time:106659ms step_avg:88.08ms
step:1212/1680 train_time:106749ms step_avg:88.08ms
step:1213/1680 train_time:106837ms step_avg:88.08ms
step:1214/1680 train_time:106927ms step_avg:88.08ms
step:1215/1680 train_time:107016ms step_avg:88.08ms
step:1216/1680 train_time:107105ms step_avg:88.08ms
step:1217/1680 train_time:107194ms step_avg:88.08ms
step:1218/1680 train_time:107283ms step_avg:88.08ms
step:1219/1680 train_time:107371ms step_avg:88.08ms
step:1220/1680 train_time:107460ms step_avg:88.08ms
step:1221/1680 train_time:107549ms step_avg:88.08ms
step:1222/1680 train_time:107639ms step_avg:88.08ms
step:1223/1680 train_time:107728ms step_avg:88.08ms
step:1224/1680 train_time:107817ms step_avg:88.09ms
step:1225/1680 train_time:107906ms step_avg:88.09ms
step:1226/1680 train_time:107997ms step_avg:88.09ms
step:1227/1680 train_time:108085ms step_avg:88.09ms
step:1228/1680 train_time:108175ms step_avg:88.09ms
step:1229/1680 train_time:108263ms step_avg:88.09ms
step:1230/1680 train_time:108353ms step_avg:88.09ms
step:1231/1680 train_time:108442ms step_avg:88.09ms
step:1232/1680 train_time:108531ms step_avg:88.09ms
step:1233/1680 train_time:108620ms step_avg:88.09ms
step:1234/1680 train_time:108710ms step_avg:88.10ms
step:1235/1680 train_time:108800ms step_avg:88.10ms
step:1236/1680 train_time:108889ms step_avg:88.10ms
step:1237/1680 train_time:108979ms step_avg:88.10ms
step:1238/1680 train_time:109069ms step_avg:88.10ms
step:1239/1680 train_time:109160ms step_avg:88.10ms
step:1240/1680 train_time:109249ms step_avg:88.10ms
step:1241/1680 train_time:109339ms step_avg:88.11ms
step:1242/1680 train_time:109427ms step_avg:88.11ms
step:1243/1680 train_time:109517ms step_avg:88.11ms
step:1244/1680 train_time:109606ms step_avg:88.11ms
step:1245/1680 train_time:109695ms step_avg:88.11ms
step:1246/1680 train_time:109784ms step_avg:88.11ms
step:1247/1680 train_time:109873ms step_avg:88.11ms
step:1248/1680 train_time:109963ms step_avg:88.11ms
step:1249/1680 train_time:110052ms step_avg:88.11ms
step:1250/1680 train_time:110142ms step_avg:88.11ms
step:1250/1680 val_loss:3.3775 train_time:110232ms step_avg:88.19ms
step:1251/1680 train_time:110250ms step_avg:88.13ms
step:1252/1680 train_time:110326ms step_avg:88.12ms
step:1253/1680 train_time:110420ms step_avg:88.12ms
step:1254/1680 train_time:110510ms step_avg:88.13ms
step:1255/1680 train_time:110599ms step_avg:88.13ms
step:1256/1680 train_time:110688ms step_avg:88.13ms
step:1257/1680 train_time:110776ms step_avg:88.13ms
step:1258/1680 train_time:110863ms step_avg:88.13ms
step:1259/1680 train_time:110951ms step_avg:88.13ms
step:1260/1680 train_time:111040ms step_avg:88.13ms
step:1261/1680 train_time:111129ms step_avg:88.13ms
step:1262/1680 train_time:111220ms step_avg:88.13ms
step:1263/1680 train_time:111311ms step_avg:88.13ms
step:1264/1680 train_time:111403ms step_avg:88.14ms
step:1265/1680 train_time:111493ms step_avg:88.14ms
step:1266/1680 train_time:111582ms step_avg:88.14ms
step:1267/1680 train_time:111671ms step_avg:88.14ms
step:1268/1680 train_time:111759ms step_avg:88.14ms
step:1269/1680 train_time:111847ms step_avg:88.14ms
step:1270/1680 train_time:111936ms step_avg:88.14ms
step:1271/1680 train_time:112023ms step_avg:88.14ms
step:1272/1680 train_time:112112ms step_avg:88.14ms
step:1273/1680 train_time:112202ms step_avg:88.14ms
step:1274/1680 train_time:112293ms step_avg:88.14ms
step:1275/1680 train_time:112384ms step_avg:88.14ms
step:1276/1680 train_time:112474ms step_avg:88.15ms
step:1277/1680 train_time:112564ms step_avg:88.15ms
step:1278/1680 train_time:112652ms step_avg:88.15ms
step:1279/1680 train_time:112741ms step_avg:88.15ms
step:1280/1680 train_time:112830ms step_avg:88.15ms
step:1281/1680 train_time:112918ms step_avg:88.15ms
step:1282/1680 train_time:113008ms step_avg:88.15ms
step:1283/1680 train_time:113097ms step_avg:88.15ms
step:1284/1680 train_time:113186ms step_avg:88.15ms
step:1285/1680 train_time:113276ms step_avg:88.15ms
step:1286/1680 train_time:113366ms step_avg:88.15ms
step:1287/1680 train_time:113457ms step_avg:88.16ms
step:1288/1680 train_time:113546ms step_avg:88.16ms
step:1289/1680 train_time:113636ms step_avg:88.16ms
step:1290/1680 train_time:113725ms step_avg:88.16ms
step:1291/1680 train_time:113814ms step_avg:88.16ms
step:1292/1680 train_time:113902ms step_avg:88.16ms
step:1293/1680 train_time:113991ms step_avg:88.16ms
step:1294/1680 train_time:114080ms step_avg:88.16ms
step:1295/1680 train_time:114170ms step_avg:88.16ms
step:1296/1680 train_time:114260ms step_avg:88.16ms
step:1297/1680 train_time:114350ms step_avg:88.17ms
step:1298/1680 train_time:114439ms step_avg:88.17ms
step:1299/1680 train_time:114529ms step_avg:88.17ms
step:1300/1680 train_time:114617ms step_avg:88.17ms
step:1301/1680 train_time:114707ms step_avg:88.17ms
step:1302/1680 train_time:114798ms step_avg:88.17ms
step:1303/1680 train_time:114887ms step_avg:88.17ms
step:1304/1680 train_time:114976ms step_avg:88.17ms
step:1305/1680 train_time:115064ms step_avg:88.17ms
step:1306/1680 train_time:115154ms step_avg:88.17ms
step:1307/1680 train_time:115242ms step_avg:88.17ms
step:1308/1680 train_time:115332ms step_avg:88.17ms
step:1309/1680 train_time:115421ms step_avg:88.18ms
step:1310/1680 train_time:115512ms step_avg:88.18ms
step:1311/1680 train_time:115601ms step_avg:88.18ms
step:1312/1680 train_time:115690ms step_avg:88.18ms
step:1313/1680 train_time:115779ms step_avg:88.18ms
step:1314/1680 train_time:115868ms step_avg:88.18ms
step:1315/1680 train_time:115957ms step_avg:88.18ms
step:1316/1680 train_time:116046ms step_avg:88.18ms
step:1317/1680 train_time:116135ms step_avg:88.18ms
step:1318/1680 train_time:116225ms step_avg:88.18ms
step:1319/1680 train_time:116314ms step_avg:88.18ms
step:1320/1680 train_time:116404ms step_avg:88.18ms
step:1321/1680 train_time:116494ms step_avg:88.19ms
step:1322/1680 train_time:116583ms step_avg:88.19ms
step:1323/1680 train_time:116672ms step_avg:88.19ms
step:1324/1680 train_time:116761ms step_avg:88.19ms
step:1325/1680 train_time:116850ms step_avg:88.19ms
step:1326/1680 train_time:116939ms step_avg:88.19ms
step:1327/1680 train_time:117028ms step_avg:88.19ms
step:1328/1680 train_time:117118ms step_avg:88.19ms
step:1329/1680 train_time:117207ms step_avg:88.19ms
step:1330/1680 train_time:117297ms step_avg:88.19ms
step:1331/1680 train_time:117387ms step_avg:88.19ms
step:1332/1680 train_time:117476ms step_avg:88.20ms
step:1333/1680 train_time:117565ms step_avg:88.20ms
step:1334/1680 train_time:117655ms step_avg:88.20ms
step:1335/1680 train_time:117744ms step_avg:88.20ms
step:1336/1680 train_time:117833ms step_avg:88.20ms
step:1337/1680 train_time:117923ms step_avg:88.20ms
step:1338/1680 train_time:118012ms step_avg:88.20ms
step:1339/1680 train_time:118102ms step_avg:88.20ms
step:1340/1680 train_time:118192ms step_avg:88.20ms
step:1341/1680 train_time:118281ms step_avg:88.20ms
step:1342/1680 train_time:118370ms step_avg:88.20ms
step:1343/1680 train_time:118460ms step_avg:88.21ms
step:1344/1680 train_time:118549ms step_avg:88.21ms
step:1345/1680 train_time:118638ms step_avg:88.21ms
step:1346/1680 train_time:118728ms step_avg:88.21ms
step:1347/1680 train_time:118818ms step_avg:88.21ms
step:1348/1680 train_time:118907ms step_avg:88.21ms
step:1349/1680 train_time:118997ms step_avg:88.21ms
step:1350/1680 train_time:119087ms step_avg:88.21ms
step:1351/1680 train_time:119177ms step_avg:88.21ms
step:1352/1680 train_time:119267ms step_avg:88.22ms
step:1353/1680 train_time:119358ms step_avg:88.22ms
step:1354/1680 train_time:119447ms step_avg:88.22ms
step:1355/1680 train_time:119536ms step_avg:88.22ms
step:1356/1680 train_time:119625ms step_avg:88.22ms
step:1357/1680 train_time:119715ms step_avg:88.22ms
step:1358/1680 train_time:119805ms step_avg:88.22ms
step:1359/1680 train_time:119893ms step_avg:88.22ms
step:1360/1680 train_time:119982ms step_avg:88.22ms
step:1361/1680 train_time:120071ms step_avg:88.22ms
step:1362/1680 train_time:120161ms step_avg:88.22ms
step:1363/1680 train_time:120250ms step_avg:88.22ms
step:1364/1680 train_time:120340ms step_avg:88.23ms
step:1365/1680 train_time:120428ms step_avg:88.23ms
step:1366/1680 train_time:120517ms step_avg:88.23ms
step:1367/1680 train_time:120607ms step_avg:88.23ms
step:1368/1680 train_time:120697ms step_avg:88.23ms
step:1369/1680 train_time:120787ms step_avg:88.23ms
step:1370/1680 train_time:120876ms step_avg:88.23ms
step:1371/1680 train_time:120965ms step_avg:88.23ms
step:1372/1680 train_time:121054ms step_avg:88.23ms
step:1373/1680 train_time:121144ms step_avg:88.23ms
step:1374/1680 train_time:121233ms step_avg:88.23ms
step:1375/1680 train_time:121323ms step_avg:88.23ms
step:1375/1680 val_loss:3.3429 train_time:121413ms step_avg:88.30ms
step:1376/1680 train_time:121431ms step_avg:88.25ms
step:1377/1680 train_time:121504ms step_avg:88.24ms
step:1378/1680 train_time:121594ms step_avg:88.24ms
step:1379/1680 train_time:121683ms step_avg:88.24ms
step:1380/1680 train_time:121771ms step_avg:88.24ms
step:1381/1680 train_time:121860ms step_avg:88.24ms
step:1382/1680 train_time:121948ms step_avg:88.24ms
step:1383/1680 train_time:122037ms step_avg:88.24ms
step:1384/1680 train_time:122126ms step_avg:88.24ms
step:1385/1680 train_time:122214ms step_avg:88.24ms
step:1386/1680 train_time:122304ms step_avg:88.24ms
step:1387/1680 train_time:122394ms step_avg:88.24ms
step:1388/1680 train_time:122485ms step_avg:88.25ms
step:1389/1680 train_time:122576ms step_avg:88.25ms
step:1390/1680 train_time:122666ms step_avg:88.25ms
step:1391/1680 train_time:122755ms step_avg:88.25ms
step:1392/1680 train_time:122844ms step_avg:88.25ms
step:1393/1680 train_time:122933ms step_avg:88.25ms
step:1394/1680 train_time:123021ms step_avg:88.25ms
step:1395/1680 train_time:123110ms step_avg:88.25ms
step:1396/1680 train_time:123198ms step_avg:88.25ms
step:1397/1680 train_time:123287ms step_avg:88.25ms
step:1398/1680 train_time:123378ms step_avg:88.25ms
step:1399/1680 train_time:123468ms step_avg:88.25ms
step:1400/1680 train_time:123558ms step_avg:88.26ms
step:1401/1680 train_time:123648ms step_avg:88.26ms
step:1402/1680 train_time:123737ms step_avg:88.26ms
step:1403/1680 train_time:123827ms step_avg:88.26ms
step:1404/1680 train_time:123916ms step_avg:88.26ms
step:1405/1680 train_time:124004ms step_avg:88.26ms
step:1406/1680 train_time:124092ms step_avg:88.26ms
step:1407/1680 train_time:124181ms step_avg:88.26ms
step:1408/1680 train_time:124270ms step_avg:88.26ms
step:1409/1680 train_time:124360ms step_avg:88.26ms
step:1410/1680 train_time:124451ms step_avg:88.26ms
step:1411/1680 train_time:124540ms step_avg:88.26ms
step:1412/1680 train_time:124631ms step_avg:88.27ms
step:1413/1680 train_time:124720ms step_avg:88.27ms
step:1414/1680 train_time:124809ms step_avg:88.27ms
step:1415/1680 train_time:124898ms step_avg:88.27ms
step:1416/1680 train_time:124986ms step_avg:88.27ms
step:1417/1680 train_time:125075ms step_avg:88.27ms
step:1418/1680 train_time:125164ms step_avg:88.27ms
step:1419/1680 train_time:125253ms step_avg:88.27ms
step:1420/1680 train_time:125342ms step_avg:88.27ms
step:1421/1680 train_time:125431ms step_avg:88.27ms
step:1422/1680 train_time:125521ms step_avg:88.27ms
step:1423/1680 train_time:125611ms step_avg:88.27ms
step:1424/1680 train_time:125700ms step_avg:88.27ms
step:1425/1680 train_time:125790ms step_avg:88.27ms
step:1426/1680 train_time:125879ms step_avg:88.27ms
step:1427/1680 train_time:125969ms step_avg:88.28ms
step:1428/1680 train_time:126058ms step_avg:88.28ms
step:1429/1680 train_time:126147ms step_avg:88.28ms
step:1430/1680 train_time:126236ms step_avg:88.28ms
step:1431/1680 train_time:126325ms step_avg:88.28ms
step:1432/1680 train_time:126414ms step_avg:88.28ms
step:1433/1680 train_time:126504ms step_avg:88.28ms
step:1434/1680 train_time:126593ms step_avg:88.28ms
step:1435/1680 train_time:126682ms step_avg:88.28ms
step:1436/1680 train_time:126772ms step_avg:88.28ms
step:1437/1680 train_time:126862ms step_avg:88.28ms
step:1438/1680 train_time:126951ms step_avg:88.28ms
step:1439/1680 train_time:127040ms step_avg:88.28ms
step:1440/1680 train_time:127129ms step_avg:88.28ms
step:1441/1680 train_time:127218ms step_avg:88.28ms
step:1442/1680 train_time:127306ms step_avg:88.28ms
step:1443/1680 train_time:127396ms step_avg:88.29ms
step:1444/1680 train_time:127485ms step_avg:88.29ms
step:1445/1680 train_time:127574ms step_avg:88.29ms
step:1446/1680 train_time:127664ms step_avg:88.29ms
step:1447/1680 train_time:127753ms step_avg:88.29ms
step:1448/1680 train_time:127843ms step_avg:88.29ms
step:1449/1680 train_time:127933ms step_avg:88.29ms
step:1450/1680 train_time:128023ms step_avg:88.29ms
step:1451/1680 train_time:128112ms step_avg:88.29ms
step:1452/1680 train_time:128201ms step_avg:88.29ms
step:1453/1680 train_time:128290ms step_avg:88.29ms
step:1454/1680 train_time:128379ms step_avg:88.29ms
step:1455/1680 train_time:128469ms step_avg:88.29ms
step:1456/1680 train_time:128559ms step_avg:88.30ms
step:1457/1680 train_time:128648ms step_avg:88.30ms
step:1458/1680 train_time:128738ms step_avg:88.30ms
step:1459/1680 train_time:128828ms step_avg:88.30ms
step:1460/1680 train_time:128917ms step_avg:88.30ms
step:1461/1680 train_time:129007ms step_avg:88.30ms
step:1462/1680 train_time:129096ms step_avg:88.30ms
step:1463/1680 train_time:129185ms step_avg:88.30ms
step:1464/1680 train_time:129274ms step_avg:88.30ms
step:1465/1680 train_time:129363ms step_avg:88.30ms
step:1466/1680 train_time:129452ms step_avg:88.30ms
step:1467/1680 train_time:129541ms step_avg:88.30ms
step:1468/1680 train_time:129631ms step_avg:88.30ms
step:1469/1680 train_time:129721ms step_avg:88.31ms
step:1470/1680 train_time:129809ms step_avg:88.31ms
step:1471/1680 train_time:129898ms step_avg:88.31ms
step:1472/1680 train_time:129988ms step_avg:88.31ms
step:1473/1680 train_time:130077ms step_avg:88.31ms
step:1474/1680 train_time:130167ms step_avg:88.31ms
step:1475/1680 train_time:130256ms step_avg:88.31ms
step:1476/1680 train_time:130346ms step_avg:88.31ms
step:1477/1680 train_time:130435ms step_avg:88.31ms
step:1478/1680 train_time:130524ms step_avg:88.31ms
step:1479/1680 train_time:130613ms step_avg:88.31ms
step:1480/1680 train_time:130703ms step_avg:88.31ms
step:1481/1680 train_time:130792ms step_avg:88.31ms
step:1482/1680 train_time:130881ms step_avg:88.31ms
step:1483/1680 train_time:130971ms step_avg:88.31ms
step:1484/1680 train_time:131059ms step_avg:88.31ms
step:1485/1680 train_time:131150ms step_avg:88.32ms
step:1486/1680 train_time:131240ms step_avg:88.32ms
step:1487/1680 train_time:131329ms step_avg:88.32ms
step:1488/1680 train_time:131417ms step_avg:88.32ms
step:1489/1680 train_time:131507ms step_avg:88.32ms
step:1490/1680 train_time:131595ms step_avg:88.32ms
step:1491/1680 train_time:131685ms step_avg:88.32ms
step:1492/1680 train_time:131774ms step_avg:88.32ms
step:1493/1680 train_time:131864ms step_avg:88.32ms
step:1494/1680 train_time:131953ms step_avg:88.32ms
step:1495/1680 train_time:132042ms step_avg:88.32ms
step:1496/1680 train_time:132131ms step_avg:88.32ms
step:1497/1680 train_time:132220ms step_avg:88.32ms
step:1498/1680 train_time:132310ms step_avg:88.32ms
step:1499/1680 train_time:132400ms step_avg:88.33ms
step:1500/1680 train_time:132490ms step_avg:88.33ms
step:1500/1680 val_loss:3.3130 train_time:132580ms step_avg:88.39ms
step:1501/1680 train_time:132598ms step_avg:88.34ms
step:1502/1680 train_time:132673ms step_avg:88.33ms
step:1503/1680 train_time:132766ms step_avg:88.33ms
step:1504/1680 train_time:132857ms step_avg:88.34ms
step:1505/1680 train_time:132946ms step_avg:88.34ms
step:1506/1680 train_time:133034ms step_avg:88.34ms
step:1507/1680 train_time:133122ms step_avg:88.34ms
step:1508/1680 train_time:133211ms step_avg:88.34ms
step:1509/1680 train_time:133299ms step_avg:88.34ms
step:1510/1680 train_time:133388ms step_avg:88.34ms
step:1511/1680 train_time:133477ms step_avg:88.34ms
step:1512/1680 train_time:133568ms step_avg:88.34ms
step:1513/1680 train_time:133658ms step_avg:88.34ms
step:1514/1680 train_time:133750ms step_avg:88.34ms
step:1515/1680 train_time:133841ms step_avg:88.34ms
step:1516/1680 train_time:133930ms step_avg:88.34ms
step:1517/1680 train_time:134018ms step_avg:88.34ms
step:1518/1680 train_time:134108ms step_avg:88.35ms
step:1519/1680 train_time:134197ms step_avg:88.35ms
step:1520/1680 train_time:134286ms step_avg:88.35ms
step:1521/1680 train_time:134374ms step_avg:88.35ms
step:1522/1680 train_time:134463ms step_avg:88.35ms
step:1523/1680 train_time:134552ms step_avg:88.35ms
step:1524/1680 train_time:134642ms step_avg:88.35ms
step:1525/1680 train_time:134733ms step_avg:88.35ms
step:1526/1680 train_time:134823ms step_avg:88.35ms
step:1527/1680 train_time:134913ms step_avg:88.35ms
step:1528/1680 train_time:135003ms step_avg:88.35ms
step:1529/1680 train_time:135092ms step_avg:88.35ms
step:1530/1680 train_time:135181ms step_avg:88.35ms
step:1531/1680 train_time:135270ms step_avg:88.35ms
step:1532/1680 train_time:135359ms step_avg:88.35ms
step:1533/1680 train_time:135448ms step_avg:88.35ms
step:1534/1680 train_time:135537ms step_avg:88.36ms
step:1535/1680 train_time:135627ms step_avg:88.36ms
step:1536/1680 train_time:135716ms step_avg:88.36ms
step:1537/1680 train_time:135806ms step_avg:88.36ms
step:1538/1680 train_time:135896ms step_avg:88.36ms
step:1539/1680 train_time:135985ms step_avg:88.36ms
step:1540/1680 train_time:136074ms step_avg:88.36ms
step:1541/1680 train_time:136162ms step_avg:88.36ms
step:1542/1680 train_time:136251ms step_avg:88.36ms
step:1543/1680 train_time:136340ms step_avg:88.36ms
step:1544/1680 train_time:136429ms step_avg:88.36ms
step:1545/1680 train_time:136519ms step_avg:88.36ms
step:1546/1680 train_time:136609ms step_avg:88.36ms
step:1547/1680 train_time:136697ms step_avg:88.36ms
step:1548/1680 train_time:136787ms step_avg:88.36ms
step:1549/1680 train_time:136876ms step_avg:88.36ms
step:1550/1680 train_time:136966ms step_avg:88.37ms
step:1551/1680 train_time:137055ms step_avg:88.37ms
step:1552/1680 train_time:137144ms step_avg:88.37ms
step:1553/1680 train_time:137233ms step_avg:88.37ms
step:1554/1680 train_time:137321ms step_avg:88.37ms
step:1555/1680 train_time:137411ms step_avg:88.37ms
step:1556/1680 train_time:137500ms step_avg:88.37ms
step:1557/1680 train_time:137590ms step_avg:88.37ms
step:1558/1680 train_time:137680ms step_avg:88.37ms
step:1559/1680 train_time:137772ms step_avg:88.37ms
step:1560/1680 train_time:137863ms step_avg:88.37ms
step:1561/1680 train_time:137952ms step_avg:88.37ms
step:1562/1680 train_time:138041ms step_avg:88.37ms
step:1563/1680 train_time:138131ms step_avg:88.38ms
step:1564/1680 train_time:138219ms step_avg:88.38ms
step:1565/1680 train_time:138308ms step_avg:88.38ms
step:1566/1680 train_time:138397ms step_avg:88.38ms
step:1567/1680 train_time:138487ms step_avg:88.38ms
step:1568/1680 train_time:138576ms step_avg:88.38ms
step:1569/1680 train_time:138665ms step_avg:88.38ms
step:1570/1680 train_time:138755ms step_avg:88.38ms
step:1571/1680 train_time:138844ms step_avg:88.38ms
step:1572/1680 train_time:138932ms step_avg:88.38ms
step:1573/1680 train_time:139022ms step_avg:88.38ms
step:1574/1680 train_time:139112ms step_avg:88.38ms
step:1575/1680 train_time:139201ms step_avg:88.38ms
step:1576/1680 train_time:139290ms step_avg:88.38ms
step:1577/1680 train_time:139380ms step_avg:88.38ms
step:1578/1680 train_time:139470ms step_avg:88.38ms
step:1579/1680 train_time:139559ms step_avg:88.38ms
step:1580/1680 train_time:139648ms step_avg:88.38ms
step:1581/1680 train_time:139737ms step_avg:88.39ms
step:1582/1680 train_time:139826ms step_avg:88.39ms
step:1583/1680 train_time:139915ms step_avg:88.39ms
step:1584/1680 train_time:140005ms step_avg:88.39ms
step:1585/1680 train_time:140094ms step_avg:88.39ms
step:1586/1680 train_time:140184ms step_avg:88.39ms
step:1587/1680 train_time:140273ms step_avg:88.39ms
step:1588/1680 train_time:140363ms step_avg:88.39ms
step:1589/1680 train_time:140453ms step_avg:88.39ms
step:1590/1680 train_time:140543ms step_avg:88.39ms
step:1591/1680 train_time:140633ms step_avg:88.39ms
step:1592/1680 train_time:140721ms step_avg:88.39ms
step:1593/1680 train_time:140812ms step_avg:88.39ms
step:1594/1680 train_time:140901ms step_avg:88.39ms
step:1595/1680 train_time:140992ms step_avg:88.40ms
step:1596/1680 train_time:141082ms step_avg:88.40ms
step:1597/1680 train_time:141172ms step_avg:88.40ms
step:1598/1680 train_time:141262ms step_avg:88.40ms
step:1599/1680 train_time:141350ms step_avg:88.40ms
step:1600/1680 train_time:141439ms step_avg:88.40ms
step:1601/1680 train_time:141528ms step_avg:88.40ms
step:1602/1680 train_time:141616ms step_avg:88.40ms
step:1603/1680 train_time:141705ms step_avg:88.40ms
step:1604/1680 train_time:141795ms step_avg:88.40ms
step:1605/1680 train_time:141884ms step_avg:88.40ms
step:1606/1680 train_time:141974ms step_avg:88.40ms
step:1607/1680 train_time:142064ms step_avg:88.40ms
step:1608/1680 train_time:142153ms step_avg:88.40ms
step:1609/1680 train_time:142243ms step_avg:88.40ms
step:1610/1680 train_time:142333ms step_avg:88.41ms
step:1611/1680 train_time:142422ms step_avg:88.41ms
step:1612/1680 train_time:142512ms step_avg:88.41ms
step:1613/1680 train_time:142601ms step_avg:88.41ms
step:1614/1680 train_time:142691ms step_avg:88.41ms
step:1615/1680 train_time:142780ms step_avg:88.41ms
step:1616/1680 train_time:142870ms step_avg:88.41ms
step:1617/1680 train_time:142959ms step_avg:88.41ms
step:1618/1680 train_time:143048ms step_avg:88.41ms
step:1619/1680 train_time:143137ms step_avg:88.41ms
step:1620/1680 train_time:143227ms step_avg:88.41ms
step:1621/1680 train_time:143316ms step_avg:88.41ms
step:1622/1680 train_time:143406ms step_avg:88.41ms
step:1623/1680 train_time:143496ms step_avg:88.41ms
step:1624/1680 train_time:143586ms step_avg:88.42ms
step:1625/1680 train_time:143676ms step_avg:88.42ms
step:1625/1680 val_loss:3.2891 train_time:143767ms step_avg:88.47ms
step:1626/1680 train_time:143786ms step_avg:88.43ms
step:1627/1680 train_time:143859ms step_avg:88.42ms
step:1628/1680 train_time:143952ms step_avg:88.42ms
step:1629/1680 train_time:144041ms step_avg:88.42ms
step:1630/1680 train_time:144129ms step_avg:88.42ms
step:1631/1680 train_time:144217ms step_avg:88.42ms
step:1632/1680 train_time:144306ms step_avg:88.42ms
step:1633/1680 train_time:144395ms step_avg:88.42ms
step:1634/1680 train_time:144483ms step_avg:88.42ms
step:1635/1680 train_time:144573ms step_avg:88.42ms
step:1636/1680 train_time:144662ms step_avg:88.42ms
step:1637/1680 train_time:144752ms step_avg:88.43ms
step:1638/1680 train_time:144843ms step_avg:88.43ms
step:1639/1680 train_time:144933ms step_avg:88.43ms
step:1640/1680 train_time:145024ms step_avg:88.43ms
step:1641/1680 train_time:145115ms step_avg:88.43ms
step:1642/1680 train_time:145205ms step_avg:88.43ms
step:1643/1680 train_time:145294ms step_avg:88.43ms
step:1644/1680 train_time:145383ms step_avg:88.43ms
step:1645/1680 train_time:145471ms step_avg:88.43ms
step:1646/1680 train_time:145561ms step_avg:88.43ms
step:1647/1680 train_time:145649ms step_avg:88.43ms
step:1648/1680 train_time:145739ms step_avg:88.43ms
step:1649/1680 train_time:145830ms step_avg:88.44ms
step:1650/1680 train_time:145919ms step_avg:88.44ms
step:1651/1680 train_time:146009ms step_avg:88.44ms
step:1652/1680 train_time:146099ms step_avg:88.44ms
step:1653/1680 train_time:146188ms step_avg:88.44ms
step:1654/1680 train_time:146277ms step_avg:88.44ms
step:1655/1680 train_time:146366ms step_avg:88.44ms
step:1656/1680 train_time:146455ms step_avg:88.44ms
step:1657/1680 train_time:146544ms step_avg:88.44ms
step:1658/1680 train_time:146633ms step_avg:88.44ms
step:1659/1680 train_time:146724ms step_avg:88.44ms
step:1660/1680 train_time:146814ms step_avg:88.44ms
step:1661/1680 train_time:146905ms step_avg:88.44ms
step:1662/1680 train_time:146995ms step_avg:88.44ms
step:1663/1680 train_time:147085ms step_avg:88.45ms
step:1664/1680 train_time:147174ms step_avg:88.45ms
step:1665/1680 train_time:147265ms step_avg:88.45ms
step:1666/1680 train_time:147354ms step_avg:88.45ms
step:1667/1680 train_time:147443ms step_avg:88.45ms
step:1668/1680 train_time:147532ms step_avg:88.45ms
step:1669/1680 train_time:147622ms step_avg:88.45ms
step:1670/1680 train_time:147711ms step_avg:88.45ms
step:1671/1680 train_time:147802ms step_avg:88.45ms
step:1672/1680 train_time:147892ms step_avg:88.45ms
step:1673/1680 train_time:147982ms step_avg:88.45ms
step:1674/1680 train_time:148072ms step_avg:88.45ms
step:1675/1680 train_time:148162ms step_avg:88.45ms
step:1676/1680 train_time:148251ms step_avg:88.46ms
step:1677/1680 train_time:148340ms step_avg:88.46ms
step:1678/1680 train_time:148429ms step_avg:88.46ms
step:1679/1680 train_time:148518ms step_avg:88.46ms
step:1680/1680 train_time:148608ms step_avg:88.46ms
step:1680/1680 val_loss:3.2782 train_time:148698ms step_avg:88.51ms
peak memory allocated: 30760 MiB reserved: 45974 MiB

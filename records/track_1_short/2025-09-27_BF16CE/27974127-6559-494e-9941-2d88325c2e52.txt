import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 13:11:36 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            117W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    170107      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    170108      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    170109      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    170110      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    170111      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    170112      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    170113      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    170114      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    170108      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    170109      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    170110      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    170111      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    170112      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    170113      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    170114      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.17ms
step:1/1680 train_time:145ms step_avg:144.63ms
step:2/1680 train_time:166ms step_avg:82.77ms
step:3/1680 train_time:228ms step_avg:76.13ms
step:4/1680 train_time:314ms step_avg:78.39ms
step:5/1680 train_time:399ms step_avg:79.86ms
step:6/1680 train_time:485ms step_avg:80.85ms
step:7/1680 train_time:572ms step_avg:81.70ms
step:8/1680 train_time:658ms step_avg:82.26ms
step:9/1680 train_time:744ms step_avg:82.67ms
step:10/1680 train_time:830ms step_avg:83.03ms
step:11/1680 train_time:916ms step_avg:83.30ms
step:12/1680 train_time:1003ms step_avg:83.61ms
step:13/1680 train_time:1094ms step_avg:84.17ms
step:14/1680 train_time:1185ms step_avg:84.61ms
step:15/1680 train_time:1272ms step_avg:84.83ms
step:16/1680 train_time:1360ms step_avg:84.99ms
step:17/1680 train_time:1446ms step_avg:85.08ms
step:18/1680 train_time:1533ms step_avg:85.16ms
step:19/1680 train_time:1620ms step_avg:85.26ms
step:20/1680 train_time:1706ms step_avg:85.31ms
step:21/1680 train_time:1792ms step_avg:85.35ms
step:22/1680 train_time:1879ms step_avg:85.41ms
step:23/1680 train_time:1966ms step_avg:85.49ms
step:24/1680 train_time:2055ms step_avg:85.62ms
step:25/1680 train_time:2144ms step_avg:85.76ms
step:26/1680 train_time:2233ms step_avg:85.87ms
step:27/1680 train_time:2321ms step_avg:85.95ms
step:28/1680 train_time:2408ms step_avg:86.01ms
step:29/1680 train_time:2495ms step_avg:86.04ms
step:30/1680 train_time:2582ms step_avg:86.08ms
step:31/1680 train_time:2670ms step_avg:86.12ms
step:32/1680 train_time:2757ms step_avg:86.14ms
step:33/1680 train_time:2843ms step_avg:86.16ms
step:34/1680 train_time:2930ms step_avg:86.18ms
step:35/1680 train_time:3017ms step_avg:86.20ms
step:36/1680 train_time:3105ms step_avg:86.24ms
step:37/1680 train_time:3192ms step_avg:86.28ms
step:38/1680 train_time:3282ms step_avg:86.37ms
step:39/1680 train_time:3370ms step_avg:86.40ms
step:40/1680 train_time:3457ms step_avg:86.42ms
step:41/1680 train_time:3543ms step_avg:86.42ms
step:42/1680 train_time:3631ms step_avg:86.46ms
step:43/1680 train_time:3718ms step_avg:86.48ms
step:44/1680 train_time:3805ms step_avg:86.48ms
step:45/1680 train_time:3892ms step_avg:86.48ms
step:46/1680 train_time:3979ms step_avg:86.50ms
step:47/1680 train_time:4066ms step_avg:86.52ms
step:48/1680 train_time:4153ms step_avg:86.53ms
step:49/1680 train_time:4242ms step_avg:86.57ms
step:50/1680 train_time:4330ms step_avg:86.61ms
step:51/1680 train_time:4418ms step_avg:86.63ms
step:52/1680 train_time:4506ms step_avg:86.65ms
step:53/1680 train_time:4593ms step_avg:86.66ms
step:54/1680 train_time:4680ms step_avg:86.66ms
step:55/1680 train_time:4767ms step_avg:86.67ms
step:56/1680 train_time:4854ms step_avg:86.68ms
step:57/1680 train_time:4941ms step_avg:86.68ms
step:58/1680 train_time:5028ms step_avg:86.69ms
step:59/1680 train_time:5116ms step_avg:86.70ms
step:60/1680 train_time:5203ms step_avg:86.72ms
step:61/1680 train_time:5291ms step_avg:86.74ms
step:62/1680 train_time:5379ms step_avg:86.76ms
step:63/1680 train_time:5466ms step_avg:86.77ms
step:64/1680 train_time:5554ms step_avg:86.78ms
step:65/1680 train_time:5642ms step_avg:86.79ms
step:66/1680 train_time:5728ms step_avg:86.79ms
step:67/1680 train_time:5815ms step_avg:86.80ms
step:68/1680 train_time:5902ms step_avg:86.79ms
step:69/1680 train_time:5989ms step_avg:86.80ms
step:70/1680 train_time:6076ms step_avg:86.80ms
step:71/1680 train_time:6164ms step_avg:86.82ms
step:72/1680 train_time:6252ms step_avg:86.83ms
step:73/1680 train_time:6339ms step_avg:86.84ms
step:74/1680 train_time:6426ms step_avg:86.84ms
step:75/1680 train_time:6515ms step_avg:86.86ms
step:76/1680 train_time:6602ms step_avg:86.87ms
step:77/1680 train_time:6688ms step_avg:86.86ms
step:78/1680 train_time:6776ms step_avg:86.87ms
step:79/1680 train_time:6862ms step_avg:86.87ms
step:80/1680 train_time:6950ms step_avg:86.87ms
step:81/1680 train_time:7036ms step_avg:86.87ms
step:82/1680 train_time:7124ms step_avg:86.87ms
step:83/1680 train_time:7211ms step_avg:86.88ms
step:84/1680 train_time:7298ms step_avg:86.88ms
step:85/1680 train_time:7385ms step_avg:86.89ms
step:86/1680 train_time:7473ms step_avg:86.90ms
step:87/1680 train_time:7561ms step_avg:86.90ms
step:88/1680 train_time:7648ms step_avg:86.91ms
step:89/1680 train_time:7736ms step_avg:86.92ms
step:90/1680 train_time:7822ms step_avg:86.92ms
step:91/1680 train_time:7910ms step_avg:86.92ms
step:92/1680 train_time:7997ms step_avg:86.93ms
step:93/1680 train_time:8084ms step_avg:86.93ms
step:94/1680 train_time:8171ms step_avg:86.93ms
step:95/1680 train_time:8258ms step_avg:86.93ms
step:96/1680 train_time:8345ms step_avg:86.92ms
step:97/1680 train_time:8432ms step_avg:86.93ms
step:98/1680 train_time:8519ms step_avg:86.93ms
step:99/1680 train_time:8607ms step_avg:86.94ms
step:100/1680 train_time:8694ms step_avg:86.94ms
step:101/1680 train_time:8781ms step_avg:86.94ms
step:102/1680 train_time:8868ms step_avg:86.94ms
step:103/1680 train_time:8955ms step_avg:86.94ms
step:104/1680 train_time:9042ms step_avg:86.94ms
step:105/1680 train_time:9129ms step_avg:86.94ms
step:106/1680 train_time:9215ms step_avg:86.94ms
step:107/1680 train_time:9302ms step_avg:86.94ms
step:108/1680 train_time:9389ms step_avg:86.94ms
step:109/1680 train_time:9476ms step_avg:86.94ms
step:110/1680 train_time:9565ms step_avg:86.95ms
step:111/1680 train_time:9652ms step_avg:86.95ms
step:112/1680 train_time:9739ms step_avg:86.95ms
step:113/1680 train_time:9825ms step_avg:86.95ms
step:114/1680 train_time:9912ms step_avg:86.95ms
step:115/1680 train_time:10000ms step_avg:86.96ms
step:116/1680 train_time:10087ms step_avg:86.95ms
step:117/1680 train_time:10174ms step_avg:86.96ms
step:118/1680 train_time:10261ms step_avg:86.95ms
step:119/1680 train_time:10348ms step_avg:86.96ms
step:120/1680 train_time:10435ms step_avg:86.96ms
step:121/1680 train_time:10522ms step_avg:86.96ms
step:122/1680 train_time:10609ms step_avg:86.96ms
step:123/1680 train_time:10696ms step_avg:86.96ms
step:124/1680 train_time:10784ms step_avg:86.97ms
step:125/1680 train_time:10871ms step_avg:86.97ms
step:125/1680 val_loss:4.3049 train_time:10960ms step_avg:87.68ms
step:126/1680 train_time:10984ms step_avg:87.17ms
step:127/1680 train_time:11050ms step_avg:87.00ms
step:128/1680 train_time:11145ms step_avg:87.07ms
step:129/1680 train_time:11239ms step_avg:87.12ms
step:130/1680 train_time:11327ms step_avg:87.13ms
step:131/1680 train_time:11414ms step_avg:87.13ms
step:132/1680 train_time:11500ms step_avg:87.12ms
step:133/1680 train_time:11586ms step_avg:87.11ms
step:134/1680 train_time:11672ms step_avg:87.11ms
step:135/1680 train_time:11758ms step_avg:87.10ms
step:136/1680 train_time:11844ms step_avg:87.09ms
step:137/1680 train_time:11930ms step_avg:87.08ms
step:138/1680 train_time:12017ms step_avg:87.08ms
step:139/1680 train_time:12105ms step_avg:87.09ms
step:140/1680 train_time:12194ms step_avg:87.10ms
step:141/1680 train_time:12283ms step_avg:87.12ms
step:142/1680 train_time:12371ms step_avg:87.12ms
step:143/1680 train_time:12459ms step_avg:87.12ms
step:144/1680 train_time:12547ms step_avg:87.13ms
step:145/1680 train_time:12633ms step_avg:87.12ms
step:146/1680 train_time:12720ms step_avg:87.12ms
step:147/1680 train_time:12806ms step_avg:87.12ms
step:148/1680 train_time:12892ms step_avg:87.11ms
step:149/1680 train_time:12979ms step_avg:87.11ms
step:150/1680 train_time:13066ms step_avg:87.11ms
step:151/1680 train_time:13154ms step_avg:87.11ms
step:152/1680 train_time:13242ms step_avg:87.12ms
step:153/1680 train_time:13330ms step_avg:87.12ms
step:154/1680 train_time:13417ms step_avg:87.12ms
step:155/1680 train_time:13504ms step_avg:87.13ms
step:156/1680 train_time:13591ms step_avg:87.12ms
step:157/1680 train_time:13677ms step_avg:87.12ms
step:158/1680 train_time:13765ms step_avg:87.12ms
step:159/1680 train_time:13851ms step_avg:87.11ms
step:160/1680 train_time:13938ms step_avg:87.11ms
step:161/1680 train_time:14025ms step_avg:87.11ms
step:162/1680 train_time:14112ms step_avg:87.11ms
step:163/1680 train_time:14199ms step_avg:87.11ms
step:164/1680 train_time:14287ms step_avg:87.11ms
step:165/1680 train_time:14374ms step_avg:87.12ms
step:166/1680 train_time:14461ms step_avg:87.12ms
step:167/1680 train_time:14548ms step_avg:87.12ms
step:168/1680 train_time:14635ms step_avg:87.11ms
step:169/1680 train_time:14721ms step_avg:87.11ms
step:170/1680 train_time:14808ms step_avg:87.10ms
step:171/1680 train_time:14895ms step_avg:87.10ms
step:172/1680 train_time:14982ms step_avg:87.10ms
step:173/1680 train_time:15068ms step_avg:87.10ms
step:174/1680 train_time:15155ms step_avg:87.10ms
step:175/1680 train_time:15244ms step_avg:87.11ms
step:176/1680 train_time:15331ms step_avg:87.11ms
step:177/1680 train_time:15418ms step_avg:87.11ms
step:178/1680 train_time:15506ms step_avg:87.11ms
step:179/1680 train_time:15593ms step_avg:87.11ms
step:180/1680 train_time:15680ms step_avg:87.11ms
step:181/1680 train_time:15766ms step_avg:87.11ms
step:182/1680 train_time:15853ms step_avg:87.10ms
step:183/1680 train_time:15940ms step_avg:87.10ms
step:184/1680 train_time:16026ms step_avg:87.10ms
step:185/1680 train_time:16114ms step_avg:87.10ms
step:186/1680 train_time:16201ms step_avg:87.10ms
step:187/1680 train_time:16289ms step_avg:87.11ms
step:188/1680 train_time:16376ms step_avg:87.11ms
step:189/1680 train_time:16463ms step_avg:87.11ms
step:190/1680 train_time:16550ms step_avg:87.10ms
step:191/1680 train_time:16637ms step_avg:87.10ms
step:192/1680 train_time:16724ms step_avg:87.10ms
step:193/1680 train_time:16811ms step_avg:87.10ms
step:194/1680 train_time:16898ms step_avg:87.10ms
step:195/1680 train_time:16984ms step_avg:87.10ms
step:196/1680 train_time:17071ms step_avg:87.10ms
step:197/1680 train_time:17159ms step_avg:87.10ms
step:198/1680 train_time:17246ms step_avg:87.10ms
step:199/1680 train_time:17334ms step_avg:87.10ms
step:200/1680 train_time:17421ms step_avg:87.10ms
step:201/1680 train_time:17508ms step_avg:87.10ms
step:202/1680 train_time:17595ms step_avg:87.10ms
step:203/1680 train_time:17683ms step_avg:87.11ms
step:204/1680 train_time:17769ms step_avg:87.10ms
step:205/1680 train_time:17856ms step_avg:87.10ms
step:206/1680 train_time:17943ms step_avg:87.10ms
step:207/1680 train_time:18030ms step_avg:87.10ms
step:208/1680 train_time:18116ms step_avg:87.09ms
step:209/1680 train_time:18203ms step_avg:87.09ms
step:210/1680 train_time:18289ms step_avg:87.09ms
step:211/1680 train_time:18377ms step_avg:87.09ms
step:212/1680 train_time:18464ms step_avg:87.10ms
step:213/1680 train_time:18551ms step_avg:87.09ms
step:214/1680 train_time:18638ms step_avg:87.09ms
step:215/1680 train_time:18725ms step_avg:87.09ms
step:216/1680 train_time:18813ms step_avg:87.10ms
step:217/1680 train_time:18900ms step_avg:87.10ms
step:218/1680 train_time:18986ms step_avg:87.09ms
step:219/1680 train_time:19073ms step_avg:87.09ms
step:220/1680 train_time:19159ms step_avg:87.09ms
step:221/1680 train_time:19246ms step_avg:87.09ms
step:222/1680 train_time:19333ms step_avg:87.09ms
step:223/1680 train_time:19420ms step_avg:87.09ms
step:224/1680 train_time:19507ms step_avg:87.09ms
step:225/1680 train_time:19594ms step_avg:87.08ms
step:226/1680 train_time:19681ms step_avg:87.09ms
step:227/1680 train_time:19768ms step_avg:87.09ms
step:228/1680 train_time:19856ms step_avg:87.09ms
step:229/1680 train_time:19943ms step_avg:87.09ms
step:230/1680 train_time:20030ms step_avg:87.09ms
step:231/1680 train_time:20117ms step_avg:87.09ms
step:232/1680 train_time:20205ms step_avg:87.09ms
step:233/1680 train_time:20291ms step_avg:87.09ms
step:234/1680 train_time:20379ms step_avg:87.09ms
step:235/1680 train_time:20466ms step_avg:87.09ms
step:236/1680 train_time:20553ms step_avg:87.09ms
step:237/1680 train_time:20640ms step_avg:87.09ms
step:238/1680 train_time:20727ms step_avg:87.09ms
step:239/1680 train_time:20814ms step_avg:87.09ms
step:240/1680 train_time:20901ms step_avg:87.09ms
step:241/1680 train_time:20988ms step_avg:87.09ms
step:242/1680 train_time:21075ms step_avg:87.09ms
step:243/1680 train_time:21162ms step_avg:87.09ms
step:244/1680 train_time:21248ms step_avg:87.08ms
step:245/1680 train_time:21336ms step_avg:87.08ms
step:246/1680 train_time:21423ms step_avg:87.08ms
step:247/1680 train_time:21509ms step_avg:87.08ms
step:248/1680 train_time:21596ms step_avg:87.08ms
step:249/1680 train_time:21684ms step_avg:87.08ms
step:250/1680 train_time:21771ms step_avg:87.08ms
step:250/1680 val_loss:3.9700 train_time:21860ms step_avg:87.44ms
step:251/1680 train_time:21879ms step_avg:87.17ms
step:252/1680 train_time:21948ms step_avg:87.10ms
step:253/1680 train_time:22039ms step_avg:87.11ms
step:254/1680 train_time:22127ms step_avg:87.12ms
step:255/1680 train_time:22214ms step_avg:87.11ms
step:256/1680 train_time:22300ms step_avg:87.11ms
step:257/1680 train_time:22386ms step_avg:87.10ms
step:258/1680 train_time:22472ms step_avg:87.10ms
step:259/1680 train_time:22558ms step_avg:87.10ms
step:260/1680 train_time:22644ms step_avg:87.09ms
step:261/1680 train_time:22730ms step_avg:87.09ms
step:262/1680 train_time:22817ms step_avg:87.09ms
step:263/1680 train_time:22905ms step_avg:87.09ms
step:264/1680 train_time:22994ms step_avg:87.10ms
step:265/1680 train_time:23083ms step_avg:87.11ms
step:266/1680 train_time:23170ms step_avg:87.11ms
step:267/1680 train_time:23257ms step_avg:87.10ms
step:268/1680 train_time:23343ms step_avg:87.10ms
step:269/1680 train_time:23430ms step_avg:87.10ms
step:270/1680 train_time:23517ms step_avg:87.10ms
step:271/1680 train_time:23603ms step_avg:87.09ms
step:272/1680 train_time:23688ms step_avg:87.09ms
step:273/1680 train_time:23775ms step_avg:87.09ms
step:274/1680 train_time:23863ms step_avg:87.09ms
step:275/1680 train_time:23950ms step_avg:87.09ms
step:276/1680 train_time:24039ms step_avg:87.10ms
step:277/1680 train_time:24127ms step_avg:87.10ms
step:278/1680 train_time:24214ms step_avg:87.10ms
step:279/1680 train_time:24300ms step_avg:87.10ms
step:280/1680 train_time:24387ms step_avg:87.10ms
step:281/1680 train_time:24473ms step_avg:87.09ms
step:282/1680 train_time:24560ms step_avg:87.09ms
step:283/1680 train_time:24646ms step_avg:87.09ms
step:284/1680 train_time:24733ms step_avg:87.09ms
step:285/1680 train_time:24819ms step_avg:87.08ms
step:286/1680 train_time:24907ms step_avg:87.09ms
step:287/1680 train_time:24995ms step_avg:87.09ms
step:288/1680 train_time:25082ms step_avg:87.09ms
step:289/1680 train_time:25170ms step_avg:87.09ms
step:290/1680 train_time:25257ms step_avg:87.09ms
step:291/1680 train_time:25344ms step_avg:87.09ms
step:292/1680 train_time:25431ms step_avg:87.09ms
step:293/1680 train_time:25517ms step_avg:87.09ms
step:294/1680 train_time:25605ms step_avg:87.09ms
step:295/1680 train_time:25691ms step_avg:87.09ms
step:296/1680 train_time:25778ms step_avg:87.09ms
step:297/1680 train_time:25865ms step_avg:87.09ms
step:298/1680 train_time:25952ms step_avg:87.09ms
step:299/1680 train_time:26040ms step_avg:87.09ms
step:300/1680 train_time:26127ms step_avg:87.09ms
step:301/1680 train_time:26213ms step_avg:87.09ms
step:302/1680 train_time:26300ms step_avg:87.09ms
step:303/1680 train_time:26388ms step_avg:87.09ms
step:304/1680 train_time:26474ms step_avg:87.09ms
step:305/1680 train_time:26561ms step_avg:87.09ms
step:306/1680 train_time:26648ms step_avg:87.09ms
step:307/1680 train_time:26735ms step_avg:87.09ms
step:308/1680 train_time:26822ms step_avg:87.08ms
step:309/1680 train_time:26909ms step_avg:87.08ms
step:310/1680 train_time:26996ms step_avg:87.08ms
step:311/1680 train_time:27083ms step_avg:87.08ms
step:312/1680 train_time:27170ms step_avg:87.08ms
step:313/1680 train_time:27257ms step_avg:87.08ms
step:314/1680 train_time:27344ms step_avg:87.08ms
step:315/1680 train_time:27431ms step_avg:87.08ms
step:316/1680 train_time:27518ms step_avg:87.08ms
step:317/1680 train_time:27605ms step_avg:87.08ms
step:318/1680 train_time:27691ms step_avg:87.08ms
step:319/1680 train_time:27778ms step_avg:87.08ms
step:320/1680 train_time:27865ms step_avg:87.08ms
step:321/1680 train_time:27951ms step_avg:87.08ms
step:322/1680 train_time:28039ms step_avg:87.08ms
step:323/1680 train_time:28125ms step_avg:87.08ms
step:324/1680 train_time:28213ms step_avg:87.08ms
step:325/1680 train_time:28300ms step_avg:87.08ms
step:326/1680 train_time:28387ms step_avg:87.08ms
step:327/1680 train_time:28473ms step_avg:87.07ms
step:328/1680 train_time:28561ms step_avg:87.08ms
step:329/1680 train_time:28648ms step_avg:87.07ms
step:330/1680 train_time:28734ms step_avg:87.07ms
step:331/1680 train_time:28821ms step_avg:87.07ms
step:332/1680 train_time:28909ms step_avg:87.08ms
step:333/1680 train_time:28996ms step_avg:87.07ms
step:334/1680 train_time:29083ms step_avg:87.07ms
step:335/1680 train_time:29170ms step_avg:87.07ms
step:336/1680 train_time:29257ms step_avg:87.08ms
step:337/1680 train_time:29344ms step_avg:87.08ms
step:338/1680 train_time:29431ms step_avg:87.07ms
step:339/1680 train_time:29518ms step_avg:87.07ms
step:340/1680 train_time:29605ms step_avg:87.07ms
step:341/1680 train_time:29692ms step_avg:87.07ms
step:342/1680 train_time:29779ms step_avg:87.07ms
step:343/1680 train_time:29867ms step_avg:87.08ms
step:344/1680 train_time:29953ms step_avg:87.07ms
step:345/1680 train_time:30041ms step_avg:87.07ms
step:346/1680 train_time:30128ms step_avg:87.07ms
step:347/1680 train_time:30215ms step_avg:87.07ms
step:348/1680 train_time:30302ms step_avg:87.07ms
step:349/1680 train_time:30389ms step_avg:87.07ms
step:350/1680 train_time:30475ms step_avg:87.07ms
step:351/1680 train_time:30563ms step_avg:87.07ms
step:352/1680 train_time:30650ms step_avg:87.07ms
step:353/1680 train_time:30737ms step_avg:87.07ms
step:354/1680 train_time:30824ms step_avg:87.07ms
step:355/1680 train_time:30911ms step_avg:87.07ms
step:356/1680 train_time:30998ms step_avg:87.07ms
step:357/1680 train_time:31086ms step_avg:87.07ms
step:358/1680 train_time:31172ms step_avg:87.07ms
step:359/1680 train_time:31259ms step_avg:87.07ms
step:360/1680 train_time:31346ms step_avg:87.07ms
step:361/1680 train_time:31433ms step_avg:87.07ms
step:362/1680 train_time:31520ms step_avg:87.07ms
step:363/1680 train_time:31607ms step_avg:87.07ms
step:364/1680 train_time:31694ms step_avg:87.07ms
step:365/1680 train_time:31781ms step_avg:87.07ms
step:366/1680 train_time:31869ms step_avg:87.07ms
step:367/1680 train_time:31955ms step_avg:87.07ms
step:368/1680 train_time:32043ms step_avg:87.07ms
step:369/1680 train_time:32130ms step_avg:87.07ms
step:370/1680 train_time:32217ms step_avg:87.07ms
step:371/1680 train_time:32304ms step_avg:87.07ms
step:372/1680 train_time:32391ms step_avg:87.07ms
step:373/1680 train_time:32478ms step_avg:87.07ms
step:374/1680 train_time:32565ms step_avg:87.07ms
step:375/1680 train_time:32652ms step_avg:87.07ms
step:375/1680 val_loss:3.8196 train_time:32740ms step_avg:87.31ms
step:376/1680 train_time:32760ms step_avg:87.13ms
step:377/1680 train_time:32831ms step_avg:87.08ms
step:378/1680 train_time:32919ms step_avg:87.09ms
step:379/1680 train_time:33007ms step_avg:87.09ms
step:380/1680 train_time:33093ms step_avg:87.09ms
step:381/1680 train_time:33179ms step_avg:87.09ms
step:382/1680 train_time:33266ms step_avg:87.08ms
step:383/1680 train_time:33351ms step_avg:87.08ms
step:384/1680 train_time:33437ms step_avg:87.08ms
step:385/1680 train_time:33524ms step_avg:87.07ms
step:386/1680 train_time:33609ms step_avg:87.07ms
step:387/1680 train_time:33698ms step_avg:87.07ms
step:388/1680 train_time:33786ms step_avg:87.08ms
step:389/1680 train_time:33875ms step_avg:87.08ms
step:390/1680 train_time:33963ms step_avg:87.08ms
step:391/1680 train_time:34049ms step_avg:87.08ms
step:392/1680 train_time:34136ms step_avg:87.08ms
step:393/1680 train_time:34222ms step_avg:87.08ms
step:394/1680 train_time:34310ms step_avg:87.08ms
step:395/1680 train_time:34396ms step_avg:87.08ms
step:396/1680 train_time:34482ms step_avg:87.08ms
step:397/1680 train_time:34570ms step_avg:87.08ms
step:398/1680 train_time:34656ms step_avg:87.08ms
step:399/1680 train_time:34745ms step_avg:87.08ms
step:400/1680 train_time:34833ms step_avg:87.08ms
step:401/1680 train_time:34920ms step_avg:87.08ms
step:402/1680 train_time:35007ms step_avg:87.08ms
step:403/1680 train_time:35094ms step_avg:87.08ms
step:404/1680 train_time:35181ms step_avg:87.08ms
step:405/1680 train_time:35268ms step_avg:87.08ms
step:406/1680 train_time:35355ms step_avg:87.08ms
step:407/1680 train_time:35441ms step_avg:87.08ms
step:408/1680 train_time:35528ms step_avg:87.08ms
step:409/1680 train_time:35614ms step_avg:87.08ms
step:410/1680 train_time:35702ms step_avg:87.08ms
step:411/1680 train_time:35789ms step_avg:87.08ms
step:412/1680 train_time:35877ms step_avg:87.08ms
step:413/1680 train_time:35964ms step_avg:87.08ms
step:414/1680 train_time:36051ms step_avg:87.08ms
step:415/1680 train_time:36138ms step_avg:87.08ms
step:416/1680 train_time:36225ms step_avg:87.08ms
step:417/1680 train_time:36312ms step_avg:87.08ms
step:418/1680 train_time:36398ms step_avg:87.08ms
step:419/1680 train_time:36485ms step_avg:87.08ms
step:420/1680 train_time:36571ms step_avg:87.07ms
step:421/1680 train_time:36658ms step_avg:87.07ms
step:422/1680 train_time:36746ms step_avg:87.07ms
step:423/1680 train_time:36834ms step_avg:87.08ms
step:424/1680 train_time:36921ms step_avg:87.08ms
step:425/1680 train_time:37009ms step_avg:87.08ms
step:426/1680 train_time:37096ms step_avg:87.08ms
step:427/1680 train_time:37182ms step_avg:87.08ms
step:428/1680 train_time:37269ms step_avg:87.08ms
step:429/1680 train_time:37356ms step_avg:87.08ms
step:430/1680 train_time:37443ms step_avg:87.08ms
step:431/1680 train_time:37531ms step_avg:87.08ms
step:432/1680 train_time:37618ms step_avg:87.08ms
step:433/1680 train_time:37704ms step_avg:87.08ms
step:434/1680 train_time:37791ms step_avg:87.08ms
step:435/1680 train_time:37878ms step_avg:87.08ms
step:436/1680 train_time:37966ms step_avg:87.08ms
step:437/1680 train_time:38054ms step_avg:87.08ms
step:438/1680 train_time:38141ms step_avg:87.08ms
step:439/1680 train_time:38228ms step_avg:87.08ms
step:440/1680 train_time:38314ms step_avg:87.08ms
step:441/1680 train_time:38401ms step_avg:87.08ms
step:442/1680 train_time:38488ms step_avg:87.08ms
step:443/1680 train_time:38575ms step_avg:87.08ms
step:444/1680 train_time:38662ms step_avg:87.08ms
step:445/1680 train_time:38749ms step_avg:87.08ms
step:446/1680 train_time:38835ms step_avg:87.07ms
step:447/1680 train_time:38923ms step_avg:87.08ms
step:448/1680 train_time:39011ms step_avg:87.08ms
step:449/1680 train_time:39098ms step_avg:87.08ms
step:450/1680 train_time:39185ms step_avg:87.08ms
step:451/1680 train_time:39271ms step_avg:87.08ms
step:452/1680 train_time:39358ms step_avg:87.08ms
step:453/1680 train_time:39446ms step_avg:87.08ms
step:454/1680 train_time:39533ms step_avg:87.08ms
step:455/1680 train_time:39620ms step_avg:87.08ms
step:456/1680 train_time:39707ms step_avg:87.08ms
step:457/1680 train_time:39794ms step_avg:87.08ms
step:458/1680 train_time:39881ms step_avg:87.08ms
step:459/1680 train_time:39969ms step_avg:87.08ms
step:460/1680 train_time:40056ms step_avg:87.08ms
step:461/1680 train_time:40144ms step_avg:87.08ms
step:462/1680 train_time:40231ms step_avg:87.08ms
step:463/1680 train_time:40318ms step_avg:87.08ms
step:464/1680 train_time:40405ms step_avg:87.08ms
step:465/1680 train_time:40492ms step_avg:87.08ms
step:466/1680 train_time:40579ms step_avg:87.08ms
step:467/1680 train_time:40666ms step_avg:87.08ms
step:468/1680 train_time:40753ms step_avg:87.08ms
step:469/1680 train_time:40840ms step_avg:87.08ms
step:470/1680 train_time:40928ms step_avg:87.08ms
step:471/1680 train_time:41014ms step_avg:87.08ms
step:472/1680 train_time:41101ms step_avg:87.08ms
step:473/1680 train_time:41188ms step_avg:87.08ms
step:474/1680 train_time:41275ms step_avg:87.08ms
step:475/1680 train_time:41362ms step_avg:87.08ms
step:476/1680 train_time:41450ms step_avg:87.08ms
step:477/1680 train_time:41537ms step_avg:87.08ms
step:478/1680 train_time:41623ms step_avg:87.08ms
step:479/1680 train_time:41710ms step_avg:87.08ms
step:480/1680 train_time:41797ms step_avg:87.08ms
step:481/1680 train_time:41884ms step_avg:87.08ms
step:482/1680 train_time:41971ms step_avg:87.08ms
step:483/1680 train_time:42058ms step_avg:87.08ms
step:484/1680 train_time:42146ms step_avg:87.08ms
step:485/1680 train_time:42233ms step_avg:87.08ms
step:486/1680 train_time:42320ms step_avg:87.08ms
step:487/1680 train_time:42407ms step_avg:87.08ms
step:488/1680 train_time:42494ms step_avg:87.08ms
step:489/1680 train_time:42580ms step_avg:87.08ms
step:490/1680 train_time:42668ms step_avg:87.08ms
step:491/1680 train_time:42754ms step_avg:87.08ms
step:492/1680 train_time:42841ms step_avg:87.08ms
step:493/1680 train_time:42929ms step_avg:87.08ms
step:494/1680 train_time:43015ms step_avg:87.07ms
step:495/1680 train_time:43102ms step_avg:87.07ms
step:496/1680 train_time:43189ms step_avg:87.08ms
step:497/1680 train_time:43276ms step_avg:87.07ms
step:498/1680 train_time:43363ms step_avg:87.07ms
step:499/1680 train_time:43450ms step_avg:87.08ms
step:500/1680 train_time:43537ms step_avg:87.07ms
step:500/1680 val_loss:3.7184 train_time:43626ms step_avg:87.25ms
step:501/1680 train_time:43645ms step_avg:87.12ms
step:502/1680 train_time:43715ms step_avg:87.08ms
step:503/1680 train_time:43805ms step_avg:87.09ms
step:504/1680 train_time:43892ms step_avg:87.09ms
step:505/1680 train_time:43979ms step_avg:87.09ms
step:506/1680 train_time:44065ms step_avg:87.08ms
step:507/1680 train_time:44151ms step_avg:87.08ms
step:508/1680 train_time:44237ms step_avg:87.08ms
step:509/1680 train_time:44323ms step_avg:87.08ms
step:510/1680 train_time:44410ms step_avg:87.08ms
step:511/1680 train_time:44497ms step_avg:87.08ms
step:512/1680 train_time:44585ms step_avg:87.08ms
step:513/1680 train_time:44674ms step_avg:87.08ms
step:514/1680 train_time:44762ms step_avg:87.09ms
step:515/1680 train_time:44850ms step_avg:87.09ms
step:516/1680 train_time:44937ms step_avg:87.09ms
step:517/1680 train_time:45023ms step_avg:87.09ms
step:518/1680 train_time:45109ms step_avg:87.08ms
step:519/1680 train_time:45195ms step_avg:87.08ms
step:520/1680 train_time:45282ms step_avg:87.08ms
step:521/1680 train_time:45369ms step_avg:87.08ms
step:522/1680 train_time:45455ms step_avg:87.08ms
step:523/1680 train_time:45542ms step_avg:87.08ms
step:524/1680 train_time:45630ms step_avg:87.08ms
step:525/1680 train_time:45718ms step_avg:87.08ms
step:526/1680 train_time:45806ms step_avg:87.08ms
step:527/1680 train_time:45894ms step_avg:87.09ms
step:528/1680 train_time:45981ms step_avg:87.09ms
step:529/1680 train_time:46068ms step_avg:87.08ms
step:530/1680 train_time:46154ms step_avg:87.08ms
step:531/1680 train_time:46241ms step_avg:87.08ms
step:532/1680 train_time:46327ms step_avg:87.08ms
step:533/1680 train_time:46414ms step_avg:87.08ms
step:534/1680 train_time:46501ms step_avg:87.08ms
step:535/1680 train_time:46588ms step_avg:87.08ms
step:536/1680 train_time:46675ms step_avg:87.08ms
step:537/1680 train_time:46763ms step_avg:87.08ms
step:538/1680 train_time:46851ms step_avg:87.08ms
step:539/1680 train_time:46938ms step_avg:87.08ms
step:540/1680 train_time:47025ms step_avg:87.08ms
step:541/1680 train_time:47113ms step_avg:87.09ms
step:542/1680 train_time:47200ms step_avg:87.08ms
step:543/1680 train_time:47287ms step_avg:87.08ms
step:544/1680 train_time:47373ms step_avg:87.08ms
step:545/1680 train_time:47460ms step_avg:87.08ms
step:546/1680 train_time:47547ms step_avg:87.08ms
step:547/1680 train_time:47634ms step_avg:87.08ms
step:548/1680 train_time:47721ms step_avg:87.08ms
step:549/1680 train_time:47810ms step_avg:87.08ms
step:550/1680 train_time:47898ms step_avg:87.09ms
step:551/1680 train_time:47987ms step_avg:87.09ms
step:552/1680 train_time:48074ms step_avg:87.09ms
step:553/1680 train_time:48162ms step_avg:87.09ms
step:554/1680 train_time:48250ms step_avg:87.09ms
step:555/1680 train_time:48338ms step_avg:87.10ms
step:556/1680 train_time:48426ms step_avg:87.10ms
step:557/1680 train_time:48514ms step_avg:87.10ms
step:558/1680 train_time:48603ms step_avg:87.10ms
step:559/1680 train_time:48691ms step_avg:87.10ms
step:560/1680 train_time:48779ms step_avg:87.11ms
step:561/1680 train_time:48868ms step_avg:87.11ms
step:562/1680 train_time:48956ms step_avg:87.11ms
step:563/1680 train_time:49044ms step_avg:87.11ms
step:564/1680 train_time:49132ms step_avg:87.11ms
step:565/1680 train_time:49220ms step_avg:87.11ms
step:566/1680 train_time:49308ms step_avg:87.12ms
step:567/1680 train_time:49396ms step_avg:87.12ms
step:568/1680 train_time:49484ms step_avg:87.12ms
step:569/1680 train_time:49572ms step_avg:87.12ms
step:570/1680 train_time:49661ms step_avg:87.12ms
step:571/1680 train_time:49748ms step_avg:87.13ms
step:572/1680 train_time:49837ms step_avg:87.13ms
step:573/1680 train_time:49926ms step_avg:87.13ms
step:574/1680 train_time:50014ms step_avg:87.13ms
step:575/1680 train_time:50103ms step_avg:87.14ms
step:576/1680 train_time:50192ms step_avg:87.14ms
step:577/1680 train_time:50280ms step_avg:87.14ms
step:578/1680 train_time:50368ms step_avg:87.14ms
step:579/1680 train_time:50456ms step_avg:87.14ms
step:580/1680 train_time:50544ms step_avg:87.14ms
step:581/1680 train_time:50633ms step_avg:87.15ms
step:582/1680 train_time:50721ms step_avg:87.15ms
step:583/1680 train_time:50809ms step_avg:87.15ms
step:584/1680 train_time:50898ms step_avg:87.15ms
step:585/1680 train_time:50986ms step_avg:87.16ms
step:586/1680 train_time:51074ms step_avg:87.16ms
step:587/1680 train_time:51162ms step_avg:87.16ms
step:588/1680 train_time:51250ms step_avg:87.16ms
step:589/1680 train_time:51338ms step_avg:87.16ms
step:590/1680 train_time:51426ms step_avg:87.16ms
step:591/1680 train_time:51514ms step_avg:87.16ms
step:592/1680 train_time:51601ms step_avg:87.16ms
step:593/1680 train_time:51690ms step_avg:87.17ms
step:594/1680 train_time:51777ms step_avg:87.17ms
step:595/1680 train_time:51867ms step_avg:87.17ms
step:596/1680 train_time:51955ms step_avg:87.17ms
step:597/1680 train_time:52043ms step_avg:87.17ms
step:598/1680 train_time:52132ms step_avg:87.18ms
step:599/1680 train_time:52220ms step_avg:87.18ms
step:600/1680 train_time:52308ms step_avg:87.18ms
step:601/1680 train_time:52396ms step_avg:87.18ms
step:602/1680 train_time:52484ms step_avg:87.18ms
step:603/1680 train_time:52571ms step_avg:87.18ms
step:604/1680 train_time:52660ms step_avg:87.18ms
step:605/1680 train_time:52748ms step_avg:87.19ms
step:606/1680 train_time:52836ms step_avg:87.19ms
step:607/1680 train_time:52924ms step_avg:87.19ms
step:608/1680 train_time:53013ms step_avg:87.19ms
step:609/1680 train_time:53101ms step_avg:87.19ms
step:610/1680 train_time:53191ms step_avg:87.20ms
step:611/1680 train_time:53278ms step_avg:87.20ms
step:612/1680 train_time:53366ms step_avg:87.20ms
step:613/1680 train_time:53454ms step_avg:87.20ms
step:614/1680 train_time:53542ms step_avg:87.20ms
step:615/1680 train_time:53630ms step_avg:87.20ms
step:616/1680 train_time:53718ms step_avg:87.21ms
step:617/1680 train_time:53807ms step_avg:87.21ms
step:618/1680 train_time:53895ms step_avg:87.21ms
step:619/1680 train_time:53983ms step_avg:87.21ms
step:620/1680 train_time:54072ms step_avg:87.21ms
step:621/1680 train_time:54160ms step_avg:87.21ms
step:622/1680 train_time:54248ms step_avg:87.22ms
step:623/1680 train_time:54336ms step_avg:87.22ms
step:624/1680 train_time:54424ms step_avg:87.22ms
step:625/1680 train_time:54511ms step_avg:87.22ms
step:625/1680 val_loss:3.6180 train_time:54601ms step_avg:87.36ms
step:626/1680 train_time:54626ms step_avg:87.26ms
step:627/1680 train_time:54692ms step_avg:87.23ms
step:628/1680 train_time:54781ms step_avg:87.23ms
step:629/1680 train_time:54872ms step_avg:87.24ms
step:630/1680 train_time:54962ms step_avg:87.24ms
step:631/1680 train_time:55049ms step_avg:87.24ms
step:632/1680 train_time:55135ms step_avg:87.24ms
step:633/1680 train_time:55223ms step_avg:87.24ms
step:634/1680 train_time:55310ms step_avg:87.24ms
step:635/1680 train_time:55397ms step_avg:87.24ms
step:636/1680 train_time:55484ms step_avg:87.24ms
step:637/1680 train_time:55575ms step_avg:87.25ms
step:638/1680 train_time:55665ms step_avg:87.25ms
step:639/1680 train_time:55753ms step_avg:87.25ms
step:640/1680 train_time:55842ms step_avg:87.25ms
step:641/1680 train_time:55933ms step_avg:87.26ms
step:642/1680 train_time:56021ms step_avg:87.26ms
step:643/1680 train_time:56109ms step_avg:87.26ms
step:644/1680 train_time:56196ms step_avg:87.26ms
step:645/1680 train_time:56284ms step_avg:87.26ms
step:646/1680 train_time:56371ms step_avg:87.26ms
step:647/1680 train_time:56459ms step_avg:87.26ms
step:648/1680 train_time:56547ms step_avg:87.26ms
step:649/1680 train_time:56636ms step_avg:87.27ms
step:650/1680 train_time:56724ms step_avg:87.27ms
step:651/1680 train_time:56812ms step_avg:87.27ms
step:652/1680 train_time:56902ms step_avg:87.27ms
step:653/1680 train_time:56991ms step_avg:87.28ms
step:654/1680 train_time:57079ms step_avg:87.28ms
step:655/1680 train_time:57167ms step_avg:87.28ms
step:656/1680 train_time:57254ms step_avg:87.28ms
step:657/1680 train_time:57341ms step_avg:87.28ms
step:658/1680 train_time:57429ms step_avg:87.28ms
step:659/1680 train_time:57517ms step_avg:87.28ms
step:660/1680 train_time:57606ms step_avg:87.28ms
step:661/1680 train_time:57694ms step_avg:87.28ms
step:662/1680 train_time:57783ms step_avg:87.29ms
step:663/1680 train_time:57872ms step_avg:87.29ms
step:664/1680 train_time:57961ms step_avg:87.29ms
step:665/1680 train_time:58049ms step_avg:87.29ms
step:666/1680 train_time:58136ms step_avg:87.29ms
step:667/1680 train_time:58224ms step_avg:87.29ms
step:668/1680 train_time:58311ms step_avg:87.29ms
step:669/1680 train_time:58399ms step_avg:87.29ms
step:670/1680 train_time:58488ms step_avg:87.30ms
step:671/1680 train_time:58575ms step_avg:87.30ms
step:672/1680 train_time:58663ms step_avg:87.30ms
step:673/1680 train_time:58751ms step_avg:87.30ms
step:674/1680 train_time:58839ms step_avg:87.30ms
step:675/1680 train_time:58928ms step_avg:87.30ms
step:676/1680 train_time:59016ms step_avg:87.30ms
step:677/1680 train_time:59105ms step_avg:87.30ms
step:678/1680 train_time:59193ms step_avg:87.31ms
step:679/1680 train_time:59281ms step_avg:87.31ms
step:680/1680 train_time:59369ms step_avg:87.31ms
step:681/1680 train_time:59456ms step_avg:87.31ms
step:682/1680 train_time:59545ms step_avg:87.31ms
step:683/1680 train_time:59633ms step_avg:87.31ms
step:684/1680 train_time:59721ms step_avg:87.31ms
step:685/1680 train_time:59810ms step_avg:87.31ms
step:686/1680 train_time:59898ms step_avg:87.32ms
step:687/1680 train_time:59987ms step_avg:87.32ms
step:688/1680 train_time:60076ms step_avg:87.32ms
step:689/1680 train_time:60164ms step_avg:87.32ms
step:690/1680 train_time:60252ms step_avg:87.32ms
step:691/1680 train_time:60340ms step_avg:87.32ms
step:692/1680 train_time:60428ms step_avg:87.32ms
step:693/1680 train_time:60516ms step_avg:87.32ms
step:694/1680 train_time:60605ms step_avg:87.33ms
step:695/1680 train_time:60692ms step_avg:87.33ms
step:696/1680 train_time:60781ms step_avg:87.33ms
step:697/1680 train_time:60870ms step_avg:87.33ms
step:698/1680 train_time:60959ms step_avg:87.33ms
step:699/1680 train_time:61048ms step_avg:87.34ms
step:700/1680 train_time:61135ms step_avg:87.34ms
step:701/1680 train_time:61224ms step_avg:87.34ms
step:702/1680 train_time:61311ms step_avg:87.34ms
step:703/1680 train_time:61399ms step_avg:87.34ms
step:704/1680 train_time:61487ms step_avg:87.34ms
step:705/1680 train_time:61574ms step_avg:87.34ms
step:706/1680 train_time:61662ms step_avg:87.34ms
step:707/1680 train_time:61751ms step_avg:87.34ms
step:708/1680 train_time:61839ms step_avg:87.34ms
step:709/1680 train_time:61928ms step_avg:87.35ms
step:710/1680 train_time:62017ms step_avg:87.35ms
step:711/1680 train_time:62105ms step_avg:87.35ms
step:712/1680 train_time:62193ms step_avg:87.35ms
step:713/1680 train_time:62281ms step_avg:87.35ms
step:714/1680 train_time:62369ms step_avg:87.35ms
step:715/1680 train_time:62457ms step_avg:87.35ms
step:716/1680 train_time:62544ms step_avg:87.35ms
step:717/1680 train_time:62632ms step_avg:87.35ms
step:718/1680 train_time:62720ms step_avg:87.35ms
step:719/1680 train_time:62808ms step_avg:87.35ms
step:720/1680 train_time:62897ms step_avg:87.36ms
step:721/1680 train_time:62985ms step_avg:87.36ms
step:722/1680 train_time:63073ms step_avg:87.36ms
step:723/1680 train_time:63162ms step_avg:87.36ms
step:724/1680 train_time:63250ms step_avg:87.36ms
step:725/1680 train_time:63338ms step_avg:87.36ms
step:726/1680 train_time:63426ms step_avg:87.36ms
step:727/1680 train_time:63514ms step_avg:87.36ms
step:728/1680 train_time:63602ms step_avg:87.37ms
step:729/1680 train_time:63691ms step_avg:87.37ms
step:730/1680 train_time:63779ms step_avg:87.37ms
step:731/1680 train_time:63868ms step_avg:87.37ms
step:732/1680 train_time:63956ms step_avg:87.37ms
step:733/1680 train_time:64044ms step_avg:87.37ms
step:734/1680 train_time:64133ms step_avg:87.37ms
step:735/1680 train_time:64220ms step_avg:87.37ms
step:736/1680 train_time:64309ms step_avg:87.38ms
step:737/1680 train_time:64397ms step_avg:87.38ms
step:738/1680 train_time:64485ms step_avg:87.38ms
step:739/1680 train_time:64573ms step_avg:87.38ms
step:740/1680 train_time:64661ms step_avg:87.38ms
step:741/1680 train_time:64749ms step_avg:87.38ms
step:742/1680 train_time:64837ms step_avg:87.38ms
step:743/1680 train_time:64926ms step_avg:87.38ms
step:744/1680 train_time:65013ms step_avg:87.38ms
step:745/1680 train_time:65102ms step_avg:87.38ms
step:746/1680 train_time:65190ms step_avg:87.39ms
step:747/1680 train_time:65278ms step_avg:87.39ms
step:748/1680 train_time:65366ms step_avg:87.39ms
step:749/1680 train_time:65454ms step_avg:87.39ms
step:750/1680 train_time:65542ms step_avg:87.39ms
step:750/1680 val_loss:3.5654 train_time:65632ms step_avg:87.51ms
step:751/1680 train_time:65653ms step_avg:87.42ms
step:752/1680 train_time:65722ms step_avg:87.40ms
step:753/1680 train_time:65811ms step_avg:87.40ms
step:754/1680 train_time:65899ms step_avg:87.40ms
step:755/1680 train_time:65986ms step_avg:87.40ms
step:756/1680 train_time:66074ms step_avg:87.40ms
step:757/1680 train_time:66162ms step_avg:87.40ms
step:758/1680 train_time:66249ms step_avg:87.40ms
step:759/1680 train_time:66336ms step_avg:87.40ms
step:760/1680 train_time:66425ms step_avg:87.40ms
step:761/1680 train_time:66513ms step_avg:87.40ms
step:762/1680 train_time:66603ms step_avg:87.41ms
step:763/1680 train_time:66693ms step_avg:87.41ms
step:764/1680 train_time:66783ms step_avg:87.41ms
step:765/1680 train_time:66871ms step_avg:87.41ms
step:766/1680 train_time:66958ms step_avg:87.41ms
step:767/1680 train_time:67047ms step_avg:87.41ms
step:768/1680 train_time:67135ms step_avg:87.42ms
step:769/1680 train_time:67222ms step_avg:87.42ms
step:770/1680 train_time:67310ms step_avg:87.42ms
step:771/1680 train_time:67398ms step_avg:87.42ms
step:772/1680 train_time:67485ms step_avg:87.42ms
step:773/1680 train_time:67574ms step_avg:87.42ms
step:774/1680 train_time:67664ms step_avg:87.42ms
step:775/1680 train_time:67754ms step_avg:87.42ms
step:776/1680 train_time:67842ms step_avg:87.43ms
step:777/1680 train_time:67930ms step_avg:87.43ms
step:778/1680 train_time:68018ms step_avg:87.43ms
step:779/1680 train_time:68106ms step_avg:87.43ms
step:780/1680 train_time:68194ms step_avg:87.43ms
step:781/1680 train_time:68282ms step_avg:87.43ms
step:782/1680 train_time:68370ms step_avg:87.43ms
step:783/1680 train_time:68459ms step_avg:87.43ms
step:784/1680 train_time:68548ms step_avg:87.43ms
step:785/1680 train_time:68637ms step_avg:87.44ms
step:786/1680 train_time:68726ms step_avg:87.44ms
step:787/1680 train_time:68815ms step_avg:87.44ms
step:788/1680 train_time:68903ms step_avg:87.44ms
step:789/1680 train_time:68992ms step_avg:87.44ms
step:790/1680 train_time:69080ms step_avg:87.44ms
step:791/1680 train_time:69168ms step_avg:87.44ms
step:792/1680 train_time:69255ms step_avg:87.44ms
step:793/1680 train_time:69343ms step_avg:87.44ms
step:794/1680 train_time:69432ms step_avg:87.45ms
step:795/1680 train_time:69519ms step_avg:87.45ms
step:796/1680 train_time:69608ms step_avg:87.45ms
step:797/1680 train_time:69696ms step_avg:87.45ms
step:798/1680 train_time:69785ms step_avg:87.45ms
step:799/1680 train_time:69873ms step_avg:87.45ms
step:800/1680 train_time:69961ms step_avg:87.45ms
step:801/1680 train_time:70049ms step_avg:87.45ms
step:802/1680 train_time:70137ms step_avg:87.45ms
step:803/1680 train_time:70225ms step_avg:87.45ms
step:804/1680 train_time:70313ms step_avg:87.45ms
step:805/1680 train_time:70400ms step_avg:87.45ms
step:806/1680 train_time:70488ms step_avg:87.45ms
step:807/1680 train_time:70576ms step_avg:87.46ms
step:808/1680 train_time:70665ms step_avg:87.46ms
step:809/1680 train_time:70753ms step_avg:87.46ms
step:810/1680 train_time:70842ms step_avg:87.46ms
step:811/1680 train_time:70930ms step_avg:87.46ms
step:812/1680 train_time:71018ms step_avg:87.46ms
step:813/1680 train_time:71107ms step_avg:87.46ms
step:814/1680 train_time:71195ms step_avg:87.46ms
step:815/1680 train_time:71283ms step_avg:87.46ms
step:816/1680 train_time:71371ms step_avg:87.46ms
step:817/1680 train_time:71459ms step_avg:87.46ms
step:818/1680 train_time:71547ms step_avg:87.47ms
step:819/1680 train_time:71636ms step_avg:87.47ms
step:820/1680 train_time:71724ms step_avg:87.47ms
step:821/1680 train_time:71812ms step_avg:87.47ms
step:822/1680 train_time:71900ms step_avg:87.47ms
step:823/1680 train_time:71988ms step_avg:87.47ms
step:824/1680 train_time:72076ms step_avg:87.47ms
step:825/1680 train_time:72165ms step_avg:87.47ms
step:826/1680 train_time:72253ms step_avg:87.47ms
step:827/1680 train_time:72341ms step_avg:87.47ms
step:828/1680 train_time:72429ms step_avg:87.47ms
step:829/1680 train_time:72516ms step_avg:87.47ms
step:830/1680 train_time:72605ms step_avg:87.48ms
step:831/1680 train_time:72693ms step_avg:87.48ms
step:832/1680 train_time:72781ms step_avg:87.48ms
step:833/1680 train_time:72869ms step_avg:87.48ms
step:834/1680 train_time:72957ms step_avg:87.48ms
step:835/1680 train_time:73045ms step_avg:87.48ms
step:836/1680 train_time:73134ms step_avg:87.48ms
step:837/1680 train_time:73222ms step_avg:87.48ms
step:838/1680 train_time:73310ms step_avg:87.48ms
step:839/1680 train_time:73398ms step_avg:87.48ms
step:840/1680 train_time:73486ms step_avg:87.48ms
step:841/1680 train_time:73575ms step_avg:87.48ms
step:842/1680 train_time:73663ms step_avg:87.49ms
step:843/1680 train_time:73753ms step_avg:87.49ms
step:844/1680 train_time:73842ms step_avg:87.49ms
step:845/1680 train_time:73930ms step_avg:87.49ms
step:846/1680 train_time:74018ms step_avg:87.49ms
step:847/1680 train_time:74105ms step_avg:87.49ms
step:848/1680 train_time:74195ms step_avg:87.49ms
step:849/1680 train_time:74284ms step_avg:87.50ms
step:850/1680 train_time:74371ms step_avg:87.50ms
step:851/1680 train_time:74459ms step_avg:87.50ms
step:852/1680 train_time:74547ms step_avg:87.50ms
step:853/1680 train_time:74636ms step_avg:87.50ms
step:854/1680 train_time:74725ms step_avg:87.50ms
step:855/1680 train_time:74813ms step_avg:87.50ms
step:856/1680 train_time:74902ms step_avg:87.50ms
step:857/1680 train_time:74990ms step_avg:87.50ms
step:858/1680 train_time:75078ms step_avg:87.50ms
step:859/1680 train_time:75166ms step_avg:87.50ms
step:860/1680 train_time:75255ms step_avg:87.51ms
step:861/1680 train_time:75344ms step_avg:87.51ms
step:862/1680 train_time:75432ms step_avg:87.51ms
step:863/1680 train_time:75520ms step_avg:87.51ms
step:864/1680 train_time:75608ms step_avg:87.51ms
step:865/1680 train_time:75696ms step_avg:87.51ms
step:866/1680 train_time:75785ms step_avg:87.51ms
step:867/1680 train_time:75873ms step_avg:87.51ms
step:868/1680 train_time:75961ms step_avg:87.51ms
step:869/1680 train_time:76049ms step_avg:87.51ms
step:870/1680 train_time:76137ms step_avg:87.51ms
step:871/1680 train_time:76225ms step_avg:87.51ms
step:872/1680 train_time:76313ms step_avg:87.52ms
step:873/1680 train_time:76401ms step_avg:87.52ms
step:874/1680 train_time:76490ms step_avg:87.52ms
step:875/1680 train_time:76577ms step_avg:87.52ms
step:875/1680 val_loss:3.5189 train_time:76666ms step_avg:87.62ms
step:876/1680 train_time:76687ms step_avg:87.54ms
step:877/1680 train_time:76759ms step_avg:87.52ms
step:878/1680 train_time:76848ms step_avg:87.53ms
step:879/1680 train_time:76936ms step_avg:87.53ms
step:880/1680 train_time:77023ms step_avg:87.53ms
step:881/1680 train_time:77110ms step_avg:87.53ms
step:882/1680 train_time:77197ms step_avg:87.53ms
step:883/1680 train_time:77285ms step_avg:87.53ms
step:884/1680 train_time:77372ms step_avg:87.53ms
step:885/1680 train_time:77460ms step_avg:87.53ms
step:886/1680 train_time:77548ms step_avg:87.53ms
step:887/1680 train_time:77639ms step_avg:87.53ms
step:888/1680 train_time:77729ms step_avg:87.53ms
step:889/1680 train_time:77818ms step_avg:87.53ms
step:890/1680 train_time:77907ms step_avg:87.54ms
step:891/1680 train_time:77995ms step_avg:87.54ms
step:892/1680 train_time:78083ms step_avg:87.54ms
step:893/1680 train_time:78170ms step_avg:87.54ms
step:894/1680 train_time:78258ms step_avg:87.54ms
step:895/1680 train_time:78344ms step_avg:87.54ms
step:896/1680 train_time:78431ms step_avg:87.53ms
step:897/1680 train_time:78519ms step_avg:87.54ms
step:898/1680 train_time:78608ms step_avg:87.54ms
step:899/1680 train_time:78697ms step_avg:87.54ms
step:900/1680 train_time:78786ms step_avg:87.54ms
step:901/1680 train_time:78876ms step_avg:87.54ms
step:902/1680 train_time:78964ms step_avg:87.54ms
step:903/1680 train_time:79053ms step_avg:87.54ms
step:904/1680 train_time:79140ms step_avg:87.54ms
step:905/1680 train_time:79227ms step_avg:87.54ms
step:906/1680 train_time:79315ms step_avg:87.54ms
step:907/1680 train_time:79402ms step_avg:87.54ms
step:908/1680 train_time:79490ms step_avg:87.54ms
step:909/1680 train_time:79579ms step_avg:87.55ms
step:910/1680 train_time:79668ms step_avg:87.55ms
step:911/1680 train_time:79756ms step_avg:87.55ms
step:912/1680 train_time:79845ms step_avg:87.55ms
step:913/1680 train_time:79934ms step_avg:87.55ms
step:914/1680 train_time:80022ms step_avg:87.55ms
step:915/1680 train_time:80109ms step_avg:87.55ms
step:916/1680 train_time:80197ms step_avg:87.55ms
step:917/1680 train_time:80285ms step_avg:87.55ms
step:918/1680 train_time:80373ms step_avg:87.55ms
step:919/1680 train_time:80461ms step_avg:87.55ms
step:920/1680 train_time:80549ms step_avg:87.55ms
step:921/1680 train_time:80637ms step_avg:87.55ms
step:922/1680 train_time:80725ms step_avg:87.55ms
step:923/1680 train_time:80815ms step_avg:87.56ms
step:924/1680 train_time:80903ms step_avg:87.56ms
step:925/1680 train_time:80991ms step_avg:87.56ms
step:926/1680 train_time:81080ms step_avg:87.56ms
step:927/1680 train_time:81167ms step_avg:87.56ms
step:928/1680 train_time:81256ms step_avg:87.56ms
step:929/1680 train_time:81344ms step_avg:87.56ms
step:930/1680 train_time:81432ms step_avg:87.56ms
step:931/1680 train_time:81519ms step_avg:87.56ms
step:932/1680 train_time:81607ms step_avg:87.56ms
step:933/1680 train_time:81696ms step_avg:87.56ms
step:934/1680 train_time:81784ms step_avg:87.56ms
step:935/1680 train_time:81874ms step_avg:87.57ms
step:936/1680 train_time:81962ms step_avg:87.57ms
step:937/1680 train_time:82049ms step_avg:87.57ms
step:938/1680 train_time:82137ms step_avg:87.57ms
step:939/1680 train_time:82225ms step_avg:87.57ms
step:940/1680 train_time:82314ms step_avg:87.57ms
step:941/1680 train_time:82402ms step_avg:87.57ms
step:942/1680 train_time:82491ms step_avg:87.57ms
step:943/1680 train_time:82579ms step_avg:87.57ms
step:944/1680 train_time:82667ms step_avg:87.57ms
step:945/1680 train_time:82756ms step_avg:87.57ms
step:946/1680 train_time:82844ms step_avg:87.57ms
step:947/1680 train_time:82932ms step_avg:87.57ms
step:948/1680 train_time:83020ms step_avg:87.57ms
step:949/1680 train_time:83108ms step_avg:87.57ms
step:950/1680 train_time:83195ms step_avg:87.57ms
step:951/1680 train_time:83283ms step_avg:87.57ms
step:952/1680 train_time:83372ms step_avg:87.58ms
step:953/1680 train_time:83460ms step_avg:87.58ms
step:954/1680 train_time:83548ms step_avg:87.58ms
step:955/1680 train_time:83636ms step_avg:87.58ms
step:956/1680 train_time:83724ms step_avg:87.58ms
step:957/1680 train_time:83813ms step_avg:87.58ms
step:958/1680 train_time:83901ms step_avg:87.58ms
step:959/1680 train_time:83990ms step_avg:87.58ms
step:960/1680 train_time:84078ms step_avg:87.58ms
step:961/1680 train_time:84166ms step_avg:87.58ms
step:962/1680 train_time:84254ms step_avg:87.58ms
step:963/1680 train_time:84341ms step_avg:87.58ms
step:964/1680 train_time:84429ms step_avg:87.58ms
step:965/1680 train_time:84518ms step_avg:87.58ms
step:966/1680 train_time:84606ms step_avg:87.58ms
step:967/1680 train_time:84695ms step_avg:87.59ms
step:968/1680 train_time:84783ms step_avg:87.59ms
step:969/1680 train_time:84872ms step_avg:87.59ms
step:970/1680 train_time:84960ms step_avg:87.59ms
step:971/1680 train_time:85048ms step_avg:87.59ms
step:972/1680 train_time:85137ms step_avg:87.59ms
step:973/1680 train_time:85224ms step_avg:87.59ms
step:974/1680 train_time:85313ms step_avg:87.59ms
step:975/1680 train_time:85401ms step_avg:87.59ms
step:976/1680 train_time:85488ms step_avg:87.59ms
step:977/1680 train_time:85577ms step_avg:87.59ms
step:978/1680 train_time:85666ms step_avg:87.59ms
step:979/1680 train_time:85754ms step_avg:87.59ms
step:980/1680 train_time:85842ms step_avg:87.59ms
step:981/1680 train_time:85931ms step_avg:87.60ms
step:982/1680 train_time:86019ms step_avg:87.60ms
step:983/1680 train_time:86107ms step_avg:87.60ms
step:984/1680 train_time:86196ms step_avg:87.60ms
step:985/1680 train_time:86284ms step_avg:87.60ms
step:986/1680 train_time:86371ms step_avg:87.60ms
step:987/1680 train_time:86459ms step_avg:87.60ms
step:988/1680 train_time:86548ms step_avg:87.60ms
step:989/1680 train_time:86636ms step_avg:87.60ms
step:990/1680 train_time:86726ms step_avg:87.60ms
step:991/1680 train_time:86814ms step_avg:87.60ms
step:992/1680 train_time:86902ms step_avg:87.60ms
step:993/1680 train_time:86991ms step_avg:87.60ms
step:994/1680 train_time:87079ms step_avg:87.60ms
step:995/1680 train_time:87167ms step_avg:87.61ms
step:996/1680 train_time:87256ms step_avg:87.61ms
step:997/1680 train_time:87344ms step_avg:87.61ms
step:998/1680 train_time:87433ms step_avg:87.61ms
step:999/1680 train_time:87520ms step_avg:87.61ms
step:1000/1680 train_time:87608ms step_avg:87.61ms
step:1000/1680 val_loss:3.4696 train_time:87698ms step_avg:87.70ms
step:1001/1680 train_time:87718ms step_avg:87.63ms
step:1002/1680 train_time:87790ms step_avg:87.61ms
step:1003/1680 train_time:87882ms step_avg:87.62ms
step:1004/1680 train_time:87971ms step_avg:87.62ms
step:1005/1680 train_time:88059ms step_avg:87.62ms
step:1006/1680 train_time:88147ms step_avg:87.62ms
step:1007/1680 train_time:88234ms step_avg:87.62ms
step:1008/1680 train_time:88322ms step_avg:87.62ms
step:1009/1680 train_time:88409ms step_avg:87.62ms
step:1010/1680 train_time:88497ms step_avg:87.62ms
step:1011/1680 train_time:88584ms step_avg:87.62ms
step:1012/1680 train_time:88674ms step_avg:87.62ms
step:1013/1680 train_time:88764ms step_avg:87.62ms
step:1014/1680 train_time:88853ms step_avg:87.63ms
step:1015/1680 train_time:88943ms step_avg:87.63ms
step:1016/1680 train_time:89031ms step_avg:87.63ms
step:1017/1680 train_time:89120ms step_avg:87.63ms
step:1018/1680 train_time:89207ms step_avg:87.63ms
step:1019/1680 train_time:89294ms step_avg:87.63ms
step:1020/1680 train_time:89382ms step_avg:87.63ms
step:1021/1680 train_time:89469ms step_avg:87.63ms
step:1022/1680 train_time:89557ms step_avg:87.63ms
step:1023/1680 train_time:89646ms step_avg:87.63ms
step:1024/1680 train_time:89735ms step_avg:87.63ms
step:1025/1680 train_time:89825ms step_avg:87.63ms
step:1026/1680 train_time:89913ms step_avg:87.63ms
step:1027/1680 train_time:90001ms step_avg:87.64ms
step:1028/1680 train_time:90090ms step_avg:87.64ms
step:1029/1680 train_time:90178ms step_avg:87.64ms
step:1030/1680 train_time:90266ms step_avg:87.64ms
step:1031/1680 train_time:90353ms step_avg:87.64ms
step:1032/1680 train_time:90441ms step_avg:87.64ms
step:1033/1680 train_time:90528ms step_avg:87.64ms
step:1034/1680 train_time:90616ms step_avg:87.64ms
step:1035/1680 train_time:90704ms step_avg:87.64ms
step:1036/1680 train_time:90792ms step_avg:87.64ms
step:1037/1680 train_time:90881ms step_avg:87.64ms
step:1038/1680 train_time:90970ms step_avg:87.64ms
step:1039/1680 train_time:91058ms step_avg:87.64ms
step:1040/1680 train_time:91147ms step_avg:87.64ms
step:1041/1680 train_time:91234ms step_avg:87.64ms
step:1042/1680 train_time:91322ms step_avg:87.64ms
step:1043/1680 train_time:91410ms step_avg:87.64ms
step:1044/1680 train_time:91497ms step_avg:87.64ms
step:1045/1680 train_time:91585ms step_avg:87.64ms
step:1046/1680 train_time:91673ms step_avg:87.64ms
step:1047/1680 train_time:91761ms step_avg:87.64ms
step:1048/1680 train_time:91850ms step_avg:87.64ms
step:1049/1680 train_time:91938ms step_avg:87.64ms
step:1050/1680 train_time:92027ms step_avg:87.64ms
step:1051/1680 train_time:92116ms step_avg:87.65ms
step:1052/1680 train_time:92204ms step_avg:87.65ms
step:1053/1680 train_time:92292ms step_avg:87.65ms
step:1054/1680 train_time:92380ms step_avg:87.65ms
step:1055/1680 train_time:92468ms step_avg:87.65ms
step:1056/1680 train_time:92556ms step_avg:87.65ms
step:1057/1680 train_time:92645ms step_avg:87.65ms
step:1058/1680 train_time:92733ms step_avg:87.65ms
step:1059/1680 train_time:92821ms step_avg:87.65ms
step:1060/1680 train_time:92909ms step_avg:87.65ms
step:1061/1680 train_time:92997ms step_avg:87.65ms
step:1062/1680 train_time:93085ms step_avg:87.65ms
step:1063/1680 train_time:93174ms step_avg:87.65ms
step:1064/1680 train_time:93262ms step_avg:87.65ms
step:1065/1680 train_time:93350ms step_avg:87.65ms
step:1066/1680 train_time:93439ms step_avg:87.65ms
step:1067/1680 train_time:93527ms step_avg:87.65ms
step:1068/1680 train_time:93616ms step_avg:87.66ms
step:1069/1680 train_time:93705ms step_avg:87.66ms
step:1070/1680 train_time:93792ms step_avg:87.66ms
step:1071/1680 train_time:93881ms step_avg:87.66ms
step:1072/1680 train_time:93969ms step_avg:87.66ms
step:1073/1680 train_time:94057ms step_avg:87.66ms
step:1074/1680 train_time:94145ms step_avg:87.66ms
step:1075/1680 train_time:94233ms step_avg:87.66ms
step:1076/1680 train_time:94322ms step_avg:87.66ms
step:1077/1680 train_time:94410ms step_avg:87.66ms
step:1078/1680 train_time:94498ms step_avg:87.66ms
step:1079/1680 train_time:94586ms step_avg:87.66ms
step:1080/1680 train_time:94674ms step_avg:87.66ms
step:1081/1680 train_time:94762ms step_avg:87.66ms
step:1082/1680 train_time:94851ms step_avg:87.66ms
step:1083/1680 train_time:94939ms step_avg:87.66ms
step:1084/1680 train_time:95028ms step_avg:87.66ms
step:1085/1680 train_time:95117ms step_avg:87.67ms
step:1086/1680 train_time:95205ms step_avg:87.67ms
step:1087/1680 train_time:95293ms step_avg:87.67ms
step:1088/1680 train_time:95382ms step_avg:87.67ms
step:1089/1680 train_time:95470ms step_avg:87.67ms
step:1090/1680 train_time:95558ms step_avg:87.67ms
step:1091/1680 train_time:95646ms step_avg:87.67ms
step:1092/1680 train_time:95734ms step_avg:87.67ms
step:1093/1680 train_time:95822ms step_avg:87.67ms
step:1094/1680 train_time:95910ms step_avg:87.67ms
step:1095/1680 train_time:95998ms step_avg:87.67ms
step:1096/1680 train_time:96087ms step_avg:87.67ms
step:1097/1680 train_time:96177ms step_avg:87.67ms
step:1098/1680 train_time:96266ms step_avg:87.67ms
step:1099/1680 train_time:96354ms step_avg:87.67ms
step:1100/1680 train_time:96443ms step_avg:87.68ms
step:1101/1680 train_time:96531ms step_avg:87.68ms
step:1102/1680 train_time:96621ms step_avg:87.68ms
step:1103/1680 train_time:96709ms step_avg:87.68ms
step:1104/1680 train_time:96799ms step_avg:87.68ms
step:1105/1680 train_time:96888ms step_avg:87.68ms
step:1106/1680 train_time:96978ms step_avg:87.68ms
step:1107/1680 train_time:97067ms step_avg:87.68ms
step:1108/1680 train_time:97156ms step_avg:87.69ms
step:1109/1680 train_time:97245ms step_avg:87.69ms
step:1110/1680 train_time:97333ms step_avg:87.69ms
step:1111/1680 train_time:97422ms step_avg:87.69ms
step:1112/1680 train_time:97511ms step_avg:87.69ms
step:1113/1680 train_time:97600ms step_avg:87.69ms
step:1114/1680 train_time:97688ms step_avg:87.69ms
step:1115/1680 train_time:97777ms step_avg:87.69ms
step:1116/1680 train_time:97867ms step_avg:87.69ms
step:1117/1680 train_time:97957ms step_avg:87.70ms
step:1118/1680 train_time:98045ms step_avg:87.70ms
step:1119/1680 train_time:98135ms step_avg:87.70ms
step:1120/1680 train_time:98223ms step_avg:87.70ms
step:1121/1680 train_time:98312ms step_avg:87.70ms
step:1122/1680 train_time:98401ms step_avg:87.70ms
step:1123/1680 train_time:98490ms step_avg:87.70ms
step:1124/1680 train_time:98578ms step_avg:87.70ms
step:1125/1680 train_time:98667ms step_avg:87.70ms
step:1125/1680 val_loss:3.4157 train_time:98757ms step_avg:87.78ms
step:1126/1680 train_time:98777ms step_avg:87.72ms
step:1127/1680 train_time:98845ms step_avg:87.71ms
step:1128/1680 train_time:98936ms step_avg:87.71ms
step:1129/1680 train_time:99026ms step_avg:87.71ms
step:1130/1680 train_time:99115ms step_avg:87.71ms
step:1131/1680 train_time:99203ms step_avg:87.71ms
step:1132/1680 train_time:99291ms step_avg:87.71ms
step:1133/1680 train_time:99379ms step_avg:87.71ms
step:1134/1680 train_time:99467ms step_avg:87.71ms
step:1135/1680 train_time:99557ms step_avg:87.71ms
step:1136/1680 train_time:99646ms step_avg:87.72ms
step:1137/1680 train_time:99740ms step_avg:87.72ms
step:1138/1680 train_time:99830ms step_avg:87.72ms
step:1139/1680 train_time:99919ms step_avg:87.73ms
step:1140/1680 train_time:100009ms step_avg:87.73ms
step:1141/1680 train_time:100098ms step_avg:87.73ms
step:1142/1680 train_time:100187ms step_avg:87.73ms
step:1143/1680 train_time:100276ms step_avg:87.73ms
step:1144/1680 train_time:100365ms step_avg:87.73ms
step:1145/1680 train_time:100454ms step_avg:87.73ms
step:1146/1680 train_time:100541ms step_avg:87.73ms
step:1147/1680 train_time:100630ms step_avg:87.73ms
step:1148/1680 train_time:100720ms step_avg:87.74ms
step:1149/1680 train_time:100809ms step_avg:87.74ms
step:1150/1680 train_time:100899ms step_avg:87.74ms
step:1151/1680 train_time:100989ms step_avg:87.74ms
step:1152/1680 train_time:101077ms step_avg:87.74ms
step:1153/1680 train_time:101166ms step_avg:87.74ms
step:1154/1680 train_time:101255ms step_avg:87.74ms
step:1155/1680 train_time:101343ms step_avg:87.74ms
step:1156/1680 train_time:101432ms step_avg:87.74ms
step:1157/1680 train_time:101520ms step_avg:87.74ms
step:1158/1680 train_time:101609ms step_avg:87.75ms
step:1159/1680 train_time:101699ms step_avg:87.75ms
step:1160/1680 train_time:101789ms step_avg:87.75ms
step:1161/1680 train_time:101878ms step_avg:87.75ms
step:1162/1680 train_time:101967ms step_avg:87.75ms
step:1163/1680 train_time:102057ms step_avg:87.75ms
step:1164/1680 train_time:102146ms step_avg:87.75ms
step:1165/1680 train_time:102235ms step_avg:87.76ms
step:1166/1680 train_time:102323ms step_avg:87.76ms
step:1167/1680 train_time:102412ms step_avg:87.76ms
step:1168/1680 train_time:102500ms step_avg:87.76ms
step:1169/1680 train_time:102589ms step_avg:87.76ms
step:1170/1680 train_time:102678ms step_avg:87.76ms
step:1171/1680 train_time:102767ms step_avg:87.76ms
step:1172/1680 train_time:102856ms step_avg:87.76ms
step:1173/1680 train_time:102944ms step_avg:87.76ms
step:1174/1680 train_time:103033ms step_avg:87.76ms
step:1175/1680 train_time:103122ms step_avg:87.76ms
step:1176/1680 train_time:103211ms step_avg:87.76ms
step:1177/1680 train_time:103300ms step_avg:87.77ms
step:1178/1680 train_time:103389ms step_avg:87.77ms
step:1179/1680 train_time:103477ms step_avg:87.77ms
step:1180/1680 train_time:103566ms step_avg:87.77ms
step:1181/1680 train_time:103655ms step_avg:87.77ms
step:1182/1680 train_time:103744ms step_avg:87.77ms
step:1183/1680 train_time:103834ms step_avg:87.77ms
step:1184/1680 train_time:103922ms step_avg:87.77ms
step:1185/1680 train_time:104011ms step_avg:87.77ms
step:1186/1680 train_time:104101ms step_avg:87.77ms
step:1187/1680 train_time:104189ms step_avg:87.77ms
step:1188/1680 train_time:104278ms step_avg:87.78ms
step:1189/1680 train_time:104367ms step_avg:87.78ms
step:1190/1680 train_time:104456ms step_avg:87.78ms
step:1191/1680 train_time:104545ms step_avg:87.78ms
step:1192/1680 train_time:104636ms step_avg:87.78ms
step:1193/1680 train_time:104725ms step_avg:87.78ms
step:1194/1680 train_time:104814ms step_avg:87.78ms
step:1195/1680 train_time:104902ms step_avg:87.78ms
step:1196/1680 train_time:104992ms step_avg:87.79ms
step:1197/1680 train_time:105080ms step_avg:87.79ms
step:1198/1680 train_time:105169ms step_avg:87.79ms
step:1199/1680 train_time:105258ms step_avg:87.79ms
step:1200/1680 train_time:105346ms step_avg:87.79ms
step:1201/1680 train_time:105434ms step_avg:87.79ms
step:1202/1680 train_time:105524ms step_avg:87.79ms
step:1203/1680 train_time:105612ms step_avg:87.79ms
step:1204/1680 train_time:105701ms step_avg:87.79ms
step:1205/1680 train_time:105790ms step_avg:87.79ms
step:1206/1680 train_time:105879ms step_avg:87.79ms
step:1207/1680 train_time:105968ms step_avg:87.79ms
step:1208/1680 train_time:106058ms step_avg:87.80ms
step:1209/1680 train_time:106147ms step_avg:87.80ms
step:1210/1680 train_time:106235ms step_avg:87.80ms
step:1211/1680 train_time:106324ms step_avg:87.80ms
step:1212/1680 train_time:106413ms step_avg:87.80ms
step:1213/1680 train_time:106502ms step_avg:87.80ms
step:1214/1680 train_time:106591ms step_avg:87.80ms
step:1215/1680 train_time:106680ms step_avg:87.80ms
step:1216/1680 train_time:106770ms step_avg:87.80ms
step:1217/1680 train_time:106858ms step_avg:87.80ms
step:1218/1680 train_time:106947ms step_avg:87.81ms
step:1219/1680 train_time:107037ms step_avg:87.81ms
step:1220/1680 train_time:107126ms step_avg:87.81ms
step:1221/1680 train_time:107215ms step_avg:87.81ms
step:1222/1680 train_time:107304ms step_avg:87.81ms
step:1223/1680 train_time:107392ms step_avg:87.81ms
step:1224/1680 train_time:107481ms step_avg:87.81ms
step:1225/1680 train_time:107570ms step_avg:87.81ms
step:1226/1680 train_time:107659ms step_avg:87.81ms
step:1227/1680 train_time:107748ms step_avg:87.81ms
step:1228/1680 train_time:107837ms step_avg:87.82ms
step:1229/1680 train_time:107926ms step_avg:87.82ms
step:1230/1680 train_time:108015ms step_avg:87.82ms
step:1231/1680 train_time:108104ms step_avg:87.82ms
step:1232/1680 train_time:108194ms step_avg:87.82ms
step:1233/1680 train_time:108282ms step_avg:87.82ms
step:1234/1680 train_time:108372ms step_avg:87.82ms
step:1235/1680 train_time:108460ms step_avg:87.82ms
step:1236/1680 train_time:108550ms step_avg:87.82ms
step:1237/1680 train_time:108639ms step_avg:87.82ms
step:1238/1680 train_time:108727ms step_avg:87.82ms
step:1239/1680 train_time:108816ms step_avg:87.83ms
step:1240/1680 train_time:108905ms step_avg:87.83ms
step:1241/1680 train_time:108994ms step_avg:87.83ms
step:1242/1680 train_time:109083ms step_avg:87.83ms
step:1243/1680 train_time:109172ms step_avg:87.83ms
step:1244/1680 train_time:109261ms step_avg:87.83ms
step:1245/1680 train_time:109350ms step_avg:87.83ms
step:1246/1680 train_time:109439ms step_avg:87.83ms
step:1247/1680 train_time:109528ms step_avg:87.83ms
step:1248/1680 train_time:109617ms step_avg:87.83ms
step:1249/1680 train_time:109705ms step_avg:87.83ms
step:1250/1680 train_time:109794ms step_avg:87.84ms
step:1250/1680 val_loss:3.3777 train_time:109884ms step_avg:87.91ms
step:1251/1680 train_time:109904ms step_avg:87.85ms
step:1252/1680 train_time:109977ms step_avg:87.84ms
step:1253/1680 train_time:110069ms step_avg:87.84ms
step:1254/1680 train_time:110159ms step_avg:87.85ms
step:1255/1680 train_time:110248ms step_avg:87.85ms
step:1256/1680 train_time:110337ms step_avg:87.85ms
step:1257/1680 train_time:110425ms step_avg:87.85ms
step:1258/1680 train_time:110513ms step_avg:87.85ms
step:1259/1680 train_time:110602ms step_avg:87.85ms
step:1260/1680 train_time:110689ms step_avg:87.85ms
step:1261/1680 train_time:110777ms step_avg:87.85ms
step:1262/1680 train_time:110866ms step_avg:87.85ms
step:1263/1680 train_time:110957ms step_avg:87.85ms
step:1264/1680 train_time:111048ms step_avg:87.85ms
step:1265/1680 train_time:111138ms step_avg:87.86ms
step:1266/1680 train_time:111227ms step_avg:87.86ms
step:1267/1680 train_time:111316ms step_avg:87.86ms
step:1268/1680 train_time:111405ms step_avg:87.86ms
step:1269/1680 train_time:111493ms step_avg:87.86ms
step:1270/1680 train_time:111581ms step_avg:87.86ms
step:1271/1680 train_time:111669ms step_avg:87.86ms
step:1272/1680 train_time:111757ms step_avg:87.86ms
step:1273/1680 train_time:111847ms step_avg:87.86ms
step:1274/1680 train_time:111938ms step_avg:87.86ms
step:1275/1680 train_time:112029ms step_avg:87.87ms
step:1276/1680 train_time:112119ms step_avg:87.87ms
step:1277/1680 train_time:112208ms step_avg:87.87ms
step:1278/1680 train_time:112298ms step_avg:87.87ms
step:1279/1680 train_time:112386ms step_avg:87.87ms
step:1280/1680 train_time:112474ms step_avg:87.87ms
step:1281/1680 train_time:112562ms step_avg:87.87ms
step:1282/1680 train_time:112651ms step_avg:87.87ms
step:1283/1680 train_time:112740ms step_avg:87.87ms
step:1284/1680 train_time:112829ms step_avg:87.87ms
step:1285/1680 train_time:112918ms step_avg:87.87ms
step:1286/1680 train_time:113009ms step_avg:87.88ms
step:1287/1680 train_time:113098ms step_avg:87.88ms
step:1288/1680 train_time:113187ms step_avg:87.88ms
step:1289/1680 train_time:113276ms step_avg:87.88ms
step:1290/1680 train_time:113365ms step_avg:87.88ms
step:1291/1680 train_time:113454ms step_avg:87.88ms
step:1292/1680 train_time:113542ms step_avg:87.88ms
step:1293/1680 train_time:113630ms step_avg:87.88ms
step:1294/1680 train_time:113719ms step_avg:87.88ms
step:1295/1680 train_time:113809ms step_avg:87.88ms
step:1296/1680 train_time:113898ms step_avg:87.88ms
step:1297/1680 train_time:113987ms step_avg:87.88ms
step:1298/1680 train_time:114076ms step_avg:87.89ms
step:1299/1680 train_time:114165ms step_avg:87.89ms
step:1300/1680 train_time:114254ms step_avg:87.89ms
step:1301/1680 train_time:114343ms step_avg:87.89ms
step:1302/1680 train_time:114432ms step_avg:87.89ms
step:1303/1680 train_time:114522ms step_avg:87.89ms
step:1304/1680 train_time:114611ms step_avg:87.89ms
step:1305/1680 train_time:114700ms step_avg:87.89ms
step:1306/1680 train_time:114789ms step_avg:87.89ms
step:1307/1680 train_time:114877ms step_avg:87.89ms
step:1308/1680 train_time:114966ms step_avg:87.89ms
step:1309/1680 train_time:115056ms step_avg:87.90ms
step:1310/1680 train_time:115145ms step_avg:87.90ms
step:1311/1680 train_time:115234ms step_avg:87.90ms
step:1312/1680 train_time:115324ms step_avg:87.90ms
step:1313/1680 train_time:115414ms step_avg:87.90ms
step:1314/1680 train_time:115503ms step_avg:87.90ms
step:1315/1680 train_time:115591ms step_avg:87.90ms
step:1316/1680 train_time:115680ms step_avg:87.90ms
step:1317/1680 train_time:115769ms step_avg:87.90ms
step:1318/1680 train_time:115858ms step_avg:87.90ms
step:1319/1680 train_time:115947ms step_avg:87.90ms
step:1320/1680 train_time:116036ms step_avg:87.91ms
step:1321/1680 train_time:116125ms step_avg:87.91ms
step:1322/1680 train_time:116214ms step_avg:87.91ms
step:1323/1680 train_time:116304ms step_avg:87.91ms
step:1324/1680 train_time:116393ms step_avg:87.91ms
step:1325/1680 train_time:116482ms step_avg:87.91ms
step:1326/1680 train_time:116571ms step_avg:87.91ms
step:1327/1680 train_time:116659ms step_avg:87.91ms
step:1328/1680 train_time:116749ms step_avg:87.91ms
step:1329/1680 train_time:116839ms step_avg:87.91ms
step:1330/1680 train_time:116927ms step_avg:87.91ms
step:1331/1680 train_time:117017ms step_avg:87.92ms
step:1332/1680 train_time:117106ms step_avg:87.92ms
step:1333/1680 train_time:117195ms step_avg:87.92ms
step:1334/1680 train_time:117284ms step_avg:87.92ms
step:1335/1680 train_time:117372ms step_avg:87.92ms
step:1336/1680 train_time:117462ms step_avg:87.92ms
step:1337/1680 train_time:117551ms step_avg:87.92ms
step:1338/1680 train_time:117640ms step_avg:87.92ms
step:1339/1680 train_time:117729ms step_avg:87.92ms
step:1340/1680 train_time:117819ms step_avg:87.92ms
step:1341/1680 train_time:117908ms step_avg:87.93ms
step:1342/1680 train_time:117996ms step_avg:87.93ms
step:1343/1680 train_time:118085ms step_avg:87.93ms
step:1344/1680 train_time:118174ms step_avg:87.93ms
step:1345/1680 train_time:118262ms step_avg:87.93ms
step:1346/1680 train_time:118351ms step_avg:87.93ms
step:1347/1680 train_time:118440ms step_avg:87.93ms
step:1348/1680 train_time:118529ms step_avg:87.93ms
step:1349/1680 train_time:118619ms step_avg:87.93ms
step:1350/1680 train_time:118708ms step_avg:87.93ms
step:1351/1680 train_time:118798ms step_avg:87.93ms
step:1352/1680 train_time:118887ms step_avg:87.93ms
step:1353/1680 train_time:118977ms step_avg:87.94ms
step:1354/1680 train_time:119066ms step_avg:87.94ms
step:1355/1680 train_time:119154ms step_avg:87.94ms
step:1356/1680 train_time:119243ms step_avg:87.94ms
step:1357/1680 train_time:119331ms step_avg:87.94ms
step:1358/1680 train_time:119419ms step_avg:87.94ms
step:1359/1680 train_time:119509ms step_avg:87.94ms
step:1360/1680 train_time:119598ms step_avg:87.94ms
step:1361/1680 train_time:119686ms step_avg:87.94ms
step:1362/1680 train_time:119775ms step_avg:87.94ms
step:1363/1680 train_time:119864ms step_avg:87.94ms
step:1364/1680 train_time:119953ms step_avg:87.94ms
step:1365/1680 train_time:120042ms step_avg:87.94ms
step:1366/1680 train_time:120131ms step_avg:87.94ms
step:1367/1680 train_time:120220ms step_avg:87.94ms
step:1368/1680 train_time:120310ms step_avg:87.95ms
step:1369/1680 train_time:120399ms step_avg:87.95ms
step:1370/1680 train_time:120487ms step_avg:87.95ms
step:1371/1680 train_time:120576ms step_avg:87.95ms
step:1372/1680 train_time:120665ms step_avg:87.95ms
step:1373/1680 train_time:120754ms step_avg:87.95ms
step:1374/1680 train_time:120843ms step_avg:87.95ms
step:1375/1680 train_time:120932ms step_avg:87.95ms
step:1375/1680 val_loss:3.3423 train_time:121023ms step_avg:88.02ms
step:1376/1680 train_time:121041ms step_avg:87.97ms
step:1377/1680 train_time:121113ms step_avg:87.95ms
step:1378/1680 train_time:121206ms step_avg:87.96ms
step:1379/1680 train_time:121296ms step_avg:87.96ms
step:1380/1680 train_time:121384ms step_avg:87.96ms
step:1381/1680 train_time:121472ms step_avg:87.96ms
step:1382/1680 train_time:121560ms step_avg:87.96ms
step:1383/1680 train_time:121648ms step_avg:87.96ms
step:1384/1680 train_time:121736ms step_avg:87.96ms
step:1385/1680 train_time:121825ms step_avg:87.96ms
step:1386/1680 train_time:121913ms step_avg:87.96ms
step:1387/1680 train_time:122002ms step_avg:87.96ms
step:1388/1680 train_time:122093ms step_avg:87.96ms
step:1389/1680 train_time:122184ms step_avg:87.97ms
step:1390/1680 train_time:122273ms step_avg:87.97ms
step:1391/1680 train_time:122362ms step_avg:87.97ms
step:1392/1680 train_time:122451ms step_avg:87.97ms
step:1393/1680 train_time:122540ms step_avg:87.97ms
step:1394/1680 train_time:122628ms step_avg:87.97ms
step:1395/1680 train_time:122716ms step_avg:87.97ms
step:1396/1680 train_time:122804ms step_avg:87.97ms
step:1397/1680 train_time:122892ms step_avg:87.97ms
step:1398/1680 train_time:122981ms step_avg:87.97ms
step:1399/1680 train_time:123071ms step_avg:87.97ms
step:1400/1680 train_time:123161ms step_avg:87.97ms
step:1401/1680 train_time:123250ms step_avg:87.97ms
step:1402/1680 train_time:123339ms step_avg:87.97ms
step:1403/1680 train_time:123428ms step_avg:87.97ms
step:1404/1680 train_time:123517ms step_avg:87.97ms
step:1405/1680 train_time:123605ms step_avg:87.97ms
step:1406/1680 train_time:123693ms step_avg:87.98ms
step:1407/1680 train_time:123782ms step_avg:87.98ms
step:1408/1680 train_time:123870ms step_avg:87.98ms
step:1409/1680 train_time:123960ms step_avg:87.98ms
step:1410/1680 train_time:124050ms step_avg:87.98ms
step:1411/1680 train_time:124140ms step_avg:87.98ms
step:1412/1680 train_time:124229ms step_avg:87.98ms
step:1413/1680 train_time:124319ms step_avg:87.98ms
step:1414/1680 train_time:124408ms step_avg:87.98ms
step:1415/1680 train_time:124497ms step_avg:87.98ms
step:1416/1680 train_time:124586ms step_avg:87.98ms
step:1417/1680 train_time:124674ms step_avg:87.98ms
step:1418/1680 train_time:124763ms step_avg:87.98ms
step:1419/1680 train_time:124851ms step_avg:87.99ms
step:1420/1680 train_time:124940ms step_avg:87.99ms
step:1421/1680 train_time:125030ms step_avg:87.99ms
step:1422/1680 train_time:125120ms step_avg:87.99ms
step:1423/1680 train_time:125209ms step_avg:87.99ms
step:1424/1680 train_time:125298ms step_avg:87.99ms
step:1425/1680 train_time:125388ms step_avg:87.99ms
step:1426/1680 train_time:125477ms step_avg:87.99ms
step:1427/1680 train_time:125566ms step_avg:87.99ms
step:1428/1680 train_time:125655ms step_avg:87.99ms
step:1429/1680 train_time:125744ms step_avg:87.99ms
step:1430/1680 train_time:125832ms step_avg:87.99ms
step:1431/1680 train_time:125921ms step_avg:88.00ms
step:1432/1680 train_time:126010ms step_avg:88.00ms
step:1433/1680 train_time:126099ms step_avg:88.00ms
step:1434/1680 train_time:126188ms step_avg:88.00ms
step:1435/1680 train_time:126277ms step_avg:88.00ms
step:1436/1680 train_time:126367ms step_avg:88.00ms
step:1437/1680 train_time:126456ms step_avg:88.00ms
step:1438/1680 train_time:126545ms step_avg:88.00ms
step:1439/1680 train_time:126633ms step_avg:88.00ms
step:1440/1680 train_time:126722ms step_avg:88.00ms
step:1441/1680 train_time:126810ms step_avg:88.00ms
step:1442/1680 train_time:126899ms step_avg:88.00ms
step:1443/1680 train_time:126988ms step_avg:88.00ms
step:1444/1680 train_time:127078ms step_avg:88.00ms
step:1445/1680 train_time:127167ms step_avg:88.01ms
step:1446/1680 train_time:127256ms step_avg:88.01ms
step:1447/1680 train_time:127345ms step_avg:88.01ms
step:1448/1680 train_time:127433ms step_avg:88.01ms
step:1449/1680 train_time:127522ms step_avg:88.01ms
step:1450/1680 train_time:127612ms step_avg:88.01ms
step:1451/1680 train_time:127700ms step_avg:88.01ms
step:1452/1680 train_time:127788ms step_avg:88.01ms
step:1453/1680 train_time:127877ms step_avg:88.01ms
step:1454/1680 train_time:127967ms step_avg:88.01ms
step:1455/1680 train_time:128056ms step_avg:88.01ms
step:1456/1680 train_time:128146ms step_avg:88.01ms
step:1457/1680 train_time:128234ms step_avg:88.01ms
step:1458/1680 train_time:128323ms step_avg:88.01ms
step:1459/1680 train_time:128412ms step_avg:88.01ms
step:1460/1680 train_time:128501ms step_avg:88.01ms
step:1461/1680 train_time:128590ms step_avg:88.02ms
step:1462/1680 train_time:128679ms step_avg:88.02ms
step:1463/1680 train_time:128768ms step_avg:88.02ms
step:1464/1680 train_time:128857ms step_avg:88.02ms
step:1465/1680 train_time:128947ms step_avg:88.02ms
step:1466/1680 train_time:129036ms step_avg:88.02ms
step:1467/1680 train_time:129126ms step_avg:88.02ms
step:1468/1680 train_time:129215ms step_avg:88.02ms
step:1469/1680 train_time:129304ms step_avg:88.02ms
step:1470/1680 train_time:129392ms step_avg:88.02ms
step:1471/1680 train_time:129481ms step_avg:88.02ms
step:1472/1680 train_time:129571ms step_avg:88.02ms
step:1473/1680 train_time:129659ms step_avg:88.02ms
step:1474/1680 train_time:129748ms step_avg:88.02ms
step:1475/1680 train_time:129837ms step_avg:88.03ms
step:1476/1680 train_time:129927ms step_avg:88.03ms
step:1477/1680 train_time:130016ms step_avg:88.03ms
step:1478/1680 train_time:130105ms step_avg:88.03ms
step:1479/1680 train_time:130193ms step_avg:88.03ms
step:1480/1680 train_time:130283ms step_avg:88.03ms
step:1481/1680 train_time:130372ms step_avg:88.03ms
step:1482/1680 train_time:130461ms step_avg:88.03ms
step:1483/1680 train_time:130550ms step_avg:88.03ms
step:1484/1680 train_time:130639ms step_avg:88.03ms
step:1485/1680 train_time:130728ms step_avg:88.03ms
step:1486/1680 train_time:130816ms step_avg:88.03ms
step:1487/1680 train_time:130905ms step_avg:88.03ms
step:1488/1680 train_time:130994ms step_avg:88.03ms
step:1489/1680 train_time:131083ms step_avg:88.03ms
step:1490/1680 train_time:131173ms step_avg:88.04ms
step:1491/1680 train_time:131262ms step_avg:88.04ms
step:1492/1680 train_time:131351ms step_avg:88.04ms
step:1493/1680 train_time:131440ms step_avg:88.04ms
step:1494/1680 train_time:131529ms step_avg:88.04ms
step:1495/1680 train_time:131618ms step_avg:88.04ms
step:1496/1680 train_time:131706ms step_avg:88.04ms
step:1497/1680 train_time:131795ms step_avg:88.04ms
step:1498/1680 train_time:131883ms step_avg:88.04ms
step:1499/1680 train_time:131972ms step_avg:88.04ms
step:1500/1680 train_time:132061ms step_avg:88.04ms
step:1500/1680 val_loss:3.3127 train_time:132151ms step_avg:88.10ms
step:1501/1680 train_time:132169ms step_avg:88.05ms
step:1502/1680 train_time:132241ms step_avg:88.04ms
step:1503/1680 train_time:132338ms step_avg:88.05ms
step:1504/1680 train_time:132428ms step_avg:88.05ms
step:1505/1680 train_time:132516ms step_avg:88.05ms
step:1506/1680 train_time:132604ms step_avg:88.05ms
step:1507/1680 train_time:132693ms step_avg:88.05ms
step:1508/1680 train_time:132781ms step_avg:88.05ms
step:1509/1680 train_time:132869ms step_avg:88.05ms
step:1510/1680 train_time:132957ms step_avg:88.05ms
step:1511/1680 train_time:133044ms step_avg:88.05ms
step:1512/1680 train_time:133134ms step_avg:88.05ms
step:1513/1680 train_time:133224ms step_avg:88.05ms
step:1514/1680 train_time:133316ms step_avg:88.06ms
step:1515/1680 train_time:133407ms step_avg:88.06ms
step:1516/1680 train_time:133496ms step_avg:88.06ms
step:1517/1680 train_time:133585ms step_avg:88.06ms
step:1518/1680 train_time:133673ms step_avg:88.06ms
step:1519/1680 train_time:133762ms step_avg:88.06ms
step:1520/1680 train_time:133851ms step_avg:88.06ms
step:1521/1680 train_time:133939ms step_avg:88.06ms
step:1522/1680 train_time:134028ms step_avg:88.06ms
step:1523/1680 train_time:134117ms step_avg:88.06ms
step:1524/1680 train_time:134207ms step_avg:88.06ms
step:1525/1680 train_time:134297ms step_avg:88.06ms
step:1526/1680 train_time:134387ms step_avg:88.06ms
step:1527/1680 train_time:134476ms step_avg:88.07ms
step:1528/1680 train_time:134565ms step_avg:88.07ms
step:1529/1680 train_time:134653ms step_avg:88.07ms
step:1530/1680 train_time:134741ms step_avg:88.07ms
step:1531/1680 train_time:134830ms step_avg:88.07ms
step:1532/1680 train_time:134919ms step_avg:88.07ms
step:1533/1680 train_time:135007ms step_avg:88.07ms
step:1534/1680 train_time:135096ms step_avg:88.07ms
step:1535/1680 train_time:135186ms step_avg:88.07ms
step:1536/1680 train_time:135276ms step_avg:88.07ms
step:1537/1680 train_time:135366ms step_avg:88.07ms
step:1538/1680 train_time:135456ms step_avg:88.07ms
step:1539/1680 train_time:135545ms step_avg:88.07ms
step:1540/1680 train_time:135634ms step_avg:88.07ms
step:1541/1680 train_time:135723ms step_avg:88.07ms
step:1542/1680 train_time:135811ms step_avg:88.07ms
step:1543/1680 train_time:135900ms step_avg:88.08ms
step:1544/1680 train_time:135989ms step_avg:88.08ms
step:1545/1680 train_time:136077ms step_avg:88.08ms
step:1546/1680 train_time:136166ms step_avg:88.08ms
step:1547/1680 train_time:136255ms step_avg:88.08ms
step:1548/1680 train_time:136344ms step_avg:88.08ms
step:1549/1680 train_time:136434ms step_avg:88.08ms
step:1550/1680 train_time:136523ms step_avg:88.08ms
step:1551/1680 train_time:136612ms step_avg:88.08ms
step:1552/1680 train_time:136700ms step_avg:88.08ms
step:1553/1680 train_time:136789ms step_avg:88.08ms
step:1554/1680 train_time:136877ms step_avg:88.08ms
step:1555/1680 train_time:136966ms step_avg:88.08ms
step:1556/1680 train_time:137055ms step_avg:88.08ms
step:1557/1680 train_time:137144ms step_avg:88.08ms
step:1558/1680 train_time:137235ms step_avg:88.08ms
step:1559/1680 train_time:137325ms step_avg:88.09ms
step:1560/1680 train_time:137414ms step_avg:88.09ms
step:1561/1680 train_time:137504ms step_avg:88.09ms
step:1562/1680 train_time:137594ms step_avg:88.09ms
step:1563/1680 train_time:137683ms step_avg:88.09ms
step:1564/1680 train_time:137771ms step_avg:88.09ms
step:1565/1680 train_time:137860ms step_avg:88.09ms
step:1566/1680 train_time:137948ms step_avg:88.09ms
step:1567/1680 train_time:138038ms step_avg:88.09ms
step:1568/1680 train_time:138127ms step_avg:88.09ms
step:1569/1680 train_time:138216ms step_avg:88.09ms
step:1570/1680 train_time:138306ms step_avg:88.09ms
step:1571/1680 train_time:138395ms step_avg:88.09ms
step:1572/1680 train_time:138485ms step_avg:88.09ms
step:1573/1680 train_time:138574ms step_avg:88.10ms
step:1574/1680 train_time:138663ms step_avg:88.10ms
step:1575/1680 train_time:138751ms step_avg:88.10ms
step:1576/1680 train_time:138840ms step_avg:88.10ms
step:1577/1680 train_time:138929ms step_avg:88.10ms
step:1578/1680 train_time:139018ms step_avg:88.10ms
step:1579/1680 train_time:139106ms step_avg:88.10ms
step:1580/1680 train_time:139196ms step_avg:88.10ms
step:1581/1680 train_time:139285ms step_avg:88.10ms
step:1582/1680 train_time:139373ms step_avg:88.10ms
step:1583/1680 train_time:139463ms step_avg:88.10ms
step:1584/1680 train_time:139552ms step_avg:88.10ms
step:1585/1680 train_time:139641ms step_avg:88.10ms
step:1586/1680 train_time:139730ms step_avg:88.10ms
step:1587/1680 train_time:139818ms step_avg:88.10ms
step:1588/1680 train_time:139907ms step_avg:88.10ms
step:1589/1680 train_time:139997ms step_avg:88.10ms
step:1590/1680 train_time:140086ms step_avg:88.10ms
step:1591/1680 train_time:140175ms step_avg:88.11ms
step:1592/1680 train_time:140264ms step_avg:88.11ms
step:1593/1680 train_time:140353ms step_avg:88.11ms
step:1594/1680 train_time:140442ms step_avg:88.11ms
step:1595/1680 train_time:140531ms step_avg:88.11ms
step:1596/1680 train_time:140619ms step_avg:88.11ms
step:1597/1680 train_time:140708ms step_avg:88.11ms
step:1598/1680 train_time:140796ms step_avg:88.11ms
step:1599/1680 train_time:140885ms step_avg:88.11ms
step:1600/1680 train_time:140974ms step_avg:88.11ms
step:1601/1680 train_time:141063ms step_avg:88.11ms
step:1602/1680 train_time:141152ms step_avg:88.11ms
step:1603/1680 train_time:141242ms step_avg:88.11ms
step:1604/1680 train_time:141332ms step_avg:88.11ms
step:1605/1680 train_time:141421ms step_avg:88.11ms
step:1606/1680 train_time:141511ms step_avg:88.11ms
step:1607/1680 train_time:141599ms step_avg:88.11ms
step:1608/1680 train_time:141688ms step_avg:88.11ms
step:1609/1680 train_time:141778ms step_avg:88.12ms
step:1610/1680 train_time:141867ms step_avg:88.12ms
step:1611/1680 train_time:141956ms step_avg:88.12ms
step:1612/1680 train_time:142044ms step_avg:88.12ms
step:1613/1680 train_time:142134ms step_avg:88.12ms
step:1614/1680 train_time:142223ms step_avg:88.12ms
step:1615/1680 train_time:142313ms step_avg:88.12ms
step:1616/1680 train_time:142402ms step_avg:88.12ms
step:1617/1680 train_time:142492ms step_avg:88.12ms
step:1618/1680 train_time:142580ms step_avg:88.12ms
step:1619/1680 train_time:142669ms step_avg:88.12ms
step:1620/1680 train_time:142759ms step_avg:88.12ms
step:1621/1680 train_time:142848ms step_avg:88.12ms
step:1622/1680 train_time:142937ms step_avg:88.12ms
step:1623/1680 train_time:143025ms step_avg:88.12ms
step:1624/1680 train_time:143114ms step_avg:88.12ms
step:1625/1680 train_time:143203ms step_avg:88.12ms
step:1625/1680 val_loss:3.2887 train_time:143294ms step_avg:88.18ms
step:1626/1680 train_time:143312ms step_avg:88.14ms
step:1627/1680 train_time:143385ms step_avg:88.13ms
step:1628/1680 train_time:143478ms step_avg:88.13ms
step:1629/1680 train_time:143567ms step_avg:88.13ms
step:1630/1680 train_time:143655ms step_avg:88.13ms
step:1631/1680 train_time:143743ms step_avg:88.13ms
step:1632/1680 train_time:143831ms step_avg:88.13ms
step:1633/1680 train_time:143919ms step_avg:88.13ms
step:1634/1680 train_time:144007ms step_avg:88.13ms
step:1635/1680 train_time:144095ms step_avg:88.13ms
step:1636/1680 train_time:144184ms step_avg:88.13ms
step:1637/1680 train_time:144274ms step_avg:88.13ms
step:1638/1680 train_time:144365ms step_avg:88.13ms
step:1639/1680 train_time:144456ms step_avg:88.14ms
step:1640/1680 train_time:144546ms step_avg:88.14ms
step:1641/1680 train_time:144637ms step_avg:88.14ms
step:1642/1680 train_time:144725ms step_avg:88.14ms
step:1643/1680 train_time:144813ms step_avg:88.14ms
step:1644/1680 train_time:144901ms step_avg:88.14ms
step:1645/1680 train_time:144989ms step_avg:88.14ms
step:1646/1680 train_time:145078ms step_avg:88.14ms
step:1647/1680 train_time:145167ms step_avg:88.14ms
step:1648/1680 train_time:145256ms step_avg:88.14ms
step:1649/1680 train_time:145346ms step_avg:88.14ms
step:1650/1680 train_time:145436ms step_avg:88.14ms
step:1651/1680 train_time:145526ms step_avg:88.14ms
step:1652/1680 train_time:145615ms step_avg:88.14ms
step:1653/1680 train_time:145704ms step_avg:88.15ms
step:1654/1680 train_time:145793ms step_avg:88.15ms
step:1655/1680 train_time:145881ms step_avg:88.15ms
step:1656/1680 train_time:145969ms step_avg:88.15ms
step:1657/1680 train_time:146058ms step_avg:88.15ms
step:1658/1680 train_time:146148ms step_avg:88.15ms
step:1659/1680 train_time:146237ms step_avg:88.15ms
step:1660/1680 train_time:146327ms step_avg:88.15ms
step:1661/1680 train_time:146416ms step_avg:88.15ms
step:1662/1680 train_time:146506ms step_avg:88.15ms
step:1663/1680 train_time:146596ms step_avg:88.15ms
step:1664/1680 train_time:146685ms step_avg:88.15ms
step:1665/1680 train_time:146774ms step_avg:88.15ms
step:1666/1680 train_time:146863ms step_avg:88.15ms
step:1667/1680 train_time:146952ms step_avg:88.15ms
step:1668/1680 train_time:147040ms step_avg:88.15ms
step:1669/1680 train_time:147129ms step_avg:88.15ms
step:1670/1680 train_time:147217ms step_avg:88.15ms
step:1671/1680 train_time:147306ms step_avg:88.15ms
step:1672/1680 train_time:147397ms step_avg:88.16ms
step:1673/1680 train_time:147486ms step_avg:88.16ms
step:1674/1680 train_time:147576ms step_avg:88.16ms
step:1675/1680 train_time:147666ms step_avg:88.16ms
step:1676/1680 train_time:147756ms step_avg:88.16ms
step:1677/1680 train_time:147844ms step_avg:88.16ms
step:1678/1680 train_time:147933ms step_avg:88.16ms
step:1679/1680 train_time:148021ms step_avg:88.16ms
step:1680/1680 train_time:148110ms step_avg:88.16ms
step:1680/1680 val_loss:3.2787 train_time:148201ms step_avg:88.22ms
peak memory allocated: 30760 MiB reserved: 46014 MiB

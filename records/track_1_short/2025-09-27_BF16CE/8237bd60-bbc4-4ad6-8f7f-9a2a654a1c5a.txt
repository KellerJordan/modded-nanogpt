import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:21:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            122W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0            116W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   29C    P0            122W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    155604      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    155605      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    155606      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    155607      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    155608      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    155609      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    155610      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    155611      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    155605      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    155606      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    155607      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    155608      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    155609      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    155610      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    155611      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:149ms step_avg:148.61ms
step:2/1680 train_time:169ms step_avg:84.53ms
step:3/1680 train_time:232ms step_avg:77.33ms
step:4/1680 train_time:317ms step_avg:79.30ms
step:5/1680 train_time:404ms step_avg:80.85ms
step:6/1680 train_time:490ms step_avg:81.67ms
step:7/1680 train_time:576ms step_avg:82.27ms
step:8/1680 train_time:662ms step_avg:82.80ms
step:9/1680 train_time:749ms step_avg:83.20ms
step:10/1680 train_time:836ms step_avg:83.56ms
step:11/1680 train_time:922ms step_avg:83.81ms
step:12/1680 train_time:1010ms step_avg:84.15ms
step:13/1680 train_time:1102ms step_avg:84.79ms
step:14/1680 train_time:1192ms step_avg:85.12ms
step:15/1680 train_time:1279ms step_avg:85.29ms
step:16/1680 train_time:1367ms step_avg:85.45ms
step:17/1680 train_time:1455ms step_avg:85.57ms
step:18/1680 train_time:1542ms step_avg:85.67ms
step:19/1680 train_time:1628ms step_avg:85.69ms
step:20/1680 train_time:1714ms step_avg:85.72ms
step:21/1680 train_time:1801ms step_avg:85.75ms
step:22/1680 train_time:1887ms step_avg:85.79ms
step:23/1680 train_time:1975ms step_avg:85.85ms
step:24/1680 train_time:2063ms step_avg:85.97ms
step:25/1680 train_time:2152ms step_avg:86.06ms
step:26/1680 train_time:2240ms step_avg:86.16ms
step:27/1680 train_time:2328ms step_avg:86.22ms
step:28/1680 train_time:2416ms step_avg:86.29ms
step:29/1680 train_time:2504ms step_avg:86.34ms
step:30/1680 train_time:2590ms step_avg:86.35ms
step:31/1680 train_time:2677ms step_avg:86.35ms
step:32/1680 train_time:2763ms step_avg:86.35ms
step:33/1680 train_time:2850ms step_avg:86.36ms
step:34/1680 train_time:2937ms step_avg:86.39ms
step:35/1680 train_time:3026ms step_avg:86.44ms
step:36/1680 train_time:3114ms step_avg:86.51ms
step:37/1680 train_time:3203ms step_avg:86.58ms
step:38/1680 train_time:3291ms step_avg:86.60ms
step:39/1680 train_time:3378ms step_avg:86.62ms
step:40/1680 train_time:3466ms step_avg:86.64ms
step:41/1680 train_time:3552ms step_avg:86.65ms
step:42/1680 train_time:3639ms step_avg:86.65ms
step:43/1680 train_time:3726ms step_avg:86.65ms
step:44/1680 train_time:3812ms step_avg:86.64ms
step:45/1680 train_time:3899ms step_avg:86.65ms
step:46/1680 train_time:3987ms step_avg:86.67ms
step:47/1680 train_time:4075ms step_avg:86.70ms
step:48/1680 train_time:4163ms step_avg:86.73ms
step:49/1680 train_time:4250ms step_avg:86.74ms
step:50/1680 train_time:4338ms step_avg:86.76ms
step:51/1680 train_time:4425ms step_avg:86.77ms
step:52/1680 train_time:4513ms step_avg:86.79ms
step:53/1680 train_time:4600ms step_avg:86.79ms
step:54/1680 train_time:4687ms step_avg:86.79ms
step:55/1680 train_time:4774ms step_avg:86.79ms
step:56/1680 train_time:4861ms step_avg:86.80ms
step:57/1680 train_time:4947ms step_avg:86.79ms
step:58/1680 train_time:5035ms step_avg:86.82ms
step:59/1680 train_time:5123ms step_avg:86.83ms
step:60/1680 train_time:5210ms step_avg:86.84ms
step:61/1680 train_time:5298ms step_avg:86.85ms
step:62/1680 train_time:5385ms step_avg:86.86ms
step:63/1680 train_time:5473ms step_avg:86.87ms
step:64/1680 train_time:5559ms step_avg:86.86ms
step:65/1680 train_time:5646ms step_avg:86.87ms
step:66/1680 train_time:5734ms step_avg:86.88ms
step:67/1680 train_time:5821ms step_avg:86.88ms
step:68/1680 train_time:5908ms step_avg:86.88ms
step:69/1680 train_time:5995ms step_avg:86.89ms
step:70/1680 train_time:6083ms step_avg:86.90ms
step:71/1680 train_time:6170ms step_avg:86.90ms
step:72/1680 train_time:6258ms step_avg:86.91ms
step:73/1680 train_time:6346ms step_avg:86.93ms
step:74/1680 train_time:6433ms step_avg:86.94ms
step:75/1680 train_time:6520ms step_avg:86.94ms
step:76/1680 train_time:6607ms step_avg:86.94ms
step:77/1680 train_time:6694ms step_avg:86.94ms
step:78/1680 train_time:6782ms step_avg:86.94ms
step:79/1680 train_time:6868ms step_avg:86.94ms
step:80/1680 train_time:6955ms step_avg:86.94ms
step:81/1680 train_time:7044ms step_avg:86.96ms
step:82/1680 train_time:7131ms step_avg:86.96ms
step:83/1680 train_time:7218ms step_avg:86.96ms
step:84/1680 train_time:7305ms step_avg:86.97ms
step:85/1680 train_time:7393ms step_avg:86.97ms
step:86/1680 train_time:7481ms step_avg:86.99ms
step:87/1680 train_time:7568ms step_avg:86.99ms
step:88/1680 train_time:7655ms step_avg:86.99ms
step:89/1680 train_time:7742ms step_avg:86.99ms
step:90/1680 train_time:7829ms step_avg:86.99ms
step:91/1680 train_time:7916ms step_avg:86.99ms
step:92/1680 train_time:8003ms step_avg:86.99ms
step:93/1680 train_time:8091ms step_avg:87.00ms
step:94/1680 train_time:8178ms step_avg:87.00ms
step:95/1680 train_time:8265ms step_avg:87.00ms
step:96/1680 train_time:8353ms step_avg:87.01ms
step:97/1680 train_time:8440ms step_avg:87.01ms
step:98/1680 train_time:8527ms step_avg:87.01ms
step:99/1680 train_time:8614ms step_avg:87.01ms
step:100/1680 train_time:8701ms step_avg:87.01ms
step:101/1680 train_time:8788ms step_avg:87.01ms
step:102/1680 train_time:8875ms step_avg:87.01ms
step:103/1680 train_time:8962ms step_avg:87.01ms
step:104/1680 train_time:9050ms step_avg:87.02ms
step:105/1680 train_time:9137ms step_avg:87.02ms
step:106/1680 train_time:9224ms step_avg:87.02ms
step:107/1680 train_time:9311ms step_avg:87.02ms
step:108/1680 train_time:9399ms step_avg:87.03ms
step:109/1680 train_time:9486ms step_avg:87.03ms
step:110/1680 train_time:9574ms step_avg:87.03ms
step:111/1680 train_time:9662ms step_avg:87.04ms
step:112/1680 train_time:9749ms step_avg:87.04ms
step:113/1680 train_time:9836ms step_avg:87.04ms
step:114/1680 train_time:9922ms step_avg:87.04ms
step:115/1680 train_time:10009ms step_avg:87.04ms
step:116/1680 train_time:10097ms step_avg:87.04ms
step:117/1680 train_time:10184ms step_avg:87.04ms
step:118/1680 train_time:10270ms step_avg:87.04ms
step:119/1680 train_time:10358ms step_avg:87.05ms
step:120/1680 train_time:10445ms step_avg:87.04ms
step:121/1680 train_time:10533ms step_avg:87.05ms
step:122/1680 train_time:10621ms step_avg:87.05ms
step:123/1680 train_time:10708ms step_avg:87.06ms
step:124/1680 train_time:10795ms step_avg:87.06ms
step:125/1680 train_time:10882ms step_avg:87.05ms
step:125/1680 val_loss:4.2925 train_time:10970ms step_avg:87.76ms
step:126/1680 train_time:10990ms step_avg:87.22ms
step:127/1680 train_time:11058ms step_avg:87.07ms
step:128/1680 train_time:11153ms step_avg:87.13ms
step:129/1680 train_time:11245ms step_avg:87.17ms
step:130/1680 train_time:11334ms step_avg:87.18ms
step:131/1680 train_time:11421ms step_avg:87.19ms
step:132/1680 train_time:11508ms step_avg:87.18ms
step:133/1680 train_time:11593ms step_avg:87.17ms
step:134/1680 train_time:11679ms step_avg:87.16ms
step:135/1680 train_time:11766ms step_avg:87.15ms
step:136/1680 train_time:11852ms step_avg:87.14ms
step:137/1680 train_time:11937ms step_avg:87.13ms
step:138/1680 train_time:12024ms step_avg:87.13ms
step:139/1680 train_time:12113ms step_avg:87.14ms
step:140/1680 train_time:12203ms step_avg:87.17ms
step:141/1680 train_time:12292ms step_avg:87.18ms
step:142/1680 train_time:12381ms step_avg:87.19ms
step:143/1680 train_time:12468ms step_avg:87.19ms
step:144/1680 train_time:12554ms step_avg:87.18ms
step:145/1680 train_time:12641ms step_avg:87.18ms
step:146/1680 train_time:12728ms step_avg:87.18ms
step:147/1680 train_time:12814ms step_avg:87.17ms
step:148/1680 train_time:12900ms step_avg:87.16ms
step:149/1680 train_time:12986ms step_avg:87.16ms
step:150/1680 train_time:13074ms step_avg:87.16ms
step:151/1680 train_time:13163ms step_avg:87.17ms
step:152/1680 train_time:13251ms step_avg:87.18ms
step:153/1680 train_time:13339ms step_avg:87.18ms
step:154/1680 train_time:13427ms step_avg:87.19ms
step:155/1680 train_time:13514ms step_avg:87.19ms
step:156/1680 train_time:13602ms step_avg:87.19ms
step:157/1680 train_time:13688ms step_avg:87.19ms
step:158/1680 train_time:13775ms step_avg:87.18ms
step:159/1680 train_time:13861ms step_avg:87.18ms
step:160/1680 train_time:13948ms step_avg:87.17ms
step:161/1680 train_time:14035ms step_avg:87.17ms
step:162/1680 train_time:14123ms step_avg:87.18ms
step:163/1680 train_time:14210ms step_avg:87.18ms
step:164/1680 train_time:14297ms step_avg:87.18ms
step:165/1680 train_time:14385ms step_avg:87.18ms
step:166/1680 train_time:14473ms step_avg:87.19ms
step:167/1680 train_time:14561ms step_avg:87.19ms
step:168/1680 train_time:14648ms step_avg:87.19ms
step:169/1680 train_time:14734ms step_avg:87.18ms
step:170/1680 train_time:14821ms step_avg:87.18ms
step:171/1680 train_time:14907ms step_avg:87.18ms
step:172/1680 train_time:14994ms step_avg:87.18ms
step:173/1680 train_time:15082ms step_avg:87.18ms
step:174/1680 train_time:15169ms step_avg:87.18ms
step:175/1680 train_time:15257ms step_avg:87.18ms
step:176/1680 train_time:15343ms step_avg:87.18ms
step:177/1680 train_time:15431ms step_avg:87.18ms
step:178/1680 train_time:15518ms step_avg:87.18ms
step:179/1680 train_time:15605ms step_avg:87.18ms
step:180/1680 train_time:15692ms step_avg:87.18ms
step:181/1680 train_time:15778ms step_avg:87.17ms
step:182/1680 train_time:15865ms step_avg:87.17ms
step:183/1680 train_time:15952ms step_avg:87.17ms
step:184/1680 train_time:16040ms step_avg:87.17ms
step:185/1680 train_time:16127ms step_avg:87.17ms
step:186/1680 train_time:16215ms step_avg:87.18ms
step:187/1680 train_time:16302ms step_avg:87.18ms
step:188/1680 train_time:16389ms step_avg:87.18ms
step:189/1680 train_time:16477ms step_avg:87.18ms
step:190/1680 train_time:16563ms step_avg:87.17ms
step:191/1680 train_time:16650ms step_avg:87.17ms
step:192/1680 train_time:16737ms step_avg:87.17ms
step:193/1680 train_time:16823ms step_avg:87.17ms
step:194/1680 train_time:16910ms step_avg:87.16ms
step:195/1680 train_time:16997ms step_avg:87.16ms
step:196/1680 train_time:17083ms step_avg:87.16ms
step:197/1680 train_time:17171ms step_avg:87.16ms
step:198/1680 train_time:17258ms step_avg:87.16ms
step:199/1680 train_time:17345ms step_avg:87.16ms
step:200/1680 train_time:17433ms step_avg:87.16ms
step:201/1680 train_time:17520ms step_avg:87.16ms
step:202/1680 train_time:17607ms step_avg:87.16ms
step:203/1680 train_time:17695ms step_avg:87.17ms
step:204/1680 train_time:17782ms step_avg:87.17ms
step:205/1680 train_time:17868ms step_avg:87.16ms
step:206/1680 train_time:17956ms step_avg:87.16ms
step:207/1680 train_time:18043ms step_avg:87.16ms
step:208/1680 train_time:18130ms step_avg:87.16ms
step:209/1680 train_time:18217ms step_avg:87.16ms
step:210/1680 train_time:18304ms step_avg:87.16ms
step:211/1680 train_time:18392ms step_avg:87.16ms
step:212/1680 train_time:18479ms step_avg:87.16ms
step:213/1680 train_time:18566ms step_avg:87.16ms
step:214/1680 train_time:18653ms step_avg:87.16ms
step:215/1680 train_time:18740ms step_avg:87.16ms
step:216/1680 train_time:18827ms step_avg:87.16ms
step:217/1680 train_time:18913ms step_avg:87.16ms
step:218/1680 train_time:19001ms step_avg:87.16ms
step:219/1680 train_time:19088ms step_avg:87.16ms
step:220/1680 train_time:19176ms step_avg:87.16ms
step:221/1680 train_time:19264ms step_avg:87.17ms
step:222/1680 train_time:19350ms step_avg:87.16ms
step:223/1680 train_time:19438ms step_avg:87.16ms
step:224/1680 train_time:19524ms step_avg:87.16ms
step:225/1680 train_time:19612ms step_avg:87.16ms
step:226/1680 train_time:19699ms step_avg:87.16ms
step:227/1680 train_time:19785ms step_avg:87.16ms
step:228/1680 train_time:19872ms step_avg:87.16ms
step:229/1680 train_time:19959ms step_avg:87.16ms
step:230/1680 train_time:20046ms step_avg:87.16ms
step:231/1680 train_time:20133ms step_avg:87.16ms
step:232/1680 train_time:20221ms step_avg:87.16ms
step:233/1680 train_time:20308ms step_avg:87.16ms
step:234/1680 train_time:20396ms step_avg:87.16ms
step:235/1680 train_time:20483ms step_avg:87.16ms
step:236/1680 train_time:20570ms step_avg:87.16ms
step:237/1680 train_time:20657ms step_avg:87.16ms
step:238/1680 train_time:20743ms step_avg:87.16ms
step:239/1680 train_time:20830ms step_avg:87.16ms
step:240/1680 train_time:20918ms step_avg:87.16ms
step:241/1680 train_time:21005ms step_avg:87.16ms
step:242/1680 train_time:21092ms step_avg:87.16ms
step:243/1680 train_time:21179ms step_avg:87.16ms
step:244/1680 train_time:21266ms step_avg:87.16ms
step:245/1680 train_time:21353ms step_avg:87.15ms
step:246/1680 train_time:21440ms step_avg:87.16ms
step:247/1680 train_time:21527ms step_avg:87.15ms
step:248/1680 train_time:21615ms step_avg:87.16ms
step:249/1680 train_time:21702ms step_avg:87.16ms
step:250/1680 train_time:21789ms step_avg:87.15ms
step:250/1680 val_loss:3.9621 train_time:21877ms step_avg:87.51ms
step:251/1680 train_time:21897ms step_avg:87.24ms
step:252/1680 train_time:21968ms step_avg:87.17ms
step:253/1680 train_time:22058ms step_avg:87.19ms
step:254/1680 train_time:22148ms step_avg:87.20ms
step:255/1680 train_time:22236ms step_avg:87.20ms
step:256/1680 train_time:22323ms step_avg:87.20ms
step:257/1680 train_time:22409ms step_avg:87.19ms
step:258/1680 train_time:22495ms step_avg:87.19ms
step:259/1680 train_time:22582ms step_avg:87.19ms
step:260/1680 train_time:22668ms step_avg:87.18ms
step:261/1680 train_time:22755ms step_avg:87.18ms
step:262/1680 train_time:22842ms step_avg:87.18ms
step:263/1680 train_time:22930ms step_avg:87.19ms
step:264/1680 train_time:23018ms step_avg:87.19ms
step:265/1680 train_time:23106ms step_avg:87.19ms
step:266/1680 train_time:23194ms step_avg:87.20ms
step:267/1680 train_time:23282ms step_avg:87.20ms
step:268/1680 train_time:23369ms step_avg:87.20ms
step:269/1680 train_time:23456ms step_avg:87.20ms
step:270/1680 train_time:23543ms step_avg:87.20ms
step:271/1680 train_time:23629ms step_avg:87.19ms
step:272/1680 train_time:23716ms step_avg:87.19ms
step:273/1680 train_time:23803ms step_avg:87.19ms
step:274/1680 train_time:23891ms step_avg:87.19ms
step:275/1680 train_time:23978ms step_avg:87.19ms
step:276/1680 train_time:24067ms step_avg:87.20ms
step:277/1680 train_time:24155ms step_avg:87.20ms
step:278/1680 train_time:24243ms step_avg:87.20ms
step:279/1680 train_time:24330ms step_avg:87.20ms
step:280/1680 train_time:24417ms step_avg:87.20ms
step:281/1680 train_time:24504ms step_avg:87.20ms
step:282/1680 train_time:24590ms step_avg:87.20ms
step:283/1680 train_time:24677ms step_avg:87.20ms
step:284/1680 train_time:24763ms step_avg:87.19ms
step:285/1680 train_time:24850ms step_avg:87.19ms
step:286/1680 train_time:24938ms step_avg:87.20ms
step:287/1680 train_time:25026ms step_avg:87.20ms
step:288/1680 train_time:25114ms step_avg:87.20ms
step:289/1680 train_time:25201ms step_avg:87.20ms
step:290/1680 train_time:25289ms step_avg:87.20ms
step:291/1680 train_time:25376ms step_avg:87.20ms
step:292/1680 train_time:25464ms step_avg:87.21ms
step:293/1680 train_time:25550ms step_avg:87.20ms
step:294/1680 train_time:25637ms step_avg:87.20ms
step:295/1680 train_time:25724ms step_avg:87.20ms
step:296/1680 train_time:25811ms step_avg:87.20ms
step:297/1680 train_time:25898ms step_avg:87.20ms
step:298/1680 train_time:25986ms step_avg:87.20ms
step:299/1680 train_time:26074ms step_avg:87.20ms
step:300/1680 train_time:26161ms step_avg:87.20ms
step:301/1680 train_time:26248ms step_avg:87.20ms
step:302/1680 train_time:26335ms step_avg:87.20ms
step:303/1680 train_time:26422ms step_avg:87.20ms
step:304/1680 train_time:26510ms step_avg:87.20ms
step:305/1680 train_time:26596ms step_avg:87.20ms
step:306/1680 train_time:26683ms step_avg:87.20ms
step:307/1680 train_time:26770ms step_avg:87.20ms
step:308/1680 train_time:26856ms step_avg:87.20ms
step:309/1680 train_time:26944ms step_avg:87.20ms
step:310/1680 train_time:27031ms step_avg:87.20ms
step:311/1680 train_time:27119ms step_avg:87.20ms
step:312/1680 train_time:27206ms step_avg:87.20ms
step:313/1680 train_time:27294ms step_avg:87.20ms
step:314/1680 train_time:27381ms step_avg:87.20ms
step:315/1680 train_time:27469ms step_avg:87.20ms
step:316/1680 train_time:27556ms step_avg:87.20ms
step:317/1680 train_time:27643ms step_avg:87.20ms
step:318/1680 train_time:27730ms step_avg:87.20ms
step:319/1680 train_time:27816ms step_avg:87.20ms
step:320/1680 train_time:27903ms step_avg:87.20ms
step:321/1680 train_time:27990ms step_avg:87.20ms
step:322/1680 train_time:28077ms step_avg:87.20ms
step:323/1680 train_time:28164ms step_avg:87.20ms
step:324/1680 train_time:28252ms step_avg:87.20ms
step:325/1680 train_time:28339ms step_avg:87.20ms
step:326/1680 train_time:28427ms step_avg:87.20ms
step:327/1680 train_time:28514ms step_avg:87.20ms
step:328/1680 train_time:28601ms step_avg:87.20ms
step:329/1680 train_time:28688ms step_avg:87.20ms
step:330/1680 train_time:28775ms step_avg:87.20ms
step:331/1680 train_time:28862ms step_avg:87.20ms
step:332/1680 train_time:28949ms step_avg:87.20ms
step:333/1680 train_time:29037ms step_avg:87.20ms
step:334/1680 train_time:29124ms step_avg:87.20ms
step:335/1680 train_time:29211ms step_avg:87.20ms
step:336/1680 train_time:29299ms step_avg:87.20ms
step:337/1680 train_time:29387ms step_avg:87.20ms
step:338/1680 train_time:29473ms step_avg:87.20ms
step:339/1680 train_time:29560ms step_avg:87.20ms
step:340/1680 train_time:29647ms step_avg:87.20ms
step:341/1680 train_time:29735ms step_avg:87.20ms
step:342/1680 train_time:29821ms step_avg:87.20ms
step:343/1680 train_time:29908ms step_avg:87.20ms
step:344/1680 train_time:29996ms step_avg:87.20ms
step:345/1680 train_time:30084ms step_avg:87.20ms
step:346/1680 train_time:30171ms step_avg:87.20ms
step:347/1680 train_time:30259ms step_avg:87.20ms
step:348/1680 train_time:30346ms step_avg:87.20ms
step:349/1680 train_time:30433ms step_avg:87.20ms
step:350/1680 train_time:30520ms step_avg:87.20ms
step:351/1680 train_time:30607ms step_avg:87.20ms
step:352/1680 train_time:30694ms step_avg:87.20ms
step:353/1680 train_time:30781ms step_avg:87.20ms
step:354/1680 train_time:30868ms step_avg:87.20ms
step:355/1680 train_time:30955ms step_avg:87.20ms
step:356/1680 train_time:31043ms step_avg:87.20ms
step:357/1680 train_time:31130ms step_avg:87.20ms
step:358/1680 train_time:31217ms step_avg:87.20ms
step:359/1680 train_time:31305ms step_avg:87.20ms
step:360/1680 train_time:31392ms step_avg:87.20ms
step:361/1680 train_time:31479ms step_avg:87.20ms
step:362/1680 train_time:31567ms step_avg:87.20ms
step:363/1680 train_time:31654ms step_avg:87.20ms
step:364/1680 train_time:31741ms step_avg:87.20ms
step:365/1680 train_time:31828ms step_avg:87.20ms
step:366/1680 train_time:31915ms step_avg:87.20ms
step:367/1680 train_time:32002ms step_avg:87.20ms
step:368/1680 train_time:32089ms step_avg:87.20ms
step:369/1680 train_time:32176ms step_avg:87.20ms
step:370/1680 train_time:32264ms step_avg:87.20ms
step:371/1680 train_time:32351ms step_avg:87.20ms
step:372/1680 train_time:32439ms step_avg:87.20ms
step:373/1680 train_time:32527ms step_avg:87.20ms
step:374/1680 train_time:32614ms step_avg:87.20ms
step:375/1680 train_time:32701ms step_avg:87.20ms
step:375/1680 val_loss:3.8127 train_time:32789ms step_avg:87.44ms
step:376/1680 train_time:32808ms step_avg:87.25ms
step:377/1680 train_time:32877ms step_avg:87.21ms
step:378/1680 train_time:32970ms step_avg:87.22ms
step:379/1680 train_time:33059ms step_avg:87.23ms
step:380/1680 train_time:33146ms step_avg:87.23ms
step:381/1680 train_time:33232ms step_avg:87.22ms
step:382/1680 train_time:33319ms step_avg:87.22ms
step:383/1680 train_time:33405ms step_avg:87.22ms
step:384/1680 train_time:33491ms step_avg:87.22ms
step:385/1680 train_time:33578ms step_avg:87.22ms
step:386/1680 train_time:33664ms step_avg:87.21ms
step:387/1680 train_time:33751ms step_avg:87.21ms
step:388/1680 train_time:33839ms step_avg:87.22ms
step:389/1680 train_time:33928ms step_avg:87.22ms
step:390/1680 train_time:34016ms step_avg:87.22ms
step:391/1680 train_time:34104ms step_avg:87.22ms
step:392/1680 train_time:34191ms step_avg:87.22ms
step:393/1680 train_time:34279ms step_avg:87.22ms
step:394/1680 train_time:34365ms step_avg:87.22ms
step:395/1680 train_time:34452ms step_avg:87.22ms
step:396/1680 train_time:34538ms step_avg:87.22ms
step:397/1680 train_time:34625ms step_avg:87.22ms
step:398/1680 train_time:34711ms step_avg:87.21ms
step:399/1680 train_time:34799ms step_avg:87.22ms
step:400/1680 train_time:34887ms step_avg:87.22ms
step:401/1680 train_time:34975ms step_avg:87.22ms
step:402/1680 train_time:35063ms step_avg:87.22ms
step:403/1680 train_time:35150ms step_avg:87.22ms
step:404/1680 train_time:35238ms step_avg:87.22ms
step:405/1680 train_time:35325ms step_avg:87.22ms
step:406/1680 train_time:35411ms step_avg:87.22ms
step:407/1680 train_time:35498ms step_avg:87.22ms
step:408/1680 train_time:35585ms step_avg:87.22ms
step:409/1680 train_time:35671ms step_avg:87.22ms
step:410/1680 train_time:35758ms step_avg:87.22ms
step:411/1680 train_time:35846ms step_avg:87.22ms
step:412/1680 train_time:35933ms step_avg:87.22ms
step:413/1680 train_time:36021ms step_avg:87.22ms
step:414/1680 train_time:36108ms step_avg:87.22ms
step:415/1680 train_time:36195ms step_avg:87.22ms
step:416/1680 train_time:36283ms step_avg:87.22ms
step:417/1680 train_time:36370ms step_avg:87.22ms
step:418/1680 train_time:36456ms step_avg:87.22ms
step:419/1680 train_time:36543ms step_avg:87.22ms
step:420/1680 train_time:36630ms step_avg:87.21ms
step:421/1680 train_time:36717ms step_avg:87.21ms
step:422/1680 train_time:36804ms step_avg:87.21ms
step:423/1680 train_time:36891ms step_avg:87.21ms
step:424/1680 train_time:36979ms step_avg:87.22ms
step:425/1680 train_time:37067ms step_avg:87.22ms
step:426/1680 train_time:37154ms step_avg:87.22ms
step:427/1680 train_time:37241ms step_avg:87.22ms
step:428/1680 train_time:37329ms step_avg:87.22ms
step:429/1680 train_time:37416ms step_avg:87.22ms
step:430/1680 train_time:37503ms step_avg:87.22ms
step:431/1680 train_time:37589ms step_avg:87.21ms
step:432/1680 train_time:37677ms step_avg:87.21ms
step:433/1680 train_time:37764ms step_avg:87.21ms
step:434/1680 train_time:37851ms step_avg:87.21ms
step:435/1680 train_time:37938ms step_avg:87.21ms
step:436/1680 train_time:38026ms step_avg:87.21ms
step:437/1680 train_time:38112ms step_avg:87.21ms
step:438/1680 train_time:38199ms step_avg:87.21ms
step:439/1680 train_time:38286ms step_avg:87.21ms
step:440/1680 train_time:38374ms step_avg:87.21ms
step:441/1680 train_time:38461ms step_avg:87.21ms
step:442/1680 train_time:38547ms step_avg:87.21ms
step:443/1680 train_time:38634ms step_avg:87.21ms
step:444/1680 train_time:38721ms step_avg:87.21ms
step:445/1680 train_time:38808ms step_avg:87.21ms
step:446/1680 train_time:38895ms step_avg:87.21ms
step:447/1680 train_time:38983ms step_avg:87.21ms
step:448/1680 train_time:39071ms step_avg:87.21ms
step:449/1680 train_time:39158ms step_avg:87.21ms
step:450/1680 train_time:39245ms step_avg:87.21ms
step:451/1680 train_time:39332ms step_avg:87.21ms
step:452/1680 train_time:39420ms step_avg:87.21ms
step:453/1680 train_time:39506ms step_avg:87.21ms
step:454/1680 train_time:39593ms step_avg:87.21ms
step:455/1680 train_time:39680ms step_avg:87.21ms
step:456/1680 train_time:39766ms step_avg:87.21ms
step:457/1680 train_time:39854ms step_avg:87.21ms
step:458/1680 train_time:39941ms step_avg:87.21ms
step:459/1680 train_time:40028ms step_avg:87.21ms
step:460/1680 train_time:40115ms step_avg:87.21ms
step:461/1680 train_time:40202ms step_avg:87.21ms
step:462/1680 train_time:40289ms step_avg:87.21ms
step:463/1680 train_time:40377ms step_avg:87.21ms
step:464/1680 train_time:40464ms step_avg:87.21ms
step:465/1680 train_time:40551ms step_avg:87.21ms
step:466/1680 train_time:40638ms step_avg:87.21ms
step:467/1680 train_time:40725ms step_avg:87.20ms
step:468/1680 train_time:40811ms step_avg:87.20ms
step:469/1680 train_time:40899ms step_avg:87.20ms
step:470/1680 train_time:40987ms step_avg:87.21ms
step:471/1680 train_time:41074ms step_avg:87.21ms
step:472/1680 train_time:41161ms step_avg:87.21ms
step:473/1680 train_time:41248ms step_avg:87.21ms
step:474/1680 train_time:41335ms step_avg:87.20ms
step:475/1680 train_time:41423ms step_avg:87.21ms
step:476/1680 train_time:41510ms step_avg:87.20ms
step:477/1680 train_time:41596ms step_avg:87.20ms
step:478/1680 train_time:41684ms step_avg:87.20ms
step:479/1680 train_time:41770ms step_avg:87.20ms
step:480/1680 train_time:41857ms step_avg:87.20ms
step:481/1680 train_time:41944ms step_avg:87.20ms
step:482/1680 train_time:42031ms step_avg:87.20ms
step:483/1680 train_time:42119ms step_avg:87.20ms
step:484/1680 train_time:42206ms step_avg:87.20ms
step:485/1680 train_time:42293ms step_avg:87.20ms
step:486/1680 train_time:42380ms step_avg:87.20ms
step:487/1680 train_time:42467ms step_avg:87.20ms
step:488/1680 train_time:42555ms step_avg:87.20ms
step:489/1680 train_time:42642ms step_avg:87.20ms
step:490/1680 train_time:42729ms step_avg:87.20ms
step:491/1680 train_time:42816ms step_avg:87.20ms
step:492/1680 train_time:42903ms step_avg:87.20ms
step:493/1680 train_time:42990ms step_avg:87.20ms
step:494/1680 train_time:43077ms step_avg:87.20ms
step:495/1680 train_time:43165ms step_avg:87.20ms
step:496/1680 train_time:43252ms step_avg:87.20ms
step:497/1680 train_time:43339ms step_avg:87.20ms
step:498/1680 train_time:43426ms step_avg:87.20ms
step:499/1680 train_time:43514ms step_avg:87.20ms
step:500/1680 train_time:43601ms step_avg:87.20ms
step:500/1680 val_loss:3.7153 train_time:43689ms step_avg:87.38ms
step:501/1680 train_time:43708ms step_avg:87.24ms
step:502/1680 train_time:43777ms step_avg:87.21ms
step:503/1680 train_time:43869ms step_avg:87.22ms
step:504/1680 train_time:43958ms step_avg:87.22ms
step:505/1680 train_time:44046ms step_avg:87.22ms
step:506/1680 train_time:44133ms step_avg:87.22ms
step:507/1680 train_time:44219ms step_avg:87.22ms
step:508/1680 train_time:44306ms step_avg:87.22ms
step:509/1680 train_time:44392ms step_avg:87.21ms
step:510/1680 train_time:44478ms step_avg:87.21ms
step:511/1680 train_time:44564ms step_avg:87.21ms
step:512/1680 train_time:44652ms step_avg:87.21ms
step:513/1680 train_time:44739ms step_avg:87.21ms
step:514/1680 train_time:44828ms step_avg:87.21ms
step:515/1680 train_time:44917ms step_avg:87.22ms
step:516/1680 train_time:45005ms step_avg:87.22ms
step:517/1680 train_time:45092ms step_avg:87.22ms
step:518/1680 train_time:45180ms step_avg:87.22ms
step:519/1680 train_time:45267ms step_avg:87.22ms
step:520/1680 train_time:45353ms step_avg:87.22ms
step:521/1680 train_time:45440ms step_avg:87.22ms
step:522/1680 train_time:45526ms step_avg:87.21ms
step:523/1680 train_time:45612ms step_avg:87.21ms
step:524/1680 train_time:45699ms step_avg:87.21ms
step:525/1680 train_time:45787ms step_avg:87.21ms
step:526/1680 train_time:45876ms step_avg:87.22ms
step:527/1680 train_time:45964ms step_avg:87.22ms
step:528/1680 train_time:46051ms step_avg:87.22ms
step:529/1680 train_time:46139ms step_avg:87.22ms
step:530/1680 train_time:46226ms step_avg:87.22ms
step:531/1680 train_time:46312ms step_avg:87.22ms
step:532/1680 train_time:46399ms step_avg:87.22ms
step:533/1680 train_time:46486ms step_avg:87.22ms
step:534/1680 train_time:46572ms step_avg:87.21ms
step:535/1680 train_time:46659ms step_avg:87.21ms
step:536/1680 train_time:46746ms step_avg:87.21ms
step:537/1680 train_time:46834ms step_avg:87.21ms
step:538/1680 train_time:46921ms step_avg:87.21ms
step:539/1680 train_time:47009ms step_avg:87.21ms
step:540/1680 train_time:47096ms step_avg:87.21ms
step:541/1680 train_time:47183ms step_avg:87.22ms
step:542/1680 train_time:47270ms step_avg:87.21ms
step:543/1680 train_time:47357ms step_avg:87.21ms
step:544/1680 train_time:47444ms step_avg:87.21ms
step:545/1680 train_time:47531ms step_avg:87.21ms
step:546/1680 train_time:47617ms step_avg:87.21ms
step:547/1680 train_time:47705ms step_avg:87.21ms
step:548/1680 train_time:47792ms step_avg:87.21ms
step:549/1680 train_time:47881ms step_avg:87.21ms
step:550/1680 train_time:47970ms step_avg:87.22ms
step:551/1680 train_time:48058ms step_avg:87.22ms
step:552/1680 train_time:48148ms step_avg:87.22ms
step:553/1680 train_time:48237ms step_avg:87.23ms
step:554/1680 train_time:48324ms step_avg:87.23ms
step:555/1680 train_time:48412ms step_avg:87.23ms
step:556/1680 train_time:48501ms step_avg:87.23ms
step:557/1680 train_time:48589ms step_avg:87.23ms
step:558/1680 train_time:48677ms step_avg:87.23ms
step:559/1680 train_time:48766ms step_avg:87.24ms
step:560/1680 train_time:48854ms step_avg:87.24ms
step:561/1680 train_time:48943ms step_avg:87.24ms
step:562/1680 train_time:49031ms step_avg:87.24ms
step:563/1680 train_time:49120ms step_avg:87.25ms
step:564/1680 train_time:49208ms step_avg:87.25ms
step:565/1680 train_time:49296ms step_avg:87.25ms
step:566/1680 train_time:49385ms step_avg:87.25ms
step:567/1680 train_time:49473ms step_avg:87.25ms
step:568/1680 train_time:49562ms step_avg:87.26ms
step:569/1680 train_time:49650ms step_avg:87.26ms
step:570/1680 train_time:49738ms step_avg:87.26ms
step:571/1680 train_time:49827ms step_avg:87.26ms
step:572/1680 train_time:49916ms step_avg:87.27ms
step:573/1680 train_time:50005ms step_avg:87.27ms
step:574/1680 train_time:50093ms step_avg:87.27ms
step:575/1680 train_time:50181ms step_avg:87.27ms
step:576/1680 train_time:50269ms step_avg:87.27ms
step:577/1680 train_time:50358ms step_avg:87.28ms
step:578/1680 train_time:50446ms step_avg:87.28ms
step:579/1680 train_time:50535ms step_avg:87.28ms
step:580/1680 train_time:50623ms step_avg:87.28ms
step:581/1680 train_time:50710ms step_avg:87.28ms
step:582/1680 train_time:50799ms step_avg:87.28ms
step:583/1680 train_time:50888ms step_avg:87.29ms
step:584/1680 train_time:50978ms step_avg:87.29ms
step:585/1680 train_time:51066ms step_avg:87.29ms
step:586/1680 train_time:51154ms step_avg:87.29ms
step:587/1680 train_time:51243ms step_avg:87.30ms
step:588/1680 train_time:51330ms step_avg:87.30ms
step:589/1680 train_time:51419ms step_avg:87.30ms
step:590/1680 train_time:51508ms step_avg:87.30ms
step:591/1680 train_time:51595ms step_avg:87.30ms
step:592/1680 train_time:51683ms step_avg:87.30ms
step:593/1680 train_time:51772ms step_avg:87.30ms
step:594/1680 train_time:51860ms step_avg:87.31ms
step:595/1680 train_time:51949ms step_avg:87.31ms
step:596/1680 train_time:52038ms step_avg:87.31ms
step:597/1680 train_time:52126ms step_avg:87.31ms
step:598/1680 train_time:52215ms step_avg:87.32ms
step:599/1680 train_time:52304ms step_avg:87.32ms
step:600/1680 train_time:52392ms step_avg:87.32ms
step:601/1680 train_time:52481ms step_avg:87.32ms
step:602/1680 train_time:52570ms step_avg:87.33ms
step:603/1680 train_time:52658ms step_avg:87.33ms
step:604/1680 train_time:52747ms step_avg:87.33ms
step:605/1680 train_time:52836ms step_avg:87.33ms
step:606/1680 train_time:52924ms step_avg:87.33ms
step:607/1680 train_time:53012ms step_avg:87.33ms
step:608/1680 train_time:53099ms step_avg:87.33ms
step:609/1680 train_time:53188ms step_avg:87.34ms
step:610/1680 train_time:53276ms step_avg:87.34ms
step:611/1680 train_time:53364ms step_avg:87.34ms
step:612/1680 train_time:53452ms step_avg:87.34ms
step:613/1680 train_time:53541ms step_avg:87.34ms
step:614/1680 train_time:53629ms step_avg:87.34ms
step:615/1680 train_time:53719ms step_avg:87.35ms
step:616/1680 train_time:53807ms step_avg:87.35ms
step:617/1680 train_time:53896ms step_avg:87.35ms
step:618/1680 train_time:53985ms step_avg:87.35ms
step:619/1680 train_time:54073ms step_avg:87.35ms
step:620/1680 train_time:54161ms step_avg:87.36ms
step:621/1680 train_time:54250ms step_avg:87.36ms
step:622/1680 train_time:54338ms step_avg:87.36ms
step:623/1680 train_time:54427ms step_avg:87.36ms
step:624/1680 train_time:54515ms step_avg:87.36ms
step:625/1680 train_time:54603ms step_avg:87.37ms
step:625/1680 val_loss:3.6149 train_time:54693ms step_avg:87.51ms
step:626/1680 train_time:54713ms step_avg:87.40ms
step:627/1680 train_time:54786ms step_avg:87.38ms
step:628/1680 train_time:54875ms step_avg:87.38ms
step:629/1680 train_time:54966ms step_avg:87.39ms
step:630/1680 train_time:55055ms step_avg:87.39ms
step:631/1680 train_time:55143ms step_avg:87.39ms
step:632/1680 train_time:55229ms step_avg:87.39ms
step:633/1680 train_time:55317ms step_avg:87.39ms
step:634/1680 train_time:55404ms step_avg:87.39ms
step:635/1680 train_time:55491ms step_avg:87.39ms
step:636/1680 train_time:55579ms step_avg:87.39ms
step:637/1680 train_time:55672ms step_avg:87.40ms
step:638/1680 train_time:55762ms step_avg:87.40ms
step:639/1680 train_time:55851ms step_avg:87.40ms
step:640/1680 train_time:55942ms step_avg:87.41ms
step:641/1680 train_time:56030ms step_avg:87.41ms
step:642/1680 train_time:56118ms step_avg:87.41ms
step:643/1680 train_time:56206ms step_avg:87.41ms
step:644/1680 train_time:56293ms step_avg:87.41ms
step:645/1680 train_time:56381ms step_avg:87.41ms
step:646/1680 train_time:56468ms step_avg:87.41ms
step:647/1680 train_time:56557ms step_avg:87.41ms
step:648/1680 train_time:56646ms step_avg:87.42ms
step:649/1680 train_time:56735ms step_avg:87.42ms
step:650/1680 train_time:56824ms step_avg:87.42ms
step:651/1680 train_time:56913ms step_avg:87.42ms
step:652/1680 train_time:57003ms step_avg:87.43ms
step:653/1680 train_time:57091ms step_avg:87.43ms
step:654/1680 train_time:57179ms step_avg:87.43ms
step:655/1680 train_time:57267ms step_avg:87.43ms
step:656/1680 train_time:57354ms step_avg:87.43ms
step:657/1680 train_time:57443ms step_avg:87.43ms
step:658/1680 train_time:57531ms step_avg:87.43ms
step:659/1680 train_time:57619ms step_avg:87.43ms
step:660/1680 train_time:57708ms step_avg:87.44ms
step:661/1680 train_time:57797ms step_avg:87.44ms
step:662/1680 train_time:57886ms step_avg:87.44ms
step:663/1680 train_time:57975ms step_avg:87.44ms
step:664/1680 train_time:58064ms step_avg:87.45ms
step:665/1680 train_time:58152ms step_avg:87.45ms
step:666/1680 train_time:58240ms step_avg:87.45ms
step:667/1680 train_time:58327ms step_avg:87.45ms
step:668/1680 train_time:58416ms step_avg:87.45ms
step:669/1680 train_time:58504ms step_avg:87.45ms
step:670/1680 train_time:58593ms step_avg:87.45ms
step:671/1680 train_time:58682ms step_avg:87.45ms
step:672/1680 train_time:58771ms step_avg:87.46ms
step:673/1680 train_time:58859ms step_avg:87.46ms
step:674/1680 train_time:58948ms step_avg:87.46ms
step:675/1680 train_time:59036ms step_avg:87.46ms
step:676/1680 train_time:59125ms step_avg:87.46ms
step:677/1680 train_time:59214ms step_avg:87.47ms
step:678/1680 train_time:59302ms step_avg:87.47ms
step:679/1680 train_time:59390ms step_avg:87.47ms
step:680/1680 train_time:59478ms step_avg:87.47ms
step:681/1680 train_time:59565ms step_avg:87.47ms
step:682/1680 train_time:59653ms step_avg:87.47ms
step:683/1680 train_time:59743ms step_avg:87.47ms
step:684/1680 train_time:59831ms step_avg:87.47ms
step:685/1680 train_time:59921ms step_avg:87.48ms
step:686/1680 train_time:60009ms step_avg:87.48ms
step:687/1680 train_time:60097ms step_avg:87.48ms
step:688/1680 train_time:60185ms step_avg:87.48ms
step:689/1680 train_time:60274ms step_avg:87.48ms
step:690/1680 train_time:60362ms step_avg:87.48ms
step:691/1680 train_time:60450ms step_avg:87.48ms
step:692/1680 train_time:60538ms step_avg:87.48ms
step:693/1680 train_time:60626ms step_avg:87.48ms
step:694/1680 train_time:60715ms step_avg:87.49ms
step:695/1680 train_time:60803ms step_avg:87.49ms
step:696/1680 train_time:60892ms step_avg:87.49ms
step:697/1680 train_time:60980ms step_avg:87.49ms
step:698/1680 train_time:61069ms step_avg:87.49ms
step:699/1680 train_time:61157ms step_avg:87.49ms
step:700/1680 train_time:61245ms step_avg:87.49ms
step:701/1680 train_time:61333ms step_avg:87.49ms
step:702/1680 train_time:61422ms step_avg:87.50ms
step:703/1680 train_time:61509ms step_avg:87.50ms
step:704/1680 train_time:61597ms step_avg:87.50ms
step:705/1680 train_time:61685ms step_avg:87.50ms
step:706/1680 train_time:61774ms step_avg:87.50ms
step:707/1680 train_time:61862ms step_avg:87.50ms
step:708/1680 train_time:61950ms step_avg:87.50ms
step:709/1680 train_time:62039ms step_avg:87.50ms
step:710/1680 train_time:62126ms step_avg:87.50ms
step:711/1680 train_time:62215ms step_avg:87.50ms
step:712/1680 train_time:62303ms step_avg:87.50ms
step:713/1680 train_time:62392ms step_avg:87.51ms
step:714/1680 train_time:62480ms step_avg:87.51ms
step:715/1680 train_time:62568ms step_avg:87.51ms
step:716/1680 train_time:62657ms step_avg:87.51ms
step:717/1680 train_time:62745ms step_avg:87.51ms
step:718/1680 train_time:62833ms step_avg:87.51ms
step:719/1680 train_time:62921ms step_avg:87.51ms
step:720/1680 train_time:63010ms step_avg:87.51ms
step:721/1680 train_time:63098ms step_avg:87.51ms
step:722/1680 train_time:63186ms step_avg:87.51ms
step:723/1680 train_time:63274ms step_avg:87.52ms
step:724/1680 train_time:63362ms step_avg:87.52ms
step:725/1680 train_time:63450ms step_avg:87.52ms
step:726/1680 train_time:63538ms step_avg:87.52ms
step:727/1680 train_time:63627ms step_avg:87.52ms
step:728/1680 train_time:63716ms step_avg:87.52ms
step:729/1680 train_time:63804ms step_avg:87.52ms
step:730/1680 train_time:63893ms step_avg:87.52ms
step:731/1680 train_time:63981ms step_avg:87.53ms
step:732/1680 train_time:64069ms step_avg:87.53ms
step:733/1680 train_time:64157ms step_avg:87.53ms
step:734/1680 train_time:64245ms step_avg:87.53ms
step:735/1680 train_time:64334ms step_avg:87.53ms
step:736/1680 train_time:64422ms step_avg:87.53ms
step:737/1680 train_time:64510ms step_avg:87.53ms
step:738/1680 train_time:64598ms step_avg:87.53ms
step:739/1680 train_time:64686ms step_avg:87.53ms
step:740/1680 train_time:64775ms step_avg:87.53ms
step:741/1680 train_time:64864ms step_avg:87.54ms
step:742/1680 train_time:64953ms step_avg:87.54ms
step:743/1680 train_time:65042ms step_avg:87.54ms
step:744/1680 train_time:65130ms step_avg:87.54ms
step:745/1680 train_time:65217ms step_avg:87.54ms
step:746/1680 train_time:65305ms step_avg:87.54ms
step:747/1680 train_time:65394ms step_avg:87.54ms
step:748/1680 train_time:65483ms step_avg:87.54ms
step:749/1680 train_time:65572ms step_avg:87.55ms
step:750/1680 train_time:65660ms step_avg:87.55ms
step:750/1680 val_loss:3.5647 train_time:65750ms step_avg:87.67ms
step:751/1680 train_time:65769ms step_avg:87.58ms
step:752/1680 train_time:65840ms step_avg:87.55ms
step:753/1680 train_time:65931ms step_avg:87.56ms
step:754/1680 train_time:66019ms step_avg:87.56ms
step:755/1680 train_time:66106ms step_avg:87.56ms
step:756/1680 train_time:66193ms step_avg:87.56ms
step:757/1680 train_time:66281ms step_avg:87.56ms
step:758/1680 train_time:66368ms step_avg:87.56ms
step:759/1680 train_time:66455ms step_avg:87.56ms
step:760/1680 train_time:66544ms step_avg:87.56ms
step:761/1680 train_time:66631ms step_avg:87.56ms
step:762/1680 train_time:66720ms step_avg:87.56ms
step:763/1680 train_time:66810ms step_avg:87.56ms
step:764/1680 train_time:66900ms step_avg:87.57ms
step:765/1680 train_time:66989ms step_avg:87.57ms
step:766/1680 train_time:67077ms step_avg:87.57ms
step:767/1680 train_time:67165ms step_avg:87.57ms
step:768/1680 train_time:67253ms step_avg:87.57ms
step:769/1680 train_time:67340ms step_avg:87.57ms
step:770/1680 train_time:67427ms step_avg:87.57ms
step:771/1680 train_time:67515ms step_avg:87.57ms
step:772/1680 train_time:67603ms step_avg:87.57ms
step:773/1680 train_time:67693ms step_avg:87.57ms
step:774/1680 train_time:67783ms step_avg:87.57ms
step:775/1680 train_time:67872ms step_avg:87.58ms
step:776/1680 train_time:67961ms step_avg:87.58ms
step:777/1680 train_time:68050ms step_avg:87.58ms
step:778/1680 train_time:68138ms step_avg:87.58ms
step:779/1680 train_time:68226ms step_avg:87.58ms
step:780/1680 train_time:68314ms step_avg:87.58ms
step:781/1680 train_time:68401ms step_avg:87.58ms
step:782/1680 train_time:68489ms step_avg:87.58ms
step:783/1680 train_time:68577ms step_avg:87.58ms
step:784/1680 train_time:68666ms step_avg:87.58ms
step:785/1680 train_time:68756ms step_avg:87.59ms
step:786/1680 train_time:68845ms step_avg:87.59ms
step:787/1680 train_time:68935ms step_avg:87.59ms
step:788/1680 train_time:69023ms step_avg:87.59ms
step:789/1680 train_time:69112ms step_avg:87.59ms
step:790/1680 train_time:69200ms step_avg:87.60ms
step:791/1680 train_time:69289ms step_avg:87.60ms
step:792/1680 train_time:69376ms step_avg:87.60ms
step:793/1680 train_time:69464ms step_avg:87.60ms
step:794/1680 train_time:69553ms step_avg:87.60ms
step:795/1680 train_time:69641ms step_avg:87.60ms
step:796/1680 train_time:69730ms step_avg:87.60ms
step:797/1680 train_time:69819ms step_avg:87.60ms
step:798/1680 train_time:69908ms step_avg:87.60ms
step:799/1680 train_time:69997ms step_avg:87.61ms
step:800/1680 train_time:70085ms step_avg:87.61ms
step:801/1680 train_time:70173ms step_avg:87.61ms
step:802/1680 train_time:70261ms step_avg:87.61ms
step:803/1680 train_time:70349ms step_avg:87.61ms
step:804/1680 train_time:70437ms step_avg:87.61ms
step:805/1680 train_time:70525ms step_avg:87.61ms
step:806/1680 train_time:70613ms step_avg:87.61ms
step:807/1680 train_time:70702ms step_avg:87.61ms
step:808/1680 train_time:70791ms step_avg:87.61ms
step:809/1680 train_time:70880ms step_avg:87.61ms
step:810/1680 train_time:70969ms step_avg:87.62ms
step:811/1680 train_time:71057ms step_avg:87.62ms
step:812/1680 train_time:71146ms step_avg:87.62ms
step:813/1680 train_time:71234ms step_avg:87.62ms
step:814/1680 train_time:71322ms step_avg:87.62ms
step:815/1680 train_time:71410ms step_avg:87.62ms
step:816/1680 train_time:71498ms step_avg:87.62ms
step:817/1680 train_time:71586ms step_avg:87.62ms
step:818/1680 train_time:71675ms step_avg:87.62ms
step:819/1680 train_time:71763ms step_avg:87.62ms
step:820/1680 train_time:71853ms step_avg:87.63ms
step:821/1680 train_time:71941ms step_avg:87.63ms
step:822/1680 train_time:72030ms step_avg:87.63ms
step:823/1680 train_time:72118ms step_avg:87.63ms
step:824/1680 train_time:72207ms step_avg:87.63ms
step:825/1680 train_time:72296ms step_avg:87.63ms
step:826/1680 train_time:72384ms step_avg:87.63ms
step:827/1680 train_time:72472ms step_avg:87.63ms
step:828/1680 train_time:72560ms step_avg:87.63ms
step:829/1680 train_time:72648ms step_avg:87.63ms
step:830/1680 train_time:72736ms step_avg:87.63ms
step:831/1680 train_time:72824ms step_avg:87.63ms
step:832/1680 train_time:72913ms step_avg:87.64ms
step:833/1680 train_time:73001ms step_avg:87.64ms
step:834/1680 train_time:73090ms step_avg:87.64ms
step:835/1680 train_time:73178ms step_avg:87.64ms
step:836/1680 train_time:73266ms step_avg:87.64ms
step:837/1680 train_time:73356ms step_avg:87.64ms
step:838/1680 train_time:73444ms step_avg:87.64ms
step:839/1680 train_time:73532ms step_avg:87.64ms
step:840/1680 train_time:73621ms step_avg:87.64ms
step:841/1680 train_time:73710ms step_avg:87.65ms
step:842/1680 train_time:73798ms step_avg:87.65ms
step:843/1680 train_time:73887ms step_avg:87.65ms
step:844/1680 train_time:73975ms step_avg:87.65ms
step:845/1680 train_time:74064ms step_avg:87.65ms
step:846/1680 train_time:74153ms step_avg:87.65ms
step:847/1680 train_time:74241ms step_avg:87.65ms
step:848/1680 train_time:74329ms step_avg:87.65ms
step:849/1680 train_time:74417ms step_avg:87.65ms
step:850/1680 train_time:74505ms step_avg:87.65ms
step:851/1680 train_time:74593ms step_avg:87.65ms
step:852/1680 train_time:74681ms step_avg:87.65ms
step:853/1680 train_time:74769ms step_avg:87.65ms
step:854/1680 train_time:74857ms step_avg:87.65ms
step:855/1680 train_time:74945ms step_avg:87.66ms
step:856/1680 train_time:75034ms step_avg:87.66ms
step:857/1680 train_time:75122ms step_avg:87.66ms
step:858/1680 train_time:75211ms step_avg:87.66ms
step:859/1680 train_time:75299ms step_avg:87.66ms
step:860/1680 train_time:75387ms step_avg:87.66ms
step:861/1680 train_time:75476ms step_avg:87.66ms
step:862/1680 train_time:75565ms step_avg:87.66ms
step:863/1680 train_time:75653ms step_avg:87.66ms
step:864/1680 train_time:75741ms step_avg:87.66ms
step:865/1680 train_time:75829ms step_avg:87.66ms
step:866/1680 train_time:75918ms step_avg:87.67ms
step:867/1680 train_time:76007ms step_avg:87.67ms
step:868/1680 train_time:76095ms step_avg:87.67ms
step:869/1680 train_time:76184ms step_avg:87.67ms
step:870/1680 train_time:76273ms step_avg:87.67ms
step:871/1680 train_time:76360ms step_avg:87.67ms
step:872/1680 train_time:76448ms step_avg:87.67ms
step:873/1680 train_time:76537ms step_avg:87.67ms
step:874/1680 train_time:76625ms step_avg:87.67ms
step:875/1680 train_time:76715ms step_avg:87.67ms
step:875/1680 val_loss:3.5181 train_time:76804ms step_avg:87.78ms
step:876/1680 train_time:76823ms step_avg:87.70ms
step:877/1680 train_time:76895ms step_avg:87.68ms
step:878/1680 train_time:76988ms step_avg:87.69ms
step:879/1680 train_time:77079ms step_avg:87.69ms
step:880/1680 train_time:77167ms step_avg:87.69ms
step:881/1680 train_time:77254ms step_avg:87.69ms
step:882/1680 train_time:77342ms step_avg:87.69ms
step:883/1680 train_time:77429ms step_avg:87.69ms
step:884/1680 train_time:77516ms step_avg:87.69ms
step:885/1680 train_time:77604ms step_avg:87.69ms
step:886/1680 train_time:77692ms step_avg:87.69ms
step:887/1680 train_time:77782ms step_avg:87.69ms
step:888/1680 train_time:77872ms step_avg:87.69ms
step:889/1680 train_time:77963ms step_avg:87.70ms
step:890/1680 train_time:78054ms step_avg:87.70ms
step:891/1680 train_time:78143ms step_avg:87.70ms
step:892/1680 train_time:78232ms step_avg:87.70ms
step:893/1680 train_time:78320ms step_avg:87.70ms
step:894/1680 train_time:78407ms step_avg:87.70ms
step:895/1680 train_time:78495ms step_avg:87.70ms
step:896/1680 train_time:78583ms step_avg:87.70ms
step:897/1680 train_time:78670ms step_avg:87.70ms
step:898/1680 train_time:78758ms step_avg:87.70ms
step:899/1680 train_time:78846ms step_avg:87.70ms
step:900/1680 train_time:78936ms step_avg:87.71ms
step:901/1680 train_time:79025ms step_avg:87.71ms
step:902/1680 train_time:79115ms step_avg:87.71ms
step:903/1680 train_time:79204ms step_avg:87.71ms
step:904/1680 train_time:79292ms step_avg:87.71ms
step:905/1680 train_time:79379ms step_avg:87.71ms
step:906/1680 train_time:79467ms step_avg:87.71ms
step:907/1680 train_time:79555ms step_avg:87.71ms
step:908/1680 train_time:79643ms step_avg:87.71ms
step:909/1680 train_time:79730ms step_avg:87.71ms
step:910/1680 train_time:79819ms step_avg:87.71ms
step:911/1680 train_time:79908ms step_avg:87.71ms
step:912/1680 train_time:79996ms step_avg:87.72ms
step:913/1680 train_time:80085ms step_avg:87.72ms
step:914/1680 train_time:80173ms step_avg:87.72ms
step:915/1680 train_time:80262ms step_avg:87.72ms
step:916/1680 train_time:80351ms step_avg:87.72ms
step:917/1680 train_time:80439ms step_avg:87.72ms
step:918/1680 train_time:80526ms step_avg:87.72ms
step:919/1680 train_time:80615ms step_avg:87.72ms
step:920/1680 train_time:80703ms step_avg:87.72ms
step:921/1680 train_time:80791ms step_avg:87.72ms
step:922/1680 train_time:80880ms step_avg:87.72ms
step:923/1680 train_time:80968ms step_avg:87.72ms
step:924/1680 train_time:81056ms step_avg:87.72ms
step:925/1680 train_time:81145ms step_avg:87.72ms
step:926/1680 train_time:81234ms step_avg:87.73ms
step:927/1680 train_time:81323ms step_avg:87.73ms
step:928/1680 train_time:81411ms step_avg:87.73ms
step:929/1680 train_time:81500ms step_avg:87.73ms
step:930/1680 train_time:81587ms step_avg:87.73ms
step:931/1680 train_time:81675ms step_avg:87.73ms
step:932/1680 train_time:81763ms step_avg:87.73ms
step:933/1680 train_time:81853ms step_avg:87.73ms
step:934/1680 train_time:81941ms step_avg:87.73ms
step:935/1680 train_time:82030ms step_avg:87.73ms
step:936/1680 train_time:82119ms step_avg:87.73ms
step:937/1680 train_time:82207ms step_avg:87.73ms
step:938/1680 train_time:82296ms step_avg:87.74ms
step:939/1680 train_time:82384ms step_avg:87.74ms
step:940/1680 train_time:82472ms step_avg:87.74ms
step:941/1680 train_time:82560ms step_avg:87.74ms
step:942/1680 train_time:82648ms step_avg:87.74ms
step:943/1680 train_time:82736ms step_avg:87.74ms
step:944/1680 train_time:82824ms step_avg:87.74ms
step:945/1680 train_time:82913ms step_avg:87.74ms
step:946/1680 train_time:83002ms step_avg:87.74ms
step:947/1680 train_time:83090ms step_avg:87.74ms
step:948/1680 train_time:83179ms step_avg:87.74ms
step:949/1680 train_time:83268ms step_avg:87.74ms
step:950/1680 train_time:83356ms step_avg:87.74ms
step:951/1680 train_time:83444ms step_avg:87.74ms
step:952/1680 train_time:83532ms step_avg:87.74ms
step:953/1680 train_time:83621ms step_avg:87.74ms
step:954/1680 train_time:83709ms step_avg:87.74ms
step:955/1680 train_time:83797ms step_avg:87.75ms
step:956/1680 train_time:83885ms step_avg:87.75ms
step:957/1680 train_time:83974ms step_avg:87.75ms
step:958/1680 train_time:84062ms step_avg:87.75ms
step:959/1680 train_time:84150ms step_avg:87.75ms
step:960/1680 train_time:84240ms step_avg:87.75ms
step:961/1680 train_time:84328ms step_avg:87.75ms
step:962/1680 train_time:84419ms step_avg:87.75ms
step:963/1680 train_time:84507ms step_avg:87.75ms
step:964/1680 train_time:84596ms step_avg:87.75ms
step:965/1680 train_time:84683ms step_avg:87.75ms
step:966/1680 train_time:84772ms step_avg:87.76ms
step:967/1680 train_time:84861ms step_avg:87.76ms
step:968/1680 train_time:84949ms step_avg:87.76ms
step:969/1680 train_time:85037ms step_avg:87.76ms
step:970/1680 train_time:85126ms step_avg:87.76ms
step:971/1680 train_time:85216ms step_avg:87.76ms
step:972/1680 train_time:85304ms step_avg:87.76ms
step:973/1680 train_time:85393ms step_avg:87.76ms
step:974/1680 train_time:85482ms step_avg:87.76ms
step:975/1680 train_time:85570ms step_avg:87.76ms
step:976/1680 train_time:85659ms step_avg:87.77ms
step:977/1680 train_time:85747ms step_avg:87.77ms
step:978/1680 train_time:85835ms step_avg:87.77ms
step:979/1680 train_time:85923ms step_avg:87.77ms
step:980/1680 train_time:86011ms step_avg:87.77ms
step:981/1680 train_time:86100ms step_avg:87.77ms
step:982/1680 train_time:86188ms step_avg:87.77ms
step:983/1680 train_time:86277ms step_avg:87.77ms
step:984/1680 train_time:86366ms step_avg:87.77ms
step:985/1680 train_time:86455ms step_avg:87.77ms
step:986/1680 train_time:86543ms step_avg:87.77ms
step:987/1680 train_time:86633ms step_avg:87.77ms
step:988/1680 train_time:86721ms step_avg:87.77ms
step:989/1680 train_time:86810ms step_avg:87.78ms
step:990/1680 train_time:86898ms step_avg:87.78ms
step:991/1680 train_time:86986ms step_avg:87.78ms
step:992/1680 train_time:87074ms step_avg:87.78ms
step:993/1680 train_time:87162ms step_avg:87.78ms
step:994/1680 train_time:87251ms step_avg:87.78ms
step:995/1680 train_time:87340ms step_avg:87.78ms
step:996/1680 train_time:87428ms step_avg:87.78ms
step:997/1680 train_time:87517ms step_avg:87.78ms
step:998/1680 train_time:87607ms step_avg:87.78ms
step:999/1680 train_time:87695ms step_avg:87.78ms
step:1000/1680 train_time:87784ms step_avg:87.78ms
step:1000/1680 val_loss:3.4674 train_time:87873ms step_avg:87.87ms
step:1001/1680 train_time:87893ms step_avg:87.81ms
step:1002/1680 train_time:87966ms step_avg:87.79ms
step:1003/1680 train_time:88058ms step_avg:87.79ms
step:1004/1680 train_time:88147ms step_avg:87.80ms
step:1005/1680 train_time:88235ms step_avg:87.80ms
step:1006/1680 train_time:88322ms step_avg:87.80ms
step:1007/1680 train_time:88410ms step_avg:87.80ms
step:1008/1680 train_time:88497ms step_avg:87.79ms
step:1009/1680 train_time:88584ms step_avg:87.79ms
step:1010/1680 train_time:88671ms step_avg:87.79ms
step:1011/1680 train_time:88759ms step_avg:87.79ms
step:1012/1680 train_time:88848ms step_avg:87.79ms
step:1013/1680 train_time:88937ms step_avg:87.80ms
step:1014/1680 train_time:89028ms step_avg:87.80ms
step:1015/1680 train_time:89118ms step_avg:87.80ms
step:1016/1680 train_time:89207ms step_avg:87.80ms
step:1017/1680 train_time:89296ms step_avg:87.80ms
step:1018/1680 train_time:89384ms step_avg:87.80ms
step:1019/1680 train_time:89472ms step_avg:87.80ms
step:1020/1680 train_time:89558ms step_avg:87.80ms
step:1021/1680 train_time:89646ms step_avg:87.80ms
step:1022/1680 train_time:89733ms step_avg:87.80ms
step:1023/1680 train_time:89821ms step_avg:87.80ms
step:1024/1680 train_time:89911ms step_avg:87.80ms
step:1025/1680 train_time:90000ms step_avg:87.80ms
step:1026/1680 train_time:90089ms step_avg:87.81ms
step:1027/1680 train_time:90177ms step_avg:87.81ms
step:1028/1680 train_time:90267ms step_avg:87.81ms
step:1029/1680 train_time:90355ms step_avg:87.81ms
step:1030/1680 train_time:90443ms step_avg:87.81ms
step:1031/1680 train_time:90530ms step_avg:87.81ms
step:1032/1680 train_time:90618ms step_avg:87.81ms
step:1033/1680 train_time:90706ms step_avg:87.81ms
step:1034/1680 train_time:90794ms step_avg:87.81ms
step:1035/1680 train_time:90883ms step_avg:87.81ms
step:1036/1680 train_time:90971ms step_avg:87.81ms
step:1037/1680 train_time:91060ms step_avg:87.81ms
step:1038/1680 train_time:91149ms step_avg:87.81ms
step:1039/1680 train_time:91237ms step_avg:87.81ms
step:1040/1680 train_time:91327ms step_avg:87.81ms
step:1041/1680 train_time:91415ms step_avg:87.81ms
step:1042/1680 train_time:91503ms step_avg:87.81ms
step:1043/1680 train_time:91591ms step_avg:87.81ms
step:1044/1680 train_time:91679ms step_avg:87.81ms
step:1045/1680 train_time:91766ms step_avg:87.81ms
step:1046/1680 train_time:91855ms step_avg:87.82ms
step:1047/1680 train_time:91944ms step_avg:87.82ms
step:1048/1680 train_time:92033ms step_avg:87.82ms
step:1049/1680 train_time:92122ms step_avg:87.82ms
step:1050/1680 train_time:92211ms step_avg:87.82ms
step:1051/1680 train_time:92299ms step_avg:87.82ms
step:1052/1680 train_time:92388ms step_avg:87.82ms
step:1053/1680 train_time:92477ms step_avg:87.82ms
step:1054/1680 train_time:92565ms step_avg:87.82ms
step:1055/1680 train_time:92653ms step_avg:87.82ms
step:1056/1680 train_time:92741ms step_avg:87.82ms
step:1057/1680 train_time:92829ms step_avg:87.82ms
step:1058/1680 train_time:92917ms step_avg:87.82ms
step:1059/1680 train_time:93006ms step_avg:87.82ms
step:1060/1680 train_time:93094ms step_avg:87.82ms
step:1061/1680 train_time:93184ms step_avg:87.83ms
step:1062/1680 train_time:93272ms step_avg:87.83ms
step:1063/1680 train_time:93361ms step_avg:87.83ms
step:1064/1680 train_time:93451ms step_avg:87.83ms
step:1065/1680 train_time:93538ms step_avg:87.83ms
step:1066/1680 train_time:93627ms step_avg:87.83ms
step:1067/1680 train_time:93715ms step_avg:87.83ms
step:1068/1680 train_time:93803ms step_avg:87.83ms
step:1069/1680 train_time:93892ms step_avg:87.83ms
step:1070/1680 train_time:93980ms step_avg:87.83ms
step:1071/1680 train_time:94068ms step_avg:87.83ms
step:1072/1680 train_time:94156ms step_avg:87.83ms
step:1073/1680 train_time:94245ms step_avg:87.83ms
step:1074/1680 train_time:94333ms step_avg:87.83ms
step:1075/1680 train_time:94422ms step_avg:87.83ms
step:1076/1680 train_time:94511ms step_avg:87.84ms
step:1077/1680 train_time:94598ms step_avg:87.83ms
step:1078/1680 train_time:94687ms step_avg:87.84ms
step:1079/1680 train_time:94775ms step_avg:87.84ms
step:1080/1680 train_time:94864ms step_avg:87.84ms
step:1081/1680 train_time:94952ms step_avg:87.84ms
step:1082/1680 train_time:95040ms step_avg:87.84ms
step:1083/1680 train_time:95129ms step_avg:87.84ms
step:1084/1680 train_time:95217ms step_avg:87.84ms
step:1085/1680 train_time:95305ms step_avg:87.84ms
step:1086/1680 train_time:95395ms step_avg:87.84ms
step:1087/1680 train_time:95484ms step_avg:87.84ms
step:1088/1680 train_time:95572ms step_avg:87.84ms
step:1089/1680 train_time:95660ms step_avg:87.84ms
step:1090/1680 train_time:95749ms step_avg:87.84ms
step:1091/1680 train_time:95837ms step_avg:87.84ms
step:1092/1680 train_time:95927ms step_avg:87.84ms
step:1093/1680 train_time:96015ms step_avg:87.85ms
step:1094/1680 train_time:96103ms step_avg:87.85ms
step:1095/1680 train_time:96193ms step_avg:87.85ms
step:1096/1680 train_time:96282ms step_avg:87.85ms
step:1097/1680 train_time:96371ms step_avg:87.85ms
step:1098/1680 train_time:96461ms step_avg:87.85ms
step:1099/1680 train_time:96552ms step_avg:87.85ms
step:1100/1680 train_time:96640ms step_avg:87.85ms
step:1101/1680 train_time:96730ms step_avg:87.86ms
step:1102/1680 train_time:96818ms step_avg:87.86ms
step:1103/1680 train_time:96907ms step_avg:87.86ms
step:1104/1680 train_time:96997ms step_avg:87.86ms
step:1105/1680 train_time:97086ms step_avg:87.86ms
step:1106/1680 train_time:97176ms step_avg:87.86ms
step:1107/1680 train_time:97265ms step_avg:87.86ms
step:1108/1680 train_time:97354ms step_avg:87.86ms
step:1109/1680 train_time:97444ms step_avg:87.87ms
step:1110/1680 train_time:97533ms step_avg:87.87ms
step:1111/1680 train_time:97623ms step_avg:87.87ms
step:1112/1680 train_time:97712ms step_avg:87.87ms
step:1113/1680 train_time:97802ms step_avg:87.87ms
step:1114/1680 train_time:97891ms step_avg:87.87ms
step:1115/1680 train_time:97980ms step_avg:87.87ms
step:1116/1680 train_time:98070ms step_avg:87.88ms
step:1117/1680 train_time:98158ms step_avg:87.88ms
step:1118/1680 train_time:98248ms step_avg:87.88ms
step:1119/1680 train_time:98337ms step_avg:87.88ms
step:1120/1680 train_time:98425ms step_avg:87.88ms
step:1121/1680 train_time:98514ms step_avg:87.88ms
step:1122/1680 train_time:98603ms step_avg:87.88ms
step:1123/1680 train_time:98693ms step_avg:87.88ms
step:1124/1680 train_time:98782ms step_avg:87.88ms
step:1125/1680 train_time:98872ms step_avg:87.89ms
step:1125/1680 val_loss:3.4151 train_time:98961ms step_avg:87.97ms
step:1126/1680 train_time:98982ms step_avg:87.91ms
step:1127/1680 train_time:99055ms step_avg:87.89ms
step:1128/1680 train_time:99145ms step_avg:87.89ms
step:1129/1680 train_time:99236ms step_avg:87.90ms
step:1130/1680 train_time:99327ms step_avg:87.90ms
step:1131/1680 train_time:99415ms step_avg:87.90ms
step:1132/1680 train_time:99503ms step_avg:87.90ms
step:1133/1680 train_time:99591ms step_avg:87.90ms
step:1134/1680 train_time:99678ms step_avg:87.90ms
step:1135/1680 train_time:99766ms step_avg:87.90ms
step:1136/1680 train_time:99854ms step_avg:87.90ms
step:1137/1680 train_time:99944ms step_avg:87.90ms
step:1138/1680 train_time:100034ms step_avg:87.90ms
step:1139/1680 train_time:100126ms step_avg:87.91ms
step:1140/1680 train_time:100217ms step_avg:87.91ms
step:1141/1680 train_time:100307ms step_avg:87.91ms
step:1142/1680 train_time:100395ms step_avg:87.91ms
step:1143/1680 train_time:100484ms step_avg:87.91ms
step:1144/1680 train_time:100573ms step_avg:87.91ms
step:1145/1680 train_time:100661ms step_avg:87.91ms
step:1146/1680 train_time:100750ms step_avg:87.91ms
step:1147/1680 train_time:100838ms step_avg:87.91ms
step:1148/1680 train_time:100927ms step_avg:87.92ms
step:1149/1680 train_time:101018ms step_avg:87.92ms
step:1150/1680 train_time:101108ms step_avg:87.92ms
step:1151/1680 train_time:101198ms step_avg:87.92ms
step:1152/1680 train_time:101288ms step_avg:87.92ms
step:1153/1680 train_time:101377ms step_avg:87.92ms
step:1154/1680 train_time:101465ms step_avg:87.92ms
step:1155/1680 train_time:101554ms step_avg:87.93ms
step:1156/1680 train_time:101643ms step_avg:87.93ms
step:1157/1680 train_time:101731ms step_avg:87.93ms
step:1158/1680 train_time:101821ms step_avg:87.93ms
step:1159/1680 train_time:101911ms step_avg:87.93ms
step:1160/1680 train_time:102000ms step_avg:87.93ms
step:1161/1680 train_time:102091ms step_avg:87.93ms
step:1162/1680 train_time:102181ms step_avg:87.94ms
step:1163/1680 train_time:102270ms step_avg:87.94ms
step:1164/1680 train_time:102359ms step_avg:87.94ms
step:1165/1680 train_time:102448ms step_avg:87.94ms
step:1166/1680 train_time:102537ms step_avg:87.94ms
step:1167/1680 train_time:102625ms step_avg:87.94ms
step:1168/1680 train_time:102714ms step_avg:87.94ms
step:1169/1680 train_time:102803ms step_avg:87.94ms
step:1170/1680 train_time:102893ms step_avg:87.94ms
step:1171/1680 train_time:102981ms step_avg:87.94ms
step:1172/1680 train_time:103071ms step_avg:87.94ms
step:1173/1680 train_time:103160ms step_avg:87.95ms
step:1174/1680 train_time:103250ms step_avg:87.95ms
step:1175/1680 train_time:103339ms step_avg:87.95ms
step:1176/1680 train_time:103428ms step_avg:87.95ms
step:1177/1680 train_time:103516ms step_avg:87.95ms
step:1178/1680 train_time:103605ms step_avg:87.95ms
step:1179/1680 train_time:103694ms step_avg:87.95ms
step:1180/1680 train_time:103783ms step_avg:87.95ms
step:1181/1680 train_time:103872ms step_avg:87.95ms
step:1182/1680 train_time:103961ms step_avg:87.95ms
step:1183/1680 train_time:104051ms step_avg:87.96ms
step:1184/1680 train_time:104141ms step_avg:87.96ms
step:1185/1680 train_time:104230ms step_avg:87.96ms
step:1186/1680 train_time:104319ms step_avg:87.96ms
step:1187/1680 train_time:104409ms step_avg:87.96ms
step:1188/1680 train_time:104498ms step_avg:87.96ms
step:1189/1680 train_time:104587ms step_avg:87.96ms
step:1190/1680 train_time:104676ms step_avg:87.96ms
step:1191/1680 train_time:104765ms step_avg:87.96ms
step:1192/1680 train_time:104854ms step_avg:87.97ms
step:1193/1680 train_time:104944ms step_avg:87.97ms
step:1194/1680 train_time:105033ms step_avg:87.97ms
step:1195/1680 train_time:105122ms step_avg:87.97ms
step:1196/1680 train_time:105213ms step_avg:87.97ms
step:1197/1680 train_time:105302ms step_avg:87.97ms
step:1198/1680 train_time:105391ms step_avg:87.97ms
step:1199/1680 train_time:105479ms step_avg:87.97ms
step:1200/1680 train_time:105569ms step_avg:87.97ms
step:1201/1680 train_time:105658ms step_avg:87.97ms
step:1202/1680 train_time:105747ms step_avg:87.98ms
step:1203/1680 train_time:105836ms step_avg:87.98ms
step:1204/1680 train_time:105925ms step_avg:87.98ms
step:1205/1680 train_time:106015ms step_avg:87.98ms
step:1206/1680 train_time:106104ms step_avg:87.98ms
step:1207/1680 train_time:106193ms step_avg:87.98ms
step:1208/1680 train_time:106282ms step_avg:87.98ms
step:1209/1680 train_time:106372ms step_avg:87.98ms
step:1210/1680 train_time:106460ms step_avg:87.98ms
step:1211/1680 train_time:106549ms step_avg:87.98ms
step:1212/1680 train_time:106639ms step_avg:87.99ms
step:1213/1680 train_time:106730ms step_avg:87.99ms
step:1214/1680 train_time:106818ms step_avg:87.99ms
step:1215/1680 train_time:106907ms step_avg:87.99ms
step:1216/1680 train_time:106996ms step_avg:87.99ms
step:1217/1680 train_time:107087ms step_avg:87.99ms
step:1218/1680 train_time:107176ms step_avg:87.99ms
step:1219/1680 train_time:107266ms step_avg:88.00ms
step:1220/1680 train_time:107356ms step_avg:88.00ms
step:1221/1680 train_time:107447ms step_avg:88.00ms
step:1222/1680 train_time:107537ms step_avg:88.00ms
step:1223/1680 train_time:107626ms step_avg:88.00ms
step:1224/1680 train_time:107716ms step_avg:88.00ms
step:1225/1680 train_time:107805ms step_avg:88.00ms
step:1226/1680 train_time:107894ms step_avg:88.01ms
step:1227/1680 train_time:107984ms step_avg:88.01ms
step:1228/1680 train_time:108074ms step_avg:88.01ms
step:1229/1680 train_time:108163ms step_avg:88.01ms
step:1230/1680 train_time:108254ms step_avg:88.01ms
step:1231/1680 train_time:108343ms step_avg:88.01ms
step:1232/1680 train_time:108433ms step_avg:88.01ms
step:1233/1680 train_time:108522ms step_avg:88.01ms
step:1234/1680 train_time:108612ms step_avg:88.02ms
step:1235/1680 train_time:108701ms step_avg:88.02ms
step:1236/1680 train_time:108789ms step_avg:88.02ms
step:1237/1680 train_time:108878ms step_avg:88.02ms
step:1238/1680 train_time:108967ms step_avg:88.02ms
step:1239/1680 train_time:109057ms step_avg:88.02ms
step:1240/1680 train_time:109146ms step_avg:88.02ms
step:1241/1680 train_time:109235ms step_avg:88.02ms
step:1242/1680 train_time:109325ms step_avg:88.02ms
step:1243/1680 train_time:109414ms step_avg:88.02ms
step:1244/1680 train_time:109504ms step_avg:88.03ms
step:1245/1680 train_time:109593ms step_avg:88.03ms
step:1246/1680 train_time:109683ms step_avg:88.03ms
step:1247/1680 train_time:109772ms step_avg:88.03ms
step:1248/1680 train_time:109861ms step_avg:88.03ms
step:1249/1680 train_time:109949ms step_avg:88.03ms
step:1250/1680 train_time:110039ms step_avg:88.03ms
step:1250/1680 val_loss:3.3769 train_time:110130ms step_avg:88.10ms
step:1251/1680 train_time:110148ms step_avg:88.05ms
step:1252/1680 train_time:110221ms step_avg:88.04ms
step:1253/1680 train_time:110312ms step_avg:88.04ms
step:1254/1680 train_time:110402ms step_avg:88.04ms
step:1255/1680 train_time:110492ms step_avg:88.04ms
step:1256/1680 train_time:110580ms step_avg:88.04ms
step:1257/1680 train_time:110668ms step_avg:88.04ms
step:1258/1680 train_time:110757ms step_avg:88.04ms
step:1259/1680 train_time:110845ms step_avg:88.04ms
step:1260/1680 train_time:110934ms step_avg:88.04ms
step:1261/1680 train_time:111022ms step_avg:88.04ms
step:1262/1680 train_time:111113ms step_avg:88.05ms
step:1263/1680 train_time:111203ms step_avg:88.05ms
step:1264/1680 train_time:111294ms step_avg:88.05ms
step:1265/1680 train_time:111385ms step_avg:88.05ms
step:1266/1680 train_time:111474ms step_avg:88.05ms
step:1267/1680 train_time:111563ms step_avg:88.05ms
step:1268/1680 train_time:111651ms step_avg:88.05ms
step:1269/1680 train_time:111740ms step_avg:88.05ms
step:1270/1680 train_time:111829ms step_avg:88.05ms
step:1271/1680 train_time:111918ms step_avg:88.05ms
step:1272/1680 train_time:112006ms step_avg:88.06ms
step:1273/1680 train_time:112095ms step_avg:88.06ms
step:1274/1680 train_time:112184ms step_avg:88.06ms
step:1275/1680 train_time:112274ms step_avg:88.06ms
step:1276/1680 train_time:112365ms step_avg:88.06ms
step:1277/1680 train_time:112455ms step_avg:88.06ms
step:1278/1680 train_time:112544ms step_avg:88.06ms
step:1279/1680 train_time:112632ms step_avg:88.06ms
step:1280/1680 train_time:112721ms step_avg:88.06ms
step:1281/1680 train_time:112811ms step_avg:88.06ms
step:1282/1680 train_time:112900ms step_avg:88.07ms
step:1283/1680 train_time:112989ms step_avg:88.07ms
step:1284/1680 train_time:113078ms step_avg:88.07ms
step:1285/1680 train_time:113168ms step_avg:88.07ms
step:1286/1680 train_time:113256ms step_avg:88.07ms
step:1287/1680 train_time:113346ms step_avg:88.07ms
step:1288/1680 train_time:113435ms step_avg:88.07ms
step:1289/1680 train_time:113525ms step_avg:88.07ms
step:1290/1680 train_time:113614ms step_avg:88.07ms
step:1291/1680 train_time:113703ms step_avg:88.07ms
step:1292/1680 train_time:113792ms step_avg:88.07ms
step:1293/1680 train_time:113881ms step_avg:88.08ms
step:1294/1680 train_time:113971ms step_avg:88.08ms
step:1295/1680 train_time:114060ms step_avg:88.08ms
step:1296/1680 train_time:114151ms step_avg:88.08ms
step:1297/1680 train_time:114241ms step_avg:88.08ms
step:1298/1680 train_time:114330ms step_avg:88.08ms
step:1299/1680 train_time:114419ms step_avg:88.08ms
step:1300/1680 train_time:114508ms step_avg:88.08ms
step:1301/1680 train_time:114597ms step_avg:88.08ms
step:1302/1680 train_time:114687ms step_avg:88.08ms
step:1303/1680 train_time:114776ms step_avg:88.09ms
step:1304/1680 train_time:114866ms step_avg:88.09ms
step:1305/1680 train_time:114955ms step_avg:88.09ms
step:1306/1680 train_time:115045ms step_avg:88.09ms
step:1307/1680 train_time:115134ms step_avg:88.09ms
step:1308/1680 train_time:115224ms step_avg:88.09ms
step:1309/1680 train_time:115313ms step_avg:88.09ms
step:1310/1680 train_time:115403ms step_avg:88.09ms
step:1311/1680 train_time:115493ms step_avg:88.10ms
step:1312/1680 train_time:115581ms step_avg:88.10ms
step:1313/1680 train_time:115671ms step_avg:88.10ms
step:1314/1680 train_time:115760ms step_avg:88.10ms
step:1315/1680 train_time:115849ms step_avg:88.10ms
step:1316/1680 train_time:115939ms step_avg:88.10ms
step:1317/1680 train_time:116027ms step_avg:88.10ms
step:1318/1680 train_time:116117ms step_avg:88.10ms
step:1319/1680 train_time:116206ms step_avg:88.10ms
step:1320/1680 train_time:116295ms step_avg:88.10ms
step:1321/1680 train_time:116385ms step_avg:88.10ms
step:1322/1680 train_time:116474ms step_avg:88.10ms
step:1323/1680 train_time:116564ms step_avg:88.11ms
step:1324/1680 train_time:116652ms step_avg:88.11ms
step:1325/1680 train_time:116742ms step_avg:88.11ms
step:1326/1680 train_time:116832ms step_avg:88.11ms
step:1327/1680 train_time:116920ms step_avg:88.11ms
step:1328/1680 train_time:117009ms step_avg:88.11ms
step:1329/1680 train_time:117097ms step_avg:88.11ms
step:1330/1680 train_time:117187ms step_avg:88.11ms
step:1331/1680 train_time:117276ms step_avg:88.11ms
step:1332/1680 train_time:117366ms step_avg:88.11ms
step:1333/1680 train_time:117456ms step_avg:88.11ms
step:1334/1680 train_time:117545ms step_avg:88.11ms
step:1335/1680 train_time:117634ms step_avg:88.12ms
step:1336/1680 train_time:117724ms step_avg:88.12ms
step:1337/1680 train_time:117813ms step_avg:88.12ms
step:1338/1680 train_time:117903ms step_avg:88.12ms
step:1339/1680 train_time:117992ms step_avg:88.12ms
step:1340/1680 train_time:118081ms step_avg:88.12ms
step:1341/1680 train_time:118170ms step_avg:88.12ms
step:1342/1680 train_time:118259ms step_avg:88.12ms
step:1343/1680 train_time:118348ms step_avg:88.12ms
step:1344/1680 train_time:118437ms step_avg:88.12ms
step:1345/1680 train_time:118527ms step_avg:88.12ms
step:1346/1680 train_time:118615ms step_avg:88.12ms
step:1347/1680 train_time:118704ms step_avg:88.13ms
step:1348/1680 train_time:118794ms step_avg:88.13ms
step:1349/1680 train_time:118884ms step_avg:88.13ms
step:1350/1680 train_time:118972ms step_avg:88.13ms
step:1351/1680 train_time:119063ms step_avg:88.13ms
step:1352/1680 train_time:119153ms step_avg:88.13ms
step:1353/1680 train_time:119243ms step_avg:88.13ms
step:1354/1680 train_time:119332ms step_avg:88.13ms
step:1355/1680 train_time:119421ms step_avg:88.13ms
step:1356/1680 train_time:119510ms step_avg:88.13ms
step:1357/1680 train_time:119599ms step_avg:88.13ms
step:1358/1680 train_time:119689ms step_avg:88.14ms
step:1359/1680 train_time:119778ms step_avg:88.14ms
step:1360/1680 train_time:119866ms step_avg:88.14ms
step:1361/1680 train_time:119955ms step_avg:88.14ms
step:1362/1680 train_time:120044ms step_avg:88.14ms
step:1363/1680 train_time:120133ms step_avg:88.14ms
step:1364/1680 train_time:120223ms step_avg:88.14ms
step:1365/1680 train_time:120312ms step_avg:88.14ms
step:1366/1680 train_time:120401ms step_avg:88.14ms
step:1367/1680 train_time:120491ms step_avg:88.14ms
step:1368/1680 train_time:120580ms step_avg:88.14ms
step:1369/1680 train_time:120669ms step_avg:88.14ms
step:1370/1680 train_time:120758ms step_avg:88.14ms
step:1371/1680 train_time:120847ms step_avg:88.15ms
step:1372/1680 train_time:120937ms step_avg:88.15ms
step:1373/1680 train_time:121025ms step_avg:88.15ms
step:1374/1680 train_time:121115ms step_avg:88.15ms
step:1375/1680 train_time:121204ms step_avg:88.15ms
step:1375/1680 val_loss:3.3422 train_time:121294ms step_avg:88.21ms
step:1376/1680 train_time:121314ms step_avg:88.16ms
step:1377/1680 train_time:121386ms step_avg:88.15ms
step:1378/1680 train_time:121477ms step_avg:88.15ms
step:1379/1680 train_time:121566ms step_avg:88.16ms
step:1380/1680 train_time:121654ms step_avg:88.16ms
step:1381/1680 train_time:121743ms step_avg:88.16ms
step:1382/1680 train_time:121832ms step_avg:88.16ms
step:1383/1680 train_time:121920ms step_avg:88.16ms
step:1384/1680 train_time:122009ms step_avg:88.16ms
step:1385/1680 train_time:122098ms step_avg:88.16ms
step:1386/1680 train_time:122186ms step_avg:88.16ms
step:1387/1680 train_time:122276ms step_avg:88.16ms
step:1388/1680 train_time:122367ms step_avg:88.16ms
step:1389/1680 train_time:122458ms step_avg:88.16ms
step:1390/1680 train_time:122547ms step_avg:88.16ms
step:1391/1680 train_time:122637ms step_avg:88.16ms
step:1392/1680 train_time:122726ms step_avg:88.17ms
step:1393/1680 train_time:122814ms step_avg:88.17ms
step:1394/1680 train_time:122902ms step_avg:88.17ms
step:1395/1680 train_time:122992ms step_avg:88.17ms
step:1396/1680 train_time:123081ms step_avg:88.17ms
step:1397/1680 train_time:123170ms step_avg:88.17ms
step:1398/1680 train_time:123260ms step_avg:88.17ms
step:1399/1680 train_time:123350ms step_avg:88.17ms
step:1400/1680 train_time:123440ms step_avg:88.17ms
step:1401/1680 train_time:123530ms step_avg:88.17ms
step:1402/1680 train_time:123619ms step_avg:88.17ms
step:1403/1680 train_time:123708ms step_avg:88.17ms
step:1404/1680 train_time:123796ms step_avg:88.17ms
step:1405/1680 train_time:123886ms step_avg:88.18ms
step:1406/1680 train_time:123974ms step_avg:88.17ms
step:1407/1680 train_time:124063ms step_avg:88.18ms
step:1408/1680 train_time:124152ms step_avg:88.18ms
step:1409/1680 train_time:124242ms step_avg:88.18ms
step:1410/1680 train_time:124332ms step_avg:88.18ms
step:1411/1680 train_time:124422ms step_avg:88.18ms
step:1412/1680 train_time:124511ms step_avg:88.18ms
step:1413/1680 train_time:124601ms step_avg:88.18ms
step:1414/1680 train_time:124690ms step_avg:88.18ms
step:1415/1680 train_time:124779ms step_avg:88.18ms
step:1416/1680 train_time:124869ms step_avg:88.18ms
step:1417/1680 train_time:124958ms step_avg:88.19ms
step:1418/1680 train_time:125046ms step_avg:88.19ms
step:1419/1680 train_time:125136ms step_avg:88.19ms
step:1420/1680 train_time:125226ms step_avg:88.19ms
step:1421/1680 train_time:125316ms step_avg:88.19ms
step:1422/1680 train_time:125406ms step_avg:88.19ms
step:1423/1680 train_time:125495ms step_avg:88.19ms
step:1424/1680 train_time:125585ms step_avg:88.19ms
step:1425/1680 train_time:125674ms step_avg:88.19ms
step:1426/1680 train_time:125764ms step_avg:88.19ms
step:1427/1680 train_time:125853ms step_avg:88.19ms
step:1428/1680 train_time:125942ms step_avg:88.20ms
step:1429/1680 train_time:126032ms step_avg:88.20ms
step:1430/1680 train_time:126121ms step_avg:88.20ms
step:1431/1680 train_time:126212ms step_avg:88.20ms
step:1432/1680 train_time:126302ms step_avg:88.20ms
step:1433/1680 train_time:126392ms step_avg:88.20ms
step:1434/1680 train_time:126481ms step_avg:88.20ms
step:1435/1680 train_time:126571ms step_avg:88.20ms
step:1436/1680 train_time:126659ms step_avg:88.20ms
step:1437/1680 train_time:126748ms step_avg:88.20ms
step:1438/1680 train_time:126837ms step_avg:88.20ms
step:1439/1680 train_time:126927ms step_avg:88.20ms
step:1440/1680 train_time:127016ms step_avg:88.21ms
step:1441/1680 train_time:127105ms step_avg:88.21ms
step:1442/1680 train_time:127195ms step_avg:88.21ms
step:1443/1680 train_time:127284ms step_avg:88.21ms
step:1444/1680 train_time:127373ms step_avg:88.21ms
step:1445/1680 train_time:127463ms step_avg:88.21ms
step:1446/1680 train_time:127554ms step_avg:88.21ms
step:1447/1680 train_time:127643ms step_avg:88.21ms
step:1448/1680 train_time:127734ms step_avg:88.21ms
step:1449/1680 train_time:127824ms step_avg:88.22ms
step:1450/1680 train_time:127914ms step_avg:88.22ms
step:1451/1680 train_time:128004ms step_avg:88.22ms
step:1452/1680 train_time:128093ms step_avg:88.22ms
step:1453/1680 train_time:128182ms step_avg:88.22ms
step:1454/1680 train_time:128271ms step_avg:88.22ms
step:1455/1680 train_time:128360ms step_avg:88.22ms
step:1456/1680 train_time:128450ms step_avg:88.22ms
step:1457/1680 train_time:128539ms step_avg:88.22ms
step:1458/1680 train_time:128628ms step_avg:88.22ms
step:1459/1680 train_time:128717ms step_avg:88.22ms
step:1460/1680 train_time:128807ms step_avg:88.22ms
step:1461/1680 train_time:128896ms step_avg:88.22ms
step:1462/1680 train_time:128986ms step_avg:88.23ms
step:1463/1680 train_time:129074ms step_avg:88.23ms
step:1464/1680 train_time:129164ms step_avg:88.23ms
step:1465/1680 train_time:129253ms step_avg:88.23ms
step:1466/1680 train_time:129342ms step_avg:88.23ms
step:1467/1680 train_time:129433ms step_avg:88.23ms
step:1468/1680 train_time:129522ms step_avg:88.23ms
step:1469/1680 train_time:129611ms step_avg:88.23ms
step:1470/1680 train_time:129700ms step_avg:88.23ms
step:1471/1680 train_time:129790ms step_avg:88.23ms
step:1472/1680 train_time:129879ms step_avg:88.23ms
step:1473/1680 train_time:129968ms step_avg:88.23ms
step:1474/1680 train_time:130057ms step_avg:88.23ms
step:1475/1680 train_time:130147ms step_avg:88.24ms
step:1476/1680 train_time:130236ms step_avg:88.24ms
step:1477/1680 train_time:130326ms step_avg:88.24ms
step:1478/1680 train_time:130415ms step_avg:88.24ms
step:1479/1680 train_time:130505ms step_avg:88.24ms
step:1480/1680 train_time:130595ms step_avg:88.24ms
step:1481/1680 train_time:130684ms step_avg:88.24ms
step:1482/1680 train_time:130773ms step_avg:88.24ms
step:1483/1680 train_time:130863ms step_avg:88.24ms
step:1484/1680 train_time:130953ms step_avg:88.24ms
step:1485/1680 train_time:131043ms step_avg:88.24ms
step:1486/1680 train_time:131132ms step_avg:88.24ms
step:1487/1680 train_time:131221ms step_avg:88.25ms
step:1488/1680 train_time:131310ms step_avg:88.25ms
step:1489/1680 train_time:131399ms step_avg:88.25ms
step:1490/1680 train_time:131489ms step_avg:88.25ms
step:1491/1680 train_time:131578ms step_avg:88.25ms
step:1492/1680 train_time:131668ms step_avg:88.25ms
step:1493/1680 train_time:131757ms step_avg:88.25ms
step:1494/1680 train_time:131846ms step_avg:88.25ms
step:1495/1680 train_time:131936ms step_avg:88.25ms
step:1496/1680 train_time:132025ms step_avg:88.25ms
step:1497/1680 train_time:132114ms step_avg:88.25ms
step:1498/1680 train_time:132202ms step_avg:88.25ms
step:1499/1680 train_time:132294ms step_avg:88.25ms
step:1500/1680 train_time:132384ms step_avg:88.26ms
step:1500/1680 val_loss:3.3130 train_time:132475ms step_avg:88.32ms
step:1501/1680 train_time:132493ms step_avg:88.27ms
step:1502/1680 train_time:132566ms step_avg:88.26ms
step:1503/1680 train_time:132658ms step_avg:88.26ms
step:1504/1680 train_time:132748ms step_avg:88.26ms
step:1505/1680 train_time:132836ms step_avg:88.26ms
step:1506/1680 train_time:132924ms step_avg:88.26ms
step:1507/1680 train_time:133012ms step_avg:88.26ms
step:1508/1680 train_time:133100ms step_avg:88.26ms
step:1509/1680 train_time:133188ms step_avg:88.26ms
step:1510/1680 train_time:133277ms step_avg:88.26ms
step:1511/1680 train_time:133366ms step_avg:88.26ms
step:1512/1680 train_time:133458ms step_avg:88.27ms
step:1513/1680 train_time:133550ms step_avg:88.27ms
step:1514/1680 train_time:133641ms step_avg:88.27ms
step:1515/1680 train_time:133732ms step_avg:88.27ms
step:1516/1680 train_time:133821ms step_avg:88.27ms
step:1517/1680 train_time:133909ms step_avg:88.27ms
step:1518/1680 train_time:133997ms step_avg:88.27ms
step:1519/1680 train_time:134086ms step_avg:88.27ms
step:1520/1680 train_time:134174ms step_avg:88.27ms
step:1521/1680 train_time:134262ms step_avg:88.27ms
step:1522/1680 train_time:134352ms step_avg:88.27ms
step:1523/1680 train_time:134442ms step_avg:88.27ms
step:1524/1680 train_time:134532ms step_avg:88.28ms
step:1525/1680 train_time:134623ms step_avg:88.28ms
step:1526/1680 train_time:134712ms step_avg:88.28ms
step:1527/1680 train_time:134802ms step_avg:88.28ms
step:1528/1680 train_time:134890ms step_avg:88.28ms
step:1529/1680 train_time:134979ms step_avg:88.28ms
step:1530/1680 train_time:135067ms step_avg:88.28ms
step:1531/1680 train_time:135156ms step_avg:88.28ms
step:1532/1680 train_time:135244ms step_avg:88.28ms
step:1533/1680 train_time:135333ms step_avg:88.28ms
step:1534/1680 train_time:135422ms step_avg:88.28ms
step:1535/1680 train_time:135511ms step_avg:88.28ms
step:1536/1680 train_time:135601ms step_avg:88.28ms
step:1537/1680 train_time:135692ms step_avg:88.28ms
step:1538/1680 train_time:135782ms step_avg:88.29ms
step:1539/1680 train_time:135871ms step_avg:88.29ms
step:1540/1680 train_time:135961ms step_avg:88.29ms
step:1541/1680 train_time:136050ms step_avg:88.29ms
step:1542/1680 train_time:136139ms step_avg:88.29ms
step:1543/1680 train_time:136228ms step_avg:88.29ms
step:1544/1680 train_time:136317ms step_avg:88.29ms
step:1545/1680 train_time:136406ms step_avg:88.29ms
step:1546/1680 train_time:136495ms step_avg:88.29ms
step:1547/1680 train_time:136584ms step_avg:88.29ms
step:1548/1680 train_time:136674ms step_avg:88.29ms
step:1549/1680 train_time:136764ms step_avg:88.29ms
step:1550/1680 train_time:136853ms step_avg:88.29ms
step:1551/1680 train_time:136942ms step_avg:88.29ms
step:1552/1680 train_time:137031ms step_avg:88.29ms
step:1553/1680 train_time:137120ms step_avg:88.29ms
step:1554/1680 train_time:137209ms step_avg:88.29ms
step:1555/1680 train_time:137298ms step_avg:88.29ms
step:1556/1680 train_time:137387ms step_avg:88.29ms
step:1557/1680 train_time:137476ms step_avg:88.30ms
step:1558/1680 train_time:137565ms step_avg:88.30ms
step:1559/1680 train_time:137654ms step_avg:88.30ms
step:1560/1680 train_time:137745ms step_avg:88.30ms
step:1561/1680 train_time:137834ms step_avg:88.30ms
step:1562/1680 train_time:137924ms step_avg:88.30ms
step:1563/1680 train_time:138013ms step_avg:88.30ms
step:1564/1680 train_time:138102ms step_avg:88.30ms
step:1565/1680 train_time:138191ms step_avg:88.30ms
step:1566/1680 train_time:138281ms step_avg:88.30ms
step:1567/1680 train_time:138371ms step_avg:88.30ms
step:1568/1680 train_time:138460ms step_avg:88.30ms
step:1569/1680 train_time:138549ms step_avg:88.30ms
step:1570/1680 train_time:138638ms step_avg:88.30ms
step:1571/1680 train_time:138729ms step_avg:88.31ms
step:1572/1680 train_time:138818ms step_avg:88.31ms
step:1573/1680 train_time:138909ms step_avg:88.31ms
step:1574/1680 train_time:138998ms step_avg:88.31ms
step:1575/1680 train_time:139087ms step_avg:88.31ms
step:1576/1680 train_time:139175ms step_avg:88.31ms
step:1577/1680 train_time:139265ms step_avg:88.31ms
step:1578/1680 train_time:139354ms step_avg:88.31ms
step:1579/1680 train_time:139444ms step_avg:88.31ms
step:1580/1680 train_time:139533ms step_avg:88.31ms
step:1581/1680 train_time:139622ms step_avg:88.31ms
step:1582/1680 train_time:139712ms step_avg:88.31ms
step:1583/1680 train_time:139802ms step_avg:88.31ms
step:1584/1680 train_time:139891ms step_avg:88.32ms
step:1585/1680 train_time:139980ms step_avg:88.32ms
step:1586/1680 train_time:140069ms step_avg:88.32ms
step:1587/1680 train_time:140158ms step_avg:88.32ms
step:1588/1680 train_time:140248ms step_avg:88.32ms
step:1589/1680 train_time:140339ms step_avg:88.32ms
step:1590/1680 train_time:140428ms step_avg:88.32ms
step:1591/1680 train_time:140516ms step_avg:88.32ms
step:1592/1680 train_time:140606ms step_avg:88.32ms
step:1593/1680 train_time:140695ms step_avg:88.32ms
step:1594/1680 train_time:140784ms step_avg:88.32ms
step:1595/1680 train_time:140873ms step_avg:88.32ms
step:1596/1680 train_time:140962ms step_avg:88.32ms
step:1597/1680 train_time:141052ms step_avg:88.32ms
step:1598/1680 train_time:141141ms step_avg:88.32ms
step:1599/1680 train_time:141231ms step_avg:88.32ms
step:1600/1680 train_time:141321ms step_avg:88.33ms
step:1601/1680 train_time:141411ms step_avg:88.33ms
step:1602/1680 train_time:141501ms step_avg:88.33ms
step:1603/1680 train_time:141590ms step_avg:88.33ms
step:1604/1680 train_time:141680ms step_avg:88.33ms
step:1605/1680 train_time:141769ms step_avg:88.33ms
step:1606/1680 train_time:141858ms step_avg:88.33ms
step:1607/1680 train_time:141947ms step_avg:88.33ms
step:1608/1680 train_time:142037ms step_avg:88.33ms
step:1609/1680 train_time:142127ms step_avg:88.33ms
step:1610/1680 train_time:142217ms step_avg:88.33ms
step:1611/1680 train_time:142306ms step_avg:88.33ms
step:1612/1680 train_time:142395ms step_avg:88.33ms
step:1613/1680 train_time:142485ms step_avg:88.34ms
step:1614/1680 train_time:142575ms step_avg:88.34ms
step:1615/1680 train_time:142663ms step_avg:88.34ms
step:1616/1680 train_time:142752ms step_avg:88.34ms
step:1617/1680 train_time:142843ms step_avg:88.34ms
step:1618/1680 train_time:142932ms step_avg:88.34ms
step:1619/1680 train_time:143021ms step_avg:88.34ms
step:1620/1680 train_time:143109ms step_avg:88.34ms
step:1621/1680 train_time:143200ms step_avg:88.34ms
step:1622/1680 train_time:143289ms step_avg:88.34ms
step:1623/1680 train_time:143380ms step_avg:88.34ms
step:1624/1680 train_time:143470ms step_avg:88.34ms
step:1625/1680 train_time:143560ms step_avg:88.34ms
step:1625/1680 val_loss:3.2892 train_time:143650ms step_avg:88.40ms
step:1626/1680 train_time:143668ms step_avg:88.36ms
step:1627/1680 train_time:143743ms step_avg:88.35ms
step:1628/1680 train_time:143837ms step_avg:88.35ms
step:1629/1680 train_time:143928ms step_avg:88.35ms
step:1630/1680 train_time:144016ms step_avg:88.35ms
step:1631/1680 train_time:144105ms step_avg:88.35ms
step:1632/1680 train_time:144193ms step_avg:88.35ms
step:1633/1680 train_time:144281ms step_avg:88.35ms
step:1634/1680 train_time:144370ms step_avg:88.35ms
step:1635/1680 train_time:144459ms step_avg:88.35ms
step:1636/1680 train_time:144547ms step_avg:88.35ms
step:1637/1680 train_time:144637ms step_avg:88.35ms
step:1638/1680 train_time:144729ms step_avg:88.36ms
step:1639/1680 train_time:144820ms step_avg:88.36ms
step:1640/1680 train_time:144910ms step_avg:88.36ms
step:1641/1680 train_time:144999ms step_avg:88.36ms
step:1642/1680 train_time:145088ms step_avg:88.36ms
step:1643/1680 train_time:145177ms step_avg:88.36ms
step:1644/1680 train_time:145265ms step_avg:88.36ms
step:1645/1680 train_time:145354ms step_avg:88.36ms
step:1646/1680 train_time:145442ms step_avg:88.36ms
step:1647/1680 train_time:145531ms step_avg:88.36ms
step:1648/1680 train_time:145621ms step_avg:88.36ms
step:1649/1680 train_time:145711ms step_avg:88.36ms
step:1650/1680 train_time:145802ms step_avg:88.37ms
step:1651/1680 train_time:145893ms step_avg:88.37ms
step:1652/1680 train_time:145982ms step_avg:88.37ms
step:1653/1680 train_time:146071ms step_avg:88.37ms
step:1654/1680 train_time:146160ms step_avg:88.37ms
step:1655/1680 train_time:146249ms step_avg:88.37ms
step:1656/1680 train_time:146337ms step_avg:88.37ms
step:1657/1680 train_time:146426ms step_avg:88.37ms
step:1658/1680 train_time:146516ms step_avg:88.37ms
step:1659/1680 train_time:146606ms step_avg:88.37ms
step:1660/1680 train_time:146694ms step_avg:88.37ms
step:1661/1680 train_time:146784ms step_avg:88.37ms
step:1662/1680 train_time:146874ms step_avg:88.37ms
step:1663/1680 train_time:146966ms step_avg:88.37ms
step:1664/1680 train_time:147055ms step_avg:88.37ms
step:1665/1680 train_time:147143ms step_avg:88.37ms
step:1666/1680 train_time:147232ms step_avg:88.37ms
step:1667/1680 train_time:147321ms step_avg:88.37ms
step:1668/1680 train_time:147409ms step_avg:88.37ms
step:1669/1680 train_time:147499ms step_avg:88.38ms
step:1670/1680 train_time:147588ms step_avg:88.38ms
step:1671/1680 train_time:147678ms step_avg:88.38ms
step:1672/1680 train_time:147768ms step_avg:88.38ms
step:1673/1680 train_time:147858ms step_avg:88.38ms
step:1674/1680 train_time:147948ms step_avg:88.38ms
step:1675/1680 train_time:148037ms step_avg:88.38ms
step:1676/1680 train_time:148127ms step_avg:88.38ms
step:1677/1680 train_time:148215ms step_avg:88.38ms
step:1678/1680 train_time:148305ms step_avg:88.38ms
step:1679/1680 train_time:148393ms step_avg:88.38ms
step:1680/1680 train_time:148482ms step_avg:88.38ms
step:1680/1680 val_loss:3.2785 train_time:148573ms step_avg:88.44ms
peak memory allocated: 30760 MiB reserved: 46354 MiB

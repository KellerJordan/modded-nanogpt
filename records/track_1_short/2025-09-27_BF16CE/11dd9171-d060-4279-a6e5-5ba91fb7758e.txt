import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 13:28:19 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            114W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    174977      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    174978      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    174979      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    174980      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    174981      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    174982      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    174983      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    174984      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    174978      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    174979      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    174980      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    174981      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    174982      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    174983      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    174984      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:141ms step_avg:140.61ms
step:2/1680 train_time:160ms step_avg:79.95ms
step:3/1680 train_time:224ms step_avg:74.73ms
step:4/1680 train_time:309ms step_avg:77.26ms
step:5/1680 train_time:396ms step_avg:79.14ms
step:6/1680 train_time:482ms step_avg:80.31ms
step:7/1680 train_time:568ms step_avg:81.13ms
step:8/1680 train_time:654ms step_avg:81.73ms
step:9/1680 train_time:740ms step_avg:82.18ms
step:10/1680 train_time:826ms step_avg:82.65ms
step:11/1680 train_time:913ms step_avg:82.96ms
step:12/1680 train_time:1001ms step_avg:83.42ms
step:13/1680 train_time:1093ms step_avg:84.04ms
step:14/1680 train_time:1183ms step_avg:84.51ms
step:15/1680 train_time:1271ms step_avg:84.71ms
step:16/1680 train_time:1359ms step_avg:84.93ms
step:17/1680 train_time:1445ms step_avg:85.02ms
step:18/1680 train_time:1532ms step_avg:85.12ms
step:19/1680 train_time:1619ms step_avg:85.20ms
step:20/1680 train_time:1705ms step_avg:85.25ms
step:21/1680 train_time:1791ms step_avg:85.30ms
step:22/1680 train_time:1877ms step_avg:85.34ms
step:23/1680 train_time:1965ms step_avg:85.44ms
step:24/1680 train_time:2053ms step_avg:85.54ms
step:25/1680 train_time:2142ms step_avg:85.68ms
step:26/1680 train_time:2232ms step_avg:85.83ms
step:27/1680 train_time:2319ms step_avg:85.90ms
step:28/1680 train_time:2406ms step_avg:85.93ms
step:29/1680 train_time:2494ms step_avg:85.99ms
step:30/1680 train_time:2581ms step_avg:86.04ms
step:31/1680 train_time:2667ms step_avg:86.04ms
step:32/1680 train_time:2754ms step_avg:86.07ms
step:33/1680 train_time:2841ms step_avg:86.09ms
step:34/1680 train_time:2929ms step_avg:86.14ms
step:35/1680 train_time:3016ms step_avg:86.18ms
step:36/1680 train_time:3104ms step_avg:86.22ms
step:37/1680 train_time:3192ms step_avg:86.26ms
step:38/1680 train_time:3280ms step_avg:86.31ms
step:39/1680 train_time:3367ms step_avg:86.34ms
step:40/1680 train_time:3454ms step_avg:86.35ms
step:41/1680 train_time:3541ms step_avg:86.37ms
step:42/1680 train_time:3628ms step_avg:86.38ms
step:43/1680 train_time:3715ms step_avg:86.39ms
step:44/1680 train_time:3801ms step_avg:86.39ms
step:45/1680 train_time:3888ms step_avg:86.41ms
step:46/1680 train_time:3976ms step_avg:86.43ms
step:47/1680 train_time:4062ms step_avg:86.43ms
step:48/1680 train_time:4150ms step_avg:86.45ms
step:49/1680 train_time:4238ms step_avg:86.49ms
step:50/1680 train_time:4327ms step_avg:86.54ms
step:51/1680 train_time:4415ms step_avg:86.56ms
step:52/1680 train_time:4502ms step_avg:86.57ms
step:53/1680 train_time:4589ms step_avg:86.58ms
step:54/1680 train_time:4675ms step_avg:86.58ms
step:55/1680 train_time:4762ms step_avg:86.59ms
step:56/1680 train_time:4849ms step_avg:86.59ms
step:57/1680 train_time:4936ms step_avg:86.59ms
step:58/1680 train_time:5023ms step_avg:86.60ms
step:59/1680 train_time:5111ms step_avg:86.63ms
step:60/1680 train_time:5199ms step_avg:86.64ms
step:61/1680 train_time:5288ms step_avg:86.69ms
step:62/1680 train_time:5376ms step_avg:86.70ms
step:63/1680 train_time:5463ms step_avg:86.71ms
step:64/1680 train_time:5550ms step_avg:86.72ms
step:65/1680 train_time:5637ms step_avg:86.72ms
step:66/1680 train_time:5724ms step_avg:86.73ms
step:67/1680 train_time:5811ms step_avg:86.73ms
step:68/1680 train_time:5898ms step_avg:86.74ms
step:69/1680 train_time:5986ms step_avg:86.75ms
step:70/1680 train_time:6073ms step_avg:86.75ms
step:71/1680 train_time:6160ms step_avg:86.76ms
step:72/1680 train_time:6248ms step_avg:86.78ms
step:73/1680 train_time:6336ms step_avg:86.79ms
step:74/1680 train_time:6422ms step_avg:86.79ms
step:75/1680 train_time:6510ms step_avg:86.80ms
step:76/1680 train_time:6597ms step_avg:86.80ms
step:77/1680 train_time:6684ms step_avg:86.81ms
step:78/1680 train_time:6771ms step_avg:86.80ms
step:79/1680 train_time:6858ms step_avg:86.81ms
step:80/1680 train_time:6945ms step_avg:86.82ms
step:81/1680 train_time:7033ms step_avg:86.82ms
step:82/1680 train_time:7120ms step_avg:86.83ms
step:83/1680 train_time:7208ms step_avg:86.84ms
step:84/1680 train_time:7295ms step_avg:86.84ms
step:85/1680 train_time:7382ms step_avg:86.85ms
step:86/1680 train_time:7469ms step_avg:86.85ms
step:87/1680 train_time:7556ms step_avg:86.85ms
step:88/1680 train_time:7643ms step_avg:86.85ms
step:89/1680 train_time:7731ms step_avg:86.86ms
step:90/1680 train_time:7817ms step_avg:86.86ms
step:91/1680 train_time:7905ms step_avg:86.86ms
step:92/1680 train_time:7992ms step_avg:86.87ms
step:93/1680 train_time:8079ms step_avg:86.87ms
step:94/1680 train_time:8166ms step_avg:86.88ms
step:95/1680 train_time:8253ms step_avg:86.88ms
step:96/1680 train_time:8341ms step_avg:86.88ms
step:97/1680 train_time:8428ms step_avg:86.89ms
step:98/1680 train_time:8515ms step_avg:86.89ms
step:99/1680 train_time:8602ms step_avg:86.89ms
step:100/1680 train_time:8689ms step_avg:86.89ms
step:101/1680 train_time:8777ms step_avg:86.90ms
step:102/1680 train_time:8864ms step_avg:86.90ms
step:103/1680 train_time:8951ms step_avg:86.90ms
step:104/1680 train_time:9038ms step_avg:86.90ms
step:105/1680 train_time:9126ms step_avg:86.91ms
step:106/1680 train_time:9212ms step_avg:86.91ms
step:107/1680 train_time:9300ms step_avg:86.91ms
step:108/1680 train_time:9388ms step_avg:86.92ms
step:109/1680 train_time:9475ms step_avg:86.92ms
step:110/1680 train_time:9562ms step_avg:86.93ms
step:111/1680 train_time:9649ms step_avg:86.93ms
step:112/1680 train_time:9736ms step_avg:86.92ms
step:113/1680 train_time:9822ms step_avg:86.92ms
step:114/1680 train_time:9910ms step_avg:86.93ms
step:115/1680 train_time:9997ms step_avg:86.93ms
step:116/1680 train_time:10084ms step_avg:86.93ms
step:117/1680 train_time:10171ms step_avg:86.93ms
step:118/1680 train_time:10258ms step_avg:86.93ms
step:119/1680 train_time:10346ms step_avg:86.94ms
step:120/1680 train_time:10433ms step_avg:86.94ms
step:121/1680 train_time:10521ms step_avg:86.95ms
step:122/1680 train_time:10609ms step_avg:86.96ms
step:123/1680 train_time:10695ms step_avg:86.95ms
step:124/1680 train_time:10783ms step_avg:86.96ms
step:125/1680 train_time:10869ms step_avg:86.96ms
step:125/1680 val_loss:4.2915 train_time:10958ms step_avg:87.66ms
step:126/1680 train_time:10977ms step_avg:87.12ms
step:127/1680 train_time:11046ms step_avg:86.98ms
step:128/1680 train_time:11144ms step_avg:87.06ms
step:129/1680 train_time:11238ms step_avg:87.12ms
step:130/1680 train_time:11326ms step_avg:87.12ms
step:131/1680 train_time:11413ms step_avg:87.12ms
step:132/1680 train_time:11499ms step_avg:87.12ms
step:133/1680 train_time:11585ms step_avg:87.11ms
step:134/1680 train_time:11671ms step_avg:87.10ms
step:135/1680 train_time:11757ms step_avg:87.09ms
step:136/1680 train_time:11843ms step_avg:87.08ms
step:137/1680 train_time:11929ms step_avg:87.07ms
step:138/1680 train_time:12015ms step_avg:87.07ms
step:139/1680 train_time:12104ms step_avg:87.08ms
step:140/1680 train_time:12194ms step_avg:87.10ms
step:141/1680 train_time:12282ms step_avg:87.10ms
step:142/1680 train_time:12369ms step_avg:87.11ms
step:143/1680 train_time:12456ms step_avg:87.11ms
step:144/1680 train_time:12543ms step_avg:87.10ms
step:145/1680 train_time:12629ms step_avg:87.10ms
step:146/1680 train_time:12716ms step_avg:87.10ms
step:147/1680 train_time:12802ms step_avg:87.09ms
step:148/1680 train_time:12888ms step_avg:87.08ms
step:149/1680 train_time:12976ms step_avg:87.08ms
step:150/1680 train_time:13063ms step_avg:87.09ms
step:151/1680 train_time:13152ms step_avg:87.10ms
step:152/1680 train_time:13240ms step_avg:87.10ms
step:153/1680 train_time:13327ms step_avg:87.10ms
step:154/1680 train_time:13414ms step_avg:87.11ms
step:155/1680 train_time:13501ms step_avg:87.10ms
step:156/1680 train_time:13587ms step_avg:87.10ms
step:157/1680 train_time:13674ms step_avg:87.10ms
step:158/1680 train_time:13761ms step_avg:87.09ms
step:159/1680 train_time:13847ms step_avg:87.09ms
step:160/1680 train_time:13935ms step_avg:87.09ms
step:161/1680 train_time:14021ms step_avg:87.09ms
step:162/1680 train_time:14109ms step_avg:87.09ms
step:163/1680 train_time:14196ms step_avg:87.09ms
step:164/1680 train_time:14283ms step_avg:87.09ms
step:165/1680 train_time:14371ms step_avg:87.09ms
step:166/1680 train_time:14458ms step_avg:87.10ms
step:167/1680 train_time:14545ms step_avg:87.10ms
step:168/1680 train_time:14633ms step_avg:87.10ms
step:169/1680 train_time:14719ms step_avg:87.09ms
step:170/1680 train_time:14805ms step_avg:87.09ms
step:171/1680 train_time:14892ms step_avg:87.09ms
step:172/1680 train_time:14979ms step_avg:87.09ms
step:173/1680 train_time:15067ms step_avg:87.09ms
step:174/1680 train_time:15154ms step_avg:87.09ms
step:175/1680 train_time:15242ms step_avg:87.09ms
step:176/1680 train_time:15330ms step_avg:87.10ms
step:177/1680 train_time:15417ms step_avg:87.10ms
step:178/1680 train_time:15504ms step_avg:87.10ms
step:179/1680 train_time:15593ms step_avg:87.11ms
step:180/1680 train_time:15680ms step_avg:87.11ms
step:181/1680 train_time:15767ms step_avg:87.11ms
step:182/1680 train_time:15853ms step_avg:87.11ms
step:183/1680 train_time:15941ms step_avg:87.11ms
step:184/1680 train_time:16028ms step_avg:87.11ms
step:185/1680 train_time:16115ms step_avg:87.11ms
step:186/1680 train_time:16203ms step_avg:87.11ms
step:187/1680 train_time:16290ms step_avg:87.11ms
step:188/1680 train_time:16377ms step_avg:87.11ms
step:189/1680 train_time:16464ms step_avg:87.11ms
step:190/1680 train_time:16552ms step_avg:87.11ms
step:191/1680 train_time:16639ms step_avg:87.12ms
step:192/1680 train_time:16726ms step_avg:87.11ms
step:193/1680 train_time:16813ms step_avg:87.11ms
step:194/1680 train_time:16900ms step_avg:87.11ms
step:195/1680 train_time:16986ms step_avg:87.11ms
step:196/1680 train_time:17074ms step_avg:87.11ms
step:197/1680 train_time:17161ms step_avg:87.11ms
step:198/1680 train_time:17249ms step_avg:87.12ms
step:199/1680 train_time:17337ms step_avg:87.12ms
step:200/1680 train_time:17424ms step_avg:87.12ms
step:201/1680 train_time:17511ms step_avg:87.12ms
step:202/1680 train_time:17598ms step_avg:87.12ms
step:203/1680 train_time:17685ms step_avg:87.12ms
step:204/1680 train_time:17771ms step_avg:87.11ms
step:205/1680 train_time:17859ms step_avg:87.12ms
step:206/1680 train_time:17945ms step_avg:87.11ms
step:207/1680 train_time:18032ms step_avg:87.11ms
step:208/1680 train_time:18119ms step_avg:87.11ms
step:209/1680 train_time:18207ms step_avg:87.11ms
step:210/1680 train_time:18294ms step_avg:87.12ms
step:211/1680 train_time:18381ms step_avg:87.12ms
step:212/1680 train_time:18469ms step_avg:87.12ms
step:213/1680 train_time:18556ms step_avg:87.12ms
step:214/1680 train_time:18643ms step_avg:87.12ms
step:215/1680 train_time:18730ms step_avg:87.12ms
step:216/1680 train_time:18817ms step_avg:87.12ms
step:217/1680 train_time:18904ms step_avg:87.12ms
step:218/1680 train_time:18991ms step_avg:87.11ms
step:219/1680 train_time:19077ms step_avg:87.11ms
step:220/1680 train_time:19165ms step_avg:87.11ms
step:221/1680 train_time:19252ms step_avg:87.11ms
step:222/1680 train_time:19339ms step_avg:87.11ms
step:223/1680 train_time:19426ms step_avg:87.11ms
step:224/1680 train_time:19514ms step_avg:87.12ms
step:225/1680 train_time:19601ms step_avg:87.11ms
step:226/1680 train_time:19687ms step_avg:87.11ms
step:227/1680 train_time:19776ms step_avg:87.12ms
step:228/1680 train_time:19862ms step_avg:87.12ms
step:229/1680 train_time:19950ms step_avg:87.12ms
step:230/1680 train_time:20037ms step_avg:87.12ms
step:231/1680 train_time:20123ms step_avg:87.11ms
step:232/1680 train_time:20211ms step_avg:87.11ms
step:233/1680 train_time:20298ms step_avg:87.12ms
step:234/1680 train_time:20386ms step_avg:87.12ms
step:235/1680 train_time:20473ms step_avg:87.12ms
step:236/1680 train_time:20560ms step_avg:87.12ms
step:237/1680 train_time:20648ms step_avg:87.12ms
step:238/1680 train_time:20735ms step_avg:87.12ms
step:239/1680 train_time:20822ms step_avg:87.12ms
step:240/1680 train_time:20909ms step_avg:87.12ms
step:241/1680 train_time:20996ms step_avg:87.12ms
step:242/1680 train_time:21083ms step_avg:87.12ms
step:243/1680 train_time:21170ms step_avg:87.12ms
step:244/1680 train_time:21258ms step_avg:87.12ms
step:245/1680 train_time:21345ms step_avg:87.12ms
step:246/1680 train_time:21433ms step_avg:87.12ms
step:247/1680 train_time:21520ms step_avg:87.12ms
step:248/1680 train_time:21607ms step_avg:87.13ms
step:249/1680 train_time:21694ms step_avg:87.13ms
step:250/1680 train_time:21782ms step_avg:87.13ms
step:250/1680 val_loss:3.9644 train_time:21870ms step_avg:87.48ms
step:251/1680 train_time:21888ms step_avg:87.20ms
step:252/1680 train_time:21959ms step_avg:87.14ms
step:253/1680 train_time:22049ms step_avg:87.15ms
step:254/1680 train_time:22137ms step_avg:87.15ms
step:255/1680 train_time:22224ms step_avg:87.15ms
step:256/1680 train_time:22311ms step_avg:87.15ms
step:257/1680 train_time:22398ms step_avg:87.15ms
step:258/1680 train_time:22484ms step_avg:87.15ms
step:259/1680 train_time:22571ms step_avg:87.15ms
step:260/1680 train_time:22657ms step_avg:87.14ms
step:261/1680 train_time:22743ms step_avg:87.14ms
step:262/1680 train_time:22831ms step_avg:87.14ms
step:263/1680 train_time:22919ms step_avg:87.14ms
step:264/1680 train_time:23008ms step_avg:87.15ms
step:265/1680 train_time:23096ms step_avg:87.15ms
step:266/1680 train_time:23182ms step_avg:87.15ms
step:267/1680 train_time:23269ms step_avg:87.15ms
step:268/1680 train_time:23356ms step_avg:87.15ms
step:269/1680 train_time:23443ms step_avg:87.15ms
step:270/1680 train_time:23529ms step_avg:87.14ms
step:271/1680 train_time:23616ms step_avg:87.14ms
step:272/1680 train_time:23702ms step_avg:87.14ms
step:273/1680 train_time:23790ms step_avg:87.14ms
step:274/1680 train_time:23878ms step_avg:87.14ms
step:275/1680 train_time:23965ms step_avg:87.14ms
step:276/1680 train_time:24053ms step_avg:87.15ms
step:277/1680 train_time:24140ms step_avg:87.15ms
step:278/1680 train_time:24227ms step_avg:87.15ms
step:279/1680 train_time:24314ms step_avg:87.15ms
step:280/1680 train_time:24400ms step_avg:87.14ms
step:281/1680 train_time:24488ms step_avg:87.14ms
step:282/1680 train_time:24574ms step_avg:87.14ms
step:283/1680 train_time:24660ms step_avg:87.14ms
step:284/1680 train_time:24747ms step_avg:87.14ms
step:285/1680 train_time:24834ms step_avg:87.14ms
step:286/1680 train_time:24922ms step_avg:87.14ms
step:287/1680 train_time:25009ms step_avg:87.14ms
step:288/1680 train_time:25096ms step_avg:87.14ms
step:289/1680 train_time:25183ms step_avg:87.14ms
step:290/1680 train_time:25270ms step_avg:87.14ms
step:291/1680 train_time:25357ms step_avg:87.14ms
step:292/1680 train_time:25445ms step_avg:87.14ms
step:293/1680 train_time:25532ms step_avg:87.14ms
step:294/1680 train_time:25618ms step_avg:87.14ms
step:295/1680 train_time:25705ms step_avg:87.14ms
step:296/1680 train_time:25792ms step_avg:87.13ms
step:297/1680 train_time:25879ms step_avg:87.13ms
step:298/1680 train_time:25966ms step_avg:87.13ms
step:299/1680 train_time:26054ms step_avg:87.14ms
step:300/1680 train_time:26140ms step_avg:87.13ms
step:301/1680 train_time:26228ms step_avg:87.14ms
step:302/1680 train_time:26315ms step_avg:87.14ms
step:303/1680 train_time:26402ms step_avg:87.14ms
step:304/1680 train_time:26489ms step_avg:87.13ms
step:305/1680 train_time:26576ms step_avg:87.13ms
step:306/1680 train_time:26662ms step_avg:87.13ms
step:307/1680 train_time:26749ms step_avg:87.13ms
step:308/1680 train_time:26836ms step_avg:87.13ms
step:309/1680 train_time:26923ms step_avg:87.13ms
step:310/1680 train_time:27011ms step_avg:87.13ms
step:311/1680 train_time:27100ms step_avg:87.14ms
step:312/1680 train_time:27187ms step_avg:87.14ms
step:313/1680 train_time:27274ms step_avg:87.14ms
step:314/1680 train_time:27360ms step_avg:87.13ms
step:315/1680 train_time:27447ms step_avg:87.13ms
step:316/1680 train_time:27535ms step_avg:87.13ms
step:317/1680 train_time:27621ms step_avg:87.13ms
step:318/1680 train_time:27708ms step_avg:87.13ms
step:319/1680 train_time:27796ms step_avg:87.13ms
step:320/1680 train_time:27883ms step_avg:87.13ms
step:321/1680 train_time:27971ms step_avg:87.14ms
step:322/1680 train_time:28058ms step_avg:87.14ms
step:323/1680 train_time:28144ms step_avg:87.13ms
step:324/1680 train_time:28232ms step_avg:87.14ms
step:325/1680 train_time:28319ms step_avg:87.13ms
step:326/1680 train_time:28406ms step_avg:87.13ms
step:327/1680 train_time:28494ms step_avg:87.14ms
step:328/1680 train_time:28581ms step_avg:87.14ms
step:329/1680 train_time:28668ms step_avg:87.14ms
step:330/1680 train_time:28755ms step_avg:87.14ms
step:331/1680 train_time:28842ms step_avg:87.14ms
step:332/1680 train_time:28930ms step_avg:87.14ms
step:333/1680 train_time:29017ms step_avg:87.14ms
step:334/1680 train_time:29104ms step_avg:87.14ms
step:335/1680 train_time:29191ms step_avg:87.14ms
step:336/1680 train_time:29278ms step_avg:87.14ms
step:337/1680 train_time:29365ms step_avg:87.14ms
step:338/1680 train_time:29453ms step_avg:87.14ms
step:339/1680 train_time:29540ms step_avg:87.14ms
step:340/1680 train_time:29627ms step_avg:87.14ms
step:341/1680 train_time:29714ms step_avg:87.14ms
step:342/1680 train_time:29801ms step_avg:87.14ms
step:343/1680 train_time:29889ms step_avg:87.14ms
step:344/1680 train_time:29976ms step_avg:87.14ms
step:345/1680 train_time:30063ms step_avg:87.14ms
step:346/1680 train_time:30151ms step_avg:87.14ms
step:347/1680 train_time:30238ms step_avg:87.14ms
step:348/1680 train_time:30325ms step_avg:87.14ms
step:349/1680 train_time:30412ms step_avg:87.14ms
step:350/1680 train_time:30499ms step_avg:87.14ms
step:351/1680 train_time:30586ms step_avg:87.14ms
step:352/1680 train_time:30673ms step_avg:87.14ms
step:353/1680 train_time:30760ms step_avg:87.14ms
step:354/1680 train_time:30847ms step_avg:87.14ms
step:355/1680 train_time:30935ms step_avg:87.14ms
step:356/1680 train_time:31022ms step_avg:87.14ms
step:357/1680 train_time:31110ms step_avg:87.14ms
step:358/1680 train_time:31197ms step_avg:87.14ms
step:359/1680 train_time:31283ms step_avg:87.14ms
step:360/1680 train_time:31370ms step_avg:87.14ms
step:361/1680 train_time:31457ms step_avg:87.14ms
step:362/1680 train_time:31544ms step_avg:87.14ms
step:363/1680 train_time:31631ms step_avg:87.14ms
step:364/1680 train_time:31718ms step_avg:87.14ms
step:365/1680 train_time:31805ms step_avg:87.14ms
step:366/1680 train_time:31893ms step_avg:87.14ms
step:367/1680 train_time:31979ms step_avg:87.14ms
step:368/1680 train_time:32067ms step_avg:87.14ms
step:369/1680 train_time:32154ms step_avg:87.14ms
step:370/1680 train_time:32242ms step_avg:87.14ms
step:371/1680 train_time:32329ms step_avg:87.14ms
step:372/1680 train_time:32417ms step_avg:87.14ms
step:373/1680 train_time:32504ms step_avg:87.14ms
step:374/1680 train_time:32591ms step_avg:87.14ms
step:375/1680 train_time:32678ms step_avg:87.14ms
step:375/1680 val_loss:3.8164 train_time:32767ms step_avg:87.38ms
step:376/1680 train_time:32788ms step_avg:87.20ms
step:377/1680 train_time:32855ms step_avg:87.15ms
step:378/1680 train_time:32947ms step_avg:87.16ms
step:379/1680 train_time:33035ms step_avg:87.16ms
step:380/1680 train_time:33122ms step_avg:87.16ms
step:381/1680 train_time:33209ms step_avg:87.16ms
step:382/1680 train_time:33296ms step_avg:87.16ms
step:383/1680 train_time:33382ms step_avg:87.16ms
step:384/1680 train_time:33468ms step_avg:87.16ms
step:385/1680 train_time:33555ms step_avg:87.15ms
step:386/1680 train_time:33640ms step_avg:87.15ms
step:387/1680 train_time:33727ms step_avg:87.15ms
step:388/1680 train_time:33815ms step_avg:87.15ms
step:389/1680 train_time:33904ms step_avg:87.16ms
step:390/1680 train_time:33994ms step_avg:87.16ms
step:391/1680 train_time:34082ms step_avg:87.17ms
step:392/1680 train_time:34169ms step_avg:87.17ms
step:393/1680 train_time:34256ms step_avg:87.16ms
step:394/1680 train_time:34342ms step_avg:87.16ms
step:395/1680 train_time:34429ms step_avg:87.16ms
step:396/1680 train_time:34516ms step_avg:87.16ms
step:397/1680 train_time:34602ms step_avg:87.16ms
step:398/1680 train_time:34689ms step_avg:87.16ms
step:399/1680 train_time:34776ms step_avg:87.16ms
step:400/1680 train_time:34865ms step_avg:87.16ms
step:401/1680 train_time:34954ms step_avg:87.17ms
step:402/1680 train_time:35041ms step_avg:87.17ms
step:403/1680 train_time:35128ms step_avg:87.17ms
step:404/1680 train_time:35215ms step_avg:87.17ms
step:405/1680 train_time:35302ms step_avg:87.17ms
step:406/1680 train_time:35388ms step_avg:87.16ms
step:407/1680 train_time:35475ms step_avg:87.16ms
step:408/1680 train_time:35562ms step_avg:87.16ms
step:409/1680 train_time:35649ms step_avg:87.16ms
step:410/1680 train_time:35735ms step_avg:87.16ms
step:411/1680 train_time:35823ms step_avg:87.16ms
step:412/1680 train_time:35911ms step_avg:87.16ms
step:413/1680 train_time:35999ms step_avg:87.16ms
step:414/1680 train_time:36086ms step_avg:87.16ms
step:415/1680 train_time:36173ms step_avg:87.16ms
step:416/1680 train_time:36260ms step_avg:87.16ms
step:417/1680 train_time:36347ms step_avg:87.16ms
step:418/1680 train_time:36434ms step_avg:87.16ms
step:419/1680 train_time:36520ms step_avg:87.16ms
step:420/1680 train_time:36607ms step_avg:87.16ms
step:421/1680 train_time:36694ms step_avg:87.16ms
step:422/1680 train_time:36781ms step_avg:87.16ms
step:423/1680 train_time:36869ms step_avg:87.16ms
step:424/1680 train_time:36957ms step_avg:87.16ms
step:425/1680 train_time:37045ms step_avg:87.16ms
step:426/1680 train_time:37132ms step_avg:87.17ms
step:427/1680 train_time:37219ms step_avg:87.16ms
step:428/1680 train_time:37306ms step_avg:87.16ms
step:429/1680 train_time:37393ms step_avg:87.16ms
step:430/1680 train_time:37480ms step_avg:87.16ms
step:431/1680 train_time:37566ms step_avg:87.16ms
step:432/1680 train_time:37653ms step_avg:87.16ms
step:433/1680 train_time:37740ms step_avg:87.16ms
step:434/1680 train_time:37827ms step_avg:87.16ms
step:435/1680 train_time:37914ms step_avg:87.16ms
step:436/1680 train_time:38001ms step_avg:87.16ms
step:437/1680 train_time:38090ms step_avg:87.16ms
step:438/1680 train_time:38176ms step_avg:87.16ms
step:439/1680 train_time:38263ms step_avg:87.16ms
step:440/1680 train_time:38350ms step_avg:87.16ms
step:441/1680 train_time:38437ms step_avg:87.16ms
step:442/1680 train_time:38523ms step_avg:87.16ms
step:443/1680 train_time:38610ms step_avg:87.16ms
step:444/1680 train_time:38697ms step_avg:87.15ms
step:445/1680 train_time:38784ms step_avg:87.15ms
step:446/1680 train_time:38871ms step_avg:87.16ms
step:447/1680 train_time:38959ms step_avg:87.16ms
step:448/1680 train_time:39047ms step_avg:87.16ms
step:449/1680 train_time:39134ms step_avg:87.16ms
step:450/1680 train_time:39221ms step_avg:87.16ms
step:451/1680 train_time:39308ms step_avg:87.16ms
step:452/1680 train_time:39395ms step_avg:87.16ms
step:453/1680 train_time:39482ms step_avg:87.16ms
step:454/1680 train_time:39569ms step_avg:87.16ms
step:455/1680 train_time:39656ms step_avg:87.16ms
step:456/1680 train_time:39743ms step_avg:87.16ms
step:457/1680 train_time:39830ms step_avg:87.16ms
step:458/1680 train_time:39917ms step_avg:87.16ms
step:459/1680 train_time:40005ms step_avg:87.16ms
step:460/1680 train_time:40092ms step_avg:87.16ms
step:461/1680 train_time:40179ms step_avg:87.16ms
step:462/1680 train_time:40267ms step_avg:87.16ms
step:463/1680 train_time:40354ms step_avg:87.16ms
step:464/1680 train_time:40441ms step_avg:87.16ms
step:465/1680 train_time:40528ms step_avg:87.16ms
step:466/1680 train_time:40615ms step_avg:87.16ms
step:467/1680 train_time:40702ms step_avg:87.16ms
step:468/1680 train_time:40789ms step_avg:87.16ms
step:469/1680 train_time:40876ms step_avg:87.15ms
step:470/1680 train_time:40963ms step_avg:87.15ms
step:471/1680 train_time:41050ms step_avg:87.15ms
step:472/1680 train_time:41136ms step_avg:87.15ms
step:473/1680 train_time:41224ms step_avg:87.16ms
step:474/1680 train_time:41312ms step_avg:87.16ms
step:475/1680 train_time:41399ms step_avg:87.16ms
step:476/1680 train_time:41486ms step_avg:87.16ms
step:477/1680 train_time:41573ms step_avg:87.15ms
step:478/1680 train_time:41660ms step_avg:87.15ms
step:479/1680 train_time:41747ms step_avg:87.15ms
step:480/1680 train_time:41834ms step_avg:87.15ms
step:481/1680 train_time:41921ms step_avg:87.15ms
step:482/1680 train_time:42008ms step_avg:87.15ms
step:483/1680 train_time:42095ms step_avg:87.15ms
step:484/1680 train_time:42182ms step_avg:87.15ms
step:485/1680 train_time:42270ms step_avg:87.15ms
step:486/1680 train_time:42356ms step_avg:87.15ms
step:487/1680 train_time:42444ms step_avg:87.15ms
step:488/1680 train_time:42531ms step_avg:87.15ms
step:489/1680 train_time:42617ms step_avg:87.15ms
step:490/1680 train_time:42705ms step_avg:87.15ms
step:491/1680 train_time:42793ms step_avg:87.15ms
step:492/1680 train_time:42880ms step_avg:87.15ms
step:493/1680 train_time:42967ms step_avg:87.15ms
step:494/1680 train_time:43054ms step_avg:87.15ms
step:495/1680 train_time:43141ms step_avg:87.15ms
step:496/1680 train_time:43228ms step_avg:87.15ms
step:497/1680 train_time:43315ms step_avg:87.15ms
step:498/1680 train_time:43402ms step_avg:87.15ms
step:499/1680 train_time:43490ms step_avg:87.15ms
step:500/1680 train_time:43576ms step_avg:87.15ms
step:500/1680 val_loss:3.7161 train_time:43665ms step_avg:87.33ms
step:501/1680 train_time:43684ms step_avg:87.19ms
step:502/1680 train_time:43754ms step_avg:87.16ms
step:503/1680 train_time:43849ms step_avg:87.17ms
step:504/1680 train_time:43937ms step_avg:87.18ms
step:505/1680 train_time:44023ms step_avg:87.18ms
step:506/1680 train_time:44110ms step_avg:87.17ms
step:507/1680 train_time:44196ms step_avg:87.17ms
step:508/1680 train_time:44282ms step_avg:87.17ms
step:509/1680 train_time:44367ms step_avg:87.17ms
step:510/1680 train_time:44454ms step_avg:87.17ms
step:511/1680 train_time:44541ms step_avg:87.16ms
step:512/1680 train_time:44628ms step_avg:87.16ms
step:513/1680 train_time:44716ms step_avg:87.17ms
step:514/1680 train_time:44806ms step_avg:87.17ms
step:515/1680 train_time:44894ms step_avg:87.17ms
step:516/1680 train_time:44982ms step_avg:87.17ms
step:517/1680 train_time:45069ms step_avg:87.17ms
step:518/1680 train_time:45155ms step_avg:87.17ms
step:519/1680 train_time:45242ms step_avg:87.17ms
step:520/1680 train_time:45328ms step_avg:87.17ms
step:521/1680 train_time:45414ms step_avg:87.17ms
step:522/1680 train_time:45500ms step_avg:87.17ms
step:523/1680 train_time:45587ms step_avg:87.16ms
step:524/1680 train_time:45675ms step_avg:87.17ms
step:525/1680 train_time:45763ms step_avg:87.17ms
step:526/1680 train_time:45851ms step_avg:87.17ms
step:527/1680 train_time:45939ms step_avg:87.17ms
step:528/1680 train_time:46027ms step_avg:87.17ms
step:529/1680 train_time:46114ms step_avg:87.17ms
step:530/1680 train_time:46201ms step_avg:87.17ms
step:531/1680 train_time:46287ms step_avg:87.17ms
step:532/1680 train_time:46373ms step_avg:87.17ms
step:533/1680 train_time:46459ms step_avg:87.17ms
step:534/1680 train_time:46547ms step_avg:87.17ms
step:535/1680 train_time:46633ms step_avg:87.16ms
step:536/1680 train_time:46721ms step_avg:87.17ms
step:537/1680 train_time:46809ms step_avg:87.17ms
step:538/1680 train_time:46897ms step_avg:87.17ms
step:539/1680 train_time:46985ms step_avg:87.17ms
step:540/1680 train_time:47072ms step_avg:87.17ms
step:541/1680 train_time:47160ms step_avg:87.17ms
step:542/1680 train_time:47247ms step_avg:87.17ms
step:543/1680 train_time:47333ms step_avg:87.17ms
step:544/1680 train_time:47419ms step_avg:87.17ms
step:545/1680 train_time:47506ms step_avg:87.17ms
step:546/1680 train_time:47593ms step_avg:87.17ms
step:547/1680 train_time:47680ms step_avg:87.17ms
step:548/1680 train_time:47768ms step_avg:87.17ms
step:549/1680 train_time:47857ms step_avg:87.17ms
step:550/1680 train_time:47947ms step_avg:87.18ms
step:551/1680 train_time:48035ms step_avg:87.18ms
step:552/1680 train_time:48123ms step_avg:87.18ms
step:553/1680 train_time:48211ms step_avg:87.18ms
step:554/1680 train_time:48299ms step_avg:87.18ms
step:555/1680 train_time:48387ms step_avg:87.18ms
step:556/1680 train_time:48475ms step_avg:87.19ms
step:557/1680 train_time:48562ms step_avg:87.19ms
step:558/1680 train_time:48650ms step_avg:87.19ms
step:559/1680 train_time:48739ms step_avg:87.19ms
step:560/1680 train_time:48828ms step_avg:87.19ms
step:561/1680 train_time:48916ms step_avg:87.19ms
step:562/1680 train_time:49005ms step_avg:87.20ms
step:563/1680 train_time:49093ms step_avg:87.20ms
step:564/1680 train_time:49180ms step_avg:87.20ms
step:565/1680 train_time:49269ms step_avg:87.20ms
step:566/1680 train_time:49357ms step_avg:87.20ms
step:567/1680 train_time:49446ms step_avg:87.21ms
step:568/1680 train_time:49533ms step_avg:87.21ms
step:569/1680 train_time:49622ms step_avg:87.21ms
step:570/1680 train_time:49711ms step_avg:87.21ms
step:571/1680 train_time:49799ms step_avg:87.21ms
step:572/1680 train_time:49888ms step_avg:87.22ms
step:573/1680 train_time:49976ms step_avg:87.22ms
step:574/1680 train_time:50064ms step_avg:87.22ms
step:575/1680 train_time:50152ms step_avg:87.22ms
step:576/1680 train_time:50241ms step_avg:87.22ms
step:577/1680 train_time:50330ms step_avg:87.23ms
step:578/1680 train_time:50418ms step_avg:87.23ms
step:579/1680 train_time:50506ms step_avg:87.23ms
step:580/1680 train_time:50594ms step_avg:87.23ms
step:581/1680 train_time:50682ms step_avg:87.23ms
step:582/1680 train_time:50771ms step_avg:87.24ms
step:583/1680 train_time:50860ms step_avg:87.24ms
step:584/1680 train_time:50949ms step_avg:87.24ms
step:585/1680 train_time:51037ms step_avg:87.24ms
step:586/1680 train_time:51125ms step_avg:87.24ms
step:587/1680 train_time:51213ms step_avg:87.25ms
step:588/1680 train_time:51302ms step_avg:87.25ms
step:589/1680 train_time:51389ms step_avg:87.25ms
step:590/1680 train_time:51477ms step_avg:87.25ms
step:591/1680 train_time:51565ms step_avg:87.25ms
step:592/1680 train_time:51653ms step_avg:87.25ms
step:593/1680 train_time:51741ms step_avg:87.25ms
step:594/1680 train_time:51830ms step_avg:87.26ms
step:595/1680 train_time:51918ms step_avg:87.26ms
step:596/1680 train_time:52008ms step_avg:87.26ms
step:597/1680 train_time:52096ms step_avg:87.26ms
step:598/1680 train_time:52185ms step_avg:87.27ms
step:599/1680 train_time:52273ms step_avg:87.27ms
step:600/1680 train_time:52361ms step_avg:87.27ms
step:601/1680 train_time:52450ms step_avg:87.27ms
step:602/1680 train_time:52538ms step_avg:87.27ms
step:603/1680 train_time:52627ms step_avg:87.27ms
step:604/1680 train_time:52715ms step_avg:87.28ms
step:605/1680 train_time:52803ms step_avg:87.28ms
step:606/1680 train_time:52892ms step_avg:87.28ms
step:607/1680 train_time:52980ms step_avg:87.28ms
step:608/1680 train_time:53069ms step_avg:87.28ms
step:609/1680 train_time:53157ms step_avg:87.29ms
step:610/1680 train_time:53245ms step_avg:87.29ms
step:611/1680 train_time:53333ms step_avg:87.29ms
step:612/1680 train_time:53421ms step_avg:87.29ms
step:613/1680 train_time:53510ms step_avg:87.29ms
step:614/1680 train_time:53598ms step_avg:87.29ms
step:615/1680 train_time:53685ms step_avg:87.29ms
step:616/1680 train_time:53773ms step_avg:87.29ms
step:617/1680 train_time:53862ms step_avg:87.30ms
step:618/1680 train_time:53951ms step_avg:87.30ms
step:619/1680 train_time:54038ms step_avg:87.30ms
step:620/1680 train_time:54127ms step_avg:87.30ms
step:621/1680 train_time:54215ms step_avg:87.30ms
step:622/1680 train_time:54302ms step_avg:87.30ms
step:623/1680 train_time:54390ms step_avg:87.30ms
step:624/1680 train_time:54479ms step_avg:87.31ms
step:625/1680 train_time:54567ms step_avg:87.31ms
step:625/1680 val_loss:3.6134 train_time:54656ms step_avg:87.45ms
step:626/1680 train_time:54677ms step_avg:87.34ms
step:627/1680 train_time:54746ms step_avg:87.31ms
step:628/1680 train_time:54836ms step_avg:87.32ms
step:629/1680 train_time:54928ms step_avg:87.33ms
step:630/1680 train_time:55018ms step_avg:87.33ms
step:631/1680 train_time:55105ms step_avg:87.33ms
step:632/1680 train_time:55192ms step_avg:87.33ms
step:633/1680 train_time:55279ms step_avg:87.33ms
step:634/1680 train_time:55366ms step_avg:87.33ms
step:635/1680 train_time:55453ms step_avg:87.33ms
step:636/1680 train_time:55540ms step_avg:87.33ms
step:637/1680 train_time:55633ms step_avg:87.34ms
step:638/1680 train_time:55723ms step_avg:87.34ms
step:639/1680 train_time:55811ms step_avg:87.34ms
step:640/1680 train_time:55902ms step_avg:87.35ms
step:641/1680 train_time:55990ms step_avg:87.35ms
step:642/1680 train_time:56078ms step_avg:87.35ms
step:643/1680 train_time:56165ms step_avg:87.35ms
step:644/1680 train_time:56253ms step_avg:87.35ms
step:645/1680 train_time:56340ms step_avg:87.35ms
step:646/1680 train_time:56427ms step_avg:87.35ms
step:647/1680 train_time:56515ms step_avg:87.35ms
step:648/1680 train_time:56603ms step_avg:87.35ms
step:649/1680 train_time:56692ms step_avg:87.35ms
step:650/1680 train_time:56781ms step_avg:87.36ms
step:651/1680 train_time:56870ms step_avg:87.36ms
step:652/1680 train_time:56959ms step_avg:87.36ms
step:653/1680 train_time:57048ms step_avg:87.36ms
step:654/1680 train_time:57137ms step_avg:87.36ms
step:655/1680 train_time:57225ms step_avg:87.37ms
step:656/1680 train_time:57313ms step_avg:87.37ms
step:657/1680 train_time:57402ms step_avg:87.37ms
step:658/1680 train_time:57489ms step_avg:87.37ms
step:659/1680 train_time:57579ms step_avg:87.37ms
step:660/1680 train_time:57666ms step_avg:87.37ms
step:661/1680 train_time:57754ms step_avg:87.37ms
step:662/1680 train_time:57844ms step_avg:87.38ms
step:663/1680 train_time:57932ms step_avg:87.38ms
step:664/1680 train_time:58021ms step_avg:87.38ms
step:665/1680 train_time:58109ms step_avg:87.38ms
step:666/1680 train_time:58197ms step_avg:87.38ms
step:667/1680 train_time:58285ms step_avg:87.38ms
step:668/1680 train_time:58373ms step_avg:87.38ms
step:669/1680 train_time:58460ms step_avg:87.38ms
step:670/1680 train_time:58548ms step_avg:87.39ms
step:671/1680 train_time:58636ms step_avg:87.39ms
step:672/1680 train_time:58724ms step_avg:87.39ms
step:673/1680 train_time:58812ms step_avg:87.39ms
step:674/1680 train_time:58901ms step_avg:87.39ms
step:675/1680 train_time:58989ms step_avg:87.39ms
step:676/1680 train_time:59078ms step_avg:87.39ms
step:677/1680 train_time:59166ms step_avg:87.39ms
step:678/1680 train_time:59254ms step_avg:87.40ms
step:679/1680 train_time:59342ms step_avg:87.40ms
step:680/1680 train_time:59430ms step_avg:87.40ms
step:681/1680 train_time:59518ms step_avg:87.40ms
step:682/1680 train_time:59607ms step_avg:87.40ms
step:683/1680 train_time:59695ms step_avg:87.40ms
step:684/1680 train_time:59784ms step_avg:87.40ms
step:685/1680 train_time:59872ms step_avg:87.40ms
step:686/1680 train_time:59959ms step_avg:87.40ms
step:687/1680 train_time:60048ms step_avg:87.41ms
step:688/1680 train_time:60136ms step_avg:87.41ms
step:689/1680 train_time:60224ms step_avg:87.41ms
step:690/1680 train_time:60312ms step_avg:87.41ms
step:691/1680 train_time:60400ms step_avg:87.41ms
step:692/1680 train_time:60488ms step_avg:87.41ms
step:693/1680 train_time:60576ms step_avg:87.41ms
step:694/1680 train_time:60664ms step_avg:87.41ms
step:695/1680 train_time:60752ms step_avg:87.41ms
step:696/1680 train_time:60840ms step_avg:87.41ms
step:697/1680 train_time:60928ms step_avg:87.41ms
step:698/1680 train_time:61017ms step_avg:87.42ms
step:699/1680 train_time:61105ms step_avg:87.42ms
step:700/1680 train_time:61194ms step_avg:87.42ms
step:701/1680 train_time:61284ms step_avg:87.42ms
step:702/1680 train_time:61372ms step_avg:87.42ms
step:703/1680 train_time:61459ms step_avg:87.42ms
step:704/1680 train_time:61547ms step_avg:87.42ms
step:705/1680 train_time:61635ms step_avg:87.43ms
step:706/1680 train_time:61724ms step_avg:87.43ms
step:707/1680 train_time:61812ms step_avg:87.43ms
step:708/1680 train_time:61900ms step_avg:87.43ms
step:709/1680 train_time:61988ms step_avg:87.43ms
step:710/1680 train_time:62076ms step_avg:87.43ms
step:711/1680 train_time:62164ms step_avg:87.43ms
step:712/1680 train_time:62252ms step_avg:87.43ms
step:713/1680 train_time:62341ms step_avg:87.43ms
step:714/1680 train_time:62428ms step_avg:87.43ms
step:715/1680 train_time:62517ms step_avg:87.44ms
step:716/1680 train_time:62605ms step_avg:87.44ms
step:717/1680 train_time:62693ms step_avg:87.44ms
step:718/1680 train_time:62782ms step_avg:87.44ms
step:719/1680 train_time:62869ms step_avg:87.44ms
step:720/1680 train_time:62957ms step_avg:87.44ms
step:721/1680 train_time:63045ms step_avg:87.44ms
step:722/1680 train_time:63133ms step_avg:87.44ms
step:723/1680 train_time:63222ms step_avg:87.44ms
step:724/1680 train_time:63310ms step_avg:87.44ms
step:725/1680 train_time:63398ms step_avg:87.45ms
step:726/1680 train_time:63486ms step_avg:87.45ms
step:727/1680 train_time:63575ms step_avg:87.45ms
step:728/1680 train_time:63662ms step_avg:87.45ms
step:729/1680 train_time:63750ms step_avg:87.45ms
step:730/1680 train_time:63838ms step_avg:87.45ms
step:731/1680 train_time:63927ms step_avg:87.45ms
step:732/1680 train_time:64015ms step_avg:87.45ms
step:733/1680 train_time:64104ms step_avg:87.45ms
step:734/1680 train_time:64192ms step_avg:87.45ms
step:735/1680 train_time:64280ms step_avg:87.46ms
step:736/1680 train_time:64368ms step_avg:87.46ms
step:737/1680 train_time:64456ms step_avg:87.46ms
step:738/1680 train_time:64544ms step_avg:87.46ms
step:739/1680 train_time:64633ms step_avg:87.46ms
step:740/1680 train_time:64720ms step_avg:87.46ms
step:741/1680 train_time:64808ms step_avg:87.46ms
step:742/1680 train_time:64896ms step_avg:87.46ms
step:743/1680 train_time:64985ms step_avg:87.46ms
step:744/1680 train_time:65073ms step_avg:87.46ms
step:745/1680 train_time:65161ms step_avg:87.46ms
step:746/1680 train_time:65248ms step_avg:87.46ms
step:747/1680 train_time:65337ms step_avg:87.47ms
step:748/1680 train_time:65427ms step_avg:87.47ms
step:749/1680 train_time:65515ms step_avg:87.47ms
step:750/1680 train_time:65603ms step_avg:87.47ms
step:750/1680 val_loss:3.5645 train_time:65694ms step_avg:87.59ms
step:751/1680 train_time:65712ms step_avg:87.50ms
step:752/1680 train_time:65784ms step_avg:87.48ms
step:753/1680 train_time:65881ms step_avg:87.49ms
step:754/1680 train_time:65971ms step_avg:87.50ms
step:755/1680 train_time:66059ms step_avg:87.50ms
step:756/1680 train_time:66147ms step_avg:87.50ms
step:757/1680 train_time:66234ms step_avg:87.50ms
step:758/1680 train_time:66321ms step_avg:87.50ms
step:759/1680 train_time:66408ms step_avg:87.49ms
step:760/1680 train_time:66495ms step_avg:87.49ms
step:761/1680 train_time:66582ms step_avg:87.49ms
step:762/1680 train_time:66670ms step_avg:87.49ms
step:763/1680 train_time:66759ms step_avg:87.50ms
step:764/1680 train_time:66849ms step_avg:87.50ms
step:765/1680 train_time:66939ms step_avg:87.50ms
step:766/1680 train_time:67029ms step_avg:87.50ms
step:767/1680 train_time:67118ms step_avg:87.51ms
step:768/1680 train_time:67206ms step_avg:87.51ms
step:769/1680 train_time:67294ms step_avg:87.51ms
step:770/1680 train_time:67382ms step_avg:87.51ms
step:771/1680 train_time:67468ms step_avg:87.51ms
step:772/1680 train_time:67556ms step_avg:87.51ms
step:773/1680 train_time:67644ms step_avg:87.51ms
step:774/1680 train_time:67732ms step_avg:87.51ms
step:775/1680 train_time:67821ms step_avg:87.51ms
step:776/1680 train_time:67911ms step_avg:87.51ms
step:777/1680 train_time:68000ms step_avg:87.52ms
step:778/1680 train_time:68089ms step_avg:87.52ms
step:779/1680 train_time:68178ms step_avg:87.52ms
step:780/1680 train_time:68266ms step_avg:87.52ms
step:781/1680 train_time:68354ms step_avg:87.52ms
step:782/1680 train_time:68442ms step_avg:87.52ms
step:783/1680 train_time:68530ms step_avg:87.52ms
step:784/1680 train_time:68618ms step_avg:87.52ms
step:785/1680 train_time:68706ms step_avg:87.52ms
step:786/1680 train_time:68794ms step_avg:87.52ms
step:787/1680 train_time:68884ms step_avg:87.53ms
step:788/1680 train_time:68973ms step_avg:87.53ms
step:789/1680 train_time:69061ms step_avg:87.53ms
step:790/1680 train_time:69149ms step_avg:87.53ms
step:791/1680 train_time:69237ms step_avg:87.53ms
step:792/1680 train_time:69325ms step_avg:87.53ms
step:793/1680 train_time:69413ms step_avg:87.53ms
step:794/1680 train_time:69501ms step_avg:87.53ms
step:795/1680 train_time:69589ms step_avg:87.53ms
step:796/1680 train_time:69677ms step_avg:87.53ms
step:797/1680 train_time:69766ms step_avg:87.54ms
step:798/1680 train_time:69854ms step_avg:87.54ms
step:799/1680 train_time:69942ms step_avg:87.54ms
step:800/1680 train_time:70031ms step_avg:87.54ms
step:801/1680 train_time:70119ms step_avg:87.54ms
step:802/1680 train_time:70208ms step_avg:87.54ms
step:803/1680 train_time:70296ms step_avg:87.54ms
step:804/1680 train_time:70383ms step_avg:87.54ms
step:805/1680 train_time:70472ms step_avg:87.54ms
step:806/1680 train_time:70559ms step_avg:87.54ms
step:807/1680 train_time:70647ms step_avg:87.54ms
step:808/1680 train_time:70736ms step_avg:87.54ms
step:809/1680 train_time:70824ms step_avg:87.55ms
step:810/1680 train_time:70913ms step_avg:87.55ms
step:811/1680 train_time:71001ms step_avg:87.55ms
step:812/1680 train_time:71089ms step_avg:87.55ms
step:813/1680 train_time:71178ms step_avg:87.55ms
step:814/1680 train_time:71266ms step_avg:87.55ms
step:815/1680 train_time:71354ms step_avg:87.55ms
step:816/1680 train_time:71443ms step_avg:87.55ms
step:817/1680 train_time:71530ms step_avg:87.55ms
step:818/1680 train_time:71618ms step_avg:87.55ms
step:819/1680 train_time:71707ms step_avg:87.55ms
step:820/1680 train_time:71795ms step_avg:87.56ms
step:821/1680 train_time:71884ms step_avg:87.56ms
step:822/1680 train_time:71972ms step_avg:87.56ms
step:823/1680 train_time:72060ms step_avg:87.56ms
step:824/1680 train_time:72149ms step_avg:87.56ms
step:825/1680 train_time:72237ms step_avg:87.56ms
step:826/1680 train_time:72325ms step_avg:87.56ms
step:827/1680 train_time:72413ms step_avg:87.56ms
step:828/1680 train_time:72501ms step_avg:87.56ms
step:829/1680 train_time:72590ms step_avg:87.56ms
step:830/1680 train_time:72678ms step_avg:87.56ms
step:831/1680 train_time:72766ms step_avg:87.56ms
step:832/1680 train_time:72854ms step_avg:87.56ms
step:833/1680 train_time:72942ms step_avg:87.57ms
step:834/1680 train_time:73030ms step_avg:87.57ms
step:835/1680 train_time:73118ms step_avg:87.57ms
step:836/1680 train_time:73207ms step_avg:87.57ms
step:837/1680 train_time:73295ms step_avg:87.57ms
step:838/1680 train_time:73383ms step_avg:87.57ms
step:839/1680 train_time:73471ms step_avg:87.57ms
step:840/1680 train_time:73559ms step_avg:87.57ms
step:841/1680 train_time:73648ms step_avg:87.57ms
step:842/1680 train_time:73736ms step_avg:87.57ms
step:843/1680 train_time:73825ms step_avg:87.57ms
step:844/1680 train_time:73913ms step_avg:87.57ms
step:845/1680 train_time:74000ms step_avg:87.57ms
step:846/1680 train_time:74088ms step_avg:87.58ms
step:847/1680 train_time:74177ms step_avg:87.58ms
step:848/1680 train_time:74266ms step_avg:87.58ms
step:849/1680 train_time:74354ms step_avg:87.58ms
step:850/1680 train_time:74442ms step_avg:87.58ms
step:851/1680 train_time:74530ms step_avg:87.58ms
step:852/1680 train_time:74618ms step_avg:87.58ms
step:853/1680 train_time:74706ms step_avg:87.58ms
step:854/1680 train_time:74794ms step_avg:87.58ms
step:855/1680 train_time:74881ms step_avg:87.58ms
step:856/1680 train_time:74971ms step_avg:87.58ms
step:857/1680 train_time:75058ms step_avg:87.58ms
step:858/1680 train_time:75147ms step_avg:87.58ms
step:859/1680 train_time:75236ms step_avg:87.59ms
step:860/1680 train_time:75325ms step_avg:87.59ms
step:861/1680 train_time:75412ms step_avg:87.59ms
step:862/1680 train_time:75501ms step_avg:87.59ms
step:863/1680 train_time:75589ms step_avg:87.59ms
step:864/1680 train_time:75677ms step_avg:87.59ms
step:865/1680 train_time:75765ms step_avg:87.59ms
step:866/1680 train_time:75854ms step_avg:87.59ms
step:867/1680 train_time:75942ms step_avg:87.59ms
step:868/1680 train_time:76029ms step_avg:87.59ms
step:869/1680 train_time:76117ms step_avg:87.59ms
step:870/1680 train_time:76206ms step_avg:87.59ms
step:871/1680 train_time:76294ms step_avg:87.59ms
step:872/1680 train_time:76382ms step_avg:87.59ms
step:873/1680 train_time:76470ms step_avg:87.59ms
step:874/1680 train_time:76559ms step_avg:87.60ms
step:875/1680 train_time:76647ms step_avg:87.60ms
step:875/1680 val_loss:3.5174 train_time:76737ms step_avg:87.70ms
step:876/1680 train_time:76755ms step_avg:87.62ms
step:877/1680 train_time:76829ms step_avg:87.60ms
step:878/1680 train_time:76922ms step_avg:87.61ms
step:879/1680 train_time:77011ms step_avg:87.61ms
step:880/1680 train_time:77099ms step_avg:87.61ms
step:881/1680 train_time:77186ms step_avg:87.61ms
step:882/1680 train_time:77273ms step_avg:87.61ms
step:883/1680 train_time:77360ms step_avg:87.61ms
step:884/1680 train_time:77447ms step_avg:87.61ms
step:885/1680 train_time:77534ms step_avg:87.61ms
step:886/1680 train_time:77621ms step_avg:87.61ms
step:887/1680 train_time:77710ms step_avg:87.61ms
step:888/1680 train_time:77800ms step_avg:87.61ms
step:889/1680 train_time:77892ms step_avg:87.62ms
step:890/1680 train_time:77981ms step_avg:87.62ms
step:891/1680 train_time:78069ms step_avg:87.62ms
step:892/1680 train_time:78157ms step_avg:87.62ms
step:893/1680 train_time:78244ms step_avg:87.62ms
step:894/1680 train_time:78332ms step_avg:87.62ms
step:895/1680 train_time:78419ms step_avg:87.62ms
step:896/1680 train_time:78506ms step_avg:87.62ms
step:897/1680 train_time:78594ms step_avg:87.62ms
step:898/1680 train_time:78682ms step_avg:87.62ms
step:899/1680 train_time:78772ms step_avg:87.62ms
step:900/1680 train_time:78866ms step_avg:87.63ms
step:901/1680 train_time:78955ms step_avg:87.63ms
step:902/1680 train_time:79044ms step_avg:87.63ms
step:903/1680 train_time:79131ms step_avg:87.63ms
step:904/1680 train_time:79220ms step_avg:87.63ms
step:905/1680 train_time:79307ms step_avg:87.63ms
step:906/1680 train_time:79395ms step_avg:87.63ms
step:907/1680 train_time:79483ms step_avg:87.63ms
step:908/1680 train_time:79570ms step_avg:87.63ms
step:909/1680 train_time:79657ms step_avg:87.63ms
step:910/1680 train_time:79746ms step_avg:87.63ms
step:911/1680 train_time:79835ms step_avg:87.63ms
step:912/1680 train_time:79924ms step_avg:87.64ms
step:913/1680 train_time:80013ms step_avg:87.64ms
step:914/1680 train_time:80102ms step_avg:87.64ms
step:915/1680 train_time:80190ms step_avg:87.64ms
step:916/1680 train_time:80278ms step_avg:87.64ms
step:917/1680 train_time:80366ms step_avg:87.64ms
step:918/1680 train_time:80454ms step_avg:87.64ms
step:919/1680 train_time:80542ms step_avg:87.64ms
step:920/1680 train_time:80630ms step_avg:87.64ms
step:921/1680 train_time:80718ms step_avg:87.64ms
step:922/1680 train_time:80806ms step_avg:87.64ms
step:923/1680 train_time:80895ms step_avg:87.64ms
step:924/1680 train_time:80985ms step_avg:87.65ms
step:925/1680 train_time:81073ms step_avg:87.65ms
step:926/1680 train_time:81161ms step_avg:87.65ms
step:927/1680 train_time:81250ms step_avg:87.65ms
step:928/1680 train_time:81338ms step_avg:87.65ms
step:929/1680 train_time:81427ms step_avg:87.65ms
step:930/1680 train_time:81516ms step_avg:87.65ms
step:931/1680 train_time:81603ms step_avg:87.65ms
step:932/1680 train_time:81692ms step_avg:87.65ms
step:933/1680 train_time:81780ms step_avg:87.65ms
step:934/1680 train_time:81868ms step_avg:87.65ms
step:935/1680 train_time:81957ms step_avg:87.65ms
step:936/1680 train_time:82045ms step_avg:87.65ms
step:937/1680 train_time:82133ms step_avg:87.66ms
step:938/1680 train_time:82222ms step_avg:87.66ms
step:939/1680 train_time:82310ms step_avg:87.66ms
step:940/1680 train_time:82399ms step_avg:87.66ms
step:941/1680 train_time:82487ms step_avg:87.66ms
step:942/1680 train_time:82576ms step_avg:87.66ms
step:943/1680 train_time:82664ms step_avg:87.66ms
step:944/1680 train_time:82752ms step_avg:87.66ms
step:945/1680 train_time:82840ms step_avg:87.66ms
step:946/1680 train_time:82928ms step_avg:87.66ms
step:947/1680 train_time:83017ms step_avg:87.66ms
step:948/1680 train_time:83105ms step_avg:87.66ms
step:949/1680 train_time:83193ms step_avg:87.66ms
step:950/1680 train_time:83281ms step_avg:87.66ms
step:951/1680 train_time:83369ms step_avg:87.66ms
step:952/1680 train_time:83458ms step_avg:87.67ms
step:953/1680 train_time:83546ms step_avg:87.67ms
step:954/1680 train_time:83634ms step_avg:87.67ms
step:955/1680 train_time:83723ms step_avg:87.67ms
step:956/1680 train_time:83811ms step_avg:87.67ms
step:957/1680 train_time:83899ms step_avg:87.67ms
step:958/1680 train_time:83986ms step_avg:87.67ms
step:959/1680 train_time:84074ms step_avg:87.67ms
step:960/1680 train_time:84163ms step_avg:87.67ms
step:961/1680 train_time:84251ms step_avg:87.67ms
step:962/1680 train_time:84340ms step_avg:87.67ms
step:963/1680 train_time:84428ms step_avg:87.67ms
step:964/1680 train_time:84515ms step_avg:87.67ms
step:965/1680 train_time:84604ms step_avg:87.67ms
step:966/1680 train_time:84692ms step_avg:87.67ms
step:967/1680 train_time:84781ms step_avg:87.67ms
step:968/1680 train_time:84869ms step_avg:87.67ms
step:969/1680 train_time:84957ms step_avg:87.67ms
step:970/1680 train_time:85045ms step_avg:87.68ms
step:971/1680 train_time:85134ms step_avg:87.68ms
step:972/1680 train_time:85222ms step_avg:87.68ms
step:973/1680 train_time:85311ms step_avg:87.68ms
step:974/1680 train_time:85398ms step_avg:87.68ms
step:975/1680 train_time:85486ms step_avg:87.68ms
step:976/1680 train_time:85574ms step_avg:87.68ms
step:977/1680 train_time:85662ms step_avg:87.68ms
step:978/1680 train_time:85750ms step_avg:87.68ms
step:979/1680 train_time:85838ms step_avg:87.68ms
step:980/1680 train_time:85926ms step_avg:87.68ms
step:981/1680 train_time:86015ms step_avg:87.68ms
step:982/1680 train_time:86103ms step_avg:87.68ms
step:983/1680 train_time:86192ms step_avg:87.68ms
step:984/1680 train_time:86280ms step_avg:87.68ms
step:985/1680 train_time:86368ms step_avg:87.68ms
step:986/1680 train_time:86456ms step_avg:87.68ms
step:987/1680 train_time:86544ms step_avg:87.68ms
step:988/1680 train_time:86632ms step_avg:87.68ms
step:989/1680 train_time:86721ms step_avg:87.69ms
step:990/1680 train_time:86809ms step_avg:87.69ms
step:991/1680 train_time:86897ms step_avg:87.69ms
step:992/1680 train_time:86986ms step_avg:87.69ms
step:993/1680 train_time:87075ms step_avg:87.69ms
step:994/1680 train_time:87163ms step_avg:87.69ms
step:995/1680 train_time:87251ms step_avg:87.69ms
step:996/1680 train_time:87340ms step_avg:87.69ms
step:997/1680 train_time:87427ms step_avg:87.69ms
step:998/1680 train_time:87515ms step_avg:87.69ms
step:999/1680 train_time:87603ms step_avg:87.69ms
step:1000/1680 train_time:87691ms step_avg:87.69ms
step:1000/1680 val_loss:3.4686 train_time:87780ms step_avg:87.78ms
step:1001/1680 train_time:87799ms step_avg:87.71ms
step:1002/1680 train_time:87871ms step_avg:87.70ms
step:1003/1680 train_time:87964ms step_avg:87.70ms
step:1004/1680 train_time:88054ms step_avg:87.70ms
step:1005/1680 train_time:88142ms step_avg:87.70ms
step:1006/1680 train_time:88229ms step_avg:87.70ms
step:1007/1680 train_time:88316ms step_avg:87.70ms
step:1008/1680 train_time:88404ms step_avg:87.70ms
step:1009/1680 train_time:88491ms step_avg:87.70ms
step:1010/1680 train_time:88579ms step_avg:87.70ms
step:1011/1680 train_time:88666ms step_avg:87.70ms
step:1012/1680 train_time:88754ms step_avg:87.70ms
step:1013/1680 train_time:88844ms step_avg:87.70ms
step:1014/1680 train_time:88935ms step_avg:87.71ms
step:1015/1680 train_time:89025ms step_avg:87.71ms
step:1016/1680 train_time:89114ms step_avg:87.71ms
step:1017/1680 train_time:89202ms step_avg:87.71ms
step:1018/1680 train_time:89290ms step_avg:87.71ms
step:1019/1680 train_time:89377ms step_avg:87.71ms
step:1020/1680 train_time:89465ms step_avg:87.71ms
step:1021/1680 train_time:89554ms step_avg:87.71ms
step:1022/1680 train_time:89641ms step_avg:87.71ms
step:1023/1680 train_time:89730ms step_avg:87.71ms
step:1024/1680 train_time:89819ms step_avg:87.71ms
step:1025/1680 train_time:89908ms step_avg:87.71ms
step:1026/1680 train_time:89996ms step_avg:87.72ms
step:1027/1680 train_time:90085ms step_avg:87.72ms
step:1028/1680 train_time:90174ms step_avg:87.72ms
step:1029/1680 train_time:90262ms step_avg:87.72ms
step:1030/1680 train_time:90350ms step_avg:87.72ms
step:1031/1680 train_time:90437ms step_avg:87.72ms
step:1032/1680 train_time:90526ms step_avg:87.72ms
step:1033/1680 train_time:90615ms step_avg:87.72ms
step:1034/1680 train_time:90704ms step_avg:87.72ms
step:1035/1680 train_time:90793ms step_avg:87.72ms
step:1036/1680 train_time:90881ms step_avg:87.72ms
step:1037/1680 train_time:90969ms step_avg:87.72ms
step:1038/1680 train_time:91058ms step_avg:87.72ms
step:1039/1680 train_time:91146ms step_avg:87.72ms
step:1040/1680 train_time:91235ms step_avg:87.73ms
step:1041/1680 train_time:91323ms step_avg:87.73ms
step:1042/1680 train_time:91411ms step_avg:87.73ms
step:1043/1680 train_time:91498ms step_avg:87.73ms
step:1044/1680 train_time:91586ms step_avg:87.73ms
step:1045/1680 train_time:91674ms step_avg:87.73ms
step:1046/1680 train_time:91763ms step_avg:87.73ms
step:1047/1680 train_time:91851ms step_avg:87.73ms
step:1048/1680 train_time:91939ms step_avg:87.73ms
step:1049/1680 train_time:92029ms step_avg:87.73ms
step:1050/1680 train_time:92117ms step_avg:87.73ms
step:1051/1680 train_time:92205ms step_avg:87.73ms
step:1052/1680 train_time:92293ms step_avg:87.73ms
step:1053/1680 train_time:92381ms step_avg:87.73ms
step:1054/1680 train_time:92469ms step_avg:87.73ms
step:1055/1680 train_time:92557ms step_avg:87.73ms
step:1056/1680 train_time:92645ms step_avg:87.73ms
step:1057/1680 train_time:92734ms step_avg:87.73ms
step:1058/1680 train_time:92822ms step_avg:87.73ms
step:1059/1680 train_time:92910ms step_avg:87.73ms
step:1060/1680 train_time:92998ms step_avg:87.73ms
step:1061/1680 train_time:93087ms step_avg:87.74ms
step:1062/1680 train_time:93176ms step_avg:87.74ms
step:1063/1680 train_time:93264ms step_avg:87.74ms
step:1064/1680 train_time:93352ms step_avg:87.74ms
step:1065/1680 train_time:93440ms step_avg:87.74ms
step:1066/1680 train_time:93528ms step_avg:87.74ms
step:1067/1680 train_time:93616ms step_avg:87.74ms
step:1068/1680 train_time:93704ms step_avg:87.74ms
step:1069/1680 train_time:93792ms step_avg:87.74ms
step:1070/1680 train_time:93881ms step_avg:87.74ms
step:1071/1680 train_time:93969ms step_avg:87.74ms
step:1072/1680 train_time:94057ms step_avg:87.74ms
step:1073/1680 train_time:94145ms step_avg:87.74ms
step:1074/1680 train_time:94234ms step_avg:87.74ms
step:1075/1680 train_time:94323ms step_avg:87.74ms
step:1076/1680 train_time:94411ms step_avg:87.74ms
step:1077/1680 train_time:94499ms step_avg:87.74ms
step:1078/1680 train_time:94587ms step_avg:87.74ms
step:1079/1680 train_time:94675ms step_avg:87.74ms
step:1080/1680 train_time:94763ms step_avg:87.74ms
step:1081/1680 train_time:94851ms step_avg:87.74ms
step:1082/1680 train_time:94939ms step_avg:87.74ms
step:1083/1680 train_time:95028ms step_avg:87.75ms
step:1084/1680 train_time:95117ms step_avg:87.75ms
step:1085/1680 train_time:95206ms step_avg:87.75ms
step:1086/1680 train_time:95294ms step_avg:87.75ms
step:1087/1680 train_time:95382ms step_avg:87.75ms
step:1088/1680 train_time:95472ms step_avg:87.75ms
step:1089/1680 train_time:95559ms step_avg:87.75ms
step:1090/1680 train_time:95648ms step_avg:87.75ms
step:1091/1680 train_time:95736ms step_avg:87.75ms
step:1092/1680 train_time:95824ms step_avg:87.75ms
step:1093/1680 train_time:95912ms step_avg:87.75ms
step:1094/1680 train_time:95999ms step_avg:87.75ms
step:1095/1680 train_time:96088ms step_avg:87.75ms
step:1096/1680 train_time:96177ms step_avg:87.75ms
step:1097/1680 train_time:96266ms step_avg:87.75ms
step:1098/1680 train_time:96355ms step_avg:87.75ms
step:1099/1680 train_time:96443ms step_avg:87.76ms
step:1100/1680 train_time:96533ms step_avg:87.76ms
step:1101/1680 train_time:96621ms step_avg:87.76ms
step:1102/1680 train_time:96710ms step_avg:87.76ms
step:1103/1680 train_time:96798ms step_avg:87.76ms
step:1104/1680 train_time:96887ms step_avg:87.76ms
step:1105/1680 train_time:96976ms step_avg:87.76ms
step:1106/1680 train_time:97066ms step_avg:87.76ms
step:1107/1680 train_time:97155ms step_avg:87.76ms
step:1108/1680 train_time:97245ms step_avg:87.77ms
step:1109/1680 train_time:97335ms step_avg:87.77ms
step:1110/1680 train_time:97424ms step_avg:87.77ms
step:1111/1680 train_time:97513ms step_avg:87.77ms
step:1112/1680 train_time:97602ms step_avg:87.77ms
step:1113/1680 train_time:97691ms step_avg:87.77ms
step:1114/1680 train_time:97779ms step_avg:87.77ms
step:1115/1680 train_time:97869ms step_avg:87.77ms
step:1116/1680 train_time:97957ms step_avg:87.78ms
step:1117/1680 train_time:98046ms step_avg:87.78ms
step:1118/1680 train_time:98135ms step_avg:87.78ms
step:1119/1680 train_time:98224ms step_avg:87.78ms
step:1120/1680 train_time:98313ms step_avg:87.78ms
step:1121/1680 train_time:98402ms step_avg:87.78ms
step:1122/1680 train_time:98491ms step_avg:87.78ms
step:1123/1680 train_time:98580ms step_avg:87.78ms
step:1124/1680 train_time:98669ms step_avg:87.78ms
step:1125/1680 train_time:98758ms step_avg:87.78ms
step:1125/1680 val_loss:3.4155 train_time:98849ms step_avg:87.87ms
step:1126/1680 train_time:98867ms step_avg:87.80ms
step:1127/1680 train_time:98939ms step_avg:87.79ms
step:1128/1680 train_time:99030ms step_avg:87.79ms
step:1129/1680 train_time:99122ms step_avg:87.80ms
step:1130/1680 train_time:99213ms step_avg:87.80ms
step:1131/1680 train_time:99301ms step_avg:87.80ms
step:1132/1680 train_time:99389ms step_avg:87.80ms
step:1133/1680 train_time:99477ms step_avg:87.80ms
step:1134/1680 train_time:99564ms step_avg:87.80ms
step:1135/1680 train_time:99652ms step_avg:87.80ms
step:1136/1680 train_time:99740ms step_avg:87.80ms
step:1137/1680 train_time:99831ms step_avg:87.80ms
step:1138/1680 train_time:99921ms step_avg:87.80ms
step:1139/1680 train_time:100012ms step_avg:87.81ms
step:1140/1680 train_time:100103ms step_avg:87.81ms
step:1141/1680 train_time:100193ms step_avg:87.81ms
step:1142/1680 train_time:100282ms step_avg:87.81ms
step:1143/1680 train_time:100370ms step_avg:87.81ms
step:1144/1680 train_time:100458ms step_avg:87.81ms
step:1145/1680 train_time:100546ms step_avg:87.81ms
step:1146/1680 train_time:100634ms step_avg:87.81ms
step:1147/1680 train_time:100722ms step_avg:87.81ms
step:1148/1680 train_time:100811ms step_avg:87.81ms
step:1149/1680 train_time:100902ms step_avg:87.82ms
step:1150/1680 train_time:100991ms step_avg:87.82ms
step:1151/1680 train_time:101081ms step_avg:87.82ms
step:1152/1680 train_time:101171ms step_avg:87.82ms
step:1153/1680 train_time:101260ms step_avg:87.82ms
step:1154/1680 train_time:101348ms step_avg:87.82ms
step:1155/1680 train_time:101438ms step_avg:87.83ms
step:1156/1680 train_time:101526ms step_avg:87.82ms
step:1157/1680 train_time:101614ms step_avg:87.83ms
step:1158/1680 train_time:101702ms step_avg:87.83ms
step:1159/1680 train_time:101791ms step_avg:87.83ms
step:1160/1680 train_time:101880ms step_avg:87.83ms
step:1161/1680 train_time:101970ms step_avg:87.83ms
step:1162/1680 train_time:102060ms step_avg:87.83ms
step:1163/1680 train_time:102150ms step_avg:87.83ms
step:1164/1680 train_time:102239ms step_avg:87.83ms
step:1165/1680 train_time:102328ms step_avg:87.83ms
step:1166/1680 train_time:102417ms step_avg:87.84ms
step:1167/1680 train_time:102505ms step_avg:87.84ms
step:1168/1680 train_time:102594ms step_avg:87.84ms
step:1169/1680 train_time:102682ms step_avg:87.84ms
step:1170/1680 train_time:102770ms step_avg:87.84ms
step:1171/1680 train_time:102859ms step_avg:87.84ms
step:1172/1680 train_time:102948ms step_avg:87.84ms
step:1173/1680 train_time:103037ms step_avg:87.84ms
step:1174/1680 train_time:103127ms step_avg:87.84ms
step:1175/1680 train_time:103216ms step_avg:87.84ms
step:1176/1680 train_time:103305ms step_avg:87.84ms
step:1177/1680 train_time:103393ms step_avg:87.84ms
step:1178/1680 train_time:103481ms step_avg:87.84ms
step:1179/1680 train_time:103570ms step_avg:87.85ms
step:1180/1680 train_time:103658ms step_avg:87.85ms
step:1181/1680 train_time:103747ms step_avg:87.85ms
step:1182/1680 train_time:103837ms step_avg:87.85ms
step:1183/1680 train_time:103925ms step_avg:87.85ms
step:1184/1680 train_time:104015ms step_avg:87.85ms
step:1185/1680 train_time:104104ms step_avg:87.85ms
step:1186/1680 train_time:104193ms step_avg:87.85ms
step:1187/1680 train_time:104282ms step_avg:87.85ms
step:1188/1680 train_time:104371ms step_avg:87.85ms
step:1189/1680 train_time:104460ms step_avg:87.85ms
step:1190/1680 train_time:104548ms step_avg:87.86ms
step:1191/1680 train_time:104638ms step_avg:87.86ms
step:1192/1680 train_time:104727ms step_avg:87.86ms
step:1193/1680 train_time:104816ms step_avg:87.86ms
step:1194/1680 train_time:104905ms step_avg:87.86ms
step:1195/1680 train_time:104994ms step_avg:87.86ms
step:1196/1680 train_time:105082ms step_avg:87.86ms
step:1197/1680 train_time:105171ms step_avg:87.86ms
step:1198/1680 train_time:105260ms step_avg:87.86ms
step:1199/1680 train_time:105349ms step_avg:87.86ms
step:1200/1680 train_time:105438ms step_avg:87.87ms
step:1201/1680 train_time:105527ms step_avg:87.87ms
step:1202/1680 train_time:105616ms step_avg:87.87ms
step:1203/1680 train_time:105704ms step_avg:87.87ms
step:1204/1680 train_time:105793ms step_avg:87.87ms
step:1205/1680 train_time:105882ms step_avg:87.87ms
step:1206/1680 train_time:105970ms step_avg:87.87ms
step:1207/1680 train_time:106059ms step_avg:87.87ms
step:1208/1680 train_time:106148ms step_avg:87.87ms
step:1209/1680 train_time:106237ms step_avg:87.87ms
step:1210/1680 train_time:106326ms step_avg:87.87ms
step:1211/1680 train_time:106415ms step_avg:87.87ms
step:1212/1680 train_time:106504ms step_avg:87.87ms
step:1213/1680 train_time:106593ms step_avg:87.88ms
step:1214/1680 train_time:106683ms step_avg:87.88ms
step:1215/1680 train_time:106772ms step_avg:87.88ms
step:1216/1680 train_time:106860ms step_avg:87.88ms
step:1217/1680 train_time:106948ms step_avg:87.88ms
step:1218/1680 train_time:107038ms step_avg:87.88ms
step:1219/1680 train_time:107128ms step_avg:87.88ms
step:1220/1680 train_time:107216ms step_avg:87.88ms
step:1221/1680 train_time:107304ms step_avg:87.88ms
step:1222/1680 train_time:107393ms step_avg:87.88ms
step:1223/1680 train_time:107482ms step_avg:87.88ms
step:1224/1680 train_time:107570ms step_avg:87.88ms
step:1225/1680 train_time:107659ms step_avg:87.89ms
step:1226/1680 train_time:107749ms step_avg:87.89ms
step:1227/1680 train_time:107838ms step_avg:87.89ms
step:1228/1680 train_time:107928ms step_avg:87.89ms
step:1229/1680 train_time:108017ms step_avg:87.89ms
step:1230/1680 train_time:108105ms step_avg:87.89ms
step:1231/1680 train_time:108195ms step_avg:87.89ms
step:1232/1680 train_time:108283ms step_avg:87.89ms
step:1233/1680 train_time:108372ms step_avg:87.89ms
step:1234/1680 train_time:108460ms step_avg:87.89ms
step:1235/1680 train_time:108549ms step_avg:87.89ms
step:1236/1680 train_time:108638ms step_avg:87.90ms
step:1237/1680 train_time:108728ms step_avg:87.90ms
step:1238/1680 train_time:108817ms step_avg:87.90ms
step:1239/1680 train_time:108906ms step_avg:87.90ms
step:1240/1680 train_time:108995ms step_avg:87.90ms
step:1241/1680 train_time:109083ms step_avg:87.90ms
step:1242/1680 train_time:109173ms step_avg:87.90ms
step:1243/1680 train_time:109261ms step_avg:87.90ms
step:1244/1680 train_time:109350ms step_avg:87.90ms
step:1245/1680 train_time:109440ms step_avg:87.90ms
step:1246/1680 train_time:109528ms step_avg:87.90ms
step:1247/1680 train_time:109618ms step_avg:87.91ms
step:1248/1680 train_time:109707ms step_avg:87.91ms
step:1249/1680 train_time:109797ms step_avg:87.91ms
step:1250/1680 train_time:109885ms step_avg:87.91ms
step:1250/1680 val_loss:3.3775 train_time:109975ms step_avg:87.98ms
step:1251/1680 train_time:109995ms step_avg:87.93ms
step:1252/1680 train_time:110066ms step_avg:87.91ms
step:1253/1680 train_time:110160ms step_avg:87.92ms
step:1254/1680 train_time:110250ms step_avg:87.92ms
step:1255/1680 train_time:110339ms step_avg:87.92ms
step:1256/1680 train_time:110427ms step_avg:87.92ms
step:1257/1680 train_time:110515ms step_avg:87.92ms
step:1258/1680 train_time:110603ms step_avg:87.92ms
step:1259/1680 train_time:110691ms step_avg:87.92ms
step:1260/1680 train_time:110779ms step_avg:87.92ms
step:1261/1680 train_time:110867ms step_avg:87.92ms
step:1262/1680 train_time:110957ms step_avg:87.92ms
step:1263/1680 train_time:111047ms step_avg:87.92ms
step:1264/1680 train_time:111138ms step_avg:87.93ms
step:1265/1680 train_time:111227ms step_avg:87.93ms
step:1266/1680 train_time:111317ms step_avg:87.93ms
step:1267/1680 train_time:111405ms step_avg:87.93ms
step:1268/1680 train_time:111493ms step_avg:87.93ms
step:1269/1680 train_time:111581ms step_avg:87.93ms
step:1270/1680 train_time:111670ms step_avg:87.93ms
step:1271/1680 train_time:111758ms step_avg:87.93ms
step:1272/1680 train_time:111848ms step_avg:87.93ms
step:1273/1680 train_time:111936ms step_avg:87.93ms
step:1274/1680 train_time:112025ms step_avg:87.93ms
step:1275/1680 train_time:112114ms step_avg:87.93ms
step:1276/1680 train_time:112203ms step_avg:87.93ms
step:1277/1680 train_time:112293ms step_avg:87.94ms
step:1278/1680 train_time:112381ms step_avg:87.94ms
step:1279/1680 train_time:112471ms step_avg:87.94ms
step:1280/1680 train_time:112560ms step_avg:87.94ms
step:1281/1680 train_time:112648ms step_avg:87.94ms
step:1282/1680 train_time:112737ms step_avg:87.94ms
step:1283/1680 train_time:112826ms step_avg:87.94ms
step:1284/1680 train_time:112914ms step_avg:87.94ms
step:1285/1680 train_time:113003ms step_avg:87.94ms
step:1286/1680 train_time:113091ms step_avg:87.94ms
step:1287/1680 train_time:113181ms step_avg:87.94ms
step:1288/1680 train_time:113270ms step_avg:87.94ms
step:1289/1680 train_time:113359ms step_avg:87.94ms
step:1290/1680 train_time:113450ms step_avg:87.95ms
step:1291/1680 train_time:113540ms step_avg:87.95ms
step:1292/1680 train_time:113628ms step_avg:87.95ms
step:1293/1680 train_time:113717ms step_avg:87.95ms
step:1294/1680 train_time:113805ms step_avg:87.95ms
step:1295/1680 train_time:113895ms step_avg:87.95ms
step:1296/1680 train_time:113983ms step_avg:87.95ms
step:1297/1680 train_time:114072ms step_avg:87.95ms
step:1298/1680 train_time:114161ms step_avg:87.95ms
step:1299/1680 train_time:114251ms step_avg:87.95ms
step:1300/1680 train_time:114340ms step_avg:87.95ms
step:1301/1680 train_time:114429ms step_avg:87.95ms
step:1302/1680 train_time:114517ms step_avg:87.96ms
step:1303/1680 train_time:114607ms step_avg:87.96ms
step:1304/1680 train_time:114695ms step_avg:87.96ms
step:1305/1680 train_time:114784ms step_avg:87.96ms
step:1306/1680 train_time:114873ms step_avg:87.96ms
step:1307/1680 train_time:114961ms step_avg:87.96ms
step:1308/1680 train_time:115050ms step_avg:87.96ms
step:1309/1680 train_time:115140ms step_avg:87.96ms
step:1310/1680 train_time:115229ms step_avg:87.96ms
step:1311/1680 train_time:115318ms step_avg:87.96ms
step:1312/1680 train_time:115408ms step_avg:87.96ms
step:1313/1680 train_time:115497ms step_avg:87.96ms
step:1314/1680 train_time:115585ms step_avg:87.96ms
step:1315/1680 train_time:115674ms step_avg:87.96ms
step:1316/1680 train_time:115762ms step_avg:87.97ms
step:1317/1680 train_time:115851ms step_avg:87.97ms
step:1318/1680 train_time:115939ms step_avg:87.97ms
step:1319/1680 train_time:116028ms step_avg:87.97ms
step:1320/1680 train_time:116117ms step_avg:87.97ms
step:1321/1680 train_time:116207ms step_avg:87.97ms
step:1322/1680 train_time:116297ms step_avg:87.97ms
step:1323/1680 train_time:116386ms step_avg:87.97ms
step:1324/1680 train_time:116474ms step_avg:87.97ms
step:1325/1680 train_time:116563ms step_avg:87.97ms
step:1326/1680 train_time:116652ms step_avg:87.97ms
step:1327/1680 train_time:116740ms step_avg:87.97ms
step:1328/1680 train_time:116828ms step_avg:87.97ms
step:1329/1680 train_time:116918ms step_avg:87.97ms
step:1330/1680 train_time:117007ms step_avg:87.97ms
step:1331/1680 train_time:117096ms step_avg:87.98ms
step:1332/1680 train_time:117185ms step_avg:87.98ms
step:1333/1680 train_time:117274ms step_avg:87.98ms
step:1334/1680 train_time:117363ms step_avg:87.98ms
step:1335/1680 train_time:117453ms step_avg:87.98ms
step:1336/1680 train_time:117542ms step_avg:87.98ms
step:1337/1680 train_time:117632ms step_avg:87.98ms
step:1338/1680 train_time:117721ms step_avg:87.98ms
step:1339/1680 train_time:117809ms step_avg:87.98ms
step:1340/1680 train_time:117898ms step_avg:87.98ms
step:1341/1680 train_time:117987ms step_avg:87.98ms
step:1342/1680 train_time:118076ms step_avg:87.98ms
step:1343/1680 train_time:118165ms step_avg:87.99ms
step:1344/1680 train_time:118254ms step_avg:87.99ms
step:1345/1680 train_time:118343ms step_avg:87.99ms
step:1346/1680 train_time:118433ms step_avg:87.99ms
step:1347/1680 train_time:118522ms step_avg:87.99ms
step:1348/1680 train_time:118610ms step_avg:87.99ms
step:1349/1680 train_time:118699ms step_avg:87.99ms
step:1350/1680 train_time:118787ms step_avg:87.99ms
step:1351/1680 train_time:118876ms step_avg:87.99ms
step:1352/1680 train_time:118965ms step_avg:87.99ms
step:1353/1680 train_time:119056ms step_avg:87.99ms
step:1354/1680 train_time:119145ms step_avg:87.99ms
step:1355/1680 train_time:119233ms step_avg:87.99ms
step:1356/1680 train_time:119322ms step_avg:88.00ms
step:1357/1680 train_time:119411ms step_avg:88.00ms
step:1358/1680 train_time:119500ms step_avg:88.00ms
step:1359/1680 train_time:119589ms step_avg:88.00ms
step:1360/1680 train_time:119678ms step_avg:88.00ms
step:1361/1680 train_time:119766ms step_avg:88.00ms
step:1362/1680 train_time:119855ms step_avg:88.00ms
step:1363/1680 train_time:119943ms step_avg:88.00ms
step:1364/1680 train_time:120032ms step_avg:88.00ms
step:1365/1680 train_time:120121ms step_avg:88.00ms
step:1366/1680 train_time:120211ms step_avg:88.00ms
step:1367/1680 train_time:120299ms step_avg:88.00ms
step:1368/1680 train_time:120388ms step_avg:88.00ms
step:1369/1680 train_time:120478ms step_avg:88.00ms
step:1370/1680 train_time:120566ms step_avg:88.00ms
step:1371/1680 train_time:120655ms step_avg:88.01ms
step:1372/1680 train_time:120744ms step_avg:88.01ms
step:1373/1680 train_time:120834ms step_avg:88.01ms
step:1374/1680 train_time:120922ms step_avg:88.01ms
step:1375/1680 train_time:121011ms step_avg:88.01ms
step:1375/1680 val_loss:3.3427 train_time:121101ms step_avg:88.07ms
step:1376/1680 train_time:121119ms step_avg:88.02ms
step:1377/1680 train_time:121194ms step_avg:88.01ms
step:1378/1680 train_time:121288ms step_avg:88.02ms
step:1379/1680 train_time:121378ms step_avg:88.02ms
step:1380/1680 train_time:121466ms step_avg:88.02ms
step:1381/1680 train_time:121554ms step_avg:88.02ms
step:1382/1680 train_time:121642ms step_avg:88.02ms
step:1383/1680 train_time:121729ms step_avg:88.02ms
step:1384/1680 train_time:121817ms step_avg:88.02ms
step:1385/1680 train_time:121905ms step_avg:88.02ms
step:1386/1680 train_time:121993ms step_avg:88.02ms
step:1387/1680 train_time:122084ms step_avg:88.02ms
step:1388/1680 train_time:122176ms step_avg:88.02ms
step:1389/1680 train_time:122266ms step_avg:88.02ms
step:1390/1680 train_time:122356ms step_avg:88.03ms
step:1391/1680 train_time:122445ms step_avg:88.03ms
step:1392/1680 train_time:122535ms step_avg:88.03ms
step:1393/1680 train_time:122622ms step_avg:88.03ms
step:1394/1680 train_time:122711ms step_avg:88.03ms
step:1395/1680 train_time:122799ms step_avg:88.03ms
step:1396/1680 train_time:122887ms step_avg:88.03ms
step:1397/1680 train_time:122976ms step_avg:88.03ms
step:1398/1680 train_time:123064ms step_avg:88.03ms
step:1399/1680 train_time:123153ms step_avg:88.03ms
step:1400/1680 train_time:123242ms step_avg:88.03ms
step:1401/1680 train_time:123333ms step_avg:88.03ms
step:1402/1680 train_time:123424ms step_avg:88.03ms
step:1403/1680 train_time:123513ms step_avg:88.04ms
step:1404/1680 train_time:123602ms step_avg:88.04ms
step:1405/1680 train_time:123691ms step_avg:88.04ms
step:1406/1680 train_time:123779ms step_avg:88.04ms
step:1407/1680 train_time:123867ms step_avg:88.04ms
step:1408/1680 train_time:123955ms step_avg:88.04ms
step:1409/1680 train_time:124044ms step_avg:88.04ms
step:1410/1680 train_time:124134ms step_avg:88.04ms
step:1411/1680 train_time:124224ms step_avg:88.04ms
step:1412/1680 train_time:124313ms step_avg:88.04ms
step:1413/1680 train_time:124403ms step_avg:88.04ms
step:1414/1680 train_time:124492ms step_avg:88.04ms
step:1415/1680 train_time:124581ms step_avg:88.04ms
step:1416/1680 train_time:124670ms step_avg:88.04ms
step:1417/1680 train_time:124758ms step_avg:88.04ms
step:1418/1680 train_time:124847ms step_avg:88.04ms
step:1419/1680 train_time:124935ms step_avg:88.04ms
step:1420/1680 train_time:125023ms step_avg:88.04ms
step:1421/1680 train_time:125113ms step_avg:88.05ms
step:1422/1680 train_time:125201ms step_avg:88.05ms
step:1423/1680 train_time:125290ms step_avg:88.05ms
step:1424/1680 train_time:125380ms step_avg:88.05ms
step:1425/1680 train_time:125469ms step_avg:88.05ms
step:1426/1680 train_time:125559ms step_avg:88.05ms
step:1427/1680 train_time:125648ms step_avg:88.05ms
step:1428/1680 train_time:125738ms step_avg:88.05ms
step:1429/1680 train_time:125827ms step_avg:88.05ms
step:1430/1680 train_time:125915ms step_avg:88.05ms
step:1431/1680 train_time:126004ms step_avg:88.05ms
step:1432/1680 train_time:126093ms step_avg:88.05ms
step:1433/1680 train_time:126182ms step_avg:88.05ms
step:1434/1680 train_time:126272ms step_avg:88.06ms
step:1435/1680 train_time:126361ms step_avg:88.06ms
step:1436/1680 train_time:126450ms step_avg:88.06ms
step:1437/1680 train_time:126539ms step_avg:88.06ms
step:1438/1680 train_time:126627ms step_avg:88.06ms
step:1439/1680 train_time:126718ms step_avg:88.06ms
step:1440/1680 train_time:126807ms step_avg:88.06ms
step:1441/1680 train_time:126897ms step_avg:88.06ms
step:1442/1680 train_time:126986ms step_avg:88.06ms
step:1443/1680 train_time:127076ms step_avg:88.06ms
step:1444/1680 train_time:127165ms step_avg:88.06ms
step:1445/1680 train_time:127254ms step_avg:88.07ms
step:1446/1680 train_time:127343ms step_avg:88.07ms
step:1447/1680 train_time:127431ms step_avg:88.07ms
step:1448/1680 train_time:127521ms step_avg:88.07ms
step:1449/1680 train_time:127610ms step_avg:88.07ms
step:1450/1680 train_time:127699ms step_avg:88.07ms
step:1451/1680 train_time:127788ms step_avg:88.07ms
step:1452/1680 train_time:127877ms step_avg:88.07ms
step:1453/1680 train_time:127966ms step_avg:88.07ms
step:1454/1680 train_time:128056ms step_avg:88.07ms
step:1455/1680 train_time:128144ms step_avg:88.07ms
step:1456/1680 train_time:128233ms step_avg:88.07ms
step:1457/1680 train_time:128322ms step_avg:88.07ms
step:1458/1680 train_time:128411ms step_avg:88.07ms
step:1459/1680 train_time:128499ms step_avg:88.07ms
step:1460/1680 train_time:128588ms step_avg:88.07ms
step:1461/1680 train_time:128678ms step_avg:88.08ms
step:1462/1680 train_time:128767ms step_avg:88.08ms
step:1463/1680 train_time:128856ms step_avg:88.08ms
step:1464/1680 train_time:128945ms step_avg:88.08ms
step:1465/1680 train_time:129034ms step_avg:88.08ms
step:1466/1680 train_time:129122ms step_avg:88.08ms
step:1467/1680 train_time:129213ms step_avg:88.08ms
step:1468/1680 train_time:129301ms step_avg:88.08ms
step:1469/1680 train_time:129391ms step_avg:88.08ms
step:1470/1680 train_time:129479ms step_avg:88.08ms
step:1471/1680 train_time:129567ms step_avg:88.08ms
step:1472/1680 train_time:129657ms step_avg:88.08ms
step:1473/1680 train_time:129746ms step_avg:88.08ms
step:1474/1680 train_time:129835ms step_avg:88.08ms
step:1475/1680 train_time:129924ms step_avg:88.08ms
step:1476/1680 train_time:130014ms step_avg:88.09ms
step:1477/1680 train_time:130103ms step_avg:88.09ms
step:1478/1680 train_time:130192ms step_avg:88.09ms
step:1479/1680 train_time:130281ms step_avg:88.09ms
step:1480/1680 train_time:130371ms step_avg:88.09ms
step:1481/1680 train_time:130460ms step_avg:88.09ms
step:1482/1680 train_time:130548ms step_avg:88.09ms
step:1483/1680 train_time:130638ms step_avg:88.09ms
step:1484/1680 train_time:130727ms step_avg:88.09ms
step:1485/1680 train_time:130816ms step_avg:88.09ms
step:1486/1680 train_time:130905ms step_avg:88.09ms
step:1487/1680 train_time:130993ms step_avg:88.09ms
step:1488/1680 train_time:131082ms step_avg:88.09ms
step:1489/1680 train_time:131172ms step_avg:88.09ms
step:1490/1680 train_time:131260ms step_avg:88.09ms
step:1491/1680 train_time:131349ms step_avg:88.09ms
step:1492/1680 train_time:131438ms step_avg:88.10ms
step:1493/1680 train_time:131527ms step_avg:88.10ms
step:1494/1680 train_time:131615ms step_avg:88.10ms
step:1495/1680 train_time:131703ms step_avg:88.10ms
step:1496/1680 train_time:131792ms step_avg:88.10ms
step:1497/1680 train_time:131881ms step_avg:88.10ms
step:1498/1680 train_time:131970ms step_avg:88.10ms
step:1499/1680 train_time:132059ms step_avg:88.10ms
step:1500/1680 train_time:132148ms step_avg:88.10ms
step:1500/1680 val_loss:3.3131 train_time:132238ms step_avg:88.16ms
step:1501/1680 train_time:132258ms step_avg:88.11ms
step:1502/1680 train_time:132331ms step_avg:88.10ms
step:1503/1680 train_time:132426ms step_avg:88.11ms
step:1504/1680 train_time:132516ms step_avg:88.11ms
step:1505/1680 train_time:132604ms step_avg:88.11ms
step:1506/1680 train_time:132692ms step_avg:88.11ms
step:1507/1680 train_time:132780ms step_avg:88.11ms
step:1508/1680 train_time:132867ms step_avg:88.11ms
step:1509/1680 train_time:132955ms step_avg:88.11ms
step:1510/1680 train_time:133043ms step_avg:88.11ms
step:1511/1680 train_time:133131ms step_avg:88.11ms
step:1512/1680 train_time:133221ms step_avg:88.11ms
step:1513/1680 train_time:133312ms step_avg:88.11ms
step:1514/1680 train_time:133404ms step_avg:88.11ms
step:1515/1680 train_time:133494ms step_avg:88.12ms
step:1516/1680 train_time:133583ms step_avg:88.12ms
step:1517/1680 train_time:133672ms step_avg:88.12ms
step:1518/1680 train_time:133760ms step_avg:88.12ms
step:1519/1680 train_time:133848ms step_avg:88.12ms
step:1520/1680 train_time:133936ms step_avg:88.12ms
step:1521/1680 train_time:134024ms step_avg:88.12ms
step:1522/1680 train_time:134112ms step_avg:88.12ms
step:1523/1680 train_time:134200ms step_avg:88.12ms
step:1524/1680 train_time:134290ms step_avg:88.12ms
step:1525/1680 train_time:134380ms step_avg:88.12ms
step:1526/1680 train_time:134471ms step_avg:88.12ms
step:1527/1680 train_time:134561ms step_avg:88.12ms
step:1528/1680 train_time:134650ms step_avg:88.12ms
step:1529/1680 train_time:134738ms step_avg:88.12ms
step:1530/1680 train_time:134827ms step_avg:88.12ms
step:1531/1680 train_time:134915ms step_avg:88.12ms
step:1532/1680 train_time:135003ms step_avg:88.12ms
step:1533/1680 train_time:135092ms step_avg:88.12ms
step:1534/1680 train_time:135180ms step_avg:88.12ms
step:1535/1680 train_time:135269ms step_avg:88.12ms
step:1536/1680 train_time:135358ms step_avg:88.12ms
step:1537/1680 train_time:135448ms step_avg:88.12ms
step:1538/1680 train_time:135537ms step_avg:88.13ms
step:1539/1680 train_time:135627ms step_avg:88.13ms
step:1540/1680 train_time:135715ms step_avg:88.13ms
step:1541/1680 train_time:135804ms step_avg:88.13ms
step:1542/1680 train_time:135892ms step_avg:88.13ms
step:1543/1680 train_time:135980ms step_avg:88.13ms
step:1544/1680 train_time:136069ms step_avg:88.13ms
step:1545/1680 train_time:136157ms step_avg:88.13ms
step:1546/1680 train_time:136246ms step_avg:88.13ms
step:1547/1680 train_time:136335ms step_avg:88.13ms
step:1548/1680 train_time:136424ms step_avg:88.13ms
step:1549/1680 train_time:136513ms step_avg:88.13ms
step:1550/1680 train_time:136602ms step_avg:88.13ms
step:1551/1680 train_time:136692ms step_avg:88.13ms
step:1552/1680 train_time:136780ms step_avg:88.13ms
step:1553/1680 train_time:136869ms step_avg:88.13ms
step:1554/1680 train_time:136957ms step_avg:88.13ms
step:1555/1680 train_time:137047ms step_avg:88.13ms
step:1556/1680 train_time:137136ms step_avg:88.13ms
step:1557/1680 train_time:137225ms step_avg:88.13ms
step:1558/1680 train_time:137314ms step_avg:88.13ms
step:1559/1680 train_time:137404ms step_avg:88.14ms
step:1560/1680 train_time:137493ms step_avg:88.14ms
step:1561/1680 train_time:137583ms step_avg:88.14ms
step:1562/1680 train_time:137671ms step_avg:88.14ms
step:1563/1680 train_time:137761ms step_avg:88.14ms
step:1564/1680 train_time:137850ms step_avg:88.14ms
step:1565/1680 train_time:137939ms step_avg:88.14ms
step:1566/1680 train_time:138029ms step_avg:88.14ms
step:1567/1680 train_time:138119ms step_avg:88.14ms
step:1568/1680 train_time:138207ms step_avg:88.14ms
step:1569/1680 train_time:138295ms step_avg:88.14ms
step:1570/1680 train_time:138384ms step_avg:88.14ms
step:1571/1680 train_time:138473ms step_avg:88.14ms
step:1572/1680 train_time:138562ms step_avg:88.14ms
step:1573/1680 train_time:138651ms step_avg:88.14ms
step:1574/1680 train_time:138740ms step_avg:88.14ms
step:1575/1680 train_time:138829ms step_avg:88.15ms
step:1576/1680 train_time:138918ms step_avg:88.15ms
step:1577/1680 train_time:139008ms step_avg:88.15ms
step:1578/1680 train_time:139099ms step_avg:88.15ms
step:1579/1680 train_time:139187ms step_avg:88.15ms
step:1580/1680 train_time:139275ms step_avg:88.15ms
step:1581/1680 train_time:139364ms step_avg:88.15ms
step:1582/1680 train_time:139453ms step_avg:88.15ms
step:1583/1680 train_time:139542ms step_avg:88.15ms
step:1584/1680 train_time:139631ms step_avg:88.15ms
step:1585/1680 train_time:139720ms step_avg:88.15ms
step:1586/1680 train_time:139809ms step_avg:88.15ms
step:1587/1680 train_time:139897ms step_avg:88.15ms
step:1588/1680 train_time:139987ms step_avg:88.15ms
step:1589/1680 train_time:140075ms step_avg:88.15ms
step:1590/1680 train_time:140164ms step_avg:88.15ms
step:1591/1680 train_time:140253ms step_avg:88.15ms
step:1592/1680 train_time:140342ms step_avg:88.15ms
step:1593/1680 train_time:140431ms step_avg:88.15ms
step:1594/1680 train_time:140519ms step_avg:88.16ms
step:1595/1680 train_time:140608ms step_avg:88.16ms
step:1596/1680 train_time:140697ms step_avg:88.16ms
step:1597/1680 train_time:140787ms step_avg:88.16ms
step:1598/1680 train_time:140876ms step_avg:88.16ms
step:1599/1680 train_time:140965ms step_avg:88.16ms
step:1600/1680 train_time:141054ms step_avg:88.16ms
step:1601/1680 train_time:141143ms step_avg:88.16ms
step:1602/1680 train_time:141232ms step_avg:88.16ms
step:1603/1680 train_time:141320ms step_avg:88.16ms
step:1604/1680 train_time:141409ms step_avg:88.16ms
step:1605/1680 train_time:141497ms step_avg:88.16ms
step:1606/1680 train_time:141586ms step_avg:88.16ms
step:1607/1680 train_time:141675ms step_avg:88.16ms
step:1608/1680 train_time:141764ms step_avg:88.16ms
step:1609/1680 train_time:141853ms step_avg:88.16ms
step:1610/1680 train_time:141942ms step_avg:88.16ms
step:1611/1680 train_time:142033ms step_avg:88.16ms
step:1612/1680 train_time:142122ms step_avg:88.16ms
step:1613/1680 train_time:142212ms step_avg:88.17ms
step:1614/1680 train_time:142301ms step_avg:88.17ms
step:1615/1680 train_time:142389ms step_avg:88.17ms
step:1616/1680 train_time:142478ms step_avg:88.17ms
step:1617/1680 train_time:142566ms step_avg:88.17ms
step:1618/1680 train_time:142655ms step_avg:88.17ms
step:1619/1680 train_time:142745ms step_avg:88.17ms
step:1620/1680 train_time:142833ms step_avg:88.17ms
step:1621/1680 train_time:142923ms step_avg:88.17ms
step:1622/1680 train_time:143013ms step_avg:88.17ms
step:1623/1680 train_time:143102ms step_avg:88.17ms
step:1624/1680 train_time:143191ms step_avg:88.17ms
step:1625/1680 train_time:143280ms step_avg:88.17ms
step:1625/1680 val_loss:3.2896 train_time:143370ms step_avg:88.23ms
step:1626/1680 train_time:143388ms step_avg:88.18ms
step:1627/1680 train_time:143463ms step_avg:88.18ms
step:1628/1680 train_time:143557ms step_avg:88.18ms
step:1629/1680 train_time:143646ms step_avg:88.18ms
step:1630/1680 train_time:143734ms step_avg:88.18ms
step:1631/1680 train_time:143823ms step_avg:88.18ms
step:1632/1680 train_time:143911ms step_avg:88.18ms
step:1633/1680 train_time:143998ms step_avg:88.18ms
step:1634/1680 train_time:144087ms step_avg:88.18ms
step:1635/1680 train_time:144175ms step_avg:88.18ms
step:1636/1680 train_time:144263ms step_avg:88.18ms
step:1637/1680 train_time:144353ms step_avg:88.18ms
step:1638/1680 train_time:144446ms step_avg:88.18ms
step:1639/1680 train_time:144538ms step_avg:88.19ms
step:1640/1680 train_time:144628ms step_avg:88.19ms
step:1641/1680 train_time:144716ms step_avg:88.19ms
step:1642/1680 train_time:144804ms step_avg:88.19ms
step:1643/1680 train_time:144894ms step_avg:88.19ms
step:1644/1680 train_time:144982ms step_avg:88.19ms
step:1645/1680 train_time:145070ms step_avg:88.19ms
step:1646/1680 train_time:145159ms step_avg:88.19ms
step:1647/1680 train_time:145247ms step_avg:88.19ms
step:1648/1680 train_time:145336ms step_avg:88.19ms
step:1649/1680 train_time:145427ms step_avg:88.19ms
step:1650/1680 train_time:145517ms step_avg:88.19ms
step:1651/1680 train_time:145607ms step_avg:88.19ms
step:1652/1680 train_time:145696ms step_avg:88.19ms
step:1653/1680 train_time:145785ms step_avg:88.19ms
step:1654/1680 train_time:145874ms step_avg:88.19ms
step:1655/1680 train_time:145962ms step_avg:88.19ms
step:1656/1680 train_time:146050ms step_avg:88.19ms
step:1657/1680 train_time:146138ms step_avg:88.19ms
step:1658/1680 train_time:146226ms step_avg:88.19ms
step:1659/1680 train_time:146315ms step_avg:88.19ms
step:1660/1680 train_time:146405ms step_avg:88.20ms
step:1661/1680 train_time:146495ms step_avg:88.20ms
step:1662/1680 train_time:146585ms step_avg:88.20ms
step:1663/1680 train_time:146674ms step_avg:88.20ms
step:1664/1680 train_time:146764ms step_avg:88.20ms
step:1665/1680 train_time:146853ms step_avg:88.20ms
step:1666/1680 train_time:146942ms step_avg:88.20ms
step:1667/1680 train_time:147030ms step_avg:88.20ms
step:1668/1680 train_time:147119ms step_avg:88.20ms
step:1669/1680 train_time:147207ms step_avg:88.20ms
step:1670/1680 train_time:147295ms step_avg:88.20ms
step:1671/1680 train_time:147384ms step_avg:88.20ms
step:1672/1680 train_time:147474ms step_avg:88.20ms
step:1673/1680 train_time:147563ms step_avg:88.20ms
step:1674/1680 train_time:147653ms step_avg:88.20ms
step:1675/1680 train_time:147742ms step_avg:88.20ms
step:1676/1680 train_time:147831ms step_avg:88.20ms
step:1677/1680 train_time:147920ms step_avg:88.21ms
step:1678/1680 train_time:148009ms step_avg:88.21ms
step:1679/1680 train_time:148098ms step_avg:88.21ms
step:1680/1680 train_time:148187ms step_avg:88.21ms
step:1680/1680 val_loss:3.2786 train_time:148277ms step_avg:88.26ms
peak memory allocated: 30760 MiB reserved: 46234 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 12:54:48 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            114W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    165264      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    165265      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    165266      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    165267      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    165268      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    165269      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    165270      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    165271      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    165265      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    165266      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    165267      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    165268      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    165269      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    165270      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    165271      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:145ms step_avg:144.94ms
step:2/1680 train_time:165ms step_avg:82.25ms
step:3/1680 train_time:229ms step_avg:76.32ms
step:4/1680 train_time:314ms step_avg:78.42ms
step:5/1680 train_time:399ms step_avg:79.87ms
step:6/1680 train_time:485ms step_avg:80.89ms
step:7/1680 train_time:571ms step_avg:81.64ms
step:8/1680 train_time:658ms step_avg:82.20ms
step:9/1680 train_time:744ms step_avg:82.64ms
step:10/1680 train_time:830ms step_avg:83.01ms
step:11/1680 train_time:916ms step_avg:83.30ms
step:12/1680 train_time:1004ms step_avg:83.68ms
step:13/1680 train_time:1094ms step_avg:84.19ms
step:14/1680 train_time:1185ms step_avg:84.62ms
step:15/1680 train_time:1273ms step_avg:84.87ms
step:16/1680 train_time:1361ms step_avg:85.03ms
step:17/1680 train_time:1448ms step_avg:85.15ms
step:18/1680 train_time:1534ms step_avg:85.22ms
step:19/1680 train_time:1621ms step_avg:85.30ms
step:20/1680 train_time:1707ms step_avg:85.36ms
step:21/1680 train_time:1794ms step_avg:85.41ms
step:22/1680 train_time:1880ms step_avg:85.46ms
step:23/1680 train_time:1967ms step_avg:85.53ms
step:24/1680 train_time:2056ms step_avg:85.66ms
step:25/1680 train_time:2145ms step_avg:85.80ms
step:26/1680 train_time:2233ms step_avg:85.90ms
step:27/1680 train_time:2321ms step_avg:85.97ms
step:28/1680 train_time:2409ms step_avg:86.02ms
step:29/1680 train_time:2495ms step_avg:86.05ms
step:30/1680 train_time:2582ms step_avg:86.07ms
step:31/1680 train_time:2668ms step_avg:86.08ms
step:32/1680 train_time:2755ms step_avg:86.09ms
step:33/1680 train_time:2842ms step_avg:86.12ms
step:34/1680 train_time:2929ms step_avg:86.15ms
step:35/1680 train_time:3017ms step_avg:86.21ms
step:36/1680 train_time:3106ms step_avg:86.28ms
step:37/1680 train_time:3194ms step_avg:86.32ms
step:38/1680 train_time:3281ms step_avg:86.35ms
step:39/1680 train_time:3369ms step_avg:86.38ms
step:40/1680 train_time:3456ms step_avg:86.40ms
step:41/1680 train_time:3544ms step_avg:86.44ms
step:42/1680 train_time:3631ms step_avg:86.46ms
step:43/1680 train_time:3718ms step_avg:86.46ms
step:44/1680 train_time:3805ms step_avg:86.47ms
step:45/1680 train_time:3892ms step_avg:86.48ms
step:46/1680 train_time:3979ms step_avg:86.50ms
step:47/1680 train_time:4066ms step_avg:86.52ms
step:48/1680 train_time:4154ms step_avg:86.55ms
step:49/1680 train_time:4242ms step_avg:86.58ms
step:50/1680 train_time:4330ms step_avg:86.59ms
step:51/1680 train_time:4417ms step_avg:86.61ms
step:52/1680 train_time:4504ms step_avg:86.62ms
step:53/1680 train_time:4591ms step_avg:86.62ms
step:54/1680 train_time:4678ms step_avg:86.63ms
step:55/1680 train_time:4765ms step_avg:86.63ms
step:56/1680 train_time:4851ms step_avg:86.63ms
step:57/1680 train_time:4939ms step_avg:86.65ms
step:58/1680 train_time:5026ms step_avg:86.66ms
step:59/1680 train_time:5114ms step_avg:86.68ms
step:60/1680 train_time:5202ms step_avg:86.69ms
step:61/1680 train_time:5289ms step_avg:86.70ms
step:62/1680 train_time:5376ms step_avg:86.71ms
step:63/1680 train_time:5463ms step_avg:86.72ms
step:64/1680 train_time:5550ms step_avg:86.72ms
step:65/1680 train_time:5638ms step_avg:86.74ms
step:66/1680 train_time:5725ms step_avg:86.74ms
step:67/1680 train_time:5812ms step_avg:86.75ms
step:68/1680 train_time:5899ms step_avg:86.75ms
step:69/1680 train_time:5986ms step_avg:86.75ms
step:70/1680 train_time:6074ms step_avg:86.77ms
step:71/1680 train_time:6161ms step_avg:86.77ms
step:72/1680 train_time:6248ms step_avg:86.78ms
step:73/1680 train_time:6335ms step_avg:86.79ms
step:74/1680 train_time:6423ms step_avg:86.80ms
step:75/1680 train_time:6510ms step_avg:86.80ms
step:76/1680 train_time:6597ms step_avg:86.80ms
step:77/1680 train_time:6684ms step_avg:86.81ms
step:78/1680 train_time:6771ms step_avg:86.81ms
step:79/1680 train_time:6858ms step_avg:86.81ms
step:80/1680 train_time:6945ms step_avg:86.81ms
step:81/1680 train_time:7031ms step_avg:86.80ms
step:82/1680 train_time:7118ms step_avg:86.80ms
step:83/1680 train_time:7205ms step_avg:86.81ms
step:84/1680 train_time:7292ms step_avg:86.81ms
step:85/1680 train_time:7380ms step_avg:86.83ms
step:86/1680 train_time:7467ms step_avg:86.83ms
step:87/1680 train_time:7554ms step_avg:86.83ms
step:88/1680 train_time:7642ms step_avg:86.84ms
step:89/1680 train_time:7728ms step_avg:86.83ms
step:90/1680 train_time:7816ms step_avg:86.84ms
step:91/1680 train_time:7904ms step_avg:86.85ms
step:92/1680 train_time:7990ms step_avg:86.85ms
step:93/1680 train_time:8077ms step_avg:86.85ms
step:94/1680 train_time:8165ms step_avg:86.86ms
step:95/1680 train_time:8252ms step_avg:86.86ms
step:96/1680 train_time:8339ms step_avg:86.86ms
step:97/1680 train_time:8426ms step_avg:86.86ms
step:98/1680 train_time:8514ms step_avg:86.88ms
step:99/1680 train_time:8601ms step_avg:86.88ms
step:100/1680 train_time:8688ms step_avg:86.88ms
step:101/1680 train_time:8775ms step_avg:86.89ms
step:102/1680 train_time:8862ms step_avg:86.89ms
step:103/1680 train_time:8950ms step_avg:86.90ms
step:104/1680 train_time:9037ms step_avg:86.90ms
step:105/1680 train_time:9124ms step_avg:86.90ms
step:106/1680 train_time:9211ms step_avg:86.90ms
step:107/1680 train_time:9298ms step_avg:86.90ms
step:108/1680 train_time:9385ms step_avg:86.90ms
step:109/1680 train_time:9473ms step_avg:86.90ms
step:110/1680 train_time:9559ms step_avg:86.90ms
step:111/1680 train_time:9647ms step_avg:86.91ms
step:112/1680 train_time:9734ms step_avg:86.91ms
step:113/1680 train_time:9821ms step_avg:86.91ms
step:114/1680 train_time:9908ms step_avg:86.91ms
step:115/1680 train_time:9995ms step_avg:86.91ms
step:116/1680 train_time:10082ms step_avg:86.91ms
step:117/1680 train_time:10169ms step_avg:86.91ms
step:118/1680 train_time:10256ms step_avg:86.91ms
step:119/1680 train_time:10343ms step_avg:86.92ms
step:120/1680 train_time:10430ms step_avg:86.92ms
step:121/1680 train_time:10517ms step_avg:86.92ms
step:122/1680 train_time:10604ms step_avg:86.92ms
step:123/1680 train_time:10691ms step_avg:86.92ms
step:124/1680 train_time:10778ms step_avg:86.92ms
step:125/1680 train_time:10865ms step_avg:86.92ms
step:125/1680 val_loss:4.3070 train_time:10953ms step_avg:87.62ms
step:126/1680 train_time:10971ms step_avg:87.07ms
step:127/1680 train_time:11043ms step_avg:86.95ms
step:128/1680 train_time:11140ms step_avg:87.03ms
step:129/1680 train_time:11230ms step_avg:87.05ms
step:130/1680 train_time:11317ms step_avg:87.05ms
step:131/1680 train_time:11405ms step_avg:87.06ms
step:132/1680 train_time:11491ms step_avg:87.05ms
step:133/1680 train_time:11577ms step_avg:87.04ms
step:134/1680 train_time:11662ms step_avg:87.03ms
step:135/1680 train_time:11749ms step_avg:87.03ms
step:136/1680 train_time:11835ms step_avg:87.02ms
step:137/1680 train_time:11920ms step_avg:87.01ms
step:138/1680 train_time:12008ms step_avg:87.01ms
step:139/1680 train_time:12097ms step_avg:87.03ms
step:140/1680 train_time:12186ms step_avg:87.04ms
step:141/1680 train_time:12274ms step_avg:87.05ms
step:142/1680 train_time:12361ms step_avg:87.05ms
step:143/1680 train_time:12448ms step_avg:87.05ms
step:144/1680 train_time:12535ms step_avg:87.05ms
step:145/1680 train_time:12622ms step_avg:87.05ms
step:146/1680 train_time:12708ms step_avg:87.04ms
step:147/1680 train_time:12794ms step_avg:87.03ms
step:148/1680 train_time:12880ms step_avg:87.03ms
step:149/1680 train_time:12967ms step_avg:87.03ms
step:150/1680 train_time:13055ms step_avg:87.03ms
step:151/1680 train_time:13144ms step_avg:87.04ms
step:152/1680 train_time:13232ms step_avg:87.05ms
step:153/1680 train_time:13320ms step_avg:87.06ms
step:154/1680 train_time:13408ms step_avg:87.06ms
step:155/1680 train_time:13495ms step_avg:87.06ms
step:156/1680 train_time:13582ms step_avg:87.06ms
step:157/1680 train_time:13668ms step_avg:87.06ms
step:158/1680 train_time:13754ms step_avg:87.05ms
step:159/1680 train_time:13840ms step_avg:87.05ms
step:160/1680 train_time:13926ms step_avg:87.04ms
step:161/1680 train_time:14013ms step_avg:87.04ms
step:162/1680 train_time:14101ms step_avg:87.04ms
step:163/1680 train_time:14189ms step_avg:87.05ms
step:164/1680 train_time:14277ms step_avg:87.05ms
step:165/1680 train_time:14364ms step_avg:87.05ms
step:166/1680 train_time:14452ms step_avg:87.06ms
step:167/1680 train_time:14538ms step_avg:87.05ms
step:168/1680 train_time:14625ms step_avg:87.05ms
step:169/1680 train_time:14712ms step_avg:87.05ms
step:170/1680 train_time:14799ms step_avg:87.05ms
step:171/1680 train_time:14885ms step_avg:87.05ms
step:172/1680 train_time:14972ms step_avg:87.05ms
step:173/1680 train_time:15059ms step_avg:87.05ms
step:174/1680 train_time:15146ms step_avg:87.05ms
step:175/1680 train_time:15234ms step_avg:87.05ms
step:176/1680 train_time:15321ms step_avg:87.05ms
step:177/1680 train_time:15409ms step_avg:87.06ms
step:178/1680 train_time:15496ms step_avg:87.06ms
step:179/1680 train_time:15583ms step_avg:87.06ms
step:180/1680 train_time:15670ms step_avg:87.06ms
step:181/1680 train_time:15756ms step_avg:87.05ms
step:182/1680 train_time:15843ms step_avg:87.05ms
step:183/1680 train_time:15929ms step_avg:87.04ms
step:184/1680 train_time:16016ms step_avg:87.04ms
step:185/1680 train_time:16103ms step_avg:87.05ms
step:186/1680 train_time:16191ms step_avg:87.05ms
step:187/1680 train_time:16278ms step_avg:87.05ms
step:188/1680 train_time:16365ms step_avg:87.05ms
step:189/1680 train_time:16452ms step_avg:87.05ms
step:190/1680 train_time:16539ms step_avg:87.05ms
step:191/1680 train_time:16626ms step_avg:87.05ms
step:192/1680 train_time:16713ms step_avg:87.05ms
step:193/1680 train_time:16801ms step_avg:87.05ms
step:194/1680 train_time:16887ms step_avg:87.05ms
step:195/1680 train_time:16974ms step_avg:87.05ms
step:196/1680 train_time:17061ms step_avg:87.04ms
step:197/1680 train_time:17148ms step_avg:87.04ms
step:198/1680 train_time:17234ms step_avg:87.04ms
step:199/1680 train_time:17322ms step_avg:87.04ms
step:200/1680 train_time:17409ms step_avg:87.04ms
step:201/1680 train_time:17496ms step_avg:87.05ms
step:202/1680 train_time:17583ms step_avg:87.05ms
step:203/1680 train_time:17670ms step_avg:87.05ms
step:204/1680 train_time:17757ms step_avg:87.04ms
step:205/1680 train_time:17844ms step_avg:87.04ms
step:206/1680 train_time:17930ms step_avg:87.04ms
step:207/1680 train_time:18018ms step_avg:87.04ms
step:208/1680 train_time:18105ms step_avg:87.04ms
step:209/1680 train_time:18192ms step_avg:87.04ms
step:210/1680 train_time:18279ms step_avg:87.04ms
step:211/1680 train_time:18366ms step_avg:87.04ms
step:212/1680 train_time:18452ms step_avg:87.04ms
step:213/1680 train_time:18540ms step_avg:87.04ms
step:214/1680 train_time:18627ms step_avg:87.04ms
step:215/1680 train_time:18713ms step_avg:87.04ms
step:216/1680 train_time:18800ms step_avg:87.04ms
step:217/1680 train_time:18887ms step_avg:87.04ms
step:218/1680 train_time:18974ms step_avg:87.04ms
step:219/1680 train_time:19061ms step_avg:87.04ms
step:220/1680 train_time:19148ms step_avg:87.04ms
step:221/1680 train_time:19235ms step_avg:87.03ms
step:222/1680 train_time:19322ms step_avg:87.04ms
step:223/1680 train_time:19409ms step_avg:87.04ms
step:224/1680 train_time:19496ms step_avg:87.03ms
step:225/1680 train_time:19583ms step_avg:87.03ms
step:226/1680 train_time:19669ms step_avg:87.03ms
step:227/1680 train_time:19756ms step_avg:87.03ms
step:228/1680 train_time:19842ms step_avg:87.03ms
step:229/1680 train_time:19929ms step_avg:87.03ms
step:230/1680 train_time:20017ms step_avg:87.03ms
step:231/1680 train_time:20104ms step_avg:87.03ms
step:232/1680 train_time:20191ms step_avg:87.03ms
step:233/1680 train_time:20277ms step_avg:87.03ms
step:234/1680 train_time:20365ms step_avg:87.03ms
step:235/1680 train_time:20451ms step_avg:87.03ms
step:236/1680 train_time:20538ms step_avg:87.02ms
step:237/1680 train_time:20625ms step_avg:87.02ms
step:238/1680 train_time:20712ms step_avg:87.03ms
step:239/1680 train_time:20799ms step_avg:87.02ms
step:240/1680 train_time:20885ms step_avg:87.02ms
step:241/1680 train_time:20972ms step_avg:87.02ms
step:242/1680 train_time:21059ms step_avg:87.02ms
step:243/1680 train_time:21146ms step_avg:87.02ms
step:244/1680 train_time:21233ms step_avg:87.02ms
step:245/1680 train_time:21320ms step_avg:87.02ms
step:246/1680 train_time:21407ms step_avg:87.02ms
step:247/1680 train_time:21494ms step_avg:87.02ms
step:248/1680 train_time:21582ms step_avg:87.02ms
step:249/1680 train_time:21668ms step_avg:87.02ms
step:250/1680 train_time:21755ms step_avg:87.02ms
step:250/1680 val_loss:3.9735 train_time:21843ms step_avg:87.37ms
step:251/1680 train_time:21861ms step_avg:87.09ms
step:252/1680 train_time:21933ms step_avg:87.04ms
step:253/1680 train_time:22021ms step_avg:87.04ms
step:254/1680 train_time:22109ms step_avg:87.04ms
step:255/1680 train_time:22196ms step_avg:87.04ms
step:256/1680 train_time:22283ms step_avg:87.04ms
step:257/1680 train_time:22369ms step_avg:87.04ms
step:258/1680 train_time:22455ms step_avg:87.04ms
step:259/1680 train_time:22541ms step_avg:87.03ms
step:260/1680 train_time:22627ms step_avg:87.03ms
step:261/1680 train_time:22714ms step_avg:87.03ms
step:262/1680 train_time:22801ms step_avg:87.03ms
step:263/1680 train_time:22890ms step_avg:87.03ms
step:264/1680 train_time:22978ms step_avg:87.04ms
step:265/1680 train_time:23066ms step_avg:87.04ms
step:266/1680 train_time:23153ms step_avg:87.04ms
step:267/1680 train_time:23240ms step_avg:87.04ms
step:268/1680 train_time:23326ms step_avg:87.04ms
step:269/1680 train_time:23413ms step_avg:87.04ms
step:270/1680 train_time:23500ms step_avg:87.04ms
step:271/1680 train_time:23586ms step_avg:87.03ms
step:272/1680 train_time:23673ms step_avg:87.03ms
step:273/1680 train_time:23759ms step_avg:87.03ms
step:274/1680 train_time:23847ms step_avg:87.03ms
step:275/1680 train_time:23935ms step_avg:87.04ms
step:276/1680 train_time:24022ms step_avg:87.04ms
step:277/1680 train_time:24110ms step_avg:87.04ms
step:278/1680 train_time:24197ms step_avg:87.04ms
step:279/1680 train_time:24284ms step_avg:87.04ms
step:280/1680 train_time:24371ms step_avg:87.04ms
step:281/1680 train_time:24457ms step_avg:87.04ms
step:282/1680 train_time:24544ms step_avg:87.04ms
step:283/1680 train_time:24630ms step_avg:87.03ms
step:284/1680 train_time:24717ms step_avg:87.03ms
step:285/1680 train_time:24804ms step_avg:87.03ms
step:286/1680 train_time:24891ms step_avg:87.03ms
step:287/1680 train_time:24978ms step_avg:87.03ms
step:288/1680 train_time:25066ms step_avg:87.03ms
step:289/1680 train_time:25152ms step_avg:87.03ms
step:290/1680 train_time:25240ms step_avg:87.03ms
step:291/1680 train_time:25327ms step_avg:87.03ms
step:292/1680 train_time:25413ms step_avg:87.03ms
step:293/1680 train_time:25500ms step_avg:87.03ms
step:294/1680 train_time:25586ms step_avg:87.03ms
step:295/1680 train_time:25673ms step_avg:87.03ms
step:296/1680 train_time:25761ms step_avg:87.03ms
step:297/1680 train_time:25848ms step_avg:87.03ms
step:298/1680 train_time:25935ms step_avg:87.03ms
step:299/1680 train_time:26024ms step_avg:87.04ms
step:300/1680 train_time:26110ms step_avg:87.03ms
step:301/1680 train_time:26197ms step_avg:87.03ms
step:302/1680 train_time:26284ms step_avg:87.03ms
step:303/1680 train_time:26371ms step_avg:87.03ms
step:304/1680 train_time:26458ms step_avg:87.03ms
step:305/1680 train_time:26545ms step_avg:87.03ms
step:306/1680 train_time:26632ms step_avg:87.03ms
step:307/1680 train_time:26718ms step_avg:87.03ms
step:308/1680 train_time:26805ms step_avg:87.03ms
step:309/1680 train_time:26892ms step_avg:87.03ms
step:310/1680 train_time:26980ms step_avg:87.03ms
step:311/1680 train_time:27068ms step_avg:87.03ms
step:312/1680 train_time:27154ms step_avg:87.03ms
step:313/1680 train_time:27241ms step_avg:87.03ms
step:314/1680 train_time:27328ms step_avg:87.03ms
step:315/1680 train_time:27415ms step_avg:87.03ms
step:316/1680 train_time:27502ms step_avg:87.03ms
step:317/1680 train_time:27588ms step_avg:87.03ms
step:318/1680 train_time:27675ms step_avg:87.03ms
step:319/1680 train_time:27762ms step_avg:87.03ms
step:320/1680 train_time:27849ms step_avg:87.03ms
step:321/1680 train_time:27936ms step_avg:87.03ms
step:322/1680 train_time:28024ms step_avg:87.03ms
step:323/1680 train_time:28111ms step_avg:87.03ms
step:324/1680 train_time:28199ms step_avg:87.03ms
step:325/1680 train_time:28286ms step_avg:87.03ms
step:326/1680 train_time:28372ms step_avg:87.03ms
step:327/1680 train_time:28459ms step_avg:87.03ms
step:328/1680 train_time:28546ms step_avg:87.03ms
step:329/1680 train_time:28632ms step_avg:87.03ms
step:330/1680 train_time:28718ms step_avg:87.03ms
step:331/1680 train_time:28805ms step_avg:87.03ms
step:332/1680 train_time:28892ms step_avg:87.02ms
step:333/1680 train_time:28979ms step_avg:87.02ms
step:334/1680 train_time:29066ms step_avg:87.03ms
step:335/1680 train_time:29153ms step_avg:87.02ms
step:336/1680 train_time:29240ms step_avg:87.02ms
step:337/1680 train_time:29327ms step_avg:87.02ms
step:338/1680 train_time:29414ms step_avg:87.02ms
step:339/1680 train_time:29502ms step_avg:87.03ms
step:340/1680 train_time:29588ms step_avg:87.02ms
step:341/1680 train_time:29675ms step_avg:87.02ms
step:342/1680 train_time:29762ms step_avg:87.02ms
step:343/1680 train_time:29849ms step_avg:87.02ms
step:344/1680 train_time:29937ms step_avg:87.02ms
step:345/1680 train_time:30024ms step_avg:87.03ms
step:346/1680 train_time:30111ms step_avg:87.03ms
step:347/1680 train_time:30198ms step_avg:87.03ms
step:348/1680 train_time:30285ms step_avg:87.03ms
step:349/1680 train_time:30372ms step_avg:87.03ms
step:350/1680 train_time:30459ms step_avg:87.03ms
step:351/1680 train_time:30546ms step_avg:87.03ms
step:352/1680 train_time:30633ms step_avg:87.03ms
step:353/1680 train_time:30720ms step_avg:87.03ms
step:354/1680 train_time:30807ms step_avg:87.02ms
step:355/1680 train_time:30894ms step_avg:87.02ms
step:356/1680 train_time:30981ms step_avg:87.03ms
step:357/1680 train_time:31068ms step_avg:87.03ms
step:358/1680 train_time:31155ms step_avg:87.02ms
step:359/1680 train_time:31242ms step_avg:87.02ms
step:360/1680 train_time:31329ms step_avg:87.02ms
step:361/1680 train_time:31416ms step_avg:87.02ms
step:362/1680 train_time:31503ms step_avg:87.02ms
step:363/1680 train_time:31589ms step_avg:87.02ms
step:364/1680 train_time:31677ms step_avg:87.02ms
step:365/1680 train_time:31764ms step_avg:87.02ms
step:366/1680 train_time:31851ms step_avg:87.03ms
step:367/1680 train_time:31938ms step_avg:87.02ms
step:368/1680 train_time:32025ms step_avg:87.02ms
step:369/1680 train_time:32112ms step_avg:87.02ms
step:370/1680 train_time:32199ms step_avg:87.03ms
step:371/1680 train_time:32286ms step_avg:87.02ms
step:372/1680 train_time:32373ms step_avg:87.02ms
step:373/1680 train_time:32460ms step_avg:87.02ms
step:374/1680 train_time:32547ms step_avg:87.02ms
step:375/1680 train_time:32635ms step_avg:87.03ms
step:375/1680 val_loss:3.8216 train_time:32723ms step_avg:87.26ms
step:376/1680 train_time:32743ms step_avg:87.08ms
step:377/1680 train_time:32814ms step_avg:87.04ms
step:378/1680 train_time:32904ms step_avg:87.05ms
step:379/1680 train_time:32992ms step_avg:87.05ms
step:380/1680 train_time:33079ms step_avg:87.05ms
step:381/1680 train_time:33165ms step_avg:87.05ms
step:382/1680 train_time:33251ms step_avg:87.04ms
step:383/1680 train_time:33337ms step_avg:87.04ms
step:384/1680 train_time:33423ms step_avg:87.04ms
step:385/1680 train_time:33509ms step_avg:87.04ms
step:386/1680 train_time:33595ms step_avg:87.03ms
step:387/1680 train_time:33684ms step_avg:87.04ms
step:388/1680 train_time:33772ms step_avg:87.04ms
step:389/1680 train_time:33861ms step_avg:87.05ms
step:390/1680 train_time:33950ms step_avg:87.05ms
step:391/1680 train_time:34037ms step_avg:87.05ms
step:392/1680 train_time:34124ms step_avg:87.05ms
step:393/1680 train_time:34210ms step_avg:87.05ms
step:394/1680 train_time:34296ms step_avg:87.05ms
step:395/1680 train_time:34383ms step_avg:87.04ms
step:396/1680 train_time:34469ms step_avg:87.04ms
step:397/1680 train_time:34556ms step_avg:87.04ms
step:398/1680 train_time:34642ms step_avg:87.04ms
step:399/1680 train_time:34729ms step_avg:87.04ms
step:400/1680 train_time:34817ms step_avg:87.04ms
step:401/1680 train_time:34906ms step_avg:87.05ms
step:402/1680 train_time:34993ms step_avg:87.05ms
step:403/1680 train_time:35080ms step_avg:87.05ms
step:404/1680 train_time:35167ms step_avg:87.05ms
step:405/1680 train_time:35254ms step_avg:87.05ms
step:406/1680 train_time:35340ms step_avg:87.04ms
step:407/1680 train_time:35426ms step_avg:87.04ms
step:408/1680 train_time:35513ms step_avg:87.04ms
step:409/1680 train_time:35599ms step_avg:87.04ms
step:410/1680 train_time:35687ms step_avg:87.04ms
step:411/1680 train_time:35773ms step_avg:87.04ms
step:412/1680 train_time:35861ms step_avg:87.04ms
step:413/1680 train_time:35948ms step_avg:87.04ms
step:414/1680 train_time:36035ms step_avg:87.04ms
step:415/1680 train_time:36122ms step_avg:87.04ms
step:416/1680 train_time:36209ms step_avg:87.04ms
step:417/1680 train_time:36296ms step_avg:87.04ms
step:418/1680 train_time:36383ms step_avg:87.04ms
step:419/1680 train_time:36470ms step_avg:87.04ms
step:420/1680 train_time:36556ms step_avg:87.04ms
step:421/1680 train_time:36643ms step_avg:87.04ms
step:422/1680 train_time:36730ms step_avg:87.04ms
step:423/1680 train_time:36817ms step_avg:87.04ms
step:424/1680 train_time:36904ms step_avg:87.04ms
step:425/1680 train_time:36991ms step_avg:87.04ms
step:426/1680 train_time:37078ms step_avg:87.04ms
step:427/1680 train_time:37165ms step_avg:87.04ms
step:428/1680 train_time:37252ms step_avg:87.04ms
step:429/1680 train_time:37339ms step_avg:87.04ms
step:430/1680 train_time:37425ms step_avg:87.04ms
step:431/1680 train_time:37512ms step_avg:87.04ms
step:432/1680 train_time:37600ms step_avg:87.04ms
step:433/1680 train_time:37687ms step_avg:87.04ms
step:434/1680 train_time:37773ms step_avg:87.03ms
step:435/1680 train_time:37860ms step_avg:87.04ms
step:436/1680 train_time:37948ms step_avg:87.04ms
step:437/1680 train_time:38035ms step_avg:87.04ms
step:438/1680 train_time:38122ms step_avg:87.04ms
step:439/1680 train_time:38209ms step_avg:87.04ms
step:440/1680 train_time:38296ms step_avg:87.04ms
step:441/1680 train_time:38383ms step_avg:87.04ms
step:442/1680 train_time:38470ms step_avg:87.04ms
step:443/1680 train_time:38557ms step_avg:87.04ms
step:444/1680 train_time:38644ms step_avg:87.04ms
step:445/1680 train_time:38731ms step_avg:87.04ms
step:446/1680 train_time:38818ms step_avg:87.04ms
step:447/1680 train_time:38905ms step_avg:87.04ms
step:448/1680 train_time:38992ms step_avg:87.04ms
step:449/1680 train_time:39080ms step_avg:87.04ms
step:450/1680 train_time:39167ms step_avg:87.04ms
step:451/1680 train_time:39253ms step_avg:87.04ms
step:452/1680 train_time:39340ms step_avg:87.04ms
step:453/1680 train_time:39427ms step_avg:87.04ms
step:454/1680 train_time:39514ms step_avg:87.04ms
step:455/1680 train_time:39601ms step_avg:87.03ms
step:456/1680 train_time:39688ms step_avg:87.04ms
step:457/1680 train_time:39776ms step_avg:87.04ms
step:458/1680 train_time:39863ms step_avg:87.04ms
step:459/1680 train_time:39950ms step_avg:87.04ms
step:460/1680 train_time:40037ms step_avg:87.04ms
step:461/1680 train_time:40125ms step_avg:87.04ms
step:462/1680 train_time:40212ms step_avg:87.04ms
step:463/1680 train_time:40299ms step_avg:87.04ms
step:464/1680 train_time:40386ms step_avg:87.04ms
step:465/1680 train_time:40473ms step_avg:87.04ms
step:466/1680 train_time:40559ms step_avg:87.04ms
step:467/1680 train_time:40647ms step_avg:87.04ms
step:468/1680 train_time:40734ms step_avg:87.04ms
step:469/1680 train_time:40821ms step_avg:87.04ms
step:470/1680 train_time:40908ms step_avg:87.04ms
step:471/1680 train_time:40995ms step_avg:87.04ms
step:472/1680 train_time:41082ms step_avg:87.04ms
step:473/1680 train_time:41169ms step_avg:87.04ms
step:474/1680 train_time:41256ms step_avg:87.04ms
step:475/1680 train_time:41343ms step_avg:87.04ms
step:476/1680 train_time:41429ms step_avg:87.04ms
step:477/1680 train_time:41516ms step_avg:87.04ms
step:478/1680 train_time:41603ms step_avg:87.03ms
step:479/1680 train_time:41689ms step_avg:87.03ms
step:480/1680 train_time:41776ms step_avg:87.03ms
step:481/1680 train_time:41863ms step_avg:87.03ms
step:482/1680 train_time:41950ms step_avg:87.03ms
step:483/1680 train_time:42037ms step_avg:87.03ms
step:484/1680 train_time:42124ms step_avg:87.03ms
step:485/1680 train_time:42211ms step_avg:87.03ms
step:486/1680 train_time:42298ms step_avg:87.03ms
step:487/1680 train_time:42385ms step_avg:87.03ms
step:488/1680 train_time:42473ms step_avg:87.03ms
step:489/1680 train_time:42559ms step_avg:87.03ms
step:490/1680 train_time:42646ms step_avg:87.03ms
step:491/1680 train_time:42733ms step_avg:87.03ms
step:492/1680 train_time:42820ms step_avg:87.03ms
step:493/1680 train_time:42907ms step_avg:87.03ms
step:494/1680 train_time:42993ms step_avg:87.03ms
step:495/1680 train_time:43081ms step_avg:87.03ms
step:496/1680 train_time:43168ms step_avg:87.03ms
step:497/1680 train_time:43255ms step_avg:87.03ms
step:498/1680 train_time:43342ms step_avg:87.03ms
step:499/1680 train_time:43430ms step_avg:87.03ms
step:500/1680 train_time:43517ms step_avg:87.03ms
step:500/1680 val_loss:3.7176 train_time:43605ms step_avg:87.21ms
step:501/1680 train_time:43623ms step_avg:87.07ms
step:502/1680 train_time:43696ms step_avg:87.04ms
step:503/1680 train_time:43788ms step_avg:87.05ms
step:504/1680 train_time:43879ms step_avg:87.06ms
step:505/1680 train_time:43967ms step_avg:87.06ms
step:506/1680 train_time:44053ms step_avg:87.06ms
step:507/1680 train_time:44139ms step_avg:87.06ms
step:508/1680 train_time:44225ms step_avg:87.06ms
step:509/1680 train_time:44311ms step_avg:87.05ms
step:510/1680 train_time:44397ms step_avg:87.05ms
step:511/1680 train_time:44483ms step_avg:87.05ms
step:512/1680 train_time:44569ms step_avg:87.05ms
step:513/1680 train_time:44657ms step_avg:87.05ms
step:514/1680 train_time:44746ms step_avg:87.05ms
step:515/1680 train_time:44834ms step_avg:87.06ms
step:516/1680 train_time:44922ms step_avg:87.06ms
step:517/1680 train_time:45009ms step_avg:87.06ms
step:518/1680 train_time:45095ms step_avg:87.06ms
step:519/1680 train_time:45181ms step_avg:87.05ms
step:520/1680 train_time:45267ms step_avg:87.05ms
step:521/1680 train_time:45354ms step_avg:87.05ms
step:522/1680 train_time:45440ms step_avg:87.05ms
step:523/1680 train_time:45526ms step_avg:87.05ms
step:524/1680 train_time:45612ms step_avg:87.05ms
step:525/1680 train_time:45700ms step_avg:87.05ms
step:526/1680 train_time:45788ms step_avg:87.05ms
step:527/1680 train_time:45875ms step_avg:87.05ms
step:528/1680 train_time:45963ms step_avg:87.05ms
step:529/1680 train_time:46050ms step_avg:87.05ms
step:530/1680 train_time:46137ms step_avg:87.05ms
step:531/1680 train_time:46224ms step_avg:87.05ms
step:532/1680 train_time:46310ms step_avg:87.05ms
step:533/1680 train_time:46397ms step_avg:87.05ms
step:534/1680 train_time:46484ms step_avg:87.05ms
step:535/1680 train_time:46571ms step_avg:87.05ms
step:536/1680 train_time:46658ms step_avg:87.05ms
step:537/1680 train_time:46745ms step_avg:87.05ms
step:538/1680 train_time:46833ms step_avg:87.05ms
step:539/1680 train_time:46920ms step_avg:87.05ms
step:540/1680 train_time:47007ms step_avg:87.05ms
step:541/1680 train_time:47095ms step_avg:87.05ms
step:542/1680 train_time:47181ms step_avg:87.05ms
step:543/1680 train_time:47268ms step_avg:87.05ms
step:544/1680 train_time:47355ms step_avg:87.05ms
step:545/1680 train_time:47442ms step_avg:87.05ms
step:546/1680 train_time:47529ms step_avg:87.05ms
step:547/1680 train_time:47616ms step_avg:87.05ms
step:548/1680 train_time:47703ms step_avg:87.05ms
step:549/1680 train_time:47793ms step_avg:87.05ms
step:550/1680 train_time:47882ms step_avg:87.06ms
step:551/1680 train_time:47970ms step_avg:87.06ms
step:552/1680 train_time:48058ms step_avg:87.06ms
step:553/1680 train_time:48146ms step_avg:87.06ms
step:554/1680 train_time:48234ms step_avg:87.06ms
step:555/1680 train_time:48322ms step_avg:87.07ms
step:556/1680 train_time:48410ms step_avg:87.07ms
step:557/1680 train_time:48498ms step_avg:87.07ms
step:558/1680 train_time:48586ms step_avg:87.07ms
step:559/1680 train_time:48674ms step_avg:87.07ms
step:560/1680 train_time:48763ms step_avg:87.08ms
step:561/1680 train_time:48852ms step_avg:87.08ms
step:562/1680 train_time:48940ms step_avg:87.08ms
step:563/1680 train_time:49028ms step_avg:87.08ms
step:564/1680 train_time:49116ms step_avg:87.09ms
step:565/1680 train_time:49204ms step_avg:87.09ms
step:566/1680 train_time:49292ms step_avg:87.09ms
step:567/1680 train_time:49379ms step_avg:87.09ms
step:568/1680 train_time:49467ms step_avg:87.09ms
step:569/1680 train_time:49555ms step_avg:87.09ms
step:570/1680 train_time:49643ms step_avg:87.09ms
step:571/1680 train_time:49731ms step_avg:87.10ms
step:572/1680 train_time:49819ms step_avg:87.10ms
step:573/1680 train_time:49908ms step_avg:87.10ms
step:574/1680 train_time:49996ms step_avg:87.10ms
step:575/1680 train_time:50084ms step_avg:87.10ms
step:576/1680 train_time:50172ms step_avg:87.10ms
step:577/1680 train_time:50261ms step_avg:87.11ms
step:578/1680 train_time:50348ms step_avg:87.11ms
step:579/1680 train_time:50436ms step_avg:87.11ms
step:580/1680 train_time:50524ms step_avg:87.11ms
step:581/1680 train_time:50611ms step_avg:87.11ms
step:582/1680 train_time:50699ms step_avg:87.11ms
step:583/1680 train_time:50789ms step_avg:87.12ms
step:584/1680 train_time:50877ms step_avg:87.12ms
step:585/1680 train_time:50965ms step_avg:87.12ms
step:586/1680 train_time:51054ms step_avg:87.12ms
step:587/1680 train_time:51143ms step_avg:87.13ms
step:588/1680 train_time:51230ms step_avg:87.13ms
step:589/1680 train_time:51318ms step_avg:87.13ms
step:590/1680 train_time:51406ms step_avg:87.13ms
step:591/1680 train_time:51494ms step_avg:87.13ms
step:592/1680 train_time:51582ms step_avg:87.13ms
step:593/1680 train_time:51670ms step_avg:87.13ms
step:594/1680 train_time:51758ms step_avg:87.14ms
step:595/1680 train_time:51846ms step_avg:87.14ms
step:596/1680 train_time:51935ms step_avg:87.14ms
step:597/1680 train_time:52024ms step_avg:87.14ms
step:598/1680 train_time:52112ms step_avg:87.14ms
step:599/1680 train_time:52200ms step_avg:87.15ms
step:600/1680 train_time:52289ms step_avg:87.15ms
step:601/1680 train_time:52377ms step_avg:87.15ms
step:602/1680 train_time:52465ms step_avg:87.15ms
step:603/1680 train_time:52553ms step_avg:87.15ms
step:604/1680 train_time:52640ms step_avg:87.15ms
step:605/1680 train_time:52728ms step_avg:87.15ms
step:606/1680 train_time:52816ms step_avg:87.15ms
step:607/1680 train_time:52904ms step_avg:87.16ms
step:608/1680 train_time:52992ms step_avg:87.16ms
step:609/1680 train_time:53080ms step_avg:87.16ms
step:610/1680 train_time:53169ms step_avg:87.16ms
step:611/1680 train_time:53257ms step_avg:87.16ms
step:612/1680 train_time:53345ms step_avg:87.17ms
step:613/1680 train_time:53433ms step_avg:87.17ms
step:614/1680 train_time:53522ms step_avg:87.17ms
step:615/1680 train_time:53610ms step_avg:87.17ms
step:616/1680 train_time:53697ms step_avg:87.17ms
step:617/1680 train_time:53785ms step_avg:87.17ms
step:618/1680 train_time:53874ms step_avg:87.17ms
step:619/1680 train_time:53962ms step_avg:87.18ms
step:620/1680 train_time:54050ms step_avg:87.18ms
step:621/1680 train_time:54138ms step_avg:87.18ms
step:622/1680 train_time:54226ms step_avg:87.18ms
step:623/1680 train_time:54314ms step_avg:87.18ms
step:624/1680 train_time:54402ms step_avg:87.18ms
step:625/1680 train_time:54490ms step_avg:87.18ms
step:625/1680 val_loss:3.6166 train_time:54580ms step_avg:87.33ms
step:626/1680 train_time:54599ms step_avg:87.22ms
step:627/1680 train_time:54669ms step_avg:87.19ms
step:628/1680 train_time:54757ms step_avg:87.19ms
step:629/1680 train_time:54848ms step_avg:87.20ms
step:630/1680 train_time:54935ms step_avg:87.20ms
step:631/1680 train_time:55022ms step_avg:87.20ms
step:632/1680 train_time:55109ms step_avg:87.20ms
step:633/1680 train_time:55197ms step_avg:87.20ms
step:634/1680 train_time:55284ms step_avg:87.20ms
step:635/1680 train_time:55371ms step_avg:87.20ms
step:636/1680 train_time:55459ms step_avg:87.20ms
step:637/1680 train_time:55550ms step_avg:87.21ms
step:638/1680 train_time:55640ms step_avg:87.21ms
step:639/1680 train_time:55728ms step_avg:87.21ms
step:640/1680 train_time:55817ms step_avg:87.21ms
step:641/1680 train_time:55905ms step_avg:87.21ms
step:642/1680 train_time:55992ms step_avg:87.22ms
step:643/1680 train_time:56080ms step_avg:87.22ms
step:644/1680 train_time:56167ms step_avg:87.22ms
step:645/1680 train_time:56255ms step_avg:87.22ms
step:646/1680 train_time:56342ms step_avg:87.22ms
step:647/1680 train_time:56431ms step_avg:87.22ms
step:648/1680 train_time:56520ms step_avg:87.22ms
step:649/1680 train_time:56609ms step_avg:87.23ms
step:650/1680 train_time:56698ms step_avg:87.23ms
step:651/1680 train_time:56786ms step_avg:87.23ms
step:652/1680 train_time:56875ms step_avg:87.23ms
step:653/1680 train_time:56963ms step_avg:87.23ms
step:654/1680 train_time:57050ms step_avg:87.23ms
step:655/1680 train_time:57138ms step_avg:87.23ms
step:656/1680 train_time:57226ms step_avg:87.23ms
step:657/1680 train_time:57313ms step_avg:87.23ms
step:658/1680 train_time:57401ms step_avg:87.24ms
step:659/1680 train_time:57489ms step_avg:87.24ms
step:660/1680 train_time:57577ms step_avg:87.24ms
step:661/1680 train_time:57666ms step_avg:87.24ms
step:662/1680 train_time:57755ms step_avg:87.24ms
step:663/1680 train_time:57843ms step_avg:87.24ms
step:664/1680 train_time:57931ms step_avg:87.25ms
step:665/1680 train_time:58019ms step_avg:87.25ms
step:666/1680 train_time:58107ms step_avg:87.25ms
step:667/1680 train_time:58194ms step_avg:87.25ms
step:668/1680 train_time:58283ms step_avg:87.25ms
step:669/1680 train_time:58370ms step_avg:87.25ms
step:670/1680 train_time:58459ms step_avg:87.25ms
step:671/1680 train_time:58547ms step_avg:87.25ms
step:672/1680 train_time:58635ms step_avg:87.25ms
step:673/1680 train_time:58723ms step_avg:87.26ms
step:674/1680 train_time:58811ms step_avg:87.26ms
step:675/1680 train_time:58899ms step_avg:87.26ms
step:676/1680 train_time:58989ms step_avg:87.26ms
step:677/1680 train_time:59077ms step_avg:87.26ms
step:678/1680 train_time:59165ms step_avg:87.26ms
step:679/1680 train_time:59254ms step_avg:87.27ms
step:680/1680 train_time:59341ms step_avg:87.27ms
step:681/1680 train_time:59429ms step_avg:87.27ms
step:682/1680 train_time:59517ms step_avg:87.27ms
step:683/1680 train_time:59605ms step_avg:87.27ms
step:684/1680 train_time:59693ms step_avg:87.27ms
step:685/1680 train_time:59781ms step_avg:87.27ms
step:686/1680 train_time:59869ms step_avg:87.27ms
step:687/1680 train_time:59957ms step_avg:87.27ms
step:688/1680 train_time:60045ms step_avg:87.27ms
step:689/1680 train_time:60133ms step_avg:87.28ms
step:690/1680 train_time:60221ms step_avg:87.28ms
step:691/1680 train_time:60310ms step_avg:87.28ms
step:692/1680 train_time:60398ms step_avg:87.28ms
step:693/1680 train_time:60486ms step_avg:87.28ms
step:694/1680 train_time:60574ms step_avg:87.28ms
step:695/1680 train_time:60663ms step_avg:87.28ms
step:696/1680 train_time:60751ms step_avg:87.29ms
step:697/1680 train_time:60839ms step_avg:87.29ms
step:698/1680 train_time:60928ms step_avg:87.29ms
step:699/1680 train_time:61015ms step_avg:87.29ms
step:700/1680 train_time:61104ms step_avg:87.29ms
step:701/1680 train_time:61192ms step_avg:87.29ms
step:702/1680 train_time:61280ms step_avg:87.29ms
step:703/1680 train_time:61368ms step_avg:87.29ms
step:704/1680 train_time:61456ms step_avg:87.30ms
step:705/1680 train_time:61544ms step_avg:87.30ms
step:706/1680 train_time:61632ms step_avg:87.30ms
step:707/1680 train_time:61720ms step_avg:87.30ms
step:708/1680 train_time:61808ms step_avg:87.30ms
step:709/1680 train_time:61896ms step_avg:87.30ms
step:710/1680 train_time:61983ms step_avg:87.30ms
step:711/1680 train_time:62072ms step_avg:87.30ms
step:712/1680 train_time:62159ms step_avg:87.30ms
step:713/1680 train_time:62248ms step_avg:87.30ms
step:714/1680 train_time:62336ms step_avg:87.31ms
step:715/1680 train_time:62424ms step_avg:87.31ms
step:716/1680 train_time:62512ms step_avg:87.31ms
step:717/1680 train_time:62601ms step_avg:87.31ms
step:718/1680 train_time:62689ms step_avg:87.31ms
step:719/1680 train_time:62777ms step_avg:87.31ms
step:720/1680 train_time:62865ms step_avg:87.31ms
step:721/1680 train_time:62953ms step_avg:87.31ms
step:722/1680 train_time:63042ms step_avg:87.32ms
step:723/1680 train_time:63129ms step_avg:87.32ms
step:724/1680 train_time:63217ms step_avg:87.32ms
step:725/1680 train_time:63305ms step_avg:87.32ms
step:726/1680 train_time:63393ms step_avg:87.32ms
step:727/1680 train_time:63482ms step_avg:87.32ms
step:728/1680 train_time:63571ms step_avg:87.32ms
step:729/1680 train_time:63659ms step_avg:87.32ms
step:730/1680 train_time:63747ms step_avg:87.32ms
step:731/1680 train_time:63835ms step_avg:87.32ms
step:732/1680 train_time:63922ms step_avg:87.33ms
step:733/1680 train_time:64011ms step_avg:87.33ms
step:734/1680 train_time:64098ms step_avg:87.33ms
step:735/1680 train_time:64186ms step_avg:87.33ms
step:736/1680 train_time:64274ms step_avg:87.33ms
step:737/1680 train_time:64362ms step_avg:87.33ms
step:738/1680 train_time:64450ms step_avg:87.33ms
step:739/1680 train_time:64539ms step_avg:87.33ms
step:740/1680 train_time:64626ms step_avg:87.33ms
step:741/1680 train_time:64714ms step_avg:87.33ms
step:742/1680 train_time:64802ms step_avg:87.33ms
step:743/1680 train_time:64891ms step_avg:87.34ms
step:744/1680 train_time:64979ms step_avg:87.34ms
step:745/1680 train_time:65067ms step_avg:87.34ms
step:746/1680 train_time:65155ms step_avg:87.34ms
step:747/1680 train_time:65243ms step_avg:87.34ms
step:748/1680 train_time:65331ms step_avg:87.34ms
step:749/1680 train_time:65419ms step_avg:87.34ms
step:750/1680 train_time:65508ms step_avg:87.34ms
step:750/1680 val_loss:3.5658 train_time:65597ms step_avg:87.46ms
step:751/1680 train_time:65615ms step_avg:87.37ms
step:752/1680 train_time:65688ms step_avg:87.35ms
step:753/1680 train_time:65780ms step_avg:87.36ms
step:754/1680 train_time:65870ms step_avg:87.36ms
step:755/1680 train_time:65958ms step_avg:87.36ms
step:756/1680 train_time:66045ms step_avg:87.36ms
step:757/1680 train_time:66133ms step_avg:87.36ms
step:758/1680 train_time:66221ms step_avg:87.36ms
step:759/1680 train_time:66308ms step_avg:87.36ms
step:760/1680 train_time:66395ms step_avg:87.36ms
step:761/1680 train_time:66482ms step_avg:87.36ms
step:762/1680 train_time:66570ms step_avg:87.36ms
step:763/1680 train_time:66659ms step_avg:87.36ms
step:764/1680 train_time:66748ms step_avg:87.37ms
step:765/1680 train_time:66838ms step_avg:87.37ms
step:766/1680 train_time:66928ms step_avg:87.37ms
step:767/1680 train_time:67016ms step_avg:87.37ms
step:768/1680 train_time:67103ms step_avg:87.37ms
step:769/1680 train_time:67191ms step_avg:87.37ms
step:770/1680 train_time:67278ms step_avg:87.37ms
step:771/1680 train_time:67365ms step_avg:87.37ms
step:772/1680 train_time:67453ms step_avg:87.37ms
step:773/1680 train_time:67541ms step_avg:87.37ms
step:774/1680 train_time:67629ms step_avg:87.38ms
step:775/1680 train_time:67718ms step_avg:87.38ms
step:776/1680 train_time:67807ms step_avg:87.38ms
step:777/1680 train_time:67896ms step_avg:87.38ms
step:778/1680 train_time:67984ms step_avg:87.38ms
step:779/1680 train_time:68073ms step_avg:87.38ms
step:780/1680 train_time:68161ms step_avg:87.39ms
step:781/1680 train_time:68248ms step_avg:87.39ms
step:782/1680 train_time:68335ms step_avg:87.39ms
step:783/1680 train_time:68423ms step_avg:87.39ms
step:784/1680 train_time:68510ms step_avg:87.39ms
step:785/1680 train_time:68599ms step_avg:87.39ms
step:786/1680 train_time:68687ms step_avg:87.39ms
step:787/1680 train_time:68775ms step_avg:87.39ms
step:788/1680 train_time:68864ms step_avg:87.39ms
step:789/1680 train_time:68954ms step_avg:87.39ms
step:790/1680 train_time:69042ms step_avg:87.40ms
step:791/1680 train_time:69130ms step_avg:87.40ms
step:792/1680 train_time:69218ms step_avg:87.40ms
step:793/1680 train_time:69306ms step_avg:87.40ms
step:794/1680 train_time:69395ms step_avg:87.40ms
step:795/1680 train_time:69483ms step_avg:87.40ms
step:796/1680 train_time:69571ms step_avg:87.40ms
step:797/1680 train_time:69659ms step_avg:87.40ms
step:798/1680 train_time:69748ms step_avg:87.40ms
step:799/1680 train_time:69837ms step_avg:87.41ms
step:800/1680 train_time:69925ms step_avg:87.41ms
step:801/1680 train_time:70014ms step_avg:87.41ms
step:802/1680 train_time:70101ms step_avg:87.41ms
step:803/1680 train_time:70189ms step_avg:87.41ms
step:804/1680 train_time:70276ms step_avg:87.41ms
step:805/1680 train_time:70364ms step_avg:87.41ms
step:806/1680 train_time:70453ms step_avg:87.41ms
step:807/1680 train_time:70541ms step_avg:87.41ms
step:808/1680 train_time:70628ms step_avg:87.41ms
step:809/1680 train_time:70716ms step_avg:87.41ms
step:810/1680 train_time:70804ms step_avg:87.41ms
step:811/1680 train_time:70892ms step_avg:87.41ms
step:812/1680 train_time:70981ms step_avg:87.42ms
step:813/1680 train_time:71069ms step_avg:87.42ms
step:814/1680 train_time:71157ms step_avg:87.42ms
step:815/1680 train_time:71245ms step_avg:87.42ms
step:816/1680 train_time:71333ms step_avg:87.42ms
step:817/1680 train_time:71422ms step_avg:87.42ms
step:818/1680 train_time:71511ms step_avg:87.42ms
step:819/1680 train_time:71599ms step_avg:87.42ms
step:820/1680 train_time:71687ms step_avg:87.42ms
step:821/1680 train_time:71775ms step_avg:87.42ms
step:822/1680 train_time:71863ms step_avg:87.42ms
step:823/1680 train_time:71952ms step_avg:87.43ms
step:824/1680 train_time:72041ms step_avg:87.43ms
step:825/1680 train_time:72130ms step_avg:87.43ms
step:826/1680 train_time:72217ms step_avg:87.43ms
step:827/1680 train_time:72305ms step_avg:87.43ms
step:828/1680 train_time:72393ms step_avg:87.43ms
step:829/1680 train_time:72481ms step_avg:87.43ms
step:830/1680 train_time:72569ms step_avg:87.43ms
step:831/1680 train_time:72657ms step_avg:87.43ms
step:832/1680 train_time:72745ms step_avg:87.43ms
step:833/1680 train_time:72833ms step_avg:87.43ms
step:834/1680 train_time:72922ms step_avg:87.44ms
step:835/1680 train_time:73009ms step_avg:87.44ms
step:836/1680 train_time:73097ms step_avg:87.44ms
step:837/1680 train_time:73186ms step_avg:87.44ms
step:838/1680 train_time:73274ms step_avg:87.44ms
step:839/1680 train_time:73362ms step_avg:87.44ms
step:840/1680 train_time:73450ms step_avg:87.44ms
step:841/1680 train_time:73538ms step_avg:87.44ms
step:842/1680 train_time:73626ms step_avg:87.44ms
step:843/1680 train_time:73715ms step_avg:87.44ms
step:844/1680 train_time:73803ms step_avg:87.44ms
step:845/1680 train_time:73891ms step_avg:87.44ms
step:846/1680 train_time:73979ms step_avg:87.45ms
step:847/1680 train_time:74068ms step_avg:87.45ms
step:848/1680 train_time:74156ms step_avg:87.45ms
step:849/1680 train_time:74244ms step_avg:87.45ms
step:850/1680 train_time:74332ms step_avg:87.45ms
step:851/1680 train_time:74420ms step_avg:87.45ms
step:852/1680 train_time:74508ms step_avg:87.45ms
step:853/1680 train_time:74596ms step_avg:87.45ms
step:854/1680 train_time:74685ms step_avg:87.45ms
step:855/1680 train_time:74773ms step_avg:87.45ms
step:856/1680 train_time:74861ms step_avg:87.45ms
step:857/1680 train_time:74949ms step_avg:87.46ms
step:858/1680 train_time:75037ms step_avg:87.46ms
step:859/1680 train_time:75126ms step_avg:87.46ms
step:860/1680 train_time:75215ms step_avg:87.46ms
step:861/1680 train_time:75303ms step_avg:87.46ms
step:862/1680 train_time:75391ms step_avg:87.46ms
step:863/1680 train_time:75479ms step_avg:87.46ms
step:864/1680 train_time:75567ms step_avg:87.46ms
step:865/1680 train_time:75655ms step_avg:87.46ms
step:866/1680 train_time:75743ms step_avg:87.46ms
step:867/1680 train_time:75832ms step_avg:87.46ms
step:868/1680 train_time:75919ms step_avg:87.46ms
step:869/1680 train_time:76007ms step_avg:87.47ms
step:870/1680 train_time:76095ms step_avg:87.47ms
step:871/1680 train_time:76184ms step_avg:87.47ms
step:872/1680 train_time:76272ms step_avg:87.47ms
step:873/1680 train_time:76360ms step_avg:87.47ms
step:874/1680 train_time:76448ms step_avg:87.47ms
step:875/1680 train_time:76536ms step_avg:87.47ms
step:875/1680 val_loss:3.5187 train_time:76626ms step_avg:87.57ms
step:876/1680 train_time:76646ms step_avg:87.50ms
step:877/1680 train_time:76719ms step_avg:87.48ms
step:878/1680 train_time:76813ms step_avg:87.49ms
step:879/1680 train_time:76903ms step_avg:87.49ms
step:880/1680 train_time:76991ms step_avg:87.49ms
step:881/1680 train_time:77078ms step_avg:87.49ms
step:882/1680 train_time:77165ms step_avg:87.49ms
step:883/1680 train_time:77252ms step_avg:87.49ms
step:884/1680 train_time:77339ms step_avg:87.49ms
step:885/1680 train_time:77426ms step_avg:87.49ms
step:886/1680 train_time:77514ms step_avg:87.49ms
step:887/1680 train_time:77603ms step_avg:87.49ms
step:888/1680 train_time:77692ms step_avg:87.49ms
step:889/1680 train_time:77782ms step_avg:87.49ms
step:890/1680 train_time:77872ms step_avg:87.50ms
step:891/1680 train_time:77961ms step_avg:87.50ms
step:892/1680 train_time:78050ms step_avg:87.50ms
step:893/1680 train_time:78137ms step_avg:87.50ms
step:894/1680 train_time:78224ms step_avg:87.50ms
step:895/1680 train_time:78312ms step_avg:87.50ms
step:896/1680 train_time:78399ms step_avg:87.50ms
step:897/1680 train_time:78487ms step_avg:87.50ms
step:898/1680 train_time:78575ms step_avg:87.50ms
step:899/1680 train_time:78664ms step_avg:87.50ms
step:900/1680 train_time:78754ms step_avg:87.50ms
step:901/1680 train_time:78842ms step_avg:87.51ms
step:902/1680 train_time:78931ms step_avg:87.51ms
step:903/1680 train_time:79019ms step_avg:87.51ms
step:904/1680 train_time:79108ms step_avg:87.51ms
step:905/1680 train_time:79196ms step_avg:87.51ms
step:906/1680 train_time:79283ms step_avg:87.51ms
step:907/1680 train_time:79371ms step_avg:87.51ms
step:908/1680 train_time:79459ms step_avg:87.51ms
step:909/1680 train_time:79546ms step_avg:87.51ms
step:910/1680 train_time:79634ms step_avg:87.51ms
step:911/1680 train_time:79724ms step_avg:87.51ms
step:912/1680 train_time:79812ms step_avg:87.51ms
step:913/1680 train_time:79902ms step_avg:87.52ms
step:914/1680 train_time:79990ms step_avg:87.52ms
step:915/1680 train_time:80078ms step_avg:87.52ms
step:916/1680 train_time:80166ms step_avg:87.52ms
step:917/1680 train_time:80255ms step_avg:87.52ms
step:918/1680 train_time:80342ms step_avg:87.52ms
step:919/1680 train_time:80430ms step_avg:87.52ms
step:920/1680 train_time:80518ms step_avg:87.52ms
step:921/1680 train_time:80605ms step_avg:87.52ms
step:922/1680 train_time:80694ms step_avg:87.52ms
step:923/1680 train_time:80783ms step_avg:87.52ms
step:924/1680 train_time:80872ms step_avg:87.52ms
step:925/1680 train_time:80961ms step_avg:87.53ms
step:926/1680 train_time:81049ms step_avg:87.53ms
step:927/1680 train_time:81136ms step_avg:87.53ms
step:928/1680 train_time:81224ms step_avg:87.53ms
step:929/1680 train_time:81312ms step_avg:87.53ms
step:930/1680 train_time:81401ms step_avg:87.53ms
step:931/1680 train_time:81489ms step_avg:87.53ms
step:932/1680 train_time:81576ms step_avg:87.53ms
step:933/1680 train_time:81664ms step_avg:87.53ms
step:934/1680 train_time:81752ms step_avg:87.53ms
step:935/1680 train_time:81840ms step_avg:87.53ms
step:936/1680 train_time:81928ms step_avg:87.53ms
step:937/1680 train_time:82017ms step_avg:87.53ms
step:938/1680 train_time:82105ms step_avg:87.53ms
step:939/1680 train_time:82193ms step_avg:87.53ms
step:940/1680 train_time:82282ms step_avg:87.53ms
step:941/1680 train_time:82369ms step_avg:87.53ms
step:942/1680 train_time:82457ms step_avg:87.53ms
step:943/1680 train_time:82545ms step_avg:87.53ms
step:944/1680 train_time:82633ms step_avg:87.53ms
step:945/1680 train_time:82721ms step_avg:87.54ms
step:946/1680 train_time:82809ms step_avg:87.54ms
step:947/1680 train_time:82898ms step_avg:87.54ms
step:948/1680 train_time:82986ms step_avg:87.54ms
step:949/1680 train_time:83075ms step_avg:87.54ms
step:950/1680 train_time:83163ms step_avg:87.54ms
step:951/1680 train_time:83251ms step_avg:87.54ms
step:952/1680 train_time:83339ms step_avg:87.54ms
step:953/1680 train_time:83426ms step_avg:87.54ms
step:954/1680 train_time:83514ms step_avg:87.54ms
step:955/1680 train_time:83601ms step_avg:87.54ms
step:956/1680 train_time:83690ms step_avg:87.54ms
step:957/1680 train_time:83778ms step_avg:87.54ms
step:958/1680 train_time:83866ms step_avg:87.54ms
step:959/1680 train_time:83954ms step_avg:87.54ms
step:960/1680 train_time:84043ms step_avg:87.54ms
step:961/1680 train_time:84131ms step_avg:87.55ms
step:962/1680 train_time:84220ms step_avg:87.55ms
step:963/1680 train_time:84307ms step_avg:87.55ms
step:964/1680 train_time:84396ms step_avg:87.55ms
step:965/1680 train_time:84484ms step_avg:87.55ms
step:966/1680 train_time:84573ms step_avg:87.55ms
step:967/1680 train_time:84662ms step_avg:87.55ms
step:968/1680 train_time:84748ms step_avg:87.55ms
step:969/1680 train_time:84837ms step_avg:87.55ms
step:970/1680 train_time:84924ms step_avg:87.55ms
step:971/1680 train_time:85013ms step_avg:87.55ms
step:972/1680 train_time:85101ms step_avg:87.55ms
step:973/1680 train_time:85189ms step_avg:87.55ms
step:974/1680 train_time:85277ms step_avg:87.55ms
step:975/1680 train_time:85366ms step_avg:87.55ms
step:976/1680 train_time:85454ms step_avg:87.55ms
step:977/1680 train_time:85542ms step_avg:87.56ms
step:978/1680 train_time:85630ms step_avg:87.56ms
step:979/1680 train_time:85718ms step_avg:87.56ms
step:980/1680 train_time:85807ms step_avg:87.56ms
step:981/1680 train_time:85895ms step_avg:87.56ms
step:982/1680 train_time:85983ms step_avg:87.56ms
step:983/1680 train_time:86071ms step_avg:87.56ms
step:984/1680 train_time:86160ms step_avg:87.56ms
step:985/1680 train_time:86247ms step_avg:87.56ms
step:986/1680 train_time:86335ms step_avg:87.56ms
step:987/1680 train_time:86423ms step_avg:87.56ms
step:988/1680 train_time:86512ms step_avg:87.56ms
step:989/1680 train_time:86600ms step_avg:87.56ms
step:990/1680 train_time:86688ms step_avg:87.56ms
step:991/1680 train_time:86776ms step_avg:87.56ms
step:992/1680 train_time:86865ms step_avg:87.57ms
step:993/1680 train_time:86953ms step_avg:87.57ms
step:994/1680 train_time:87041ms step_avg:87.57ms
step:995/1680 train_time:87129ms step_avg:87.57ms
step:996/1680 train_time:87217ms step_avg:87.57ms
step:997/1680 train_time:87305ms step_avg:87.57ms
step:998/1680 train_time:87393ms step_avg:87.57ms
step:999/1680 train_time:87482ms step_avg:87.57ms
step:1000/1680 train_time:87570ms step_avg:87.57ms
step:1000/1680 val_loss:3.4694 train_time:87660ms step_avg:87.66ms
step:1001/1680 train_time:87678ms step_avg:87.59ms
step:1002/1680 train_time:87753ms step_avg:87.58ms
step:1003/1680 train_time:87846ms step_avg:87.58ms
step:1004/1680 train_time:87935ms step_avg:87.59ms
step:1005/1680 train_time:88023ms step_avg:87.58ms
step:1006/1680 train_time:88110ms step_avg:87.58ms
step:1007/1680 train_time:88197ms step_avg:87.58ms
step:1008/1680 train_time:88285ms step_avg:87.58ms
step:1009/1680 train_time:88372ms step_avg:87.58ms
step:1010/1680 train_time:88459ms step_avg:87.58ms
step:1011/1680 train_time:88546ms step_avg:87.58ms
step:1012/1680 train_time:88635ms step_avg:87.58ms
step:1013/1680 train_time:88725ms step_avg:87.59ms
step:1014/1680 train_time:88815ms step_avg:87.59ms
step:1015/1680 train_time:88905ms step_avg:87.59ms
step:1016/1680 train_time:88993ms step_avg:87.59ms
step:1017/1680 train_time:89081ms step_avg:87.59ms
step:1018/1680 train_time:89168ms step_avg:87.59ms
step:1019/1680 train_time:89256ms step_avg:87.59ms
step:1020/1680 train_time:89343ms step_avg:87.59ms
step:1021/1680 train_time:89430ms step_avg:87.59ms
step:1022/1680 train_time:89518ms step_avg:87.59ms
step:1023/1680 train_time:89606ms step_avg:87.59ms
step:1024/1680 train_time:89695ms step_avg:87.59ms
step:1025/1680 train_time:89785ms step_avg:87.60ms
step:1026/1680 train_time:89874ms step_avg:87.60ms
step:1027/1680 train_time:89963ms step_avg:87.60ms
step:1028/1680 train_time:90051ms step_avg:87.60ms
step:1029/1680 train_time:90139ms step_avg:87.60ms
step:1030/1680 train_time:90226ms step_avg:87.60ms
step:1031/1680 train_time:90314ms step_avg:87.60ms
step:1032/1680 train_time:90402ms step_avg:87.60ms
step:1033/1680 train_time:90489ms step_avg:87.60ms
step:1034/1680 train_time:90577ms step_avg:87.60ms
step:1035/1680 train_time:90666ms step_avg:87.60ms
step:1036/1680 train_time:90754ms step_avg:87.60ms
step:1037/1680 train_time:90843ms step_avg:87.60ms
step:1038/1680 train_time:90931ms step_avg:87.60ms
step:1039/1680 train_time:91019ms step_avg:87.60ms
step:1040/1680 train_time:91108ms step_avg:87.60ms
step:1041/1680 train_time:91197ms step_avg:87.61ms
step:1042/1680 train_time:91284ms step_avg:87.60ms
step:1043/1680 train_time:91372ms step_avg:87.61ms
step:1044/1680 train_time:91460ms step_avg:87.61ms
step:1045/1680 train_time:91548ms step_avg:87.61ms
step:1046/1680 train_time:91636ms step_avg:87.61ms
step:1047/1680 train_time:91725ms step_avg:87.61ms
step:1048/1680 train_time:91814ms step_avg:87.61ms
step:1049/1680 train_time:91902ms step_avg:87.61ms
step:1050/1680 train_time:91990ms step_avg:87.61ms
step:1051/1680 train_time:92078ms step_avg:87.61ms
step:1052/1680 train_time:92166ms step_avg:87.61ms
step:1053/1680 train_time:92254ms step_avg:87.61ms
step:1054/1680 train_time:92342ms step_avg:87.61ms
step:1055/1680 train_time:92429ms step_avg:87.61ms
step:1056/1680 train_time:92518ms step_avg:87.61ms
step:1057/1680 train_time:92606ms step_avg:87.61ms
step:1058/1680 train_time:92694ms step_avg:87.61ms
step:1059/1680 train_time:92783ms step_avg:87.61ms
step:1060/1680 train_time:92871ms step_avg:87.61ms
step:1061/1680 train_time:92959ms step_avg:87.61ms
step:1062/1680 train_time:93047ms step_avg:87.61ms
step:1063/1680 train_time:93135ms step_avg:87.62ms
step:1064/1680 train_time:93223ms step_avg:87.62ms
step:1065/1680 train_time:93311ms step_avg:87.62ms
step:1066/1680 train_time:93398ms step_avg:87.62ms
step:1067/1680 train_time:93486ms step_avg:87.62ms
step:1068/1680 train_time:93574ms step_avg:87.62ms
step:1069/1680 train_time:93661ms step_avg:87.62ms
step:1070/1680 train_time:93750ms step_avg:87.62ms
step:1071/1680 train_time:93838ms step_avg:87.62ms
step:1072/1680 train_time:93927ms step_avg:87.62ms
step:1073/1680 train_time:94015ms step_avg:87.62ms
step:1074/1680 train_time:94103ms step_avg:87.62ms
step:1075/1680 train_time:94191ms step_avg:87.62ms
step:1076/1680 train_time:94279ms step_avg:87.62ms
step:1077/1680 train_time:94367ms step_avg:87.62ms
step:1078/1680 train_time:94455ms step_avg:87.62ms
step:1079/1680 train_time:94543ms step_avg:87.62ms
step:1080/1680 train_time:94631ms step_avg:87.62ms
step:1081/1680 train_time:94719ms step_avg:87.62ms
step:1082/1680 train_time:94808ms step_avg:87.62ms
step:1083/1680 train_time:94896ms step_avg:87.62ms
step:1084/1680 train_time:94984ms step_avg:87.62ms
step:1085/1680 train_time:95072ms step_avg:87.62ms
step:1086/1680 train_time:95160ms step_avg:87.62ms
step:1087/1680 train_time:95249ms step_avg:87.63ms
step:1088/1680 train_time:95338ms step_avg:87.63ms
step:1089/1680 train_time:95425ms step_avg:87.63ms
step:1090/1680 train_time:95513ms step_avg:87.63ms
step:1091/1680 train_time:95601ms step_avg:87.63ms
step:1092/1680 train_time:95689ms step_avg:87.63ms
step:1093/1680 train_time:95777ms step_avg:87.63ms
step:1094/1680 train_time:95865ms step_avg:87.63ms
step:1095/1680 train_time:95954ms step_avg:87.63ms
step:1096/1680 train_time:96043ms step_avg:87.63ms
step:1097/1680 train_time:96131ms step_avg:87.63ms
step:1098/1680 train_time:96220ms step_avg:87.63ms
step:1099/1680 train_time:96310ms step_avg:87.63ms
step:1100/1680 train_time:96399ms step_avg:87.64ms
step:1101/1680 train_time:96488ms step_avg:87.64ms
step:1102/1680 train_time:96577ms step_avg:87.64ms
step:1103/1680 train_time:96665ms step_avg:87.64ms
step:1104/1680 train_time:96755ms step_avg:87.64ms
step:1105/1680 train_time:96843ms step_avg:87.64ms
step:1106/1680 train_time:96933ms step_avg:87.64ms
step:1107/1680 train_time:97022ms step_avg:87.64ms
step:1108/1680 train_time:97111ms step_avg:87.65ms
step:1109/1680 train_time:97200ms step_avg:87.65ms
step:1110/1680 train_time:97288ms step_avg:87.65ms
step:1111/1680 train_time:97377ms step_avg:87.65ms
step:1112/1680 train_time:97466ms step_avg:87.65ms
step:1113/1680 train_time:97555ms step_avg:87.65ms
step:1114/1680 train_time:97643ms step_avg:87.65ms
step:1115/1680 train_time:97734ms step_avg:87.65ms
step:1116/1680 train_time:97823ms step_avg:87.66ms
step:1117/1680 train_time:97912ms step_avg:87.66ms
step:1118/1680 train_time:98002ms step_avg:87.66ms
step:1119/1680 train_time:98091ms step_avg:87.66ms
step:1120/1680 train_time:98179ms step_avg:87.66ms
step:1121/1680 train_time:98267ms step_avg:87.66ms
step:1122/1680 train_time:98356ms step_avg:87.66ms
step:1123/1680 train_time:98445ms step_avg:87.66ms
step:1124/1680 train_time:98534ms step_avg:87.66ms
step:1125/1680 train_time:98623ms step_avg:87.66ms
step:1125/1680 val_loss:3.4160 train_time:98713ms step_avg:87.75ms
step:1126/1680 train_time:98731ms step_avg:87.68ms
step:1127/1680 train_time:98803ms step_avg:87.67ms
step:1128/1680 train_time:98893ms step_avg:87.67ms
step:1129/1680 train_time:98984ms step_avg:87.67ms
step:1130/1680 train_time:99072ms step_avg:87.67ms
step:1131/1680 train_time:99160ms step_avg:87.67ms
step:1132/1680 train_time:99248ms step_avg:87.68ms
step:1133/1680 train_time:99337ms step_avg:87.68ms
step:1134/1680 train_time:99424ms step_avg:87.68ms
step:1135/1680 train_time:99512ms step_avg:87.68ms
step:1136/1680 train_time:99602ms step_avg:87.68ms
step:1137/1680 train_time:99693ms step_avg:87.68ms
step:1138/1680 train_time:99783ms step_avg:87.68ms
step:1139/1680 train_time:99874ms step_avg:87.69ms
step:1140/1680 train_time:99963ms step_avg:87.69ms
step:1141/1680 train_time:100052ms step_avg:87.69ms
step:1142/1680 train_time:100141ms step_avg:87.69ms
step:1143/1680 train_time:100230ms step_avg:87.69ms
step:1144/1680 train_time:100318ms step_avg:87.69ms
step:1145/1680 train_time:100406ms step_avg:87.69ms
step:1146/1680 train_time:100494ms step_avg:87.69ms
step:1147/1680 train_time:100584ms step_avg:87.69ms
step:1148/1680 train_time:100673ms step_avg:87.69ms
step:1149/1680 train_time:100763ms step_avg:87.70ms
step:1150/1680 train_time:100853ms step_avg:87.70ms
step:1151/1680 train_time:100942ms step_avg:87.70ms
step:1152/1680 train_time:101031ms step_avg:87.70ms
step:1153/1680 train_time:101120ms step_avg:87.70ms
step:1154/1680 train_time:101208ms step_avg:87.70ms
step:1155/1680 train_time:101297ms step_avg:87.70ms
step:1156/1680 train_time:101385ms step_avg:87.70ms
step:1157/1680 train_time:101473ms step_avg:87.70ms
step:1158/1680 train_time:101562ms step_avg:87.70ms
step:1159/1680 train_time:101651ms step_avg:87.71ms
step:1160/1680 train_time:101741ms step_avg:87.71ms
step:1161/1680 train_time:101831ms step_avg:87.71ms
step:1162/1680 train_time:101921ms step_avg:87.71ms
step:1163/1680 train_time:102010ms step_avg:87.71ms
step:1164/1680 train_time:102099ms step_avg:87.71ms
step:1165/1680 train_time:102188ms step_avg:87.71ms
step:1166/1680 train_time:102276ms step_avg:87.72ms
step:1167/1680 train_time:102365ms step_avg:87.72ms
step:1168/1680 train_time:102453ms step_avg:87.72ms
step:1169/1680 train_time:102543ms step_avg:87.72ms
step:1170/1680 train_time:102632ms step_avg:87.72ms
step:1171/1680 train_time:102721ms step_avg:87.72ms
step:1172/1680 train_time:102810ms step_avg:87.72ms
step:1173/1680 train_time:102900ms step_avg:87.72ms
step:1174/1680 train_time:102989ms step_avg:87.72ms
step:1175/1680 train_time:103078ms step_avg:87.73ms
step:1176/1680 train_time:103166ms step_avg:87.73ms
step:1177/1680 train_time:103255ms step_avg:87.73ms
step:1178/1680 train_time:103343ms step_avg:87.73ms
step:1179/1680 train_time:103431ms step_avg:87.73ms
step:1180/1680 train_time:103520ms step_avg:87.73ms
step:1181/1680 train_time:103609ms step_avg:87.73ms
step:1182/1680 train_time:103698ms step_avg:87.73ms
step:1183/1680 train_time:103787ms step_avg:87.73ms
step:1184/1680 train_time:103876ms step_avg:87.73ms
step:1185/1680 train_time:103965ms step_avg:87.73ms
step:1186/1680 train_time:104054ms step_avg:87.74ms
step:1187/1680 train_time:104143ms step_avg:87.74ms
step:1188/1680 train_time:104232ms step_avg:87.74ms
step:1189/1680 train_time:104321ms step_avg:87.74ms
step:1190/1680 train_time:104410ms step_avg:87.74ms
step:1191/1680 train_time:104499ms step_avg:87.74ms
step:1192/1680 train_time:104587ms step_avg:87.74ms
step:1193/1680 train_time:104676ms step_avg:87.74ms
step:1194/1680 train_time:104765ms step_avg:87.74ms
step:1195/1680 train_time:104854ms step_avg:87.74ms
step:1196/1680 train_time:104943ms step_avg:87.75ms
step:1197/1680 train_time:105032ms step_avg:87.75ms
step:1198/1680 train_time:105121ms step_avg:87.75ms
step:1199/1680 train_time:105209ms step_avg:87.75ms
step:1200/1680 train_time:105298ms step_avg:87.75ms
step:1201/1680 train_time:105387ms step_avg:87.75ms
step:1202/1680 train_time:105476ms step_avg:87.75ms
step:1203/1680 train_time:105565ms step_avg:87.75ms
step:1204/1680 train_time:105654ms step_avg:87.75ms
step:1205/1680 train_time:105743ms step_avg:87.75ms
step:1206/1680 train_time:105831ms step_avg:87.75ms
step:1207/1680 train_time:105921ms step_avg:87.76ms
step:1208/1680 train_time:106010ms step_avg:87.76ms
step:1209/1680 train_time:106099ms step_avg:87.76ms
step:1210/1680 train_time:106188ms step_avg:87.76ms
step:1211/1680 train_time:106277ms step_avg:87.76ms
step:1212/1680 train_time:106366ms step_avg:87.76ms
step:1213/1680 train_time:106455ms step_avg:87.76ms
step:1214/1680 train_time:106544ms step_avg:87.76ms
step:1215/1680 train_time:106633ms step_avg:87.76ms
step:1216/1680 train_time:106722ms step_avg:87.77ms
step:1217/1680 train_time:106811ms step_avg:87.77ms
step:1218/1680 train_time:106901ms step_avg:87.77ms
step:1219/1680 train_time:106990ms step_avg:87.77ms
step:1220/1680 train_time:107080ms step_avg:87.77ms
step:1221/1680 train_time:107169ms step_avg:87.77ms
step:1222/1680 train_time:107257ms step_avg:87.77ms
step:1223/1680 train_time:107347ms step_avg:87.77ms
step:1224/1680 train_time:107436ms step_avg:87.77ms
step:1225/1680 train_time:107525ms step_avg:87.78ms
step:1226/1680 train_time:107614ms step_avg:87.78ms
step:1227/1680 train_time:107703ms step_avg:87.78ms
step:1228/1680 train_time:107792ms step_avg:87.78ms
step:1229/1680 train_time:107882ms step_avg:87.78ms
step:1230/1680 train_time:107971ms step_avg:87.78ms
step:1231/1680 train_time:108061ms step_avg:87.78ms
step:1232/1680 train_time:108151ms step_avg:87.78ms
step:1233/1680 train_time:108241ms step_avg:87.79ms
step:1234/1680 train_time:108330ms step_avg:87.79ms
step:1235/1680 train_time:108419ms step_avg:87.79ms
step:1236/1680 train_time:108508ms step_avg:87.79ms
step:1237/1680 train_time:108596ms step_avg:87.79ms
step:1238/1680 train_time:108685ms step_avg:87.79ms
step:1239/1680 train_time:108773ms step_avg:87.79ms
step:1240/1680 train_time:108863ms step_avg:87.79ms
step:1241/1680 train_time:108952ms step_avg:87.79ms
step:1242/1680 train_time:109041ms step_avg:87.79ms
step:1243/1680 train_time:109130ms step_avg:87.80ms
step:1244/1680 train_time:109219ms step_avg:87.80ms
step:1245/1680 train_time:109309ms step_avg:87.80ms
step:1246/1680 train_time:109397ms step_avg:87.80ms
step:1247/1680 train_time:109486ms step_avg:87.80ms
step:1248/1680 train_time:109575ms step_avg:87.80ms
step:1249/1680 train_time:109663ms step_avg:87.80ms
step:1250/1680 train_time:109752ms step_avg:87.80ms
step:1250/1680 val_loss:3.3777 train_time:109842ms step_avg:87.87ms
step:1251/1680 train_time:109859ms step_avg:87.82ms
step:1252/1680 train_time:109936ms step_avg:87.81ms
step:1253/1680 train_time:110031ms step_avg:87.81ms
step:1254/1680 train_time:110121ms step_avg:87.82ms
step:1255/1680 train_time:110210ms step_avg:87.82ms
step:1256/1680 train_time:110297ms step_avg:87.82ms
step:1257/1680 train_time:110385ms step_avg:87.82ms
step:1258/1680 train_time:110472ms step_avg:87.82ms
step:1259/1680 train_time:110560ms step_avg:87.82ms
step:1260/1680 train_time:110648ms step_avg:87.82ms
step:1261/1680 train_time:110735ms step_avg:87.82ms
step:1262/1680 train_time:110825ms step_avg:87.82ms
step:1263/1680 train_time:110916ms step_avg:87.82ms
step:1264/1680 train_time:111007ms step_avg:87.82ms
step:1265/1680 train_time:111097ms step_avg:87.82ms
step:1266/1680 train_time:111187ms step_avg:87.83ms
step:1267/1680 train_time:111275ms step_avg:87.83ms
step:1268/1680 train_time:111363ms step_avg:87.83ms
step:1269/1680 train_time:111452ms step_avg:87.83ms
step:1270/1680 train_time:111540ms step_avg:87.83ms
step:1271/1680 train_time:111628ms step_avg:87.83ms
step:1272/1680 train_time:111716ms step_avg:87.83ms
step:1273/1680 train_time:111805ms step_avg:87.83ms
step:1274/1680 train_time:111896ms step_avg:87.83ms
step:1275/1680 train_time:111987ms step_avg:87.83ms
step:1276/1680 train_time:112078ms step_avg:87.84ms
step:1277/1680 train_time:112168ms step_avg:87.84ms
step:1278/1680 train_time:112256ms step_avg:87.84ms
step:1279/1680 train_time:112344ms step_avg:87.84ms
step:1280/1680 train_time:112433ms step_avg:87.84ms
step:1281/1680 train_time:112521ms step_avg:87.84ms
step:1282/1680 train_time:112610ms step_avg:87.84ms
step:1283/1680 train_time:112698ms step_avg:87.84ms
step:1284/1680 train_time:112787ms step_avg:87.84ms
step:1285/1680 train_time:112876ms step_avg:87.84ms
step:1286/1680 train_time:112965ms step_avg:87.84ms
step:1287/1680 train_time:113055ms step_avg:87.84ms
step:1288/1680 train_time:113145ms step_avg:87.85ms
step:1289/1680 train_time:113234ms step_avg:87.85ms
step:1290/1680 train_time:113323ms step_avg:87.85ms
step:1291/1680 train_time:113412ms step_avg:87.85ms
step:1292/1680 train_time:113500ms step_avg:87.85ms
step:1293/1680 train_time:113589ms step_avg:87.85ms
step:1294/1680 train_time:113678ms step_avg:87.85ms
step:1295/1680 train_time:113766ms step_avg:87.85ms
step:1296/1680 train_time:113854ms step_avg:87.85ms
step:1297/1680 train_time:113943ms step_avg:87.85ms
step:1298/1680 train_time:114032ms step_avg:87.85ms
step:1299/1680 train_time:114122ms step_avg:87.85ms
step:1300/1680 train_time:114211ms step_avg:87.85ms
step:1301/1680 train_time:114300ms step_avg:87.86ms
step:1302/1680 train_time:114389ms step_avg:87.86ms
step:1303/1680 train_time:114478ms step_avg:87.86ms
step:1304/1680 train_time:114567ms step_avg:87.86ms
step:1305/1680 train_time:114655ms step_avg:87.86ms
step:1306/1680 train_time:114744ms step_avg:87.86ms
step:1307/1680 train_time:114832ms step_avg:87.86ms
step:1308/1680 train_time:114922ms step_avg:87.86ms
step:1309/1680 train_time:115011ms step_avg:87.86ms
step:1310/1680 train_time:115101ms step_avg:87.86ms
step:1311/1680 train_time:115190ms step_avg:87.86ms
step:1312/1680 train_time:115279ms step_avg:87.87ms
step:1313/1680 train_time:115369ms step_avg:87.87ms
step:1314/1680 train_time:115457ms step_avg:87.87ms
step:1315/1680 train_time:115546ms step_avg:87.87ms
step:1316/1680 train_time:115635ms step_avg:87.87ms
step:1317/1680 train_time:115724ms step_avg:87.87ms
step:1318/1680 train_time:115813ms step_avg:87.87ms
step:1319/1680 train_time:115902ms step_avg:87.87ms
step:1320/1680 train_time:115992ms step_avg:87.87ms
step:1321/1680 train_time:116082ms step_avg:87.87ms
step:1322/1680 train_time:116172ms step_avg:87.88ms
step:1323/1680 train_time:116262ms step_avg:87.88ms
step:1324/1680 train_time:116351ms step_avg:87.88ms
step:1325/1680 train_time:116440ms step_avg:87.88ms
step:1326/1680 train_time:116528ms step_avg:87.88ms
step:1327/1680 train_time:116616ms step_avg:87.88ms
step:1328/1680 train_time:116705ms step_avg:87.88ms
step:1329/1680 train_time:116794ms step_avg:87.88ms
step:1330/1680 train_time:116885ms step_avg:87.88ms
step:1331/1680 train_time:116974ms step_avg:87.88ms
step:1332/1680 train_time:117063ms step_avg:87.89ms
step:1333/1680 train_time:117152ms step_avg:87.89ms
step:1334/1680 train_time:117241ms step_avg:87.89ms
step:1335/1680 train_time:117330ms step_avg:87.89ms
step:1336/1680 train_time:117419ms step_avg:87.89ms
step:1337/1680 train_time:117508ms step_avg:87.89ms
step:1338/1680 train_time:117597ms step_avg:87.89ms
step:1339/1680 train_time:117685ms step_avg:87.89ms
step:1340/1680 train_time:117774ms step_avg:87.89ms
step:1341/1680 train_time:117863ms step_avg:87.89ms
step:1342/1680 train_time:117951ms step_avg:87.89ms
step:1343/1680 train_time:118040ms step_avg:87.89ms
step:1344/1680 train_time:118129ms step_avg:87.89ms
step:1345/1680 train_time:118218ms step_avg:87.89ms
step:1346/1680 train_time:118307ms step_avg:87.90ms
step:1347/1680 train_time:118397ms step_avg:87.90ms
step:1348/1680 train_time:118485ms step_avg:87.90ms
step:1349/1680 train_time:118574ms step_avg:87.90ms
step:1350/1680 train_time:118663ms step_avg:87.90ms
step:1351/1680 train_time:118752ms step_avg:87.90ms
step:1352/1680 train_time:118842ms step_avg:87.90ms
step:1353/1680 train_time:118933ms step_avg:87.90ms
step:1354/1680 train_time:119022ms step_avg:87.90ms
step:1355/1680 train_time:119111ms step_avg:87.90ms
step:1356/1680 train_time:119199ms step_avg:87.90ms
step:1357/1680 train_time:119288ms step_avg:87.91ms
step:1358/1680 train_time:119378ms step_avg:87.91ms
step:1359/1680 train_time:119467ms step_avg:87.91ms
step:1360/1680 train_time:119556ms step_avg:87.91ms
step:1361/1680 train_time:119644ms step_avg:87.91ms
step:1362/1680 train_time:119733ms step_avg:87.91ms
step:1363/1680 train_time:119823ms step_avg:87.91ms
step:1364/1680 train_time:119911ms step_avg:87.91ms
step:1365/1680 train_time:120000ms step_avg:87.91ms
step:1366/1680 train_time:120089ms step_avg:87.91ms
step:1367/1680 train_time:120178ms step_avg:87.91ms
step:1368/1680 train_time:120267ms step_avg:87.91ms
step:1369/1680 train_time:120356ms step_avg:87.92ms
step:1370/1680 train_time:120445ms step_avg:87.92ms
step:1371/1680 train_time:120534ms step_avg:87.92ms
step:1372/1680 train_time:120623ms step_avg:87.92ms
step:1373/1680 train_time:120711ms step_avg:87.92ms
step:1374/1680 train_time:120800ms step_avg:87.92ms
step:1375/1680 train_time:120889ms step_avg:87.92ms
step:1375/1680 val_loss:3.3433 train_time:120980ms step_avg:87.99ms
step:1376/1680 train_time:120998ms step_avg:87.93ms
step:1377/1680 train_time:121071ms step_avg:87.92ms
step:1378/1680 train_time:121163ms step_avg:87.93ms
step:1379/1680 train_time:121252ms step_avg:87.93ms
step:1380/1680 train_time:121340ms step_avg:87.93ms
step:1381/1680 train_time:121428ms step_avg:87.93ms
step:1382/1680 train_time:121516ms step_avg:87.93ms
step:1383/1680 train_time:121604ms step_avg:87.93ms
step:1384/1680 train_time:121692ms step_avg:87.93ms
step:1385/1680 train_time:121781ms step_avg:87.93ms
step:1386/1680 train_time:121870ms step_avg:87.93ms
step:1387/1680 train_time:121961ms step_avg:87.93ms
step:1388/1680 train_time:122052ms step_avg:87.93ms
step:1389/1680 train_time:122142ms step_avg:87.93ms
step:1390/1680 train_time:122231ms step_avg:87.94ms
step:1391/1680 train_time:122320ms step_avg:87.94ms
step:1392/1680 train_time:122409ms step_avg:87.94ms
step:1393/1680 train_time:122497ms step_avg:87.94ms
step:1394/1680 train_time:122585ms step_avg:87.94ms
step:1395/1680 train_time:122673ms step_avg:87.94ms
step:1396/1680 train_time:122762ms step_avg:87.94ms
step:1397/1680 train_time:122852ms step_avg:87.94ms
step:1398/1680 train_time:122941ms step_avg:87.94ms
step:1399/1680 train_time:123030ms step_avg:87.94ms
step:1400/1680 train_time:123120ms step_avg:87.94ms
step:1401/1680 train_time:123209ms step_avg:87.94ms
step:1402/1680 train_time:123297ms step_avg:87.94ms
step:1403/1680 train_time:123387ms step_avg:87.94ms
step:1404/1680 train_time:123475ms step_avg:87.95ms
step:1405/1680 train_time:123564ms step_avg:87.95ms
step:1406/1680 train_time:123653ms step_avg:87.95ms
step:1407/1680 train_time:123741ms step_avg:87.95ms
step:1408/1680 train_time:123829ms step_avg:87.95ms
step:1409/1680 train_time:123918ms step_avg:87.95ms
step:1410/1680 train_time:124008ms step_avg:87.95ms
step:1411/1680 train_time:124097ms step_avg:87.95ms
step:1412/1680 train_time:124187ms step_avg:87.95ms
step:1413/1680 train_time:124277ms step_avg:87.95ms
step:1414/1680 train_time:124366ms step_avg:87.95ms
step:1415/1680 train_time:124456ms step_avg:87.95ms
step:1416/1680 train_time:124544ms step_avg:87.95ms
step:1417/1680 train_time:124634ms step_avg:87.96ms
step:1418/1680 train_time:124722ms step_avg:87.96ms
step:1419/1680 train_time:124811ms step_avg:87.96ms
step:1420/1680 train_time:124900ms step_avg:87.96ms
step:1421/1680 train_time:124989ms step_avg:87.96ms
step:1422/1680 train_time:125079ms step_avg:87.96ms
step:1423/1680 train_time:125168ms step_avg:87.96ms
step:1424/1680 train_time:125257ms step_avg:87.96ms
step:1425/1680 train_time:125346ms step_avg:87.96ms
step:1426/1680 train_time:125435ms step_avg:87.96ms
step:1427/1680 train_time:125524ms step_avg:87.96ms
step:1428/1680 train_time:125613ms step_avg:87.96ms
step:1429/1680 train_time:125702ms step_avg:87.97ms
step:1430/1680 train_time:125791ms step_avg:87.97ms
step:1431/1680 train_time:125880ms step_avg:87.97ms
step:1432/1680 train_time:125969ms step_avg:87.97ms
step:1433/1680 train_time:126059ms step_avg:87.97ms
step:1434/1680 train_time:126148ms step_avg:87.97ms
step:1435/1680 train_time:126238ms step_avg:87.97ms
step:1436/1680 train_time:126328ms step_avg:87.97ms
step:1437/1680 train_time:126417ms step_avg:87.97ms
step:1438/1680 train_time:126506ms step_avg:87.97ms
step:1439/1680 train_time:126594ms step_avg:87.97ms
step:1440/1680 train_time:126683ms step_avg:87.97ms
step:1441/1680 train_time:126772ms step_avg:87.97ms
step:1442/1680 train_time:126861ms step_avg:87.98ms
step:1443/1680 train_time:126950ms step_avg:87.98ms
step:1444/1680 train_time:127039ms step_avg:87.98ms
step:1445/1680 train_time:127128ms step_avg:87.98ms
step:1446/1680 train_time:127217ms step_avg:87.98ms
step:1447/1680 train_time:127306ms step_avg:87.98ms
step:1448/1680 train_time:127395ms step_avg:87.98ms
step:1449/1680 train_time:127485ms step_avg:87.98ms
step:1450/1680 train_time:127574ms step_avg:87.98ms
step:1451/1680 train_time:127664ms step_avg:87.98ms
step:1452/1680 train_time:127753ms step_avg:87.98ms
step:1453/1680 train_time:127842ms step_avg:87.98ms
step:1454/1680 train_time:127931ms step_avg:87.99ms
step:1455/1680 train_time:128020ms step_avg:87.99ms
step:1456/1680 train_time:128109ms step_avg:87.99ms
step:1457/1680 train_time:128198ms step_avg:87.99ms
step:1458/1680 train_time:128287ms step_avg:87.99ms
step:1459/1680 train_time:128376ms step_avg:87.99ms
step:1460/1680 train_time:128466ms step_avg:87.99ms
step:1461/1680 train_time:128555ms step_avg:87.99ms
step:1462/1680 train_time:128644ms step_avg:87.99ms
step:1463/1680 train_time:128734ms step_avg:87.99ms
step:1464/1680 train_time:128823ms step_avg:87.99ms
step:1465/1680 train_time:128911ms step_avg:87.99ms
step:1466/1680 train_time:129001ms step_avg:87.99ms
step:1467/1680 train_time:129090ms step_avg:88.00ms
step:1468/1680 train_time:129178ms step_avg:88.00ms
step:1469/1680 train_time:129268ms step_avg:88.00ms
step:1470/1680 train_time:129356ms step_avg:88.00ms
step:1471/1680 train_time:129446ms step_avg:88.00ms
step:1472/1680 train_time:129535ms step_avg:88.00ms
step:1473/1680 train_time:129626ms step_avg:88.00ms
step:1474/1680 train_time:129715ms step_avg:88.00ms
step:1475/1680 train_time:129804ms step_avg:88.00ms
step:1476/1680 train_time:129892ms step_avg:88.00ms
step:1477/1680 train_time:129981ms step_avg:88.00ms
step:1478/1680 train_time:130070ms step_avg:88.00ms
step:1479/1680 train_time:130160ms step_avg:88.01ms
step:1480/1680 train_time:130249ms step_avg:88.01ms
step:1481/1680 train_time:130338ms step_avg:88.01ms
step:1482/1680 train_time:130427ms step_avg:88.01ms
step:1483/1680 train_time:130516ms step_avg:88.01ms
step:1484/1680 train_time:130606ms step_avg:88.01ms
step:1485/1680 train_time:130695ms step_avg:88.01ms
step:1486/1680 train_time:130783ms step_avg:88.01ms
step:1487/1680 train_time:130873ms step_avg:88.01ms
step:1488/1680 train_time:130961ms step_avg:88.01ms
step:1489/1680 train_time:131050ms step_avg:88.01ms
step:1490/1680 train_time:131139ms step_avg:88.01ms
step:1491/1680 train_time:131229ms step_avg:88.01ms
step:1492/1680 train_time:131318ms step_avg:88.01ms
step:1493/1680 train_time:131407ms step_avg:88.02ms
step:1494/1680 train_time:131496ms step_avg:88.02ms
step:1495/1680 train_time:131585ms step_avg:88.02ms
step:1496/1680 train_time:131673ms step_avg:88.02ms
step:1497/1680 train_time:131762ms step_avg:88.02ms
step:1498/1680 train_time:131851ms step_avg:88.02ms
step:1499/1680 train_time:131941ms step_avg:88.02ms
step:1500/1680 train_time:132030ms step_avg:88.02ms
step:1500/1680 val_loss:3.3135 train_time:132121ms step_avg:88.08ms
step:1501/1680 train_time:132139ms step_avg:88.03ms
step:1502/1680 train_time:132212ms step_avg:88.02ms
step:1503/1680 train_time:132304ms step_avg:88.03ms
step:1504/1680 train_time:132395ms step_avg:88.03ms
step:1505/1680 train_time:132483ms step_avg:88.03ms
step:1506/1680 train_time:132572ms step_avg:88.03ms
step:1507/1680 train_time:132660ms step_avg:88.03ms
step:1508/1680 train_time:132748ms step_avg:88.03ms
step:1509/1680 train_time:132836ms step_avg:88.03ms
step:1510/1680 train_time:132924ms step_avg:88.03ms
step:1511/1680 train_time:133012ms step_avg:88.03ms
step:1512/1680 train_time:133103ms step_avg:88.03ms
step:1513/1680 train_time:133193ms step_avg:88.03ms
step:1514/1680 train_time:133284ms step_avg:88.03ms
step:1515/1680 train_time:133374ms step_avg:88.04ms
step:1516/1680 train_time:133463ms step_avg:88.04ms
step:1517/1680 train_time:133551ms step_avg:88.04ms
step:1518/1680 train_time:133640ms step_avg:88.04ms
step:1519/1680 train_time:133728ms step_avg:88.04ms
step:1520/1680 train_time:133816ms step_avg:88.04ms
step:1521/1680 train_time:133904ms step_avg:88.04ms
step:1522/1680 train_time:133993ms step_avg:88.04ms
step:1523/1680 train_time:134082ms step_avg:88.04ms
step:1524/1680 train_time:134171ms step_avg:88.04ms
step:1525/1680 train_time:134262ms step_avg:88.04ms
step:1526/1680 train_time:134351ms step_avg:88.04ms
step:1527/1680 train_time:134441ms step_avg:88.04ms
step:1528/1680 train_time:134529ms step_avg:88.04ms
step:1529/1680 train_time:134617ms step_avg:88.04ms
step:1530/1680 train_time:134706ms step_avg:88.04ms
step:1531/1680 train_time:134794ms step_avg:88.04ms
step:1532/1680 train_time:134882ms step_avg:88.04ms
step:1533/1680 train_time:134971ms step_avg:88.04ms
step:1534/1680 train_time:135061ms step_avg:88.04ms
step:1535/1680 train_time:135149ms step_avg:88.05ms
step:1536/1680 train_time:135239ms step_avg:88.05ms
step:1537/1680 train_time:135329ms step_avg:88.05ms
step:1538/1680 train_time:135418ms step_avg:88.05ms
step:1539/1680 train_time:135507ms step_avg:88.05ms
step:1540/1680 train_time:135596ms step_avg:88.05ms
step:1541/1680 train_time:135686ms step_avg:88.05ms
step:1542/1680 train_time:135774ms step_avg:88.05ms
step:1543/1680 train_time:135863ms step_avg:88.05ms
step:1544/1680 train_time:135951ms step_avg:88.05ms
step:1545/1680 train_time:136040ms step_avg:88.05ms
step:1546/1680 train_time:136129ms step_avg:88.05ms
step:1547/1680 train_time:136218ms step_avg:88.05ms
step:1548/1680 train_time:136308ms step_avg:88.05ms
step:1549/1680 train_time:136398ms step_avg:88.06ms
step:1550/1680 train_time:136487ms step_avg:88.06ms
step:1551/1680 train_time:136575ms step_avg:88.06ms
step:1552/1680 train_time:136664ms step_avg:88.06ms
step:1553/1680 train_time:136753ms step_avg:88.06ms
step:1554/1680 train_time:136842ms step_avg:88.06ms
step:1555/1680 train_time:136930ms step_avg:88.06ms
step:1556/1680 train_time:137019ms step_avg:88.06ms
step:1557/1680 train_time:137108ms step_avg:88.06ms
step:1558/1680 train_time:137197ms step_avg:88.06ms
step:1559/1680 train_time:137286ms step_avg:88.06ms
step:1560/1680 train_time:137375ms step_avg:88.06ms
step:1561/1680 train_time:137464ms step_avg:88.06ms
step:1562/1680 train_time:137554ms step_avg:88.06ms
step:1563/1680 train_time:137642ms step_avg:88.06ms
step:1564/1680 train_time:137731ms step_avg:88.06ms
step:1565/1680 train_time:137820ms step_avg:88.06ms
step:1566/1680 train_time:137908ms step_avg:88.06ms
step:1567/1680 train_time:137998ms step_avg:88.06ms
step:1568/1680 train_time:138087ms step_avg:88.07ms
step:1569/1680 train_time:138176ms step_avg:88.07ms
step:1570/1680 train_time:138265ms step_avg:88.07ms
step:1571/1680 train_time:138354ms step_avg:88.07ms
step:1572/1680 train_time:138442ms step_avg:88.07ms
step:1573/1680 train_time:138531ms step_avg:88.07ms
step:1574/1680 train_time:138621ms step_avg:88.07ms
step:1575/1680 train_time:138709ms step_avg:88.07ms
step:1576/1680 train_time:138798ms step_avg:88.07ms
step:1577/1680 train_time:138887ms step_avg:88.07ms
step:1578/1680 train_time:138977ms step_avg:88.07ms
step:1579/1680 train_time:139066ms step_avg:88.07ms
step:1580/1680 train_time:139155ms step_avg:88.07ms
step:1581/1680 train_time:139244ms step_avg:88.07ms
step:1582/1680 train_time:139333ms step_avg:88.07ms
step:1583/1680 train_time:139422ms step_avg:88.07ms
step:1584/1680 train_time:139511ms step_avg:88.07ms
step:1585/1680 train_time:139600ms step_avg:88.08ms
step:1586/1680 train_time:139688ms step_avg:88.08ms
step:1587/1680 train_time:139777ms step_avg:88.08ms
step:1588/1680 train_time:139866ms step_avg:88.08ms
step:1589/1680 train_time:139956ms step_avg:88.08ms
step:1590/1680 train_time:140045ms step_avg:88.08ms
step:1591/1680 train_time:140134ms step_avg:88.08ms
step:1592/1680 train_time:140223ms step_avg:88.08ms
step:1593/1680 train_time:140312ms step_avg:88.08ms
step:1594/1680 train_time:140400ms step_avg:88.08ms
step:1595/1680 train_time:140490ms step_avg:88.08ms
step:1596/1680 train_time:140579ms step_avg:88.08ms
step:1597/1680 train_time:140667ms step_avg:88.08ms
step:1598/1680 train_time:140757ms step_avg:88.08ms
step:1599/1680 train_time:140846ms step_avg:88.08ms
step:1600/1680 train_time:140935ms step_avg:88.08ms
step:1601/1680 train_time:141024ms step_avg:88.08ms
step:1602/1680 train_time:141112ms step_avg:88.09ms
step:1603/1680 train_time:141202ms step_avg:88.09ms
step:1604/1680 train_time:141291ms step_avg:88.09ms
step:1605/1680 train_time:141380ms step_avg:88.09ms
step:1606/1680 train_time:141469ms step_avg:88.09ms
step:1607/1680 train_time:141558ms step_avg:88.09ms
step:1608/1680 train_time:141646ms step_avg:88.09ms
step:1609/1680 train_time:141736ms step_avg:88.09ms
step:1610/1680 train_time:141825ms step_avg:88.09ms
step:1611/1680 train_time:141914ms step_avg:88.09ms
step:1612/1680 train_time:142003ms step_avg:88.09ms
step:1613/1680 train_time:142091ms step_avg:88.09ms
step:1614/1680 train_time:142181ms step_avg:88.09ms
step:1615/1680 train_time:142270ms step_avg:88.09ms
step:1616/1680 train_time:142360ms step_avg:88.09ms
step:1617/1680 train_time:142449ms step_avg:88.09ms
step:1618/1680 train_time:142538ms step_avg:88.10ms
step:1619/1680 train_time:142627ms step_avg:88.10ms
step:1620/1680 train_time:142716ms step_avg:88.10ms
step:1621/1680 train_time:142806ms step_avg:88.10ms
step:1622/1680 train_time:142895ms step_avg:88.10ms
step:1623/1680 train_time:142984ms step_avg:88.10ms
step:1624/1680 train_time:143073ms step_avg:88.10ms
step:1625/1680 train_time:143161ms step_avg:88.10ms
step:1625/1680 val_loss:3.2898 train_time:143251ms step_avg:88.15ms
step:1626/1680 train_time:143269ms step_avg:88.11ms
step:1627/1680 train_time:143343ms step_avg:88.10ms
step:1628/1680 train_time:143438ms step_avg:88.11ms
step:1629/1680 train_time:143526ms step_avg:88.11ms
step:1630/1680 train_time:143615ms step_avg:88.11ms
step:1631/1680 train_time:143704ms step_avg:88.11ms
step:1632/1680 train_time:143792ms step_avg:88.11ms
step:1633/1680 train_time:143880ms step_avg:88.11ms
step:1634/1680 train_time:143968ms step_avg:88.11ms
step:1635/1680 train_time:144056ms step_avg:88.11ms
step:1636/1680 train_time:144144ms step_avg:88.11ms
step:1637/1680 train_time:144234ms step_avg:88.11ms
step:1638/1680 train_time:144326ms step_avg:88.11ms
step:1639/1680 train_time:144417ms step_avg:88.11ms
step:1640/1680 train_time:144507ms step_avg:88.11ms
step:1641/1680 train_time:144597ms step_avg:88.11ms
step:1642/1680 train_time:144685ms step_avg:88.12ms
step:1643/1680 train_time:144773ms step_avg:88.12ms
step:1644/1680 train_time:144861ms step_avg:88.12ms
step:1645/1680 train_time:144950ms step_avg:88.12ms
step:1646/1680 train_time:145038ms step_avg:88.12ms
step:1647/1680 train_time:145126ms step_avg:88.12ms
step:1648/1680 train_time:145215ms step_avg:88.12ms
step:1649/1680 train_time:145304ms step_avg:88.12ms
step:1650/1680 train_time:145394ms step_avg:88.12ms
step:1651/1680 train_time:145484ms step_avg:88.12ms
step:1652/1680 train_time:145574ms step_avg:88.12ms
step:1653/1680 train_time:145662ms step_avg:88.12ms
step:1654/1680 train_time:145750ms step_avg:88.12ms
step:1655/1680 train_time:145840ms step_avg:88.12ms
step:1656/1680 train_time:145928ms step_avg:88.12ms
step:1657/1680 train_time:146017ms step_avg:88.12ms
step:1658/1680 train_time:146106ms step_avg:88.12ms
step:1659/1680 train_time:146195ms step_avg:88.12ms
step:1660/1680 train_time:146283ms step_avg:88.12ms
step:1661/1680 train_time:146372ms step_avg:88.12ms
step:1662/1680 train_time:146461ms step_avg:88.12ms
step:1663/1680 train_time:146552ms step_avg:88.12ms
step:1664/1680 train_time:146641ms step_avg:88.13ms
step:1665/1680 train_time:146730ms step_avg:88.13ms
step:1666/1680 train_time:146820ms step_avg:88.13ms
step:1667/1680 train_time:146908ms step_avg:88.13ms
step:1668/1680 train_time:146997ms step_avg:88.13ms
step:1669/1680 train_time:147085ms step_avg:88.13ms
step:1670/1680 train_time:147174ms step_avg:88.13ms
step:1671/1680 train_time:147262ms step_avg:88.13ms
step:1672/1680 train_time:147351ms step_avg:88.13ms
step:1673/1680 train_time:147440ms step_avg:88.13ms
step:1674/1680 train_time:147530ms step_avg:88.13ms
step:1675/1680 train_time:147621ms step_avg:88.13ms
step:1676/1680 train_time:147711ms step_avg:88.13ms
step:1677/1680 train_time:147800ms step_avg:88.13ms
step:1678/1680 train_time:147889ms step_avg:88.13ms
step:1679/1680 train_time:147977ms step_avg:88.13ms
step:1680/1680 train_time:148066ms step_avg:88.13ms
step:1680/1680 val_loss:3.2791 train_time:148156ms step_avg:88.19ms
peak memory allocated: 30760 MiB reserved: 46114 MiB

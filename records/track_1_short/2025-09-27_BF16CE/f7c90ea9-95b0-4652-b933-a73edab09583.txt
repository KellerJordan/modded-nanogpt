import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 13:36:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            116W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   26C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            114W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    177392      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    177393      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    177394      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    177395      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    177396      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    177397      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    177398      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    177399      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    177393      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    177394      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    177395      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    177396      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    177397      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    177398      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    177399      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:139ms step_avg:139.07ms
step:2/1680 train_time:161ms step_avg:80.25ms
step:3/1680 train_time:224ms step_avg:74.73ms
step:4/1680 train_time:309ms step_avg:77.27ms
step:5/1680 train_time:395ms step_avg:78.96ms
step:6/1680 train_time:480ms step_avg:80.07ms
step:7/1680 train_time:566ms step_avg:80.92ms
step:8/1680 train_time:653ms step_avg:81.59ms
step:9/1680 train_time:739ms step_avg:82.09ms
step:10/1680 train_time:825ms step_avg:82.45ms
step:11/1680 train_time:911ms step_avg:82.78ms
step:12/1680 train_time:999ms step_avg:83.22ms
step:13/1680 train_time:1091ms step_avg:83.89ms
step:14/1680 train_time:1180ms step_avg:84.29ms
step:15/1680 train_time:1270ms step_avg:84.67ms
step:16/1680 train_time:1357ms step_avg:84.81ms
step:17/1680 train_time:1444ms step_avg:84.91ms
step:18/1680 train_time:1530ms step_avg:85.00ms
step:19/1680 train_time:1616ms step_avg:85.06ms
step:20/1680 train_time:1702ms step_avg:85.11ms
step:21/1680 train_time:1789ms step_avg:85.20ms
step:22/1680 train_time:1876ms step_avg:85.26ms
step:23/1680 train_time:1963ms step_avg:85.34ms
step:24/1680 train_time:2051ms step_avg:85.45ms
step:25/1680 train_time:2140ms step_avg:85.61ms
step:26/1680 train_time:2230ms step_avg:85.76ms
step:27/1680 train_time:2319ms step_avg:85.88ms
step:28/1680 train_time:2406ms step_avg:85.93ms
step:29/1680 train_time:2493ms step_avg:85.96ms
step:30/1680 train_time:2579ms step_avg:85.98ms
step:31/1680 train_time:2666ms step_avg:85.99ms
step:32/1680 train_time:2753ms step_avg:86.04ms
step:33/1680 train_time:2840ms step_avg:86.06ms
step:34/1680 train_time:2927ms step_avg:86.09ms
step:35/1680 train_time:3015ms step_avg:86.14ms
step:36/1680 train_time:3105ms step_avg:86.24ms
step:37/1680 train_time:3193ms step_avg:86.30ms
step:38/1680 train_time:3282ms step_avg:86.36ms
step:39/1680 train_time:3369ms step_avg:86.39ms
step:40/1680 train_time:3456ms step_avg:86.41ms
step:41/1680 train_time:3543ms step_avg:86.42ms
step:42/1680 train_time:3630ms step_avg:86.42ms
step:43/1680 train_time:3718ms step_avg:86.45ms
step:44/1680 train_time:3804ms step_avg:86.46ms
step:45/1680 train_time:3891ms step_avg:86.48ms
step:46/1680 train_time:3979ms step_avg:86.49ms
step:47/1680 train_time:4066ms step_avg:86.50ms
step:48/1680 train_time:4154ms step_avg:86.54ms
step:49/1680 train_time:4241ms step_avg:86.55ms
step:50/1680 train_time:4329ms step_avg:86.58ms
step:51/1680 train_time:4416ms step_avg:86.59ms
step:52/1680 train_time:4504ms step_avg:86.61ms
step:53/1680 train_time:4591ms step_avg:86.62ms
step:54/1680 train_time:4678ms step_avg:86.63ms
step:55/1680 train_time:4765ms step_avg:86.65ms
step:56/1680 train_time:4853ms step_avg:86.66ms
step:57/1680 train_time:4940ms step_avg:86.67ms
step:58/1680 train_time:5026ms step_avg:86.66ms
step:59/1680 train_time:5114ms step_avg:86.68ms
step:60/1680 train_time:5201ms step_avg:86.69ms
step:61/1680 train_time:5290ms step_avg:86.72ms
step:62/1680 train_time:5377ms step_avg:86.72ms
step:63/1680 train_time:5464ms step_avg:86.74ms
step:64/1680 train_time:5551ms step_avg:86.73ms
step:65/1680 train_time:5638ms step_avg:86.74ms
step:66/1680 train_time:5725ms step_avg:86.75ms
step:67/1680 train_time:5812ms step_avg:86.75ms
step:68/1680 train_time:5899ms step_avg:86.75ms
step:69/1680 train_time:5987ms step_avg:86.76ms
step:70/1680 train_time:6074ms step_avg:86.77ms
step:71/1680 train_time:6161ms step_avg:86.78ms
step:72/1680 train_time:6248ms step_avg:86.78ms
step:73/1680 train_time:6335ms step_avg:86.78ms
step:74/1680 train_time:6422ms step_avg:86.79ms
step:75/1680 train_time:6510ms step_avg:86.80ms
step:76/1680 train_time:6597ms step_avg:86.80ms
step:77/1680 train_time:6684ms step_avg:86.81ms
step:78/1680 train_time:6771ms step_avg:86.81ms
step:79/1680 train_time:6859ms step_avg:86.82ms
step:80/1680 train_time:6946ms step_avg:86.82ms
step:81/1680 train_time:7032ms step_avg:86.82ms
step:82/1680 train_time:7120ms step_avg:86.83ms
step:83/1680 train_time:7207ms step_avg:86.83ms
step:84/1680 train_time:7295ms step_avg:86.84ms
step:85/1680 train_time:7382ms step_avg:86.85ms
step:86/1680 train_time:7469ms step_avg:86.85ms
step:87/1680 train_time:7556ms step_avg:86.85ms
step:88/1680 train_time:7643ms step_avg:86.85ms
step:89/1680 train_time:7729ms step_avg:86.85ms
step:90/1680 train_time:7817ms step_avg:86.85ms
step:91/1680 train_time:7904ms step_avg:86.86ms
step:92/1680 train_time:7992ms step_avg:86.87ms
step:93/1680 train_time:8079ms step_avg:86.87ms
step:94/1680 train_time:8166ms step_avg:86.87ms
step:95/1680 train_time:8253ms step_avg:86.88ms
step:96/1680 train_time:8340ms step_avg:86.88ms
step:97/1680 train_time:8427ms step_avg:86.88ms
step:98/1680 train_time:8514ms step_avg:86.88ms
step:99/1680 train_time:8601ms step_avg:86.88ms
step:100/1680 train_time:8689ms step_avg:86.89ms
step:101/1680 train_time:8776ms step_avg:86.89ms
step:102/1680 train_time:8863ms step_avg:86.89ms
step:103/1680 train_time:8950ms step_avg:86.89ms
step:104/1680 train_time:9037ms step_avg:86.89ms
step:105/1680 train_time:9124ms step_avg:86.90ms
step:106/1680 train_time:9212ms step_avg:86.91ms
step:107/1680 train_time:9300ms step_avg:86.91ms
step:108/1680 train_time:9387ms step_avg:86.92ms
step:109/1680 train_time:9474ms step_avg:86.92ms
step:110/1680 train_time:9562ms step_avg:86.92ms
step:111/1680 train_time:9649ms step_avg:86.93ms
step:112/1680 train_time:9736ms step_avg:86.93ms
step:113/1680 train_time:9823ms step_avg:86.93ms
step:114/1680 train_time:9909ms step_avg:86.92ms
step:115/1680 train_time:9997ms step_avg:86.93ms
step:116/1680 train_time:10084ms step_avg:86.93ms
step:117/1680 train_time:10171ms step_avg:86.94ms
step:118/1680 train_time:10258ms step_avg:86.93ms
step:119/1680 train_time:10346ms step_avg:86.94ms
step:120/1680 train_time:10433ms step_avg:86.94ms
step:121/1680 train_time:10519ms step_avg:86.94ms
step:122/1680 train_time:10606ms step_avg:86.94ms
step:123/1680 train_time:10694ms step_avg:86.94ms
step:124/1680 train_time:10781ms step_avg:86.94ms
step:125/1680 train_time:10868ms step_avg:86.94ms
step:125/1680 val_loss:4.3077 train_time:10956ms step_avg:87.65ms
step:126/1680 train_time:10975ms step_avg:87.11ms
step:127/1680 train_time:11047ms step_avg:86.98ms
step:128/1680 train_time:11142ms step_avg:87.05ms
step:129/1680 train_time:11231ms step_avg:87.06ms
step:130/1680 train_time:11318ms step_avg:87.07ms
step:131/1680 train_time:11405ms step_avg:87.06ms
step:132/1680 train_time:11492ms step_avg:87.06ms
step:133/1680 train_time:11577ms step_avg:87.05ms
step:134/1680 train_time:11663ms step_avg:87.04ms
step:135/1680 train_time:11749ms step_avg:87.03ms
step:136/1680 train_time:11835ms step_avg:87.02ms
step:137/1680 train_time:11922ms step_avg:87.02ms
step:138/1680 train_time:12009ms step_avg:87.02ms
step:139/1680 train_time:12098ms step_avg:87.03ms
step:140/1680 train_time:12186ms step_avg:87.04ms
step:141/1680 train_time:12274ms step_avg:87.05ms
step:142/1680 train_time:12361ms step_avg:87.05ms
step:143/1680 train_time:12449ms step_avg:87.05ms
step:144/1680 train_time:12535ms step_avg:87.05ms
step:145/1680 train_time:12621ms step_avg:87.04ms
step:146/1680 train_time:12708ms step_avg:87.04ms
step:147/1680 train_time:12794ms step_avg:87.04ms
step:148/1680 train_time:12880ms step_avg:87.03ms
step:149/1680 train_time:12968ms step_avg:87.03ms
step:150/1680 train_time:13055ms step_avg:87.04ms
step:151/1680 train_time:13143ms step_avg:87.04ms
step:152/1680 train_time:13230ms step_avg:87.04ms
step:153/1680 train_time:13318ms step_avg:87.04ms
step:154/1680 train_time:13405ms step_avg:87.04ms
step:155/1680 train_time:13492ms step_avg:87.05ms
step:156/1680 train_time:13579ms step_avg:87.04ms
step:157/1680 train_time:13665ms step_avg:87.04ms
step:158/1680 train_time:13752ms step_avg:87.04ms
step:159/1680 train_time:13838ms step_avg:87.03ms
step:160/1680 train_time:13925ms step_avg:87.03ms
step:161/1680 train_time:14012ms step_avg:87.03ms
step:162/1680 train_time:14099ms step_avg:87.03ms
step:163/1680 train_time:14187ms step_avg:87.04ms
step:164/1680 train_time:14275ms step_avg:87.04ms
step:165/1680 train_time:14362ms step_avg:87.05ms
step:166/1680 train_time:14449ms step_avg:87.04ms
step:167/1680 train_time:14536ms step_avg:87.04ms
step:168/1680 train_time:14623ms step_avg:87.04ms
step:169/1680 train_time:14709ms step_avg:87.04ms
step:170/1680 train_time:14797ms step_avg:87.04ms
step:171/1680 train_time:14884ms step_avg:87.04ms
step:172/1680 train_time:14971ms step_avg:87.04ms
step:173/1680 train_time:15058ms step_avg:87.04ms
step:174/1680 train_time:15145ms step_avg:87.04ms
step:175/1680 train_time:15233ms step_avg:87.04ms
step:176/1680 train_time:15321ms step_avg:87.05ms
step:177/1680 train_time:15408ms step_avg:87.05ms
step:178/1680 train_time:15495ms step_avg:87.05ms
step:179/1680 train_time:15581ms step_avg:87.05ms
step:180/1680 train_time:15668ms step_avg:87.04ms
step:181/1680 train_time:15754ms step_avg:87.04ms
step:182/1680 train_time:15841ms step_avg:87.04ms
step:183/1680 train_time:15928ms step_avg:87.04ms
step:184/1680 train_time:16016ms step_avg:87.04ms
step:185/1680 train_time:16102ms step_avg:87.04ms
step:186/1680 train_time:16190ms step_avg:87.05ms
step:187/1680 train_time:16278ms step_avg:87.05ms
step:188/1680 train_time:16365ms step_avg:87.05ms
step:189/1680 train_time:16452ms step_avg:87.05ms
step:190/1680 train_time:16539ms step_avg:87.05ms
step:191/1680 train_time:16627ms step_avg:87.05ms
step:192/1680 train_time:16714ms step_avg:87.05ms
step:193/1680 train_time:16801ms step_avg:87.05ms
step:194/1680 train_time:16888ms step_avg:87.05ms
step:195/1680 train_time:16975ms step_avg:87.05ms
step:196/1680 train_time:17062ms step_avg:87.05ms
step:197/1680 train_time:17149ms step_avg:87.05ms
step:198/1680 train_time:17236ms step_avg:87.05ms
step:199/1680 train_time:17323ms step_avg:87.05ms
step:200/1680 train_time:17410ms step_avg:87.05ms
step:201/1680 train_time:17497ms step_avg:87.05ms
step:202/1680 train_time:17584ms step_avg:87.05ms
step:203/1680 train_time:17672ms step_avg:87.05ms
step:204/1680 train_time:17759ms step_avg:87.05ms
step:205/1680 train_time:17846ms step_avg:87.05ms
step:206/1680 train_time:17933ms step_avg:87.05ms
step:207/1680 train_time:18020ms step_avg:87.05ms
step:208/1680 train_time:18107ms step_avg:87.05ms
step:209/1680 train_time:18194ms step_avg:87.05ms
step:210/1680 train_time:18280ms step_avg:87.05ms
step:211/1680 train_time:18367ms step_avg:87.05ms
step:212/1680 train_time:18455ms step_avg:87.05ms
step:213/1680 train_time:18542ms step_avg:87.05ms
step:214/1680 train_time:18628ms step_avg:87.05ms
step:215/1680 train_time:18715ms step_avg:87.05ms
step:216/1680 train_time:18803ms step_avg:87.05ms
step:217/1680 train_time:18890ms step_avg:87.05ms
step:218/1680 train_time:18977ms step_avg:87.05ms
step:219/1680 train_time:19064ms step_avg:87.05ms
step:220/1680 train_time:19152ms step_avg:87.05ms
step:221/1680 train_time:19238ms step_avg:87.05ms
step:222/1680 train_time:19325ms step_avg:87.05ms
step:223/1680 train_time:19412ms step_avg:87.05ms
step:224/1680 train_time:19499ms step_avg:87.05ms
step:225/1680 train_time:19586ms step_avg:87.05ms
step:226/1680 train_time:19673ms step_avg:87.05ms
step:227/1680 train_time:19761ms step_avg:87.05ms
step:228/1680 train_time:19848ms step_avg:87.05ms
step:229/1680 train_time:19935ms step_avg:87.05ms
step:230/1680 train_time:20022ms step_avg:87.05ms
step:231/1680 train_time:20109ms step_avg:87.05ms
step:232/1680 train_time:20196ms step_avg:87.05ms
step:233/1680 train_time:20283ms step_avg:87.05ms
step:234/1680 train_time:20370ms step_avg:87.05ms
step:235/1680 train_time:20457ms step_avg:87.05ms
step:236/1680 train_time:20544ms step_avg:87.05ms
step:237/1680 train_time:20631ms step_avg:87.05ms
step:238/1680 train_time:20717ms step_avg:87.05ms
step:239/1680 train_time:20805ms step_avg:87.05ms
step:240/1680 train_time:20893ms step_avg:87.05ms
step:241/1680 train_time:20979ms step_avg:87.05ms
step:242/1680 train_time:21066ms step_avg:87.05ms
step:243/1680 train_time:21153ms step_avg:87.05ms
step:244/1680 train_time:21240ms step_avg:87.05ms
step:245/1680 train_time:21327ms step_avg:87.05ms
step:246/1680 train_time:21414ms step_avg:87.05ms
step:247/1680 train_time:21501ms step_avg:87.05ms
step:248/1680 train_time:21588ms step_avg:87.05ms
step:249/1680 train_time:21675ms step_avg:87.05ms
step:250/1680 train_time:21762ms step_avg:87.05ms
step:250/1680 val_loss:3.9735 train_time:21851ms step_avg:87.40ms
step:251/1680 train_time:21870ms step_avg:87.13ms
step:252/1680 train_time:21939ms step_avg:87.06ms
step:253/1680 train_time:22034ms step_avg:87.09ms
step:254/1680 train_time:22122ms step_avg:87.09ms
step:255/1680 train_time:22209ms step_avg:87.09ms
step:256/1680 train_time:22294ms step_avg:87.09ms
step:257/1680 train_time:22381ms step_avg:87.08ms
step:258/1680 train_time:22466ms step_avg:87.08ms
step:259/1680 train_time:22552ms step_avg:87.07ms
step:260/1680 train_time:22639ms step_avg:87.07ms
step:261/1680 train_time:22725ms step_avg:87.07ms
step:262/1680 train_time:22813ms step_avg:87.07ms
step:263/1680 train_time:22900ms step_avg:87.07ms
step:264/1680 train_time:22989ms step_avg:87.08ms
step:265/1680 train_time:23078ms step_avg:87.09ms
step:266/1680 train_time:23165ms step_avg:87.09ms
step:267/1680 train_time:23252ms step_avg:87.09ms
step:268/1680 train_time:23339ms step_avg:87.08ms
step:269/1680 train_time:23425ms step_avg:87.08ms
step:270/1680 train_time:23511ms step_avg:87.08ms
step:271/1680 train_time:23597ms step_avg:87.07ms
step:272/1680 train_time:23684ms step_avg:87.07ms
step:273/1680 train_time:23770ms step_avg:87.07ms
step:274/1680 train_time:23857ms step_avg:87.07ms
step:275/1680 train_time:23945ms step_avg:87.07ms
step:276/1680 train_time:24033ms step_avg:87.08ms
step:277/1680 train_time:24120ms step_avg:87.08ms
step:278/1680 train_time:24207ms step_avg:87.08ms
step:279/1680 train_time:24295ms step_avg:87.08ms
step:280/1680 train_time:24381ms step_avg:87.08ms
step:281/1680 train_time:24469ms step_avg:87.08ms
step:282/1680 train_time:24555ms step_avg:87.07ms
step:283/1680 train_time:24641ms step_avg:87.07ms
step:284/1680 train_time:24728ms step_avg:87.07ms
step:285/1680 train_time:24815ms step_avg:87.07ms
step:286/1680 train_time:24902ms step_avg:87.07ms
step:287/1680 train_time:24989ms step_avg:87.07ms
step:288/1680 train_time:25078ms step_avg:87.08ms
step:289/1680 train_time:25165ms step_avg:87.08ms
step:290/1680 train_time:25252ms step_avg:87.08ms
step:291/1680 train_time:25339ms step_avg:87.08ms
step:292/1680 train_time:25425ms step_avg:87.07ms
step:293/1680 train_time:25512ms step_avg:87.07ms
step:294/1680 train_time:25599ms step_avg:87.07ms
step:295/1680 train_time:25685ms step_avg:87.07ms
step:296/1680 train_time:25772ms step_avg:87.07ms
step:297/1680 train_time:25859ms step_avg:87.07ms
step:298/1680 train_time:25945ms step_avg:87.06ms
step:299/1680 train_time:26032ms step_avg:87.06ms
step:300/1680 train_time:26120ms step_avg:87.07ms
step:301/1680 train_time:26207ms step_avg:87.07ms
step:302/1680 train_time:26294ms step_avg:87.07ms
step:303/1680 train_time:26381ms step_avg:87.07ms
step:304/1680 train_time:26468ms step_avg:87.07ms
step:305/1680 train_time:26555ms step_avg:87.07ms
step:306/1680 train_time:26642ms step_avg:87.06ms
step:307/1680 train_time:26729ms step_avg:87.06ms
step:308/1680 train_time:26816ms step_avg:87.06ms
step:309/1680 train_time:26903ms step_avg:87.06ms
step:310/1680 train_time:26990ms step_avg:87.07ms
step:311/1680 train_time:27077ms step_avg:87.07ms
step:312/1680 train_time:27165ms step_avg:87.07ms
step:313/1680 train_time:27251ms step_avg:87.07ms
step:314/1680 train_time:27339ms step_avg:87.07ms
step:315/1680 train_time:27426ms step_avg:87.07ms
step:316/1680 train_time:27513ms step_avg:87.07ms
step:317/1680 train_time:27599ms step_avg:87.06ms
step:318/1680 train_time:27686ms step_avg:87.06ms
step:319/1680 train_time:27773ms step_avg:87.06ms
step:320/1680 train_time:27859ms step_avg:87.06ms
step:321/1680 train_time:27946ms step_avg:87.06ms
step:322/1680 train_time:28033ms step_avg:87.06ms
step:323/1680 train_time:28120ms step_avg:87.06ms
step:324/1680 train_time:28207ms step_avg:87.06ms
step:325/1680 train_time:28294ms step_avg:87.06ms
step:326/1680 train_time:28382ms step_avg:87.06ms
step:327/1680 train_time:28469ms step_avg:87.06ms
step:328/1680 train_time:28556ms step_avg:87.06ms
step:329/1680 train_time:28642ms step_avg:87.06ms
step:330/1680 train_time:28729ms step_avg:87.06ms
step:331/1680 train_time:28817ms step_avg:87.06ms
step:332/1680 train_time:28903ms step_avg:87.06ms
step:333/1680 train_time:28990ms step_avg:87.06ms
step:334/1680 train_time:29077ms step_avg:87.06ms
step:335/1680 train_time:29164ms step_avg:87.06ms
step:336/1680 train_time:29252ms step_avg:87.06ms
step:337/1680 train_time:29339ms step_avg:87.06ms
step:338/1680 train_time:29426ms step_avg:87.06ms
step:339/1680 train_time:29513ms step_avg:87.06ms
step:340/1680 train_time:29600ms step_avg:87.06ms
step:341/1680 train_time:29687ms step_avg:87.06ms
step:342/1680 train_time:29773ms step_avg:87.06ms
step:343/1680 train_time:29860ms step_avg:87.06ms
step:344/1680 train_time:29947ms step_avg:87.06ms
step:345/1680 train_time:30034ms step_avg:87.06ms
step:346/1680 train_time:30121ms step_avg:87.06ms
step:347/1680 train_time:30209ms step_avg:87.06ms
step:348/1680 train_time:30295ms step_avg:87.06ms
step:349/1680 train_time:30383ms step_avg:87.06ms
step:350/1680 train_time:30470ms step_avg:87.06ms
step:351/1680 train_time:30557ms step_avg:87.06ms
step:352/1680 train_time:30644ms step_avg:87.06ms
step:353/1680 train_time:30731ms step_avg:87.06ms
step:354/1680 train_time:30817ms step_avg:87.05ms
step:355/1680 train_time:30904ms step_avg:87.05ms
step:356/1680 train_time:30990ms step_avg:87.05ms
step:357/1680 train_time:31078ms step_avg:87.05ms
step:358/1680 train_time:31165ms step_avg:87.05ms
step:359/1680 train_time:31252ms step_avg:87.05ms
step:360/1680 train_time:31339ms step_avg:87.05ms
step:361/1680 train_time:31426ms step_avg:87.05ms
step:362/1680 train_time:31512ms step_avg:87.05ms
step:363/1680 train_time:31599ms step_avg:87.05ms
step:364/1680 train_time:31686ms step_avg:87.05ms
step:365/1680 train_time:31774ms step_avg:87.05ms
step:366/1680 train_time:31862ms step_avg:87.05ms
step:367/1680 train_time:31949ms step_avg:87.05ms
step:368/1680 train_time:32036ms step_avg:87.05ms
step:369/1680 train_time:32123ms step_avg:87.05ms
step:370/1680 train_time:32209ms step_avg:87.05ms
step:371/1680 train_time:32297ms step_avg:87.05ms
step:372/1680 train_time:32383ms step_avg:87.05ms
step:373/1680 train_time:32470ms step_avg:87.05ms
step:374/1680 train_time:32558ms step_avg:87.05ms
step:375/1680 train_time:32645ms step_avg:87.05ms
step:375/1680 val_loss:3.8179 train_time:32733ms step_avg:87.29ms
step:376/1680 train_time:32753ms step_avg:87.11ms
step:377/1680 train_time:32824ms step_avg:87.07ms
step:378/1680 train_time:32915ms step_avg:87.08ms
step:379/1680 train_time:33004ms step_avg:87.08ms
step:380/1680 train_time:33090ms step_avg:87.08ms
step:381/1680 train_time:33176ms step_avg:87.08ms
step:382/1680 train_time:33262ms step_avg:87.07ms
step:383/1680 train_time:33348ms step_avg:87.07ms
step:384/1680 train_time:33434ms step_avg:87.07ms
step:385/1680 train_time:33520ms step_avg:87.07ms
step:386/1680 train_time:33606ms step_avg:87.06ms
step:387/1680 train_time:33694ms step_avg:87.06ms
step:388/1680 train_time:33781ms step_avg:87.07ms
step:389/1680 train_time:33870ms step_avg:87.07ms
step:390/1680 train_time:33960ms step_avg:87.08ms
step:391/1680 train_time:34047ms step_avg:87.08ms
step:392/1680 train_time:34133ms step_avg:87.08ms
step:393/1680 train_time:34220ms step_avg:87.07ms
step:394/1680 train_time:34306ms step_avg:87.07ms
step:395/1680 train_time:34392ms step_avg:87.07ms
step:396/1680 train_time:34479ms step_avg:87.07ms
step:397/1680 train_time:34565ms step_avg:87.07ms
step:398/1680 train_time:34652ms step_avg:87.06ms
step:399/1680 train_time:34739ms step_avg:87.06ms
step:400/1680 train_time:34827ms step_avg:87.07ms
step:401/1680 train_time:34914ms step_avg:87.07ms
step:402/1680 train_time:35002ms step_avg:87.07ms
step:403/1680 train_time:35089ms step_avg:87.07ms
step:404/1680 train_time:35176ms step_avg:87.07ms
step:405/1680 train_time:35262ms step_avg:87.07ms
step:406/1680 train_time:35349ms step_avg:87.07ms
step:407/1680 train_time:35436ms step_avg:87.07ms
step:408/1680 train_time:35523ms step_avg:87.07ms
step:409/1680 train_time:35609ms step_avg:87.06ms
step:410/1680 train_time:35696ms step_avg:87.06ms
step:411/1680 train_time:35783ms step_avg:87.06ms
step:412/1680 train_time:35870ms step_avg:87.06ms
step:413/1680 train_time:35958ms step_avg:87.07ms
step:414/1680 train_time:36045ms step_avg:87.07ms
step:415/1680 train_time:36133ms step_avg:87.07ms
step:416/1680 train_time:36220ms step_avg:87.07ms
step:417/1680 train_time:36306ms step_avg:87.07ms
step:418/1680 train_time:36393ms step_avg:87.06ms
step:419/1680 train_time:36479ms step_avg:87.06ms
step:420/1680 train_time:36567ms step_avg:87.06ms
step:421/1680 train_time:36653ms step_avg:87.06ms
step:422/1680 train_time:36740ms step_avg:87.06ms
step:423/1680 train_time:36828ms step_avg:87.06ms
step:424/1680 train_time:36915ms step_avg:87.06ms
step:425/1680 train_time:37002ms step_avg:87.06ms
step:426/1680 train_time:37089ms step_avg:87.06ms
step:427/1680 train_time:37176ms step_avg:87.06ms
step:428/1680 train_time:37264ms step_avg:87.07ms
step:429/1680 train_time:37351ms step_avg:87.06ms
step:430/1680 train_time:37438ms step_avg:87.07ms
step:431/1680 train_time:37525ms step_avg:87.07ms
step:432/1680 train_time:37612ms step_avg:87.06ms
step:433/1680 train_time:37698ms step_avg:87.06ms
step:434/1680 train_time:37785ms step_avg:87.06ms
step:435/1680 train_time:37872ms step_avg:87.06ms
step:436/1680 train_time:37960ms step_avg:87.07ms
step:437/1680 train_time:38048ms step_avg:87.07ms
step:438/1680 train_time:38134ms step_avg:87.06ms
step:439/1680 train_time:38222ms step_avg:87.07ms
step:440/1680 train_time:38309ms step_avg:87.07ms
step:441/1680 train_time:38396ms step_avg:87.07ms
step:442/1680 train_time:38483ms step_avg:87.07ms
step:443/1680 train_time:38569ms step_avg:87.06ms
step:444/1680 train_time:38656ms step_avg:87.06ms
step:445/1680 train_time:38743ms step_avg:87.06ms
step:446/1680 train_time:38830ms step_avg:87.06ms
step:447/1680 train_time:38917ms step_avg:87.06ms
step:448/1680 train_time:39004ms step_avg:87.06ms
step:449/1680 train_time:39091ms step_avg:87.06ms
step:450/1680 train_time:39178ms step_avg:87.06ms
step:451/1680 train_time:39265ms step_avg:87.06ms
step:452/1680 train_time:39353ms step_avg:87.06ms
step:453/1680 train_time:39440ms step_avg:87.06ms
step:454/1680 train_time:39527ms step_avg:87.06ms
step:455/1680 train_time:39613ms step_avg:87.06ms
step:456/1680 train_time:39700ms step_avg:87.06ms
step:457/1680 train_time:39786ms step_avg:87.06ms
step:458/1680 train_time:39873ms step_avg:87.06ms
step:459/1680 train_time:39961ms step_avg:87.06ms
step:460/1680 train_time:40049ms step_avg:87.06ms
step:461/1680 train_time:40135ms step_avg:87.06ms
step:462/1680 train_time:40223ms step_avg:87.06ms
step:463/1680 train_time:40310ms step_avg:87.06ms
step:464/1680 train_time:40397ms step_avg:87.06ms
step:465/1680 train_time:40484ms step_avg:87.06ms
step:466/1680 train_time:40571ms step_avg:87.06ms
step:467/1680 train_time:40658ms step_avg:87.06ms
step:468/1680 train_time:40744ms step_avg:87.06ms
step:469/1680 train_time:40832ms step_avg:87.06ms
step:470/1680 train_time:40919ms step_avg:87.06ms
step:471/1680 train_time:41005ms step_avg:87.06ms
step:472/1680 train_time:41093ms step_avg:87.06ms
step:473/1680 train_time:41180ms step_avg:87.06ms
step:474/1680 train_time:41267ms step_avg:87.06ms
step:475/1680 train_time:41354ms step_avg:87.06ms
step:476/1680 train_time:41442ms step_avg:87.06ms
step:477/1680 train_time:41528ms step_avg:87.06ms
step:478/1680 train_time:41615ms step_avg:87.06ms
step:479/1680 train_time:41702ms step_avg:87.06ms
step:480/1680 train_time:41789ms step_avg:87.06ms
step:481/1680 train_time:41877ms step_avg:87.06ms
step:482/1680 train_time:41964ms step_avg:87.06ms
step:483/1680 train_time:42052ms step_avg:87.06ms
step:484/1680 train_time:42139ms step_avg:87.06ms
step:485/1680 train_time:42225ms step_avg:87.06ms
step:486/1680 train_time:42313ms step_avg:87.06ms
step:487/1680 train_time:42400ms step_avg:87.06ms
step:488/1680 train_time:42487ms step_avg:87.06ms
step:489/1680 train_time:42573ms step_avg:87.06ms
step:490/1680 train_time:42660ms step_avg:87.06ms
step:491/1680 train_time:42747ms step_avg:87.06ms
step:492/1680 train_time:42834ms step_avg:87.06ms
step:493/1680 train_time:42923ms step_avg:87.07ms
step:494/1680 train_time:43009ms step_avg:87.06ms
step:495/1680 train_time:43097ms step_avg:87.06ms
step:496/1680 train_time:43184ms step_avg:87.06ms
step:497/1680 train_time:43271ms step_avg:87.06ms
step:498/1680 train_time:43358ms step_avg:87.06ms
step:499/1680 train_time:43446ms step_avg:87.07ms
step:500/1680 train_time:43533ms step_avg:87.07ms
step:500/1680 val_loss:3.7175 train_time:43621ms step_avg:87.24ms
step:501/1680 train_time:43640ms step_avg:87.11ms
step:502/1680 train_time:43710ms step_avg:87.07ms
step:503/1680 train_time:43800ms step_avg:87.08ms
step:504/1680 train_time:43888ms step_avg:87.08ms
step:505/1680 train_time:43975ms step_avg:87.08ms
step:506/1680 train_time:44061ms step_avg:87.08ms
step:507/1680 train_time:44146ms step_avg:87.07ms
step:508/1680 train_time:44233ms step_avg:87.07ms
step:509/1680 train_time:44319ms step_avg:87.07ms
step:510/1680 train_time:44406ms step_avg:87.07ms
step:511/1680 train_time:44493ms step_avg:87.07ms
step:512/1680 train_time:44580ms step_avg:87.07ms
step:513/1680 train_time:44668ms step_avg:87.07ms
step:514/1680 train_time:44757ms step_avg:87.08ms
step:515/1680 train_time:44845ms step_avg:87.08ms
step:516/1680 train_time:44932ms step_avg:87.08ms
step:517/1680 train_time:45019ms step_avg:87.08ms
step:518/1680 train_time:45106ms step_avg:87.08ms
step:519/1680 train_time:45192ms step_avg:87.07ms
step:520/1680 train_time:45278ms step_avg:87.07ms
step:521/1680 train_time:45365ms step_avg:87.07ms
step:522/1680 train_time:45451ms step_avg:87.07ms
step:523/1680 train_time:45539ms step_avg:87.07ms
step:524/1680 train_time:45626ms step_avg:87.07ms
step:525/1680 train_time:45715ms step_avg:87.08ms
step:526/1680 train_time:45802ms step_avg:87.08ms
step:527/1680 train_time:45889ms step_avg:87.08ms
step:528/1680 train_time:45977ms step_avg:87.08ms
step:529/1680 train_time:46063ms step_avg:87.08ms
step:530/1680 train_time:46150ms step_avg:87.08ms
step:531/1680 train_time:46237ms step_avg:87.08ms
step:532/1680 train_time:46324ms step_avg:87.07ms
step:533/1680 train_time:46410ms step_avg:87.07ms
step:534/1680 train_time:46497ms step_avg:87.07ms
step:535/1680 train_time:46585ms step_avg:87.07ms
step:536/1680 train_time:46673ms step_avg:87.08ms
step:537/1680 train_time:46760ms step_avg:87.08ms
step:538/1680 train_time:46847ms step_avg:87.08ms
step:539/1680 train_time:46935ms step_avg:87.08ms
step:540/1680 train_time:47023ms step_avg:87.08ms
step:541/1680 train_time:47109ms step_avg:87.08ms
step:542/1680 train_time:47196ms step_avg:87.08ms
step:543/1680 train_time:47283ms step_avg:87.08ms
step:544/1680 train_time:47370ms step_avg:87.08ms
step:545/1680 train_time:47456ms step_avg:87.08ms
step:546/1680 train_time:47543ms step_avg:87.08ms
step:547/1680 train_time:47630ms step_avg:87.08ms
step:548/1680 train_time:47718ms step_avg:87.08ms
step:549/1680 train_time:47806ms step_avg:87.08ms
step:550/1680 train_time:47894ms step_avg:87.08ms
step:551/1680 train_time:47983ms step_avg:87.08ms
step:552/1680 train_time:48071ms step_avg:87.09ms
step:553/1680 train_time:48159ms step_avg:87.09ms
step:554/1680 train_time:48246ms step_avg:87.09ms
step:555/1680 train_time:48334ms step_avg:87.09ms
step:556/1680 train_time:48423ms step_avg:87.09ms
step:557/1680 train_time:48510ms step_avg:87.09ms
step:558/1680 train_time:48598ms step_avg:87.09ms
step:559/1680 train_time:48686ms step_avg:87.10ms
step:560/1680 train_time:48774ms step_avg:87.10ms
step:561/1680 train_time:48862ms step_avg:87.10ms
step:562/1680 train_time:48950ms step_avg:87.10ms
step:563/1680 train_time:49038ms step_avg:87.10ms
step:564/1680 train_time:49127ms step_avg:87.10ms
step:565/1680 train_time:49215ms step_avg:87.11ms
step:566/1680 train_time:49303ms step_avg:87.11ms
step:567/1680 train_time:49390ms step_avg:87.11ms
step:568/1680 train_time:49478ms step_avg:87.11ms
step:569/1680 train_time:49566ms step_avg:87.11ms
step:570/1680 train_time:49654ms step_avg:87.11ms
step:571/1680 train_time:49742ms step_avg:87.11ms
step:572/1680 train_time:49830ms step_avg:87.12ms
step:573/1680 train_time:49918ms step_avg:87.12ms
step:574/1680 train_time:50006ms step_avg:87.12ms
step:575/1680 train_time:50094ms step_avg:87.12ms
step:576/1680 train_time:50182ms step_avg:87.12ms
step:577/1680 train_time:50270ms step_avg:87.12ms
step:578/1680 train_time:50358ms step_avg:87.13ms
step:579/1680 train_time:50446ms step_avg:87.13ms
step:580/1680 train_time:50534ms step_avg:87.13ms
step:581/1680 train_time:50622ms step_avg:87.13ms
step:582/1680 train_time:50710ms step_avg:87.13ms
step:583/1680 train_time:50799ms step_avg:87.13ms
step:584/1680 train_time:50886ms step_avg:87.13ms
step:585/1680 train_time:50974ms step_avg:87.14ms
step:586/1680 train_time:51062ms step_avg:87.14ms
step:587/1680 train_time:51150ms step_avg:87.14ms
step:588/1680 train_time:51238ms step_avg:87.14ms
step:589/1680 train_time:51326ms step_avg:87.14ms
step:590/1680 train_time:51414ms step_avg:87.14ms
step:591/1680 train_time:51502ms step_avg:87.14ms
step:592/1680 train_time:51591ms step_avg:87.15ms
step:593/1680 train_time:51678ms step_avg:87.15ms
step:594/1680 train_time:51766ms step_avg:87.15ms
step:595/1680 train_time:51854ms step_avg:87.15ms
step:596/1680 train_time:51942ms step_avg:87.15ms
step:597/1680 train_time:52029ms step_avg:87.15ms
step:598/1680 train_time:52118ms step_avg:87.15ms
step:599/1680 train_time:52205ms step_avg:87.15ms
step:600/1680 train_time:52294ms step_avg:87.16ms
step:601/1680 train_time:52382ms step_avg:87.16ms
step:602/1680 train_time:52469ms step_avg:87.16ms
step:603/1680 train_time:52557ms step_avg:87.16ms
step:604/1680 train_time:52645ms step_avg:87.16ms
step:605/1680 train_time:52733ms step_avg:87.16ms
step:606/1680 train_time:52821ms step_avg:87.16ms
step:607/1680 train_time:52909ms step_avg:87.16ms
step:608/1680 train_time:52997ms step_avg:87.17ms
step:609/1680 train_time:53086ms step_avg:87.17ms
step:610/1680 train_time:53173ms step_avg:87.17ms
step:611/1680 train_time:53262ms step_avg:87.17ms
step:612/1680 train_time:53349ms step_avg:87.17ms
step:613/1680 train_time:53437ms step_avg:87.17ms
step:614/1680 train_time:53526ms step_avg:87.18ms
step:615/1680 train_time:53614ms step_avg:87.18ms
step:616/1680 train_time:53702ms step_avg:87.18ms
step:617/1680 train_time:53790ms step_avg:87.18ms
step:618/1680 train_time:53878ms step_avg:87.18ms
step:619/1680 train_time:53966ms step_avg:87.18ms
step:620/1680 train_time:54054ms step_avg:87.18ms
step:621/1680 train_time:54142ms step_avg:87.19ms
step:622/1680 train_time:54230ms step_avg:87.19ms
step:623/1680 train_time:54318ms step_avg:87.19ms
step:624/1680 train_time:54406ms step_avg:87.19ms
step:625/1680 train_time:54494ms step_avg:87.19ms
step:625/1680 val_loss:3.6157 train_time:54584ms step_avg:87.33ms
step:626/1680 train_time:54604ms step_avg:87.23ms
step:627/1680 train_time:54674ms step_avg:87.20ms
step:628/1680 train_time:54763ms step_avg:87.20ms
step:629/1680 train_time:54854ms step_avg:87.21ms
step:630/1680 train_time:54943ms step_avg:87.21ms
step:631/1680 train_time:55029ms step_avg:87.21ms
step:632/1680 train_time:55116ms step_avg:87.21ms
step:633/1680 train_time:55203ms step_avg:87.21ms
step:634/1680 train_time:55290ms step_avg:87.21ms
step:635/1680 train_time:55377ms step_avg:87.21ms
step:636/1680 train_time:55464ms step_avg:87.21ms
step:637/1680 train_time:55557ms step_avg:87.22ms
step:638/1680 train_time:55647ms step_avg:87.22ms
step:639/1680 train_time:55736ms step_avg:87.22ms
step:640/1680 train_time:55824ms step_avg:87.23ms
step:641/1680 train_time:55913ms step_avg:87.23ms
step:642/1680 train_time:56001ms step_avg:87.23ms
step:643/1680 train_time:56089ms step_avg:87.23ms
step:644/1680 train_time:56177ms step_avg:87.23ms
step:645/1680 train_time:56264ms step_avg:87.23ms
step:646/1680 train_time:56352ms step_avg:87.23ms
step:647/1680 train_time:56439ms step_avg:87.23ms
step:648/1680 train_time:56528ms step_avg:87.24ms
step:649/1680 train_time:56617ms step_avg:87.24ms
step:650/1680 train_time:56706ms step_avg:87.24ms
step:651/1680 train_time:56795ms step_avg:87.24ms
step:652/1680 train_time:56883ms step_avg:87.24ms
step:653/1680 train_time:56972ms step_avg:87.25ms
step:654/1680 train_time:57059ms step_avg:87.25ms
step:655/1680 train_time:57147ms step_avg:87.25ms
step:656/1680 train_time:57234ms step_avg:87.25ms
step:657/1680 train_time:57322ms step_avg:87.25ms
step:658/1680 train_time:57409ms step_avg:87.25ms
step:659/1680 train_time:57498ms step_avg:87.25ms
step:660/1680 train_time:57586ms step_avg:87.25ms
step:661/1680 train_time:57674ms step_avg:87.25ms
step:662/1680 train_time:57763ms step_avg:87.25ms
step:663/1680 train_time:57851ms step_avg:87.26ms
step:664/1680 train_time:57939ms step_avg:87.26ms
step:665/1680 train_time:58027ms step_avg:87.26ms
step:666/1680 train_time:58115ms step_avg:87.26ms
step:667/1680 train_time:58202ms step_avg:87.26ms
step:668/1680 train_time:58290ms step_avg:87.26ms
step:669/1680 train_time:58378ms step_avg:87.26ms
step:670/1680 train_time:58466ms step_avg:87.26ms
step:671/1680 train_time:58553ms step_avg:87.26ms
step:672/1680 train_time:58642ms step_avg:87.26ms
step:673/1680 train_time:58730ms step_avg:87.27ms
step:674/1680 train_time:58819ms step_avg:87.27ms
step:675/1680 train_time:58907ms step_avg:87.27ms
step:676/1680 train_time:58994ms step_avg:87.27ms
step:677/1680 train_time:59082ms step_avg:87.27ms
step:678/1680 train_time:59170ms step_avg:87.27ms
step:679/1680 train_time:59259ms step_avg:87.27ms
step:680/1680 train_time:59346ms step_avg:87.27ms
step:681/1680 train_time:59434ms step_avg:87.27ms
step:682/1680 train_time:59521ms step_avg:87.27ms
step:683/1680 train_time:59610ms step_avg:87.28ms
step:684/1680 train_time:59699ms step_avg:87.28ms
step:685/1680 train_time:59787ms step_avg:87.28ms
step:686/1680 train_time:59874ms step_avg:87.28ms
step:687/1680 train_time:59962ms step_avg:87.28ms
step:688/1680 train_time:60050ms step_avg:87.28ms
step:689/1680 train_time:60138ms step_avg:87.28ms
step:690/1680 train_time:60226ms step_avg:87.28ms
step:691/1680 train_time:60314ms step_avg:87.28ms
step:692/1680 train_time:60401ms step_avg:87.29ms
step:693/1680 train_time:60490ms step_avg:87.29ms
step:694/1680 train_time:60578ms step_avg:87.29ms
step:695/1680 train_time:60665ms step_avg:87.29ms
step:696/1680 train_time:60754ms step_avg:87.29ms
step:697/1680 train_time:60843ms step_avg:87.29ms
step:698/1680 train_time:60931ms step_avg:87.29ms
step:699/1680 train_time:61019ms step_avg:87.29ms
step:700/1680 train_time:61107ms step_avg:87.30ms
step:701/1680 train_time:61195ms step_avg:87.30ms
step:702/1680 train_time:61283ms step_avg:87.30ms
step:703/1680 train_time:61370ms step_avg:87.30ms
step:704/1680 train_time:61458ms step_avg:87.30ms
step:705/1680 train_time:61546ms step_avg:87.30ms
step:706/1680 train_time:61634ms step_avg:87.30ms
step:707/1680 train_time:61722ms step_avg:87.30ms
step:708/1680 train_time:61810ms step_avg:87.30ms
step:709/1680 train_time:61898ms step_avg:87.30ms
step:710/1680 train_time:61985ms step_avg:87.30ms
step:711/1680 train_time:62073ms step_avg:87.30ms
step:712/1680 train_time:62161ms step_avg:87.30ms
step:713/1680 train_time:62249ms step_avg:87.31ms
step:714/1680 train_time:62338ms step_avg:87.31ms
step:715/1680 train_time:62425ms step_avg:87.31ms
step:716/1680 train_time:62513ms step_avg:87.31ms
step:717/1680 train_time:62601ms step_avg:87.31ms
step:718/1680 train_time:62689ms step_avg:87.31ms
step:719/1680 train_time:62777ms step_avg:87.31ms
step:720/1680 train_time:62864ms step_avg:87.31ms
step:721/1680 train_time:62952ms step_avg:87.31ms
step:722/1680 train_time:63040ms step_avg:87.31ms
step:723/1680 train_time:63128ms step_avg:87.31ms
step:724/1680 train_time:63216ms step_avg:87.31ms
step:725/1680 train_time:63304ms step_avg:87.32ms
step:726/1680 train_time:63392ms step_avg:87.32ms
step:727/1680 train_time:63479ms step_avg:87.32ms
step:728/1680 train_time:63567ms step_avg:87.32ms
step:729/1680 train_time:63655ms step_avg:87.32ms
step:730/1680 train_time:63743ms step_avg:87.32ms
step:731/1680 train_time:63831ms step_avg:87.32ms
step:732/1680 train_time:63919ms step_avg:87.32ms
step:733/1680 train_time:64007ms step_avg:87.32ms
step:734/1680 train_time:64095ms step_avg:87.32ms
step:735/1680 train_time:64183ms step_avg:87.32ms
step:736/1680 train_time:64272ms step_avg:87.33ms
step:737/1680 train_time:64360ms step_avg:87.33ms
step:738/1680 train_time:64449ms step_avg:87.33ms
step:739/1680 train_time:64537ms step_avg:87.33ms
step:740/1680 train_time:64625ms step_avg:87.33ms
step:741/1680 train_time:64713ms step_avg:87.33ms
step:742/1680 train_time:64801ms step_avg:87.33ms
step:743/1680 train_time:64889ms step_avg:87.33ms
step:744/1680 train_time:64976ms step_avg:87.33ms
step:745/1680 train_time:65065ms step_avg:87.34ms
step:746/1680 train_time:65152ms step_avg:87.34ms
step:747/1680 train_time:65240ms step_avg:87.34ms
step:748/1680 train_time:65329ms step_avg:87.34ms
step:749/1680 train_time:65417ms step_avg:87.34ms
step:750/1680 train_time:65504ms step_avg:87.34ms
step:750/1680 val_loss:3.5643 train_time:65594ms step_avg:87.46ms
step:751/1680 train_time:65613ms step_avg:87.37ms
step:752/1680 train_time:65684ms step_avg:87.35ms
step:753/1680 train_time:65775ms step_avg:87.35ms
step:754/1680 train_time:65865ms step_avg:87.35ms
step:755/1680 train_time:65953ms step_avg:87.36ms
step:756/1680 train_time:66040ms step_avg:87.35ms
step:757/1680 train_time:66127ms step_avg:87.35ms
step:758/1680 train_time:66214ms step_avg:87.35ms
step:759/1680 train_time:66301ms step_avg:87.35ms
step:760/1680 train_time:66388ms step_avg:87.35ms
step:761/1680 train_time:66475ms step_avg:87.35ms
step:762/1680 train_time:66564ms step_avg:87.35ms
step:763/1680 train_time:66653ms step_avg:87.36ms
step:764/1680 train_time:66743ms step_avg:87.36ms
step:765/1680 train_time:66832ms step_avg:87.36ms
step:766/1680 train_time:66921ms step_avg:87.36ms
step:767/1680 train_time:67009ms step_avg:87.36ms
step:768/1680 train_time:67096ms step_avg:87.36ms
step:769/1680 train_time:67183ms step_avg:87.36ms
step:770/1680 train_time:67270ms step_avg:87.36ms
step:771/1680 train_time:67357ms step_avg:87.36ms
step:772/1680 train_time:67445ms step_avg:87.36ms
step:773/1680 train_time:67533ms step_avg:87.36ms
step:774/1680 train_time:67621ms step_avg:87.37ms
step:775/1680 train_time:67710ms step_avg:87.37ms
step:776/1680 train_time:67799ms step_avg:87.37ms
step:777/1680 train_time:67888ms step_avg:87.37ms
step:778/1680 train_time:67976ms step_avg:87.37ms
step:779/1680 train_time:68065ms step_avg:87.37ms
step:780/1680 train_time:68153ms step_avg:87.38ms
step:781/1680 train_time:68240ms step_avg:87.38ms
step:782/1680 train_time:68327ms step_avg:87.38ms
step:783/1680 train_time:68415ms step_avg:87.38ms
step:784/1680 train_time:68503ms step_avg:87.38ms
step:785/1680 train_time:68591ms step_avg:87.38ms
step:786/1680 train_time:68680ms step_avg:87.38ms
step:787/1680 train_time:68769ms step_avg:87.38ms
step:788/1680 train_time:68857ms step_avg:87.38ms
step:789/1680 train_time:68945ms step_avg:87.38ms
step:790/1680 train_time:69034ms step_avg:87.38ms
step:791/1680 train_time:69121ms step_avg:87.38ms
step:792/1680 train_time:69208ms step_avg:87.38ms
step:793/1680 train_time:69296ms step_avg:87.38ms
step:794/1680 train_time:69383ms step_avg:87.38ms
step:795/1680 train_time:69471ms step_avg:87.39ms
step:796/1680 train_time:69559ms step_avg:87.39ms
step:797/1680 train_time:69648ms step_avg:87.39ms
step:798/1680 train_time:69736ms step_avg:87.39ms
step:799/1680 train_time:69824ms step_avg:87.39ms
step:800/1680 train_time:69912ms step_avg:87.39ms
step:801/1680 train_time:70000ms step_avg:87.39ms
step:802/1680 train_time:70087ms step_avg:87.39ms
step:803/1680 train_time:70175ms step_avg:87.39ms
step:804/1680 train_time:70263ms step_avg:87.39ms
step:805/1680 train_time:70351ms step_avg:87.39ms
step:806/1680 train_time:70438ms step_avg:87.39ms
step:807/1680 train_time:70526ms step_avg:87.39ms
step:808/1680 train_time:70614ms step_avg:87.39ms
step:809/1680 train_time:70702ms step_avg:87.39ms
step:810/1680 train_time:70791ms step_avg:87.40ms
step:811/1680 train_time:70879ms step_avg:87.40ms
step:812/1680 train_time:70967ms step_avg:87.40ms
step:813/1680 train_time:71056ms step_avg:87.40ms
step:814/1680 train_time:71144ms step_avg:87.40ms
step:815/1680 train_time:71231ms step_avg:87.40ms
step:816/1680 train_time:71319ms step_avg:87.40ms
step:817/1680 train_time:71406ms step_avg:87.40ms
step:818/1680 train_time:71494ms step_avg:87.40ms
step:819/1680 train_time:71582ms step_avg:87.40ms
step:820/1680 train_time:71671ms step_avg:87.40ms
step:821/1680 train_time:71759ms step_avg:87.40ms
step:822/1680 train_time:71848ms step_avg:87.41ms
step:823/1680 train_time:71936ms step_avg:87.41ms
step:824/1680 train_time:72024ms step_avg:87.41ms
step:825/1680 train_time:72112ms step_avg:87.41ms
step:826/1680 train_time:72200ms step_avg:87.41ms
step:827/1680 train_time:72288ms step_avg:87.41ms
step:828/1680 train_time:72376ms step_avg:87.41ms
step:829/1680 train_time:72464ms step_avg:87.41ms
step:830/1680 train_time:72552ms step_avg:87.41ms
step:831/1680 train_time:72639ms step_avg:87.41ms
step:832/1680 train_time:72727ms step_avg:87.41ms
step:833/1680 train_time:72815ms step_avg:87.41ms
step:834/1680 train_time:72903ms step_avg:87.41ms
step:835/1680 train_time:72991ms step_avg:87.41ms
step:836/1680 train_time:73079ms step_avg:87.42ms
step:837/1680 train_time:73167ms step_avg:87.42ms
step:838/1680 train_time:73255ms step_avg:87.42ms
step:839/1680 train_time:73343ms step_avg:87.42ms
step:840/1680 train_time:73432ms step_avg:87.42ms
step:841/1680 train_time:73520ms step_avg:87.42ms
step:842/1680 train_time:73608ms step_avg:87.42ms
step:843/1680 train_time:73696ms step_avg:87.42ms
step:844/1680 train_time:73784ms step_avg:87.42ms
step:845/1680 train_time:73873ms step_avg:87.42ms
step:846/1680 train_time:73960ms step_avg:87.42ms
step:847/1680 train_time:74049ms step_avg:87.43ms
step:848/1680 train_time:74137ms step_avg:87.43ms
step:849/1680 train_time:74225ms step_avg:87.43ms
step:850/1680 train_time:74312ms step_avg:87.43ms
step:851/1680 train_time:74401ms step_avg:87.43ms
step:852/1680 train_time:74489ms step_avg:87.43ms
step:853/1680 train_time:74577ms step_avg:87.43ms
step:854/1680 train_time:74665ms step_avg:87.43ms
step:855/1680 train_time:74753ms step_avg:87.43ms
step:856/1680 train_time:74841ms step_avg:87.43ms
step:857/1680 train_time:74928ms step_avg:87.43ms
step:858/1680 train_time:75017ms step_avg:87.43ms
step:859/1680 train_time:75105ms step_avg:87.43ms
step:860/1680 train_time:75194ms step_avg:87.43ms
step:861/1680 train_time:75282ms step_avg:87.44ms
step:862/1680 train_time:75371ms step_avg:87.44ms
step:863/1680 train_time:75459ms step_avg:87.44ms
step:864/1680 train_time:75546ms step_avg:87.44ms
step:865/1680 train_time:75634ms step_avg:87.44ms
step:866/1680 train_time:75722ms step_avg:87.44ms
step:867/1680 train_time:75810ms step_avg:87.44ms
step:868/1680 train_time:75898ms step_avg:87.44ms
step:869/1680 train_time:75987ms step_avg:87.44ms
step:870/1680 train_time:76075ms step_avg:87.44ms
step:871/1680 train_time:76163ms step_avg:87.44ms
step:872/1680 train_time:76252ms step_avg:87.44ms
step:873/1680 train_time:76339ms step_avg:87.44ms
step:874/1680 train_time:76427ms step_avg:87.45ms
step:875/1680 train_time:76515ms step_avg:87.45ms
step:875/1680 val_loss:3.5204 train_time:76605ms step_avg:87.55ms
step:876/1680 train_time:76623ms step_avg:87.47ms
step:877/1680 train_time:76696ms step_avg:87.45ms
step:878/1680 train_time:76789ms step_avg:87.46ms
step:879/1680 train_time:76878ms step_avg:87.46ms
step:880/1680 train_time:76965ms step_avg:87.46ms
step:881/1680 train_time:77052ms step_avg:87.46ms
step:882/1680 train_time:77139ms step_avg:87.46ms
step:883/1680 train_time:77226ms step_avg:87.46ms
step:884/1680 train_time:77313ms step_avg:87.46ms
step:885/1680 train_time:77401ms step_avg:87.46ms
step:886/1680 train_time:77488ms step_avg:87.46ms
step:887/1680 train_time:77576ms step_avg:87.46ms
step:888/1680 train_time:77667ms step_avg:87.46ms
step:889/1680 train_time:77758ms step_avg:87.47ms
step:890/1680 train_time:77847ms step_avg:87.47ms
step:891/1680 train_time:77935ms step_avg:87.47ms
step:892/1680 train_time:78023ms step_avg:87.47ms
step:893/1680 train_time:78111ms step_avg:87.47ms
step:894/1680 train_time:78198ms step_avg:87.47ms
step:895/1680 train_time:78286ms step_avg:87.47ms
step:896/1680 train_time:78373ms step_avg:87.47ms
step:897/1680 train_time:78460ms step_avg:87.47ms
step:898/1680 train_time:78548ms step_avg:87.47ms
step:899/1680 train_time:78637ms step_avg:87.47ms
step:900/1680 train_time:78727ms step_avg:87.47ms
step:901/1680 train_time:78816ms step_avg:87.48ms
step:902/1680 train_time:78906ms step_avg:87.48ms
step:903/1680 train_time:78995ms step_avg:87.48ms
step:904/1680 train_time:79083ms step_avg:87.48ms
step:905/1680 train_time:79170ms step_avg:87.48ms
step:906/1680 train_time:79258ms step_avg:87.48ms
step:907/1680 train_time:79346ms step_avg:87.48ms
step:908/1680 train_time:79433ms step_avg:87.48ms
step:909/1680 train_time:79521ms step_avg:87.48ms
step:910/1680 train_time:79609ms step_avg:87.48ms
step:911/1680 train_time:79699ms step_avg:87.48ms
step:912/1680 train_time:79788ms step_avg:87.49ms
step:913/1680 train_time:79876ms step_avg:87.49ms
step:914/1680 train_time:79964ms step_avg:87.49ms
step:915/1680 train_time:80052ms step_avg:87.49ms
step:916/1680 train_time:80140ms step_avg:87.49ms
step:917/1680 train_time:80228ms step_avg:87.49ms
step:918/1680 train_time:80315ms step_avg:87.49ms
step:919/1680 train_time:80402ms step_avg:87.49ms
step:920/1680 train_time:80490ms step_avg:87.49ms
step:921/1680 train_time:80578ms step_avg:87.49ms
step:922/1680 train_time:80667ms step_avg:87.49ms
step:923/1680 train_time:80755ms step_avg:87.49ms
step:924/1680 train_time:80844ms step_avg:87.49ms
step:925/1680 train_time:80932ms step_avg:87.49ms
step:926/1680 train_time:81020ms step_avg:87.49ms
step:927/1680 train_time:81108ms step_avg:87.50ms
step:928/1680 train_time:81197ms step_avg:87.50ms
step:929/1680 train_time:81285ms step_avg:87.50ms
step:930/1680 train_time:81372ms step_avg:87.50ms
step:931/1680 train_time:81460ms step_avg:87.50ms
step:932/1680 train_time:81548ms step_avg:87.50ms
step:933/1680 train_time:81636ms step_avg:87.50ms
step:934/1680 train_time:81725ms step_avg:87.50ms
step:935/1680 train_time:81813ms step_avg:87.50ms
step:936/1680 train_time:81902ms step_avg:87.50ms
step:937/1680 train_time:81990ms step_avg:87.50ms
step:938/1680 train_time:82078ms step_avg:87.50ms
step:939/1680 train_time:82166ms step_avg:87.50ms
step:940/1680 train_time:82255ms step_avg:87.51ms
step:941/1680 train_time:82343ms step_avg:87.51ms
step:942/1680 train_time:82430ms step_avg:87.51ms
step:943/1680 train_time:82519ms step_avg:87.51ms
step:944/1680 train_time:82607ms step_avg:87.51ms
step:945/1680 train_time:82695ms step_avg:87.51ms
step:946/1680 train_time:82783ms step_avg:87.51ms
step:947/1680 train_time:82872ms step_avg:87.51ms
step:948/1680 train_time:82960ms step_avg:87.51ms
step:949/1680 train_time:83048ms step_avg:87.51ms
step:950/1680 train_time:83135ms step_avg:87.51ms
step:951/1680 train_time:83223ms step_avg:87.51ms
step:952/1680 train_time:83311ms step_avg:87.51ms
step:953/1680 train_time:83400ms step_avg:87.51ms
step:954/1680 train_time:83488ms step_avg:87.51ms
step:955/1680 train_time:83575ms step_avg:87.51ms
step:956/1680 train_time:83663ms step_avg:87.51ms
step:957/1680 train_time:83751ms step_avg:87.51ms
step:958/1680 train_time:83839ms step_avg:87.51ms
step:959/1680 train_time:83927ms step_avg:87.52ms
step:960/1680 train_time:84016ms step_avg:87.52ms
step:961/1680 train_time:84104ms step_avg:87.52ms
step:962/1680 train_time:84192ms step_avg:87.52ms
step:963/1680 train_time:84280ms step_avg:87.52ms
step:964/1680 train_time:84368ms step_avg:87.52ms
step:965/1680 train_time:84456ms step_avg:87.52ms
step:966/1680 train_time:84544ms step_avg:87.52ms
step:967/1680 train_time:84632ms step_avg:87.52ms
step:968/1680 train_time:84720ms step_avg:87.52ms
step:969/1680 train_time:84808ms step_avg:87.52ms
step:970/1680 train_time:84896ms step_avg:87.52ms
step:971/1680 train_time:84984ms step_avg:87.52ms
step:972/1680 train_time:85072ms step_avg:87.52ms
step:973/1680 train_time:85160ms step_avg:87.52ms
step:974/1680 train_time:85248ms step_avg:87.52ms
step:975/1680 train_time:85336ms step_avg:87.52ms
step:976/1680 train_time:85424ms step_avg:87.52ms
step:977/1680 train_time:85512ms step_avg:87.53ms
step:978/1680 train_time:85601ms step_avg:87.53ms
step:979/1680 train_time:85689ms step_avg:87.53ms
step:980/1680 train_time:85777ms step_avg:87.53ms
step:981/1680 train_time:85865ms step_avg:87.53ms
step:982/1680 train_time:85953ms step_avg:87.53ms
step:983/1680 train_time:86042ms step_avg:87.53ms
step:984/1680 train_time:86129ms step_avg:87.53ms
step:985/1680 train_time:86218ms step_avg:87.53ms
step:986/1680 train_time:86306ms step_avg:87.53ms
step:987/1680 train_time:86394ms step_avg:87.53ms
step:988/1680 train_time:86482ms step_avg:87.53ms
step:989/1680 train_time:86570ms step_avg:87.53ms
step:990/1680 train_time:86657ms step_avg:87.53ms
step:991/1680 train_time:86747ms step_avg:87.53ms
step:992/1680 train_time:86835ms step_avg:87.54ms
step:993/1680 train_time:86923ms step_avg:87.54ms
step:994/1680 train_time:87011ms step_avg:87.54ms
step:995/1680 train_time:87100ms step_avg:87.54ms
step:996/1680 train_time:87187ms step_avg:87.54ms
step:997/1680 train_time:87275ms step_avg:87.54ms
step:998/1680 train_time:87363ms step_avg:87.54ms
step:999/1680 train_time:87451ms step_avg:87.54ms
step:1000/1680 train_time:87539ms step_avg:87.54ms
step:1000/1680 val_loss:3.4694 train_time:87628ms step_avg:87.63ms
step:1001/1680 train_time:87647ms step_avg:87.56ms
step:1002/1680 train_time:87720ms step_avg:87.54ms
step:1003/1680 train_time:87810ms step_avg:87.55ms
step:1004/1680 train_time:87899ms step_avg:87.55ms
step:1005/1680 train_time:87987ms step_avg:87.55ms
step:1006/1680 train_time:88074ms step_avg:87.55ms
step:1007/1680 train_time:88161ms step_avg:87.55ms
step:1008/1680 train_time:88248ms step_avg:87.55ms
step:1009/1680 train_time:88335ms step_avg:87.55ms
step:1010/1680 train_time:88422ms step_avg:87.55ms
step:1011/1680 train_time:88510ms step_avg:87.55ms
step:1012/1680 train_time:88598ms step_avg:87.55ms
step:1013/1680 train_time:88689ms step_avg:87.55ms
step:1014/1680 train_time:88779ms step_avg:87.55ms
step:1015/1680 train_time:88870ms step_avg:87.56ms
step:1016/1680 train_time:88958ms step_avg:87.56ms
step:1017/1680 train_time:89047ms step_avg:87.56ms
step:1018/1680 train_time:89134ms step_avg:87.56ms
step:1019/1680 train_time:89221ms step_avg:87.56ms
step:1020/1680 train_time:89308ms step_avg:87.56ms
step:1021/1680 train_time:89395ms step_avg:87.56ms
step:1022/1680 train_time:89482ms step_avg:87.56ms
step:1023/1680 train_time:89570ms step_avg:87.56ms
step:1024/1680 train_time:89659ms step_avg:87.56ms
step:1025/1680 train_time:89748ms step_avg:87.56ms
step:1026/1680 train_time:89837ms step_avg:87.56ms
step:1027/1680 train_time:89926ms step_avg:87.56ms
step:1028/1680 train_time:90014ms step_avg:87.56ms
step:1029/1680 train_time:90102ms step_avg:87.56ms
step:1030/1680 train_time:90189ms step_avg:87.56ms
step:1031/1680 train_time:90277ms step_avg:87.56ms
step:1032/1680 train_time:90364ms step_avg:87.56ms
step:1033/1680 train_time:90452ms step_avg:87.56ms
step:1034/1680 train_time:90540ms step_avg:87.56ms
step:1035/1680 train_time:90629ms step_avg:87.56ms
step:1036/1680 train_time:90717ms step_avg:87.56ms
step:1037/1680 train_time:90806ms step_avg:87.57ms
step:1038/1680 train_time:90895ms step_avg:87.57ms
step:1039/1680 train_time:90984ms step_avg:87.57ms
step:1040/1680 train_time:91072ms step_avg:87.57ms
step:1041/1680 train_time:91160ms step_avg:87.57ms
step:1042/1680 train_time:91247ms step_avg:87.57ms
step:1043/1680 train_time:91334ms step_avg:87.57ms
step:1044/1680 train_time:91422ms step_avg:87.57ms
step:1045/1680 train_time:91510ms step_avg:87.57ms
step:1046/1680 train_time:91598ms step_avg:87.57ms
step:1047/1680 train_time:91687ms step_avg:87.57ms
step:1048/1680 train_time:91775ms step_avg:87.57ms
step:1049/1680 train_time:91864ms step_avg:87.57ms
step:1050/1680 train_time:91953ms step_avg:87.57ms
step:1051/1680 train_time:92041ms step_avg:87.57ms
step:1052/1680 train_time:92130ms step_avg:87.58ms
step:1053/1680 train_time:92217ms step_avg:87.58ms
step:1054/1680 train_time:92305ms step_avg:87.58ms
step:1055/1680 train_time:92393ms step_avg:87.58ms
step:1056/1680 train_time:92480ms step_avg:87.58ms
step:1057/1680 train_time:92568ms step_avg:87.58ms
step:1058/1680 train_time:92656ms step_avg:87.58ms
step:1059/1680 train_time:92744ms step_avg:87.58ms
step:1060/1680 train_time:92832ms step_avg:87.58ms
step:1061/1680 train_time:92920ms step_avg:87.58ms
step:1062/1680 train_time:93008ms step_avg:87.58ms
step:1063/1680 train_time:93097ms step_avg:87.58ms
step:1064/1680 train_time:93185ms step_avg:87.58ms
step:1065/1680 train_time:93273ms step_avg:87.58ms
step:1066/1680 train_time:93361ms step_avg:87.58ms
step:1067/1680 train_time:93449ms step_avg:87.58ms
step:1068/1680 train_time:93536ms step_avg:87.58ms
step:1069/1680 train_time:93625ms step_avg:87.58ms
step:1070/1680 train_time:93712ms step_avg:87.58ms
step:1071/1680 train_time:93800ms step_avg:87.58ms
step:1072/1680 train_time:93889ms step_avg:87.58ms
step:1073/1680 train_time:93978ms step_avg:87.58ms
step:1074/1680 train_time:94067ms step_avg:87.59ms
step:1075/1680 train_time:94155ms step_avg:87.59ms
step:1076/1680 train_time:94243ms step_avg:87.59ms
step:1077/1680 train_time:94331ms step_avg:87.59ms
step:1078/1680 train_time:94418ms step_avg:87.59ms
step:1079/1680 train_time:94506ms step_avg:87.59ms
step:1080/1680 train_time:94594ms step_avg:87.59ms
step:1081/1680 train_time:94682ms step_avg:87.59ms
step:1082/1680 train_time:94770ms step_avg:87.59ms
step:1083/1680 train_time:94859ms step_avg:87.59ms
step:1084/1680 train_time:94947ms step_avg:87.59ms
step:1085/1680 train_time:95037ms step_avg:87.59ms
step:1086/1680 train_time:95125ms step_avg:87.59ms
step:1087/1680 train_time:95213ms step_avg:87.59ms
step:1088/1680 train_time:95300ms step_avg:87.59ms
step:1089/1680 train_time:95388ms step_avg:87.59ms
step:1090/1680 train_time:95476ms step_avg:87.59ms
step:1091/1680 train_time:95564ms step_avg:87.59ms
step:1092/1680 train_time:95653ms step_avg:87.59ms
step:1093/1680 train_time:95740ms step_avg:87.59ms
step:1094/1680 train_time:95828ms step_avg:87.59ms
step:1095/1680 train_time:95916ms step_avg:87.59ms
step:1096/1680 train_time:96005ms step_avg:87.60ms
step:1097/1680 train_time:96094ms step_avg:87.60ms
step:1098/1680 train_time:96183ms step_avg:87.60ms
step:1099/1680 train_time:96272ms step_avg:87.60ms
step:1100/1680 train_time:96361ms step_avg:87.60ms
step:1101/1680 train_time:96449ms step_avg:87.60ms
step:1102/1680 train_time:96537ms step_avg:87.60ms
step:1103/1680 train_time:96625ms step_avg:87.60ms
step:1104/1680 train_time:96714ms step_avg:87.60ms
step:1105/1680 train_time:96803ms step_avg:87.60ms
step:1106/1680 train_time:96893ms step_avg:87.61ms
step:1107/1680 train_time:96982ms step_avg:87.61ms
step:1108/1680 train_time:97072ms step_avg:87.61ms
step:1109/1680 train_time:97160ms step_avg:87.61ms
step:1110/1680 train_time:97250ms step_avg:87.61ms
step:1111/1680 train_time:97340ms step_avg:87.61ms
step:1112/1680 train_time:97428ms step_avg:87.62ms
step:1113/1680 train_time:97517ms step_avg:87.62ms
step:1114/1680 train_time:97606ms step_avg:87.62ms
step:1115/1680 train_time:97694ms step_avg:87.62ms
step:1116/1680 train_time:97783ms step_avg:87.62ms
step:1117/1680 train_time:97873ms step_avg:87.62ms
step:1118/1680 train_time:97962ms step_avg:87.62ms
step:1119/1680 train_time:98051ms step_avg:87.62ms
step:1120/1680 train_time:98139ms step_avg:87.62ms
step:1121/1680 train_time:98228ms step_avg:87.63ms
step:1122/1680 train_time:98317ms step_avg:87.63ms
step:1123/1680 train_time:98406ms step_avg:87.63ms
step:1124/1680 train_time:98495ms step_avg:87.63ms
step:1125/1680 train_time:98585ms step_avg:87.63ms
step:1125/1680 val_loss:3.4151 train_time:98675ms step_avg:87.71ms
step:1126/1680 train_time:98695ms step_avg:87.65ms
step:1127/1680 train_time:98765ms step_avg:87.64ms
step:1128/1680 train_time:98857ms step_avg:87.64ms
step:1129/1680 train_time:98950ms step_avg:87.64ms
step:1130/1680 train_time:99040ms step_avg:87.65ms
step:1131/1680 train_time:99128ms step_avg:87.65ms
step:1132/1680 train_time:99216ms step_avg:87.65ms
step:1133/1680 train_time:99304ms step_avg:87.65ms
step:1134/1680 train_time:99391ms step_avg:87.65ms
step:1135/1680 train_time:99479ms step_avg:87.65ms
step:1136/1680 train_time:99567ms step_avg:87.65ms
step:1137/1680 train_time:99657ms step_avg:87.65ms
step:1138/1680 train_time:99748ms step_avg:87.65ms
step:1139/1680 train_time:99839ms step_avg:87.65ms
step:1140/1680 train_time:99930ms step_avg:87.66ms
step:1141/1680 train_time:100018ms step_avg:87.66ms
step:1142/1680 train_time:100107ms step_avg:87.66ms
step:1143/1680 train_time:100195ms step_avg:87.66ms
step:1144/1680 train_time:100284ms step_avg:87.66ms
step:1145/1680 train_time:100372ms step_avg:87.66ms
step:1146/1680 train_time:100460ms step_avg:87.66ms
step:1147/1680 train_time:100548ms step_avg:87.66ms
step:1148/1680 train_time:100637ms step_avg:87.66ms
step:1149/1680 train_time:100727ms step_avg:87.66ms
step:1150/1680 train_time:100817ms step_avg:87.67ms
step:1151/1680 train_time:100908ms step_avg:87.67ms
step:1152/1680 train_time:100997ms step_avg:87.67ms
step:1153/1680 train_time:101086ms step_avg:87.67ms
step:1154/1680 train_time:101174ms step_avg:87.67ms
step:1155/1680 train_time:101264ms step_avg:87.67ms
step:1156/1680 train_time:101352ms step_avg:87.68ms
step:1157/1680 train_time:101441ms step_avg:87.68ms
step:1158/1680 train_time:101529ms step_avg:87.68ms
step:1159/1680 train_time:101618ms step_avg:87.68ms
step:1160/1680 train_time:101707ms step_avg:87.68ms
step:1161/1680 train_time:101797ms step_avg:87.68ms
step:1162/1680 train_time:101887ms step_avg:87.68ms
step:1163/1680 train_time:101976ms step_avg:87.68ms
step:1164/1680 train_time:102066ms step_avg:87.69ms
step:1165/1680 train_time:102155ms step_avg:87.69ms
step:1166/1680 train_time:102244ms step_avg:87.69ms
step:1167/1680 train_time:102333ms step_avg:87.69ms
step:1168/1680 train_time:102422ms step_avg:87.69ms
step:1169/1680 train_time:102511ms step_avg:87.69ms
step:1170/1680 train_time:102599ms step_avg:87.69ms
step:1171/1680 train_time:102688ms step_avg:87.69ms
step:1172/1680 train_time:102777ms step_avg:87.69ms
step:1173/1680 train_time:102866ms step_avg:87.69ms
step:1174/1680 train_time:102955ms step_avg:87.70ms
step:1175/1680 train_time:103045ms step_avg:87.70ms
step:1176/1680 train_time:103133ms step_avg:87.70ms
step:1177/1680 train_time:103222ms step_avg:87.70ms
step:1178/1680 train_time:103311ms step_avg:87.70ms
step:1179/1680 train_time:103399ms step_avg:87.70ms
step:1180/1680 train_time:103488ms step_avg:87.70ms
step:1181/1680 train_time:103577ms step_avg:87.70ms
step:1182/1680 train_time:103666ms step_avg:87.70ms
step:1183/1680 train_time:103755ms step_avg:87.71ms
step:1184/1680 train_time:103844ms step_avg:87.71ms
step:1185/1680 train_time:103933ms step_avg:87.71ms
step:1186/1680 train_time:104022ms step_avg:87.71ms
step:1187/1680 train_time:104112ms step_avg:87.71ms
step:1188/1680 train_time:104200ms step_avg:87.71ms
step:1189/1680 train_time:104290ms step_avg:87.71ms
step:1190/1680 train_time:104379ms step_avg:87.71ms
step:1191/1680 train_time:104467ms step_avg:87.71ms
step:1192/1680 train_time:104556ms step_avg:87.71ms
step:1193/1680 train_time:104645ms step_avg:87.72ms
step:1194/1680 train_time:104734ms step_avg:87.72ms
step:1195/1680 train_time:104823ms step_avg:87.72ms
step:1196/1680 train_time:104912ms step_avg:87.72ms
step:1197/1680 train_time:105001ms step_avg:87.72ms
step:1198/1680 train_time:105090ms step_avg:87.72ms
step:1199/1680 train_time:105178ms step_avg:87.72ms
step:1200/1680 train_time:105267ms step_avg:87.72ms
step:1201/1680 train_time:105356ms step_avg:87.72ms
step:1202/1680 train_time:105445ms step_avg:87.72ms
step:1203/1680 train_time:105534ms step_avg:87.73ms
step:1204/1680 train_time:105623ms step_avg:87.73ms
step:1205/1680 train_time:105713ms step_avg:87.73ms
step:1206/1680 train_time:105801ms step_avg:87.73ms
step:1207/1680 train_time:105891ms step_avg:87.73ms
step:1208/1680 train_time:105980ms step_avg:87.73ms
step:1209/1680 train_time:106068ms step_avg:87.73ms
step:1210/1680 train_time:106156ms step_avg:87.73ms
step:1211/1680 train_time:106245ms step_avg:87.73ms
step:1212/1680 train_time:106335ms step_avg:87.73ms
step:1213/1680 train_time:106424ms step_avg:87.74ms
step:1214/1680 train_time:106512ms step_avg:87.74ms
step:1215/1680 train_time:106601ms step_avg:87.74ms
step:1216/1680 train_time:106690ms step_avg:87.74ms
step:1217/1680 train_time:106779ms step_avg:87.74ms
step:1218/1680 train_time:106868ms step_avg:87.74ms
step:1219/1680 train_time:106957ms step_avg:87.74ms
step:1220/1680 train_time:107047ms step_avg:87.74ms
step:1221/1680 train_time:107136ms step_avg:87.74ms
step:1222/1680 train_time:107225ms step_avg:87.75ms
step:1223/1680 train_time:107315ms step_avg:87.75ms
step:1224/1680 train_time:107404ms step_avg:87.75ms
step:1225/1680 train_time:107492ms step_avg:87.75ms
step:1226/1680 train_time:107581ms step_avg:87.75ms
step:1227/1680 train_time:107670ms step_avg:87.75ms
step:1228/1680 train_time:107759ms step_avg:87.75ms
step:1229/1680 train_time:107848ms step_avg:87.75ms
step:1230/1680 train_time:107937ms step_avg:87.75ms
step:1231/1680 train_time:108026ms step_avg:87.75ms
step:1232/1680 train_time:108115ms step_avg:87.76ms
step:1233/1680 train_time:108204ms step_avg:87.76ms
step:1234/1680 train_time:108292ms step_avg:87.76ms
step:1235/1680 train_time:108381ms step_avg:87.76ms
step:1236/1680 train_time:108469ms step_avg:87.76ms
step:1237/1680 train_time:108558ms step_avg:87.76ms
step:1238/1680 train_time:108647ms step_avg:87.76ms
step:1239/1680 train_time:108736ms step_avg:87.76ms
step:1240/1680 train_time:108825ms step_avg:87.76ms
step:1241/1680 train_time:108915ms step_avg:87.76ms
step:1242/1680 train_time:109004ms step_avg:87.77ms
step:1243/1680 train_time:109094ms step_avg:87.77ms
step:1244/1680 train_time:109184ms step_avg:87.77ms
step:1245/1680 train_time:109273ms step_avg:87.77ms
step:1246/1680 train_time:109362ms step_avg:87.77ms
step:1247/1680 train_time:109451ms step_avg:87.77ms
step:1248/1680 train_time:109539ms step_avg:87.77ms
step:1249/1680 train_time:109628ms step_avg:87.77ms
step:1250/1680 train_time:109717ms step_avg:87.77ms
step:1250/1680 val_loss:3.3778 train_time:109807ms step_avg:87.85ms
step:1251/1680 train_time:109826ms step_avg:87.79ms
step:1252/1680 train_time:109899ms step_avg:87.78ms
step:1253/1680 train_time:109992ms step_avg:87.78ms
step:1254/1680 train_time:110082ms step_avg:87.78ms
step:1255/1680 train_time:110170ms step_avg:87.79ms
step:1256/1680 train_time:110259ms step_avg:87.79ms
step:1257/1680 train_time:110347ms step_avg:87.79ms
step:1258/1680 train_time:110435ms step_avg:87.79ms
step:1259/1680 train_time:110523ms step_avg:87.79ms
step:1260/1680 train_time:110610ms step_avg:87.79ms
step:1261/1680 train_time:110697ms step_avg:87.79ms
step:1262/1680 train_time:110788ms step_avg:87.79ms
step:1263/1680 train_time:110879ms step_avg:87.79ms
step:1264/1680 train_time:110969ms step_avg:87.79ms
step:1265/1680 train_time:111058ms step_avg:87.79ms
step:1266/1680 train_time:111147ms step_avg:87.79ms
step:1267/1680 train_time:111235ms step_avg:87.79ms
step:1268/1680 train_time:111324ms step_avg:87.79ms
step:1269/1680 train_time:111412ms step_avg:87.79ms
step:1270/1680 train_time:111500ms step_avg:87.80ms
step:1271/1680 train_time:111588ms step_avg:87.80ms
step:1272/1680 train_time:111676ms step_avg:87.80ms
step:1273/1680 train_time:111766ms step_avg:87.80ms
step:1274/1680 train_time:111857ms step_avg:87.80ms
step:1275/1680 train_time:111947ms step_avg:87.80ms
step:1276/1680 train_time:112037ms step_avg:87.80ms
step:1277/1680 train_time:112126ms step_avg:87.80ms
step:1278/1680 train_time:112215ms step_avg:87.81ms
step:1279/1680 train_time:112303ms step_avg:87.81ms
step:1280/1680 train_time:112392ms step_avg:87.81ms
step:1281/1680 train_time:112480ms step_avg:87.81ms
step:1282/1680 train_time:112568ms step_avg:87.81ms
step:1283/1680 train_time:112657ms step_avg:87.81ms
step:1284/1680 train_time:112746ms step_avg:87.81ms
step:1285/1680 train_time:112836ms step_avg:87.81ms
step:1286/1680 train_time:112925ms step_avg:87.81ms
step:1287/1680 train_time:113015ms step_avg:87.81ms
step:1288/1680 train_time:113105ms step_avg:87.81ms
step:1289/1680 train_time:113193ms step_avg:87.81ms
step:1290/1680 train_time:113282ms step_avg:87.82ms
step:1291/1680 train_time:113372ms step_avg:87.82ms
step:1292/1680 train_time:113460ms step_avg:87.82ms
step:1293/1680 train_time:113549ms step_avg:87.82ms
step:1294/1680 train_time:113638ms step_avg:87.82ms
step:1295/1680 train_time:113727ms step_avg:87.82ms
step:1296/1680 train_time:113816ms step_avg:87.82ms
step:1297/1680 train_time:113906ms step_avg:87.82ms
step:1298/1680 train_time:113996ms step_avg:87.82ms
step:1299/1680 train_time:114085ms step_avg:87.83ms
step:1300/1680 train_time:114174ms step_avg:87.83ms
step:1301/1680 train_time:114263ms step_avg:87.83ms
step:1302/1680 train_time:114352ms step_avg:87.83ms
step:1303/1680 train_time:114441ms step_avg:87.83ms
step:1304/1680 train_time:114530ms step_avg:87.83ms
step:1305/1680 train_time:114618ms step_avg:87.83ms
step:1306/1680 train_time:114708ms step_avg:87.83ms
step:1307/1680 train_time:114796ms step_avg:87.83ms
step:1308/1680 train_time:114885ms step_avg:87.83ms
step:1309/1680 train_time:114976ms step_avg:87.83ms
step:1310/1680 train_time:115065ms step_avg:87.84ms
step:1311/1680 train_time:115154ms step_avg:87.84ms
step:1312/1680 train_time:115244ms step_avg:87.84ms
step:1313/1680 train_time:115333ms step_avg:87.84ms
step:1314/1680 train_time:115421ms step_avg:87.84ms
step:1315/1680 train_time:115510ms step_avg:87.84ms
step:1316/1680 train_time:115598ms step_avg:87.84ms
step:1317/1680 train_time:115687ms step_avg:87.84ms
step:1318/1680 train_time:115776ms step_avg:87.84ms
step:1319/1680 train_time:115866ms step_avg:87.84ms
step:1320/1680 train_time:115956ms step_avg:87.85ms
step:1321/1680 train_time:116045ms step_avg:87.85ms
step:1322/1680 train_time:116134ms step_avg:87.85ms
step:1323/1680 train_time:116224ms step_avg:87.85ms
step:1324/1680 train_time:116312ms step_avg:87.85ms
step:1325/1680 train_time:116401ms step_avg:87.85ms
step:1326/1680 train_time:116490ms step_avg:87.85ms
step:1327/1680 train_time:116578ms step_avg:87.85ms
step:1328/1680 train_time:116667ms step_avg:87.85ms
step:1329/1680 train_time:116757ms step_avg:87.85ms
step:1330/1680 train_time:116846ms step_avg:87.85ms
step:1331/1680 train_time:116935ms step_avg:87.85ms
step:1332/1680 train_time:117024ms step_avg:87.86ms
step:1333/1680 train_time:117112ms step_avg:87.86ms
step:1334/1680 train_time:117201ms step_avg:87.86ms
step:1335/1680 train_time:117290ms step_avg:87.86ms
step:1336/1680 train_time:117378ms step_avg:87.86ms
step:1337/1680 train_time:117467ms step_avg:87.86ms
step:1338/1680 train_time:117556ms step_avg:87.86ms
step:1339/1680 train_time:117645ms step_avg:87.86ms
step:1340/1680 train_time:117734ms step_avg:87.86ms
step:1341/1680 train_time:117822ms step_avg:87.86ms
step:1342/1680 train_time:117912ms step_avg:87.86ms
step:1343/1680 train_time:118000ms step_avg:87.86ms
step:1344/1680 train_time:118090ms step_avg:87.86ms
step:1345/1680 train_time:118179ms step_avg:87.87ms
step:1346/1680 train_time:118268ms step_avg:87.87ms
step:1347/1680 train_time:118357ms step_avg:87.87ms
step:1348/1680 train_time:118446ms step_avg:87.87ms
step:1349/1680 train_time:118536ms step_avg:87.87ms
step:1350/1680 train_time:118625ms step_avg:87.87ms
step:1351/1680 train_time:118713ms step_avg:87.87ms
step:1352/1680 train_time:118802ms step_avg:87.87ms
step:1353/1680 train_time:118891ms step_avg:87.87ms
step:1354/1680 train_time:118981ms step_avg:87.87ms
step:1355/1680 train_time:119070ms step_avg:87.87ms
step:1356/1680 train_time:119160ms step_avg:87.88ms
step:1357/1680 train_time:119249ms step_avg:87.88ms
step:1358/1680 train_time:119337ms step_avg:87.88ms
step:1359/1680 train_time:119426ms step_avg:87.88ms
step:1360/1680 train_time:119515ms step_avg:87.88ms
step:1361/1680 train_time:119604ms step_avg:87.88ms
step:1362/1680 train_time:119693ms step_avg:87.88ms
step:1363/1680 train_time:119781ms step_avg:87.88ms
step:1364/1680 train_time:119870ms step_avg:87.88ms
step:1365/1680 train_time:119959ms step_avg:87.88ms
step:1366/1680 train_time:120049ms step_avg:87.88ms
step:1367/1680 train_time:120138ms step_avg:87.88ms
step:1368/1680 train_time:120228ms step_avg:87.89ms
step:1369/1680 train_time:120316ms step_avg:87.89ms
step:1370/1680 train_time:120405ms step_avg:87.89ms
step:1371/1680 train_time:120495ms step_avg:87.89ms
step:1372/1680 train_time:120584ms step_avg:87.89ms
step:1373/1680 train_time:120674ms step_avg:87.89ms
step:1374/1680 train_time:120763ms step_avg:87.89ms
step:1375/1680 train_time:120853ms step_avg:87.89ms
step:1375/1680 val_loss:3.3427 train_time:120943ms step_avg:87.96ms
step:1376/1680 train_time:120961ms step_avg:87.91ms
step:1377/1680 train_time:121035ms step_avg:87.90ms
step:1378/1680 train_time:121127ms step_avg:87.90ms
step:1379/1680 train_time:121216ms step_avg:87.90ms
step:1380/1680 train_time:121304ms step_avg:87.90ms
step:1381/1680 train_time:121393ms step_avg:87.90ms
step:1382/1680 train_time:121480ms step_avg:87.90ms
step:1383/1680 train_time:121569ms step_avg:87.90ms
step:1384/1680 train_time:121656ms step_avg:87.90ms
step:1385/1680 train_time:121745ms step_avg:87.90ms
step:1386/1680 train_time:121833ms step_avg:87.90ms
step:1387/1680 train_time:121923ms step_avg:87.90ms
step:1388/1680 train_time:122016ms step_avg:87.91ms
step:1389/1680 train_time:122106ms step_avg:87.91ms
step:1390/1680 train_time:122196ms step_avg:87.91ms
step:1391/1680 train_time:122286ms step_avg:87.91ms
step:1392/1680 train_time:122375ms step_avg:87.91ms
step:1393/1680 train_time:122462ms step_avg:87.91ms
step:1394/1680 train_time:122551ms step_avg:87.91ms
step:1395/1680 train_time:122639ms step_avg:87.91ms
step:1396/1680 train_time:122727ms step_avg:87.91ms
step:1397/1680 train_time:122816ms step_avg:87.91ms
step:1398/1680 train_time:122906ms step_avg:87.92ms
step:1399/1680 train_time:122997ms step_avg:87.92ms
step:1400/1680 train_time:123087ms step_avg:87.92ms
step:1401/1680 train_time:123177ms step_avg:87.92ms
step:1402/1680 train_time:123267ms step_avg:87.92ms
step:1403/1680 train_time:123356ms step_avg:87.92ms
step:1404/1680 train_time:123444ms step_avg:87.92ms
step:1405/1680 train_time:123533ms step_avg:87.92ms
step:1406/1680 train_time:123621ms step_avg:87.92ms
step:1407/1680 train_time:123709ms step_avg:87.92ms
step:1408/1680 train_time:123798ms step_avg:87.92ms
step:1409/1680 train_time:123887ms step_avg:87.93ms
step:1410/1680 train_time:123978ms step_avg:87.93ms
step:1411/1680 train_time:124067ms step_avg:87.93ms
step:1412/1680 train_time:124157ms step_avg:87.93ms
step:1413/1680 train_time:124246ms step_avg:87.93ms
step:1414/1680 train_time:124335ms step_avg:87.93ms
step:1415/1680 train_time:124423ms step_avg:87.93ms
step:1416/1680 train_time:124512ms step_avg:87.93ms
step:1417/1680 train_time:124601ms step_avg:87.93ms
step:1418/1680 train_time:124690ms step_avg:87.93ms
step:1419/1680 train_time:124778ms step_avg:87.93ms
step:1420/1680 train_time:124867ms step_avg:87.93ms
step:1421/1680 train_time:124956ms step_avg:87.94ms
step:1422/1680 train_time:125045ms step_avg:87.94ms
step:1423/1680 train_time:125136ms step_avg:87.94ms
step:1424/1680 train_time:125225ms step_avg:87.94ms
step:1425/1680 train_time:125314ms step_avg:87.94ms
step:1426/1680 train_time:125403ms step_avg:87.94ms
step:1427/1680 train_time:125493ms step_avg:87.94ms
step:1428/1680 train_time:125581ms step_avg:87.94ms
step:1429/1680 train_time:125670ms step_avg:87.94ms
step:1430/1680 train_time:125758ms step_avg:87.94ms
step:1431/1680 train_time:125848ms step_avg:87.94ms
step:1432/1680 train_time:125938ms step_avg:87.95ms
step:1433/1680 train_time:126027ms step_avg:87.95ms
step:1434/1680 train_time:126116ms step_avg:87.95ms
step:1435/1680 train_time:126205ms step_avg:87.95ms
step:1436/1680 train_time:126296ms step_avg:87.95ms
step:1437/1680 train_time:126385ms step_avg:87.95ms
step:1438/1680 train_time:126474ms step_avg:87.95ms
step:1439/1680 train_time:126562ms step_avg:87.95ms
step:1440/1680 train_time:126650ms step_avg:87.95ms
step:1441/1680 train_time:126739ms step_avg:87.95ms
step:1442/1680 train_time:126828ms step_avg:87.95ms
step:1443/1680 train_time:126917ms step_avg:87.95ms
step:1444/1680 train_time:127006ms step_avg:87.95ms
step:1445/1680 train_time:127096ms step_avg:87.96ms
step:1446/1680 train_time:127185ms step_avg:87.96ms
step:1447/1680 train_time:127275ms step_avg:87.96ms
step:1448/1680 train_time:127363ms step_avg:87.96ms
step:1449/1680 train_time:127453ms step_avg:87.96ms
step:1450/1680 train_time:127541ms step_avg:87.96ms
step:1451/1680 train_time:127630ms step_avg:87.96ms
step:1452/1680 train_time:127718ms step_avg:87.96ms
step:1453/1680 train_time:127806ms step_avg:87.96ms
step:1454/1680 train_time:127896ms step_avg:87.96ms
step:1455/1680 train_time:127985ms step_avg:87.96ms
step:1456/1680 train_time:128074ms step_avg:87.96ms
step:1457/1680 train_time:128163ms step_avg:87.96ms
step:1458/1680 train_time:128251ms step_avg:87.96ms
step:1459/1680 train_time:128341ms step_avg:87.96ms
step:1460/1680 train_time:128430ms step_avg:87.97ms
step:1461/1680 train_time:128519ms step_avg:87.97ms
step:1462/1680 train_time:128609ms step_avg:87.97ms
step:1463/1680 train_time:128697ms step_avg:87.97ms
step:1464/1680 train_time:128786ms step_avg:87.97ms
step:1465/1680 train_time:128874ms step_avg:87.97ms
step:1466/1680 train_time:128962ms step_avg:87.97ms
step:1467/1680 train_time:129052ms step_avg:87.97ms
step:1468/1680 train_time:129141ms step_avg:87.97ms
step:1469/1680 train_time:129231ms step_avg:87.97ms
step:1470/1680 train_time:129319ms step_avg:87.97ms
step:1471/1680 train_time:129408ms step_avg:87.97ms
step:1472/1680 train_time:129498ms step_avg:87.97ms
step:1473/1680 train_time:129588ms step_avg:87.98ms
step:1474/1680 train_time:129676ms step_avg:87.98ms
step:1475/1680 train_time:129764ms step_avg:87.98ms
step:1476/1680 train_time:129853ms step_avg:87.98ms
step:1477/1680 train_time:129942ms step_avg:87.98ms
step:1478/1680 train_time:130030ms step_avg:87.98ms
step:1479/1680 train_time:130119ms step_avg:87.98ms
step:1480/1680 train_time:130210ms step_avg:87.98ms
step:1481/1680 train_time:130300ms step_avg:87.98ms
step:1482/1680 train_time:130389ms step_avg:87.98ms
step:1483/1680 train_time:130478ms step_avg:87.98ms
step:1484/1680 train_time:130568ms step_avg:87.98ms
step:1485/1680 train_time:130656ms step_avg:87.98ms
step:1486/1680 train_time:130745ms step_avg:87.98ms
step:1487/1680 train_time:130834ms step_avg:87.99ms
step:1488/1680 train_time:130923ms step_avg:87.99ms
step:1489/1680 train_time:131011ms step_avg:87.99ms
step:1490/1680 train_time:131100ms step_avg:87.99ms
step:1491/1680 train_time:131189ms step_avg:87.99ms
step:1492/1680 train_time:131278ms step_avg:87.99ms
step:1493/1680 train_time:131367ms step_avg:87.99ms
step:1494/1680 train_time:131456ms step_avg:87.99ms
step:1495/1680 train_time:131545ms step_avg:87.99ms
step:1496/1680 train_time:131635ms step_avg:87.99ms
step:1497/1680 train_time:131723ms step_avg:87.99ms
step:1498/1680 train_time:131812ms step_avg:87.99ms
step:1499/1680 train_time:131901ms step_avg:87.99ms
step:1500/1680 train_time:131989ms step_avg:87.99ms
step:1500/1680 val_loss:3.3134 train_time:132079ms step_avg:88.05ms
step:1501/1680 train_time:132097ms step_avg:88.01ms
step:1502/1680 train_time:132171ms step_avg:88.00ms
step:1503/1680 train_time:132263ms step_avg:88.00ms
step:1504/1680 train_time:132352ms step_avg:88.00ms
step:1505/1680 train_time:132440ms step_avg:88.00ms
step:1506/1680 train_time:132528ms step_avg:88.00ms
step:1507/1680 train_time:132616ms step_avg:88.00ms
step:1508/1680 train_time:132704ms step_avg:88.00ms
step:1509/1680 train_time:132791ms step_avg:88.00ms
step:1510/1680 train_time:132880ms step_avg:88.00ms
step:1511/1680 train_time:132968ms step_avg:88.00ms
step:1512/1680 train_time:133060ms step_avg:88.00ms
step:1513/1680 train_time:133150ms step_avg:88.00ms
step:1514/1680 train_time:133241ms step_avg:88.01ms
step:1515/1680 train_time:133330ms step_avg:88.01ms
step:1516/1680 train_time:133419ms step_avg:88.01ms
step:1517/1680 train_time:133508ms step_avg:88.01ms
step:1518/1680 train_time:133597ms step_avg:88.01ms
step:1519/1680 train_time:133685ms step_avg:88.01ms
step:1520/1680 train_time:133774ms step_avg:88.01ms
step:1521/1680 train_time:133862ms step_avg:88.01ms
step:1522/1680 train_time:133951ms step_avg:88.01ms
step:1523/1680 train_time:134041ms step_avg:88.01ms
step:1524/1680 train_time:134130ms step_avg:88.01ms
step:1525/1680 train_time:134220ms step_avg:88.01ms
step:1526/1680 train_time:134310ms step_avg:88.01ms
step:1527/1680 train_time:134399ms step_avg:88.02ms
step:1528/1680 train_time:134488ms step_avg:88.02ms
step:1529/1680 train_time:134577ms step_avg:88.02ms
step:1530/1680 train_time:134665ms step_avg:88.02ms
step:1531/1680 train_time:134754ms step_avg:88.02ms
step:1532/1680 train_time:134843ms step_avg:88.02ms
step:1533/1680 train_time:134932ms step_avg:88.02ms
step:1534/1680 train_time:135021ms step_avg:88.02ms
step:1535/1680 train_time:135110ms step_avg:88.02ms
step:1536/1680 train_time:135199ms step_avg:88.02ms
step:1537/1680 train_time:135289ms step_avg:88.02ms
step:1538/1680 train_time:135378ms step_avg:88.02ms
step:1539/1680 train_time:135467ms step_avg:88.02ms
step:1540/1680 train_time:135556ms step_avg:88.02ms
step:1541/1680 train_time:135644ms step_avg:88.02ms
step:1542/1680 train_time:135734ms step_avg:88.02ms
step:1543/1680 train_time:135822ms step_avg:88.02ms
step:1544/1680 train_time:135910ms step_avg:88.02ms
step:1545/1680 train_time:135999ms step_avg:88.03ms
step:1546/1680 train_time:136088ms step_avg:88.03ms
step:1547/1680 train_time:136178ms step_avg:88.03ms
step:1548/1680 train_time:136268ms step_avg:88.03ms
step:1549/1680 train_time:136357ms step_avg:88.03ms
step:1550/1680 train_time:136446ms step_avg:88.03ms
step:1551/1680 train_time:136535ms step_avg:88.03ms
step:1552/1680 train_time:136623ms step_avg:88.03ms
step:1553/1680 train_time:136711ms step_avg:88.03ms
step:1554/1680 train_time:136800ms step_avg:88.03ms
step:1555/1680 train_time:136888ms step_avg:88.03ms
step:1556/1680 train_time:136977ms step_avg:88.03ms
step:1557/1680 train_time:137066ms step_avg:88.03ms
step:1558/1680 train_time:137157ms step_avg:88.03ms
step:1559/1680 train_time:137246ms step_avg:88.03ms
step:1560/1680 train_time:137337ms step_avg:88.04ms
step:1561/1680 train_time:137426ms step_avg:88.04ms
step:1562/1680 train_time:137516ms step_avg:88.04ms
step:1563/1680 train_time:137604ms step_avg:88.04ms
step:1564/1680 train_time:137693ms step_avg:88.04ms
step:1565/1680 train_time:137781ms step_avg:88.04ms
step:1566/1680 train_time:137870ms step_avg:88.04ms
step:1567/1680 train_time:137959ms step_avg:88.04ms
step:1568/1680 train_time:138048ms step_avg:88.04ms
step:1569/1680 train_time:138137ms step_avg:88.04ms
step:1570/1680 train_time:138226ms step_avg:88.04ms
step:1571/1680 train_time:138315ms step_avg:88.04ms
step:1572/1680 train_time:138404ms step_avg:88.04ms
step:1573/1680 train_time:138494ms step_avg:88.04ms
step:1574/1680 train_time:138583ms step_avg:88.05ms
step:1575/1680 train_time:138673ms step_avg:88.05ms
step:1576/1680 train_time:138761ms step_avg:88.05ms
step:1577/1680 train_time:138849ms step_avg:88.05ms
step:1578/1680 train_time:138939ms step_avg:88.05ms
step:1579/1680 train_time:139028ms step_avg:88.05ms
step:1580/1680 train_time:139117ms step_avg:88.05ms
step:1581/1680 train_time:139206ms step_avg:88.05ms
step:1582/1680 train_time:139295ms step_avg:88.05ms
step:1583/1680 train_time:139383ms step_avg:88.05ms
step:1584/1680 train_time:139473ms step_avg:88.05ms
step:1585/1680 train_time:139562ms step_avg:88.05ms
step:1586/1680 train_time:139652ms step_avg:88.05ms
step:1587/1680 train_time:139740ms step_avg:88.05ms
step:1588/1680 train_time:139829ms step_avg:88.05ms
step:1589/1680 train_time:139918ms step_avg:88.05ms
step:1590/1680 train_time:140007ms step_avg:88.05ms
step:1591/1680 train_time:140096ms step_avg:88.06ms
step:1592/1680 train_time:140185ms step_avg:88.06ms
step:1593/1680 train_time:140273ms step_avg:88.06ms
step:1594/1680 train_time:140362ms step_avg:88.06ms
step:1595/1680 train_time:140451ms step_avg:88.06ms
step:1596/1680 train_time:140540ms step_avg:88.06ms
step:1597/1680 train_time:140631ms step_avg:88.06ms
step:1598/1680 train_time:140719ms step_avg:88.06ms
step:1599/1680 train_time:140808ms step_avg:88.06ms
step:1600/1680 train_time:140896ms step_avg:88.06ms
step:1601/1680 train_time:140985ms step_avg:88.06ms
step:1602/1680 train_time:141075ms step_avg:88.06ms
step:1603/1680 train_time:141163ms step_avg:88.06ms
step:1604/1680 train_time:141252ms step_avg:88.06ms
step:1605/1680 train_time:141341ms step_avg:88.06ms
step:1606/1680 train_time:141430ms step_avg:88.06ms
step:1607/1680 train_time:141518ms step_avg:88.06ms
step:1608/1680 train_time:141608ms step_avg:88.06ms
step:1609/1680 train_time:141697ms step_avg:88.07ms
step:1610/1680 train_time:141786ms step_avg:88.07ms
step:1611/1680 train_time:141875ms step_avg:88.07ms
step:1612/1680 train_time:141964ms step_avg:88.07ms
step:1613/1680 train_time:142054ms step_avg:88.07ms
step:1614/1680 train_time:142142ms step_avg:88.07ms
step:1615/1680 train_time:142231ms step_avg:88.07ms
step:1616/1680 train_time:142320ms step_avg:88.07ms
step:1617/1680 train_time:142408ms step_avg:88.07ms
step:1618/1680 train_time:142498ms step_avg:88.07ms
step:1619/1680 train_time:142587ms step_avg:88.07ms
step:1620/1680 train_time:142676ms step_avg:88.07ms
step:1621/1680 train_time:142766ms step_avg:88.07ms
step:1622/1680 train_time:142855ms step_avg:88.07ms
step:1623/1680 train_time:142944ms step_avg:88.07ms
step:1624/1680 train_time:143032ms step_avg:88.07ms
step:1625/1680 train_time:143121ms step_avg:88.07ms
step:1625/1680 val_loss:3.2897 train_time:143212ms step_avg:88.13ms
step:1626/1680 train_time:143230ms step_avg:88.09ms
step:1627/1680 train_time:143305ms step_avg:88.08ms
step:1628/1680 train_time:143400ms step_avg:88.08ms
step:1629/1680 train_time:143489ms step_avg:88.08ms
step:1630/1680 train_time:143578ms step_avg:88.08ms
step:1631/1680 train_time:143665ms step_avg:88.08ms
step:1632/1680 train_time:143753ms step_avg:88.08ms
step:1633/1680 train_time:143841ms step_avg:88.08ms
step:1634/1680 train_time:143930ms step_avg:88.08ms
step:1635/1680 train_time:144018ms step_avg:88.08ms
step:1636/1680 train_time:144106ms step_avg:88.08ms
step:1637/1680 train_time:144196ms step_avg:88.09ms
step:1638/1680 train_time:144287ms step_avg:88.09ms
step:1639/1680 train_time:144379ms step_avg:88.09ms
step:1640/1680 train_time:144469ms step_avg:88.09ms
step:1641/1680 train_time:144559ms step_avg:88.09ms
step:1642/1680 train_time:144647ms step_avg:88.09ms
step:1643/1680 train_time:144735ms step_avg:88.09ms
step:1644/1680 train_time:144823ms step_avg:88.09ms
step:1645/1680 train_time:144912ms step_avg:88.09ms
step:1646/1680 train_time:145000ms step_avg:88.09ms
step:1647/1680 train_time:145088ms step_avg:88.09ms
step:1648/1680 train_time:145179ms step_avg:88.09ms
step:1649/1680 train_time:145268ms step_avg:88.09ms
step:1650/1680 train_time:145359ms step_avg:88.10ms
step:1651/1680 train_time:145448ms step_avg:88.10ms
step:1652/1680 train_time:145538ms step_avg:88.10ms
step:1653/1680 train_time:145626ms step_avg:88.10ms
step:1654/1680 train_time:145715ms step_avg:88.10ms
step:1655/1680 train_time:145804ms step_avg:88.10ms
step:1656/1680 train_time:145892ms step_avg:88.10ms
step:1657/1680 train_time:145981ms step_avg:88.10ms
step:1658/1680 train_time:146069ms step_avg:88.10ms
step:1659/1680 train_time:146159ms step_avg:88.10ms
step:1660/1680 train_time:146247ms step_avg:88.10ms
step:1661/1680 train_time:146337ms step_avg:88.10ms
step:1662/1680 train_time:146427ms step_avg:88.10ms
step:1663/1680 train_time:146517ms step_avg:88.10ms
step:1664/1680 train_time:146606ms step_avg:88.10ms
step:1665/1680 train_time:146695ms step_avg:88.11ms
step:1666/1680 train_time:146783ms step_avg:88.11ms
step:1667/1680 train_time:146873ms step_avg:88.11ms
step:1668/1680 train_time:146961ms step_avg:88.11ms
step:1669/1680 train_time:147050ms step_avg:88.11ms
step:1670/1680 train_time:147138ms step_avg:88.11ms
step:1671/1680 train_time:147227ms step_avg:88.11ms
step:1672/1680 train_time:147317ms step_avg:88.11ms
step:1673/1680 train_time:147406ms step_avg:88.11ms
step:1674/1680 train_time:147496ms step_avg:88.11ms
step:1675/1680 train_time:147585ms step_avg:88.11ms
step:1676/1680 train_time:147674ms step_avg:88.11ms
step:1677/1680 train_time:147762ms step_avg:88.11ms
step:1678/1680 train_time:147851ms step_avg:88.11ms
step:1679/1680 train_time:147940ms step_avg:88.11ms
step:1680/1680 train_time:148028ms step_avg:88.11ms
step:1680/1680 val_loss:3.2790 train_time:148119ms step_avg:88.17ms
peak memory allocated: 30760 MiB reserved: 46054 MiB

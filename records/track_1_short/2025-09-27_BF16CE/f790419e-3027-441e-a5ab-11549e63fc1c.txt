import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 13:40:51 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   27C    P0            121W /  700W |    5856MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            119W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            116W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            116W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            121W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    178592      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    178593      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    178594      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    178595      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    178596      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    178597      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    178598      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    178599      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    178593      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    178594      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    178595      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    178596      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    178597      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    178598      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    178599      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/1680 train_time:152ms step_avg:152.00ms
step:2/1680 train_time:172ms step_avg:86.24ms
step:3/1680 train_time:236ms step_avg:78.55ms
step:4/1680 train_time:321ms step_avg:80.15ms
step:5/1680 train_time:406ms step_avg:81.30ms
step:6/1680 train_time:493ms step_avg:82.16ms
step:7/1680 train_time:579ms step_avg:82.72ms
step:8/1680 train_time:665ms step_avg:83.13ms
step:9/1680 train_time:752ms step_avg:83.52ms
step:10/1680 train_time:838ms step_avg:83.76ms
step:11/1680 train_time:924ms step_avg:84.02ms
step:12/1680 train_time:1013ms step_avg:84.38ms
step:13/1680 train_time:1102ms step_avg:84.80ms
step:14/1680 train_time:1192ms step_avg:85.15ms
step:15/1680 train_time:1279ms step_avg:85.30ms
step:16/1680 train_time:1366ms step_avg:85.39ms
step:17/1680 train_time:1453ms step_avg:85.48ms
step:18/1680 train_time:1539ms step_avg:85.52ms
step:19/1680 train_time:1627ms step_avg:85.62ms
step:20/1680 train_time:1713ms step_avg:85.66ms
step:21/1680 train_time:1799ms step_avg:85.68ms
step:22/1680 train_time:1886ms step_avg:85.72ms
step:23/1680 train_time:1973ms step_avg:85.79ms
step:24/1680 train_time:2061ms step_avg:85.88ms
step:25/1680 train_time:2150ms step_avg:86.00ms
step:26/1680 train_time:2238ms step_avg:86.07ms
step:27/1680 train_time:2326ms step_avg:86.14ms
step:28/1680 train_time:2413ms step_avg:86.18ms
step:29/1680 train_time:2500ms step_avg:86.21ms
step:30/1680 train_time:2588ms step_avg:86.27ms
step:31/1680 train_time:2675ms step_avg:86.29ms
step:32/1680 train_time:2761ms step_avg:86.28ms
step:33/1680 train_time:2848ms step_avg:86.30ms
step:34/1680 train_time:2935ms step_avg:86.33ms
step:35/1680 train_time:3023ms step_avg:86.36ms
step:36/1680 train_time:3112ms step_avg:86.44ms
step:37/1680 train_time:3199ms step_avg:86.47ms
step:38/1680 train_time:3287ms step_avg:86.50ms
step:39/1680 train_time:3374ms step_avg:86.52ms
step:40/1680 train_time:3461ms step_avg:86.52ms
step:41/1680 train_time:3548ms step_avg:86.53ms
step:42/1680 train_time:3635ms step_avg:86.54ms
step:43/1680 train_time:3722ms step_avg:86.55ms
step:44/1680 train_time:3808ms step_avg:86.55ms
step:45/1680 train_time:3895ms step_avg:86.55ms
step:46/1680 train_time:3981ms step_avg:86.54ms
step:47/1680 train_time:4068ms step_avg:86.55ms
step:48/1680 train_time:4156ms step_avg:86.59ms
step:49/1680 train_time:4244ms step_avg:86.61ms
step:50/1680 train_time:4331ms step_avg:86.62ms
step:51/1680 train_time:4418ms step_avg:86.63ms
step:52/1680 train_time:4506ms step_avg:86.65ms
step:53/1680 train_time:4593ms step_avg:86.66ms
step:54/1680 train_time:4680ms step_avg:86.66ms
step:55/1680 train_time:4767ms step_avg:86.67ms
step:56/1680 train_time:4854ms step_avg:86.67ms
step:57/1680 train_time:4940ms step_avg:86.67ms
step:58/1680 train_time:5028ms step_avg:86.68ms
step:59/1680 train_time:5115ms step_avg:86.69ms
step:60/1680 train_time:5204ms step_avg:86.73ms
step:61/1680 train_time:5291ms step_avg:86.73ms
step:62/1680 train_time:5377ms step_avg:86.73ms
step:63/1680 train_time:5464ms step_avg:86.73ms
step:64/1680 train_time:5552ms step_avg:86.75ms
step:65/1680 train_time:5639ms step_avg:86.75ms
step:66/1680 train_time:5726ms step_avg:86.75ms
step:67/1680 train_time:5813ms step_avg:86.77ms
step:68/1680 train_time:5902ms step_avg:86.79ms
step:69/1680 train_time:5988ms step_avg:86.79ms
step:70/1680 train_time:6075ms step_avg:86.79ms
step:71/1680 train_time:6163ms step_avg:86.81ms
step:72/1680 train_time:6251ms step_avg:86.81ms
step:73/1680 train_time:6338ms step_avg:86.82ms
step:74/1680 train_time:6425ms step_avg:86.83ms
step:75/1680 train_time:6513ms step_avg:86.84ms
step:76/1680 train_time:6600ms step_avg:86.85ms
step:77/1680 train_time:6687ms step_avg:86.84ms
step:78/1680 train_time:6775ms step_avg:86.87ms
step:79/1680 train_time:6862ms step_avg:86.85ms
step:80/1680 train_time:6949ms step_avg:86.86ms
step:81/1680 train_time:7036ms step_avg:86.87ms
step:82/1680 train_time:7124ms step_avg:86.88ms
step:83/1680 train_time:7211ms step_avg:86.88ms
step:84/1680 train_time:7298ms step_avg:86.88ms
step:85/1680 train_time:7386ms step_avg:86.89ms
step:86/1680 train_time:7473ms step_avg:86.89ms
step:87/1680 train_time:7560ms step_avg:86.90ms
step:88/1680 train_time:7648ms step_avg:86.91ms
step:89/1680 train_time:7735ms step_avg:86.91ms
step:90/1680 train_time:7822ms step_avg:86.91ms
step:91/1680 train_time:7909ms step_avg:86.92ms
step:92/1680 train_time:7996ms step_avg:86.91ms
step:93/1680 train_time:8083ms step_avg:86.91ms
step:94/1680 train_time:8170ms step_avg:86.91ms
step:95/1680 train_time:8257ms step_avg:86.92ms
step:96/1680 train_time:8345ms step_avg:86.92ms
step:97/1680 train_time:8432ms step_avg:86.93ms
step:98/1680 train_time:8519ms step_avg:86.92ms
step:99/1680 train_time:8606ms step_avg:86.92ms
step:100/1680 train_time:8693ms step_avg:86.93ms
step:101/1680 train_time:8780ms step_avg:86.93ms
step:102/1680 train_time:8867ms step_avg:86.93ms
step:103/1680 train_time:8954ms step_avg:86.93ms
step:104/1680 train_time:9041ms step_avg:86.93ms
step:105/1680 train_time:9128ms step_avg:86.93ms
step:106/1680 train_time:9215ms step_avg:86.93ms
step:107/1680 train_time:9302ms step_avg:86.94ms
step:108/1680 train_time:9389ms step_avg:86.94ms
step:109/1680 train_time:9477ms step_avg:86.94ms
step:110/1680 train_time:9563ms step_avg:86.94ms
step:111/1680 train_time:9651ms step_avg:86.94ms
step:112/1680 train_time:9738ms step_avg:86.95ms
step:113/1680 train_time:9826ms step_avg:86.95ms
step:114/1680 train_time:9913ms step_avg:86.96ms
step:115/1680 train_time:10000ms step_avg:86.96ms
step:116/1680 train_time:10087ms step_avg:86.96ms
step:117/1680 train_time:10174ms step_avg:86.96ms
step:118/1680 train_time:10261ms step_avg:86.96ms
step:119/1680 train_time:10348ms step_avg:86.96ms
step:120/1680 train_time:10436ms step_avg:86.96ms
step:121/1680 train_time:10522ms step_avg:86.96ms
step:122/1680 train_time:10609ms step_avg:86.96ms
step:123/1680 train_time:10696ms step_avg:86.96ms
step:124/1680 train_time:10784ms step_avg:86.97ms
step:125/1680 train_time:10871ms step_avg:86.97ms
step:125/1680 val_loss:4.3006 train_time:10960ms step_avg:87.68ms
step:126/1680 train_time:10982ms step_avg:87.16ms
step:127/1680 train_time:11050ms step_avg:87.01ms
step:128/1680 train_time:11150ms step_avg:87.11ms
step:129/1680 train_time:11238ms step_avg:87.12ms
step:130/1680 train_time:11325ms step_avg:87.11ms
step:131/1680 train_time:11411ms step_avg:87.11ms
step:132/1680 train_time:11497ms step_avg:87.10ms
step:133/1680 train_time:11583ms step_avg:87.09ms
step:134/1680 train_time:11669ms step_avg:87.08ms
step:135/1680 train_time:11755ms step_avg:87.07ms
step:136/1680 train_time:11841ms step_avg:87.07ms
step:137/1680 train_time:11928ms step_avg:87.06ms
step:138/1680 train_time:12014ms step_avg:87.06ms
step:139/1680 train_time:12105ms step_avg:87.09ms
step:140/1680 train_time:12194ms step_avg:87.10ms
step:141/1680 train_time:12283ms step_avg:87.11ms
step:142/1680 train_time:12370ms step_avg:87.11ms
step:143/1680 train_time:12457ms step_avg:87.11ms
step:144/1680 train_time:12543ms step_avg:87.11ms
step:145/1680 train_time:12629ms step_avg:87.10ms
step:146/1680 train_time:12715ms step_avg:87.09ms
step:147/1680 train_time:12802ms step_avg:87.09ms
step:148/1680 train_time:12888ms step_avg:87.08ms
step:149/1680 train_time:12976ms step_avg:87.08ms
step:150/1680 train_time:13064ms step_avg:87.09ms
step:151/1680 train_time:13152ms step_avg:87.10ms
step:152/1680 train_time:13241ms step_avg:87.11ms
step:153/1680 train_time:13329ms step_avg:87.11ms
step:154/1680 train_time:13415ms step_avg:87.11ms
step:155/1680 train_time:13502ms step_avg:87.11ms
step:156/1680 train_time:13589ms step_avg:87.11ms
step:157/1680 train_time:13675ms step_avg:87.10ms
step:158/1680 train_time:13762ms step_avg:87.10ms
step:159/1680 train_time:13848ms step_avg:87.10ms
step:160/1680 train_time:13935ms step_avg:87.09ms
step:161/1680 train_time:14022ms step_avg:87.09ms
step:162/1680 train_time:14110ms step_avg:87.10ms
step:163/1680 train_time:14197ms step_avg:87.10ms
step:164/1680 train_time:14285ms step_avg:87.10ms
step:165/1680 train_time:14372ms step_avg:87.10ms
step:166/1680 train_time:14459ms step_avg:87.10ms
step:167/1680 train_time:14546ms step_avg:87.10ms
step:168/1680 train_time:14632ms step_avg:87.10ms
step:169/1680 train_time:14719ms step_avg:87.09ms
step:170/1680 train_time:14806ms step_avg:87.09ms
step:171/1680 train_time:14892ms step_avg:87.09ms
step:172/1680 train_time:14980ms step_avg:87.09ms
step:173/1680 train_time:15068ms step_avg:87.10ms
step:174/1680 train_time:15155ms step_avg:87.10ms
step:175/1680 train_time:15242ms step_avg:87.10ms
step:176/1680 train_time:15329ms step_avg:87.10ms
step:177/1680 train_time:15417ms step_avg:87.10ms
step:178/1680 train_time:15504ms step_avg:87.10ms
step:179/1680 train_time:15591ms step_avg:87.10ms
step:180/1680 train_time:15679ms step_avg:87.10ms
step:181/1680 train_time:15765ms step_avg:87.10ms
step:182/1680 train_time:15852ms step_avg:87.10ms
step:183/1680 train_time:15939ms step_avg:87.10ms
step:184/1680 train_time:16025ms step_avg:87.09ms
step:185/1680 train_time:16112ms step_avg:87.09ms
step:186/1680 train_time:16200ms step_avg:87.10ms
step:187/1680 train_time:16288ms step_avg:87.10ms
step:188/1680 train_time:16375ms step_avg:87.10ms
step:189/1680 train_time:16461ms step_avg:87.10ms
step:190/1680 train_time:16548ms step_avg:87.10ms
step:191/1680 train_time:16635ms step_avg:87.10ms
step:192/1680 train_time:16722ms step_avg:87.10ms
step:193/1680 train_time:16809ms step_avg:87.09ms
step:194/1680 train_time:16896ms step_avg:87.09ms
step:195/1680 train_time:16983ms step_avg:87.09ms
step:196/1680 train_time:17070ms step_avg:87.09ms
step:197/1680 train_time:17157ms step_avg:87.09ms
step:198/1680 train_time:17244ms step_avg:87.09ms
step:199/1680 train_time:17331ms step_avg:87.09ms
step:200/1680 train_time:17418ms step_avg:87.09ms
step:201/1680 train_time:17506ms step_avg:87.09ms
step:202/1680 train_time:17593ms step_avg:87.09ms
step:203/1680 train_time:17680ms step_avg:87.09ms
step:204/1680 train_time:17767ms step_avg:87.10ms
step:205/1680 train_time:17855ms step_avg:87.10ms
step:206/1680 train_time:17941ms step_avg:87.09ms
step:207/1680 train_time:18028ms step_avg:87.09ms
step:208/1680 train_time:18114ms step_avg:87.09ms
step:209/1680 train_time:18202ms step_avg:87.09ms
step:210/1680 train_time:18289ms step_avg:87.09ms
step:211/1680 train_time:18376ms step_avg:87.09ms
step:212/1680 train_time:18463ms step_avg:87.09ms
step:213/1680 train_time:18549ms step_avg:87.09ms
step:214/1680 train_time:18636ms step_avg:87.08ms
step:215/1680 train_time:18722ms step_avg:87.08ms
step:216/1680 train_time:18809ms step_avg:87.08ms
step:217/1680 train_time:18896ms step_avg:87.08ms
step:218/1680 train_time:18983ms step_avg:87.08ms
step:219/1680 train_time:19070ms step_avg:87.08ms
step:220/1680 train_time:19157ms step_avg:87.08ms
step:221/1680 train_time:19244ms step_avg:87.07ms
step:222/1680 train_time:19331ms step_avg:87.08ms
step:223/1680 train_time:19419ms step_avg:87.08ms
step:224/1680 train_time:19505ms step_avg:87.08ms
step:225/1680 train_time:19593ms step_avg:87.08ms
step:226/1680 train_time:19680ms step_avg:87.08ms
step:227/1680 train_time:19767ms step_avg:87.08ms
step:228/1680 train_time:19855ms step_avg:87.08ms
step:229/1680 train_time:19941ms step_avg:87.08ms
step:230/1680 train_time:20028ms step_avg:87.08ms
step:231/1680 train_time:20116ms step_avg:87.08ms
step:232/1680 train_time:20203ms step_avg:87.08ms
step:233/1680 train_time:20290ms step_avg:87.08ms
step:234/1680 train_time:20378ms step_avg:87.08ms
step:235/1680 train_time:20464ms step_avg:87.08ms
step:236/1680 train_time:20552ms step_avg:87.08ms
step:237/1680 train_time:20639ms step_avg:87.08ms
step:238/1680 train_time:20726ms step_avg:87.08ms
step:239/1680 train_time:20813ms step_avg:87.08ms
step:240/1680 train_time:20900ms step_avg:87.08ms
step:241/1680 train_time:20987ms step_avg:87.08ms
step:242/1680 train_time:21075ms step_avg:87.09ms
step:243/1680 train_time:21163ms step_avg:87.09ms
step:244/1680 train_time:21249ms step_avg:87.09ms
step:245/1680 train_time:21336ms step_avg:87.09ms
step:246/1680 train_time:21423ms step_avg:87.09ms
step:247/1680 train_time:21510ms step_avg:87.09ms
step:248/1680 train_time:21597ms step_avg:87.09ms
step:249/1680 train_time:21684ms step_avg:87.09ms
step:250/1680 train_time:21771ms step_avg:87.08ms
step:250/1680 val_loss:3.9658 train_time:21860ms step_avg:87.44ms
step:251/1680 train_time:21879ms step_avg:87.17ms
step:252/1680 train_time:21951ms step_avg:87.11ms
step:253/1680 train_time:22045ms step_avg:87.13ms
step:254/1680 train_time:22133ms step_avg:87.14ms
step:255/1680 train_time:22220ms step_avg:87.14ms
step:256/1680 train_time:22306ms step_avg:87.13ms
step:257/1680 train_time:22392ms step_avg:87.13ms
step:258/1680 train_time:22478ms step_avg:87.12ms
step:259/1680 train_time:22564ms step_avg:87.12ms
step:260/1680 train_time:22650ms step_avg:87.12ms
step:261/1680 train_time:22736ms step_avg:87.11ms
step:262/1680 train_time:22824ms step_avg:87.11ms
step:263/1680 train_time:22912ms step_avg:87.12ms
step:264/1680 train_time:23000ms step_avg:87.12ms
step:265/1680 train_time:23089ms step_avg:87.13ms
step:266/1680 train_time:23177ms step_avg:87.13ms
step:267/1680 train_time:23263ms step_avg:87.13ms
step:268/1680 train_time:23350ms step_avg:87.13ms
step:269/1680 train_time:23437ms step_avg:87.13ms
step:270/1680 train_time:23522ms step_avg:87.12ms
step:271/1680 train_time:23608ms step_avg:87.12ms
step:272/1680 train_time:23695ms step_avg:87.11ms
step:273/1680 train_time:23781ms step_avg:87.11ms
step:274/1680 train_time:23869ms step_avg:87.11ms
step:275/1680 train_time:23957ms step_avg:87.12ms
step:276/1680 train_time:24045ms step_avg:87.12ms
step:277/1680 train_time:24133ms step_avg:87.12ms
step:278/1680 train_time:24220ms step_avg:87.12ms
step:279/1680 train_time:24307ms step_avg:87.12ms
step:280/1680 train_time:24394ms step_avg:87.12ms
step:281/1680 train_time:24480ms step_avg:87.12ms
step:282/1680 train_time:24568ms step_avg:87.12ms
step:283/1680 train_time:24654ms step_avg:87.12ms
step:284/1680 train_time:24740ms step_avg:87.11ms
step:285/1680 train_time:24828ms step_avg:87.11ms
step:286/1680 train_time:24914ms step_avg:87.11ms
step:287/1680 train_time:25003ms step_avg:87.12ms
step:288/1680 train_time:25090ms step_avg:87.12ms
step:289/1680 train_time:25178ms step_avg:87.12ms
step:290/1680 train_time:25265ms step_avg:87.12ms
step:291/1680 train_time:25352ms step_avg:87.12ms
step:292/1680 train_time:25439ms step_avg:87.12ms
step:293/1680 train_time:25526ms step_avg:87.12ms
step:294/1680 train_time:25612ms step_avg:87.11ms
step:295/1680 train_time:25698ms step_avg:87.11ms
step:296/1680 train_time:25784ms step_avg:87.11ms
step:297/1680 train_time:25871ms step_avg:87.11ms
step:298/1680 train_time:25959ms step_avg:87.11ms
step:299/1680 train_time:26046ms step_avg:87.11ms
step:300/1680 train_time:26133ms step_avg:87.11ms
step:301/1680 train_time:26221ms step_avg:87.11ms
step:302/1680 train_time:26309ms step_avg:87.11ms
step:303/1680 train_time:26395ms step_avg:87.11ms
step:304/1680 train_time:26483ms step_avg:87.11ms
step:305/1680 train_time:26570ms step_avg:87.11ms
step:306/1680 train_time:26657ms step_avg:87.11ms
step:307/1680 train_time:26744ms step_avg:87.11ms
step:308/1680 train_time:26831ms step_avg:87.11ms
step:309/1680 train_time:26917ms step_avg:87.11ms
step:310/1680 train_time:27004ms step_avg:87.11ms
step:311/1680 train_time:27092ms step_avg:87.11ms
step:312/1680 train_time:27179ms step_avg:87.11ms
step:313/1680 train_time:27267ms step_avg:87.11ms
step:314/1680 train_time:27354ms step_avg:87.11ms
step:315/1680 train_time:27441ms step_avg:87.11ms
step:316/1680 train_time:27528ms step_avg:87.12ms
step:317/1680 train_time:27616ms step_avg:87.12ms
step:318/1680 train_time:27704ms step_avg:87.12ms
step:319/1680 train_time:27791ms step_avg:87.12ms
step:320/1680 train_time:27878ms step_avg:87.12ms
step:321/1680 train_time:27964ms step_avg:87.12ms
step:322/1680 train_time:28052ms step_avg:87.12ms
step:323/1680 train_time:28139ms step_avg:87.12ms
step:324/1680 train_time:28227ms step_avg:87.12ms
step:325/1680 train_time:28313ms step_avg:87.12ms
step:326/1680 train_time:28401ms step_avg:87.12ms
step:327/1680 train_time:28488ms step_avg:87.12ms
step:328/1680 train_time:28576ms step_avg:87.12ms
step:329/1680 train_time:28663ms step_avg:87.12ms
step:330/1680 train_time:28750ms step_avg:87.12ms
step:331/1680 train_time:28837ms step_avg:87.12ms
step:332/1680 train_time:28924ms step_avg:87.12ms
step:333/1680 train_time:29011ms step_avg:87.12ms
step:334/1680 train_time:29098ms step_avg:87.12ms
step:335/1680 train_time:29185ms step_avg:87.12ms
step:336/1680 train_time:29273ms step_avg:87.12ms
step:337/1680 train_time:29359ms step_avg:87.12ms
step:338/1680 train_time:29446ms step_avg:87.12ms
step:339/1680 train_time:29533ms step_avg:87.12ms
step:340/1680 train_time:29620ms step_avg:87.12ms
step:341/1680 train_time:29707ms step_avg:87.12ms
step:342/1680 train_time:29794ms step_avg:87.12ms
step:343/1680 train_time:29880ms step_avg:87.11ms
step:344/1680 train_time:29967ms step_avg:87.11ms
step:345/1680 train_time:30054ms step_avg:87.11ms
step:346/1680 train_time:30142ms step_avg:87.11ms
step:347/1680 train_time:30229ms step_avg:87.12ms
step:348/1680 train_time:30316ms step_avg:87.11ms
step:349/1680 train_time:30404ms step_avg:87.12ms
step:350/1680 train_time:30491ms step_avg:87.12ms
step:351/1680 train_time:30578ms step_avg:87.12ms
step:352/1680 train_time:30665ms step_avg:87.12ms
step:353/1680 train_time:30752ms step_avg:87.12ms
step:354/1680 train_time:30839ms step_avg:87.12ms
step:355/1680 train_time:30927ms step_avg:87.12ms
step:356/1680 train_time:31014ms step_avg:87.12ms
step:357/1680 train_time:31100ms step_avg:87.11ms
step:358/1680 train_time:31187ms step_avg:87.12ms
step:359/1680 train_time:31274ms step_avg:87.12ms
step:360/1680 train_time:31361ms step_avg:87.12ms
step:361/1680 train_time:31449ms step_avg:87.12ms
step:362/1680 train_time:31536ms step_avg:87.12ms
step:363/1680 train_time:31623ms step_avg:87.11ms
step:364/1680 train_time:31709ms step_avg:87.11ms
step:365/1680 train_time:31797ms step_avg:87.11ms
step:366/1680 train_time:31884ms step_avg:87.11ms
step:367/1680 train_time:31971ms step_avg:87.11ms
step:368/1680 train_time:32058ms step_avg:87.11ms
step:369/1680 train_time:32146ms step_avg:87.12ms
step:370/1680 train_time:32233ms step_avg:87.12ms
step:371/1680 train_time:32320ms step_avg:87.12ms
step:372/1680 train_time:32407ms step_avg:87.12ms
step:373/1680 train_time:32495ms step_avg:87.12ms
step:374/1680 train_time:32582ms step_avg:87.12ms
step:375/1680 train_time:32670ms step_avg:87.12ms
step:375/1680 val_loss:3.8196 train_time:32758ms step_avg:87.36ms
step:376/1680 train_time:32778ms step_avg:87.18ms
step:377/1680 train_time:32849ms step_avg:87.13ms
step:378/1680 train_time:32940ms step_avg:87.14ms
step:379/1680 train_time:33029ms step_avg:87.15ms
step:380/1680 train_time:33116ms step_avg:87.15ms
step:381/1680 train_time:33203ms step_avg:87.15ms
step:382/1680 train_time:33290ms step_avg:87.15ms
step:383/1680 train_time:33376ms step_avg:87.14ms
step:384/1680 train_time:33462ms step_avg:87.14ms
step:385/1680 train_time:33549ms step_avg:87.14ms
step:386/1680 train_time:33634ms step_avg:87.14ms
step:387/1680 train_time:33722ms step_avg:87.14ms
step:388/1680 train_time:33811ms step_avg:87.14ms
step:389/1680 train_time:33899ms step_avg:87.14ms
step:390/1680 train_time:33988ms step_avg:87.15ms
step:391/1680 train_time:34075ms step_avg:87.15ms
step:392/1680 train_time:34163ms step_avg:87.15ms
step:393/1680 train_time:34249ms step_avg:87.15ms
step:394/1680 train_time:34336ms step_avg:87.15ms
step:395/1680 train_time:34422ms step_avg:87.14ms
step:396/1680 train_time:34508ms step_avg:87.14ms
step:397/1680 train_time:34595ms step_avg:87.14ms
step:398/1680 train_time:34681ms step_avg:87.14ms
step:399/1680 train_time:34769ms step_avg:87.14ms
step:400/1680 train_time:34856ms step_avg:87.14ms
step:401/1680 train_time:34944ms step_avg:87.14ms
step:402/1680 train_time:35033ms step_avg:87.15ms
step:403/1680 train_time:35120ms step_avg:87.15ms
step:404/1680 train_time:35207ms step_avg:87.15ms
step:405/1680 train_time:35295ms step_avg:87.15ms
step:406/1680 train_time:35382ms step_avg:87.15ms
step:407/1680 train_time:35468ms step_avg:87.14ms
step:408/1680 train_time:35554ms step_avg:87.14ms
step:409/1680 train_time:35641ms step_avg:87.14ms
step:410/1680 train_time:35728ms step_avg:87.14ms
step:411/1680 train_time:35815ms step_avg:87.14ms
step:412/1680 train_time:35903ms step_avg:87.14ms
step:413/1680 train_time:35990ms step_avg:87.14ms
step:414/1680 train_time:36077ms step_avg:87.14ms
step:415/1680 train_time:36164ms step_avg:87.14ms
step:416/1680 train_time:36251ms step_avg:87.14ms
step:417/1680 train_time:36339ms step_avg:87.14ms
step:418/1680 train_time:36426ms step_avg:87.14ms
step:419/1680 train_time:36512ms step_avg:87.14ms
step:420/1680 train_time:36600ms step_avg:87.14ms
step:421/1680 train_time:36686ms step_avg:87.14ms
step:422/1680 train_time:36774ms step_avg:87.14ms
step:423/1680 train_time:36861ms step_avg:87.14ms
step:424/1680 train_time:36949ms step_avg:87.14ms
step:425/1680 train_time:37036ms step_avg:87.14ms
step:426/1680 train_time:37124ms step_avg:87.15ms
step:427/1680 train_time:37211ms step_avg:87.15ms
step:428/1680 train_time:37298ms step_avg:87.15ms
step:429/1680 train_time:37386ms step_avg:87.15ms
step:430/1680 train_time:37473ms step_avg:87.15ms
step:431/1680 train_time:37560ms step_avg:87.15ms
step:432/1680 train_time:37646ms step_avg:87.14ms
step:433/1680 train_time:37734ms step_avg:87.14ms
step:434/1680 train_time:37820ms step_avg:87.14ms
step:435/1680 train_time:37908ms step_avg:87.14ms
step:436/1680 train_time:37995ms step_avg:87.14ms
step:437/1680 train_time:38082ms step_avg:87.14ms
step:438/1680 train_time:38169ms step_avg:87.14ms
step:439/1680 train_time:38256ms step_avg:87.14ms
step:440/1680 train_time:38343ms step_avg:87.14ms
step:441/1680 train_time:38430ms step_avg:87.14ms
step:442/1680 train_time:38517ms step_avg:87.14ms
step:443/1680 train_time:38604ms step_avg:87.14ms
step:444/1680 train_time:38692ms step_avg:87.14ms
step:445/1680 train_time:38779ms step_avg:87.14ms
step:446/1680 train_time:38866ms step_avg:87.14ms
step:447/1680 train_time:38952ms step_avg:87.14ms
step:448/1680 train_time:39041ms step_avg:87.14ms
step:449/1680 train_time:39128ms step_avg:87.14ms
step:450/1680 train_time:39216ms step_avg:87.15ms
step:451/1680 train_time:39303ms step_avg:87.15ms
step:452/1680 train_time:39391ms step_avg:87.15ms
step:453/1680 train_time:39478ms step_avg:87.15ms
step:454/1680 train_time:39564ms step_avg:87.15ms
step:455/1680 train_time:39652ms step_avg:87.15ms
step:456/1680 train_time:39739ms step_avg:87.15ms
step:457/1680 train_time:39826ms step_avg:87.15ms
step:458/1680 train_time:39914ms step_avg:87.15ms
step:459/1680 train_time:40001ms step_avg:87.15ms
step:460/1680 train_time:40088ms step_avg:87.15ms
step:461/1680 train_time:40175ms step_avg:87.15ms
step:462/1680 train_time:40263ms step_avg:87.15ms
step:463/1680 train_time:40349ms step_avg:87.15ms
step:464/1680 train_time:40436ms step_avg:87.15ms
step:465/1680 train_time:40523ms step_avg:87.15ms
step:466/1680 train_time:40610ms step_avg:87.15ms
step:467/1680 train_time:40698ms step_avg:87.15ms
step:468/1680 train_time:40784ms step_avg:87.15ms
step:469/1680 train_time:40872ms step_avg:87.15ms
step:470/1680 train_time:40959ms step_avg:87.15ms
step:471/1680 train_time:41047ms step_avg:87.15ms
step:472/1680 train_time:41134ms step_avg:87.15ms
step:473/1680 train_time:41222ms step_avg:87.15ms
step:474/1680 train_time:41309ms step_avg:87.15ms
step:475/1680 train_time:41396ms step_avg:87.15ms
step:476/1680 train_time:41483ms step_avg:87.15ms
step:477/1680 train_time:41570ms step_avg:87.15ms
step:478/1680 train_time:41657ms step_avg:87.15ms
step:479/1680 train_time:41744ms step_avg:87.15ms
step:480/1680 train_time:41831ms step_avg:87.15ms
step:481/1680 train_time:41917ms step_avg:87.15ms
step:482/1680 train_time:42005ms step_avg:87.15ms
step:483/1680 train_time:42093ms step_avg:87.15ms
step:484/1680 train_time:42180ms step_avg:87.15ms
step:485/1680 train_time:42268ms step_avg:87.15ms
step:486/1680 train_time:42355ms step_avg:87.15ms
step:487/1680 train_time:42442ms step_avg:87.15ms
step:488/1680 train_time:42528ms step_avg:87.15ms
step:489/1680 train_time:42615ms step_avg:87.15ms
step:490/1680 train_time:42703ms step_avg:87.15ms
step:491/1680 train_time:42790ms step_avg:87.15ms
step:492/1680 train_time:42877ms step_avg:87.15ms
step:493/1680 train_time:42964ms step_avg:87.15ms
step:494/1680 train_time:43052ms step_avg:87.15ms
step:495/1680 train_time:43139ms step_avg:87.15ms
step:496/1680 train_time:43226ms step_avg:87.15ms
step:497/1680 train_time:43313ms step_avg:87.15ms
step:498/1680 train_time:43401ms step_avg:87.15ms
step:499/1680 train_time:43487ms step_avg:87.15ms
step:500/1680 train_time:43575ms step_avg:87.15ms
step:500/1680 val_loss:3.7183 train_time:43662ms step_avg:87.32ms
step:501/1680 train_time:43682ms step_avg:87.19ms
step:502/1680 train_time:43751ms step_avg:87.15ms
step:503/1680 train_time:43842ms step_avg:87.16ms
step:504/1680 train_time:43932ms step_avg:87.17ms
step:505/1680 train_time:44019ms step_avg:87.17ms
step:506/1680 train_time:44106ms step_avg:87.17ms
step:507/1680 train_time:44192ms step_avg:87.16ms
step:508/1680 train_time:44278ms step_avg:87.16ms
step:509/1680 train_time:44364ms step_avg:87.16ms
step:510/1680 train_time:44451ms step_avg:87.16ms
step:511/1680 train_time:44537ms step_avg:87.16ms
step:512/1680 train_time:44624ms step_avg:87.16ms
step:513/1680 train_time:44712ms step_avg:87.16ms
step:514/1680 train_time:44801ms step_avg:87.16ms
step:515/1680 train_time:44889ms step_avg:87.16ms
step:516/1680 train_time:44977ms step_avg:87.16ms
step:517/1680 train_time:45064ms step_avg:87.16ms
step:518/1680 train_time:45151ms step_avg:87.16ms
step:519/1680 train_time:45238ms step_avg:87.16ms
step:520/1680 train_time:45324ms step_avg:87.16ms
step:521/1680 train_time:45410ms step_avg:87.16ms
step:522/1680 train_time:45496ms step_avg:87.16ms
step:523/1680 train_time:45584ms step_avg:87.16ms
step:524/1680 train_time:45671ms step_avg:87.16ms
step:525/1680 train_time:45759ms step_avg:87.16ms
step:526/1680 train_time:45846ms step_avg:87.16ms
step:527/1680 train_time:45934ms step_avg:87.16ms
step:528/1680 train_time:46022ms step_avg:87.16ms
step:529/1680 train_time:46109ms step_avg:87.16ms
step:530/1680 train_time:46196ms step_avg:87.16ms
step:531/1680 train_time:46282ms step_avg:87.16ms
step:532/1680 train_time:46369ms step_avg:87.16ms
step:533/1680 train_time:46456ms step_avg:87.16ms
step:534/1680 train_time:46543ms step_avg:87.16ms
step:535/1680 train_time:46630ms step_avg:87.16ms
step:536/1680 train_time:46717ms step_avg:87.16ms
step:537/1680 train_time:46805ms step_avg:87.16ms
step:538/1680 train_time:46892ms step_avg:87.16ms
step:539/1680 train_time:46980ms step_avg:87.16ms
step:540/1680 train_time:47068ms step_avg:87.16ms
step:541/1680 train_time:47155ms step_avg:87.16ms
step:542/1680 train_time:47242ms step_avg:87.16ms
step:543/1680 train_time:47329ms step_avg:87.16ms
step:544/1680 train_time:47416ms step_avg:87.16ms
step:545/1680 train_time:47504ms step_avg:87.16ms
step:546/1680 train_time:47590ms step_avg:87.16ms
step:547/1680 train_time:47676ms step_avg:87.16ms
step:548/1680 train_time:47764ms step_avg:87.16ms
step:549/1680 train_time:47852ms step_avg:87.16ms
step:550/1680 train_time:47941ms step_avg:87.17ms
step:551/1680 train_time:48030ms step_avg:87.17ms
step:552/1680 train_time:48119ms step_avg:87.17ms
step:553/1680 train_time:48207ms step_avg:87.17ms
step:554/1680 train_time:48296ms step_avg:87.18ms
step:555/1680 train_time:48384ms step_avg:87.18ms
step:556/1680 train_time:48472ms step_avg:87.18ms
step:557/1680 train_time:48560ms step_avg:87.18ms
step:558/1680 train_time:48648ms step_avg:87.18ms
step:559/1680 train_time:48736ms step_avg:87.18ms
step:560/1680 train_time:48824ms step_avg:87.19ms
step:561/1680 train_time:48912ms step_avg:87.19ms
step:562/1680 train_time:49001ms step_avg:87.19ms
step:563/1680 train_time:49089ms step_avg:87.19ms
step:564/1680 train_time:49177ms step_avg:87.19ms
step:565/1680 train_time:49265ms step_avg:87.20ms
step:566/1680 train_time:49354ms step_avg:87.20ms
step:567/1680 train_time:49442ms step_avg:87.20ms
step:568/1680 train_time:49530ms step_avg:87.20ms
step:569/1680 train_time:49618ms step_avg:87.20ms
step:570/1680 train_time:49706ms step_avg:87.20ms
step:571/1680 train_time:49794ms step_avg:87.20ms
step:572/1680 train_time:49883ms step_avg:87.21ms
step:573/1680 train_time:49971ms step_avg:87.21ms
step:574/1680 train_time:50059ms step_avg:87.21ms
step:575/1680 train_time:50147ms step_avg:87.21ms
step:576/1680 train_time:50235ms step_avg:87.21ms
step:577/1680 train_time:50323ms step_avg:87.22ms
step:578/1680 train_time:50411ms step_avg:87.22ms
step:579/1680 train_time:50500ms step_avg:87.22ms
step:580/1680 train_time:50588ms step_avg:87.22ms
step:581/1680 train_time:50676ms step_avg:87.22ms
step:582/1680 train_time:50765ms step_avg:87.22ms
step:583/1680 train_time:50853ms step_avg:87.23ms
step:584/1680 train_time:50941ms step_avg:87.23ms
step:585/1680 train_time:51030ms step_avg:87.23ms
step:586/1680 train_time:51118ms step_avg:87.23ms
step:587/1680 train_time:51206ms step_avg:87.23ms
step:588/1680 train_time:51294ms step_avg:87.23ms
step:589/1680 train_time:51382ms step_avg:87.24ms
step:590/1680 train_time:51470ms step_avg:87.24ms
step:591/1680 train_time:51558ms step_avg:87.24ms
step:592/1680 train_time:51647ms step_avg:87.24ms
step:593/1680 train_time:51735ms step_avg:87.24ms
step:594/1680 train_time:51824ms step_avg:87.25ms
step:595/1680 train_time:51912ms step_avg:87.25ms
step:596/1680 train_time:52001ms step_avg:87.25ms
step:597/1680 train_time:52090ms step_avg:87.25ms
step:598/1680 train_time:52178ms step_avg:87.25ms
step:599/1680 train_time:52267ms step_avg:87.26ms
step:600/1680 train_time:52355ms step_avg:87.26ms
step:601/1680 train_time:52442ms step_avg:87.26ms
step:602/1680 train_time:52530ms step_avg:87.26ms
step:603/1680 train_time:52618ms step_avg:87.26ms
step:604/1680 train_time:52706ms step_avg:87.26ms
step:605/1680 train_time:52794ms step_avg:87.26ms
step:606/1680 train_time:52883ms step_avg:87.27ms
step:607/1680 train_time:52972ms step_avg:87.27ms
step:608/1680 train_time:53060ms step_avg:87.27ms
step:609/1680 train_time:53148ms step_avg:87.27ms
step:610/1680 train_time:53236ms step_avg:87.27ms
step:611/1680 train_time:53324ms step_avg:87.27ms
step:612/1680 train_time:53411ms step_avg:87.27ms
step:613/1680 train_time:53500ms step_avg:87.28ms
step:614/1680 train_time:53588ms step_avg:87.28ms
step:615/1680 train_time:53676ms step_avg:87.28ms
step:616/1680 train_time:53765ms step_avg:87.28ms
step:617/1680 train_time:53853ms step_avg:87.28ms
step:618/1680 train_time:53941ms step_avg:87.28ms
step:619/1680 train_time:54029ms step_avg:87.29ms
step:620/1680 train_time:54118ms step_avg:87.29ms
step:621/1680 train_time:54206ms step_avg:87.29ms
step:622/1680 train_time:54294ms step_avg:87.29ms
step:623/1680 train_time:54382ms step_avg:87.29ms
step:624/1680 train_time:54470ms step_avg:87.29ms
step:625/1680 train_time:54559ms step_avg:87.29ms
step:625/1680 val_loss:3.6198 train_time:54649ms step_avg:87.44ms
step:626/1680 train_time:54668ms step_avg:87.33ms
step:627/1680 train_time:54740ms step_avg:87.30ms
step:628/1680 train_time:54827ms step_avg:87.30ms
step:629/1680 train_time:54917ms step_avg:87.31ms
step:630/1680 train_time:55005ms step_avg:87.31ms
step:631/1680 train_time:55093ms step_avg:87.31ms
step:632/1680 train_time:55181ms step_avg:87.31ms
step:633/1680 train_time:55268ms step_avg:87.31ms
step:634/1680 train_time:55355ms step_avg:87.31ms
step:635/1680 train_time:55442ms step_avg:87.31ms
step:636/1680 train_time:55530ms step_avg:87.31ms
step:637/1680 train_time:55621ms step_avg:87.32ms
step:638/1680 train_time:55711ms step_avg:87.32ms
step:639/1680 train_time:55800ms step_avg:87.32ms
step:640/1680 train_time:55889ms step_avg:87.33ms
step:641/1680 train_time:55977ms step_avg:87.33ms
step:642/1680 train_time:56066ms step_avg:87.33ms
step:643/1680 train_time:56154ms step_avg:87.33ms
step:644/1680 train_time:56242ms step_avg:87.33ms
step:645/1680 train_time:56330ms step_avg:87.33ms
step:646/1680 train_time:56417ms step_avg:87.33ms
step:647/1680 train_time:56505ms step_avg:87.33ms
step:648/1680 train_time:56594ms step_avg:87.34ms
step:649/1680 train_time:56683ms step_avg:87.34ms
step:650/1680 train_time:56772ms step_avg:87.34ms
step:651/1680 train_time:56860ms step_avg:87.34ms
step:652/1680 train_time:56949ms step_avg:87.35ms
step:653/1680 train_time:57037ms step_avg:87.35ms
step:654/1680 train_time:57125ms step_avg:87.35ms
step:655/1680 train_time:57213ms step_avg:87.35ms
step:656/1680 train_time:57300ms step_avg:87.35ms
step:657/1680 train_time:57388ms step_avg:87.35ms
step:658/1680 train_time:57476ms step_avg:87.35ms
step:659/1680 train_time:57564ms step_avg:87.35ms
step:660/1680 train_time:57653ms step_avg:87.35ms
step:661/1680 train_time:57741ms step_avg:87.35ms
step:662/1680 train_time:57829ms step_avg:87.36ms
step:663/1680 train_time:57919ms step_avg:87.36ms
step:664/1680 train_time:58008ms step_avg:87.36ms
step:665/1680 train_time:58095ms step_avg:87.36ms
step:666/1680 train_time:58183ms step_avg:87.36ms
step:667/1680 train_time:58271ms step_avg:87.36ms
step:668/1680 train_time:58358ms step_avg:87.36ms
step:669/1680 train_time:58447ms step_avg:87.36ms
step:670/1680 train_time:58536ms step_avg:87.37ms
step:671/1680 train_time:58623ms step_avg:87.37ms
step:672/1680 train_time:58712ms step_avg:87.37ms
step:673/1680 train_time:58800ms step_avg:87.37ms
step:674/1680 train_time:58889ms step_avg:87.37ms
step:675/1680 train_time:58977ms step_avg:87.37ms
step:676/1680 train_time:59065ms step_avg:87.37ms
step:677/1680 train_time:59153ms step_avg:87.38ms
step:678/1680 train_time:59241ms step_avg:87.38ms
step:679/1680 train_time:59328ms step_avg:87.38ms
step:680/1680 train_time:59417ms step_avg:87.38ms
step:681/1680 train_time:59505ms step_avg:87.38ms
step:682/1680 train_time:59594ms step_avg:87.38ms
step:683/1680 train_time:59682ms step_avg:87.38ms
step:684/1680 train_time:59770ms step_avg:87.38ms
step:685/1680 train_time:59859ms step_avg:87.38ms
step:686/1680 train_time:59947ms step_avg:87.39ms
step:687/1680 train_time:60035ms step_avg:87.39ms
step:688/1680 train_time:60123ms step_avg:87.39ms
step:689/1680 train_time:60212ms step_avg:87.39ms
step:690/1680 train_time:60300ms step_avg:87.39ms
step:691/1680 train_time:60387ms step_avg:87.39ms
step:692/1680 train_time:60476ms step_avg:87.39ms
step:693/1680 train_time:60565ms step_avg:87.39ms
step:694/1680 train_time:60653ms step_avg:87.40ms
step:695/1680 train_time:60741ms step_avg:87.40ms
step:696/1680 train_time:60830ms step_avg:87.40ms
step:697/1680 train_time:60918ms step_avg:87.40ms
step:698/1680 train_time:61006ms step_avg:87.40ms
step:699/1680 train_time:61094ms step_avg:87.40ms
step:700/1680 train_time:61182ms step_avg:87.40ms
step:701/1680 train_time:61270ms step_avg:87.40ms
step:702/1680 train_time:61358ms step_avg:87.40ms
step:703/1680 train_time:61446ms step_avg:87.41ms
step:704/1680 train_time:61534ms step_avg:87.41ms
step:705/1680 train_time:61623ms step_avg:87.41ms
step:706/1680 train_time:61711ms step_avg:87.41ms
step:707/1680 train_time:61799ms step_avg:87.41ms
step:708/1680 train_time:61888ms step_avg:87.41ms
step:709/1680 train_time:61977ms step_avg:87.41ms
step:710/1680 train_time:62065ms step_avg:87.42ms
step:711/1680 train_time:62152ms step_avg:87.42ms
step:712/1680 train_time:62241ms step_avg:87.42ms
step:713/1680 train_time:62329ms step_avg:87.42ms
step:714/1680 train_time:62417ms step_avg:87.42ms
step:715/1680 train_time:62506ms step_avg:87.42ms
step:716/1680 train_time:62594ms step_avg:87.42ms
step:717/1680 train_time:62682ms step_avg:87.42ms
step:718/1680 train_time:62770ms step_avg:87.42ms
step:719/1680 train_time:62858ms step_avg:87.42ms
step:720/1680 train_time:62946ms step_avg:87.42ms
step:721/1680 train_time:63034ms step_avg:87.43ms
step:722/1680 train_time:63123ms step_avg:87.43ms
step:723/1680 train_time:63211ms step_avg:87.43ms
step:724/1680 train_time:63299ms step_avg:87.43ms
step:725/1680 train_time:63387ms step_avg:87.43ms
step:726/1680 train_time:63476ms step_avg:87.43ms
step:727/1680 train_time:63563ms step_avg:87.43ms
step:728/1680 train_time:63651ms step_avg:87.43ms
step:729/1680 train_time:63739ms step_avg:87.43ms
step:730/1680 train_time:63827ms step_avg:87.43ms
step:731/1680 train_time:63916ms step_avg:87.44ms
step:732/1680 train_time:64004ms step_avg:87.44ms
step:733/1680 train_time:64092ms step_avg:87.44ms
step:734/1680 train_time:64179ms step_avg:87.44ms
step:735/1680 train_time:64268ms step_avg:87.44ms
step:736/1680 train_time:64355ms step_avg:87.44ms
step:737/1680 train_time:64443ms step_avg:87.44ms
step:738/1680 train_time:64531ms step_avg:87.44ms
step:739/1680 train_time:64620ms step_avg:87.44ms
step:740/1680 train_time:64708ms step_avg:87.44ms
step:741/1680 train_time:64796ms step_avg:87.44ms
step:742/1680 train_time:64885ms step_avg:87.45ms
step:743/1680 train_time:64974ms step_avg:87.45ms
step:744/1680 train_time:65063ms step_avg:87.45ms
step:745/1680 train_time:65152ms step_avg:87.45ms
step:746/1680 train_time:65239ms step_avg:87.45ms
step:747/1680 train_time:65327ms step_avg:87.45ms
step:748/1680 train_time:65416ms step_avg:87.45ms
step:749/1680 train_time:65504ms step_avg:87.45ms
step:750/1680 train_time:65592ms step_avg:87.46ms
step:750/1680 val_loss:3.5683 train_time:65682ms step_avg:87.58ms
step:751/1680 train_time:65700ms step_avg:87.48ms
step:752/1680 train_time:65774ms step_avg:87.47ms
step:753/1680 train_time:65867ms step_avg:87.47ms
step:754/1680 train_time:65955ms step_avg:87.47ms
step:755/1680 train_time:66043ms step_avg:87.47ms
step:756/1680 train_time:66131ms step_avg:87.48ms
step:757/1680 train_time:66218ms step_avg:87.47ms
step:758/1680 train_time:66305ms step_avg:87.47ms
step:759/1680 train_time:66393ms step_avg:87.47ms
step:760/1680 train_time:66480ms step_avg:87.47ms
step:761/1680 train_time:66567ms step_avg:87.47ms
step:762/1680 train_time:66657ms step_avg:87.48ms
step:763/1680 train_time:66747ms step_avg:87.48ms
step:764/1680 train_time:66836ms step_avg:87.48ms
step:765/1680 train_time:66926ms step_avg:87.48ms
step:766/1680 train_time:67015ms step_avg:87.49ms
step:767/1680 train_time:67103ms step_avg:87.49ms
step:768/1680 train_time:67190ms step_avg:87.49ms
step:769/1680 train_time:67277ms step_avg:87.49ms
step:770/1680 train_time:67365ms step_avg:87.49ms
step:771/1680 train_time:67453ms step_avg:87.49ms
step:772/1680 train_time:67541ms step_avg:87.49ms
step:773/1680 train_time:67630ms step_avg:87.49ms
step:774/1680 train_time:67719ms step_avg:87.49ms
step:775/1680 train_time:67808ms step_avg:87.49ms
step:776/1680 train_time:67897ms step_avg:87.50ms
step:777/1680 train_time:67986ms step_avg:87.50ms
step:778/1680 train_time:68075ms step_avg:87.50ms
step:779/1680 train_time:68162ms step_avg:87.50ms
step:780/1680 train_time:68250ms step_avg:87.50ms
step:781/1680 train_time:68337ms step_avg:87.50ms
step:782/1680 train_time:68425ms step_avg:87.50ms
step:783/1680 train_time:68513ms step_avg:87.50ms
step:784/1680 train_time:68601ms step_avg:87.50ms
step:785/1680 train_time:68691ms step_avg:87.50ms
step:786/1680 train_time:68780ms step_avg:87.51ms
step:787/1680 train_time:68870ms step_avg:87.51ms
step:788/1680 train_time:68958ms step_avg:87.51ms
step:789/1680 train_time:69047ms step_avg:87.51ms
step:790/1680 train_time:69135ms step_avg:87.51ms
step:791/1680 train_time:69223ms step_avg:87.51ms
step:792/1680 train_time:69311ms step_avg:87.51ms
step:793/1680 train_time:69398ms step_avg:87.51ms
step:794/1680 train_time:69486ms step_avg:87.51ms
step:795/1680 train_time:69574ms step_avg:87.51ms
step:796/1680 train_time:69664ms step_avg:87.52ms
step:797/1680 train_time:69752ms step_avg:87.52ms
step:798/1680 train_time:69842ms step_avg:87.52ms
step:799/1680 train_time:69931ms step_avg:87.52ms
step:800/1680 train_time:70019ms step_avg:87.52ms
step:801/1680 train_time:70108ms step_avg:87.52ms
step:802/1680 train_time:70196ms step_avg:87.53ms
step:803/1680 train_time:70284ms step_avg:87.53ms
step:804/1680 train_time:70371ms step_avg:87.53ms
step:805/1680 train_time:70459ms step_avg:87.53ms
step:806/1680 train_time:70547ms step_avg:87.53ms
step:807/1680 train_time:70636ms step_avg:87.53ms
step:808/1680 train_time:70725ms step_avg:87.53ms
step:809/1680 train_time:70814ms step_avg:87.53ms
step:810/1680 train_time:70903ms step_avg:87.53ms
step:811/1680 train_time:70991ms step_avg:87.54ms
step:812/1680 train_time:71079ms step_avg:87.54ms
step:813/1680 train_time:71168ms step_avg:87.54ms
step:814/1680 train_time:71255ms step_avg:87.54ms
step:815/1680 train_time:71343ms step_avg:87.54ms
step:816/1680 train_time:71431ms step_avg:87.54ms
step:817/1680 train_time:71519ms step_avg:87.54ms
step:818/1680 train_time:71607ms step_avg:87.54ms
step:819/1680 train_time:71696ms step_avg:87.54ms
step:820/1680 train_time:71784ms step_avg:87.54ms
step:821/1680 train_time:71873ms step_avg:87.54ms
step:822/1680 train_time:71961ms step_avg:87.54ms
step:823/1680 train_time:72049ms step_avg:87.54ms
step:824/1680 train_time:72138ms step_avg:87.55ms
step:825/1680 train_time:72227ms step_avg:87.55ms
step:826/1680 train_time:72314ms step_avg:87.55ms
step:827/1680 train_time:72401ms step_avg:87.55ms
step:828/1680 train_time:72489ms step_avg:87.55ms
step:829/1680 train_time:72577ms step_avg:87.55ms
step:830/1680 train_time:72666ms step_avg:87.55ms
step:831/1680 train_time:72754ms step_avg:87.55ms
step:832/1680 train_time:72842ms step_avg:87.55ms
step:833/1680 train_time:72931ms step_avg:87.55ms
step:834/1680 train_time:73019ms step_avg:87.55ms
step:835/1680 train_time:73107ms step_avg:87.55ms
step:836/1680 train_time:73195ms step_avg:87.55ms
step:837/1680 train_time:73283ms step_avg:87.55ms
step:838/1680 train_time:73371ms step_avg:87.56ms
step:839/1680 train_time:73459ms step_avg:87.56ms
step:840/1680 train_time:73547ms step_avg:87.56ms
step:841/1680 train_time:73635ms step_avg:87.56ms
step:842/1680 train_time:73724ms step_avg:87.56ms
step:843/1680 train_time:73811ms step_avg:87.56ms
step:844/1680 train_time:73899ms step_avg:87.56ms
step:845/1680 train_time:73988ms step_avg:87.56ms
step:846/1680 train_time:74076ms step_avg:87.56ms
step:847/1680 train_time:74165ms step_avg:87.56ms
step:848/1680 train_time:74253ms step_avg:87.56ms
step:849/1680 train_time:74341ms step_avg:87.56ms
step:850/1680 train_time:74429ms step_avg:87.56ms
step:851/1680 train_time:74517ms step_avg:87.56ms
step:852/1680 train_time:74606ms step_avg:87.57ms
step:853/1680 train_time:74694ms step_avg:87.57ms
step:854/1680 train_time:74782ms step_avg:87.57ms
step:855/1680 train_time:74870ms step_avg:87.57ms
step:856/1680 train_time:74959ms step_avg:87.57ms
step:857/1680 train_time:75048ms step_avg:87.57ms
step:858/1680 train_time:75138ms step_avg:87.57ms
step:859/1680 train_time:75227ms step_avg:87.58ms
step:860/1680 train_time:75315ms step_avg:87.58ms
step:861/1680 train_time:75403ms step_avg:87.58ms
step:862/1680 train_time:75492ms step_avg:87.58ms
step:863/1680 train_time:75580ms step_avg:87.58ms
step:864/1680 train_time:75668ms step_avg:87.58ms
step:865/1680 train_time:75757ms step_avg:87.58ms
step:866/1680 train_time:75844ms step_avg:87.58ms
step:867/1680 train_time:75933ms step_avg:87.58ms
step:868/1680 train_time:76020ms step_avg:87.58ms
step:869/1680 train_time:76108ms step_avg:87.58ms
step:870/1680 train_time:76197ms step_avg:87.58ms
step:871/1680 train_time:76285ms step_avg:87.58ms
step:872/1680 train_time:76373ms step_avg:87.58ms
step:873/1680 train_time:76460ms step_avg:87.58ms
step:874/1680 train_time:76549ms step_avg:87.58ms
step:875/1680 train_time:76636ms step_avg:87.58ms
step:875/1680 val_loss:3.5233 train_time:76727ms step_avg:87.69ms
step:876/1680 train_time:76746ms step_avg:87.61ms
step:877/1680 train_time:76819ms step_avg:87.59ms
step:878/1680 train_time:76911ms step_avg:87.60ms
step:879/1680 train_time:76999ms step_avg:87.60ms
step:880/1680 train_time:77087ms step_avg:87.60ms
step:881/1680 train_time:77174ms step_avg:87.60ms
step:882/1680 train_time:77261ms step_avg:87.60ms
step:883/1680 train_time:77348ms step_avg:87.60ms
step:884/1680 train_time:77435ms step_avg:87.60ms
step:885/1680 train_time:77523ms step_avg:87.60ms
step:886/1680 train_time:77610ms step_avg:87.60ms
step:887/1680 train_time:77700ms step_avg:87.60ms
step:888/1680 train_time:77790ms step_avg:87.60ms
step:889/1680 train_time:77880ms step_avg:87.60ms
step:890/1680 train_time:77968ms step_avg:87.61ms
step:891/1680 train_time:78057ms step_avg:87.61ms
step:892/1680 train_time:78144ms step_avg:87.61ms
step:893/1680 train_time:78232ms step_avg:87.61ms
step:894/1680 train_time:78319ms step_avg:87.61ms
step:895/1680 train_time:78407ms step_avg:87.61ms
step:896/1680 train_time:78495ms step_avg:87.61ms
step:897/1680 train_time:78582ms step_avg:87.61ms
step:898/1680 train_time:78671ms step_avg:87.61ms
step:899/1680 train_time:78760ms step_avg:87.61ms
step:900/1680 train_time:78850ms step_avg:87.61ms
step:901/1680 train_time:78938ms step_avg:87.61ms
step:902/1680 train_time:79028ms step_avg:87.61ms
step:903/1680 train_time:79116ms step_avg:87.61ms
step:904/1680 train_time:79204ms step_avg:87.62ms
step:905/1680 train_time:79291ms step_avg:87.61ms
step:906/1680 train_time:79379ms step_avg:87.61ms
step:907/1680 train_time:79467ms step_avg:87.62ms
step:908/1680 train_time:79555ms step_avg:87.62ms
step:909/1680 train_time:79643ms step_avg:87.62ms
step:910/1680 train_time:79732ms step_avg:87.62ms
step:911/1680 train_time:79821ms step_avg:87.62ms
step:912/1680 train_time:79910ms step_avg:87.62ms
step:913/1680 train_time:79998ms step_avg:87.62ms
step:914/1680 train_time:80087ms step_avg:87.62ms
step:915/1680 train_time:80176ms step_avg:87.62ms
step:916/1680 train_time:80263ms step_avg:87.62ms
step:917/1680 train_time:80352ms step_avg:87.62ms
step:918/1680 train_time:80440ms step_avg:87.62ms
step:919/1680 train_time:80527ms step_avg:87.62ms
step:920/1680 train_time:80616ms step_avg:87.63ms
step:921/1680 train_time:80704ms step_avg:87.63ms
step:922/1680 train_time:80793ms step_avg:87.63ms
step:923/1680 train_time:80881ms step_avg:87.63ms
step:924/1680 train_time:80969ms step_avg:87.63ms
step:925/1680 train_time:81058ms step_avg:87.63ms
step:926/1680 train_time:81147ms step_avg:87.63ms
step:927/1680 train_time:81235ms step_avg:87.63ms
step:928/1680 train_time:81324ms step_avg:87.63ms
step:929/1680 train_time:81411ms step_avg:87.63ms
step:930/1680 train_time:81500ms step_avg:87.63ms
step:931/1680 train_time:81588ms step_avg:87.63ms
step:932/1680 train_time:81677ms step_avg:87.64ms
step:933/1680 train_time:81765ms step_avg:87.64ms
step:934/1680 train_time:81854ms step_avg:87.64ms
step:935/1680 train_time:81942ms step_avg:87.64ms
step:936/1680 train_time:82031ms step_avg:87.64ms
step:937/1680 train_time:82119ms step_avg:87.64ms
step:938/1680 train_time:82207ms step_avg:87.64ms
step:939/1680 train_time:82295ms step_avg:87.64ms
step:940/1680 train_time:82383ms step_avg:87.64ms
step:941/1680 train_time:82471ms step_avg:87.64ms
step:942/1680 train_time:82559ms step_avg:87.64ms
step:943/1680 train_time:82647ms step_avg:87.64ms
step:944/1680 train_time:82735ms step_avg:87.64ms
step:945/1680 train_time:82824ms step_avg:87.64ms
step:946/1680 train_time:82911ms step_avg:87.64ms
step:947/1680 train_time:83000ms step_avg:87.64ms
step:948/1680 train_time:83088ms step_avg:87.65ms
step:949/1680 train_time:83176ms step_avg:87.65ms
step:950/1680 train_time:83264ms step_avg:87.65ms
step:951/1680 train_time:83352ms step_avg:87.65ms
step:952/1680 train_time:83440ms step_avg:87.65ms
step:953/1680 train_time:83528ms step_avg:87.65ms
step:954/1680 train_time:83617ms step_avg:87.65ms
step:955/1680 train_time:83705ms step_avg:87.65ms
step:956/1680 train_time:83793ms step_avg:87.65ms
step:957/1680 train_time:83882ms step_avg:87.65ms
step:958/1680 train_time:83970ms step_avg:87.65ms
step:959/1680 train_time:84059ms step_avg:87.65ms
step:960/1680 train_time:84147ms step_avg:87.65ms
step:961/1680 train_time:84235ms step_avg:87.65ms
step:962/1680 train_time:84323ms step_avg:87.65ms
step:963/1680 train_time:84411ms step_avg:87.65ms
step:964/1680 train_time:84500ms step_avg:87.66ms
step:965/1680 train_time:84589ms step_avg:87.66ms
step:966/1680 train_time:84677ms step_avg:87.66ms
step:967/1680 train_time:84765ms step_avg:87.66ms
step:968/1680 train_time:84853ms step_avg:87.66ms
step:969/1680 train_time:84941ms step_avg:87.66ms
step:970/1680 train_time:85029ms step_avg:87.66ms
step:971/1680 train_time:85118ms step_avg:87.66ms
step:972/1680 train_time:85206ms step_avg:87.66ms
step:973/1680 train_time:85294ms step_avg:87.66ms
step:974/1680 train_time:85382ms step_avg:87.66ms
step:975/1680 train_time:85470ms step_avg:87.66ms
step:976/1680 train_time:85558ms step_avg:87.66ms
step:977/1680 train_time:85646ms step_avg:87.66ms
step:978/1680 train_time:85735ms step_avg:87.66ms
step:979/1680 train_time:85824ms step_avg:87.66ms
step:980/1680 train_time:85912ms step_avg:87.66ms
step:981/1680 train_time:86000ms step_avg:87.67ms
step:982/1680 train_time:86088ms step_avg:87.67ms
step:983/1680 train_time:86176ms step_avg:87.67ms
step:984/1680 train_time:86264ms step_avg:87.67ms
step:985/1680 train_time:86353ms step_avg:87.67ms
step:986/1680 train_time:86441ms step_avg:87.67ms
step:987/1680 train_time:86528ms step_avg:87.67ms
step:988/1680 train_time:86617ms step_avg:87.67ms
step:989/1680 train_time:86706ms step_avg:87.67ms
step:990/1680 train_time:86794ms step_avg:87.67ms
step:991/1680 train_time:86883ms step_avg:87.67ms
step:992/1680 train_time:86972ms step_avg:87.67ms
step:993/1680 train_time:87060ms step_avg:87.67ms
step:994/1680 train_time:87148ms step_avg:87.67ms
step:995/1680 train_time:87237ms step_avg:87.68ms
step:996/1680 train_time:87325ms step_avg:87.68ms
step:997/1680 train_time:87413ms step_avg:87.68ms
step:998/1680 train_time:87501ms step_avg:87.68ms
step:999/1680 train_time:87589ms step_avg:87.68ms
step:1000/1680 train_time:87678ms step_avg:87.68ms
step:1000/1680 val_loss:3.4726 train_time:87768ms step_avg:87.77ms
step:1001/1680 train_time:87787ms step_avg:87.70ms
step:1002/1680 train_time:87858ms step_avg:87.68ms
step:1003/1680 train_time:87949ms step_avg:87.69ms
step:1004/1680 train_time:88037ms step_avg:87.69ms
step:1005/1680 train_time:88125ms step_avg:87.69ms
step:1006/1680 train_time:88212ms step_avg:87.69ms
step:1007/1680 train_time:88299ms step_avg:87.68ms
step:1008/1680 train_time:88386ms step_avg:87.68ms
step:1009/1680 train_time:88473ms step_avg:87.68ms
step:1010/1680 train_time:88562ms step_avg:87.68ms
step:1011/1680 train_time:88649ms step_avg:87.68ms
step:1012/1680 train_time:88738ms step_avg:87.69ms
step:1013/1680 train_time:88828ms step_avg:87.69ms
step:1014/1680 train_time:88918ms step_avg:87.69ms
step:1015/1680 train_time:89006ms step_avg:87.69ms
step:1016/1680 train_time:89095ms step_avg:87.69ms
step:1017/1680 train_time:89183ms step_avg:87.69ms
step:1018/1680 train_time:89270ms step_avg:87.69ms
step:1019/1680 train_time:89358ms step_avg:87.69ms
step:1020/1680 train_time:89446ms step_avg:87.69ms
step:1021/1680 train_time:89533ms step_avg:87.69ms
step:1022/1680 train_time:89621ms step_avg:87.69ms
step:1023/1680 train_time:89709ms step_avg:87.69ms
step:1024/1680 train_time:89798ms step_avg:87.69ms
step:1025/1680 train_time:89888ms step_avg:87.70ms
step:1026/1680 train_time:89977ms step_avg:87.70ms
step:1027/1680 train_time:90065ms step_avg:87.70ms
step:1028/1680 train_time:90153ms step_avg:87.70ms
step:1029/1680 train_time:90241ms step_avg:87.70ms
step:1030/1680 train_time:90329ms step_avg:87.70ms
step:1031/1680 train_time:90417ms step_avg:87.70ms
step:1032/1680 train_time:90505ms step_avg:87.70ms
step:1033/1680 train_time:90593ms step_avg:87.70ms
step:1034/1680 train_time:90682ms step_avg:87.70ms
step:1035/1680 train_time:90770ms step_avg:87.70ms
step:1036/1680 train_time:90859ms step_avg:87.70ms
step:1037/1680 train_time:90948ms step_avg:87.70ms
step:1038/1680 train_time:91036ms step_avg:87.70ms
step:1039/1680 train_time:91125ms step_avg:87.70ms
step:1040/1680 train_time:91213ms step_avg:87.70ms
step:1041/1680 train_time:91301ms step_avg:87.70ms
step:1042/1680 train_time:91389ms step_avg:87.70ms
step:1043/1680 train_time:91477ms step_avg:87.71ms
step:1044/1680 train_time:91566ms step_avg:87.71ms
step:1045/1680 train_time:91654ms step_avg:87.71ms
step:1046/1680 train_time:91743ms step_avg:87.71ms
step:1047/1680 train_time:91831ms step_avg:87.71ms
step:1048/1680 train_time:91919ms step_avg:87.71ms
step:1049/1680 train_time:92007ms step_avg:87.71ms
step:1050/1680 train_time:92096ms step_avg:87.71ms
step:1051/1680 train_time:92184ms step_avg:87.71ms
step:1052/1680 train_time:92273ms step_avg:87.71ms
step:1053/1680 train_time:92361ms step_avg:87.71ms
step:1054/1680 train_time:92449ms step_avg:87.71ms
step:1055/1680 train_time:92537ms step_avg:87.71ms
step:1056/1680 train_time:92626ms step_avg:87.71ms
step:1057/1680 train_time:92715ms step_avg:87.71ms
step:1058/1680 train_time:92803ms step_avg:87.72ms
step:1059/1680 train_time:92892ms step_avg:87.72ms
step:1060/1680 train_time:92979ms step_avg:87.72ms
step:1061/1680 train_time:93068ms step_avg:87.72ms
step:1062/1680 train_time:93157ms step_avg:87.72ms
step:1063/1680 train_time:93245ms step_avg:87.72ms
step:1064/1680 train_time:93332ms step_avg:87.72ms
step:1065/1680 train_time:93420ms step_avg:87.72ms
step:1066/1680 train_time:93509ms step_avg:87.72ms
step:1067/1680 train_time:93597ms step_avg:87.72ms
step:1068/1680 train_time:93686ms step_avg:87.72ms
step:1069/1680 train_time:93776ms step_avg:87.72ms
step:1070/1680 train_time:93865ms step_avg:87.72ms
step:1071/1680 train_time:93953ms step_avg:87.72ms
step:1072/1680 train_time:94042ms step_avg:87.73ms
step:1073/1680 train_time:94130ms step_avg:87.73ms
step:1074/1680 train_time:94218ms step_avg:87.73ms
step:1075/1680 train_time:94306ms step_avg:87.73ms
step:1076/1680 train_time:94393ms step_avg:87.73ms
step:1077/1680 train_time:94482ms step_avg:87.73ms
step:1078/1680 train_time:94569ms step_avg:87.73ms
step:1079/1680 train_time:94658ms step_avg:87.73ms
step:1080/1680 train_time:94748ms step_avg:87.73ms
step:1081/1680 train_time:94836ms step_avg:87.73ms
step:1082/1680 train_time:94925ms step_avg:87.73ms
step:1083/1680 train_time:95013ms step_avg:87.73ms
step:1084/1680 train_time:95101ms step_avg:87.73ms
step:1085/1680 train_time:95189ms step_avg:87.73ms
step:1086/1680 train_time:95277ms step_avg:87.73ms
step:1087/1680 train_time:95365ms step_avg:87.73ms
step:1088/1680 train_time:95454ms step_avg:87.73ms
step:1089/1680 train_time:95542ms step_avg:87.73ms
step:1090/1680 train_time:95630ms step_avg:87.73ms
step:1091/1680 train_time:95719ms step_avg:87.73ms
step:1092/1680 train_time:95807ms step_avg:87.74ms
step:1093/1680 train_time:95896ms step_avg:87.74ms
step:1094/1680 train_time:95985ms step_avg:87.74ms
step:1095/1680 train_time:96074ms step_avg:87.74ms
step:1096/1680 train_time:96162ms step_avg:87.74ms
step:1097/1680 train_time:96252ms step_avg:87.74ms
step:1098/1680 train_time:96340ms step_avg:87.74ms
step:1099/1680 train_time:96429ms step_avg:87.74ms
step:1100/1680 train_time:96518ms step_avg:87.74ms
step:1101/1680 train_time:96607ms step_avg:87.75ms
step:1102/1680 train_time:96696ms step_avg:87.75ms
step:1103/1680 train_time:96785ms step_avg:87.75ms
step:1104/1680 train_time:96874ms step_avg:87.75ms
step:1105/1680 train_time:96964ms step_avg:87.75ms
step:1106/1680 train_time:97054ms step_avg:87.75ms
step:1107/1680 train_time:97142ms step_avg:87.75ms
step:1108/1680 train_time:97232ms step_avg:87.75ms
step:1109/1680 train_time:97321ms step_avg:87.76ms
step:1110/1680 train_time:97409ms step_avg:87.76ms
step:1111/1680 train_time:97498ms step_avg:87.76ms
step:1112/1680 train_time:97587ms step_avg:87.76ms
step:1113/1680 train_time:97676ms step_avg:87.76ms
step:1114/1680 train_time:97764ms step_avg:87.76ms
step:1115/1680 train_time:97853ms step_avg:87.76ms
step:1116/1680 train_time:97943ms step_avg:87.76ms
step:1117/1680 train_time:98032ms step_avg:87.76ms
step:1118/1680 train_time:98121ms step_avg:87.76ms
step:1119/1680 train_time:98210ms step_avg:87.77ms
step:1120/1680 train_time:98298ms step_avg:87.77ms
step:1121/1680 train_time:98387ms step_avg:87.77ms
step:1122/1680 train_time:98475ms step_avg:87.77ms
step:1123/1680 train_time:98565ms step_avg:87.77ms
step:1124/1680 train_time:98654ms step_avg:87.77ms
step:1125/1680 train_time:98744ms step_avg:87.77ms
step:1125/1680 val_loss:3.4192 train_time:98835ms step_avg:87.85ms
step:1126/1680 train_time:98854ms step_avg:87.79ms
step:1127/1680 train_time:98924ms step_avg:87.78ms
step:1128/1680 train_time:99015ms step_avg:87.78ms
step:1129/1680 train_time:99106ms step_avg:87.78ms
step:1130/1680 train_time:99195ms step_avg:87.78ms
step:1131/1680 train_time:99283ms step_avg:87.78ms
step:1132/1680 train_time:99371ms step_avg:87.78ms
step:1133/1680 train_time:99459ms step_avg:87.78ms
step:1134/1680 train_time:99547ms step_avg:87.78ms
step:1135/1680 train_time:99634ms step_avg:87.78ms
step:1136/1680 train_time:99724ms step_avg:87.79ms
step:1137/1680 train_time:99815ms step_avg:87.79ms
step:1138/1680 train_time:99905ms step_avg:87.79ms
step:1139/1680 train_time:99996ms step_avg:87.79ms
step:1140/1680 train_time:100086ms step_avg:87.79ms
step:1141/1680 train_time:100175ms step_avg:87.80ms
step:1142/1680 train_time:100263ms step_avg:87.80ms
step:1143/1680 train_time:100352ms step_avg:87.80ms
step:1144/1680 train_time:100440ms step_avg:87.80ms
step:1145/1680 train_time:100528ms step_avg:87.80ms
step:1146/1680 train_time:100615ms step_avg:87.80ms
step:1147/1680 train_time:100704ms step_avg:87.80ms
step:1148/1680 train_time:100794ms step_avg:87.80ms
step:1149/1680 train_time:100884ms step_avg:87.80ms
step:1150/1680 train_time:100975ms step_avg:87.80ms
step:1151/1680 train_time:101065ms step_avg:87.81ms
step:1152/1680 train_time:101154ms step_avg:87.81ms
step:1153/1680 train_time:101243ms step_avg:87.81ms
step:1154/1680 train_time:101331ms step_avg:87.81ms
step:1155/1680 train_time:101418ms step_avg:87.81ms
step:1156/1680 train_time:101506ms step_avg:87.81ms
step:1157/1680 train_time:101595ms step_avg:87.81ms
step:1158/1680 train_time:101683ms step_avg:87.81ms
step:1159/1680 train_time:101772ms step_avg:87.81ms
step:1160/1680 train_time:101861ms step_avg:87.81ms
step:1161/1680 train_time:101952ms step_avg:87.81ms
step:1162/1680 train_time:102041ms step_avg:87.82ms
step:1163/1680 train_time:102131ms step_avg:87.82ms
step:1164/1680 train_time:102219ms step_avg:87.82ms
step:1165/1680 train_time:102308ms step_avg:87.82ms
step:1166/1680 train_time:102396ms step_avg:87.82ms
step:1167/1680 train_time:102485ms step_avg:87.82ms
step:1168/1680 train_time:102573ms step_avg:87.82ms
step:1169/1680 train_time:102662ms step_avg:87.82ms
step:1170/1680 train_time:102751ms step_avg:87.82ms
step:1171/1680 train_time:102840ms step_avg:87.82ms
step:1172/1680 train_time:102928ms step_avg:87.82ms
step:1173/1680 train_time:103018ms step_avg:87.82ms
step:1174/1680 train_time:103106ms step_avg:87.82ms
step:1175/1680 train_time:103195ms step_avg:87.83ms
step:1176/1680 train_time:103284ms step_avg:87.83ms
step:1177/1680 train_time:103373ms step_avg:87.83ms
step:1178/1680 train_time:103463ms step_avg:87.83ms
step:1179/1680 train_time:103551ms step_avg:87.83ms
step:1180/1680 train_time:103639ms step_avg:87.83ms
step:1181/1680 train_time:103728ms step_avg:87.83ms
step:1182/1680 train_time:103817ms step_avg:87.83ms
step:1183/1680 train_time:103907ms step_avg:87.83ms
step:1184/1680 train_time:103996ms step_avg:87.83ms
step:1185/1680 train_time:104085ms step_avg:87.84ms
step:1186/1680 train_time:104173ms step_avg:87.84ms
step:1187/1680 train_time:104262ms step_avg:87.84ms
step:1188/1680 train_time:104351ms step_avg:87.84ms
step:1189/1680 train_time:104439ms step_avg:87.84ms
step:1190/1680 train_time:104528ms step_avg:87.84ms
step:1191/1680 train_time:104617ms step_avg:87.84ms
step:1192/1680 train_time:104706ms step_avg:87.84ms
step:1193/1680 train_time:104795ms step_avg:87.84ms
step:1194/1680 train_time:104885ms step_avg:87.84ms
step:1195/1680 train_time:104974ms step_avg:87.84ms
step:1196/1680 train_time:105062ms step_avg:87.84ms
step:1197/1680 train_time:105151ms step_avg:87.85ms
step:1198/1680 train_time:105240ms step_avg:87.85ms
step:1199/1680 train_time:105329ms step_avg:87.85ms
step:1200/1680 train_time:105418ms step_avg:87.85ms
step:1201/1680 train_time:105506ms step_avg:87.85ms
step:1202/1680 train_time:105595ms step_avg:87.85ms
step:1203/1680 train_time:105683ms step_avg:87.85ms
step:1204/1680 train_time:105772ms step_avg:87.85ms
step:1205/1680 train_time:105861ms step_avg:87.85ms
step:1206/1680 train_time:105950ms step_avg:87.85ms
step:1207/1680 train_time:106040ms step_avg:87.85ms
step:1208/1680 train_time:106128ms step_avg:87.85ms
step:1209/1680 train_time:106218ms step_avg:87.86ms
step:1210/1680 train_time:106307ms step_avg:87.86ms
step:1211/1680 train_time:106396ms step_avg:87.86ms
step:1212/1680 train_time:106486ms step_avg:87.86ms
step:1213/1680 train_time:106575ms step_avg:87.86ms
step:1214/1680 train_time:106663ms step_avg:87.86ms
step:1215/1680 train_time:106753ms step_avg:87.86ms
step:1216/1680 train_time:106842ms step_avg:87.86ms
step:1217/1680 train_time:106931ms step_avg:87.86ms
step:1218/1680 train_time:107020ms step_avg:87.87ms
step:1219/1680 train_time:107110ms step_avg:87.87ms
step:1220/1680 train_time:107200ms step_avg:87.87ms
step:1221/1680 train_time:107290ms step_avg:87.87ms
step:1222/1680 train_time:107379ms step_avg:87.87ms
step:1223/1680 train_time:107467ms step_avg:87.87ms
step:1224/1680 train_time:107556ms step_avg:87.87ms
step:1225/1680 train_time:107644ms step_avg:87.87ms
step:1226/1680 train_time:107733ms step_avg:87.87ms
step:1227/1680 train_time:107822ms step_avg:87.87ms
step:1228/1680 train_time:107911ms step_avg:87.88ms
step:1229/1680 train_time:108000ms step_avg:87.88ms
step:1230/1680 train_time:108089ms step_avg:87.88ms
step:1231/1680 train_time:108180ms step_avg:87.88ms
step:1232/1680 train_time:108270ms step_avg:87.88ms
step:1233/1680 train_time:108359ms step_avg:87.88ms
step:1234/1680 train_time:108448ms step_avg:87.88ms
step:1235/1680 train_time:108537ms step_avg:87.88ms
step:1236/1680 train_time:108626ms step_avg:87.88ms
step:1237/1680 train_time:108715ms step_avg:87.89ms
step:1238/1680 train_time:108805ms step_avg:87.89ms
step:1239/1680 train_time:108894ms step_avg:87.89ms
step:1240/1680 train_time:108983ms step_avg:87.89ms
step:1241/1680 train_time:109072ms step_avg:87.89ms
step:1242/1680 train_time:109161ms step_avg:87.89ms
step:1243/1680 train_time:109250ms step_avg:87.89ms
step:1244/1680 train_time:109340ms step_avg:87.89ms
step:1245/1680 train_time:109429ms step_avg:87.89ms
step:1246/1680 train_time:109518ms step_avg:87.90ms
step:1247/1680 train_time:109607ms step_avg:87.90ms
step:1248/1680 train_time:109695ms step_avg:87.90ms
step:1249/1680 train_time:109784ms step_avg:87.90ms
step:1250/1680 train_time:109873ms step_avg:87.90ms
step:1250/1680 val_loss:3.3811 train_time:109963ms step_avg:87.97ms
step:1251/1680 train_time:109982ms step_avg:87.91ms
step:1252/1680 train_time:110057ms step_avg:87.91ms
step:1253/1680 train_time:110149ms step_avg:87.91ms
step:1254/1680 train_time:110237ms step_avg:87.91ms
step:1255/1680 train_time:110325ms step_avg:87.91ms
step:1256/1680 train_time:110413ms step_avg:87.91ms
step:1257/1680 train_time:110500ms step_avg:87.91ms
step:1258/1680 train_time:110588ms step_avg:87.91ms
step:1259/1680 train_time:110677ms step_avg:87.91ms
step:1260/1680 train_time:110766ms step_avg:87.91ms
step:1261/1680 train_time:110854ms step_avg:87.91ms
step:1262/1680 train_time:110945ms step_avg:87.91ms
step:1263/1680 train_time:111038ms step_avg:87.92ms
step:1264/1680 train_time:111128ms step_avg:87.92ms
step:1265/1680 train_time:111218ms step_avg:87.92ms
step:1266/1680 train_time:111306ms step_avg:87.92ms
step:1267/1680 train_time:111394ms step_avg:87.92ms
step:1268/1680 train_time:111482ms step_avg:87.92ms
step:1269/1680 train_time:111570ms step_avg:87.92ms
step:1270/1680 train_time:111659ms step_avg:87.92ms
step:1271/1680 train_time:111747ms step_avg:87.92ms
step:1272/1680 train_time:111836ms step_avg:87.92ms
step:1273/1680 train_time:111927ms step_avg:87.92ms
step:1274/1680 train_time:112019ms step_avg:87.93ms
step:1275/1680 train_time:112108ms step_avg:87.93ms
step:1276/1680 train_time:112198ms step_avg:87.93ms
step:1277/1680 train_time:112288ms step_avg:87.93ms
step:1278/1680 train_time:112377ms step_avg:87.93ms
step:1279/1680 train_time:112466ms step_avg:87.93ms
step:1280/1680 train_time:112554ms step_avg:87.93ms
step:1281/1680 train_time:112642ms step_avg:87.93ms
step:1282/1680 train_time:112730ms step_avg:87.93ms
step:1283/1680 train_time:112820ms step_avg:87.93ms
step:1284/1680 train_time:112909ms step_avg:87.94ms
step:1285/1680 train_time:113000ms step_avg:87.94ms
step:1286/1680 train_time:113090ms step_avg:87.94ms
step:1287/1680 train_time:113180ms step_avg:87.94ms
step:1288/1680 train_time:113269ms step_avg:87.94ms
step:1289/1680 train_time:113358ms step_avg:87.94ms
step:1290/1680 train_time:113447ms step_avg:87.94ms
step:1291/1680 train_time:113537ms step_avg:87.94ms
step:1292/1680 train_time:113625ms step_avg:87.95ms
step:1293/1680 train_time:113713ms step_avg:87.95ms
step:1294/1680 train_time:113802ms step_avg:87.95ms
step:1295/1680 train_time:113891ms step_avg:87.95ms
step:1296/1680 train_time:113980ms step_avg:87.95ms
step:1297/1680 train_time:114069ms step_avg:87.95ms
step:1298/1680 train_time:114159ms step_avg:87.95ms
step:1299/1680 train_time:114248ms step_avg:87.95ms
step:1300/1680 train_time:114337ms step_avg:87.95ms
step:1301/1680 train_time:114427ms step_avg:87.95ms
step:1302/1680 train_time:114515ms step_avg:87.95ms
step:1303/1680 train_time:114603ms step_avg:87.95ms
step:1304/1680 train_time:114692ms step_avg:87.95ms
step:1305/1680 train_time:114781ms step_avg:87.95ms
step:1306/1680 train_time:114869ms step_avg:87.96ms
step:1307/1680 train_time:114958ms step_avg:87.96ms
step:1308/1680 train_time:115047ms step_avg:87.96ms
step:1309/1680 train_time:115137ms step_avg:87.96ms
step:1310/1680 train_time:115227ms step_avg:87.96ms
step:1311/1680 train_time:115316ms step_avg:87.96ms
step:1312/1680 train_time:115405ms step_avg:87.96ms
step:1313/1680 train_time:115494ms step_avg:87.96ms
step:1314/1680 train_time:115583ms step_avg:87.96ms
step:1315/1680 train_time:115672ms step_avg:87.96ms
step:1316/1680 train_time:115761ms step_avg:87.96ms
step:1317/1680 train_time:115850ms step_avg:87.97ms
step:1318/1680 train_time:115939ms step_avg:87.97ms
step:1319/1680 train_time:116028ms step_avg:87.97ms
step:1320/1680 train_time:116119ms step_avg:87.97ms
step:1321/1680 train_time:116209ms step_avg:87.97ms
step:1322/1680 train_time:116299ms step_avg:87.97ms
step:1323/1680 train_time:116388ms step_avg:87.97ms
step:1324/1680 train_time:116477ms step_avg:87.97ms
step:1325/1680 train_time:116566ms step_avg:87.97ms
step:1326/1680 train_time:116655ms step_avg:87.97ms
step:1327/1680 train_time:116744ms step_avg:87.98ms
step:1328/1680 train_time:116832ms step_avg:87.98ms
step:1329/1680 train_time:116922ms step_avg:87.98ms
step:1330/1680 train_time:117011ms step_avg:87.98ms
step:1331/1680 train_time:117100ms step_avg:87.98ms
step:1332/1680 train_time:117190ms step_avg:87.98ms
step:1333/1680 train_time:117279ms step_avg:87.98ms
step:1334/1680 train_time:117368ms step_avg:87.98ms
step:1335/1680 train_time:117457ms step_avg:87.98ms
step:1336/1680 train_time:117546ms step_avg:87.98ms
step:1337/1680 train_time:117636ms step_avg:87.99ms
step:1338/1680 train_time:117725ms step_avg:87.99ms
step:1339/1680 train_time:117813ms step_avg:87.99ms
step:1340/1680 train_time:117902ms step_avg:87.99ms
step:1341/1680 train_time:117991ms step_avg:87.99ms
step:1342/1680 train_time:118079ms step_avg:87.99ms
step:1343/1680 train_time:118168ms step_avg:87.99ms
step:1344/1680 train_time:118258ms step_avg:87.99ms
step:1345/1680 train_time:118347ms step_avg:87.99ms
step:1346/1680 train_time:118436ms step_avg:87.99ms
step:1347/1680 train_time:118526ms step_avg:87.99ms
step:1348/1680 train_time:118615ms step_avg:87.99ms
step:1349/1680 train_time:118704ms step_avg:87.99ms
step:1350/1680 train_time:118793ms step_avg:87.99ms
step:1351/1680 train_time:118882ms step_avg:88.00ms
step:1352/1680 train_time:118971ms step_avg:88.00ms
step:1353/1680 train_time:119060ms step_avg:88.00ms
step:1354/1680 train_time:119150ms step_avg:88.00ms
step:1355/1680 train_time:119239ms step_avg:88.00ms
step:1356/1680 train_time:119329ms step_avg:88.00ms
step:1357/1680 train_time:119419ms step_avg:88.00ms
step:1358/1680 train_time:119507ms step_avg:88.00ms
step:1359/1680 train_time:119595ms step_avg:88.00ms
step:1360/1680 train_time:119684ms step_avg:88.00ms
step:1361/1680 train_time:119772ms step_avg:88.00ms
step:1362/1680 train_time:119862ms step_avg:88.00ms
step:1363/1680 train_time:119951ms step_avg:88.00ms
step:1364/1680 train_time:120039ms step_avg:88.01ms
step:1365/1680 train_time:120128ms step_avg:88.01ms
step:1366/1680 train_time:120219ms step_avg:88.01ms
step:1367/1680 train_time:120308ms step_avg:88.01ms
step:1368/1680 train_time:120397ms step_avg:88.01ms
step:1369/1680 train_time:120487ms step_avg:88.01ms
step:1370/1680 train_time:120575ms step_avg:88.01ms
step:1371/1680 train_time:120664ms step_avg:88.01ms
step:1372/1680 train_time:120753ms step_avg:88.01ms
step:1373/1680 train_time:120842ms step_avg:88.01ms
step:1374/1680 train_time:120930ms step_avg:88.01ms
step:1375/1680 train_time:121019ms step_avg:88.01ms
step:1375/1680 val_loss:3.3468 train_time:121109ms step_avg:88.08ms
step:1376/1680 train_time:121127ms step_avg:88.03ms
step:1377/1680 train_time:121203ms step_avg:88.02ms
step:1378/1680 train_time:121295ms step_avg:88.02ms
step:1379/1680 train_time:121386ms step_avg:88.02ms
step:1380/1680 train_time:121474ms step_avg:88.02ms
step:1381/1680 train_time:121564ms step_avg:88.03ms
step:1382/1680 train_time:121652ms step_avg:88.03ms
step:1383/1680 train_time:121739ms step_avg:88.02ms
step:1384/1680 train_time:121827ms step_avg:88.03ms
step:1385/1680 train_time:121915ms step_avg:88.03ms
step:1386/1680 train_time:122003ms step_avg:88.03ms
step:1387/1680 train_time:122093ms step_avg:88.03ms
step:1388/1680 train_time:122185ms step_avg:88.03ms
step:1389/1680 train_time:122276ms step_avg:88.03ms
step:1390/1680 train_time:122367ms step_avg:88.03ms
step:1391/1680 train_time:122456ms step_avg:88.03ms
step:1392/1680 train_time:122545ms step_avg:88.04ms
step:1393/1680 train_time:122633ms step_avg:88.04ms
step:1394/1680 train_time:122721ms step_avg:88.04ms
step:1395/1680 train_time:122810ms step_avg:88.04ms
step:1396/1680 train_time:122898ms step_avg:88.04ms
step:1397/1680 train_time:122987ms step_avg:88.04ms
step:1398/1680 train_time:123076ms step_avg:88.04ms
step:1399/1680 train_time:123167ms step_avg:88.04ms
step:1400/1680 train_time:123257ms step_avg:88.04ms
step:1401/1680 train_time:123347ms step_avg:88.04ms
step:1402/1680 train_time:123437ms step_avg:88.04ms
step:1403/1680 train_time:123527ms step_avg:88.05ms
step:1404/1680 train_time:123617ms step_avg:88.05ms
step:1405/1680 train_time:123706ms step_avg:88.05ms
step:1406/1680 train_time:123794ms step_avg:88.05ms
step:1407/1680 train_time:123883ms step_avg:88.05ms
step:1408/1680 train_time:123972ms step_avg:88.05ms
step:1409/1680 train_time:124061ms step_avg:88.05ms
step:1410/1680 train_time:124150ms step_avg:88.05ms
step:1411/1680 train_time:124239ms step_avg:88.05ms
step:1412/1680 train_time:124328ms step_avg:88.05ms
step:1413/1680 train_time:124417ms step_avg:88.05ms
step:1414/1680 train_time:124508ms step_avg:88.05ms
step:1415/1680 train_time:124597ms step_avg:88.05ms
step:1416/1680 train_time:124686ms step_avg:88.06ms
step:1417/1680 train_time:124775ms step_avg:88.06ms
step:1418/1680 train_time:124864ms step_avg:88.06ms
step:1419/1680 train_time:124952ms step_avg:88.06ms
step:1420/1680 train_time:125041ms step_avg:88.06ms
step:1421/1680 train_time:125130ms step_avg:88.06ms
step:1422/1680 train_time:125220ms step_avg:88.06ms
step:1423/1680 train_time:125309ms step_avg:88.06ms
step:1424/1680 train_time:125398ms step_avg:88.06ms
step:1425/1680 train_time:125488ms step_avg:88.06ms
step:1426/1680 train_time:125577ms step_avg:88.06ms
step:1427/1680 train_time:125666ms step_avg:88.06ms
step:1428/1680 train_time:125755ms step_avg:88.06ms
step:1429/1680 train_time:125844ms step_avg:88.06ms
step:1430/1680 train_time:125933ms step_avg:88.06ms
step:1431/1680 train_time:126022ms step_avg:88.07ms
step:1432/1680 train_time:126110ms step_avg:88.07ms
step:1433/1680 train_time:126199ms step_avg:88.07ms
step:1434/1680 train_time:126288ms step_avg:88.07ms
step:1435/1680 train_time:126378ms step_avg:88.07ms
step:1436/1680 train_time:126468ms step_avg:88.07ms
step:1437/1680 train_time:126557ms step_avg:88.07ms
step:1438/1680 train_time:126646ms step_avg:88.07ms
step:1439/1680 train_time:126734ms step_avg:88.07ms
step:1440/1680 train_time:126823ms step_avg:88.07ms
step:1441/1680 train_time:126912ms step_avg:88.07ms
step:1442/1680 train_time:127001ms step_avg:88.07ms
step:1443/1680 train_time:127090ms step_avg:88.07ms
step:1444/1680 train_time:127180ms step_avg:88.07ms
step:1445/1680 train_time:127268ms step_avg:88.07ms
step:1446/1680 train_time:127357ms step_avg:88.08ms
step:1447/1680 train_time:127447ms step_avg:88.08ms
step:1448/1680 train_time:127536ms step_avg:88.08ms
step:1449/1680 train_time:127624ms step_avg:88.08ms
step:1450/1680 train_time:127714ms step_avg:88.08ms
step:1451/1680 train_time:127803ms step_avg:88.08ms
step:1452/1680 train_time:127891ms step_avg:88.08ms
step:1453/1680 train_time:127979ms step_avg:88.08ms
step:1454/1680 train_time:128069ms step_avg:88.08ms
step:1455/1680 train_time:128158ms step_avg:88.08ms
step:1456/1680 train_time:128247ms step_avg:88.08ms
step:1457/1680 train_time:128336ms step_avg:88.08ms
step:1458/1680 train_time:128426ms step_avg:88.08ms
step:1459/1680 train_time:128515ms step_avg:88.08ms
step:1460/1680 train_time:128605ms step_avg:88.09ms
step:1461/1680 train_time:128694ms step_avg:88.09ms
step:1462/1680 train_time:128783ms step_avg:88.09ms
step:1463/1680 train_time:128872ms step_avg:88.09ms
step:1464/1680 train_time:128961ms step_avg:88.09ms
step:1465/1680 train_time:129050ms step_avg:88.09ms
step:1466/1680 train_time:129138ms step_avg:88.09ms
step:1467/1680 train_time:129227ms step_avg:88.09ms
step:1468/1680 train_time:129316ms step_avg:88.09ms
step:1469/1680 train_time:129406ms step_avg:88.09ms
step:1470/1680 train_time:129495ms step_avg:88.09ms
step:1471/1680 train_time:129584ms step_avg:88.09ms
step:1472/1680 train_time:129674ms step_avg:88.09ms
step:1473/1680 train_time:129763ms step_avg:88.09ms
step:1474/1680 train_time:129852ms step_avg:88.10ms
step:1475/1680 train_time:129942ms step_avg:88.10ms
step:1476/1680 train_time:130030ms step_avg:88.10ms
step:1477/1680 train_time:130119ms step_avg:88.10ms
step:1478/1680 train_time:130208ms step_avg:88.10ms
step:1479/1680 train_time:130297ms step_avg:88.10ms
step:1480/1680 train_time:130387ms step_avg:88.10ms
step:1481/1680 train_time:130476ms step_avg:88.10ms
step:1482/1680 train_time:130566ms step_avg:88.10ms
step:1483/1680 train_time:130655ms step_avg:88.10ms
step:1484/1680 train_time:130744ms step_avg:88.10ms
step:1485/1680 train_time:130833ms step_avg:88.10ms
step:1486/1680 train_time:130922ms step_avg:88.10ms
step:1487/1680 train_time:131011ms step_avg:88.10ms
step:1488/1680 train_time:131100ms step_avg:88.11ms
step:1489/1680 train_time:131190ms step_avg:88.11ms
step:1490/1680 train_time:131278ms step_avg:88.11ms
step:1491/1680 train_time:131368ms step_avg:88.11ms
step:1492/1680 train_time:131457ms step_avg:88.11ms
step:1493/1680 train_time:131545ms step_avg:88.11ms
step:1494/1680 train_time:131635ms step_avg:88.11ms
step:1495/1680 train_time:131724ms step_avg:88.11ms
step:1496/1680 train_time:131812ms step_avg:88.11ms
step:1497/1680 train_time:131902ms step_avg:88.11ms
step:1498/1680 train_time:131992ms step_avg:88.11ms
step:1499/1680 train_time:132081ms step_avg:88.11ms
step:1500/1680 train_time:132170ms step_avg:88.11ms
step:1500/1680 val_loss:3.3167 train_time:132260ms step_avg:88.17ms
step:1501/1680 train_time:132279ms step_avg:88.13ms
step:1502/1680 train_time:132351ms step_avg:88.12ms
step:1503/1680 train_time:132443ms step_avg:88.12ms
step:1504/1680 train_time:132533ms step_avg:88.12ms
step:1505/1680 train_time:132621ms step_avg:88.12ms
step:1506/1680 train_time:132709ms step_avg:88.12ms
step:1507/1680 train_time:132796ms step_avg:88.12ms
step:1508/1680 train_time:132885ms step_avg:88.12ms
step:1509/1680 train_time:132972ms step_avg:88.12ms
step:1510/1680 train_time:133061ms step_avg:88.12ms
step:1511/1680 train_time:133150ms step_avg:88.12ms
step:1512/1680 train_time:133240ms step_avg:88.12ms
step:1513/1680 train_time:133330ms step_avg:88.12ms
step:1514/1680 train_time:133424ms step_avg:88.13ms
step:1515/1680 train_time:133514ms step_avg:88.13ms
step:1516/1680 train_time:133603ms step_avg:88.13ms
step:1517/1680 train_time:133691ms step_avg:88.13ms
step:1518/1680 train_time:133780ms step_avg:88.13ms
step:1519/1680 train_time:133868ms step_avg:88.13ms
step:1520/1680 train_time:133956ms step_avg:88.13ms
step:1521/1680 train_time:134044ms step_avg:88.13ms
step:1522/1680 train_time:134133ms step_avg:88.13ms
step:1523/1680 train_time:134222ms step_avg:88.13ms
step:1524/1680 train_time:134312ms step_avg:88.13ms
step:1525/1680 train_time:134402ms step_avg:88.13ms
step:1526/1680 train_time:134493ms step_avg:88.13ms
step:1527/1680 train_time:134583ms step_avg:88.14ms
step:1528/1680 train_time:134672ms step_avg:88.14ms
step:1529/1680 train_time:134761ms step_avg:88.14ms
step:1530/1680 train_time:134850ms step_avg:88.14ms
step:1531/1680 train_time:134938ms step_avg:88.14ms
step:1532/1680 train_time:135027ms step_avg:88.14ms
step:1533/1680 train_time:135115ms step_avg:88.14ms
step:1534/1680 train_time:135204ms step_avg:88.14ms
step:1535/1680 train_time:135293ms step_avg:88.14ms
step:1536/1680 train_time:135382ms step_avg:88.14ms
step:1537/1680 train_time:135472ms step_avg:88.14ms
step:1538/1680 train_time:135561ms step_avg:88.14ms
step:1539/1680 train_time:135650ms step_avg:88.14ms
step:1540/1680 train_time:135739ms step_avg:88.14ms
step:1541/1680 train_time:135828ms step_avg:88.14ms
step:1542/1680 train_time:135917ms step_avg:88.14ms
step:1543/1680 train_time:136006ms step_avg:88.14ms
step:1544/1680 train_time:136095ms step_avg:88.14ms
step:1545/1680 train_time:136184ms step_avg:88.15ms
step:1546/1680 train_time:136272ms step_avg:88.15ms
step:1547/1680 train_time:136362ms step_avg:88.15ms
step:1548/1680 train_time:136451ms step_avg:88.15ms
step:1549/1680 train_time:136539ms step_avg:88.15ms
step:1550/1680 train_time:136629ms step_avg:88.15ms
step:1551/1680 train_time:136718ms step_avg:88.15ms
step:1552/1680 train_time:136808ms step_avg:88.15ms
step:1553/1680 train_time:136896ms step_avg:88.15ms
step:1554/1680 train_time:136985ms step_avg:88.15ms
step:1555/1680 train_time:137073ms step_avg:88.15ms
step:1556/1680 train_time:137163ms step_avg:88.15ms
step:1557/1680 train_time:137252ms step_avg:88.15ms
step:1558/1680 train_time:137341ms step_avg:88.15ms
step:1559/1680 train_time:137430ms step_avg:88.15ms
step:1560/1680 train_time:137519ms step_avg:88.15ms
step:1561/1680 train_time:137609ms step_avg:88.15ms
step:1562/1680 train_time:137698ms step_avg:88.15ms
step:1563/1680 train_time:137788ms step_avg:88.16ms
step:1564/1680 train_time:137876ms step_avg:88.16ms
step:1565/1680 train_time:137965ms step_avg:88.16ms
step:1566/1680 train_time:138054ms step_avg:88.16ms
step:1567/1680 train_time:138143ms step_avg:88.16ms
step:1568/1680 train_time:138232ms step_avg:88.16ms
step:1569/1680 train_time:138321ms step_avg:88.16ms
step:1570/1680 train_time:138411ms step_avg:88.16ms
step:1571/1680 train_time:138501ms step_avg:88.16ms
step:1572/1680 train_time:138589ms step_avg:88.16ms
step:1573/1680 train_time:138678ms step_avg:88.16ms
step:1574/1680 train_time:138769ms step_avg:88.16ms
step:1575/1680 train_time:138857ms step_avg:88.16ms
step:1576/1680 train_time:138946ms step_avg:88.16ms
step:1577/1680 train_time:139035ms step_avg:88.16ms
step:1578/1680 train_time:139124ms step_avg:88.16ms
step:1579/1680 train_time:139213ms step_avg:88.16ms
step:1580/1680 train_time:139301ms step_avg:88.17ms
step:1581/1680 train_time:139391ms step_avg:88.17ms
step:1582/1680 train_time:139480ms step_avg:88.17ms
step:1583/1680 train_time:139569ms step_avg:88.17ms
step:1584/1680 train_time:139658ms step_avg:88.17ms
step:1585/1680 train_time:139748ms step_avg:88.17ms
step:1586/1680 train_time:139837ms step_avg:88.17ms
step:1587/1680 train_time:139926ms step_avg:88.17ms
step:1588/1680 train_time:140015ms step_avg:88.17ms
step:1589/1680 train_time:140105ms step_avg:88.17ms
step:1590/1680 train_time:140193ms step_avg:88.17ms
step:1591/1680 train_time:140282ms step_avg:88.17ms
step:1592/1680 train_time:140370ms step_avg:88.17ms
step:1593/1680 train_time:140459ms step_avg:88.17ms
step:1594/1680 train_time:140548ms step_avg:88.17ms
step:1595/1680 train_time:140637ms step_avg:88.17ms
step:1596/1680 train_time:140726ms step_avg:88.17ms
step:1597/1680 train_time:140816ms step_avg:88.18ms
step:1598/1680 train_time:140905ms step_avg:88.18ms
step:1599/1680 train_time:140994ms step_avg:88.18ms
step:1600/1680 train_time:141083ms step_avg:88.18ms
step:1601/1680 train_time:141172ms step_avg:88.18ms
step:1602/1680 train_time:141262ms step_avg:88.18ms
step:1603/1680 train_time:141351ms step_avg:88.18ms
step:1604/1680 train_time:141439ms step_avg:88.18ms
step:1605/1680 train_time:141528ms step_avg:88.18ms
step:1606/1680 train_time:141617ms step_avg:88.18ms
step:1607/1680 train_time:141706ms step_avg:88.18ms
step:1608/1680 train_time:141795ms step_avg:88.18ms
step:1609/1680 train_time:141884ms step_avg:88.18ms
step:1610/1680 train_time:141974ms step_avg:88.18ms
step:1611/1680 train_time:142062ms step_avg:88.18ms
step:1612/1680 train_time:142151ms step_avg:88.18ms
step:1613/1680 train_time:142240ms step_avg:88.18ms
step:1614/1680 train_time:142329ms step_avg:88.18ms
step:1615/1680 train_time:142418ms step_avg:88.18ms
step:1616/1680 train_time:142509ms step_avg:88.19ms
step:1617/1680 train_time:142598ms step_avg:88.19ms
step:1618/1680 train_time:142687ms step_avg:88.19ms
step:1619/1680 train_time:142776ms step_avg:88.19ms
step:1620/1680 train_time:142865ms step_avg:88.19ms
step:1621/1680 train_time:142955ms step_avg:88.19ms
step:1622/1680 train_time:143044ms step_avg:88.19ms
step:1623/1680 train_time:143133ms step_avg:88.19ms
step:1624/1680 train_time:143222ms step_avg:88.19ms
step:1625/1680 train_time:143311ms step_avg:88.19ms
step:1625/1680 val_loss:3.2929 train_time:143401ms step_avg:88.25ms
step:1626/1680 train_time:143420ms step_avg:88.20ms
step:1627/1680 train_time:143493ms step_avg:88.19ms
step:1628/1680 train_time:143584ms step_avg:88.20ms
step:1629/1680 train_time:143673ms step_avg:88.20ms
step:1630/1680 train_time:143762ms step_avg:88.20ms
step:1631/1680 train_time:143850ms step_avg:88.20ms
step:1632/1680 train_time:143938ms step_avg:88.20ms
step:1633/1680 train_time:144026ms step_avg:88.20ms
step:1634/1680 train_time:144114ms step_avg:88.20ms
step:1635/1680 train_time:144203ms step_avg:88.20ms
step:1636/1680 train_time:144293ms step_avg:88.20ms
step:1637/1680 train_time:144383ms step_avg:88.20ms
step:1638/1680 train_time:144474ms step_avg:88.20ms
step:1639/1680 train_time:144564ms step_avg:88.20ms
step:1640/1680 train_time:144654ms step_avg:88.20ms
step:1641/1680 train_time:144743ms step_avg:88.20ms
step:1642/1680 train_time:144832ms step_avg:88.20ms
step:1643/1680 train_time:144921ms step_avg:88.21ms
step:1644/1680 train_time:145009ms step_avg:88.20ms
step:1645/1680 train_time:145097ms step_avg:88.20ms
step:1646/1680 train_time:145186ms step_avg:88.21ms
step:1647/1680 train_time:145274ms step_avg:88.21ms
step:1648/1680 train_time:145366ms step_avg:88.21ms
step:1649/1680 train_time:145455ms step_avg:88.21ms
step:1650/1680 train_time:145544ms step_avg:88.21ms
step:1651/1680 train_time:145634ms step_avg:88.21ms
step:1652/1680 train_time:145723ms step_avg:88.21ms
step:1653/1680 train_time:145812ms step_avg:88.21ms
step:1654/1680 train_time:145901ms step_avg:88.21ms
step:1655/1680 train_time:145991ms step_avg:88.21ms
step:1656/1680 train_time:146079ms step_avg:88.21ms
step:1657/1680 train_time:146168ms step_avg:88.21ms
step:1658/1680 train_time:146257ms step_avg:88.21ms
step:1659/1680 train_time:146346ms step_avg:88.21ms
step:1660/1680 train_time:146436ms step_avg:88.21ms
step:1661/1680 train_time:146526ms step_avg:88.22ms
step:1662/1680 train_time:146615ms step_avg:88.22ms
step:1663/1680 train_time:146704ms step_avg:88.22ms
step:1664/1680 train_time:146793ms step_avg:88.22ms
step:1665/1680 train_time:146883ms step_avg:88.22ms
step:1666/1680 train_time:146973ms step_avg:88.22ms
step:1667/1680 train_time:147061ms step_avg:88.22ms
step:1668/1680 train_time:147150ms step_avg:88.22ms
step:1669/1680 train_time:147239ms step_avg:88.22ms
step:1670/1680 train_time:147328ms step_avg:88.22ms
step:1671/1680 train_time:147418ms step_avg:88.22ms
step:1672/1680 train_time:147507ms step_avg:88.22ms
step:1673/1680 train_time:147596ms step_avg:88.22ms
step:1674/1680 train_time:147686ms step_avg:88.22ms
step:1675/1680 train_time:147775ms step_avg:88.22ms
step:1676/1680 train_time:147864ms step_avg:88.22ms
step:1677/1680 train_time:147953ms step_avg:88.22ms
step:1678/1680 train_time:148043ms step_avg:88.23ms
step:1679/1680 train_time:148132ms step_avg:88.23ms
step:1680/1680 train_time:148220ms step_avg:88.23ms
step:1680/1680 val_loss:3.2823 train_time:148311ms step_avg:88.28ms
peak memory allocated: 30760 MiB reserved: 45914 MiB

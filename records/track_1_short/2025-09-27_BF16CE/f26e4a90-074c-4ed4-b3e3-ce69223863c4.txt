import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from flash_attn_interface import flash_attn_varlen_func
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_1_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_1(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_1_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ns_line_2_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from ns_line_1_kernel, but also loads and adds a block of A
    # Performance is slightly slower than ns_line_1_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ns_line_2(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ns_line_2_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def newton_schulz_triton(G: torch.Tensor):
    a, b, c = (3.4445, -4.7750, 2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    ns_line_3 = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the NS iterations
    for _ in range(5):
        ns_line_1(X, out=A)  # A = X @ X.mT
        ns_line_2(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        ns_line_3(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    Though empirically small 1D params perform efficiently here:
        NS approximately performs a magnitude normalization of the grad
        This hyper-optimized class has faster execution time than the current impl of Adam for small params

    Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%4==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute NS of 1 and schedule all gather
        6. wait on step 2, then compute NS of 2 and schedule all gather
        7. wait on step 3, then compute NS of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before NS
        8. wait on 4, then compute NS of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum)
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per size, while giving attn its own param group for resize op.
        """
        params = list(params)
        param_groups = []
        attn_subset = [p for p in params if p.module == 'attn']
        non_attn_subset = [p for p in params if p.module != 'attn']
        param_groups.append(dict(params=attn_subset))

        sizes = {p.shape for p in non_attn_subset}
        for size in sizes:
            group_params = [p for p in non_attn_subset if p.shape == size]
            param_groups.append(dict(params=group_params))
        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_ranks = {
            'smear_gate': 1, # 1 param
            'attn_gate': 2, # 10 params
            'attn': 3, # 10 params
            'mlp': 4, # 22 params
        }
        params = list(params)
        params.sort(key=lambda x: module_ranks.get(x.module))
        idx = 0
        group_sizes = [1,10,16,16]
        assert len(params)==sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            group_params = params[idx:idx+size]
            param_groups.append(dict(params=group_params))
            idx += size
        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, and @vagrawal.
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            num_params = len(params)
            padded_num_params = (
                (num_params + world_size - 1) // world_size * world_size
            )

            grads_to_stack = [p.grad for p in params]
            if padded_num_params > num_params:
                padding_grad = torch.zeros_like(params[0].grad)
                grads_to_stack.extend(
                    [padding_grad] * (padded_num_params - num_params)
                )

            stacked_grads = torch.stack(grads_to_stack)

            chunk_size = padded_num_params // world_size
            grad_chunk = torch.empty(
                (chunk_size, *params[0].grad.shape),
                dtype=stacked_grads.dtype,
                device=stacked_grads.device,
            )

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(
                {
                    "params": params,
                    "grad_chunk": grad_chunk,
                    "reduce_future": reduce_future,
                    "chunk_size": chunk_size,
                    "padded_num_params": padded_num_params,
                }
            )

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = info["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = info["chunk_size"]
            start_idx = rank * chunk_size

            # Determine effective LR and WD once per group, assuming constant for same-shaped params.
            # This helps in vectorizing operations later.
            p_example = params[0]  # All params in a group have the same shape.
            eff_lr_val = (
                group["lr"]
                * max(1, p_example.size(-2) / p_example.size(-1)) ** 0.5
                * getattr(p_example, "lr_mul", 1.0)
            )
            eff_weight_decay_val = (
                group["lr"]
                * group["weight_decay"]
                * getattr(p_example, "wd_mul", 1.0)
            )

            # Prepare a contiguous buffer for the updated parameters for this rank's chunk.
            # This buffer will serve as the input_tensor for dist.all_gather_into_tensor.
            updated_param_chunk = torch.empty(
                (chunk_size, *p_example.shape),
                dtype=p_example.dtype,
                device=p_example.device,
            )

            # List to collect update_grad tensors for batched zeropower computation.
            update_grads_for_zeropower = []

            # Process each parameter in this rank's chunk.
            for i in range(chunk_size):
                param_idx = start_idx + i

                if param_idx >= len(params):
                    # For padding: Fill the corresponding part of the updated_param_chunk with zeros.
                    # These padded entries will not be used by other ranks in the all_gather, but
                    # initializing them prevents uninitialized memory access issues.
                    updated_param_chunk[i].zero_()
                    # Also append a zero tensor for zeropower input if it must be padded.
                    update_grads_for_zeropower.append(
                        torch.zeros_like(p_example.grad)
                    )
                    continue
                p = params[param_idx]
                grad = grad_chunk[
                    i
                ]  # This gradient corresponds to the current parameter p.
                state = self.state[p]

                # Initialize momentum buffer if not present
                if not state:
                    state["momentum_buffer"] = torch.zeros_like(grad)

                momentum_buffer = state["momentum_buffer"]

                # Apply momentum update directly to the persistent momentum buffer in-place.
                momentum_buffer.lerp_(grad, 1 - group["momentum"])

                # Compute the actual `update_grad` for zeropower. This creates a new tensor.
                update_grad = grad.lerp(momentum_buffer, group["momentum"])
                update_grads_for_zeropower.append(update_grad)

                # Copy the current parameter value into the temporary buffer.
                updated_param_chunk[i].copy_(p)

                # Apply weight decay directly to the buffer.
                updated_param_chunk[i].mul_(1 - eff_weight_decay_val)

            # Stack the individual `update_grad` tensors for efficient batched zeropower computation.
            batched_update_grads = torch.stack(update_grads_for_zeropower)

            # Compute zeropower for the entire chunk in a single, batched call.
            original_shape = batched_update_grads.shape
            # Reshape attn params from [hdim, dim*4] to [4,hdim,dim] to apply NS indepedently to Q,K,V,O
            module_idx = start_idx if start_idx<len(params) else 0
            if getattr(params[module_idx],'module','none')=='attn':
                for p in params[module_idx:module_idx+chunk_size]:
                    assert getattr(params[module_idx],'module','none')=='attn'
                batch = 4 * original_shape[0]
                d1 = original_shape[1]
                d2 = original_shape[2] // 4
                batched = batched_update_grads.view(batch, d1, d2)
                v_chunk = newton_schulz_triton(batched)
                v_chunk = v_chunk.view(original_shape)
            else:
                v_chunk = newton_schulz_triton(batched_update_grads)

            # Add the computed zeropower update to the parameters in the buffer.
            # This loop applies the zeropower output (v_chunk) to the `updated_param_chunk` buffer.
            for i in range(chunk_size):
                param_idx = start_idx + i
                if param_idx >= len(params):  # Skip padded entries again.
                    continue

                # Add the computed zeropower update to the parameter in the buffer.
                updated_param_chunk[i].add_(v_chunk[i], alpha=-eff_lr_val)

            stacked_params = torch.empty(
                (info["padded_num_params"], *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device,
            )
            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_param_chunk, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # DistributedAdam implementation by @vagrawal

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        world_size = dist.get_world_size()
        reduce_scatter_futures: list[torch.Future] = []
        all_gather_futures: list[torch.Future] = []
        grad_slices = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for base_i in range(len(params)):
                grad = params[base_i].grad
                rank_size = grad.shape[0] // world_size
                grad_slice = torch.empty_like(grad[:rank_size])
                reduce_scatter_futures.append(dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future())
                grad_slices.append(grad_slice)

        idx = 0
        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            params = group['params']
            for base in range(len(params)):
                reduce_scatter_futures[idx].wait()
                p = params[base]
                rank_size = p.shape[0] // world_size
                p_slice = p[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(p, "lr_mul", 1.0)
                state = self.state[p]
                g_slice = grad_slices[idx]
                # State init
                if not state:
                    state["step"] = torch.tensor(
                        0, dtype=torch.int64, device=p.device
                    )
                    state["exp_avg"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                    state["exp_avg_sq"] = torch.zeros(
                        p_slice.shape,
                        dtype=torch.bfloat16,
                        device=p_slice.device,
                    )
                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(p, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (torch.sqrt(bias2) / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)
                idx += 1
                all_gather_futures.append(dist.all_gather_into_tensor(p, p_slice, async_op=True).get_future())
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.module='attn'
        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.module = 'attn_gate'
        self.attn_gate.weight.detach().zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4,self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens, max_seqlen_q=max_len, max_seqlen_k=max_len,
                                   causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4,self.hdim, self.dim)[3].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.module='mlp'
        self.c_proj.module='mlp'
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.detach().zero_()
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.module = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 6) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.0, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(num_layers), #extra zeros params for smear_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        # smear token embed forward 1 position @classiclarryd
        smear_lambda = self.scalars[5 * len(self.blocks)]
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # U-net design by @brendanh0gan
        skip_connections = []
        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)

        n = len(self.blocks) // 2

        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i >= n and i<11:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = torch.sigmoid(logits / logits.new_tensor(7.5)) * logits.new_tensor(30.0)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 24 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_iterations: int = 1640 # number of iterations to run
    iteration_extension = 40 # number of iterations to continue training at final cooldown and window size
    cooldown_frac: int = 0.5 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_validate: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_long_validate: int = 20 # extend long windows out even further

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.8, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = Muon(hidden_matrix_params + gate_params, lr=0.05, momentum=0.95, weight_decay=0.0)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = min(0.9999,step / args.num_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    if step == args.num_iterations+args.iteration_extension:
        return args.ws_validate//2, args.ws_validate
    x = min(step / (1 + args.num_iterations),0.9999)
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx]//2, args.ws_schedule[ws_idx]

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_long = args.ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    new_ws_long = args.ws_schedule[step % len(args.ws_schedule)]  # each window size is a new graph, need to warm up each with YaRN params
    if new_ws_long > ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    elif new_ws_long<ws_long:
        model.yarn.reset()
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset()
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations + args.iteration_extension
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_long_validate
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = torch.zeros((), device=device, dtype=torch.float32)
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for _ in range(grad_accum_steps):
        inputs, targets, cum_seqlens = next(train_loader)
        model(inputs, targets, cum_seqlens, ws_short, ws_long).backward()
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.4.0
Sat Sep 27 13:20:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    5856MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0            118W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   22C    P0            115W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   26C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   26C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            114W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   27C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   24C    P0            120W /  700W |    1518MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A    172549      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    172550      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    172551      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    172552      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    172553      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    172554      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    172555      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A    172556      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A    172550      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A    172551      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A    172552      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A    172553      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A    172554      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A    172555      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A    172556      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1680 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1680 train_time:141ms step_avg:140.53ms
step:2/1680 train_time:161ms step_avg:80.26ms
step:3/1680 train_time:226ms step_avg:75.22ms
step:4/1680 train_time:311ms step_avg:77.70ms
step:5/1680 train_time:397ms step_avg:79.31ms
step:6/1680 train_time:483ms step_avg:80.44ms
step:7/1680 train_time:569ms step_avg:81.28ms
step:8/1680 train_time:655ms step_avg:81.86ms
step:9/1680 train_time:741ms step_avg:82.36ms
step:10/1680 train_time:828ms step_avg:82.79ms
step:11/1680 train_time:914ms step_avg:83.14ms
step:12/1680 train_time:1003ms step_avg:83.55ms
step:13/1680 train_time:1094ms step_avg:84.19ms
step:14/1680 train_time:1184ms step_avg:84.60ms
step:15/1680 train_time:1272ms step_avg:84.80ms
step:16/1680 train_time:1359ms step_avg:84.93ms
step:17/1680 train_time:1446ms step_avg:85.06ms
step:18/1680 train_time:1533ms step_avg:85.15ms
step:19/1680 train_time:1619ms step_avg:85.22ms
step:20/1680 train_time:1706ms step_avg:85.28ms
step:21/1680 train_time:1792ms step_avg:85.34ms
step:22/1680 train_time:1879ms step_avg:85.39ms
step:23/1680 train_time:1966ms step_avg:85.49ms
step:24/1680 train_time:2054ms step_avg:85.60ms
step:25/1680 train_time:2143ms step_avg:85.71ms
step:26/1680 train_time:2231ms step_avg:85.83ms
step:27/1680 train_time:2319ms step_avg:85.90ms
step:28/1680 train_time:2407ms step_avg:85.96ms
step:29/1680 train_time:2494ms step_avg:86.01ms
step:30/1680 train_time:2582ms step_avg:86.06ms
step:31/1680 train_time:2668ms step_avg:86.08ms
step:32/1680 train_time:2755ms step_avg:86.09ms
step:33/1680 train_time:2841ms step_avg:86.10ms
step:34/1680 train_time:2928ms step_avg:86.13ms
step:35/1680 train_time:3016ms step_avg:86.16ms
step:36/1680 train_time:3104ms step_avg:86.21ms
step:37/1680 train_time:3193ms step_avg:86.30ms
step:38/1680 train_time:3281ms step_avg:86.35ms
step:39/1680 train_time:3370ms step_avg:86.40ms
step:40/1680 train_time:3457ms step_avg:86.41ms
step:41/1680 train_time:3544ms step_avg:86.43ms
step:42/1680 train_time:3631ms step_avg:86.45ms
step:43/1680 train_time:3718ms step_avg:86.46ms
step:44/1680 train_time:3805ms step_avg:86.47ms
step:45/1680 train_time:3893ms step_avg:86.50ms
step:46/1680 train_time:3980ms step_avg:86.51ms
step:47/1680 train_time:4067ms step_avg:86.54ms
step:48/1680 train_time:4155ms step_avg:86.56ms
step:49/1680 train_time:4242ms step_avg:86.58ms
step:50/1680 train_time:4330ms step_avg:86.60ms
step:51/1680 train_time:4417ms step_avg:86.60ms
step:52/1680 train_time:4504ms step_avg:86.61ms
step:53/1680 train_time:4592ms step_avg:86.64ms
step:54/1680 train_time:4679ms step_avg:86.64ms
step:55/1680 train_time:4766ms step_avg:86.66ms
step:56/1680 train_time:4853ms step_avg:86.66ms
step:57/1680 train_time:4940ms step_avg:86.67ms
step:58/1680 train_time:5028ms step_avg:86.68ms
step:59/1680 train_time:5115ms step_avg:86.69ms
step:60/1680 train_time:5202ms step_avg:86.71ms
step:61/1680 train_time:5291ms step_avg:86.73ms
step:62/1680 train_time:5378ms step_avg:86.74ms
step:63/1680 train_time:5465ms step_avg:86.75ms
step:64/1680 train_time:5552ms step_avg:86.76ms
step:65/1680 train_time:5640ms step_avg:86.77ms
step:66/1680 train_time:5728ms step_avg:86.78ms
step:67/1680 train_time:5814ms step_avg:86.78ms
step:68/1680 train_time:5902ms step_avg:86.80ms
step:69/1680 train_time:5991ms step_avg:86.82ms
step:70/1680 train_time:6077ms step_avg:86.82ms
step:71/1680 train_time:6165ms step_avg:86.82ms
step:72/1680 train_time:6251ms step_avg:86.82ms
step:73/1680 train_time:6339ms step_avg:86.83ms
step:74/1680 train_time:6426ms step_avg:86.84ms
step:75/1680 train_time:6513ms step_avg:86.85ms
step:76/1680 train_time:6602ms step_avg:86.86ms
step:77/1680 train_time:6689ms step_avg:86.87ms
step:78/1680 train_time:6776ms step_avg:86.87ms
step:79/1680 train_time:6863ms step_avg:86.87ms
step:80/1680 train_time:6950ms step_avg:86.88ms
step:81/1680 train_time:7037ms step_avg:86.87ms
step:82/1680 train_time:7125ms step_avg:86.89ms
step:83/1680 train_time:7213ms step_avg:86.90ms
step:84/1680 train_time:7300ms step_avg:86.90ms
step:85/1680 train_time:7387ms step_avg:86.90ms
step:86/1680 train_time:7474ms step_avg:86.91ms
step:87/1680 train_time:7561ms step_avg:86.91ms
step:88/1680 train_time:7648ms step_avg:86.91ms
step:89/1680 train_time:7735ms step_avg:86.92ms
step:90/1680 train_time:7823ms step_avg:86.92ms
step:91/1680 train_time:7911ms step_avg:86.93ms
step:92/1680 train_time:7997ms step_avg:86.93ms
step:93/1680 train_time:8085ms step_avg:86.93ms
step:94/1680 train_time:8172ms step_avg:86.94ms
step:95/1680 train_time:8259ms step_avg:86.94ms
step:96/1680 train_time:8345ms step_avg:86.93ms
step:97/1680 train_time:8433ms step_avg:86.94ms
step:98/1680 train_time:8520ms step_avg:86.93ms
step:99/1680 train_time:8607ms step_avg:86.94ms
step:100/1680 train_time:8694ms step_avg:86.94ms
step:101/1680 train_time:8780ms step_avg:86.93ms
step:102/1680 train_time:8868ms step_avg:86.94ms
step:103/1680 train_time:8955ms step_avg:86.94ms
step:104/1680 train_time:9042ms step_avg:86.94ms
step:105/1680 train_time:9130ms step_avg:86.95ms
step:106/1680 train_time:9216ms step_avg:86.95ms
step:107/1680 train_time:9304ms step_avg:86.95ms
step:108/1680 train_time:9392ms step_avg:86.96ms
step:109/1680 train_time:9478ms step_avg:86.96ms
step:110/1680 train_time:9566ms step_avg:86.96ms
step:111/1680 train_time:9653ms step_avg:86.96ms
step:112/1680 train_time:9740ms step_avg:86.96ms
step:113/1680 train_time:9828ms step_avg:86.97ms
step:114/1680 train_time:9915ms step_avg:86.97ms
step:115/1680 train_time:10002ms step_avg:86.98ms
step:116/1680 train_time:10089ms step_avg:86.98ms
step:117/1680 train_time:10176ms step_avg:86.98ms
step:118/1680 train_time:10264ms step_avg:86.98ms
step:119/1680 train_time:10351ms step_avg:86.98ms
step:120/1680 train_time:10438ms step_avg:86.98ms
step:121/1680 train_time:10525ms step_avg:86.99ms
step:122/1680 train_time:10612ms step_avg:86.99ms
step:123/1680 train_time:10701ms step_avg:87.00ms
step:124/1680 train_time:10788ms step_avg:87.00ms
step:125/1680 train_time:10875ms step_avg:87.00ms
step:125/1680 val_loss:4.3142 train_time:10964ms step_avg:87.71ms
step:126/1680 train_time:10983ms step_avg:87.17ms
step:127/1680 train_time:11055ms step_avg:87.05ms
step:128/1680 train_time:11152ms step_avg:87.12ms
step:129/1680 train_time:11243ms step_avg:87.16ms
step:130/1680 train_time:11330ms step_avg:87.16ms
step:131/1680 train_time:11416ms step_avg:87.15ms
step:132/1680 train_time:11503ms step_avg:87.14ms
step:133/1680 train_time:11588ms step_avg:87.13ms
step:134/1680 train_time:11674ms step_avg:87.12ms
step:135/1680 train_time:11760ms step_avg:87.11ms
step:136/1680 train_time:11846ms step_avg:87.10ms
step:137/1680 train_time:11932ms step_avg:87.09ms
step:138/1680 train_time:12019ms step_avg:87.10ms
step:139/1680 train_time:12110ms step_avg:87.12ms
step:140/1680 train_time:12198ms step_avg:87.13ms
step:141/1680 train_time:12287ms step_avg:87.14ms
step:142/1680 train_time:12374ms step_avg:87.14ms
step:143/1680 train_time:12461ms step_avg:87.14ms
step:144/1680 train_time:12547ms step_avg:87.13ms
step:145/1680 train_time:12633ms step_avg:87.12ms
step:146/1680 train_time:12719ms step_avg:87.12ms
step:147/1680 train_time:12805ms step_avg:87.11ms
step:148/1680 train_time:12892ms step_avg:87.11ms
step:149/1680 train_time:12979ms step_avg:87.10ms
step:150/1680 train_time:13067ms step_avg:87.11ms
step:151/1680 train_time:13156ms step_avg:87.13ms
step:152/1680 train_time:13245ms step_avg:87.14ms
step:153/1680 train_time:13332ms step_avg:87.14ms
step:154/1680 train_time:13419ms step_avg:87.13ms
step:155/1680 train_time:13505ms step_avg:87.13ms
step:156/1680 train_time:13592ms step_avg:87.13ms
step:157/1680 train_time:13678ms step_avg:87.12ms
step:158/1680 train_time:13765ms step_avg:87.12ms
step:159/1680 train_time:13851ms step_avg:87.12ms
step:160/1680 train_time:13938ms step_avg:87.11ms
step:161/1680 train_time:14025ms step_avg:87.11ms
step:162/1680 train_time:14112ms step_avg:87.11ms
step:163/1680 train_time:14200ms step_avg:87.12ms
step:164/1680 train_time:14289ms step_avg:87.13ms
step:165/1680 train_time:14378ms step_avg:87.14ms
step:166/1680 train_time:14465ms step_avg:87.14ms
step:167/1680 train_time:14551ms step_avg:87.13ms
step:168/1680 train_time:14638ms step_avg:87.13ms
step:169/1680 train_time:14725ms step_avg:87.13ms
step:170/1680 train_time:14811ms step_avg:87.12ms
step:171/1680 train_time:14897ms step_avg:87.12ms
step:172/1680 train_time:14985ms step_avg:87.12ms
step:173/1680 train_time:15072ms step_avg:87.12ms
step:174/1680 train_time:15160ms step_avg:87.12ms
step:175/1680 train_time:15248ms step_avg:87.13ms
step:176/1680 train_time:15336ms step_avg:87.14ms
step:177/1680 train_time:15423ms step_avg:87.14ms
step:178/1680 train_time:15510ms step_avg:87.14ms
step:179/1680 train_time:15597ms step_avg:87.14ms
step:180/1680 train_time:15684ms step_avg:87.13ms
step:181/1680 train_time:15771ms step_avg:87.13ms
step:182/1680 train_time:15858ms step_avg:87.13ms
step:183/1680 train_time:15945ms step_avg:87.13ms
step:184/1680 train_time:16031ms step_avg:87.13ms
step:185/1680 train_time:16118ms step_avg:87.13ms
step:186/1680 train_time:16206ms step_avg:87.13ms
step:187/1680 train_time:16293ms step_avg:87.13ms
step:188/1680 train_time:16380ms step_avg:87.13ms
step:189/1680 train_time:16468ms step_avg:87.13ms
step:190/1680 train_time:16554ms step_avg:87.13ms
step:191/1680 train_time:16642ms step_avg:87.13ms
step:192/1680 train_time:16729ms step_avg:87.13ms
step:193/1680 train_time:16815ms step_avg:87.13ms
step:194/1680 train_time:16903ms step_avg:87.13ms
step:195/1680 train_time:16990ms step_avg:87.13ms
step:196/1680 train_time:17076ms step_avg:87.12ms
step:197/1680 train_time:17164ms step_avg:87.13ms
step:198/1680 train_time:17252ms step_avg:87.13ms
step:199/1680 train_time:17338ms step_avg:87.13ms
step:200/1680 train_time:17426ms step_avg:87.13ms
step:201/1680 train_time:17512ms step_avg:87.13ms
step:202/1680 train_time:17600ms step_avg:87.13ms
step:203/1680 train_time:17687ms step_avg:87.13ms
step:204/1680 train_time:17774ms step_avg:87.13ms
step:205/1680 train_time:17861ms step_avg:87.13ms
step:206/1680 train_time:17949ms step_avg:87.13ms
step:207/1680 train_time:18036ms step_avg:87.13ms
step:208/1680 train_time:18123ms step_avg:87.13ms
step:209/1680 train_time:18211ms step_avg:87.13ms
step:210/1680 train_time:18299ms step_avg:87.14ms
step:211/1680 train_time:18385ms step_avg:87.13ms
step:212/1680 train_time:18473ms step_avg:87.14ms
step:213/1680 train_time:18561ms step_avg:87.14ms
step:214/1680 train_time:18647ms step_avg:87.14ms
step:215/1680 train_time:18734ms step_avg:87.13ms
step:216/1680 train_time:18821ms step_avg:87.13ms
step:217/1680 train_time:18908ms step_avg:87.13ms
step:218/1680 train_time:18995ms step_avg:87.13ms
step:219/1680 train_time:19082ms step_avg:87.13ms
step:220/1680 train_time:19169ms step_avg:87.13ms
step:221/1680 train_time:19256ms step_avg:87.13ms
step:222/1680 train_time:19343ms step_avg:87.13ms
step:223/1680 train_time:19430ms step_avg:87.13ms
step:224/1680 train_time:19517ms step_avg:87.13ms
step:225/1680 train_time:19604ms step_avg:87.13ms
step:226/1680 train_time:19691ms step_avg:87.13ms
step:227/1680 train_time:19778ms step_avg:87.13ms
step:228/1680 train_time:19865ms step_avg:87.13ms
step:229/1680 train_time:19952ms step_avg:87.13ms
step:230/1680 train_time:20038ms step_avg:87.12ms
step:231/1680 train_time:20125ms step_avg:87.12ms
step:232/1680 train_time:20212ms step_avg:87.12ms
step:233/1680 train_time:20299ms step_avg:87.12ms
step:234/1680 train_time:20387ms step_avg:87.12ms
step:235/1680 train_time:20474ms step_avg:87.12ms
step:236/1680 train_time:20561ms step_avg:87.12ms
step:237/1680 train_time:20648ms step_avg:87.12ms
step:238/1680 train_time:20735ms step_avg:87.12ms
step:239/1680 train_time:20823ms step_avg:87.12ms
step:240/1680 train_time:20910ms step_avg:87.12ms
step:241/1680 train_time:20997ms step_avg:87.12ms
step:242/1680 train_time:21084ms step_avg:87.12ms
step:243/1680 train_time:21171ms step_avg:87.12ms
step:244/1680 train_time:21258ms step_avg:87.12ms
step:245/1680 train_time:21346ms step_avg:87.12ms
step:246/1680 train_time:21432ms step_avg:87.12ms
step:247/1680 train_time:21519ms step_avg:87.12ms
step:248/1680 train_time:21606ms step_avg:87.12ms
step:249/1680 train_time:21694ms step_avg:87.12ms
step:250/1680 train_time:21780ms step_avg:87.12ms
step:250/1680 val_loss:3.9678 train_time:21869ms step_avg:87.48ms
step:251/1680 train_time:21888ms step_avg:87.20ms
step:252/1680 train_time:21958ms step_avg:87.13ms
step:253/1680 train_time:22048ms step_avg:87.15ms
step:254/1680 train_time:22136ms step_avg:87.15ms
step:255/1680 train_time:22224ms step_avg:87.15ms
step:256/1680 train_time:22311ms step_avg:87.15ms
step:257/1680 train_time:22397ms step_avg:87.15ms
step:258/1680 train_time:22483ms step_avg:87.14ms
step:259/1680 train_time:22569ms step_avg:87.14ms
step:260/1680 train_time:22655ms step_avg:87.14ms
step:261/1680 train_time:22741ms step_avg:87.13ms
step:262/1680 train_time:22828ms step_avg:87.13ms
step:263/1680 train_time:22917ms step_avg:87.14ms
step:264/1680 train_time:23007ms step_avg:87.15ms
step:265/1680 train_time:23095ms step_avg:87.15ms
step:266/1680 train_time:23183ms step_avg:87.15ms
step:267/1680 train_time:23270ms step_avg:87.15ms
step:268/1680 train_time:23357ms step_avg:87.15ms
step:269/1680 train_time:23443ms step_avg:87.15ms
step:270/1680 train_time:23530ms step_avg:87.15ms
step:271/1680 train_time:23617ms step_avg:87.15ms
step:272/1680 train_time:23703ms step_avg:87.14ms
step:273/1680 train_time:23790ms step_avg:87.14ms
step:274/1680 train_time:23877ms step_avg:87.14ms
step:275/1680 train_time:23965ms step_avg:87.14ms
step:276/1680 train_time:24053ms step_avg:87.15ms
step:277/1680 train_time:24141ms step_avg:87.15ms
step:278/1680 train_time:24228ms step_avg:87.15ms
step:279/1680 train_time:24315ms step_avg:87.15ms
step:280/1680 train_time:24402ms step_avg:87.15ms
step:281/1680 train_time:24489ms step_avg:87.15ms
step:282/1680 train_time:24575ms step_avg:87.14ms
step:283/1680 train_time:24661ms step_avg:87.14ms
step:284/1680 train_time:24748ms step_avg:87.14ms
step:285/1680 train_time:24835ms step_avg:87.14ms
step:286/1680 train_time:24922ms step_avg:87.14ms
step:287/1680 train_time:25010ms step_avg:87.14ms
step:288/1680 train_time:25097ms step_avg:87.14ms
step:289/1680 train_time:25184ms step_avg:87.14ms
step:290/1680 train_time:25272ms step_avg:87.14ms
step:291/1680 train_time:25359ms step_avg:87.14ms
step:292/1680 train_time:25446ms step_avg:87.14ms
step:293/1680 train_time:25533ms step_avg:87.15ms
step:294/1680 train_time:25621ms step_avg:87.14ms
step:295/1680 train_time:25707ms step_avg:87.14ms
step:296/1680 train_time:25795ms step_avg:87.14ms
step:297/1680 train_time:25881ms step_avg:87.14ms
step:298/1680 train_time:25968ms step_avg:87.14ms
step:299/1680 train_time:26056ms step_avg:87.15ms
step:300/1680 train_time:26144ms step_avg:87.15ms
step:301/1680 train_time:26232ms step_avg:87.15ms
step:302/1680 train_time:26319ms step_avg:87.15ms
step:303/1680 train_time:26406ms step_avg:87.15ms
step:304/1680 train_time:26493ms step_avg:87.15ms
step:305/1680 train_time:26579ms step_avg:87.14ms
step:306/1680 train_time:26666ms step_avg:87.14ms
step:307/1680 train_time:26753ms step_avg:87.14ms
step:308/1680 train_time:26840ms step_avg:87.14ms
step:309/1680 train_time:26927ms step_avg:87.14ms
step:310/1680 train_time:27015ms step_avg:87.14ms
step:311/1680 train_time:27101ms step_avg:87.14ms
step:312/1680 train_time:27188ms step_avg:87.14ms
step:313/1680 train_time:27277ms step_avg:87.15ms
step:314/1680 train_time:27364ms step_avg:87.15ms
step:315/1680 train_time:27451ms step_avg:87.15ms
step:316/1680 train_time:27538ms step_avg:87.15ms
step:317/1680 train_time:27625ms step_avg:87.15ms
step:318/1680 train_time:27712ms step_avg:87.14ms
step:319/1680 train_time:27799ms step_avg:87.14ms
step:320/1680 train_time:27886ms step_avg:87.14ms
step:321/1680 train_time:27974ms step_avg:87.15ms
step:322/1680 train_time:28060ms step_avg:87.14ms
step:323/1680 train_time:28147ms step_avg:87.14ms
step:324/1680 train_time:28234ms step_avg:87.14ms
step:325/1680 train_time:28322ms step_avg:87.14ms
step:326/1680 train_time:28409ms step_avg:87.14ms
step:327/1680 train_time:28496ms step_avg:87.14ms
step:328/1680 train_time:28582ms step_avg:87.14ms
step:329/1680 train_time:28670ms step_avg:87.14ms
step:330/1680 train_time:28757ms step_avg:87.14ms
step:331/1680 train_time:28844ms step_avg:87.14ms
step:332/1680 train_time:28930ms step_avg:87.14ms
step:333/1680 train_time:29017ms step_avg:87.14ms
step:334/1680 train_time:29104ms step_avg:87.14ms
step:335/1680 train_time:29191ms step_avg:87.14ms
step:336/1680 train_time:29278ms step_avg:87.14ms
step:337/1680 train_time:29365ms step_avg:87.14ms
step:338/1680 train_time:29452ms step_avg:87.14ms
step:339/1680 train_time:29539ms step_avg:87.14ms
step:340/1680 train_time:29626ms step_avg:87.14ms
step:341/1680 train_time:29714ms step_avg:87.14ms
step:342/1680 train_time:29800ms step_avg:87.14ms
step:343/1680 train_time:29888ms step_avg:87.14ms
step:344/1680 train_time:29975ms step_avg:87.14ms
step:345/1680 train_time:30062ms step_avg:87.14ms
step:346/1680 train_time:30149ms step_avg:87.14ms
step:347/1680 train_time:30237ms step_avg:87.14ms
step:348/1680 train_time:30324ms step_avg:87.14ms
step:349/1680 train_time:30412ms step_avg:87.14ms
step:350/1680 train_time:30499ms step_avg:87.14ms
step:351/1680 train_time:30585ms step_avg:87.14ms
step:352/1680 train_time:30672ms step_avg:87.14ms
step:353/1680 train_time:30759ms step_avg:87.14ms
step:354/1680 train_time:30846ms step_avg:87.13ms
step:355/1680 train_time:30932ms step_avg:87.13ms
step:356/1680 train_time:31019ms step_avg:87.13ms
step:357/1680 train_time:31106ms step_avg:87.13ms
step:358/1680 train_time:31193ms step_avg:87.13ms
step:359/1680 train_time:31280ms step_avg:87.13ms
step:360/1680 train_time:31367ms step_avg:87.13ms
step:361/1680 train_time:31455ms step_avg:87.13ms
step:362/1680 train_time:31542ms step_avg:87.13ms
step:363/1680 train_time:31630ms step_avg:87.13ms
step:364/1680 train_time:31718ms step_avg:87.14ms
step:365/1680 train_time:31805ms step_avg:87.14ms
step:366/1680 train_time:31891ms step_avg:87.13ms
step:367/1680 train_time:31978ms step_avg:87.13ms
step:368/1680 train_time:32065ms step_avg:87.13ms
step:369/1680 train_time:32152ms step_avg:87.13ms
step:370/1680 train_time:32240ms step_avg:87.14ms
step:371/1680 train_time:32327ms step_avg:87.13ms
step:372/1680 train_time:32414ms step_avg:87.13ms
step:373/1680 train_time:32500ms step_avg:87.13ms
step:374/1680 train_time:32587ms step_avg:87.13ms
step:375/1680 train_time:32675ms step_avg:87.13ms
step:375/1680 val_loss:3.8154 train_time:32764ms step_avg:87.37ms
step:376/1680 train_time:32783ms step_avg:87.19ms
step:377/1680 train_time:32852ms step_avg:87.14ms
step:378/1680 train_time:32944ms step_avg:87.15ms
step:379/1680 train_time:33033ms step_avg:87.16ms
step:380/1680 train_time:33120ms step_avg:87.16ms
step:381/1680 train_time:33206ms step_avg:87.15ms
step:382/1680 train_time:33292ms step_avg:87.15ms
step:383/1680 train_time:33378ms step_avg:87.15ms
step:384/1680 train_time:33464ms step_avg:87.15ms
step:385/1680 train_time:33551ms step_avg:87.14ms
step:386/1680 train_time:33636ms step_avg:87.14ms
step:387/1680 train_time:33724ms step_avg:87.14ms
step:388/1680 train_time:33812ms step_avg:87.14ms
step:389/1680 train_time:33901ms step_avg:87.15ms
step:390/1680 train_time:33989ms step_avg:87.15ms
step:391/1680 train_time:34077ms step_avg:87.15ms
step:392/1680 train_time:34164ms step_avg:87.15ms
step:393/1680 train_time:34251ms step_avg:87.15ms
step:394/1680 train_time:34337ms step_avg:87.15ms
step:395/1680 train_time:34423ms step_avg:87.15ms
step:396/1680 train_time:34509ms step_avg:87.14ms
step:397/1680 train_time:34595ms step_avg:87.14ms
step:398/1680 train_time:34682ms step_avg:87.14ms
step:399/1680 train_time:34769ms step_avg:87.14ms
step:400/1680 train_time:34857ms step_avg:87.14ms
step:401/1680 train_time:34945ms step_avg:87.15ms
step:402/1680 train_time:35033ms step_avg:87.15ms
step:403/1680 train_time:35121ms step_avg:87.15ms
step:404/1680 train_time:35208ms step_avg:87.15ms
step:405/1680 train_time:35294ms step_avg:87.15ms
step:406/1680 train_time:35381ms step_avg:87.15ms
step:407/1680 train_time:35467ms step_avg:87.14ms
step:408/1680 train_time:35554ms step_avg:87.14ms
step:409/1680 train_time:35640ms step_avg:87.14ms
step:410/1680 train_time:35727ms step_avg:87.14ms
step:411/1680 train_time:35815ms step_avg:87.14ms
step:412/1680 train_time:35903ms step_avg:87.14ms
step:413/1680 train_time:35990ms step_avg:87.14ms
step:414/1680 train_time:36079ms step_avg:87.15ms
step:415/1680 train_time:36166ms step_avg:87.15ms
step:416/1680 train_time:36253ms step_avg:87.15ms
step:417/1680 train_time:36340ms step_avg:87.15ms
step:418/1680 train_time:36427ms step_avg:87.15ms
step:419/1680 train_time:36514ms step_avg:87.14ms
step:420/1680 train_time:36600ms step_avg:87.14ms
step:421/1680 train_time:36687ms step_avg:87.14ms
step:422/1680 train_time:36774ms step_avg:87.14ms
step:423/1680 train_time:36861ms step_avg:87.14ms
step:424/1680 train_time:36949ms step_avg:87.14ms
step:425/1680 train_time:37036ms step_avg:87.14ms
step:426/1680 train_time:37123ms step_avg:87.14ms
step:427/1680 train_time:37211ms step_avg:87.15ms
step:428/1680 train_time:37298ms step_avg:87.15ms
step:429/1680 train_time:37385ms step_avg:87.14ms
step:430/1680 train_time:37472ms step_avg:87.14ms
step:431/1680 train_time:37559ms step_avg:87.14ms
step:432/1680 train_time:37645ms step_avg:87.14ms
step:433/1680 train_time:37732ms step_avg:87.14ms
step:434/1680 train_time:37819ms step_avg:87.14ms
step:435/1680 train_time:37906ms step_avg:87.14ms
step:436/1680 train_time:37994ms step_avg:87.14ms
step:437/1680 train_time:38081ms step_avg:87.14ms
step:438/1680 train_time:38168ms step_avg:87.14ms
step:439/1680 train_time:38255ms step_avg:87.14ms
step:440/1680 train_time:38342ms step_avg:87.14ms
step:441/1680 train_time:38429ms step_avg:87.14ms
step:442/1680 train_time:38517ms step_avg:87.14ms
step:443/1680 train_time:38603ms step_avg:87.14ms
step:444/1680 train_time:38690ms step_avg:87.14ms
step:445/1680 train_time:38778ms step_avg:87.14ms
step:446/1680 train_time:38865ms step_avg:87.14ms
step:447/1680 train_time:38953ms step_avg:87.14ms
step:448/1680 train_time:39040ms step_avg:87.14ms
step:449/1680 train_time:39127ms step_avg:87.14ms
step:450/1680 train_time:39215ms step_avg:87.14ms
step:451/1680 train_time:39302ms step_avg:87.14ms
step:452/1680 train_time:39389ms step_avg:87.14ms
step:453/1680 train_time:39477ms step_avg:87.15ms
step:454/1680 train_time:39564ms step_avg:87.14ms
step:455/1680 train_time:39651ms step_avg:87.14ms
step:456/1680 train_time:39737ms step_avg:87.14ms
step:457/1680 train_time:39824ms step_avg:87.14ms
step:458/1680 train_time:39911ms step_avg:87.14ms
step:459/1680 train_time:39998ms step_avg:87.14ms
step:460/1680 train_time:40085ms step_avg:87.14ms
step:461/1680 train_time:40172ms step_avg:87.14ms
step:462/1680 train_time:40259ms step_avg:87.14ms
step:463/1680 train_time:40346ms step_avg:87.14ms
step:464/1680 train_time:40433ms step_avg:87.14ms
step:465/1680 train_time:40520ms step_avg:87.14ms
step:466/1680 train_time:40607ms step_avg:87.14ms
step:467/1680 train_time:40694ms step_avg:87.14ms
step:468/1680 train_time:40782ms step_avg:87.14ms
step:469/1680 train_time:40868ms step_avg:87.14ms
step:470/1680 train_time:40955ms step_avg:87.14ms
step:471/1680 train_time:41042ms step_avg:87.14ms
step:472/1680 train_time:41129ms step_avg:87.14ms
step:473/1680 train_time:41217ms step_avg:87.14ms
step:474/1680 train_time:41304ms step_avg:87.14ms
step:475/1680 train_time:41391ms step_avg:87.14ms
step:476/1680 train_time:41478ms step_avg:87.14ms
step:477/1680 train_time:41565ms step_avg:87.14ms
step:478/1680 train_time:41652ms step_avg:87.14ms
step:479/1680 train_time:41739ms step_avg:87.14ms
step:480/1680 train_time:41826ms step_avg:87.14ms
step:481/1680 train_time:41913ms step_avg:87.14ms
step:482/1680 train_time:42000ms step_avg:87.14ms
step:483/1680 train_time:42087ms step_avg:87.14ms
step:484/1680 train_time:42174ms step_avg:87.14ms
step:485/1680 train_time:42261ms step_avg:87.14ms
step:486/1680 train_time:42348ms step_avg:87.14ms
step:487/1680 train_time:42435ms step_avg:87.13ms
step:488/1680 train_time:42522ms step_avg:87.13ms
step:489/1680 train_time:42608ms step_avg:87.13ms
step:490/1680 train_time:42696ms step_avg:87.13ms
step:491/1680 train_time:42783ms step_avg:87.14ms
step:492/1680 train_time:42870ms step_avg:87.13ms
step:493/1680 train_time:42957ms step_avg:87.13ms
step:494/1680 train_time:43043ms step_avg:87.13ms
step:495/1680 train_time:43130ms step_avg:87.13ms
step:496/1680 train_time:43218ms step_avg:87.13ms
step:497/1680 train_time:43305ms step_avg:87.13ms
step:498/1680 train_time:43392ms step_avg:87.13ms
step:499/1680 train_time:43479ms step_avg:87.13ms
step:500/1680 train_time:43566ms step_avg:87.13ms
step:500/1680 val_loss:3.7161 train_time:43655ms step_avg:87.31ms
step:501/1680 train_time:43674ms step_avg:87.17ms
step:502/1680 train_time:43746ms step_avg:87.14ms
step:503/1680 train_time:43836ms step_avg:87.15ms
step:504/1680 train_time:43924ms step_avg:87.15ms
step:505/1680 train_time:44010ms step_avg:87.15ms
step:506/1680 train_time:44097ms step_avg:87.15ms
step:507/1680 train_time:44183ms step_avg:87.15ms
step:508/1680 train_time:44269ms step_avg:87.14ms
step:509/1680 train_time:44355ms step_avg:87.14ms
step:510/1680 train_time:44441ms step_avg:87.14ms
step:511/1680 train_time:44528ms step_avg:87.14ms
step:512/1680 train_time:44614ms step_avg:87.14ms
step:513/1680 train_time:44703ms step_avg:87.14ms
step:514/1680 train_time:44791ms step_avg:87.14ms
step:515/1680 train_time:44881ms step_avg:87.15ms
step:516/1680 train_time:44968ms step_avg:87.15ms
step:517/1680 train_time:45055ms step_avg:87.15ms
step:518/1680 train_time:45142ms step_avg:87.15ms
step:519/1680 train_time:45228ms step_avg:87.14ms
step:520/1680 train_time:45314ms step_avg:87.14ms
step:521/1680 train_time:45401ms step_avg:87.14ms
step:522/1680 train_time:45488ms step_avg:87.14ms
step:523/1680 train_time:45574ms step_avg:87.14ms
step:524/1680 train_time:45661ms step_avg:87.14ms
step:525/1680 train_time:45749ms step_avg:87.14ms
step:526/1680 train_time:45837ms step_avg:87.14ms
step:527/1680 train_time:45924ms step_avg:87.14ms
step:528/1680 train_time:46012ms step_avg:87.14ms
step:529/1680 train_time:46099ms step_avg:87.14ms
step:530/1680 train_time:46186ms step_avg:87.14ms
step:531/1680 train_time:46273ms step_avg:87.14ms
step:532/1680 train_time:46360ms step_avg:87.14ms
step:533/1680 train_time:46446ms step_avg:87.14ms
step:534/1680 train_time:46532ms step_avg:87.14ms
step:535/1680 train_time:46619ms step_avg:87.14ms
step:536/1680 train_time:46706ms step_avg:87.14ms
step:537/1680 train_time:46793ms step_avg:87.14ms
step:538/1680 train_time:46882ms step_avg:87.14ms
step:539/1680 train_time:46969ms step_avg:87.14ms
step:540/1680 train_time:47057ms step_avg:87.14ms
step:541/1680 train_time:47145ms step_avg:87.14ms
step:542/1680 train_time:47232ms step_avg:87.14ms
step:543/1680 train_time:47319ms step_avg:87.14ms
step:544/1680 train_time:47405ms step_avg:87.14ms
step:545/1680 train_time:47492ms step_avg:87.14ms
step:546/1680 train_time:47579ms step_avg:87.14ms
step:547/1680 train_time:47667ms step_avg:87.14ms
step:548/1680 train_time:47754ms step_avg:87.14ms
step:549/1680 train_time:47842ms step_avg:87.14ms
step:550/1680 train_time:47930ms step_avg:87.15ms
step:551/1680 train_time:48019ms step_avg:87.15ms
step:552/1680 train_time:48107ms step_avg:87.15ms
step:553/1680 train_time:48196ms step_avg:87.15ms
step:554/1680 train_time:48284ms step_avg:87.15ms
step:555/1680 train_time:48373ms step_avg:87.16ms
step:556/1680 train_time:48461ms step_avg:87.16ms
step:557/1680 train_time:48549ms step_avg:87.16ms
step:558/1680 train_time:48637ms step_avg:87.16ms
step:559/1680 train_time:48725ms step_avg:87.16ms
step:560/1680 train_time:48813ms step_avg:87.17ms
step:561/1680 train_time:48901ms step_avg:87.17ms
step:562/1680 train_time:48990ms step_avg:87.17ms
step:563/1680 train_time:49078ms step_avg:87.17ms
step:564/1680 train_time:49167ms step_avg:87.18ms
step:565/1680 train_time:49256ms step_avg:87.18ms
step:566/1680 train_time:49345ms step_avg:87.18ms
step:567/1680 train_time:49433ms step_avg:87.18ms
step:568/1680 train_time:49521ms step_avg:87.18ms
step:569/1680 train_time:49608ms step_avg:87.18ms
step:570/1680 train_time:49696ms step_avg:87.19ms
step:571/1680 train_time:49784ms step_avg:87.19ms
step:572/1680 train_time:49872ms step_avg:87.19ms
step:573/1680 train_time:49960ms step_avg:87.19ms
step:574/1680 train_time:50048ms step_avg:87.19ms
step:575/1680 train_time:50136ms step_avg:87.19ms
step:576/1680 train_time:50225ms step_avg:87.20ms
step:577/1680 train_time:50314ms step_avg:87.20ms
step:578/1680 train_time:50402ms step_avg:87.20ms
step:579/1680 train_time:50490ms step_avg:87.20ms
step:580/1680 train_time:50579ms step_avg:87.20ms
step:581/1680 train_time:50666ms step_avg:87.21ms
step:582/1680 train_time:50755ms step_avg:87.21ms
step:583/1680 train_time:50845ms step_avg:87.21ms
step:584/1680 train_time:50933ms step_avg:87.21ms
step:585/1680 train_time:51022ms step_avg:87.22ms
step:586/1680 train_time:51110ms step_avg:87.22ms
step:587/1680 train_time:51197ms step_avg:87.22ms
step:588/1680 train_time:51286ms step_avg:87.22ms
step:589/1680 train_time:51374ms step_avg:87.22ms
step:590/1680 train_time:51462ms step_avg:87.22ms
step:591/1680 train_time:51549ms step_avg:87.22ms
step:592/1680 train_time:51638ms step_avg:87.23ms
step:593/1680 train_time:51726ms step_avg:87.23ms
step:594/1680 train_time:51815ms step_avg:87.23ms
step:595/1680 train_time:51903ms step_avg:87.23ms
step:596/1680 train_time:51991ms step_avg:87.23ms
step:597/1680 train_time:52080ms step_avg:87.24ms
step:598/1680 train_time:52168ms step_avg:87.24ms
step:599/1680 train_time:52256ms step_avg:87.24ms
step:600/1680 train_time:52345ms step_avg:87.24ms
step:601/1680 train_time:52432ms step_avg:87.24ms
step:602/1680 train_time:52520ms step_avg:87.24ms
step:603/1680 train_time:52609ms step_avg:87.25ms
step:604/1680 train_time:52697ms step_avg:87.25ms
step:605/1680 train_time:52785ms step_avg:87.25ms
step:606/1680 train_time:52874ms step_avg:87.25ms
step:607/1680 train_time:52962ms step_avg:87.25ms
step:608/1680 train_time:53050ms step_avg:87.25ms
step:609/1680 train_time:53139ms step_avg:87.26ms
step:610/1680 train_time:53227ms step_avg:87.26ms
step:611/1680 train_time:53315ms step_avg:87.26ms
step:612/1680 train_time:53404ms step_avg:87.26ms
step:613/1680 train_time:53493ms step_avg:87.26ms
step:614/1680 train_time:53581ms step_avg:87.27ms
step:615/1680 train_time:53670ms step_avg:87.27ms
step:616/1680 train_time:53758ms step_avg:87.27ms
step:617/1680 train_time:53847ms step_avg:87.27ms
step:618/1680 train_time:53934ms step_avg:87.27ms
step:619/1680 train_time:54022ms step_avg:87.27ms
step:620/1680 train_time:54111ms step_avg:87.28ms
step:621/1680 train_time:54199ms step_avg:87.28ms
step:622/1680 train_time:54287ms step_avg:87.28ms
step:623/1680 train_time:54375ms step_avg:87.28ms
step:624/1680 train_time:54463ms step_avg:87.28ms
step:625/1680 train_time:54551ms step_avg:87.28ms
step:625/1680 val_loss:3.6159 train_time:54641ms step_avg:87.43ms
step:626/1680 train_time:54660ms step_avg:87.32ms
step:627/1680 train_time:54731ms step_avg:87.29ms
step:628/1680 train_time:54821ms step_avg:87.30ms
step:629/1680 train_time:54911ms step_avg:87.30ms
step:630/1680 train_time:55000ms step_avg:87.30ms
step:631/1680 train_time:55087ms step_avg:87.30ms
step:632/1680 train_time:55174ms step_avg:87.30ms
step:633/1680 train_time:55262ms step_avg:87.30ms
step:634/1680 train_time:55348ms step_avg:87.30ms
step:635/1680 train_time:55435ms step_avg:87.30ms
step:636/1680 train_time:55523ms step_avg:87.30ms
step:637/1680 train_time:55614ms step_avg:87.31ms
step:638/1680 train_time:55704ms step_avg:87.31ms
step:639/1680 train_time:55793ms step_avg:87.31ms
step:640/1680 train_time:55884ms step_avg:87.32ms
step:641/1680 train_time:55973ms step_avg:87.32ms
step:642/1680 train_time:56062ms step_avg:87.32ms
step:643/1680 train_time:56148ms step_avg:87.32ms
step:644/1680 train_time:56235ms step_avg:87.32ms
step:645/1680 train_time:56323ms step_avg:87.32ms
step:646/1680 train_time:56410ms step_avg:87.32ms
step:647/1680 train_time:56498ms step_avg:87.32ms
step:648/1680 train_time:56587ms step_avg:87.33ms
step:649/1680 train_time:56675ms step_avg:87.33ms
step:650/1680 train_time:56765ms step_avg:87.33ms
step:651/1680 train_time:56854ms step_avg:87.33ms
step:652/1680 train_time:56942ms step_avg:87.33ms
step:653/1680 train_time:57030ms step_avg:87.34ms
step:654/1680 train_time:57119ms step_avg:87.34ms
step:655/1680 train_time:57206ms step_avg:87.34ms
step:656/1680 train_time:57294ms step_avg:87.34ms
step:657/1680 train_time:57382ms step_avg:87.34ms
step:658/1680 train_time:57469ms step_avg:87.34ms
step:659/1680 train_time:57558ms step_avg:87.34ms
step:660/1680 train_time:57647ms step_avg:87.34ms
step:661/1680 train_time:57736ms step_avg:87.35ms
step:662/1680 train_time:57824ms step_avg:87.35ms
step:663/1680 train_time:57913ms step_avg:87.35ms
step:664/1680 train_time:58002ms step_avg:87.35ms
step:665/1680 train_time:58090ms step_avg:87.35ms
step:666/1680 train_time:58177ms step_avg:87.35ms
step:667/1680 train_time:58265ms step_avg:87.35ms
step:668/1680 train_time:58353ms step_avg:87.35ms
step:669/1680 train_time:58440ms step_avg:87.35ms
step:670/1680 train_time:58528ms step_avg:87.36ms
step:671/1680 train_time:58617ms step_avg:87.36ms
step:672/1680 train_time:58706ms step_avg:87.36ms
step:673/1680 train_time:58795ms step_avg:87.36ms
step:674/1680 train_time:58884ms step_avg:87.36ms
step:675/1680 train_time:58972ms step_avg:87.37ms
step:676/1680 train_time:59062ms step_avg:87.37ms
step:677/1680 train_time:59150ms step_avg:87.37ms
step:678/1680 train_time:59237ms step_avg:87.37ms
step:679/1680 train_time:59325ms step_avg:87.37ms
step:680/1680 train_time:59413ms step_avg:87.37ms
step:681/1680 train_time:59501ms step_avg:87.37ms
step:682/1680 train_time:59589ms step_avg:87.37ms
step:683/1680 train_time:59679ms step_avg:87.38ms
step:684/1680 train_time:59767ms step_avg:87.38ms
step:685/1680 train_time:59856ms step_avg:87.38ms
step:686/1680 train_time:59944ms step_avg:87.38ms
step:687/1680 train_time:60033ms step_avg:87.38ms
step:688/1680 train_time:60121ms step_avg:87.39ms
step:689/1680 train_time:60208ms step_avg:87.39ms
step:690/1680 train_time:60296ms step_avg:87.39ms
step:691/1680 train_time:60384ms step_avg:87.39ms
step:692/1680 train_time:60472ms step_avg:87.39ms
step:693/1680 train_time:60561ms step_avg:87.39ms
step:694/1680 train_time:60649ms step_avg:87.39ms
step:695/1680 train_time:60737ms step_avg:87.39ms
step:696/1680 train_time:60826ms step_avg:87.39ms
step:697/1680 train_time:60914ms step_avg:87.40ms
step:698/1680 train_time:61004ms step_avg:87.40ms
step:699/1680 train_time:61091ms step_avg:87.40ms
step:700/1680 train_time:61179ms step_avg:87.40ms
step:701/1680 train_time:61267ms step_avg:87.40ms
step:702/1680 train_time:61355ms step_avg:87.40ms
step:703/1680 train_time:61443ms step_avg:87.40ms
step:704/1680 train_time:61532ms step_avg:87.40ms
step:705/1680 train_time:61620ms step_avg:87.40ms
step:706/1680 train_time:61708ms step_avg:87.41ms
step:707/1680 train_time:61796ms step_avg:87.41ms
step:708/1680 train_time:61885ms step_avg:87.41ms
step:709/1680 train_time:61974ms step_avg:87.41ms
step:710/1680 train_time:62063ms step_avg:87.41ms
step:711/1680 train_time:62151ms step_avg:87.41ms
step:712/1680 train_time:62239ms step_avg:87.41ms
step:713/1680 train_time:62327ms step_avg:87.41ms
step:714/1680 train_time:62415ms step_avg:87.42ms
step:715/1680 train_time:62503ms step_avg:87.42ms
step:716/1680 train_time:62591ms step_avg:87.42ms
step:717/1680 train_time:62679ms step_avg:87.42ms
step:718/1680 train_time:62767ms step_avg:87.42ms
step:719/1680 train_time:62855ms step_avg:87.42ms
step:720/1680 train_time:62943ms step_avg:87.42ms
step:721/1680 train_time:63032ms step_avg:87.42ms
step:722/1680 train_time:63120ms step_avg:87.42ms
step:723/1680 train_time:63208ms step_avg:87.42ms
step:724/1680 train_time:63296ms step_avg:87.43ms
step:725/1680 train_time:63384ms step_avg:87.43ms
step:726/1680 train_time:63473ms step_avg:87.43ms
step:727/1680 train_time:63562ms step_avg:87.43ms
step:728/1680 train_time:63649ms step_avg:87.43ms
step:729/1680 train_time:63737ms step_avg:87.43ms
step:730/1680 train_time:63825ms step_avg:87.43ms
step:731/1680 train_time:63914ms step_avg:87.43ms
step:732/1680 train_time:64003ms step_avg:87.44ms
step:733/1680 train_time:64090ms step_avg:87.44ms
step:734/1680 train_time:64178ms step_avg:87.44ms
step:735/1680 train_time:64266ms step_avg:87.44ms
step:736/1680 train_time:64354ms step_avg:87.44ms
step:737/1680 train_time:64443ms step_avg:87.44ms
step:738/1680 train_time:64531ms step_avg:87.44ms
step:739/1680 train_time:64619ms step_avg:87.44ms
step:740/1680 train_time:64707ms step_avg:87.44ms
step:741/1680 train_time:64795ms step_avg:87.44ms
step:742/1680 train_time:64883ms step_avg:87.44ms
step:743/1680 train_time:64972ms step_avg:87.44ms
step:744/1680 train_time:65060ms step_avg:87.45ms
step:745/1680 train_time:65147ms step_avg:87.45ms
step:746/1680 train_time:65236ms step_avg:87.45ms
step:747/1680 train_time:65323ms step_avg:87.45ms
step:748/1680 train_time:65412ms step_avg:87.45ms
step:749/1680 train_time:65499ms step_avg:87.45ms
step:750/1680 train_time:65588ms step_avg:87.45ms
step:750/1680 val_loss:3.5655 train_time:65678ms step_avg:87.57ms
step:751/1680 train_time:65697ms step_avg:87.48ms
step:752/1680 train_time:65769ms step_avg:87.46ms
step:753/1680 train_time:65862ms step_avg:87.47ms
step:754/1680 train_time:65951ms step_avg:87.47ms
step:755/1680 train_time:66038ms step_avg:87.47ms
step:756/1680 train_time:66125ms step_avg:87.47ms
step:757/1680 train_time:66212ms step_avg:87.47ms
step:758/1680 train_time:66300ms step_avg:87.47ms
step:759/1680 train_time:66387ms step_avg:87.47ms
step:760/1680 train_time:66476ms step_avg:87.47ms
step:761/1680 train_time:66563ms step_avg:87.47ms
step:762/1680 train_time:66652ms step_avg:87.47ms
step:763/1680 train_time:66741ms step_avg:87.47ms
step:764/1680 train_time:66832ms step_avg:87.48ms
step:765/1680 train_time:66921ms step_avg:87.48ms
step:766/1680 train_time:67009ms step_avg:87.48ms
step:767/1680 train_time:67096ms step_avg:87.48ms
step:768/1680 train_time:67184ms step_avg:87.48ms
step:769/1680 train_time:67271ms step_avg:87.48ms
step:770/1680 train_time:67359ms step_avg:87.48ms
step:771/1680 train_time:67447ms step_avg:87.48ms
step:772/1680 train_time:67535ms step_avg:87.48ms
step:773/1680 train_time:67623ms step_avg:87.48ms
step:774/1680 train_time:67713ms step_avg:87.48ms
step:775/1680 train_time:67802ms step_avg:87.49ms
step:776/1680 train_time:67891ms step_avg:87.49ms
step:777/1680 train_time:67980ms step_avg:87.49ms
step:778/1680 train_time:68068ms step_avg:87.49ms
step:779/1680 train_time:68156ms step_avg:87.49ms
step:780/1680 train_time:68243ms step_avg:87.49ms
step:781/1680 train_time:68331ms step_avg:87.49ms
step:782/1680 train_time:68419ms step_avg:87.49ms
step:783/1680 train_time:68507ms step_avg:87.49ms
step:784/1680 train_time:68594ms step_avg:87.49ms
step:785/1680 train_time:68683ms step_avg:87.49ms
step:786/1680 train_time:68771ms step_avg:87.50ms
step:787/1680 train_time:68860ms step_avg:87.50ms
step:788/1680 train_time:68948ms step_avg:87.50ms
step:789/1680 train_time:69037ms step_avg:87.50ms
step:790/1680 train_time:69125ms step_avg:87.50ms
step:791/1680 train_time:69214ms step_avg:87.50ms
step:792/1680 train_time:69301ms step_avg:87.50ms
step:793/1680 train_time:69389ms step_avg:87.50ms
step:794/1680 train_time:69478ms step_avg:87.50ms
step:795/1680 train_time:69566ms step_avg:87.50ms
step:796/1680 train_time:69654ms step_avg:87.51ms
step:797/1680 train_time:69743ms step_avg:87.51ms
step:798/1680 train_time:69831ms step_avg:87.51ms
step:799/1680 train_time:69919ms step_avg:87.51ms
step:800/1680 train_time:70007ms step_avg:87.51ms
step:801/1680 train_time:70095ms step_avg:87.51ms
step:802/1680 train_time:70183ms step_avg:87.51ms
step:803/1680 train_time:70271ms step_avg:87.51ms
step:804/1680 train_time:70359ms step_avg:87.51ms
step:805/1680 train_time:70446ms step_avg:87.51ms
step:806/1680 train_time:70534ms step_avg:87.51ms
step:807/1680 train_time:70623ms step_avg:87.51ms
step:808/1680 train_time:70711ms step_avg:87.51ms
step:809/1680 train_time:70800ms step_avg:87.52ms
step:810/1680 train_time:70888ms step_avg:87.52ms
step:811/1680 train_time:70976ms step_avg:87.52ms
step:812/1680 train_time:71064ms step_avg:87.52ms
step:813/1680 train_time:71154ms step_avg:87.52ms
step:814/1680 train_time:71242ms step_avg:87.52ms
step:815/1680 train_time:71330ms step_avg:87.52ms
step:816/1680 train_time:71418ms step_avg:87.52ms
step:817/1680 train_time:71506ms step_avg:87.52ms
step:818/1680 train_time:71594ms step_avg:87.52ms
step:819/1680 train_time:71682ms step_avg:87.52ms
step:820/1680 train_time:71770ms step_avg:87.52ms
step:821/1680 train_time:71859ms step_avg:87.53ms
step:822/1680 train_time:71947ms step_avg:87.53ms
step:823/1680 train_time:72035ms step_avg:87.53ms
step:824/1680 train_time:72124ms step_avg:87.53ms
step:825/1680 train_time:72213ms step_avg:87.53ms
step:826/1680 train_time:72302ms step_avg:87.53ms
step:827/1680 train_time:72389ms step_avg:87.53ms
step:828/1680 train_time:72477ms step_avg:87.53ms
step:829/1680 train_time:72565ms step_avg:87.53ms
step:830/1680 train_time:72653ms step_avg:87.53ms
step:831/1680 train_time:72742ms step_avg:87.54ms
step:832/1680 train_time:72830ms step_avg:87.54ms
step:833/1680 train_time:72918ms step_avg:87.54ms
step:834/1680 train_time:73006ms step_avg:87.54ms
step:835/1680 train_time:73094ms step_avg:87.54ms
step:836/1680 train_time:73182ms step_avg:87.54ms
step:837/1680 train_time:73271ms step_avg:87.54ms
step:838/1680 train_time:73360ms step_avg:87.54ms
step:839/1680 train_time:73448ms step_avg:87.54ms
step:840/1680 train_time:73536ms step_avg:87.54ms
step:841/1680 train_time:73623ms step_avg:87.54ms
step:842/1680 train_time:73712ms step_avg:87.54ms
step:843/1680 train_time:73800ms step_avg:87.54ms
step:844/1680 train_time:73888ms step_avg:87.55ms
step:845/1680 train_time:73977ms step_avg:87.55ms
step:846/1680 train_time:74065ms step_avg:87.55ms
step:847/1680 train_time:74153ms step_avg:87.55ms
step:848/1680 train_time:74242ms step_avg:87.55ms
step:849/1680 train_time:74330ms step_avg:87.55ms
step:850/1680 train_time:74419ms step_avg:87.55ms
step:851/1680 train_time:74507ms step_avg:87.55ms
step:852/1680 train_time:74595ms step_avg:87.55ms
step:853/1680 train_time:74683ms step_avg:87.55ms
step:854/1680 train_time:74771ms step_avg:87.55ms
step:855/1680 train_time:74860ms step_avg:87.56ms
step:856/1680 train_time:74948ms step_avg:87.56ms
step:857/1680 train_time:75037ms step_avg:87.56ms
step:858/1680 train_time:75125ms step_avg:87.56ms
step:859/1680 train_time:75213ms step_avg:87.56ms
step:860/1680 train_time:75302ms step_avg:87.56ms
step:861/1680 train_time:75390ms step_avg:87.56ms
step:862/1680 train_time:75478ms step_avg:87.56ms
step:863/1680 train_time:75567ms step_avg:87.56ms
step:864/1680 train_time:75655ms step_avg:87.56ms
step:865/1680 train_time:75744ms step_avg:87.56ms
step:866/1680 train_time:75832ms step_avg:87.57ms
step:867/1680 train_time:75920ms step_avg:87.57ms
step:868/1680 train_time:76008ms step_avg:87.57ms
step:869/1680 train_time:76096ms step_avg:87.57ms
step:870/1680 train_time:76184ms step_avg:87.57ms
step:871/1680 train_time:76272ms step_avg:87.57ms
step:872/1680 train_time:76360ms step_avg:87.57ms
step:873/1680 train_time:76449ms step_avg:87.57ms
step:874/1680 train_time:76537ms step_avg:87.57ms
step:875/1680 train_time:76625ms step_avg:87.57ms
step:875/1680 val_loss:3.5193 train_time:76714ms step_avg:87.67ms
step:876/1680 train_time:76733ms step_avg:87.59ms
step:877/1680 train_time:76805ms step_avg:87.58ms
step:878/1680 train_time:76896ms step_avg:87.58ms
step:879/1680 train_time:76987ms step_avg:87.58ms
step:880/1680 train_time:77075ms step_avg:87.59ms
step:881/1680 train_time:77162ms step_avg:87.58ms
step:882/1680 train_time:77249ms step_avg:87.58ms
step:883/1680 train_time:77336ms step_avg:87.58ms
step:884/1680 train_time:77423ms step_avg:87.58ms
step:885/1680 train_time:77510ms step_avg:87.58ms
step:886/1680 train_time:77598ms step_avg:87.58ms
step:887/1680 train_time:77687ms step_avg:87.58ms
step:888/1680 train_time:77778ms step_avg:87.59ms
step:889/1680 train_time:77868ms step_avg:87.59ms
step:890/1680 train_time:77957ms step_avg:87.59ms
step:891/1680 train_time:78046ms step_avg:87.59ms
step:892/1680 train_time:78134ms step_avg:87.59ms
step:893/1680 train_time:78221ms step_avg:87.59ms
step:894/1680 train_time:78309ms step_avg:87.59ms
step:895/1680 train_time:78396ms step_avg:87.59ms
step:896/1680 train_time:78483ms step_avg:87.59ms
step:897/1680 train_time:78571ms step_avg:87.59ms
step:898/1680 train_time:78659ms step_avg:87.59ms
step:899/1680 train_time:78748ms step_avg:87.60ms
step:900/1680 train_time:78837ms step_avg:87.60ms
step:901/1680 train_time:78927ms step_avg:87.60ms
step:902/1680 train_time:79016ms step_avg:87.60ms
step:903/1680 train_time:79104ms step_avg:87.60ms
step:904/1680 train_time:79192ms step_avg:87.60ms
step:905/1680 train_time:79280ms step_avg:87.60ms
step:906/1680 train_time:79368ms step_avg:87.60ms
step:907/1680 train_time:79455ms step_avg:87.60ms
step:908/1680 train_time:79543ms step_avg:87.60ms
step:909/1680 train_time:79630ms step_avg:87.60ms
step:910/1680 train_time:79719ms step_avg:87.60ms
step:911/1680 train_time:79809ms step_avg:87.61ms
step:912/1680 train_time:79897ms step_avg:87.61ms
step:913/1680 train_time:79986ms step_avg:87.61ms
step:914/1680 train_time:80074ms step_avg:87.61ms
step:915/1680 train_time:80162ms step_avg:87.61ms
step:916/1680 train_time:80249ms step_avg:87.61ms
step:917/1680 train_time:80337ms step_avg:87.61ms
step:918/1680 train_time:80425ms step_avg:87.61ms
step:919/1680 train_time:80514ms step_avg:87.61ms
step:920/1680 train_time:80601ms step_avg:87.61ms
step:921/1680 train_time:80689ms step_avg:87.61ms
step:922/1680 train_time:80778ms step_avg:87.61ms
step:923/1680 train_time:80867ms step_avg:87.61ms
step:924/1680 train_time:80956ms step_avg:87.61ms
step:925/1680 train_time:81044ms step_avg:87.62ms
step:926/1680 train_time:81133ms step_avg:87.62ms
step:927/1680 train_time:81221ms step_avg:87.62ms
step:928/1680 train_time:81309ms step_avg:87.62ms
step:929/1680 train_time:81397ms step_avg:87.62ms
step:930/1680 train_time:81485ms step_avg:87.62ms
step:931/1680 train_time:81574ms step_avg:87.62ms
step:932/1680 train_time:81663ms step_avg:87.62ms
step:933/1680 train_time:81751ms step_avg:87.62ms
step:934/1680 train_time:81839ms step_avg:87.62ms
step:935/1680 train_time:81929ms step_avg:87.62ms
step:936/1680 train_time:82018ms step_avg:87.63ms
step:937/1680 train_time:82107ms step_avg:87.63ms
step:938/1680 train_time:82195ms step_avg:87.63ms
step:939/1680 train_time:82283ms step_avg:87.63ms
step:940/1680 train_time:82371ms step_avg:87.63ms
step:941/1680 train_time:82458ms step_avg:87.63ms
step:942/1680 train_time:82547ms step_avg:87.63ms
step:943/1680 train_time:82635ms step_avg:87.63ms
step:944/1680 train_time:82723ms step_avg:87.63ms
step:945/1680 train_time:82812ms step_avg:87.63ms
step:946/1680 train_time:82900ms step_avg:87.63ms
step:947/1680 train_time:82990ms step_avg:87.63ms
step:948/1680 train_time:83078ms step_avg:87.63ms
step:949/1680 train_time:83166ms step_avg:87.63ms
step:950/1680 train_time:83253ms step_avg:87.63ms
step:951/1680 train_time:83341ms step_avg:87.64ms
step:952/1680 train_time:83429ms step_avg:87.64ms
step:953/1680 train_time:83517ms step_avg:87.64ms
step:954/1680 train_time:83605ms step_avg:87.64ms
step:955/1680 train_time:83693ms step_avg:87.64ms
step:956/1680 train_time:83782ms step_avg:87.64ms
step:957/1680 train_time:83870ms step_avg:87.64ms
step:958/1680 train_time:83958ms step_avg:87.64ms
step:959/1680 train_time:84047ms step_avg:87.64ms
step:960/1680 train_time:84135ms step_avg:87.64ms
step:961/1680 train_time:84223ms step_avg:87.64ms
step:962/1680 train_time:84311ms step_avg:87.64ms
step:963/1680 train_time:84399ms step_avg:87.64ms
step:964/1680 train_time:84487ms step_avg:87.64ms
step:965/1680 train_time:84576ms step_avg:87.64ms
step:966/1680 train_time:84664ms step_avg:87.64ms
step:967/1680 train_time:84751ms step_avg:87.64ms
step:968/1680 train_time:84839ms step_avg:87.64ms
step:969/1680 train_time:84927ms step_avg:87.64ms
step:970/1680 train_time:85016ms step_avg:87.65ms
step:971/1680 train_time:85104ms step_avg:87.65ms
step:972/1680 train_time:85193ms step_avg:87.65ms
step:973/1680 train_time:85282ms step_avg:87.65ms
step:974/1680 train_time:85370ms step_avg:87.65ms
step:975/1680 train_time:85458ms step_avg:87.65ms
step:976/1680 train_time:85546ms step_avg:87.65ms
step:977/1680 train_time:85634ms step_avg:87.65ms
step:978/1680 train_time:85722ms step_avg:87.65ms
step:979/1680 train_time:85811ms step_avg:87.65ms
step:980/1680 train_time:85898ms step_avg:87.65ms
step:981/1680 train_time:85986ms step_avg:87.65ms
step:982/1680 train_time:86075ms step_avg:87.65ms
step:983/1680 train_time:86164ms step_avg:87.65ms
step:984/1680 train_time:86253ms step_avg:87.66ms
step:985/1680 train_time:86340ms step_avg:87.66ms
step:986/1680 train_time:86428ms step_avg:87.66ms
step:987/1680 train_time:86516ms step_avg:87.66ms
step:988/1680 train_time:86604ms step_avg:87.66ms
step:989/1680 train_time:86692ms step_avg:87.66ms
step:990/1680 train_time:86780ms step_avg:87.66ms
step:991/1680 train_time:86869ms step_avg:87.66ms
step:992/1680 train_time:86956ms step_avg:87.66ms
step:993/1680 train_time:87045ms step_avg:87.66ms
step:994/1680 train_time:87133ms step_avg:87.66ms
step:995/1680 train_time:87221ms step_avg:87.66ms
step:996/1680 train_time:87309ms step_avg:87.66ms
step:997/1680 train_time:87397ms step_avg:87.66ms
step:998/1680 train_time:87485ms step_avg:87.66ms
step:999/1680 train_time:87573ms step_avg:87.66ms
step:1000/1680 train_time:87660ms step_avg:87.66ms
step:1000/1680 val_loss:3.4679 train_time:87749ms step_avg:87.75ms
step:1001/1680 train_time:87768ms step_avg:87.68ms
step:1002/1680 train_time:87841ms step_avg:87.67ms
step:1003/1680 train_time:87932ms step_avg:87.67ms
step:1004/1680 train_time:88022ms step_avg:87.67ms
step:1005/1680 train_time:88110ms step_avg:87.67ms
step:1006/1680 train_time:88197ms step_avg:87.67ms
step:1007/1680 train_time:88284ms step_avg:87.67ms
step:1008/1680 train_time:88371ms step_avg:87.67ms
step:1009/1680 train_time:88458ms step_avg:87.67ms
step:1010/1680 train_time:88546ms step_avg:87.67ms
step:1011/1680 train_time:88633ms step_avg:87.67ms
step:1012/1680 train_time:88722ms step_avg:87.67ms
step:1013/1680 train_time:88812ms step_avg:87.67ms
step:1014/1680 train_time:88902ms step_avg:87.67ms
step:1015/1680 train_time:88991ms step_avg:87.68ms
step:1016/1680 train_time:89080ms step_avg:87.68ms
step:1017/1680 train_time:89168ms step_avg:87.68ms
step:1018/1680 train_time:89255ms step_avg:87.68ms
step:1019/1680 train_time:89343ms step_avg:87.68ms
step:1020/1680 train_time:89431ms step_avg:87.68ms
step:1021/1680 train_time:89518ms step_avg:87.68ms
step:1022/1680 train_time:89605ms step_avg:87.68ms
step:1023/1680 train_time:89693ms step_avg:87.68ms
step:1024/1680 train_time:89781ms step_avg:87.68ms
step:1025/1680 train_time:89871ms step_avg:87.68ms
step:1026/1680 train_time:89960ms step_avg:87.68ms
step:1027/1680 train_time:90050ms step_avg:87.68ms
step:1028/1680 train_time:90139ms step_avg:87.68ms
step:1029/1680 train_time:90227ms step_avg:87.68ms
step:1030/1680 train_time:90314ms step_avg:87.68ms
step:1031/1680 train_time:90402ms step_avg:87.68ms
step:1032/1680 train_time:90490ms step_avg:87.68ms
step:1033/1680 train_time:90577ms step_avg:87.68ms
step:1034/1680 train_time:90666ms step_avg:87.68ms
step:1035/1680 train_time:90754ms step_avg:87.68ms
step:1036/1680 train_time:90842ms step_avg:87.69ms
step:1037/1680 train_time:90931ms step_avg:87.69ms
step:1038/1680 train_time:91020ms step_avg:87.69ms
step:1039/1680 train_time:91109ms step_avg:87.69ms
step:1040/1680 train_time:91197ms step_avg:87.69ms
step:1041/1680 train_time:91285ms step_avg:87.69ms
step:1042/1680 train_time:91373ms step_avg:87.69ms
step:1043/1680 train_time:91461ms step_avg:87.69ms
step:1044/1680 train_time:91549ms step_avg:87.69ms
step:1045/1680 train_time:91638ms step_avg:87.69ms
step:1046/1680 train_time:91726ms step_avg:87.69ms
step:1047/1680 train_time:91814ms step_avg:87.69ms
step:1048/1680 train_time:91902ms step_avg:87.69ms
step:1049/1680 train_time:91990ms step_avg:87.69ms
step:1050/1680 train_time:92079ms step_avg:87.69ms
step:1051/1680 train_time:92168ms step_avg:87.70ms
step:1052/1680 train_time:92257ms step_avg:87.70ms
step:1053/1680 train_time:92344ms step_avg:87.70ms
step:1054/1680 train_time:92432ms step_avg:87.70ms
step:1055/1680 train_time:92520ms step_avg:87.70ms
step:1056/1680 train_time:92608ms step_avg:87.70ms
step:1057/1680 train_time:92697ms step_avg:87.70ms
step:1058/1680 train_time:92785ms step_avg:87.70ms
step:1059/1680 train_time:92873ms step_avg:87.70ms
step:1060/1680 train_time:92961ms step_avg:87.70ms
step:1061/1680 train_time:93051ms step_avg:87.70ms
step:1062/1680 train_time:93140ms step_avg:87.70ms
step:1063/1680 train_time:93229ms step_avg:87.70ms
step:1064/1680 train_time:93316ms step_avg:87.70ms
step:1065/1680 train_time:93404ms step_avg:87.70ms
step:1066/1680 train_time:93491ms step_avg:87.70ms
step:1067/1680 train_time:93579ms step_avg:87.70ms
step:1068/1680 train_time:93668ms step_avg:87.70ms
step:1069/1680 train_time:93756ms step_avg:87.70ms
step:1070/1680 train_time:93845ms step_avg:87.71ms
step:1071/1680 train_time:93933ms step_avg:87.71ms
step:1072/1680 train_time:94021ms step_avg:87.71ms
step:1073/1680 train_time:94110ms step_avg:87.71ms
step:1074/1680 train_time:94198ms step_avg:87.71ms
step:1075/1680 train_time:94287ms step_avg:87.71ms
step:1076/1680 train_time:94374ms step_avg:87.71ms
step:1077/1680 train_time:94463ms step_avg:87.71ms
step:1078/1680 train_time:94550ms step_avg:87.71ms
step:1079/1680 train_time:94639ms step_avg:87.71ms
step:1080/1680 train_time:94728ms step_avg:87.71ms
step:1081/1680 train_time:94816ms step_avg:87.71ms
step:1082/1680 train_time:94904ms step_avg:87.71ms
step:1083/1680 train_time:94992ms step_avg:87.71ms
step:1084/1680 train_time:95081ms step_avg:87.71ms
step:1085/1680 train_time:95169ms step_avg:87.71ms
step:1086/1680 train_time:95257ms step_avg:87.71ms
step:1087/1680 train_time:95345ms step_avg:87.71ms
step:1088/1680 train_time:95433ms step_avg:87.71ms
step:1089/1680 train_time:95521ms step_avg:87.71ms
step:1090/1680 train_time:95609ms step_avg:87.71ms
step:1091/1680 train_time:95698ms step_avg:87.72ms
step:1092/1680 train_time:95786ms step_avg:87.72ms
step:1093/1680 train_time:95873ms step_avg:87.72ms
step:1094/1680 train_time:95962ms step_avg:87.72ms
step:1095/1680 train_time:96051ms step_avg:87.72ms
step:1096/1680 train_time:96140ms step_avg:87.72ms
step:1097/1680 train_time:96230ms step_avg:87.72ms
step:1098/1680 train_time:96318ms step_avg:87.72ms
step:1099/1680 train_time:96407ms step_avg:87.72ms
step:1100/1680 train_time:96495ms step_avg:87.72ms
step:1101/1680 train_time:96585ms step_avg:87.72ms
step:1102/1680 train_time:96674ms step_avg:87.73ms
step:1103/1680 train_time:96763ms step_avg:87.73ms
step:1104/1680 train_time:96852ms step_avg:87.73ms
step:1105/1680 train_time:96941ms step_avg:87.73ms
step:1106/1680 train_time:97030ms step_avg:87.73ms
step:1107/1680 train_time:97119ms step_avg:87.73ms
step:1108/1680 train_time:97209ms step_avg:87.73ms
step:1109/1680 train_time:97298ms step_avg:87.73ms
step:1110/1680 train_time:97386ms step_avg:87.74ms
step:1111/1680 train_time:97476ms step_avg:87.74ms
step:1112/1680 train_time:97564ms step_avg:87.74ms
step:1113/1680 train_time:97652ms step_avg:87.74ms
step:1114/1680 train_time:97741ms step_avg:87.74ms
step:1115/1680 train_time:97831ms step_avg:87.74ms
step:1116/1680 train_time:97920ms step_avg:87.74ms
step:1117/1680 train_time:98009ms step_avg:87.74ms
step:1118/1680 train_time:98098ms step_avg:87.74ms
step:1119/1680 train_time:98188ms step_avg:87.75ms
step:1120/1680 train_time:98277ms step_avg:87.75ms
step:1121/1680 train_time:98366ms step_avg:87.75ms
step:1122/1680 train_time:98454ms step_avg:87.75ms
step:1123/1680 train_time:98543ms step_avg:87.75ms
step:1124/1680 train_time:98632ms step_avg:87.75ms
step:1125/1680 train_time:98720ms step_avg:87.75ms
step:1125/1680 val_loss:3.4153 train_time:98811ms step_avg:87.83ms
step:1126/1680 train_time:98831ms step_avg:87.77ms
step:1127/1680 train_time:98900ms step_avg:87.75ms
step:1128/1680 train_time:98993ms step_avg:87.76ms
step:1129/1680 train_time:99084ms step_avg:87.76ms
step:1130/1680 train_time:99173ms step_avg:87.76ms
step:1131/1680 train_time:99262ms step_avg:87.76ms
step:1132/1680 train_time:99349ms step_avg:87.76ms
step:1133/1680 train_time:99437ms step_avg:87.76ms
step:1134/1680 train_time:99524ms step_avg:87.76ms
step:1135/1680 train_time:99612ms step_avg:87.76ms
step:1136/1680 train_time:99700ms step_avg:87.76ms
step:1137/1680 train_time:99790ms step_avg:87.77ms
step:1138/1680 train_time:99882ms step_avg:87.77ms
step:1139/1680 train_time:99973ms step_avg:87.77ms
step:1140/1680 train_time:100064ms step_avg:87.78ms
step:1141/1680 train_time:100153ms step_avg:87.78ms
step:1142/1680 train_time:100242ms step_avg:87.78ms
step:1143/1680 train_time:100330ms step_avg:87.78ms
step:1144/1680 train_time:100418ms step_avg:87.78ms
step:1145/1680 train_time:100507ms step_avg:87.78ms
step:1146/1680 train_time:100594ms step_avg:87.78ms
step:1147/1680 train_time:100683ms step_avg:87.78ms
step:1148/1680 train_time:100772ms step_avg:87.78ms
step:1149/1680 train_time:100862ms step_avg:87.78ms
step:1150/1680 train_time:100951ms step_avg:87.78ms
step:1151/1680 train_time:101042ms step_avg:87.79ms
step:1152/1680 train_time:101131ms step_avg:87.79ms
step:1153/1680 train_time:101220ms step_avg:87.79ms
step:1154/1680 train_time:101309ms step_avg:87.79ms
step:1155/1680 train_time:101399ms step_avg:87.79ms
step:1156/1680 train_time:101488ms step_avg:87.79ms
step:1157/1680 train_time:101575ms step_avg:87.79ms
step:1158/1680 train_time:101664ms step_avg:87.79ms
step:1159/1680 train_time:101752ms step_avg:87.79ms
step:1160/1680 train_time:101842ms step_avg:87.79ms
step:1161/1680 train_time:101931ms step_avg:87.80ms
step:1162/1680 train_time:102021ms step_avg:87.80ms
step:1163/1680 train_time:102110ms step_avg:87.80ms
step:1164/1680 train_time:102200ms step_avg:87.80ms
step:1165/1680 train_time:102290ms step_avg:87.80ms
step:1166/1680 train_time:102379ms step_avg:87.80ms
step:1167/1680 train_time:102467ms step_avg:87.80ms
step:1168/1680 train_time:102556ms step_avg:87.80ms
step:1169/1680 train_time:102645ms step_avg:87.81ms
step:1170/1680 train_time:102734ms step_avg:87.81ms
step:1171/1680 train_time:102823ms step_avg:87.81ms
step:1172/1680 train_time:102912ms step_avg:87.81ms
step:1173/1680 train_time:103001ms step_avg:87.81ms
step:1174/1680 train_time:103091ms step_avg:87.81ms
step:1175/1680 train_time:103180ms step_avg:87.81ms
step:1176/1680 train_time:103269ms step_avg:87.81ms
step:1177/1680 train_time:103359ms step_avg:87.82ms
step:1178/1680 train_time:103449ms step_avg:87.82ms
step:1179/1680 train_time:103537ms step_avg:87.82ms
step:1180/1680 train_time:103625ms step_avg:87.82ms
step:1181/1680 train_time:103715ms step_avg:87.82ms
step:1182/1680 train_time:103804ms step_avg:87.82ms
step:1183/1680 train_time:103893ms step_avg:87.82ms
step:1184/1680 train_time:103981ms step_avg:87.82ms
step:1185/1680 train_time:104070ms step_avg:87.82ms
step:1186/1680 train_time:104160ms step_avg:87.82ms
step:1187/1680 train_time:104249ms step_avg:87.83ms
step:1188/1680 train_time:104337ms step_avg:87.83ms
step:1189/1680 train_time:104427ms step_avg:87.83ms
step:1190/1680 train_time:104516ms step_avg:87.83ms
step:1191/1680 train_time:104604ms step_avg:87.83ms
step:1192/1680 train_time:104693ms step_avg:87.83ms
step:1193/1680 train_time:104782ms step_avg:87.83ms
step:1194/1680 train_time:104871ms step_avg:87.83ms
step:1195/1680 train_time:104960ms step_avg:87.83ms
step:1196/1680 train_time:105049ms step_avg:87.83ms
step:1197/1680 train_time:105138ms step_avg:87.83ms
step:1198/1680 train_time:105227ms step_avg:87.84ms
step:1199/1680 train_time:105315ms step_avg:87.84ms
step:1200/1680 train_time:105405ms step_avg:87.84ms
step:1201/1680 train_time:105493ms step_avg:87.84ms
step:1202/1680 train_time:105582ms step_avg:87.84ms
step:1203/1680 train_time:105670ms step_avg:87.84ms
step:1204/1680 train_time:105759ms step_avg:87.84ms
step:1205/1680 train_time:105849ms step_avg:87.84ms
step:1206/1680 train_time:105938ms step_avg:87.84ms
step:1207/1680 train_time:106027ms step_avg:87.84ms
step:1208/1680 train_time:106116ms step_avg:87.84ms
step:1209/1680 train_time:106204ms step_avg:87.84ms
step:1210/1680 train_time:106293ms step_avg:87.85ms
step:1211/1680 train_time:106382ms step_avg:87.85ms
step:1212/1680 train_time:106472ms step_avg:87.85ms
step:1213/1680 train_time:106561ms step_avg:87.85ms
step:1214/1680 train_time:106649ms step_avg:87.85ms
step:1215/1680 train_time:106738ms step_avg:87.85ms
step:1216/1680 train_time:106827ms step_avg:87.85ms
step:1217/1680 train_time:106916ms step_avg:87.85ms
step:1218/1680 train_time:107006ms step_avg:87.85ms
step:1219/1680 train_time:107095ms step_avg:87.85ms
step:1220/1680 train_time:107185ms step_avg:87.86ms
step:1221/1680 train_time:107273ms step_avg:87.86ms
step:1222/1680 train_time:107362ms step_avg:87.86ms
step:1223/1680 train_time:107451ms step_avg:87.86ms
step:1224/1680 train_time:107539ms step_avg:87.86ms
step:1225/1680 train_time:107629ms step_avg:87.86ms
step:1226/1680 train_time:107717ms step_avg:87.86ms
step:1227/1680 train_time:107806ms step_avg:87.86ms
step:1228/1680 train_time:107896ms step_avg:87.86ms
step:1229/1680 train_time:107985ms step_avg:87.86ms
step:1230/1680 train_time:108074ms step_avg:87.87ms
step:1231/1680 train_time:108164ms step_avg:87.87ms
step:1232/1680 train_time:108253ms step_avg:87.87ms
step:1233/1680 train_time:108342ms step_avg:87.87ms
step:1234/1680 train_time:108431ms step_avg:87.87ms
step:1235/1680 train_time:108520ms step_avg:87.87ms
step:1236/1680 train_time:108610ms step_avg:87.87ms
step:1237/1680 train_time:108699ms step_avg:87.87ms
step:1238/1680 train_time:108789ms step_avg:87.87ms
step:1239/1680 train_time:108878ms step_avg:87.88ms
step:1240/1680 train_time:108966ms step_avg:87.88ms
step:1241/1680 train_time:109055ms step_avg:87.88ms
step:1242/1680 train_time:109144ms step_avg:87.88ms
step:1243/1680 train_time:109232ms step_avg:87.88ms
step:1244/1680 train_time:109322ms step_avg:87.88ms
step:1245/1680 train_time:109411ms step_avg:87.88ms
step:1246/1680 train_time:109500ms step_avg:87.88ms
step:1247/1680 train_time:109589ms step_avg:87.88ms
step:1248/1680 train_time:109677ms step_avg:87.88ms
step:1249/1680 train_time:109767ms step_avg:87.88ms
step:1250/1680 train_time:109856ms step_avg:87.88ms
step:1250/1680 val_loss:3.3770 train_time:109946ms step_avg:87.96ms
step:1251/1680 train_time:109964ms step_avg:87.90ms
step:1252/1680 train_time:110038ms step_avg:87.89ms
step:1253/1680 train_time:110130ms step_avg:87.89ms
step:1254/1680 train_time:110220ms step_avg:87.89ms
step:1255/1680 train_time:110308ms step_avg:87.89ms
step:1256/1680 train_time:110396ms step_avg:87.89ms
step:1257/1680 train_time:110484ms step_avg:87.89ms
step:1258/1680 train_time:110572ms step_avg:87.90ms
step:1259/1680 train_time:110661ms step_avg:87.90ms
step:1260/1680 train_time:110749ms step_avg:87.90ms
step:1261/1680 train_time:110837ms step_avg:87.90ms
step:1262/1680 train_time:110927ms step_avg:87.90ms
step:1263/1680 train_time:111018ms step_avg:87.90ms
step:1264/1680 train_time:111109ms step_avg:87.90ms
step:1265/1680 train_time:111198ms step_avg:87.90ms
step:1266/1680 train_time:111286ms step_avg:87.90ms
step:1267/1680 train_time:111375ms step_avg:87.90ms
step:1268/1680 train_time:111463ms step_avg:87.90ms
step:1269/1680 train_time:111552ms step_avg:87.91ms
step:1270/1680 train_time:111640ms step_avg:87.91ms
step:1271/1680 train_time:111729ms step_avg:87.91ms
step:1272/1680 train_time:111817ms step_avg:87.91ms
step:1273/1680 train_time:111906ms step_avg:87.91ms
step:1274/1680 train_time:111997ms step_avg:87.91ms
step:1275/1680 train_time:112087ms step_avg:87.91ms
step:1276/1680 train_time:112177ms step_avg:87.91ms
step:1277/1680 train_time:112265ms step_avg:87.91ms
step:1278/1680 train_time:112355ms step_avg:87.91ms
step:1279/1680 train_time:112443ms step_avg:87.92ms
step:1280/1680 train_time:112532ms step_avg:87.92ms
step:1281/1680 train_time:112620ms step_avg:87.92ms
step:1282/1680 train_time:112709ms step_avg:87.92ms
step:1283/1680 train_time:112798ms step_avg:87.92ms
step:1284/1680 train_time:112887ms step_avg:87.92ms
step:1285/1680 train_time:112977ms step_avg:87.92ms
step:1286/1680 train_time:113066ms step_avg:87.92ms
step:1287/1680 train_time:113155ms step_avg:87.92ms
step:1288/1680 train_time:113245ms step_avg:87.92ms
step:1289/1680 train_time:113335ms step_avg:87.93ms
step:1290/1680 train_time:113425ms step_avg:87.93ms
step:1291/1680 train_time:113514ms step_avg:87.93ms
step:1292/1680 train_time:113602ms step_avg:87.93ms
step:1293/1680 train_time:113690ms step_avg:87.93ms
step:1294/1680 train_time:113778ms step_avg:87.93ms
step:1295/1680 train_time:113867ms step_avg:87.93ms
step:1296/1680 train_time:113956ms step_avg:87.93ms
step:1297/1680 train_time:114046ms step_avg:87.93ms
step:1298/1680 train_time:114135ms step_avg:87.93ms
step:1299/1680 train_time:114224ms step_avg:87.93ms
step:1300/1680 train_time:114313ms step_avg:87.93ms
step:1301/1680 train_time:114401ms step_avg:87.93ms
step:1302/1680 train_time:114490ms step_avg:87.93ms
step:1303/1680 train_time:114579ms step_avg:87.93ms
step:1304/1680 train_time:114669ms step_avg:87.94ms
step:1305/1680 train_time:114758ms step_avg:87.94ms
step:1306/1680 train_time:114846ms step_avg:87.94ms
step:1307/1680 train_time:114936ms step_avg:87.94ms
step:1308/1680 train_time:115024ms step_avg:87.94ms
step:1309/1680 train_time:115113ms step_avg:87.94ms
step:1310/1680 train_time:115203ms step_avg:87.94ms
step:1311/1680 train_time:115293ms step_avg:87.94ms
step:1312/1680 train_time:115381ms step_avg:87.94ms
step:1313/1680 train_time:115470ms step_avg:87.94ms
step:1314/1680 train_time:115559ms step_avg:87.94ms
step:1315/1680 train_time:115648ms step_avg:87.95ms
step:1316/1680 train_time:115737ms step_avg:87.95ms
step:1317/1680 train_time:115825ms step_avg:87.95ms
step:1318/1680 train_time:115914ms step_avg:87.95ms
step:1319/1680 train_time:116003ms step_avg:87.95ms
step:1320/1680 train_time:116092ms step_avg:87.95ms
step:1321/1680 train_time:116181ms step_avg:87.95ms
step:1322/1680 train_time:116271ms step_avg:87.95ms
step:1323/1680 train_time:116361ms step_avg:87.95ms
step:1324/1680 train_time:116451ms step_avg:87.95ms
step:1325/1680 train_time:116541ms step_avg:87.96ms
step:1326/1680 train_time:116630ms step_avg:87.96ms
step:1327/1680 train_time:116718ms step_avg:87.96ms
step:1328/1680 train_time:116807ms step_avg:87.96ms
step:1329/1680 train_time:116896ms step_avg:87.96ms
step:1330/1680 train_time:116985ms step_avg:87.96ms
step:1331/1680 train_time:117075ms step_avg:87.96ms
step:1332/1680 train_time:117165ms step_avg:87.96ms
step:1333/1680 train_time:117253ms step_avg:87.96ms
step:1334/1680 train_time:117343ms step_avg:87.96ms
step:1335/1680 train_time:117432ms step_avg:87.96ms
step:1336/1680 train_time:117521ms step_avg:87.96ms
step:1337/1680 train_time:117611ms step_avg:87.97ms
step:1338/1680 train_time:117700ms step_avg:87.97ms
step:1339/1680 train_time:117789ms step_avg:87.97ms
step:1340/1680 train_time:117877ms step_avg:87.97ms
step:1341/1680 train_time:117966ms step_avg:87.97ms
step:1342/1680 train_time:118055ms step_avg:87.97ms
step:1343/1680 train_time:118144ms step_avg:87.97ms
step:1344/1680 train_time:118234ms step_avg:87.97ms
step:1345/1680 train_time:118322ms step_avg:87.97ms
step:1346/1680 train_time:118411ms step_avg:87.97ms
step:1347/1680 train_time:118500ms step_avg:87.97ms
step:1348/1680 train_time:118589ms step_avg:87.97ms
step:1349/1680 train_time:118678ms step_avg:87.97ms
step:1350/1680 train_time:118767ms step_avg:87.98ms
step:1351/1680 train_time:118855ms step_avg:87.98ms
step:1352/1680 train_time:118944ms step_avg:87.98ms
step:1353/1680 train_time:119034ms step_avg:87.98ms
step:1354/1680 train_time:119123ms step_avg:87.98ms
step:1355/1680 train_time:119212ms step_avg:87.98ms
step:1356/1680 train_time:119301ms step_avg:87.98ms
step:1357/1680 train_time:119391ms step_avg:87.98ms
step:1358/1680 train_time:119479ms step_avg:87.98ms
step:1359/1680 train_time:119568ms step_avg:87.98ms
step:1360/1680 train_time:119658ms step_avg:87.98ms
step:1361/1680 train_time:119747ms step_avg:87.98ms
step:1362/1680 train_time:119835ms step_avg:87.98ms
step:1363/1680 train_time:119924ms step_avg:87.99ms
step:1364/1680 train_time:120012ms step_avg:87.99ms
step:1365/1680 train_time:120101ms step_avg:87.99ms
step:1366/1680 train_time:120190ms step_avg:87.99ms
step:1367/1680 train_time:120279ms step_avg:87.99ms
step:1368/1680 train_time:120369ms step_avg:87.99ms
step:1369/1680 train_time:120458ms step_avg:87.99ms
step:1370/1680 train_time:120548ms step_avg:87.99ms
step:1371/1680 train_time:120637ms step_avg:87.99ms
step:1372/1680 train_time:120727ms step_avg:87.99ms
step:1373/1680 train_time:120816ms step_avg:87.99ms
step:1374/1680 train_time:120904ms step_avg:87.99ms
step:1375/1680 train_time:120993ms step_avg:88.00ms
step:1375/1680 val_loss:3.3421 train_time:121084ms step_avg:88.06ms
step:1376/1680 train_time:121103ms step_avg:88.01ms
step:1377/1680 train_time:121175ms step_avg:88.00ms
step:1378/1680 train_time:121268ms step_avg:88.00ms
step:1379/1680 train_time:121356ms step_avg:88.00ms
step:1380/1680 train_time:121444ms step_avg:88.00ms
step:1381/1680 train_time:121533ms step_avg:88.00ms
step:1382/1680 train_time:121622ms step_avg:88.00ms
step:1383/1680 train_time:121709ms step_avg:88.00ms
step:1384/1680 train_time:121798ms step_avg:88.00ms
step:1385/1680 train_time:121887ms step_avg:88.00ms
step:1386/1680 train_time:121975ms step_avg:88.01ms
step:1387/1680 train_time:122064ms step_avg:88.01ms
step:1388/1680 train_time:122155ms step_avg:88.01ms
step:1389/1680 train_time:122246ms step_avg:88.01ms
step:1390/1680 train_time:122335ms step_avg:88.01ms
step:1391/1680 train_time:122424ms step_avg:88.01ms
step:1392/1680 train_time:122513ms step_avg:88.01ms
step:1393/1680 train_time:122601ms step_avg:88.01ms
step:1394/1680 train_time:122689ms step_avg:88.01ms
step:1395/1680 train_time:122778ms step_avg:88.01ms
step:1396/1680 train_time:122867ms step_avg:88.01ms
step:1397/1680 train_time:122956ms step_avg:88.01ms
step:1398/1680 train_time:123045ms step_avg:88.02ms
step:1399/1680 train_time:123135ms step_avg:88.02ms
step:1400/1680 train_time:123224ms step_avg:88.02ms
step:1401/1680 train_time:123314ms step_avg:88.02ms
step:1402/1680 train_time:123404ms step_avg:88.02ms
step:1403/1680 train_time:123492ms step_avg:88.02ms
step:1404/1680 train_time:123581ms step_avg:88.02ms
step:1405/1680 train_time:123669ms step_avg:88.02ms
step:1406/1680 train_time:123758ms step_avg:88.02ms
step:1407/1680 train_time:123846ms step_avg:88.02ms
step:1408/1680 train_time:123935ms step_avg:88.02ms
step:1409/1680 train_time:124025ms step_avg:88.02ms
step:1410/1680 train_time:124114ms step_avg:88.02ms
step:1411/1680 train_time:124204ms step_avg:88.03ms
step:1412/1680 train_time:124294ms step_avg:88.03ms
step:1413/1680 train_time:124384ms step_avg:88.03ms
step:1414/1680 train_time:124473ms step_avg:88.03ms
step:1415/1680 train_time:124561ms step_avg:88.03ms
step:1416/1680 train_time:124649ms step_avg:88.03ms
step:1417/1680 train_time:124738ms step_avg:88.03ms
step:1418/1680 train_time:124826ms step_avg:88.03ms
step:1419/1680 train_time:124915ms step_avg:88.03ms
step:1420/1680 train_time:125004ms step_avg:88.03ms
step:1421/1680 train_time:125093ms step_avg:88.03ms
step:1422/1680 train_time:125183ms step_avg:88.03ms
step:1423/1680 train_time:125272ms step_avg:88.03ms
step:1424/1680 train_time:125361ms step_avg:88.03ms
step:1425/1680 train_time:125450ms step_avg:88.04ms
step:1426/1680 train_time:125540ms step_avg:88.04ms
step:1427/1680 train_time:125629ms step_avg:88.04ms
step:1428/1680 train_time:125718ms step_avg:88.04ms
step:1429/1680 train_time:125808ms step_avg:88.04ms
step:1430/1680 train_time:125898ms step_avg:88.04ms
step:1431/1680 train_time:125986ms step_avg:88.04ms
step:1432/1680 train_time:126076ms step_avg:88.04ms
step:1433/1680 train_time:126165ms step_avg:88.04ms
step:1434/1680 train_time:126255ms step_avg:88.04ms
step:1435/1680 train_time:126345ms step_avg:88.04ms
step:1436/1680 train_time:126434ms step_avg:88.05ms
step:1437/1680 train_time:126522ms step_avg:88.05ms
step:1438/1680 train_time:126611ms step_avg:88.05ms
step:1439/1680 train_time:126700ms step_avg:88.05ms
step:1440/1680 train_time:126790ms step_avg:88.05ms
step:1441/1680 train_time:126880ms step_avg:88.05ms
step:1442/1680 train_time:126968ms step_avg:88.05ms
step:1443/1680 train_time:127057ms step_avg:88.05ms
step:1444/1680 train_time:127147ms step_avg:88.05ms
step:1445/1680 train_time:127236ms step_avg:88.05ms
step:1446/1680 train_time:127325ms step_avg:88.05ms
step:1447/1680 train_time:127415ms step_avg:88.05ms
step:1448/1680 train_time:127504ms step_avg:88.06ms
step:1449/1680 train_time:127594ms step_avg:88.06ms
step:1450/1680 train_time:127683ms step_avg:88.06ms
step:1451/1680 train_time:127772ms step_avg:88.06ms
step:1452/1680 train_time:127862ms step_avg:88.06ms
step:1453/1680 train_time:127950ms step_avg:88.06ms
step:1454/1680 train_time:128038ms step_avg:88.06ms
step:1455/1680 train_time:128127ms step_avg:88.06ms
step:1456/1680 train_time:128217ms step_avg:88.06ms
step:1457/1680 train_time:128305ms step_avg:88.06ms
step:1458/1680 train_time:128395ms step_avg:88.06ms
step:1459/1680 train_time:128484ms step_avg:88.06ms
step:1460/1680 train_time:128573ms step_avg:88.06ms
step:1461/1680 train_time:128661ms step_avg:88.06ms
step:1462/1680 train_time:128750ms step_avg:88.06ms
step:1463/1680 train_time:128839ms step_avg:88.07ms
step:1464/1680 train_time:128928ms step_avg:88.07ms
step:1465/1680 train_time:129017ms step_avg:88.07ms
step:1466/1680 train_time:129106ms step_avg:88.07ms
step:1467/1680 train_time:129195ms step_avg:88.07ms
step:1468/1680 train_time:129284ms step_avg:88.07ms
step:1469/1680 train_time:129374ms step_avg:88.07ms
step:1470/1680 train_time:129462ms step_avg:88.07ms
step:1471/1680 train_time:129551ms step_avg:88.07ms
step:1472/1680 train_time:129640ms step_avg:88.07ms
step:1473/1680 train_time:129729ms step_avg:88.07ms
step:1474/1680 train_time:129819ms step_avg:88.07ms
step:1475/1680 train_time:129909ms step_avg:88.07ms
step:1476/1680 train_time:129998ms step_avg:88.07ms
step:1477/1680 train_time:130088ms step_avg:88.08ms
step:1478/1680 train_time:130176ms step_avg:88.08ms
step:1479/1680 train_time:130266ms step_avg:88.08ms
step:1480/1680 train_time:130356ms step_avg:88.08ms
step:1481/1680 train_time:130445ms step_avg:88.08ms
step:1482/1680 train_time:130534ms step_avg:88.08ms
step:1483/1680 train_time:130622ms step_avg:88.08ms
step:1484/1680 train_time:130712ms step_avg:88.08ms
step:1485/1680 train_time:130801ms step_avg:88.08ms
step:1486/1680 train_time:130889ms step_avg:88.08ms
step:1487/1680 train_time:130979ms step_avg:88.08ms
step:1488/1680 train_time:131068ms step_avg:88.08ms
step:1489/1680 train_time:131156ms step_avg:88.08ms
step:1490/1680 train_time:131245ms step_avg:88.08ms
step:1491/1680 train_time:131334ms step_avg:88.08ms
step:1492/1680 train_time:131423ms step_avg:88.08ms
step:1493/1680 train_time:131513ms step_avg:88.09ms
step:1494/1680 train_time:131602ms step_avg:88.09ms
step:1495/1680 train_time:131691ms step_avg:88.09ms
step:1496/1680 train_time:131780ms step_avg:88.09ms
step:1497/1680 train_time:131869ms step_avg:88.09ms
step:1498/1680 train_time:131958ms step_avg:88.09ms
step:1499/1680 train_time:132047ms step_avg:88.09ms
step:1500/1680 train_time:132137ms step_avg:88.09ms
step:1500/1680 val_loss:3.3122 train_time:132227ms step_avg:88.15ms
step:1501/1680 train_time:132247ms step_avg:88.11ms
step:1502/1680 train_time:132318ms step_avg:88.09ms
step:1503/1680 train_time:132409ms step_avg:88.10ms
step:1504/1680 train_time:132498ms step_avg:88.10ms
step:1505/1680 train_time:132586ms step_avg:88.10ms
step:1506/1680 train_time:132674ms step_avg:88.10ms
step:1507/1680 train_time:132762ms step_avg:88.10ms
step:1508/1680 train_time:132850ms step_avg:88.10ms
step:1509/1680 train_time:132937ms step_avg:88.10ms
step:1510/1680 train_time:133026ms step_avg:88.10ms
step:1511/1680 train_time:133114ms step_avg:88.10ms
step:1512/1680 train_time:133204ms step_avg:88.10ms
step:1513/1680 train_time:133294ms step_avg:88.10ms
step:1514/1680 train_time:133385ms step_avg:88.10ms
step:1515/1680 train_time:133475ms step_avg:88.10ms
step:1516/1680 train_time:133565ms step_avg:88.10ms
step:1517/1680 train_time:133654ms step_avg:88.10ms
step:1518/1680 train_time:133742ms step_avg:88.10ms
step:1519/1680 train_time:133831ms step_avg:88.10ms
step:1520/1680 train_time:133920ms step_avg:88.11ms
step:1521/1680 train_time:134009ms step_avg:88.11ms
step:1522/1680 train_time:134097ms step_avg:88.11ms
step:1523/1680 train_time:134186ms step_avg:88.11ms
step:1524/1680 train_time:134275ms step_avg:88.11ms
step:1525/1680 train_time:134366ms step_avg:88.11ms
step:1526/1680 train_time:134457ms step_avg:88.11ms
step:1527/1680 train_time:134546ms step_avg:88.11ms
step:1528/1680 train_time:134636ms step_avg:88.11ms
step:1529/1680 train_time:134724ms step_avg:88.11ms
step:1530/1680 train_time:134813ms step_avg:88.11ms
step:1531/1680 train_time:134901ms step_avg:88.11ms
step:1532/1680 train_time:134989ms step_avg:88.11ms
step:1533/1680 train_time:135078ms step_avg:88.11ms
step:1534/1680 train_time:135167ms step_avg:88.11ms
step:1535/1680 train_time:135257ms step_avg:88.12ms
step:1536/1680 train_time:135347ms step_avg:88.12ms
step:1537/1680 train_time:135436ms step_avg:88.12ms
step:1538/1680 train_time:135526ms step_avg:88.12ms
step:1539/1680 train_time:135614ms step_avg:88.12ms
step:1540/1680 train_time:135704ms step_avg:88.12ms
step:1541/1680 train_time:135793ms step_avg:88.12ms
step:1542/1680 train_time:135881ms step_avg:88.12ms
step:1543/1680 train_time:135970ms step_avg:88.12ms
step:1544/1680 train_time:136058ms step_avg:88.12ms
step:1545/1680 train_time:136148ms step_avg:88.12ms
step:1546/1680 train_time:136237ms step_avg:88.12ms
step:1547/1680 train_time:136326ms step_avg:88.12ms
step:1548/1680 train_time:136416ms step_avg:88.12ms
step:1549/1680 train_time:136504ms step_avg:88.12ms
step:1550/1680 train_time:136593ms step_avg:88.12ms
step:1551/1680 train_time:136682ms step_avg:88.13ms
step:1552/1680 train_time:136770ms step_avg:88.13ms
step:1553/1680 train_time:136860ms step_avg:88.13ms
step:1554/1680 train_time:136948ms step_avg:88.13ms
step:1555/1680 train_time:137037ms step_avg:88.13ms
step:1556/1680 train_time:137126ms step_avg:88.13ms
step:1557/1680 train_time:137216ms step_avg:88.13ms
step:1558/1680 train_time:137305ms step_avg:88.13ms
step:1559/1680 train_time:137395ms step_avg:88.13ms
step:1560/1680 train_time:137484ms step_avg:88.13ms
step:1561/1680 train_time:137573ms step_avg:88.13ms
step:1562/1680 train_time:137662ms step_avg:88.13ms
step:1563/1680 train_time:137751ms step_avg:88.13ms
step:1564/1680 train_time:137840ms step_avg:88.13ms
step:1565/1680 train_time:137928ms step_avg:88.13ms
step:1566/1680 train_time:138018ms step_avg:88.13ms
step:1567/1680 train_time:138106ms step_avg:88.13ms
step:1568/1680 train_time:138196ms step_avg:88.14ms
step:1569/1680 train_time:138285ms step_avg:88.14ms
step:1570/1680 train_time:138376ms step_avg:88.14ms
step:1571/1680 train_time:138464ms step_avg:88.14ms
step:1572/1680 train_time:138553ms step_avg:88.14ms
step:1573/1680 train_time:138643ms step_avg:88.14ms
step:1574/1680 train_time:138732ms step_avg:88.14ms
step:1575/1680 train_time:138821ms step_avg:88.14ms
step:1576/1680 train_time:138910ms step_avg:88.14ms
step:1577/1680 train_time:139000ms step_avg:88.14ms
step:1578/1680 train_time:139089ms step_avg:88.14ms
step:1579/1680 train_time:139178ms step_avg:88.14ms
step:1580/1680 train_time:139267ms step_avg:88.14ms
step:1581/1680 train_time:139357ms step_avg:88.14ms
step:1582/1680 train_time:139446ms step_avg:88.15ms
step:1583/1680 train_time:139536ms step_avg:88.15ms
step:1584/1680 train_time:139625ms step_avg:88.15ms
step:1585/1680 train_time:139714ms step_avg:88.15ms
step:1586/1680 train_time:139803ms step_avg:88.15ms
step:1587/1680 train_time:139892ms step_avg:88.15ms
step:1588/1680 train_time:139981ms step_avg:88.15ms
step:1589/1680 train_time:140070ms step_avg:88.15ms
step:1590/1680 train_time:140159ms step_avg:88.15ms
step:1591/1680 train_time:140249ms step_avg:88.15ms
step:1592/1680 train_time:140338ms step_avg:88.15ms
step:1593/1680 train_time:140428ms step_avg:88.15ms
step:1594/1680 train_time:140517ms step_avg:88.15ms
step:1595/1680 train_time:140606ms step_avg:88.15ms
step:1596/1680 train_time:140695ms step_avg:88.15ms
step:1597/1680 train_time:140784ms step_avg:88.16ms
step:1598/1680 train_time:140873ms step_avg:88.16ms
step:1599/1680 train_time:140962ms step_avg:88.16ms
step:1600/1680 train_time:141052ms step_avg:88.16ms
step:1601/1680 train_time:141141ms step_avg:88.16ms
step:1602/1680 train_time:141230ms step_avg:88.16ms
step:1603/1680 train_time:141319ms step_avg:88.16ms
step:1604/1680 train_time:141408ms step_avg:88.16ms
step:1605/1680 train_time:141497ms step_avg:88.16ms
step:1606/1680 train_time:141586ms step_avg:88.16ms
step:1607/1680 train_time:141675ms step_avg:88.16ms
step:1608/1680 train_time:141764ms step_avg:88.16ms
step:1609/1680 train_time:141853ms step_avg:88.16ms
step:1610/1680 train_time:141943ms step_avg:88.16ms
step:1611/1680 train_time:142032ms step_avg:88.16ms
step:1612/1680 train_time:142121ms step_avg:88.16ms
step:1613/1680 train_time:142210ms step_avg:88.17ms
step:1614/1680 train_time:142300ms step_avg:88.17ms
step:1615/1680 train_time:142389ms step_avg:88.17ms
step:1616/1680 train_time:142478ms step_avg:88.17ms
step:1617/1680 train_time:142567ms step_avg:88.17ms
step:1618/1680 train_time:142656ms step_avg:88.17ms
step:1619/1680 train_time:142745ms step_avg:88.17ms
step:1620/1680 train_time:142834ms step_avg:88.17ms
step:1621/1680 train_time:142923ms step_avg:88.17ms
step:1622/1680 train_time:143011ms step_avg:88.17ms
step:1623/1680 train_time:143101ms step_avg:88.17ms
step:1624/1680 train_time:143189ms step_avg:88.17ms
step:1625/1680 train_time:143278ms step_avg:88.17ms
step:1625/1680 val_loss:3.2883 train_time:143368ms step_avg:88.23ms
step:1626/1680 train_time:143387ms step_avg:88.18ms
step:1627/1680 train_time:143461ms step_avg:88.17ms
step:1628/1680 train_time:143551ms step_avg:88.18ms
step:1629/1680 train_time:143641ms step_avg:88.18ms
step:1630/1680 train_time:143729ms step_avg:88.18ms
step:1631/1680 train_time:143818ms step_avg:88.18ms
step:1632/1680 train_time:143905ms step_avg:88.18ms
step:1633/1680 train_time:143993ms step_avg:88.18ms
step:1634/1680 train_time:144082ms step_avg:88.18ms
step:1635/1680 train_time:144170ms step_avg:88.18ms
step:1636/1680 train_time:144259ms step_avg:88.18ms
step:1637/1680 train_time:144350ms step_avg:88.18ms
step:1638/1680 train_time:144442ms step_avg:88.18ms
step:1639/1680 train_time:144532ms step_avg:88.18ms
step:1640/1680 train_time:144621ms step_avg:88.18ms
step:1641/1680 train_time:144711ms step_avg:88.18ms
step:1642/1680 train_time:144800ms step_avg:88.19ms
step:1643/1680 train_time:144889ms step_avg:88.19ms
step:1644/1680 train_time:144977ms step_avg:88.19ms
step:1645/1680 train_time:145066ms step_avg:88.19ms
step:1646/1680 train_time:145154ms step_avg:88.19ms
step:1647/1680 train_time:145242ms step_avg:88.19ms
step:1648/1680 train_time:145332ms step_avg:88.19ms
step:1649/1680 train_time:145422ms step_avg:88.19ms
step:1650/1680 train_time:145512ms step_avg:88.19ms
step:1651/1680 train_time:145602ms step_avg:88.19ms
step:1652/1680 train_time:145691ms step_avg:88.19ms
step:1653/1680 train_time:145780ms step_avg:88.19ms
step:1654/1680 train_time:145869ms step_avg:88.19ms
step:1655/1680 train_time:145957ms step_avg:88.19ms
step:1656/1680 train_time:146047ms step_avg:88.19ms
step:1657/1680 train_time:146135ms step_avg:88.19ms
step:1658/1680 train_time:146223ms step_avg:88.19ms
step:1659/1680 train_time:146312ms step_avg:88.19ms
step:1660/1680 train_time:146401ms step_avg:88.19ms
step:1661/1680 train_time:146491ms step_avg:88.19ms
step:1662/1680 train_time:146580ms step_avg:88.20ms
step:1663/1680 train_time:146670ms step_avg:88.20ms
step:1664/1680 train_time:146758ms step_avg:88.20ms
step:1665/1680 train_time:146847ms step_avg:88.20ms
step:1666/1680 train_time:146936ms step_avg:88.20ms
step:1667/1680 train_time:147024ms step_avg:88.20ms
step:1668/1680 train_time:147113ms step_avg:88.20ms
step:1669/1680 train_time:147202ms step_avg:88.20ms
step:1670/1680 train_time:147291ms step_avg:88.20ms
step:1671/1680 train_time:147381ms step_avg:88.20ms
step:1672/1680 train_time:147470ms step_avg:88.20ms
step:1673/1680 train_time:147559ms step_avg:88.20ms
step:1674/1680 train_time:147648ms step_avg:88.20ms
step:1675/1680 train_time:147738ms step_avg:88.20ms
step:1676/1680 train_time:147827ms step_avg:88.20ms
step:1677/1680 train_time:147916ms step_avg:88.20ms
step:1678/1680 train_time:148005ms step_avg:88.20ms
step:1679/1680 train_time:148094ms step_avg:88.20ms
step:1680/1680 train_time:148182ms step_avg:88.20ms
step:1680/1680 val_loss:3.2774 train_time:148273ms step_avg:88.26ms
peak memory allocated: 30760 MiB reserved: 45774 MiB

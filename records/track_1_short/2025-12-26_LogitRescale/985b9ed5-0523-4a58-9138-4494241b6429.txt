import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits+5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1840  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec 26 23:22:18 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     74142      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     74143      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     74144      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     74145      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     74146      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     74147      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     74148      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     74149      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 613, 614, 615, 1226, 1227, 1228, 1839, 1840, 1841] for warmup
Resetting Model
step:0/1880 val_loss:10.8279 train_time:0ms step_avg:0.04ms
step:1/1880 train_time:68ms step_avg:67.80ms
step:2/1880 train_time:91ms step_avg:45.69ms
step:3/1880 train_time:117ms step_avg:38.96ms
step:4/1880 train_time:151ms step_avg:37.70ms
step:5/1880 train_time:184ms step_avg:36.89ms
step:6/1880 train_time:282ms step_avg:47.06ms
step:7/1880 train_time:395ms step_avg:56.40ms
step:8/1880 train_time:429ms step_avg:53.58ms
step:9/1880 train_time:462ms step_avg:51.38ms
step:10/1880 train_time:496ms step_avg:49.63ms
step:11/1880 train_time:530ms step_avg:48.19ms
step:12/1880 train_time:564ms step_avg:47.02ms
step:13/1880 train_time:598ms step_avg:45.99ms
step:14/1880 train_time:632ms step_avg:45.14ms
step:15/1880 train_time:666ms step_avg:44.39ms
step:16/1880 train_time:700ms step_avg:43.73ms
step:17/1880 train_time:734ms step_avg:43.17ms
step:18/1880 train_time:768ms step_avg:42.68ms
step:19/1880 train_time:802ms step_avg:42.21ms
step:20/1880 train_time:836ms step_avg:41.80ms
step:21/1880 train_time:870ms step_avg:41.45ms
step:22/1880 train_time:905ms step_avg:41.11ms
step:23/1880 train_time:938ms step_avg:40.79ms
step:24/1880 train_time:972ms step_avg:40.51ms
step:25/1880 train_time:1006ms step_avg:40.24ms
step:26/1880 train_time:1040ms step_avg:40.00ms
step:27/1880 train_time:1074ms step_avg:39.77ms
step:28/1880 train_time:1108ms step_avg:39.57ms
step:29/1880 train_time:1142ms step_avg:39.37ms
step:30/1880 train_time:1176ms step_avg:39.20ms
step:31/1880 train_time:1210ms step_avg:39.03ms
step:32/1880 train_time:1244ms step_avg:38.87ms
step:33/1880 train_time:1278ms step_avg:38.72ms
step:34/1880 train_time:1313ms step_avg:38.61ms
step:35/1880 train_time:1347ms step_avg:38.49ms
step:36/1880 train_time:1382ms step_avg:38.38ms
step:37/1880 train_time:1416ms step_avg:38.26ms
step:38/1880 train_time:1450ms step_avg:38.16ms
step:39/1880 train_time:1484ms step_avg:38.05ms
step:40/1880 train_time:1518ms step_avg:37.96ms
step:41/1880 train_time:1553ms step_avg:37.87ms
step:42/1880 train_time:1587ms step_avg:37.79ms
step:43/1880 train_time:1621ms step_avg:37.70ms
step:44/1880 train_time:1655ms step_avg:37.62ms
step:45/1880 train_time:1689ms step_avg:37.54ms
step:46/1880 train_time:1724ms step_avg:37.48ms
step:47/1880 train_time:1758ms step_avg:37.40ms
step:48/1880 train_time:1792ms step_avg:37.34ms
step:49/1880 train_time:1826ms step_avg:37.27ms
step:50/1880 train_time:1860ms step_avg:37.21ms
step:51/1880 train_time:1894ms step_avg:37.14ms
step:52/1880 train_time:1929ms step_avg:37.09ms
step:53/1880 train_time:1962ms step_avg:37.02ms
step:54/1880 train_time:1996ms step_avg:36.97ms
step:55/1880 train_time:2030ms step_avg:36.91ms
step:56/1880 train_time:2064ms step_avg:36.86ms
step:57/1880 train_time:2098ms step_avg:36.80ms
step:58/1880 train_time:2132ms step_avg:36.75ms
step:59/1880 train_time:2166ms step_avg:36.71ms
step:60/1880 train_time:2200ms step_avg:36.67ms
step:61/1880 train_time:2234ms step_avg:36.62ms
step:62/1880 train_time:2268ms step_avg:36.58ms
step:63/1880 train_time:2302ms step_avg:36.54ms
step:64/1880 train_time:2336ms step_avg:36.50ms
step:65/1880 train_time:2370ms step_avg:36.46ms
step:66/1880 train_time:2404ms step_avg:36.43ms
step:67/1880 train_time:2438ms step_avg:36.39ms
step:68/1880 train_time:2473ms step_avg:36.36ms
step:69/1880 train_time:2507ms step_avg:36.33ms
step:70/1880 train_time:2541ms step_avg:36.30ms
step:71/1880 train_time:2575ms step_avg:36.27ms
step:72/1880 train_time:2610ms step_avg:36.25ms
step:73/1880 train_time:2644ms step_avg:36.22ms
step:74/1880 train_time:2678ms step_avg:36.19ms
step:75/1880 train_time:2712ms step_avg:36.17ms
step:76/1880 train_time:2747ms step_avg:36.14ms
step:77/1880 train_time:2781ms step_avg:36.11ms
step:78/1880 train_time:2815ms step_avg:36.09ms
step:79/1880 train_time:2849ms step_avg:36.07ms
step:80/1880 train_time:2884ms step_avg:36.05ms
step:81/1880 train_time:2917ms step_avg:36.02ms
step:82/1880 train_time:2952ms step_avg:36.00ms
step:83/1880 train_time:2986ms step_avg:35.98ms
step:84/1880 train_time:3020ms step_avg:35.96ms
step:85/1880 train_time:3054ms step_avg:35.93ms
step:86/1880 train_time:3088ms step_avg:35.91ms
step:87/1880 train_time:3122ms step_avg:35.88ms
step:88/1880 train_time:3156ms step_avg:35.86ms
step:89/1880 train_time:3190ms step_avg:35.84ms
step:90/1880 train_time:3224ms step_avg:35.82ms
step:91/1880 train_time:3258ms step_avg:35.80ms
step:92/1880 train_time:3292ms step_avg:35.78ms
step:93/1880 train_time:3326ms step_avg:35.76ms
step:94/1880 train_time:3360ms step_avg:35.75ms
step:95/1880 train_time:3394ms step_avg:35.73ms
step:96/1880 train_time:3428ms step_avg:35.71ms
step:97/1880 train_time:3462ms step_avg:35.69ms
step:98/1880 train_time:3496ms step_avg:35.68ms
step:99/1880 train_time:3530ms step_avg:35.66ms
step:100/1880 train_time:3564ms step_avg:35.64ms
step:101/1880 train_time:3599ms step_avg:35.64ms
step:102/1880 train_time:3632ms step_avg:35.61ms
step:103/1880 train_time:3666ms step_avg:35.60ms
step:104/1880 train_time:3701ms step_avg:35.58ms
step:105/1880 train_time:3734ms step_avg:35.57ms
step:106/1880 train_time:3769ms step_avg:35.55ms
step:107/1880 train_time:3803ms step_avg:35.54ms
step:108/1880 train_time:3837ms step_avg:35.53ms
step:109/1880 train_time:3870ms step_avg:35.51ms
step:110/1880 train_time:3905ms step_avg:35.50ms
step:111/1880 train_time:3939ms step_avg:35.48ms
step:112/1880 train_time:3973ms step_avg:35.47ms
step:113/1880 train_time:4006ms step_avg:35.46ms
step:114/1880 train_time:4041ms step_avg:35.44ms
step:115/1880 train_time:4075ms step_avg:35.43ms
step:116/1880 train_time:4109ms step_avg:35.42ms
step:117/1880 train_time:4143ms step_avg:35.41ms
step:118/1880 train_time:4177ms step_avg:35.40ms
step:119/1880 train_time:4211ms step_avg:35.38ms
step:120/1880 train_time:4245ms step_avg:35.37ms
step:121/1880 train_time:4279ms step_avg:35.36ms
step:122/1880 train_time:4313ms step_avg:35.35ms
step:123/1880 train_time:4347ms step_avg:35.34ms
step:124/1880 train_time:4381ms step_avg:35.33ms
step:125/1880 train_time:4414ms step_avg:35.32ms
step:126/1880 train_time:4449ms step_avg:35.31ms
step:127/1880 train_time:4482ms step_avg:35.29ms
step:128/1880 train_time:4516ms step_avg:35.29ms
step:129/1880 train_time:4550ms step_avg:35.27ms
step:130/1880 train_time:4585ms step_avg:35.27ms
step:131/1880 train_time:4618ms step_avg:35.26ms
step:132/1880 train_time:4653ms step_avg:35.25ms
step:133/1880 train_time:4687ms step_avg:35.24ms
step:134/1880 train_time:4721ms step_avg:35.23ms
step:135/1880 train_time:4755ms step_avg:35.22ms
step:136/1880 train_time:4789ms step_avg:35.21ms
step:137/1880 train_time:4823ms step_avg:35.20ms
step:138/1880 train_time:4857ms step_avg:35.19ms
step:139/1880 train_time:4891ms step_avg:35.18ms
step:140/1880 train_time:4925ms step_avg:35.18ms
step:141/1880 train_time:4959ms step_avg:35.17ms
step:142/1880 train_time:4993ms step_avg:35.16ms
step:143/1880 train_time:5027ms step_avg:35.15ms
step:144/1880 train_time:5061ms step_avg:35.15ms
step:145/1880 train_time:5095ms step_avg:35.14ms
step:146/1880 train_time:5129ms step_avg:35.13ms
step:147/1880 train_time:5163ms step_avg:35.12ms
step:148/1880 train_time:5197ms step_avg:35.11ms
step:149/1880 train_time:5231ms step_avg:35.11ms
step:150/1880 train_time:5266ms step_avg:35.11ms
step:151/1880 train_time:5300ms step_avg:35.10ms
step:152/1880 train_time:5334ms step_avg:35.09ms
step:153/1880 train_time:5368ms step_avg:35.08ms
step:154/1880 train_time:5402ms step_avg:35.08ms
step:155/1880 train_time:5436ms step_avg:35.07ms
step:156/1880 train_time:5470ms step_avg:35.06ms
step:157/1880 train_time:5504ms step_avg:35.06ms
step:158/1880 train_time:5538ms step_avg:35.05ms
step:159/1880 train_time:5572ms step_avg:35.04ms
step:160/1880 train_time:5606ms step_avg:35.04ms
step:161/1880 train_time:5640ms step_avg:35.03ms
step:162/1880 train_time:5674ms step_avg:35.02ms
step:163/1880 train_time:5708ms step_avg:35.02ms
step:164/1880 train_time:5742ms step_avg:35.01ms
step:165/1880 train_time:5776ms step_avg:35.00ms
step:166/1880 train_time:5810ms step_avg:35.00ms
step:167/1880 train_time:5844ms step_avg:34.99ms
step:168/1880 train_time:5878ms step_avg:34.99ms
step:169/1880 train_time:5912ms step_avg:34.99ms
step:170/1880 train_time:5947ms step_avg:34.98ms
step:171/1880 train_time:5981ms step_avg:34.97ms
step:172/1880 train_time:6015ms step_avg:34.97ms
step:173/1880 train_time:6048ms step_avg:34.96ms
step:174/1880 train_time:6082ms step_avg:34.96ms
step:175/1880 train_time:6116ms step_avg:34.95ms
step:176/1880 train_time:6150ms step_avg:34.94ms
step:177/1880 train_time:6184ms step_avg:34.94ms
step:178/1880 train_time:6218ms step_avg:34.93ms
step:179/1880 train_time:6252ms step_avg:34.93ms
step:180/1880 train_time:6286ms step_avg:34.92ms
step:181/1880 train_time:6320ms step_avg:34.92ms
step:182/1880 train_time:6354ms step_avg:34.91ms
step:183/1880 train_time:6388ms step_avg:34.91ms
step:184/1880 train_time:6422ms step_avg:34.90ms
step:185/1880 train_time:6456ms step_avg:34.90ms
step:186/1880 train_time:6490ms step_avg:34.89ms
step:187/1880 train_time:6524ms step_avg:34.89ms
step:188/1880 train_time:6558ms step_avg:34.88ms
step:189/1880 train_time:6591ms step_avg:34.88ms
step:190/1880 train_time:6626ms step_avg:34.87ms
step:191/1880 train_time:6659ms step_avg:34.87ms
step:192/1880 train_time:6695ms step_avg:34.87ms
step:193/1880 train_time:6727ms step_avg:34.85ms
step:194/1880 train_time:6761ms step_avg:34.85ms
step:195/1880 train_time:6795ms step_avg:34.84ms
step:196/1880 train_time:6829ms step_avg:34.84ms
step:197/1880 train_time:6863ms step_avg:34.84ms
step:198/1880 train_time:6897ms step_avg:34.83ms
step:199/1880 train_time:6931ms step_avg:34.83ms
step:200/1880 train_time:6965ms step_avg:34.82ms
step:201/1880 train_time:6999ms step_avg:34.82ms
step:202/1880 train_time:7033ms step_avg:34.81ms
step:203/1880 train_time:7067ms step_avg:34.81ms
step:204/1880 train_time:7101ms step_avg:34.81ms
step:205/1880 train_time:7135ms step_avg:34.80ms
step:206/1880 train_time:7169ms step_avg:34.80ms
step:207/1880 train_time:7203ms step_avg:34.80ms
step:208/1880 train_time:7237ms step_avg:34.79ms
step:209/1880 train_time:7271ms step_avg:34.79ms
step:210/1880 train_time:7305ms step_avg:34.78ms
step:211/1880 train_time:7338ms step_avg:34.78ms
step:212/1880 train_time:7373ms step_avg:34.78ms
step:213/1880 train_time:7406ms step_avg:34.77ms
step:214/1880 train_time:7440ms step_avg:34.77ms
step:215/1880 train_time:7474ms step_avg:34.76ms
step:216/1880 train_time:7509ms step_avg:34.76ms
step:217/1880 train_time:7543ms step_avg:34.76ms
step:218/1880 train_time:7577ms step_avg:34.76ms
step:219/1880 train_time:7611ms step_avg:34.75ms
step:220/1880 train_time:7646ms step_avg:34.75ms
step:221/1880 train_time:7679ms step_avg:34.75ms
step:222/1880 train_time:7714ms step_avg:34.75ms
step:223/1880 train_time:7747ms step_avg:34.74ms
step:224/1880 train_time:7781ms step_avg:34.74ms
step:225/1880 train_time:7815ms step_avg:34.73ms
step:226/1880 train_time:7849ms step_avg:34.73ms
step:227/1880 train_time:7883ms step_avg:34.73ms
step:228/1880 train_time:7917ms step_avg:34.72ms
step:229/1880 train_time:7951ms step_avg:34.72ms
step:230/1880 train_time:7985ms step_avg:34.72ms
step:231/1880 train_time:8019ms step_avg:34.71ms
step:232/1880 train_time:8053ms step_avg:34.71ms
step:233/1880 train_time:8087ms step_avg:34.71ms
step:234/1880 train_time:8121ms step_avg:34.71ms
step:235/1880 train_time:8155ms step_avg:34.70ms
step:236/1880 train_time:8189ms step_avg:34.70ms
step:237/1880 train_time:8223ms step_avg:34.69ms
step:238/1880 train_time:8257ms step_avg:34.69ms
step:239/1880 train_time:8290ms step_avg:34.69ms
step:240/1880 train_time:8325ms step_avg:34.69ms
step:241/1880 train_time:8358ms step_avg:34.68ms
step:242/1880 train_time:8393ms step_avg:34.68ms
step:243/1880 train_time:8427ms step_avg:34.68ms
step:244/1880 train_time:8461ms step_avg:34.67ms
step:245/1880 train_time:8495ms step_avg:34.67ms
step:246/1880 train_time:8529ms step_avg:34.67ms
step:247/1880 train_time:8563ms step_avg:34.67ms
step:248/1880 train_time:8597ms step_avg:34.66ms
step:249/1880 train_time:8631ms step_avg:34.66ms
step:250/1880 train_time:8665ms step_avg:34.66ms
step:250/1880 val_loss:4.6324 train_time:8702ms step_avg:34.81ms
step:251/1880 train_time:8722ms step_avg:34.75ms
step:252/1880 train_time:8740ms step_avg:34.68ms
step:253/1880 train_time:8770ms step_avg:34.67ms
step:254/1880 train_time:8806ms step_avg:34.67ms
step:255/1880 train_time:8841ms step_avg:34.67ms
step:256/1880 train_time:8875ms step_avg:34.67ms
step:257/1880 train_time:8909ms step_avg:34.67ms
step:258/1880 train_time:8943ms step_avg:34.66ms
step:259/1880 train_time:8977ms step_avg:34.66ms
step:260/1880 train_time:9011ms step_avg:34.66ms
step:261/1880 train_time:9045ms step_avg:34.65ms
step:262/1880 train_time:9079ms step_avg:34.65ms
step:263/1880 train_time:9112ms step_avg:34.65ms
step:264/1880 train_time:9146ms step_avg:34.65ms
step:265/1880 train_time:9180ms step_avg:34.64ms
step:266/1880 train_time:9214ms step_avg:34.64ms
step:267/1880 train_time:9248ms step_avg:34.64ms
step:268/1880 train_time:9282ms step_avg:34.63ms
step:269/1880 train_time:9316ms step_avg:34.63ms
step:270/1880 train_time:9352ms step_avg:34.64ms
step:271/1880 train_time:9383ms step_avg:34.62ms
step:272/1880 train_time:9417ms step_avg:34.62ms
step:273/1880 train_time:9451ms step_avg:34.62ms
step:274/1880 train_time:9484ms step_avg:34.61ms
step:275/1880 train_time:9518ms step_avg:34.61ms
step:276/1880 train_time:9552ms step_avg:34.61ms
step:277/1880 train_time:9586ms step_avg:34.61ms
step:278/1880 train_time:9620ms step_avg:34.60ms
step:279/1880 train_time:9654ms step_avg:34.60ms
step:280/1880 train_time:9688ms step_avg:34.60ms
step:281/1880 train_time:9723ms step_avg:34.60ms
step:282/1880 train_time:9757ms step_avg:34.60ms
step:283/1880 train_time:9791ms step_avg:34.60ms
step:284/1880 train_time:9826ms step_avg:34.60ms
step:285/1880 train_time:9860ms step_avg:34.60ms
step:286/1880 train_time:9894ms step_avg:34.59ms
step:287/1880 train_time:9928ms step_avg:34.59ms
step:288/1880 train_time:9962ms step_avg:34.59ms
step:289/1880 train_time:9996ms step_avg:34.59ms
step:290/1880 train_time:10030ms step_avg:34.59ms
step:291/1880 train_time:10064ms step_avg:34.58ms
step:292/1880 train_time:10098ms step_avg:34.58ms
step:293/1880 train_time:10132ms step_avg:34.58ms
step:294/1880 train_time:10166ms step_avg:34.58ms
step:295/1880 train_time:10199ms step_avg:34.57ms
step:296/1880 train_time:10233ms step_avg:34.57ms
step:297/1880 train_time:10267ms step_avg:34.57ms
step:298/1880 train_time:10301ms step_avg:34.57ms
step:299/1880 train_time:10335ms step_avg:34.56ms
step:300/1880 train_time:10369ms step_avg:34.56ms
step:301/1880 train_time:10402ms step_avg:34.56ms
step:302/1880 train_time:10436ms step_avg:34.56ms
step:303/1880 train_time:10470ms step_avg:34.55ms
step:304/1880 train_time:10504ms step_avg:34.55ms
step:305/1880 train_time:10537ms step_avg:34.55ms
step:306/1880 train_time:10571ms step_avg:34.55ms
step:307/1880 train_time:10605ms step_avg:34.54ms
step:308/1880 train_time:10639ms step_avg:34.54ms
step:309/1880 train_time:10673ms step_avg:34.54ms
step:310/1880 train_time:10707ms step_avg:34.54ms
step:311/1880 train_time:10741ms step_avg:34.54ms
step:312/1880 train_time:10775ms step_avg:34.53ms
step:313/1880 train_time:10809ms step_avg:34.53ms
step:314/1880 train_time:10843ms step_avg:34.53ms
step:315/1880 train_time:10877ms step_avg:34.53ms
step:316/1880 train_time:10911ms step_avg:34.53ms
step:317/1880 train_time:10944ms step_avg:34.53ms
step:318/1880 train_time:10979ms step_avg:34.52ms
step:319/1880 train_time:11013ms step_avg:34.52ms
step:320/1880 train_time:11047ms step_avg:34.52ms
step:321/1880 train_time:11081ms step_avg:34.52ms
step:322/1880 train_time:11115ms step_avg:34.52ms
step:323/1880 train_time:11148ms step_avg:34.51ms
step:324/1880 train_time:11182ms step_avg:34.51ms
step:325/1880 train_time:11216ms step_avg:34.51ms
step:326/1880 train_time:11252ms step_avg:34.52ms
step:327/1880 train_time:11284ms step_avg:34.51ms
step:328/1880 train_time:11319ms step_avg:34.51ms
step:329/1880 train_time:11353ms step_avg:34.51ms
step:330/1880 train_time:11387ms step_avg:34.51ms
step:331/1880 train_time:11420ms step_avg:34.50ms
step:332/1880 train_time:11454ms step_avg:34.50ms
step:333/1880 train_time:11488ms step_avg:34.50ms
step:334/1880 train_time:11522ms step_avg:34.50ms
step:335/1880 train_time:11556ms step_avg:34.50ms
step:336/1880 train_time:11590ms step_avg:34.49ms
step:337/1880 train_time:11624ms step_avg:34.49ms
step:338/1880 train_time:11658ms step_avg:34.49ms
step:339/1880 train_time:11692ms step_avg:34.49ms
step:340/1880 train_time:11726ms step_avg:34.49ms
step:341/1880 train_time:11760ms step_avg:34.49ms
step:342/1880 train_time:11794ms step_avg:34.49ms
step:343/1880 train_time:11828ms step_avg:34.48ms
step:344/1880 train_time:11862ms step_avg:34.48ms
step:345/1880 train_time:11896ms step_avg:34.48ms
step:346/1880 train_time:11930ms step_avg:34.48ms
step:347/1880 train_time:11964ms step_avg:34.48ms
step:348/1880 train_time:11998ms step_avg:34.48ms
step:349/1880 train_time:12032ms step_avg:34.48ms
step:350/1880 train_time:12067ms step_avg:34.48ms
step:351/1880 train_time:12101ms step_avg:34.47ms
step:352/1880 train_time:12135ms step_avg:34.47ms
step:353/1880 train_time:12168ms step_avg:34.47ms
step:354/1880 train_time:12203ms step_avg:34.47ms
step:355/1880 train_time:12237ms step_avg:34.47ms
step:356/1880 train_time:12271ms step_avg:34.47ms
step:357/1880 train_time:12306ms step_avg:34.47ms
step:358/1880 train_time:12340ms step_avg:34.47ms
step:359/1880 train_time:12373ms step_avg:34.47ms
step:360/1880 train_time:12407ms step_avg:34.46ms
step:361/1880 train_time:12441ms step_avg:34.46ms
step:362/1880 train_time:12475ms step_avg:34.46ms
step:363/1880 train_time:12509ms step_avg:34.46ms
step:364/1880 train_time:12543ms step_avg:34.46ms
step:365/1880 train_time:12576ms step_avg:34.46ms
step:366/1880 train_time:12611ms step_avg:34.46ms
step:367/1880 train_time:12645ms step_avg:34.45ms
step:368/1880 train_time:12679ms step_avg:34.45ms
step:369/1880 train_time:12713ms step_avg:34.45ms
step:370/1880 train_time:12747ms step_avg:34.45ms
step:371/1880 train_time:12781ms step_avg:34.45ms
step:372/1880 train_time:12815ms step_avg:34.45ms
step:373/1880 train_time:12849ms step_avg:34.45ms
step:374/1880 train_time:12883ms step_avg:34.45ms
step:375/1880 train_time:12917ms step_avg:34.44ms
step:376/1880 train_time:12951ms step_avg:34.44ms
step:377/1880 train_time:12985ms step_avg:34.44ms
step:378/1880 train_time:13019ms step_avg:34.44ms
step:379/1880 train_time:13053ms step_avg:34.44ms
step:380/1880 train_time:13087ms step_avg:34.44ms
step:381/1880 train_time:13122ms step_avg:34.44ms
step:382/1880 train_time:13156ms step_avg:34.44ms
step:383/1880 train_time:13189ms step_avg:34.44ms
step:384/1880 train_time:13224ms step_avg:34.44ms
step:385/1880 train_time:13258ms step_avg:34.44ms
step:386/1880 train_time:13292ms step_avg:34.44ms
step:387/1880 train_time:13326ms step_avg:34.43ms
step:388/1880 train_time:13360ms step_avg:34.43ms
step:389/1880 train_time:13394ms step_avg:34.43ms
step:390/1880 train_time:13428ms step_avg:34.43ms
step:391/1880 train_time:13461ms step_avg:34.43ms
step:392/1880 train_time:13495ms step_avg:34.43ms
step:393/1880 train_time:13529ms step_avg:34.43ms
step:394/1880 train_time:13563ms step_avg:34.42ms
step:395/1880 train_time:13597ms step_avg:34.42ms
step:396/1880 train_time:13631ms step_avg:34.42ms
step:397/1880 train_time:13666ms step_avg:34.42ms
step:398/1880 train_time:13700ms step_avg:34.42ms
step:399/1880 train_time:13733ms step_avg:34.42ms
step:400/1880 train_time:13767ms step_avg:34.42ms
step:401/1880 train_time:13801ms step_avg:34.42ms
step:402/1880 train_time:13835ms step_avg:34.42ms
step:403/1880 train_time:13869ms step_avg:34.41ms
step:404/1880 train_time:13903ms step_avg:34.41ms
step:405/1880 train_time:13937ms step_avg:34.41ms
step:406/1880 train_time:13972ms step_avg:34.41ms
step:407/1880 train_time:14006ms step_avg:34.41ms
step:408/1880 train_time:14040ms step_avg:34.41ms
step:409/1880 train_time:14073ms step_avg:34.41ms
step:410/1880 train_time:14107ms step_avg:34.41ms
step:411/1880 train_time:14141ms step_avg:34.41ms
step:412/1880 train_time:14175ms step_avg:34.41ms
step:413/1880 train_time:14209ms step_avg:34.40ms
step:414/1880 train_time:14243ms step_avg:34.40ms
step:415/1880 train_time:14277ms step_avg:34.40ms
step:416/1880 train_time:14311ms step_avg:34.40ms
step:417/1880 train_time:14345ms step_avg:34.40ms
step:418/1880 train_time:14380ms step_avg:34.40ms
step:419/1880 train_time:14413ms step_avg:34.40ms
step:420/1880 train_time:14447ms step_avg:34.40ms
step:421/1880 train_time:14481ms step_avg:34.40ms
step:422/1880 train_time:14515ms step_avg:34.40ms
step:423/1880 train_time:14549ms step_avg:34.39ms
step:424/1880 train_time:14583ms step_avg:34.39ms
step:425/1880 train_time:14617ms step_avg:34.39ms
step:426/1880 train_time:14651ms step_avg:34.39ms
step:427/1880 train_time:14685ms step_avg:34.39ms
step:428/1880 train_time:14720ms step_avg:34.39ms
step:429/1880 train_time:14753ms step_avg:34.39ms
step:430/1880 train_time:14788ms step_avg:34.39ms
step:431/1880 train_time:14821ms step_avg:34.39ms
step:432/1880 train_time:14855ms step_avg:34.39ms
step:433/1880 train_time:14889ms step_avg:34.39ms
step:434/1880 train_time:14923ms step_avg:34.38ms
step:435/1880 train_time:14957ms step_avg:34.38ms
step:436/1880 train_time:14991ms step_avg:34.38ms
step:437/1880 train_time:15025ms step_avg:34.38ms
step:438/1880 train_time:15059ms step_avg:34.38ms
step:439/1880 train_time:15093ms step_avg:34.38ms
step:440/1880 train_time:15127ms step_avg:34.38ms
step:441/1880 train_time:15161ms step_avg:34.38ms
step:442/1880 train_time:15195ms step_avg:34.38ms
step:443/1880 train_time:15229ms step_avg:34.38ms
step:444/1880 train_time:15263ms step_avg:34.38ms
step:445/1880 train_time:15296ms step_avg:34.37ms
step:446/1880 train_time:15331ms step_avg:34.37ms
step:447/1880 train_time:15364ms step_avg:34.37ms
step:448/1880 train_time:15399ms step_avg:34.37ms
step:449/1880 train_time:15432ms step_avg:34.37ms
step:450/1880 train_time:15466ms step_avg:34.37ms
step:451/1880 train_time:15500ms step_avg:34.37ms
step:452/1880 train_time:15534ms step_avg:34.37ms
step:453/1880 train_time:15568ms step_avg:34.37ms
step:454/1880 train_time:15602ms step_avg:34.37ms
step:455/1880 train_time:15636ms step_avg:34.37ms
step:456/1880 train_time:15670ms step_avg:34.36ms
step:457/1880 train_time:15704ms step_avg:34.36ms
step:458/1880 train_time:15738ms step_avg:34.36ms
step:459/1880 train_time:15772ms step_avg:34.36ms
step:460/1880 train_time:15806ms step_avg:34.36ms
step:461/1880 train_time:15840ms step_avg:34.36ms
step:462/1880 train_time:15874ms step_avg:34.36ms
step:463/1880 train_time:15908ms step_avg:34.36ms
step:464/1880 train_time:15942ms step_avg:34.36ms
step:465/1880 train_time:15976ms step_avg:34.36ms
step:466/1880 train_time:16010ms step_avg:34.36ms
step:467/1880 train_time:16044ms step_avg:34.36ms
step:468/1880 train_time:16079ms step_avg:34.36ms
step:469/1880 train_time:16113ms step_avg:34.36ms
step:470/1880 train_time:16147ms step_avg:34.36ms
step:471/1880 train_time:16181ms step_avg:34.35ms
step:472/1880 train_time:16215ms step_avg:34.35ms
step:473/1880 train_time:16249ms step_avg:34.35ms
step:474/1880 train_time:16283ms step_avg:34.35ms
step:475/1880 train_time:16317ms step_avg:34.35ms
step:476/1880 train_time:16351ms step_avg:34.35ms
step:477/1880 train_time:16385ms step_avg:34.35ms
step:478/1880 train_time:16419ms step_avg:34.35ms
step:479/1880 train_time:16453ms step_avg:34.35ms
step:480/1880 train_time:16487ms step_avg:34.35ms
step:481/1880 train_time:16520ms step_avg:34.35ms
step:482/1880 train_time:16554ms step_avg:34.34ms
step:483/1880 train_time:16588ms step_avg:34.34ms
step:484/1880 train_time:16622ms step_avg:34.34ms
step:485/1880 train_time:16656ms step_avg:34.34ms
step:486/1880 train_time:16690ms step_avg:34.34ms
step:487/1880 train_time:16724ms step_avg:34.34ms
step:488/1880 train_time:16758ms step_avg:34.34ms
step:489/1880 train_time:16792ms step_avg:34.34ms
step:490/1880 train_time:16826ms step_avg:34.34ms
step:491/1880 train_time:16860ms step_avg:34.34ms
step:492/1880 train_time:16894ms step_avg:34.34ms
step:493/1880 train_time:16928ms step_avg:34.34ms
step:494/1880 train_time:16962ms step_avg:34.34ms
step:495/1880 train_time:16996ms step_avg:34.33ms
step:496/1880 train_time:17030ms step_avg:34.33ms
step:497/1880 train_time:17064ms step_avg:34.33ms
step:498/1880 train_time:17098ms step_avg:34.33ms
step:499/1880 train_time:17132ms step_avg:34.33ms
step:500/1880 train_time:17167ms step_avg:34.33ms
step:500/1880 val_loss:4.2848 train_time:17203ms step_avg:34.41ms
step:501/1880 train_time:17223ms step_avg:34.38ms
step:502/1880 train_time:17242ms step_avg:34.35ms
step:503/1880 train_time:17271ms step_avg:34.34ms
step:504/1880 train_time:17305ms step_avg:34.34ms
step:505/1880 train_time:17340ms step_avg:34.34ms
step:506/1880 train_time:17375ms step_avg:34.34ms
step:507/1880 train_time:17409ms step_avg:34.34ms
step:508/1880 train_time:17444ms step_avg:34.34ms
step:509/1880 train_time:17478ms step_avg:34.34ms
step:510/1880 train_time:17513ms step_avg:34.34ms
step:511/1880 train_time:17547ms step_avg:34.34ms
step:512/1880 train_time:17581ms step_avg:34.34ms
step:513/1880 train_time:17614ms step_avg:34.34ms
step:514/1880 train_time:17648ms step_avg:34.33ms
step:515/1880 train_time:17682ms step_avg:34.33ms
step:516/1880 train_time:17716ms step_avg:34.33ms
step:517/1880 train_time:17749ms step_avg:34.33ms
step:518/1880 train_time:17783ms step_avg:34.33ms
step:519/1880 train_time:17817ms step_avg:34.33ms
step:520/1880 train_time:17851ms step_avg:34.33ms
step:521/1880 train_time:17885ms step_avg:34.33ms
step:522/1880 train_time:17919ms step_avg:34.33ms
step:523/1880 train_time:17952ms step_avg:34.33ms
step:524/1880 train_time:17986ms step_avg:34.33ms
step:525/1880 train_time:18020ms step_avg:34.32ms
step:526/1880 train_time:18054ms step_avg:34.32ms
step:527/1880 train_time:18087ms step_avg:34.32ms
step:528/1880 train_time:18121ms step_avg:34.32ms
step:529/1880 train_time:18155ms step_avg:34.32ms
step:530/1880 train_time:18189ms step_avg:34.32ms
step:531/1880 train_time:18223ms step_avg:34.32ms
step:532/1880 train_time:18258ms step_avg:34.32ms
step:533/1880 train_time:18292ms step_avg:34.32ms
step:534/1880 train_time:18327ms step_avg:34.32ms
step:535/1880 train_time:18361ms step_avg:34.32ms
step:536/1880 train_time:18395ms step_avg:34.32ms
step:537/1880 train_time:18429ms step_avg:34.32ms
step:538/1880 train_time:18463ms step_avg:34.32ms
step:539/1880 train_time:18497ms step_avg:34.32ms
step:540/1880 train_time:18531ms step_avg:34.32ms
step:541/1880 train_time:18565ms step_avg:34.32ms
step:542/1880 train_time:18599ms step_avg:34.32ms
step:543/1880 train_time:18633ms step_avg:34.31ms
step:544/1880 train_time:18667ms step_avg:34.31ms
step:545/1880 train_time:18701ms step_avg:34.31ms
step:546/1880 train_time:18735ms step_avg:34.31ms
step:547/1880 train_time:18768ms step_avg:34.31ms
step:548/1880 train_time:18803ms step_avg:34.31ms
step:549/1880 train_time:18836ms step_avg:34.31ms
step:550/1880 train_time:18870ms step_avg:34.31ms
step:551/1880 train_time:18904ms step_avg:34.31ms
step:552/1880 train_time:18938ms step_avg:34.31ms
step:553/1880 train_time:18972ms step_avg:34.31ms
step:554/1880 train_time:19006ms step_avg:34.31ms
step:555/1880 train_time:19040ms step_avg:34.31ms
step:556/1880 train_time:19074ms step_avg:34.31ms
step:557/1880 train_time:19108ms step_avg:34.31ms
step:558/1880 train_time:19142ms step_avg:34.30ms
step:559/1880 train_time:19176ms step_avg:34.30ms
step:560/1880 train_time:19210ms step_avg:34.30ms
step:561/1880 train_time:19243ms step_avg:34.30ms
step:562/1880 train_time:19278ms step_avg:34.30ms
step:563/1880 train_time:19312ms step_avg:34.30ms
step:564/1880 train_time:19347ms step_avg:34.30ms
step:565/1880 train_time:19381ms step_avg:34.30ms
step:566/1880 train_time:19415ms step_avg:34.30ms
step:567/1880 train_time:19449ms step_avg:34.30ms
step:568/1880 train_time:19483ms step_avg:34.30ms
step:569/1880 train_time:19517ms step_avg:34.30ms
step:570/1880 train_time:19551ms step_avg:34.30ms
step:571/1880 train_time:19585ms step_avg:34.30ms
step:572/1880 train_time:19619ms step_avg:34.30ms
step:573/1880 train_time:19653ms step_avg:34.30ms
step:574/1880 train_time:19687ms step_avg:34.30ms
step:575/1880 train_time:19721ms step_avg:34.30ms
step:576/1880 train_time:19755ms step_avg:34.30ms
step:577/1880 train_time:19789ms step_avg:34.30ms
step:578/1880 train_time:19823ms step_avg:34.30ms
step:579/1880 train_time:19857ms step_avg:34.29ms
step:580/1880 train_time:19891ms step_avg:34.29ms
step:581/1880 train_time:19925ms step_avg:34.29ms
step:582/1880 train_time:19959ms step_avg:34.29ms
step:583/1880 train_time:19992ms step_avg:34.29ms
step:584/1880 train_time:20027ms step_avg:34.29ms
step:585/1880 train_time:20061ms step_avg:34.29ms
step:586/1880 train_time:20095ms step_avg:34.29ms
step:587/1880 train_time:20129ms step_avg:34.29ms
step:588/1880 train_time:20163ms step_avg:34.29ms
step:589/1880 train_time:20196ms step_avg:34.29ms
step:590/1880 train_time:20230ms step_avg:34.29ms
step:591/1880 train_time:20264ms step_avg:34.29ms
step:592/1880 train_time:20299ms step_avg:34.29ms
step:593/1880 train_time:20333ms step_avg:34.29ms
step:594/1880 train_time:20367ms step_avg:34.29ms
step:595/1880 train_time:20401ms step_avg:34.29ms
step:596/1880 train_time:20435ms step_avg:34.29ms
step:597/1880 train_time:20469ms step_avg:34.29ms
step:598/1880 train_time:20503ms step_avg:34.29ms
step:599/1880 train_time:20537ms step_avg:34.29ms
step:600/1880 train_time:20572ms step_avg:34.29ms
step:601/1880 train_time:20606ms step_avg:34.29ms
step:602/1880 train_time:20640ms step_avg:34.29ms
step:603/1880 train_time:20674ms step_avg:34.29ms
step:604/1880 train_time:20708ms step_avg:34.29ms
step:605/1880 train_time:20742ms step_avg:34.28ms
step:606/1880 train_time:20776ms step_avg:34.28ms
step:607/1880 train_time:20810ms step_avg:34.28ms
step:608/1880 train_time:20844ms step_avg:34.28ms
step:609/1880 train_time:20878ms step_avg:34.28ms
step:610/1880 train_time:20913ms step_avg:34.28ms
step:611/1880 train_time:20946ms step_avg:34.28ms
step:612/1880 train_time:20980ms step_avg:34.28ms
step:613/1880 train_time:21014ms step_avg:34.28ms
step:614/1880 train_time:21048ms step_avg:34.28ms
step:615/1880 train_time:21083ms step_avg:34.28ms
step:616/1880 train_time:21143ms step_avg:34.32ms
step:617/1880 train_time:21205ms step_avg:34.37ms
step:618/1880 train_time:21267ms step_avg:34.41ms
step:619/1880 train_time:21329ms step_avg:34.46ms
step:620/1880 train_time:21390ms step_avg:34.50ms
step:621/1880 train_time:21452ms step_avg:34.54ms
step:622/1880 train_time:21513ms step_avg:34.59ms
step:623/1880 train_time:21575ms step_avg:34.63ms
step:624/1880 train_time:21636ms step_avg:34.67ms
step:625/1880 train_time:21697ms step_avg:34.72ms
step:626/1880 train_time:21758ms step_avg:34.76ms
step:627/1880 train_time:21821ms step_avg:34.80ms
step:628/1880 train_time:21882ms step_avg:34.84ms
step:629/1880 train_time:21944ms step_avg:34.89ms
step:630/1880 train_time:22008ms step_avg:34.93ms
step:631/1880 train_time:22067ms step_avg:34.97ms
step:632/1880 train_time:22128ms step_avg:35.01ms
step:633/1880 train_time:22190ms step_avg:35.06ms
step:634/1880 train_time:22252ms step_avg:35.10ms
step:635/1880 train_time:22314ms step_avg:35.14ms
step:636/1880 train_time:22374ms step_avg:35.18ms
step:637/1880 train_time:22436ms step_avg:35.22ms
step:638/1880 train_time:22498ms step_avg:35.26ms
step:639/1880 train_time:22561ms step_avg:35.31ms
step:640/1880 train_time:22622ms step_avg:35.35ms
step:641/1880 train_time:22683ms step_avg:35.39ms
step:642/1880 train_time:22745ms step_avg:35.43ms
step:643/1880 train_time:22807ms step_avg:35.47ms
step:644/1880 train_time:22869ms step_avg:35.51ms
step:645/1880 train_time:22930ms step_avg:35.55ms
step:646/1880 train_time:22992ms step_avg:35.59ms
step:647/1880 train_time:23054ms step_avg:35.63ms
step:648/1880 train_time:23116ms step_avg:35.67ms
step:649/1880 train_time:23177ms step_avg:35.71ms
step:650/1880 train_time:23239ms step_avg:35.75ms
step:651/1880 train_time:23301ms step_avg:35.79ms
step:652/1880 train_time:23363ms step_avg:35.83ms
step:653/1880 train_time:23425ms step_avg:35.87ms
step:654/1880 train_time:23487ms step_avg:35.91ms
step:655/1880 train_time:23549ms step_avg:35.95ms
step:656/1880 train_time:23611ms step_avg:35.99ms
step:657/1880 train_time:23673ms step_avg:36.03ms
step:658/1880 train_time:23734ms step_avg:36.07ms
step:659/1880 train_time:23796ms step_avg:36.11ms
step:660/1880 train_time:23858ms step_avg:36.15ms
step:661/1880 train_time:23920ms step_avg:36.19ms
step:662/1880 train_time:23982ms step_avg:36.23ms
step:663/1880 train_time:24045ms step_avg:36.27ms
step:664/1880 train_time:24107ms step_avg:36.31ms
step:665/1880 train_time:24168ms step_avg:36.34ms
step:666/1880 train_time:24229ms step_avg:36.38ms
step:667/1880 train_time:24291ms step_avg:36.42ms
step:668/1880 train_time:24352ms step_avg:36.46ms
step:669/1880 train_time:24414ms step_avg:36.49ms
step:670/1880 train_time:24475ms step_avg:36.53ms
step:671/1880 train_time:24538ms step_avg:36.57ms
step:672/1880 train_time:24600ms step_avg:36.61ms
step:673/1880 train_time:24662ms step_avg:36.65ms
step:674/1880 train_time:24724ms step_avg:36.68ms
step:675/1880 train_time:24785ms step_avg:36.72ms
step:676/1880 train_time:24846ms step_avg:36.75ms
step:677/1880 train_time:24908ms step_avg:36.79ms
step:678/1880 train_time:24969ms step_avg:36.83ms
step:679/1880 train_time:25030ms step_avg:36.86ms
step:680/1880 train_time:25091ms step_avg:36.90ms
step:681/1880 train_time:25153ms step_avg:36.94ms
step:682/1880 train_time:25214ms step_avg:36.97ms
step:683/1880 train_time:25276ms step_avg:37.01ms
step:684/1880 train_time:25337ms step_avg:37.04ms
step:685/1880 train_time:25400ms step_avg:37.08ms
step:686/1880 train_time:25462ms step_avg:37.12ms
step:687/1880 train_time:25525ms step_avg:37.15ms
step:688/1880 train_time:25587ms step_avg:37.19ms
step:689/1880 train_time:25649ms step_avg:37.23ms
step:690/1880 train_time:25710ms step_avg:37.26ms
step:691/1880 train_time:25772ms step_avg:37.30ms
step:692/1880 train_time:25833ms step_avg:37.33ms
step:693/1880 train_time:25895ms step_avg:37.37ms
step:694/1880 train_time:25956ms step_avg:37.40ms
step:695/1880 train_time:26018ms step_avg:37.44ms
step:696/1880 train_time:26079ms step_avg:37.47ms
step:697/1880 train_time:26142ms step_avg:37.51ms
step:698/1880 train_time:26204ms step_avg:37.54ms
step:699/1880 train_time:26265ms step_avg:37.58ms
step:700/1880 train_time:26326ms step_avg:37.61ms
step:701/1880 train_time:26388ms step_avg:37.64ms
step:702/1880 train_time:26449ms step_avg:37.68ms
step:703/1880 train_time:26512ms step_avg:37.71ms
step:704/1880 train_time:26573ms step_avg:37.75ms
step:705/1880 train_time:26635ms step_avg:37.78ms
step:706/1880 train_time:26696ms step_avg:37.81ms
step:707/1880 train_time:26758ms step_avg:37.85ms
step:708/1880 train_time:26819ms step_avg:37.88ms
step:709/1880 train_time:26881ms step_avg:37.91ms
step:710/1880 train_time:26943ms step_avg:37.95ms
step:711/1880 train_time:27005ms step_avg:37.98ms
step:712/1880 train_time:27066ms step_avg:38.01ms
step:713/1880 train_time:27128ms step_avg:38.05ms
step:714/1880 train_time:27189ms step_avg:38.08ms
step:715/1880 train_time:27251ms step_avg:38.11ms
step:716/1880 train_time:27313ms step_avg:38.15ms
step:717/1880 train_time:27374ms step_avg:38.18ms
step:718/1880 train_time:27435ms step_avg:38.21ms
step:719/1880 train_time:27496ms step_avg:38.24ms
step:720/1880 train_time:27557ms step_avg:38.27ms
step:721/1880 train_time:27619ms step_avg:38.31ms
step:722/1880 train_time:27681ms step_avg:38.34ms
step:723/1880 train_time:27743ms step_avg:38.37ms
step:724/1880 train_time:27807ms step_avg:38.41ms
step:725/1880 train_time:27866ms step_avg:38.44ms
step:726/1880 train_time:27927ms step_avg:38.47ms
step:727/1880 train_time:27989ms step_avg:38.50ms
step:728/1880 train_time:28050ms step_avg:38.53ms
step:729/1880 train_time:28113ms step_avg:38.56ms
step:730/1880 train_time:28173ms step_avg:38.59ms
step:731/1880 train_time:28235ms step_avg:38.63ms
step:732/1880 train_time:28296ms step_avg:38.66ms
step:733/1880 train_time:28358ms step_avg:38.69ms
step:734/1880 train_time:28419ms step_avg:38.72ms
step:735/1880 train_time:28481ms step_avg:38.75ms
step:736/1880 train_time:28542ms step_avg:38.78ms
step:737/1880 train_time:28604ms step_avg:38.81ms
step:738/1880 train_time:28665ms step_avg:38.84ms
step:739/1880 train_time:28728ms step_avg:38.87ms
step:740/1880 train_time:28789ms step_avg:38.90ms
step:741/1880 train_time:28851ms step_avg:38.93ms
step:742/1880 train_time:28912ms step_avg:38.96ms
step:743/1880 train_time:28974ms step_avg:39.00ms
step:744/1880 train_time:29035ms step_avg:39.03ms
step:745/1880 train_time:29097ms step_avg:39.06ms
step:746/1880 train_time:29158ms step_avg:39.09ms
step:747/1880 train_time:29220ms step_avg:39.12ms
step:748/1880 train_time:29282ms step_avg:39.15ms
step:749/1880 train_time:29344ms step_avg:39.18ms
step:750/1880 train_time:29405ms step_avg:39.21ms
step:750/1880 val_loss:4.0225 train_time:29469ms step_avg:39.29ms
step:751/1880 train_time:29489ms step_avg:39.27ms
step:752/1880 train_time:29530ms step_avg:39.27ms
step:753/1880 train_time:29592ms step_avg:39.30ms
step:754/1880 train_time:29654ms step_avg:39.33ms
step:755/1880 train_time:29715ms step_avg:39.36ms
step:756/1880 train_time:29777ms step_avg:39.39ms
step:757/1880 train_time:29839ms step_avg:39.42ms
step:758/1880 train_time:29900ms step_avg:39.45ms
step:759/1880 train_time:29961ms step_avg:39.47ms
step:760/1880 train_time:30022ms step_avg:39.50ms
step:761/1880 train_time:30083ms step_avg:39.53ms
step:762/1880 train_time:30143ms step_avg:39.56ms
step:763/1880 train_time:30204ms step_avg:39.59ms
step:764/1880 train_time:30265ms step_avg:39.61ms
step:765/1880 train_time:30327ms step_avg:39.64ms
step:766/1880 train_time:30388ms step_avg:39.67ms
step:767/1880 train_time:30451ms step_avg:39.70ms
step:768/1880 train_time:30513ms step_avg:39.73ms
step:769/1880 train_time:30576ms step_avg:39.76ms
step:770/1880 train_time:30638ms step_avg:39.79ms
step:771/1880 train_time:30700ms step_avg:39.82ms
step:772/1880 train_time:30761ms step_avg:39.85ms
step:773/1880 train_time:30824ms step_avg:39.88ms
step:774/1880 train_time:30885ms step_avg:39.90ms
step:775/1880 train_time:30947ms step_avg:39.93ms
step:776/1880 train_time:31008ms step_avg:39.96ms
step:777/1880 train_time:31069ms step_avg:39.99ms
step:778/1880 train_time:31131ms step_avg:40.01ms
step:779/1880 train_time:31192ms step_avg:40.04ms
step:780/1880 train_time:31253ms step_avg:40.07ms
step:781/1880 train_time:31315ms step_avg:40.10ms
step:782/1880 train_time:31377ms step_avg:40.12ms
step:783/1880 train_time:31439ms step_avg:40.15ms
step:784/1880 train_time:31501ms step_avg:40.18ms
step:785/1880 train_time:31563ms step_avg:40.21ms
step:786/1880 train_time:31624ms step_avg:40.23ms
step:787/1880 train_time:31686ms step_avg:40.26ms
step:788/1880 train_time:31747ms step_avg:40.29ms
step:789/1880 train_time:31810ms step_avg:40.32ms
step:790/1880 train_time:31871ms step_avg:40.34ms
step:791/1880 train_time:31933ms step_avg:40.37ms
step:792/1880 train_time:31994ms step_avg:40.40ms
step:793/1880 train_time:32056ms step_avg:40.42ms
step:794/1880 train_time:32117ms step_avg:40.45ms
step:795/1880 train_time:32179ms step_avg:40.48ms
step:796/1880 train_time:32241ms step_avg:40.50ms
step:797/1880 train_time:32303ms step_avg:40.53ms
step:798/1880 train_time:32364ms step_avg:40.56ms
step:799/1880 train_time:32426ms step_avg:40.58ms
step:800/1880 train_time:32487ms step_avg:40.61ms
step:801/1880 train_time:32549ms step_avg:40.64ms
step:802/1880 train_time:32610ms step_avg:40.66ms
step:803/1880 train_time:32672ms step_avg:40.69ms
step:804/1880 train_time:32734ms step_avg:40.71ms
step:805/1880 train_time:32796ms step_avg:40.74ms
step:806/1880 train_time:32857ms step_avg:40.77ms
step:807/1880 train_time:32919ms step_avg:40.79ms
step:808/1880 train_time:32981ms step_avg:40.82ms
step:809/1880 train_time:33042ms step_avg:40.84ms
step:810/1880 train_time:33103ms step_avg:40.87ms
step:811/1880 train_time:33165ms step_avg:40.89ms
step:812/1880 train_time:33226ms step_avg:40.92ms
step:813/1880 train_time:33288ms step_avg:40.94ms
step:814/1880 train_time:33349ms step_avg:40.97ms
step:815/1880 train_time:33411ms step_avg:41.00ms
step:816/1880 train_time:33473ms step_avg:41.02ms
step:817/1880 train_time:33535ms step_avg:41.05ms
step:818/1880 train_time:33597ms step_avg:41.07ms
step:819/1880 train_time:33660ms step_avg:41.10ms
step:820/1880 train_time:33722ms step_avg:41.12ms
step:821/1880 train_time:33784ms step_avg:41.15ms
step:822/1880 train_time:33845ms step_avg:41.17ms
step:823/1880 train_time:33907ms step_avg:41.20ms
step:824/1880 train_time:33968ms step_avg:41.22ms
step:825/1880 train_time:34031ms step_avg:41.25ms
step:826/1880 train_time:34092ms step_avg:41.27ms
step:827/1880 train_time:34153ms step_avg:41.30ms
step:828/1880 train_time:34215ms step_avg:41.32ms
step:829/1880 train_time:34277ms step_avg:41.35ms
step:830/1880 train_time:34339ms step_avg:41.37ms
step:831/1880 train_time:34401ms step_avg:41.40ms
step:832/1880 train_time:34462ms step_avg:41.42ms
step:833/1880 train_time:34524ms step_avg:41.45ms
step:834/1880 train_time:34585ms step_avg:41.47ms
step:835/1880 train_time:34648ms step_avg:41.49ms
step:836/1880 train_time:34709ms step_avg:41.52ms
step:837/1880 train_time:34771ms step_avg:41.54ms
step:838/1880 train_time:34832ms step_avg:41.57ms
step:839/1880 train_time:34894ms step_avg:41.59ms
step:840/1880 train_time:34956ms step_avg:41.61ms
step:841/1880 train_time:35018ms step_avg:41.64ms
step:842/1880 train_time:35080ms step_avg:41.66ms
step:843/1880 train_time:35142ms step_avg:41.69ms
step:844/1880 train_time:35203ms step_avg:41.71ms
step:845/1880 train_time:35265ms step_avg:41.73ms
step:846/1880 train_time:35326ms step_avg:41.76ms
step:847/1880 train_time:35387ms step_avg:41.78ms
step:848/1880 train_time:35448ms step_avg:41.80ms
step:849/1880 train_time:35510ms step_avg:41.83ms
step:850/1880 train_time:35572ms step_avg:41.85ms
step:851/1880 train_time:35633ms step_avg:41.87ms
step:852/1880 train_time:35696ms step_avg:41.90ms
step:853/1880 train_time:35758ms step_avg:41.92ms
step:854/1880 train_time:35820ms step_avg:41.94ms
step:855/1880 train_time:35882ms step_avg:41.97ms
step:856/1880 train_time:35944ms step_avg:41.99ms
step:857/1880 train_time:36005ms step_avg:42.01ms
step:858/1880 train_time:36067ms step_avg:42.04ms
step:859/1880 train_time:36129ms step_avg:42.06ms
step:860/1880 train_time:36190ms step_avg:42.08ms
step:861/1880 train_time:36252ms step_avg:42.10ms
step:862/1880 train_time:36313ms step_avg:42.13ms
step:863/1880 train_time:36375ms step_avg:42.15ms
step:864/1880 train_time:36438ms step_avg:42.17ms
step:865/1880 train_time:36500ms step_avg:42.20ms
step:866/1880 train_time:36562ms step_avg:42.22ms
step:867/1880 train_time:36624ms step_avg:42.24ms
step:868/1880 train_time:36685ms step_avg:42.26ms
step:869/1880 train_time:36747ms step_avg:42.29ms
step:870/1880 train_time:36808ms step_avg:42.31ms
step:871/1880 train_time:36870ms step_avg:42.33ms
step:872/1880 train_time:36932ms step_avg:42.35ms
step:873/1880 train_time:36994ms step_avg:42.38ms
step:874/1880 train_time:37055ms step_avg:42.40ms
step:875/1880 train_time:37117ms step_avg:42.42ms
step:876/1880 train_time:37178ms step_avg:42.44ms
step:877/1880 train_time:37240ms step_avg:42.46ms
step:878/1880 train_time:37301ms step_avg:42.48ms
step:879/1880 train_time:37363ms step_avg:42.51ms
step:880/1880 train_time:37424ms step_avg:42.53ms
step:881/1880 train_time:37487ms step_avg:42.55ms
step:882/1880 train_time:37549ms step_avg:42.57ms
step:883/1880 train_time:37610ms step_avg:42.59ms
step:884/1880 train_time:37671ms step_avg:42.61ms
step:885/1880 train_time:37733ms step_avg:42.64ms
step:886/1880 train_time:37795ms step_avg:42.66ms
step:887/1880 train_time:37856ms step_avg:42.68ms
step:888/1880 train_time:37919ms step_avg:42.70ms
step:889/1880 train_time:37981ms step_avg:42.72ms
step:890/1880 train_time:38042ms step_avg:42.74ms
step:891/1880 train_time:38104ms step_avg:42.76ms
step:892/1880 train_time:38164ms step_avg:42.79ms
step:893/1880 train_time:38226ms step_avg:42.81ms
step:894/1880 train_time:38287ms step_avg:42.83ms
step:895/1880 train_time:38349ms step_avg:42.85ms
step:896/1880 train_time:38410ms step_avg:42.87ms
step:897/1880 train_time:38471ms step_avg:42.89ms
step:898/1880 train_time:38533ms step_avg:42.91ms
step:899/1880 train_time:38595ms step_avg:42.93ms
step:900/1880 train_time:38656ms step_avg:42.95ms
step:901/1880 train_time:38718ms step_avg:42.97ms
step:902/1880 train_time:38780ms step_avg:42.99ms
step:903/1880 train_time:38843ms step_avg:43.02ms
step:904/1880 train_time:38905ms step_avg:43.04ms
step:905/1880 train_time:38967ms step_avg:43.06ms
step:906/1880 train_time:39028ms step_avg:43.08ms
step:907/1880 train_time:39090ms step_avg:43.10ms
step:908/1880 train_time:39152ms step_avg:43.12ms
step:909/1880 train_time:39213ms step_avg:43.14ms
step:910/1880 train_time:39275ms step_avg:43.16ms
step:911/1880 train_time:39337ms step_avg:43.18ms
step:912/1880 train_time:39399ms step_avg:43.20ms
step:913/1880 train_time:39461ms step_avg:43.22ms
step:914/1880 train_time:39522ms step_avg:43.24ms
step:915/1880 train_time:39584ms step_avg:43.26ms
step:916/1880 train_time:39645ms step_avg:43.28ms
step:917/1880 train_time:39707ms step_avg:43.30ms
step:918/1880 train_time:39768ms step_avg:43.32ms
step:919/1880 train_time:39830ms step_avg:43.34ms
step:920/1880 train_time:39891ms step_avg:43.36ms
step:921/1880 train_time:39953ms step_avg:43.38ms
step:922/1880 train_time:40015ms step_avg:43.40ms
step:923/1880 train_time:40077ms step_avg:43.42ms
step:924/1880 train_time:40140ms step_avg:43.44ms
step:925/1880 train_time:40202ms step_avg:43.46ms
step:926/1880 train_time:40263ms step_avg:43.48ms
step:927/1880 train_time:40325ms step_avg:43.50ms
step:928/1880 train_time:40386ms step_avg:43.52ms
step:929/1880 train_time:40448ms step_avg:43.54ms
step:930/1880 train_time:40509ms step_avg:43.56ms
step:931/1880 train_time:40571ms step_avg:43.58ms
step:932/1880 train_time:40632ms step_avg:43.60ms
step:933/1880 train_time:40694ms step_avg:43.62ms
step:934/1880 train_time:40756ms step_avg:43.64ms
step:935/1880 train_time:40818ms step_avg:43.66ms
step:936/1880 train_time:40880ms step_avg:43.68ms
step:937/1880 train_time:40942ms step_avg:43.69ms
step:938/1880 train_time:41003ms step_avg:43.71ms
step:939/1880 train_time:41065ms step_avg:43.73ms
step:940/1880 train_time:41127ms step_avg:43.75ms
step:941/1880 train_time:41189ms step_avg:43.77ms
step:942/1880 train_time:41250ms step_avg:43.79ms
step:943/1880 train_time:41312ms step_avg:43.81ms
step:944/1880 train_time:41373ms step_avg:43.83ms
step:945/1880 train_time:41436ms step_avg:43.85ms
step:946/1880 train_time:41498ms step_avg:43.87ms
step:947/1880 train_time:41560ms step_avg:43.89ms
step:948/1880 train_time:41622ms step_avg:43.90ms
step:949/1880 train_time:41683ms step_avg:43.92ms
step:950/1880 train_time:41745ms step_avg:43.94ms
step:951/1880 train_time:41807ms step_avg:43.96ms
step:952/1880 train_time:41868ms step_avg:43.98ms
step:953/1880 train_time:41930ms step_avg:44.00ms
step:954/1880 train_time:41992ms step_avg:44.02ms
step:955/1880 train_time:42054ms step_avg:44.04ms
step:956/1880 train_time:42116ms step_avg:44.05ms
step:957/1880 train_time:42178ms step_avg:44.07ms
step:958/1880 train_time:42240ms step_avg:44.09ms
step:959/1880 train_time:42302ms step_avg:44.11ms
step:960/1880 train_time:42363ms step_avg:44.13ms
step:961/1880 train_time:42425ms step_avg:44.15ms
step:962/1880 train_time:42486ms step_avg:44.16ms
step:963/1880 train_time:42548ms step_avg:44.18ms
step:964/1880 train_time:42609ms step_avg:44.20ms
step:965/1880 train_time:42671ms step_avg:44.22ms
step:966/1880 train_time:42733ms step_avg:44.24ms
step:967/1880 train_time:42794ms step_avg:44.25ms
step:968/1880 train_time:42856ms step_avg:44.27ms
step:969/1880 train_time:42919ms step_avg:44.29ms
step:970/1880 train_time:42981ms step_avg:44.31ms
step:971/1880 train_time:43042ms step_avg:44.33ms
step:972/1880 train_time:43103ms step_avg:44.34ms
step:973/1880 train_time:43165ms step_avg:44.36ms
step:974/1880 train_time:43227ms step_avg:44.38ms
step:975/1880 train_time:43289ms step_avg:44.40ms
step:976/1880 train_time:43350ms step_avg:44.42ms
step:977/1880 train_time:43411ms step_avg:44.43ms
step:978/1880 train_time:43474ms step_avg:44.45ms
step:979/1880 train_time:43536ms step_avg:44.47ms
step:980/1880 train_time:43598ms step_avg:44.49ms
step:981/1880 train_time:43660ms step_avg:44.51ms
step:982/1880 train_time:43721ms step_avg:44.52ms
step:983/1880 train_time:43783ms step_avg:44.54ms
step:984/1880 train_time:43844ms step_avg:44.56ms
step:985/1880 train_time:43907ms step_avg:44.58ms
step:986/1880 train_time:43968ms step_avg:44.59ms
step:987/1880 train_time:44029ms step_avg:44.61ms
step:988/1880 train_time:44090ms step_avg:44.63ms
step:989/1880 train_time:44152ms step_avg:44.64ms
step:990/1880 train_time:44213ms step_avg:44.66ms
step:991/1880 train_time:44276ms step_avg:44.68ms
step:992/1880 train_time:44337ms step_avg:44.70ms
step:993/1880 train_time:44399ms step_avg:44.71ms
step:994/1880 train_time:44461ms step_avg:44.73ms
step:995/1880 train_time:44522ms step_avg:44.75ms
step:996/1880 train_time:44584ms step_avg:44.76ms
step:997/1880 train_time:44646ms step_avg:44.78ms
step:998/1880 train_time:44707ms step_avg:44.80ms
step:999/1880 train_time:44769ms step_avg:44.81ms
step:1000/1880 train_time:44830ms step_avg:44.83ms
step:1000/1880 val_loss:3.7806 train_time:44894ms step_avg:44.89ms
step:1001/1880 train_time:44914ms step_avg:44.87ms
step:1002/1880 train_time:44955ms step_avg:44.86ms
step:1003/1880 train_time:45018ms step_avg:44.88ms
step:1004/1880 train_time:45081ms step_avg:44.90ms
step:1005/1880 train_time:45143ms step_avg:44.92ms
step:1006/1880 train_time:45204ms step_avg:44.93ms
step:1007/1880 train_time:45265ms step_avg:44.95ms
step:1008/1880 train_time:45326ms step_avg:44.97ms
step:1009/1880 train_time:45387ms step_avg:44.98ms
step:1010/1880 train_time:45447ms step_avg:45.00ms
step:1011/1880 train_time:45508ms step_avg:45.01ms
step:1012/1880 train_time:45569ms step_avg:45.03ms
step:1013/1880 train_time:45630ms step_avg:45.04ms
step:1014/1880 train_time:45691ms step_avg:45.06ms
step:1015/1880 train_time:45752ms step_avg:45.08ms
step:1016/1880 train_time:45814ms step_avg:45.09ms
step:1017/1880 train_time:45879ms step_avg:45.11ms
step:1018/1880 train_time:45941ms step_avg:45.13ms
step:1019/1880 train_time:46004ms step_avg:45.15ms
step:1020/1880 train_time:46066ms step_avg:45.16ms
step:1021/1880 train_time:46129ms step_avg:45.18ms
step:1022/1880 train_time:46190ms step_avg:45.20ms
step:1023/1880 train_time:46253ms step_avg:45.21ms
step:1024/1880 train_time:46314ms step_avg:45.23ms
step:1025/1880 train_time:46376ms step_avg:45.24ms
step:1026/1880 train_time:46437ms step_avg:45.26ms
step:1027/1880 train_time:46498ms step_avg:45.28ms
step:1028/1880 train_time:46559ms step_avg:45.29ms
step:1029/1880 train_time:46620ms step_avg:45.31ms
step:1030/1880 train_time:46681ms step_avg:45.32ms
step:1031/1880 train_time:46742ms step_avg:45.34ms
step:1032/1880 train_time:46803ms step_avg:45.35ms
step:1033/1880 train_time:46865ms step_avg:45.37ms
step:1034/1880 train_time:46927ms step_avg:45.38ms
step:1035/1880 train_time:46990ms step_avg:45.40ms
step:1036/1880 train_time:47052ms step_avg:45.42ms
step:1037/1880 train_time:47114ms step_avg:45.43ms
step:1038/1880 train_time:47176ms step_avg:45.45ms
step:1039/1880 train_time:47238ms step_avg:45.46ms
step:1040/1880 train_time:47300ms step_avg:45.48ms
step:1041/1880 train_time:47361ms step_avg:45.50ms
step:1042/1880 train_time:47422ms step_avg:45.51ms
step:1043/1880 train_time:47483ms step_avg:45.53ms
step:1044/1880 train_time:47544ms step_avg:45.54ms
step:1045/1880 train_time:47605ms step_avg:45.55ms
step:1046/1880 train_time:47665ms step_avg:45.57ms
step:1047/1880 train_time:47727ms step_avg:45.58ms
step:1048/1880 train_time:47788ms step_avg:45.60ms
step:1049/1880 train_time:47850ms step_avg:45.61ms
step:1050/1880 train_time:47912ms step_avg:45.63ms
step:1051/1880 train_time:47975ms step_avg:45.65ms
step:1052/1880 train_time:48038ms step_avg:45.66ms
step:1053/1880 train_time:48100ms step_avg:45.68ms
step:1054/1880 train_time:48161ms step_avg:45.69ms
step:1055/1880 train_time:48223ms step_avg:45.71ms
step:1056/1880 train_time:48284ms step_avg:45.72ms
step:1057/1880 train_time:48346ms step_avg:45.74ms
step:1058/1880 train_time:48407ms step_avg:45.75ms
step:1059/1880 train_time:48469ms step_avg:45.77ms
step:1060/1880 train_time:48530ms step_avg:45.78ms
step:1061/1880 train_time:48591ms step_avg:45.80ms
step:1062/1880 train_time:48653ms step_avg:45.81ms
step:1063/1880 train_time:48715ms step_avg:45.83ms
step:1064/1880 train_time:48776ms step_avg:45.84ms
step:1065/1880 train_time:48837ms step_avg:45.86ms
step:1066/1880 train_time:48899ms step_avg:45.87ms
step:1067/1880 train_time:48960ms step_avg:45.89ms
step:1068/1880 train_time:49022ms step_avg:45.90ms
step:1069/1880 train_time:49084ms step_avg:45.92ms
step:1070/1880 train_time:49145ms step_avg:45.93ms
step:1071/1880 train_time:49208ms step_avg:45.95ms
step:1072/1880 train_time:49269ms step_avg:45.96ms
step:1073/1880 train_time:49330ms step_avg:45.97ms
step:1074/1880 train_time:49393ms step_avg:45.99ms
step:1075/1880 train_time:49455ms step_avg:46.00ms
step:1076/1880 train_time:49516ms step_avg:46.02ms
step:1077/1880 train_time:49577ms step_avg:46.03ms
step:1078/1880 train_time:49638ms step_avg:46.05ms
step:1079/1880 train_time:49700ms step_avg:46.06ms
step:1080/1880 train_time:49761ms step_avg:46.08ms
step:1081/1880 train_time:49823ms step_avg:46.09ms
step:1082/1880 train_time:49884ms step_avg:46.10ms
step:1083/1880 train_time:49946ms step_avg:46.12ms
step:1084/1880 train_time:50007ms step_avg:46.13ms
step:1085/1880 train_time:50069ms step_avg:46.15ms
step:1086/1880 train_time:50132ms step_avg:46.16ms
step:1087/1880 train_time:50194ms step_avg:46.18ms
step:1088/1880 train_time:50256ms step_avg:46.19ms
step:1089/1880 train_time:50317ms step_avg:46.20ms
step:1090/1880 train_time:50379ms step_avg:46.22ms
step:1091/1880 train_time:50440ms step_avg:46.23ms
step:1092/1880 train_time:50501ms step_avg:46.25ms
step:1093/1880 train_time:50563ms step_avg:46.26ms
step:1094/1880 train_time:50625ms step_avg:46.27ms
step:1095/1880 train_time:50687ms step_avg:46.29ms
step:1096/1880 train_time:50748ms step_avg:46.30ms
step:1097/1880 train_time:50809ms step_avg:46.32ms
step:1098/1880 train_time:50870ms step_avg:46.33ms
step:1099/1880 train_time:50932ms step_avg:46.34ms
step:1100/1880 train_time:50993ms step_avg:46.36ms
step:1101/1880 train_time:51055ms step_avg:46.37ms
step:1102/1880 train_time:51116ms step_avg:46.39ms
step:1103/1880 train_time:51178ms step_avg:46.40ms
step:1104/1880 train_time:51240ms step_avg:46.41ms
step:1105/1880 train_time:51301ms step_avg:46.43ms
step:1106/1880 train_time:51362ms step_avg:46.44ms
step:1107/1880 train_time:51424ms step_avg:46.45ms
step:1108/1880 train_time:51485ms step_avg:46.47ms
step:1109/1880 train_time:51546ms step_avg:46.48ms
step:1110/1880 train_time:51607ms step_avg:46.49ms
step:1111/1880 train_time:51669ms step_avg:46.51ms
step:1112/1880 train_time:51730ms step_avg:46.52ms
step:1113/1880 train_time:51792ms step_avg:46.53ms
step:1114/1880 train_time:51853ms step_avg:46.55ms
step:1115/1880 train_time:51914ms step_avg:46.56ms
step:1116/1880 train_time:51976ms step_avg:46.57ms
step:1117/1880 train_time:52038ms step_avg:46.59ms
step:1118/1880 train_time:52099ms step_avg:46.60ms
step:1119/1880 train_time:52161ms step_avg:46.61ms
step:1120/1880 train_time:52222ms step_avg:46.63ms
step:1121/1880 train_time:52283ms step_avg:46.64ms
step:1122/1880 train_time:52344ms step_avg:46.65ms
step:1123/1880 train_time:52406ms step_avg:46.67ms
step:1124/1880 train_time:52467ms step_avg:46.68ms
step:1125/1880 train_time:52529ms step_avg:46.69ms
step:1126/1880 train_time:52590ms step_avg:46.70ms
step:1127/1880 train_time:52651ms step_avg:46.72ms
step:1128/1880 train_time:52712ms step_avg:46.73ms
step:1129/1880 train_time:52774ms step_avg:46.74ms
step:1130/1880 train_time:52835ms step_avg:46.76ms
step:1131/1880 train_time:52897ms step_avg:46.77ms
step:1132/1880 train_time:52959ms step_avg:46.78ms
step:1133/1880 train_time:53020ms step_avg:46.80ms
step:1134/1880 train_time:53081ms step_avg:46.81ms
step:1135/1880 train_time:53143ms step_avg:46.82ms
step:1136/1880 train_time:53204ms step_avg:46.83ms
step:1137/1880 train_time:53265ms step_avg:46.85ms
step:1138/1880 train_time:53326ms step_avg:46.86ms
step:1139/1880 train_time:53388ms step_avg:46.87ms
step:1140/1880 train_time:53449ms step_avg:46.88ms
step:1141/1880 train_time:53511ms step_avg:46.90ms
step:1142/1880 train_time:53572ms step_avg:46.91ms
step:1143/1880 train_time:53634ms step_avg:46.92ms
step:1144/1880 train_time:53695ms step_avg:46.94ms
step:1145/1880 train_time:53757ms step_avg:46.95ms
step:1146/1880 train_time:53818ms step_avg:46.96ms
step:1147/1880 train_time:53880ms step_avg:46.97ms
step:1148/1880 train_time:53941ms step_avg:46.99ms
step:1149/1880 train_time:54002ms step_avg:47.00ms
step:1150/1880 train_time:54064ms step_avg:47.01ms
step:1151/1880 train_time:54125ms step_avg:47.02ms
step:1152/1880 train_time:54186ms step_avg:47.04ms
step:1153/1880 train_time:54247ms step_avg:47.05ms
step:1154/1880 train_time:54308ms step_avg:47.06ms
step:1155/1880 train_time:54370ms step_avg:47.07ms
step:1156/1880 train_time:54431ms step_avg:47.09ms
step:1157/1880 train_time:54493ms step_avg:47.10ms
step:1158/1880 train_time:54554ms step_avg:47.11ms
step:1159/1880 train_time:54616ms step_avg:47.12ms
step:1160/1880 train_time:54678ms step_avg:47.14ms
step:1161/1880 train_time:54739ms step_avg:47.15ms
step:1162/1880 train_time:54801ms step_avg:47.16ms
step:1163/1880 train_time:54863ms step_avg:47.17ms
step:1164/1880 train_time:54924ms step_avg:47.19ms
step:1165/1880 train_time:54987ms step_avg:47.20ms
step:1166/1880 train_time:55046ms step_avg:47.21ms
step:1167/1880 train_time:55108ms step_avg:47.22ms
step:1168/1880 train_time:55170ms step_avg:47.23ms
step:1169/1880 train_time:55232ms step_avg:47.25ms
step:1170/1880 train_time:55293ms step_avg:47.26ms
step:1171/1880 train_time:55355ms step_avg:47.27ms
step:1172/1880 train_time:55416ms step_avg:47.28ms
step:1173/1880 train_time:55478ms step_avg:47.30ms
step:1174/1880 train_time:55538ms step_avg:47.31ms
step:1175/1880 train_time:55600ms step_avg:47.32ms
step:1176/1880 train_time:55661ms step_avg:47.33ms
step:1177/1880 train_time:55722ms step_avg:47.34ms
step:1178/1880 train_time:55783ms step_avg:47.35ms
step:1179/1880 train_time:55845ms step_avg:47.37ms
step:1180/1880 train_time:55906ms step_avg:47.38ms
step:1181/1880 train_time:55968ms step_avg:47.39ms
step:1182/1880 train_time:56029ms step_avg:47.40ms
step:1183/1880 train_time:56091ms step_avg:47.41ms
step:1184/1880 train_time:56152ms step_avg:47.43ms
step:1185/1880 train_time:56215ms step_avg:47.44ms
step:1186/1880 train_time:56275ms step_avg:47.45ms
step:1187/1880 train_time:56337ms step_avg:47.46ms
step:1188/1880 train_time:56398ms step_avg:47.47ms
step:1189/1880 train_time:56460ms step_avg:47.49ms
step:1190/1880 train_time:56521ms step_avg:47.50ms
step:1191/1880 train_time:56584ms step_avg:47.51ms
step:1192/1880 train_time:56643ms step_avg:47.52ms
step:1193/1880 train_time:56705ms step_avg:47.53ms
step:1194/1880 train_time:56766ms step_avg:47.54ms
step:1195/1880 train_time:56828ms step_avg:47.55ms
step:1196/1880 train_time:56889ms step_avg:47.57ms
step:1197/1880 train_time:56951ms step_avg:47.58ms
step:1198/1880 train_time:57013ms step_avg:47.59ms
step:1199/1880 train_time:57075ms step_avg:47.60ms
step:1200/1880 train_time:57137ms step_avg:47.61ms
step:1201/1880 train_time:57199ms step_avg:47.63ms
step:1202/1880 train_time:57260ms step_avg:47.64ms
step:1203/1880 train_time:57322ms step_avg:47.65ms
step:1204/1880 train_time:57383ms step_avg:47.66ms
step:1205/1880 train_time:57445ms step_avg:47.67ms
step:1206/1880 train_time:57506ms step_avg:47.68ms
step:1207/1880 train_time:57568ms step_avg:47.69ms
step:1208/1880 train_time:57629ms step_avg:47.71ms
step:1209/1880 train_time:57691ms step_avg:47.72ms
step:1210/1880 train_time:57752ms step_avg:47.73ms
step:1211/1880 train_time:57814ms step_avg:47.74ms
step:1212/1880 train_time:57876ms step_avg:47.75ms
step:1213/1880 train_time:57937ms step_avg:47.76ms
step:1214/1880 train_time:57998ms step_avg:47.77ms
step:1215/1880 train_time:58060ms step_avg:47.79ms
step:1216/1880 train_time:58122ms step_avg:47.80ms
step:1217/1880 train_time:58184ms step_avg:47.81ms
step:1218/1880 train_time:58244ms step_avg:47.82ms
step:1219/1880 train_time:58306ms step_avg:47.83ms
step:1220/1880 train_time:58367ms step_avg:47.84ms
step:1221/1880 train_time:58429ms step_avg:47.85ms
step:1222/1880 train_time:58490ms step_avg:47.86ms
step:1223/1880 train_time:58551ms step_avg:47.88ms
step:1224/1880 train_time:58613ms step_avg:47.89ms
step:1225/1880 train_time:58674ms step_avg:47.90ms
step:1226/1880 train_time:58735ms step_avg:47.91ms
step:1227/1880 train_time:58797ms step_avg:47.92ms
step:1228/1880 train_time:58859ms step_avg:47.93ms
step:1229/1880 train_time:58948ms step_avg:47.96ms
step:1230/1880 train_time:59035ms step_avg:48.00ms
step:1231/1880 train_time:59123ms step_avg:48.03ms
step:1232/1880 train_time:59213ms step_avg:48.06ms
step:1233/1880 train_time:59301ms step_avg:48.09ms
step:1234/1880 train_time:59389ms step_avg:48.13ms
step:1235/1880 train_time:59477ms step_avg:48.16ms
step:1236/1880 train_time:59565ms step_avg:48.19ms
step:1237/1880 train_time:59654ms step_avg:48.22ms
step:1238/1880 train_time:59742ms step_avg:48.26ms
step:1239/1880 train_time:59831ms step_avg:48.29ms
step:1240/1880 train_time:59918ms step_avg:48.32ms
step:1241/1880 train_time:60007ms step_avg:48.35ms
step:1242/1880 train_time:60094ms step_avg:48.38ms
step:1243/1880 train_time:60182ms step_avg:48.42ms
step:1244/1880 train_time:60271ms step_avg:48.45ms
step:1245/1880 train_time:60358ms step_avg:48.48ms
step:1246/1880 train_time:60447ms step_avg:48.51ms
step:1247/1880 train_time:60535ms step_avg:48.54ms
step:1248/1880 train_time:60623ms step_avg:48.58ms
step:1249/1880 train_time:60711ms step_avg:48.61ms
step:1250/1880 train_time:60798ms step_avg:48.64ms
step:1250/1880 val_loss:3.5387 train_time:60890ms step_avg:48.71ms
step:1251/1880 train_time:60909ms step_avg:48.69ms
step:1252/1880 train_time:60979ms step_avg:48.71ms
step:1253/1880 train_time:61071ms step_avg:48.74ms
step:1254/1880 train_time:61159ms step_avg:48.77ms
step:1255/1880 train_time:61247ms step_avg:48.80ms
step:1256/1880 train_time:61334ms step_avg:48.83ms
step:1257/1880 train_time:61420ms step_avg:48.86ms
step:1258/1880 train_time:61507ms step_avg:48.89ms
step:1259/1880 train_time:61594ms step_avg:48.92ms
step:1260/1880 train_time:61682ms step_avg:48.95ms
step:1261/1880 train_time:61769ms step_avg:48.98ms
step:1262/1880 train_time:61857ms step_avg:49.02ms
step:1263/1880 train_time:61948ms step_avg:49.05ms
step:1264/1880 train_time:62037ms step_avg:49.08ms
step:1265/1880 train_time:62127ms step_avg:49.11ms
step:1266/1880 train_time:62214ms step_avg:49.14ms
step:1267/1880 train_time:62302ms step_avg:49.17ms
step:1268/1880 train_time:62389ms step_avg:49.20ms
step:1269/1880 train_time:62475ms step_avg:49.23ms
step:1270/1880 train_time:62563ms step_avg:49.26ms
step:1271/1880 train_time:62650ms step_avg:49.29ms
step:1272/1880 train_time:62737ms step_avg:49.32ms
step:1273/1880 train_time:62826ms step_avg:49.35ms
step:1274/1880 train_time:62913ms step_avg:49.38ms
step:1275/1880 train_time:63004ms step_avg:49.41ms
step:1276/1880 train_time:63092ms step_avg:49.45ms
step:1277/1880 train_time:63181ms step_avg:49.48ms
step:1278/1880 train_time:63268ms step_avg:49.51ms
step:1279/1880 train_time:63356ms step_avg:49.54ms
step:1280/1880 train_time:63443ms step_avg:49.56ms
step:1281/1880 train_time:63531ms step_avg:49.59ms
step:1282/1880 train_time:63618ms step_avg:49.62ms
step:1283/1880 train_time:63706ms step_avg:49.65ms
step:1284/1880 train_time:63794ms step_avg:49.68ms
step:1285/1880 train_time:63883ms step_avg:49.71ms
step:1286/1880 train_time:63972ms step_avg:49.74ms
step:1287/1880 train_time:64061ms step_avg:49.78ms
step:1288/1880 train_time:64149ms step_avg:49.81ms
step:1289/1880 train_time:64237ms step_avg:49.84ms
step:1290/1880 train_time:64324ms step_avg:49.86ms
step:1291/1880 train_time:64411ms step_avg:49.89ms
step:1292/1880 train_time:64499ms step_avg:49.92ms
step:1293/1880 train_time:64586ms step_avg:49.95ms
step:1294/1880 train_time:64674ms step_avg:49.98ms
step:1295/1880 train_time:64762ms step_avg:50.01ms
step:1296/1880 train_time:64850ms step_avg:50.04ms
step:1297/1880 train_time:64939ms step_avg:50.07ms
step:1298/1880 train_time:65027ms step_avg:50.10ms
step:1299/1880 train_time:65116ms step_avg:50.13ms
step:1300/1880 train_time:65203ms step_avg:50.16ms
step:1301/1880 train_time:65291ms step_avg:50.19ms
step:1302/1880 train_time:65378ms step_avg:50.21ms
step:1303/1880 train_time:65466ms step_avg:50.24ms
step:1304/1880 train_time:65553ms step_avg:50.27ms
step:1305/1880 train_time:65642ms step_avg:50.30ms
step:1306/1880 train_time:65729ms step_avg:50.33ms
step:1307/1880 train_time:65818ms step_avg:50.36ms
step:1308/1880 train_time:65906ms step_avg:50.39ms
step:1309/1880 train_time:65994ms step_avg:50.42ms
step:1310/1880 train_time:66082ms step_avg:50.44ms
step:1311/1880 train_time:66171ms step_avg:50.47ms
step:1312/1880 train_time:66259ms step_avg:50.50ms
step:1313/1880 train_time:66347ms step_avg:50.53ms
step:1314/1880 train_time:66435ms step_avg:50.56ms
step:1315/1880 train_time:66523ms step_avg:50.59ms
step:1316/1880 train_time:66611ms step_avg:50.62ms
step:1317/1880 train_time:66698ms step_avg:50.64ms
step:1318/1880 train_time:66785ms step_avg:50.67ms
step:1319/1880 train_time:66874ms step_avg:50.70ms
step:1320/1880 train_time:66962ms step_avg:50.73ms
step:1321/1880 train_time:67050ms step_avg:50.76ms
step:1322/1880 train_time:67139ms step_avg:50.79ms
step:1323/1880 train_time:67227ms step_avg:50.81ms
step:1324/1880 train_time:67315ms step_avg:50.84ms
step:1325/1880 train_time:67403ms step_avg:50.87ms
step:1326/1880 train_time:67491ms step_avg:50.90ms
step:1327/1880 train_time:67578ms step_avg:50.93ms
step:1328/1880 train_time:67665ms step_avg:50.95ms
step:1329/1880 train_time:67753ms step_avg:50.98ms
step:1330/1880 train_time:67841ms step_avg:51.01ms
step:1331/1880 train_time:67929ms step_avg:51.04ms
step:1332/1880 train_time:68019ms step_avg:51.07ms
step:1333/1880 train_time:68107ms step_avg:51.09ms
step:1334/1880 train_time:68196ms step_avg:51.12ms
step:1335/1880 train_time:68283ms step_avg:51.15ms
step:1336/1880 train_time:68372ms step_avg:51.18ms
step:1337/1880 train_time:68461ms step_avg:51.20ms
step:1338/1880 train_time:68548ms step_avg:51.23ms
step:1339/1880 train_time:68636ms step_avg:51.26ms
step:1340/1880 train_time:68723ms step_avg:51.29ms
step:1341/1880 train_time:68811ms step_avg:51.31ms
step:1342/1880 train_time:68899ms step_avg:51.34ms
step:1343/1880 train_time:68987ms step_avg:51.37ms
step:1344/1880 train_time:69074ms step_avg:51.39ms
step:1345/1880 train_time:69164ms step_avg:51.42ms
step:1346/1880 train_time:69253ms step_avg:51.45ms
step:1347/1880 train_time:69341ms step_avg:51.48ms
step:1348/1880 train_time:69429ms step_avg:51.51ms
step:1349/1880 train_time:69517ms step_avg:51.53ms
step:1350/1880 train_time:69604ms step_avg:51.56ms
step:1351/1880 train_time:69692ms step_avg:51.59ms
step:1352/1880 train_time:69781ms step_avg:51.61ms
step:1353/1880 train_time:69869ms step_avg:51.64ms
step:1354/1880 train_time:69957ms step_avg:51.67ms
step:1355/1880 train_time:70045ms step_avg:51.69ms
step:1356/1880 train_time:70133ms step_avg:51.72ms
step:1357/1880 train_time:70221ms step_avg:51.75ms
step:1358/1880 train_time:70310ms step_avg:51.77ms
step:1359/1880 train_time:70398ms step_avg:51.80ms
step:1360/1880 train_time:70485ms step_avg:51.83ms
step:1361/1880 train_time:70573ms step_avg:51.85ms
step:1362/1880 train_time:70660ms step_avg:51.88ms
step:1363/1880 train_time:70748ms step_avg:51.91ms
step:1364/1880 train_time:70836ms step_avg:51.93ms
step:1365/1880 train_time:70924ms step_avg:51.96ms
step:1366/1880 train_time:71013ms step_avg:51.99ms
step:1367/1880 train_time:71100ms step_avg:52.01ms
step:1368/1880 train_time:71188ms step_avg:52.04ms
step:1369/1880 train_time:71277ms step_avg:52.06ms
step:1370/1880 train_time:71364ms step_avg:52.09ms
step:1371/1880 train_time:71452ms step_avg:52.12ms
step:1372/1880 train_time:71541ms step_avg:52.14ms
step:1373/1880 train_time:71631ms step_avg:52.17ms
step:1374/1880 train_time:71718ms step_avg:52.20ms
step:1375/1880 train_time:71805ms step_avg:52.22ms
step:1376/1880 train_time:71893ms step_avg:52.25ms
step:1377/1880 train_time:71983ms step_avg:52.28ms
step:1378/1880 train_time:72071ms step_avg:52.30ms
step:1379/1880 train_time:72160ms step_avg:52.33ms
step:1380/1880 train_time:72248ms step_avg:52.35ms
step:1381/1880 train_time:72336ms step_avg:52.38ms
step:1382/1880 train_time:72423ms step_avg:52.40ms
step:1383/1880 train_time:72512ms step_avg:52.43ms
step:1384/1880 train_time:72601ms step_avg:52.46ms
step:1385/1880 train_time:72689ms step_avg:52.48ms
step:1386/1880 train_time:72777ms step_avg:52.51ms
step:1387/1880 train_time:72864ms step_avg:52.53ms
step:1388/1880 train_time:72952ms step_avg:52.56ms
step:1389/1880 train_time:73041ms step_avg:52.59ms
step:1390/1880 train_time:73130ms step_avg:52.61ms
step:1391/1880 train_time:73218ms step_avg:52.64ms
step:1392/1880 train_time:73305ms step_avg:52.66ms
step:1393/1880 train_time:73393ms step_avg:52.69ms
step:1394/1880 train_time:73482ms step_avg:52.71ms
step:1395/1880 train_time:73570ms step_avg:52.74ms
step:1396/1880 train_time:73658ms step_avg:52.76ms
step:1397/1880 train_time:73746ms step_avg:52.79ms
step:1398/1880 train_time:73834ms step_avg:52.81ms
step:1399/1880 train_time:73922ms step_avg:52.84ms
step:1400/1880 train_time:74010ms step_avg:52.86ms
step:1401/1880 train_time:74098ms step_avg:52.89ms
step:1402/1880 train_time:74186ms step_avg:52.91ms
step:1403/1880 train_time:74274ms step_avg:52.94ms
step:1404/1880 train_time:74362ms step_avg:52.96ms
step:1405/1880 train_time:74450ms step_avg:52.99ms
step:1406/1880 train_time:74540ms step_avg:53.02ms
step:1407/1880 train_time:74627ms step_avg:53.04ms
step:1408/1880 train_time:74714ms step_avg:53.06ms
step:1409/1880 train_time:74802ms step_avg:53.09ms
step:1410/1880 train_time:74889ms step_avg:53.11ms
step:1411/1880 train_time:74977ms step_avg:53.14ms
step:1412/1880 train_time:75064ms step_avg:53.16ms
step:1413/1880 train_time:75152ms step_avg:53.19ms
step:1414/1880 train_time:75241ms step_avg:53.21ms
step:1415/1880 train_time:75329ms step_avg:53.24ms
step:1416/1880 train_time:75419ms step_avg:53.26ms
step:1417/1880 train_time:75505ms step_avg:53.28ms
step:1418/1880 train_time:75592ms step_avg:53.31ms
step:1419/1880 train_time:75681ms step_avg:53.33ms
step:1420/1880 train_time:75769ms step_avg:53.36ms
step:1421/1880 train_time:75857ms step_avg:53.38ms
step:1422/1880 train_time:75944ms step_avg:53.41ms
step:1423/1880 train_time:76032ms step_avg:53.43ms
step:1424/1880 train_time:76121ms step_avg:53.46ms
step:1425/1880 train_time:76209ms step_avg:53.48ms
step:1426/1880 train_time:76298ms step_avg:53.50ms
step:1427/1880 train_time:76385ms step_avg:53.53ms
step:1428/1880 train_time:76473ms step_avg:53.55ms
step:1429/1880 train_time:76561ms step_avg:53.58ms
step:1430/1880 train_time:76648ms step_avg:53.60ms
step:1431/1880 train_time:76737ms step_avg:53.62ms
step:1432/1880 train_time:76825ms step_avg:53.65ms
step:1433/1880 train_time:76913ms step_avg:53.67ms
step:1434/1880 train_time:77001ms step_avg:53.70ms
step:1435/1880 train_time:77089ms step_avg:53.72ms
step:1436/1880 train_time:77177ms step_avg:53.74ms
step:1437/1880 train_time:77265ms step_avg:53.77ms
step:1438/1880 train_time:77353ms step_avg:53.79ms
step:1439/1880 train_time:77442ms step_avg:53.82ms
step:1440/1880 train_time:77530ms step_avg:53.84ms
step:1441/1880 train_time:77618ms step_avg:53.86ms
step:1442/1880 train_time:77706ms step_avg:53.89ms
step:1443/1880 train_time:77793ms step_avg:53.91ms
step:1444/1880 train_time:77881ms step_avg:53.93ms
step:1445/1880 train_time:77969ms step_avg:53.96ms
step:1446/1880 train_time:78057ms step_avg:53.98ms
step:1447/1880 train_time:78145ms step_avg:54.00ms
step:1448/1880 train_time:78234ms step_avg:54.03ms
step:1449/1880 train_time:78322ms step_avg:54.05ms
step:1450/1880 train_time:78410ms step_avg:54.08ms
step:1451/1880 train_time:78498ms step_avg:54.10ms
step:1452/1880 train_time:78585ms step_avg:54.12ms
step:1453/1880 train_time:78673ms step_avg:54.15ms
step:1454/1880 train_time:78761ms step_avg:54.17ms
step:1455/1880 train_time:78849ms step_avg:54.19ms
step:1456/1880 train_time:78938ms step_avg:54.22ms
step:1457/1880 train_time:79025ms step_avg:54.24ms
step:1458/1880 train_time:79113ms step_avg:54.26ms
step:1459/1880 train_time:79202ms step_avg:54.29ms
step:1460/1880 train_time:79290ms step_avg:54.31ms
step:1461/1880 train_time:79379ms step_avg:54.33ms
step:1462/1880 train_time:79466ms step_avg:54.35ms
step:1463/1880 train_time:79554ms step_avg:54.38ms
step:1464/1880 train_time:79642ms step_avg:54.40ms
step:1465/1880 train_time:79730ms step_avg:54.42ms
step:1466/1880 train_time:79817ms step_avg:54.45ms
step:1467/1880 train_time:79906ms step_avg:54.47ms
step:1468/1880 train_time:79993ms step_avg:54.49ms
step:1469/1880 train_time:80082ms step_avg:54.51ms
step:1470/1880 train_time:80171ms step_avg:54.54ms
step:1471/1880 train_time:80260ms step_avg:54.56ms
step:1472/1880 train_time:80348ms step_avg:54.58ms
step:1473/1880 train_time:80435ms step_avg:54.61ms
step:1474/1880 train_time:80522ms step_avg:54.63ms
step:1475/1880 train_time:80610ms step_avg:54.65ms
step:1476/1880 train_time:80699ms step_avg:54.67ms
step:1477/1880 train_time:80787ms step_avg:54.70ms
step:1478/1880 train_time:80874ms step_avg:54.72ms
step:1479/1880 train_time:80962ms step_avg:54.74ms
step:1480/1880 train_time:81050ms step_avg:54.76ms
step:1481/1880 train_time:81138ms step_avg:54.79ms
step:1482/1880 train_time:81225ms step_avg:54.81ms
step:1483/1880 train_time:81315ms step_avg:54.83ms
step:1484/1880 train_time:81403ms step_avg:54.85ms
step:1485/1880 train_time:81491ms step_avg:54.88ms
step:1486/1880 train_time:81579ms step_avg:54.90ms
step:1487/1880 train_time:81667ms step_avg:54.92ms
step:1488/1880 train_time:81755ms step_avg:54.94ms
step:1489/1880 train_time:81842ms step_avg:54.96ms
step:1490/1880 train_time:81931ms step_avg:54.99ms
step:1491/1880 train_time:82019ms step_avg:55.01ms
step:1492/1880 train_time:82106ms step_avg:55.03ms
step:1493/1880 train_time:82194ms step_avg:55.05ms
step:1494/1880 train_time:82283ms step_avg:55.08ms
step:1495/1880 train_time:82371ms step_avg:55.10ms
step:1496/1880 train_time:82459ms step_avg:55.12ms
step:1497/1880 train_time:82547ms step_avg:55.14ms
step:1498/1880 train_time:82635ms step_avg:55.16ms
step:1499/1880 train_time:82723ms step_avg:55.19ms
step:1500/1880 train_time:82811ms step_avg:55.21ms
step:1500/1880 val_loss:3.4105 train_time:82901ms step_avg:55.27ms
step:1501/1880 train_time:82922ms step_avg:55.24ms
step:1502/1880 train_time:82994ms step_avg:55.26ms
step:1503/1880 train_time:83084ms step_avg:55.28ms
step:1504/1880 train_time:83172ms step_avg:55.30ms
step:1505/1880 train_time:83259ms step_avg:55.32ms
step:1506/1880 train_time:83346ms step_avg:55.34ms
step:1507/1880 train_time:83434ms step_avg:55.36ms
step:1508/1880 train_time:83521ms step_avg:55.39ms
step:1509/1880 train_time:83609ms step_avg:55.41ms
step:1510/1880 train_time:83695ms step_avg:55.43ms
step:1511/1880 train_time:83783ms step_avg:55.45ms
step:1512/1880 train_time:83874ms step_avg:55.47ms
step:1513/1880 train_time:83964ms step_avg:55.49ms
step:1514/1880 train_time:84053ms step_avg:55.52ms
step:1515/1880 train_time:84142ms step_avg:55.54ms
step:1516/1880 train_time:84229ms step_avg:55.56ms
step:1517/1880 train_time:84317ms step_avg:55.58ms
step:1518/1880 train_time:84403ms step_avg:55.60ms
step:1519/1880 train_time:84491ms step_avg:55.62ms
step:1520/1880 train_time:84577ms step_avg:55.64ms
step:1521/1880 train_time:84664ms step_avg:55.66ms
step:1522/1880 train_time:84752ms step_avg:55.68ms
step:1523/1880 train_time:84841ms step_avg:55.71ms
step:1524/1880 train_time:84930ms step_avg:55.73ms
step:1525/1880 train_time:85020ms step_avg:55.75ms
step:1526/1880 train_time:85108ms step_avg:55.77ms
step:1527/1880 train_time:85196ms step_avg:55.79ms
step:1528/1880 train_time:85284ms step_avg:55.81ms
step:1529/1880 train_time:85373ms step_avg:55.84ms
step:1530/1880 train_time:85460ms step_avg:55.86ms
step:1531/1880 train_time:85548ms step_avg:55.88ms
step:1532/1880 train_time:85634ms step_avg:55.90ms
step:1533/1880 train_time:85722ms step_avg:55.92ms
step:1534/1880 train_time:85810ms step_avg:55.94ms
step:1535/1880 train_time:85899ms step_avg:55.96ms
step:1536/1880 train_time:85989ms step_avg:55.98ms
step:1537/1880 train_time:86077ms step_avg:56.00ms
step:1538/1880 train_time:86165ms step_avg:56.02ms
step:1539/1880 train_time:86253ms step_avg:56.05ms
step:1540/1880 train_time:86341ms step_avg:56.07ms
step:1541/1880 train_time:86428ms step_avg:56.09ms
step:1542/1880 train_time:86515ms step_avg:56.11ms
step:1543/1880 train_time:86603ms step_avg:56.13ms
step:1544/1880 train_time:86691ms step_avg:56.15ms
step:1545/1880 train_time:86778ms step_avg:56.17ms
step:1546/1880 train_time:86867ms step_avg:56.19ms
step:1547/1880 train_time:86955ms step_avg:56.21ms
step:1548/1880 train_time:87044ms step_avg:56.23ms
step:1549/1880 train_time:87133ms step_avg:56.25ms
step:1550/1880 train_time:87221ms step_avg:56.27ms
step:1551/1880 train_time:87310ms step_avg:56.29ms
step:1552/1880 train_time:87397ms step_avg:56.31ms
step:1553/1880 train_time:87484ms step_avg:56.33ms
step:1554/1880 train_time:87573ms step_avg:56.35ms
step:1555/1880 train_time:87660ms step_avg:56.37ms
step:1556/1880 train_time:87748ms step_avg:56.39ms
step:1557/1880 train_time:87835ms step_avg:56.41ms
step:1558/1880 train_time:87924ms step_avg:56.43ms
step:1559/1880 train_time:88013ms step_avg:56.45ms
step:1560/1880 train_time:88102ms step_avg:56.48ms
step:1561/1880 train_time:88190ms step_avg:56.50ms
step:1562/1880 train_time:88278ms step_avg:56.52ms
step:1563/1880 train_time:88367ms step_avg:56.54ms
step:1564/1880 train_time:88454ms step_avg:56.56ms
step:1565/1880 train_time:88542ms step_avg:56.58ms
step:1566/1880 train_time:88629ms step_avg:56.60ms
step:1567/1880 train_time:88717ms step_avg:56.62ms
step:1568/1880 train_time:88805ms step_avg:56.64ms
step:1569/1880 train_time:88894ms step_avg:56.66ms
step:1570/1880 train_time:88982ms step_avg:56.68ms
step:1571/1880 train_time:89071ms step_avg:56.70ms
step:1572/1880 train_time:89158ms step_avg:56.72ms
step:1573/1880 train_time:89247ms step_avg:56.74ms
step:1574/1880 train_time:89334ms step_avg:56.76ms
step:1575/1880 train_time:89422ms step_avg:56.78ms
step:1576/1880 train_time:89510ms step_avg:56.80ms
step:1577/1880 train_time:89598ms step_avg:56.82ms
step:1578/1880 train_time:89685ms step_avg:56.83ms
step:1579/1880 train_time:89774ms step_avg:56.85ms
step:1580/1880 train_time:89862ms step_avg:56.87ms
step:1581/1880 train_time:89952ms step_avg:56.90ms
step:1582/1880 train_time:90039ms step_avg:56.91ms
step:1583/1880 train_time:90127ms step_avg:56.93ms
step:1584/1880 train_time:90215ms step_avg:56.95ms
step:1585/1880 train_time:90303ms step_avg:56.97ms
step:1586/1880 train_time:90392ms step_avg:56.99ms
step:1587/1880 train_time:90479ms step_avg:57.01ms
step:1588/1880 train_time:90567ms step_avg:57.03ms
step:1589/1880 train_time:90655ms step_avg:57.05ms
step:1590/1880 train_time:90743ms step_avg:57.07ms
step:1591/1880 train_time:90831ms step_avg:57.09ms
step:1592/1880 train_time:90918ms step_avg:57.11ms
step:1593/1880 train_time:91008ms step_avg:57.13ms
step:1594/1880 train_time:91095ms step_avg:57.15ms
step:1595/1880 train_time:91182ms step_avg:57.17ms
step:1596/1880 train_time:91271ms step_avg:57.19ms
step:1597/1880 train_time:91359ms step_avg:57.21ms
step:1598/1880 train_time:91446ms step_avg:57.23ms
step:1599/1880 train_time:91535ms step_avg:57.25ms
step:1600/1880 train_time:91623ms step_avg:57.26ms
step:1601/1880 train_time:91711ms step_avg:57.28ms
step:1602/1880 train_time:91798ms step_avg:57.30ms
step:1603/1880 train_time:91887ms step_avg:57.32ms
step:1604/1880 train_time:91975ms step_avg:57.34ms
step:1605/1880 train_time:92063ms step_avg:57.36ms
step:1606/1880 train_time:92151ms step_avg:57.38ms
step:1607/1880 train_time:92239ms step_avg:57.40ms
step:1608/1880 train_time:92327ms step_avg:57.42ms
step:1609/1880 train_time:92416ms step_avg:57.44ms
step:1610/1880 train_time:92505ms step_avg:57.46ms
step:1611/1880 train_time:92594ms step_avg:57.48ms
step:1612/1880 train_time:92681ms step_avg:57.49ms
step:1613/1880 train_time:92770ms step_avg:57.51ms
step:1614/1880 train_time:92857ms step_avg:57.53ms
step:1615/1880 train_time:92946ms step_avg:57.55ms
step:1616/1880 train_time:93033ms step_avg:57.57ms
step:1617/1880 train_time:93122ms step_avg:57.59ms
step:1618/1880 train_time:93210ms step_avg:57.61ms
step:1619/1880 train_time:93297ms step_avg:57.63ms
step:1620/1880 train_time:93385ms step_avg:57.65ms
step:1621/1880 train_time:93475ms step_avg:57.66ms
step:1622/1880 train_time:93563ms step_avg:57.68ms
step:1623/1880 train_time:93651ms step_avg:57.70ms
step:1624/1880 train_time:93738ms step_avg:57.72ms
step:1625/1880 train_time:93826ms step_avg:57.74ms
step:1626/1880 train_time:93914ms step_avg:57.76ms
step:1627/1880 train_time:94003ms step_avg:57.78ms
step:1628/1880 train_time:94091ms step_avg:57.80ms
step:1629/1880 train_time:94178ms step_avg:57.81ms
step:1630/1880 train_time:94266ms step_avg:57.83ms
step:1631/1880 train_time:94355ms step_avg:57.85ms
step:1632/1880 train_time:94443ms step_avg:57.87ms
step:1633/1880 train_time:94531ms step_avg:57.89ms
step:1634/1880 train_time:94619ms step_avg:57.91ms
step:1635/1880 train_time:94708ms step_avg:57.93ms
step:1636/1880 train_time:94795ms step_avg:57.94ms
step:1637/1880 train_time:94883ms step_avg:57.96ms
step:1638/1880 train_time:94971ms step_avg:57.98ms
step:1639/1880 train_time:95060ms step_avg:58.00ms
step:1640/1880 train_time:95148ms step_avg:58.02ms
step:1641/1880 train_time:95236ms step_avg:58.04ms
step:1642/1880 train_time:95324ms step_avg:58.05ms
step:1643/1880 train_time:95413ms step_avg:58.07ms
step:1644/1880 train_time:95501ms step_avg:58.09ms
step:1645/1880 train_time:95589ms step_avg:58.11ms
step:1646/1880 train_time:95677ms step_avg:58.13ms
step:1647/1880 train_time:95764ms step_avg:58.14ms
step:1648/1880 train_time:95852ms step_avg:58.16ms
step:1649/1880 train_time:95941ms step_avg:58.18ms
step:1650/1880 train_time:96029ms step_avg:58.20ms
step:1651/1880 train_time:96117ms step_avg:58.22ms
step:1652/1880 train_time:96205ms step_avg:58.24ms
step:1653/1880 train_time:96294ms step_avg:58.25ms
step:1654/1880 train_time:96381ms step_avg:58.27ms
step:1655/1880 train_time:96470ms step_avg:58.29ms
step:1656/1880 train_time:96557ms step_avg:58.31ms
step:1657/1880 train_time:96646ms step_avg:58.33ms
step:1658/1880 train_time:96733ms step_avg:58.34ms
step:1659/1880 train_time:96821ms step_avg:58.36ms
step:1660/1880 train_time:96909ms step_avg:58.38ms
step:1661/1880 train_time:96997ms step_avg:58.40ms
step:1662/1880 train_time:97086ms step_avg:58.41ms
step:1663/1880 train_time:97175ms step_avg:58.43ms
step:1664/1880 train_time:97263ms step_avg:58.45ms
step:1665/1880 train_time:97351ms step_avg:58.47ms
step:1666/1880 train_time:97438ms step_avg:58.49ms
step:1667/1880 train_time:97527ms step_avg:58.50ms
step:1668/1880 train_time:97615ms step_avg:58.52ms
step:1669/1880 train_time:97703ms step_avg:58.54ms
step:1670/1880 train_time:97793ms step_avg:58.56ms
step:1671/1880 train_time:97880ms step_avg:58.58ms
step:1672/1880 train_time:97967ms step_avg:58.59ms
step:1673/1880 train_time:98055ms step_avg:58.61ms
step:1674/1880 train_time:98143ms step_avg:58.63ms
step:1675/1880 train_time:98232ms step_avg:58.65ms
step:1676/1880 train_time:98320ms step_avg:58.66ms
step:1677/1880 train_time:98409ms step_avg:58.68ms
step:1678/1880 train_time:98496ms step_avg:58.70ms
step:1679/1880 train_time:98585ms step_avg:58.72ms
step:1680/1880 train_time:98675ms step_avg:58.73ms
step:1681/1880 train_time:98764ms step_avg:58.75ms
step:1682/1880 train_time:98851ms step_avg:58.77ms
step:1683/1880 train_time:98938ms step_avg:58.79ms
step:1684/1880 train_time:99027ms step_avg:58.80ms
step:1685/1880 train_time:99115ms step_avg:58.82ms
step:1686/1880 train_time:99204ms step_avg:58.84ms
step:1687/1880 train_time:99293ms step_avg:58.86ms
step:1688/1880 train_time:99381ms step_avg:58.88ms
step:1689/1880 train_time:99470ms step_avg:58.89ms
step:1690/1880 train_time:99556ms step_avg:58.91ms
step:1691/1880 train_time:99645ms step_avg:58.93ms
step:1692/1880 train_time:99733ms step_avg:58.94ms
step:1693/1880 train_time:99821ms step_avg:58.96ms
step:1694/1880 train_time:99909ms step_avg:58.98ms
step:1695/1880 train_time:99997ms step_avg:59.00ms
step:1696/1880 train_time:100085ms step_avg:59.01ms
step:1697/1880 train_time:100174ms step_avg:59.03ms
step:1698/1880 train_time:100262ms step_avg:59.05ms
step:1699/1880 train_time:100353ms step_avg:59.07ms
step:1700/1880 train_time:100439ms step_avg:59.08ms
step:1701/1880 train_time:100526ms step_avg:59.10ms
step:1702/1880 train_time:100614ms step_avg:59.12ms
step:1703/1880 train_time:100701ms step_avg:59.13ms
step:1704/1880 train_time:100788ms step_avg:59.15ms
step:1705/1880 train_time:100876ms step_avg:59.16ms
step:1706/1880 train_time:100965ms step_avg:59.18ms
step:1707/1880 train_time:101053ms step_avg:59.20ms
step:1708/1880 train_time:101141ms step_avg:59.22ms
step:1709/1880 train_time:101229ms step_avg:59.23ms
step:1710/1880 train_time:101317ms step_avg:59.25ms
step:1711/1880 train_time:101405ms step_avg:59.27ms
step:1712/1880 train_time:101493ms step_avg:59.28ms
step:1713/1880 train_time:101582ms step_avg:59.30ms
step:1714/1880 train_time:101671ms step_avg:59.32ms
step:1715/1880 train_time:101758ms step_avg:59.33ms
step:1716/1880 train_time:101847ms step_avg:59.35ms
step:1717/1880 train_time:101935ms step_avg:59.37ms
step:1718/1880 train_time:102023ms step_avg:59.38ms
step:1719/1880 train_time:102112ms step_avg:59.40ms
step:1720/1880 train_time:102199ms step_avg:59.42ms
step:1721/1880 train_time:102287ms step_avg:59.43ms
step:1722/1880 train_time:102376ms step_avg:59.45ms
step:1723/1880 train_time:102464ms step_avg:59.47ms
step:1724/1880 train_time:102552ms step_avg:59.48ms
step:1725/1880 train_time:102640ms step_avg:59.50ms
step:1726/1880 train_time:102728ms step_avg:59.52ms
step:1727/1880 train_time:102815ms step_avg:59.53ms
step:1728/1880 train_time:102904ms step_avg:59.55ms
step:1729/1880 train_time:102993ms step_avg:59.57ms
step:1730/1880 train_time:103082ms step_avg:59.58ms
step:1731/1880 train_time:103170ms step_avg:59.60ms
step:1732/1880 train_time:103256ms step_avg:59.62ms
step:1733/1880 train_time:103344ms step_avg:59.63ms
step:1734/1880 train_time:103431ms step_avg:59.65ms
step:1735/1880 train_time:103520ms step_avg:59.67ms
step:1736/1880 train_time:103608ms step_avg:59.68ms
step:1737/1880 train_time:103696ms step_avg:59.70ms
step:1738/1880 train_time:103784ms step_avg:59.71ms
step:1739/1880 train_time:103872ms step_avg:59.73ms
step:1740/1880 train_time:103960ms step_avg:59.75ms
step:1741/1880 train_time:104049ms step_avg:59.76ms
step:1742/1880 train_time:104137ms step_avg:59.78ms
step:1743/1880 train_time:104225ms step_avg:59.80ms
step:1744/1880 train_time:104313ms step_avg:59.81ms
step:1745/1880 train_time:104401ms step_avg:59.83ms
step:1746/1880 train_time:104490ms step_avg:59.85ms
step:1747/1880 train_time:104578ms step_avg:59.86ms
step:1748/1880 train_time:104666ms step_avg:59.88ms
step:1749/1880 train_time:104755ms step_avg:59.89ms
step:1750/1880 train_time:104843ms step_avg:59.91ms
step:1750/1880 val_loss:3.3152 train_time:104933ms step_avg:59.96ms
step:1751/1880 train_time:104954ms step_avg:59.94ms
step:1752/1880 train_time:105023ms step_avg:59.94ms
step:1753/1880 train_time:105115ms step_avg:59.96ms
step:1754/1880 train_time:105202ms step_avg:59.98ms
step:1755/1880 train_time:105288ms step_avg:59.99ms
step:1756/1880 train_time:105375ms step_avg:60.01ms
step:1757/1880 train_time:105462ms step_avg:60.02ms
step:1758/1880 train_time:105548ms step_avg:60.04ms
step:1759/1880 train_time:105636ms step_avg:60.05ms
step:1760/1880 train_time:105724ms step_avg:60.07ms
step:1761/1880 train_time:105811ms step_avg:60.09ms
step:1762/1880 train_time:105900ms step_avg:60.10ms
step:1763/1880 train_time:105991ms step_avg:60.12ms
step:1764/1880 train_time:106081ms step_avg:60.14ms
step:1765/1880 train_time:106170ms step_avg:60.15ms
step:1766/1880 train_time:106257ms step_avg:60.17ms
step:1767/1880 train_time:106345ms step_avg:60.18ms
step:1768/1880 train_time:106433ms step_avg:60.20ms
step:1769/1880 train_time:106520ms step_avg:60.21ms
step:1770/1880 train_time:106608ms step_avg:60.23ms
step:1771/1880 train_time:106696ms step_avg:60.25ms
step:1772/1880 train_time:106784ms step_avg:60.26ms
step:1773/1880 train_time:106872ms step_avg:60.28ms
step:1774/1880 train_time:106961ms step_avg:60.29ms
step:1775/1880 train_time:107052ms step_avg:60.31ms
step:1776/1880 train_time:107140ms step_avg:60.33ms
step:1777/1880 train_time:107228ms step_avg:60.34ms
step:1778/1880 train_time:107315ms step_avg:60.36ms
step:1779/1880 train_time:107404ms step_avg:60.37ms
step:1780/1880 train_time:107491ms step_avg:60.39ms
step:1781/1880 train_time:107578ms step_avg:60.40ms
step:1782/1880 train_time:107665ms step_avg:60.42ms
step:1783/1880 train_time:107754ms step_avg:60.43ms
step:1784/1880 train_time:107841ms step_avg:60.45ms
step:1785/1880 train_time:107931ms step_avg:60.47ms
step:1786/1880 train_time:108019ms step_avg:60.48ms
step:1787/1880 train_time:108107ms step_avg:60.50ms
step:1788/1880 train_time:108196ms step_avg:60.51ms
step:1789/1880 train_time:108285ms step_avg:60.53ms
step:1790/1880 train_time:108372ms step_avg:60.54ms
step:1791/1880 train_time:108460ms step_avg:60.56ms
step:1792/1880 train_time:108547ms step_avg:60.57ms
step:1793/1880 train_time:108635ms step_avg:60.59ms
step:1794/1880 train_time:108723ms step_avg:60.60ms
step:1795/1880 train_time:108811ms step_avg:60.62ms
step:1796/1880 train_time:108900ms step_avg:60.63ms
step:1797/1880 train_time:108989ms step_avg:60.65ms
step:1798/1880 train_time:109077ms step_avg:60.67ms
step:1799/1880 train_time:109165ms step_avg:60.68ms
step:1800/1880 train_time:109254ms step_avg:60.70ms
step:1801/1880 train_time:109341ms step_avg:60.71ms
step:1802/1880 train_time:109429ms step_avg:60.73ms
step:1803/1880 train_time:109517ms step_avg:60.74ms
step:1804/1880 train_time:109604ms step_avg:60.76ms
step:1805/1880 train_time:109692ms step_avg:60.77ms
step:1806/1880 train_time:109781ms step_avg:60.79ms
step:1807/1880 train_time:109867ms step_avg:60.80ms
step:1808/1880 train_time:109955ms step_avg:60.82ms
step:1809/1880 train_time:110044ms step_avg:60.83ms
step:1810/1880 train_time:110133ms step_avg:60.85ms
step:1811/1880 train_time:110221ms step_avg:60.86ms
step:1812/1880 train_time:110309ms step_avg:60.88ms
step:1813/1880 train_time:110398ms step_avg:60.89ms
step:1814/1880 train_time:110486ms step_avg:60.91ms
step:1815/1880 train_time:110575ms step_avg:60.92ms
step:1816/1880 train_time:110662ms step_avg:60.94ms
step:1817/1880 train_time:110750ms step_avg:60.95ms
step:1818/1880 train_time:110838ms step_avg:60.97ms
step:1819/1880 train_time:110927ms step_avg:60.98ms
step:1820/1880 train_time:111016ms step_avg:61.00ms
step:1821/1880 train_time:111105ms step_avg:61.01ms
step:1822/1880 train_time:111193ms step_avg:61.03ms
step:1823/1880 train_time:111281ms step_avg:61.04ms
step:1824/1880 train_time:111368ms step_avg:61.06ms
step:1825/1880 train_time:111456ms step_avg:61.07ms
step:1826/1880 train_time:111543ms step_avg:61.09ms
step:1827/1880 train_time:111631ms step_avg:61.10ms
step:1828/1880 train_time:111718ms step_avg:61.12ms
step:1829/1880 train_time:111807ms step_avg:61.13ms
step:1830/1880 train_time:111895ms step_avg:61.14ms
step:1831/1880 train_time:111982ms step_avg:61.16ms
step:1832/1880 train_time:112071ms step_avg:61.17ms
step:1833/1880 train_time:112159ms step_avg:61.19ms
step:1834/1880 train_time:112247ms step_avg:61.20ms
step:1835/1880 train_time:112336ms step_avg:61.22ms
step:1836/1880 train_time:112424ms step_avg:61.23ms
step:1837/1880 train_time:112512ms step_avg:61.25ms
step:1838/1880 train_time:112599ms step_avg:61.26ms
step:1839/1880 train_time:112688ms step_avg:61.28ms
step:1840/1880 train_time:112776ms step_avg:61.29ms
step:1841/1880 train_time:112864ms step_avg:61.31ms
step:1842/1880 train_time:112953ms step_avg:61.32ms
step:1843/1880 train_time:113042ms step_avg:61.34ms
step:1844/1880 train_time:113130ms step_avg:61.35ms
step:1845/1880 train_time:113219ms step_avg:61.37ms
step:1846/1880 train_time:113307ms step_avg:61.38ms
step:1847/1880 train_time:113396ms step_avg:61.39ms
step:1848/1880 train_time:113484ms step_avg:61.41ms
step:1849/1880 train_time:113572ms step_avg:61.42ms
step:1850/1880 train_time:113658ms step_avg:61.44ms
step:1851/1880 train_time:113746ms step_avg:61.45ms
step:1852/1880 train_time:113835ms step_avg:61.47ms
step:1853/1880 train_time:113923ms step_avg:61.48ms
step:1854/1880 train_time:114012ms step_avg:61.50ms
step:1855/1880 train_time:114101ms step_avg:61.51ms
step:1856/1880 train_time:114190ms step_avg:61.52ms
step:1857/1880 train_time:114278ms step_avg:61.54ms
step:1858/1880 train_time:114368ms step_avg:61.55ms
step:1859/1880 train_time:114456ms step_avg:61.57ms
step:1860/1880 train_time:114545ms step_avg:61.58ms
step:1861/1880 train_time:114634ms step_avg:61.60ms
step:1862/1880 train_time:114721ms step_avg:61.61ms
step:1863/1880 train_time:114810ms step_avg:61.63ms
step:1864/1880 train_time:114897ms step_avg:61.64ms
step:1865/1880 train_time:114988ms step_avg:61.66ms
step:1866/1880 train_time:115075ms step_avg:61.67ms
step:1867/1880 train_time:115163ms step_avg:61.68ms
step:1868/1880 train_time:115252ms step_avg:61.70ms
step:1869/1880 train_time:115340ms step_avg:61.71ms
step:1870/1880 train_time:115429ms step_avg:61.73ms
step:1871/1880 train_time:115517ms step_avg:61.74ms
step:1872/1880 train_time:115604ms step_avg:61.75ms
step:1873/1880 train_time:115693ms step_avg:61.77ms
step:1874/1880 train_time:115781ms step_avg:61.78ms
step:1875/1880 train_time:115869ms step_avg:61.80ms
step:1876/1880 train_time:115957ms step_avg:61.81ms
step:1877/1880 train_time:116045ms step_avg:61.82ms
step:1878/1880 train_time:116134ms step_avg:61.84ms
step:1879/1880 train_time:116223ms step_avg:61.85ms
step:1880/1880 train_time:116312ms step_avg:61.87ms
step:1880/1880 val_loss:3.2803 train_time:116402ms step_avg:61.92ms
peak memory allocated: 29709 MiB reserved: 44618 MiB

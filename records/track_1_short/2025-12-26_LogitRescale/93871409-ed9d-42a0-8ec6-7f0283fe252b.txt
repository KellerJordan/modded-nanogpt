import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits+5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1840  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec 26 23:14:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     69982      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     69983      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     69984      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     69985      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     69986      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     69987      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     69988      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     69989      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 613, 614, 615, 1226, 1227, 1228, 1839, 1840, 1841] for warmup
Resetting Model
step:0/1880 val_loss:10.8307 train_time:0ms step_avg:0.03ms
step:1/1880 train_time:80ms step_avg:79.77ms
step:2/1880 train_time:104ms step_avg:52.01ms
step:3/1880 train_time:125ms step_avg:41.60ms
step:4/1880 train_time:159ms step_avg:39.65ms
step:5/1880 train_time:192ms step_avg:38.45ms
step:6/1880 train_time:265ms step_avg:44.24ms
step:7/1880 train_time:396ms step_avg:56.58ms
step:8/1880 train_time:430ms step_avg:53.76ms
step:9/1880 train_time:464ms step_avg:51.53ms
step:10/1880 train_time:498ms step_avg:49.78ms
step:11/1880 train_time:532ms step_avg:48.33ms
step:12/1880 train_time:566ms step_avg:47.14ms
step:13/1880 train_time:600ms step_avg:46.12ms
step:14/1880 train_time:634ms step_avg:45.26ms
step:15/1880 train_time:667ms step_avg:44.49ms
step:16/1880 train_time:701ms step_avg:43.84ms
step:17/1880 train_time:735ms step_avg:43.24ms
step:18/1880 train_time:769ms step_avg:42.73ms
step:19/1880 train_time:803ms step_avg:42.27ms
step:20/1880 train_time:837ms step_avg:41.87ms
step:21/1880 train_time:871ms step_avg:41.48ms
step:22/1880 train_time:905ms step_avg:41.15ms
step:23/1880 train_time:939ms step_avg:40.85ms
step:24/1880 train_time:974ms step_avg:40.57ms
step:25/1880 train_time:1007ms step_avg:40.29ms
step:26/1880 train_time:1041ms step_avg:40.06ms
step:27/1880 train_time:1075ms step_avg:39.82ms
step:28/1880 train_time:1109ms step_avg:39.62ms
step:29/1880 train_time:1143ms step_avg:39.43ms
step:30/1880 train_time:1178ms step_avg:39.25ms
step:31/1880 train_time:1211ms step_avg:39.07ms
step:32/1880 train_time:1245ms step_avg:38.91ms
step:33/1880 train_time:1279ms step_avg:38.76ms
step:34/1880 train_time:1313ms step_avg:38.63ms
step:35/1880 train_time:1348ms step_avg:38.52ms
step:36/1880 train_time:1383ms step_avg:38.42ms
step:37/1880 train_time:1418ms step_avg:38.32ms
step:38/1880 train_time:1453ms step_avg:38.23ms
step:39/1880 train_time:1487ms step_avg:38.12ms
step:40/1880 train_time:1521ms step_avg:38.03ms
step:41/1880 train_time:1555ms step_avg:37.93ms
step:42/1880 train_time:1589ms step_avg:37.84ms
step:43/1880 train_time:1624ms step_avg:37.76ms
step:44/1880 train_time:1658ms step_avg:37.69ms
step:45/1880 train_time:1692ms step_avg:37.61ms
step:46/1880 train_time:1726ms step_avg:37.53ms
step:47/1880 train_time:1760ms step_avg:37.45ms
step:48/1880 train_time:1794ms step_avg:37.38ms
step:49/1880 train_time:1828ms step_avg:37.30ms
step:50/1880 train_time:1862ms step_avg:37.24ms
step:51/1880 train_time:1896ms step_avg:37.18ms
step:52/1880 train_time:1930ms step_avg:37.12ms
step:53/1880 train_time:1964ms step_avg:37.06ms
step:54/1880 train_time:1998ms step_avg:37.01ms
step:55/1880 train_time:2032ms step_avg:36.95ms
step:56/1880 train_time:2066ms step_avg:36.90ms
step:57/1880 train_time:2100ms step_avg:36.84ms
step:58/1880 train_time:2134ms step_avg:36.80ms
step:59/1880 train_time:2168ms step_avg:36.74ms
step:60/1880 train_time:2202ms step_avg:36.69ms
step:61/1880 train_time:2236ms step_avg:36.65ms
step:62/1880 train_time:2270ms step_avg:36.61ms
step:63/1880 train_time:2304ms step_avg:36.57ms
step:64/1880 train_time:2339ms step_avg:36.55ms
step:65/1880 train_time:2373ms step_avg:36.51ms
step:66/1880 train_time:2407ms step_avg:36.48ms
step:67/1880 train_time:2441ms step_avg:36.44ms
step:68/1880 train_time:2475ms step_avg:36.40ms
step:69/1880 train_time:2509ms step_avg:36.37ms
step:70/1880 train_time:2544ms step_avg:36.34ms
step:71/1880 train_time:2578ms step_avg:36.32ms
step:72/1880 train_time:2613ms step_avg:36.29ms
step:73/1880 train_time:2647ms step_avg:36.25ms
step:74/1880 train_time:2681ms step_avg:36.23ms
step:75/1880 train_time:2715ms step_avg:36.20ms
step:76/1880 train_time:2749ms step_avg:36.17ms
step:77/1880 train_time:2783ms step_avg:36.15ms
step:78/1880 train_time:2818ms step_avg:36.12ms
step:79/1880 train_time:2852ms step_avg:36.10ms
step:80/1880 train_time:2886ms step_avg:36.07ms
step:81/1880 train_time:2920ms step_avg:36.05ms
step:82/1880 train_time:2954ms step_avg:36.03ms
step:83/1880 train_time:2988ms step_avg:36.00ms
step:84/1880 train_time:3022ms step_avg:35.98ms
step:85/1880 train_time:3056ms step_avg:35.95ms
step:86/1880 train_time:3090ms step_avg:35.93ms
step:87/1880 train_time:3124ms step_avg:35.91ms
step:88/1880 train_time:3158ms step_avg:35.89ms
step:89/1880 train_time:3192ms step_avg:35.86ms
step:90/1880 train_time:3226ms step_avg:35.85ms
step:91/1880 train_time:3260ms step_avg:35.82ms
step:92/1880 train_time:3294ms step_avg:35.81ms
step:93/1880 train_time:3328ms step_avg:35.78ms
step:94/1880 train_time:3362ms step_avg:35.77ms
step:95/1880 train_time:3396ms step_avg:35.75ms
step:96/1880 train_time:3430ms step_avg:35.73ms
step:97/1880 train_time:3464ms step_avg:35.71ms
step:98/1880 train_time:3499ms step_avg:35.70ms
step:99/1880 train_time:3533ms step_avg:35.68ms
step:100/1880 train_time:3567ms step_avg:35.67ms
step:101/1880 train_time:3601ms step_avg:35.65ms
step:102/1880 train_time:3635ms step_avg:35.64ms
step:103/1880 train_time:3669ms step_avg:35.62ms
step:104/1880 train_time:3703ms step_avg:35.60ms
step:105/1880 train_time:3737ms step_avg:35.59ms
step:106/1880 train_time:3771ms step_avg:35.58ms
step:107/1880 train_time:3805ms step_avg:35.56ms
step:108/1880 train_time:3839ms step_avg:35.55ms
step:109/1880 train_time:3873ms step_avg:35.53ms
step:110/1880 train_time:3907ms step_avg:35.52ms
step:111/1880 train_time:3941ms step_avg:35.51ms
step:112/1880 train_time:3976ms step_avg:35.50ms
step:113/1880 train_time:4009ms step_avg:35.48ms
step:114/1880 train_time:4043ms step_avg:35.47ms
step:115/1880 train_time:4078ms step_avg:35.46ms
step:116/1880 train_time:4112ms step_avg:35.45ms
step:117/1880 train_time:4146ms step_avg:35.43ms
step:118/1880 train_time:4180ms step_avg:35.42ms
step:119/1880 train_time:4213ms step_avg:35.41ms
step:120/1880 train_time:4247ms step_avg:35.40ms
step:121/1880 train_time:4282ms step_avg:35.39ms
step:122/1880 train_time:4316ms step_avg:35.38ms
step:123/1880 train_time:4350ms step_avg:35.37ms
step:124/1880 train_time:4384ms step_avg:35.36ms
step:125/1880 train_time:4418ms step_avg:35.35ms
step:126/1880 train_time:4453ms step_avg:35.34ms
step:127/1880 train_time:4487ms step_avg:35.33ms
step:128/1880 train_time:4521ms step_avg:35.32ms
step:129/1880 train_time:4555ms step_avg:35.31ms
step:130/1880 train_time:4590ms step_avg:35.30ms
step:131/1880 train_time:4624ms step_avg:35.30ms
step:132/1880 train_time:4658ms step_avg:35.29ms
step:133/1880 train_time:4692ms step_avg:35.28ms
step:134/1880 train_time:4727ms step_avg:35.27ms
step:135/1880 train_time:4761ms step_avg:35.27ms
step:136/1880 train_time:4795ms step_avg:35.26ms
step:137/1880 train_time:4829ms step_avg:35.25ms
step:138/1880 train_time:4863ms step_avg:35.24ms
step:139/1880 train_time:4897ms step_avg:35.23ms
step:140/1880 train_time:4931ms step_avg:35.22ms
step:141/1880 train_time:4965ms step_avg:35.21ms
step:142/1880 train_time:5000ms step_avg:35.21ms
step:143/1880 train_time:5033ms step_avg:35.20ms
step:144/1880 train_time:5067ms step_avg:35.19ms
step:145/1880 train_time:5101ms step_avg:35.18ms
step:146/1880 train_time:5135ms step_avg:35.17ms
step:147/1880 train_time:5169ms step_avg:35.16ms
step:148/1880 train_time:5203ms step_avg:35.16ms
step:149/1880 train_time:5237ms step_avg:35.15ms
step:150/1880 train_time:5271ms step_avg:35.14ms
step:151/1880 train_time:5305ms step_avg:35.13ms
step:152/1880 train_time:5340ms step_avg:35.13ms
step:153/1880 train_time:5373ms step_avg:35.12ms
step:154/1880 train_time:5407ms step_avg:35.11ms
step:155/1880 train_time:5442ms step_avg:35.11ms
step:156/1880 train_time:5476ms step_avg:35.10ms
step:157/1880 train_time:5509ms step_avg:35.09ms
step:158/1880 train_time:5543ms step_avg:35.08ms
step:159/1880 train_time:5577ms step_avg:35.08ms
step:160/1880 train_time:5611ms step_avg:35.07ms
step:161/1880 train_time:5645ms step_avg:35.06ms
step:162/1880 train_time:5679ms step_avg:35.06ms
step:163/1880 train_time:5713ms step_avg:35.05ms
step:164/1880 train_time:5747ms step_avg:35.04ms
step:165/1880 train_time:5781ms step_avg:35.04ms
step:166/1880 train_time:5815ms step_avg:35.03ms
step:167/1880 train_time:5849ms step_avg:35.02ms
step:168/1880 train_time:5883ms step_avg:35.02ms
step:169/1880 train_time:5917ms step_avg:35.01ms
step:170/1880 train_time:5952ms step_avg:35.01ms
step:171/1880 train_time:5986ms step_avg:35.00ms
step:172/1880 train_time:6020ms step_avg:35.00ms
step:173/1880 train_time:6054ms step_avg:34.99ms
step:174/1880 train_time:6088ms step_avg:34.99ms
step:175/1880 train_time:6122ms step_avg:34.98ms
step:176/1880 train_time:6156ms step_avg:34.98ms
step:177/1880 train_time:6190ms step_avg:34.97ms
step:178/1880 train_time:6224ms step_avg:34.97ms
step:179/1880 train_time:6258ms step_avg:34.96ms
step:180/1880 train_time:6292ms step_avg:34.96ms
step:181/1880 train_time:6326ms step_avg:34.95ms
step:182/1880 train_time:6360ms step_avg:34.94ms
step:183/1880 train_time:6393ms step_avg:34.94ms
step:184/1880 train_time:6428ms step_avg:34.93ms
step:185/1880 train_time:6461ms step_avg:34.93ms
step:186/1880 train_time:6495ms step_avg:34.92ms
step:187/1880 train_time:6529ms step_avg:34.91ms
step:188/1880 train_time:6563ms step_avg:34.91ms
step:189/1880 train_time:6597ms step_avg:34.90ms
step:190/1880 train_time:6631ms step_avg:34.90ms
step:191/1880 train_time:6665ms step_avg:34.90ms
step:192/1880 train_time:6699ms step_avg:34.89ms
step:193/1880 train_time:6733ms step_avg:34.88ms
step:194/1880 train_time:6767ms step_avg:34.88ms
step:195/1880 train_time:6801ms step_avg:34.88ms
step:196/1880 train_time:6835ms step_avg:34.87ms
step:197/1880 train_time:6869ms step_avg:34.87ms
step:198/1880 train_time:6903ms step_avg:34.86ms
step:199/1880 train_time:6937ms step_avg:34.86ms
step:200/1880 train_time:6971ms step_avg:34.86ms
step:201/1880 train_time:7005ms step_avg:34.85ms
step:202/1880 train_time:7039ms step_avg:34.85ms
step:203/1880 train_time:7073ms step_avg:34.84ms
step:204/1880 train_time:7107ms step_avg:34.84ms
step:205/1880 train_time:7141ms step_avg:34.83ms
step:206/1880 train_time:7175ms step_avg:34.83ms
step:207/1880 train_time:7209ms step_avg:34.82ms
step:208/1880 train_time:7243ms step_avg:34.82ms
step:209/1880 train_time:7277ms step_avg:34.82ms
step:210/1880 train_time:7311ms step_avg:34.81ms
step:211/1880 train_time:7345ms step_avg:34.81ms
step:212/1880 train_time:7379ms step_avg:34.81ms
step:213/1880 train_time:7413ms step_avg:34.80ms
step:214/1880 train_time:7447ms step_avg:34.80ms
step:215/1880 train_time:7481ms step_avg:34.79ms
step:216/1880 train_time:7515ms step_avg:34.79ms
step:217/1880 train_time:7548ms step_avg:34.79ms
step:218/1880 train_time:7583ms step_avg:34.78ms
step:219/1880 train_time:7617ms step_avg:34.78ms
step:220/1880 train_time:7651ms step_avg:34.78ms
step:221/1880 train_time:7685ms step_avg:34.78ms
step:222/1880 train_time:7720ms step_avg:34.77ms
step:223/1880 train_time:7754ms step_avg:34.77ms
step:224/1880 train_time:7788ms step_avg:34.77ms
step:225/1880 train_time:7822ms step_avg:34.76ms
step:226/1880 train_time:7856ms step_avg:34.76ms
step:227/1880 train_time:7889ms step_avg:34.75ms
step:228/1880 train_time:7924ms step_avg:34.75ms
step:229/1880 train_time:7958ms step_avg:34.75ms
step:230/1880 train_time:7992ms step_avg:34.75ms
step:231/1880 train_time:8026ms step_avg:34.74ms
step:232/1880 train_time:8060ms step_avg:34.74ms
step:233/1880 train_time:8093ms step_avg:34.74ms
step:234/1880 train_time:8127ms step_avg:34.73ms
step:235/1880 train_time:8161ms step_avg:34.73ms
step:236/1880 train_time:8195ms step_avg:34.73ms
step:237/1880 train_time:8229ms step_avg:34.72ms
step:238/1880 train_time:8263ms step_avg:34.72ms
step:239/1880 train_time:8297ms step_avg:34.72ms
step:240/1880 train_time:8331ms step_avg:34.71ms
step:241/1880 train_time:8365ms step_avg:34.71ms
step:242/1880 train_time:8399ms step_avg:34.71ms
step:243/1880 train_time:8433ms step_avg:34.70ms
step:244/1880 train_time:8467ms step_avg:34.70ms
step:245/1880 train_time:8501ms step_avg:34.70ms
step:246/1880 train_time:8535ms step_avg:34.70ms
step:247/1880 train_time:8569ms step_avg:34.69ms
step:248/1880 train_time:8603ms step_avg:34.69ms
step:249/1880 train_time:8637ms step_avg:34.69ms
step:250/1880 train_time:8672ms step_avg:34.69ms
step:250/1880 val_loss:4.6018 train_time:8709ms step_avg:34.84ms
step:251/1880 train_time:8728ms step_avg:34.77ms
step:252/1880 train_time:8748ms step_avg:34.71ms
step:253/1880 train_time:8777ms step_avg:34.69ms
step:254/1880 train_time:8811ms step_avg:34.69ms
step:255/1880 train_time:8846ms step_avg:34.69ms
step:256/1880 train_time:8881ms step_avg:34.69ms
step:257/1880 train_time:8914ms step_avg:34.69ms
step:258/1880 train_time:8949ms step_avg:34.68ms
step:259/1880 train_time:8982ms step_avg:34.68ms
step:260/1880 train_time:9017ms step_avg:34.68ms
step:261/1880 train_time:9050ms step_avg:34.68ms
step:262/1880 train_time:9085ms step_avg:34.67ms
step:263/1880 train_time:9118ms step_avg:34.67ms
step:264/1880 train_time:9152ms step_avg:34.67ms
step:265/1880 train_time:9186ms step_avg:34.66ms
step:266/1880 train_time:9220ms step_avg:34.66ms
step:267/1880 train_time:9254ms step_avg:34.66ms
step:268/1880 train_time:9288ms step_avg:34.66ms
step:269/1880 train_time:9321ms step_avg:34.65ms
step:270/1880 train_time:9355ms step_avg:34.65ms
step:271/1880 train_time:9389ms step_avg:34.65ms
step:272/1880 train_time:9423ms step_avg:34.64ms
step:273/1880 train_time:9457ms step_avg:34.64ms
step:274/1880 train_time:9491ms step_avg:34.64ms
step:275/1880 train_time:9525ms step_avg:34.64ms
step:276/1880 train_time:9559ms step_avg:34.63ms
step:277/1880 train_time:9593ms step_avg:34.63ms
step:278/1880 train_time:9627ms step_avg:34.63ms
step:279/1880 train_time:9660ms step_avg:34.62ms
step:280/1880 train_time:9694ms step_avg:34.62ms
step:281/1880 train_time:9728ms step_avg:34.62ms
step:282/1880 train_time:9762ms step_avg:34.62ms
step:283/1880 train_time:9797ms step_avg:34.62ms
step:284/1880 train_time:9831ms step_avg:34.62ms
step:285/1880 train_time:9865ms step_avg:34.62ms
step:286/1880 train_time:9899ms step_avg:34.61ms
step:287/1880 train_time:9933ms step_avg:34.61ms
step:288/1880 train_time:9968ms step_avg:34.61ms
step:289/1880 train_time:10002ms step_avg:34.61ms
step:290/1880 train_time:10036ms step_avg:34.61ms
step:291/1880 train_time:10070ms step_avg:34.60ms
step:292/1880 train_time:10104ms step_avg:34.60ms
step:293/1880 train_time:10138ms step_avg:34.60ms
step:294/1880 train_time:10172ms step_avg:34.60ms
step:295/1880 train_time:10206ms step_avg:34.60ms
step:296/1880 train_time:10240ms step_avg:34.59ms
step:297/1880 train_time:10274ms step_avg:34.59ms
step:298/1880 train_time:10307ms step_avg:34.59ms
step:299/1880 train_time:10341ms step_avg:34.59ms
step:300/1880 train_time:10375ms step_avg:34.58ms
step:301/1880 train_time:10409ms step_avg:34.58ms
step:302/1880 train_time:10443ms step_avg:34.58ms
step:303/1880 train_time:10477ms step_avg:34.58ms
step:304/1880 train_time:10511ms step_avg:34.57ms
step:305/1880 train_time:10544ms step_avg:34.57ms
step:306/1880 train_time:10578ms step_avg:34.57ms
step:307/1880 train_time:10612ms step_avg:34.57ms
step:308/1880 train_time:10646ms step_avg:34.57ms
step:309/1880 train_time:10680ms step_avg:34.56ms
step:310/1880 train_time:10714ms step_avg:34.56ms
step:311/1880 train_time:10748ms step_avg:34.56ms
step:312/1880 train_time:10782ms step_avg:34.56ms
step:313/1880 train_time:10816ms step_avg:34.56ms
step:314/1880 train_time:10851ms step_avg:34.56ms
step:315/1880 train_time:10884ms step_avg:34.55ms
step:316/1880 train_time:10919ms step_avg:34.55ms
step:317/1880 train_time:10953ms step_avg:34.55ms
step:318/1880 train_time:10987ms step_avg:34.55ms
step:319/1880 train_time:11021ms step_avg:34.55ms
step:320/1880 train_time:11055ms step_avg:34.55ms
step:321/1880 train_time:11089ms step_avg:34.55ms
step:322/1880 train_time:11123ms step_avg:34.54ms
step:323/1880 train_time:11157ms step_avg:34.54ms
step:324/1880 train_time:11191ms step_avg:34.54ms
step:325/1880 train_time:11225ms step_avg:34.54ms
step:326/1880 train_time:11259ms step_avg:34.54ms
step:327/1880 train_time:11293ms step_avg:34.54ms
step:328/1880 train_time:11327ms step_avg:34.53ms
step:329/1880 train_time:11361ms step_avg:34.53ms
step:330/1880 train_time:11395ms step_avg:34.53ms
step:331/1880 train_time:11429ms step_avg:34.53ms
step:332/1880 train_time:11463ms step_avg:34.53ms
step:333/1880 train_time:11497ms step_avg:34.53ms
step:334/1880 train_time:11531ms step_avg:34.52ms
step:335/1880 train_time:11565ms step_avg:34.52ms
step:336/1880 train_time:11599ms step_avg:34.52ms
step:337/1880 train_time:11633ms step_avg:34.52ms
step:338/1880 train_time:11667ms step_avg:34.52ms
step:339/1880 train_time:11700ms step_avg:34.51ms
step:340/1880 train_time:11735ms step_avg:34.51ms
step:341/1880 train_time:11769ms step_avg:34.51ms
step:342/1880 train_time:11803ms step_avg:34.51ms
step:343/1880 train_time:11837ms step_avg:34.51ms
step:344/1880 train_time:11871ms step_avg:34.51ms
step:345/1880 train_time:11905ms step_avg:34.51ms
step:346/1880 train_time:11940ms step_avg:34.51ms
step:347/1880 train_time:11974ms step_avg:34.51ms
step:348/1880 train_time:12008ms step_avg:34.51ms
step:349/1880 train_time:12041ms step_avg:34.50ms
step:350/1880 train_time:12075ms step_avg:34.50ms
step:351/1880 train_time:12109ms step_avg:34.50ms
step:352/1880 train_time:12143ms step_avg:34.50ms
step:353/1880 train_time:12178ms step_avg:34.50ms
step:354/1880 train_time:12212ms step_avg:34.50ms
step:355/1880 train_time:12246ms step_avg:34.49ms
step:356/1880 train_time:12280ms step_avg:34.49ms
step:357/1880 train_time:12313ms step_avg:34.49ms
step:358/1880 train_time:12347ms step_avg:34.49ms
step:359/1880 train_time:12381ms step_avg:34.49ms
step:360/1880 train_time:12415ms step_avg:34.49ms
step:361/1880 train_time:12449ms step_avg:34.49ms
step:362/1880 train_time:12483ms step_avg:34.48ms
step:363/1880 train_time:12517ms step_avg:34.48ms
step:364/1880 train_time:12552ms step_avg:34.48ms
step:365/1880 train_time:12585ms step_avg:34.48ms
step:366/1880 train_time:12619ms step_avg:34.48ms
step:367/1880 train_time:12653ms step_avg:34.48ms
step:368/1880 train_time:12687ms step_avg:34.48ms
step:369/1880 train_time:12721ms step_avg:34.47ms
step:370/1880 train_time:12755ms step_avg:34.47ms
step:371/1880 train_time:12789ms step_avg:34.47ms
step:372/1880 train_time:12823ms step_avg:34.47ms
step:373/1880 train_time:12857ms step_avg:34.47ms
step:374/1880 train_time:12891ms step_avg:34.47ms
step:375/1880 train_time:12925ms step_avg:34.47ms
step:376/1880 train_time:12959ms step_avg:34.47ms
step:377/1880 train_time:12993ms step_avg:34.46ms
step:378/1880 train_time:13027ms step_avg:34.46ms
step:379/1880 train_time:13061ms step_avg:34.46ms
step:380/1880 train_time:13096ms step_avg:34.46ms
step:381/1880 train_time:13130ms step_avg:34.46ms
step:382/1880 train_time:13164ms step_avg:34.46ms
step:383/1880 train_time:13198ms step_avg:34.46ms
step:384/1880 train_time:13232ms step_avg:34.46ms
step:385/1880 train_time:13266ms step_avg:34.46ms
step:386/1880 train_time:13300ms step_avg:34.46ms
step:387/1880 train_time:13334ms step_avg:34.45ms
step:388/1880 train_time:13368ms step_avg:34.45ms
step:389/1880 train_time:13402ms step_avg:34.45ms
step:390/1880 train_time:13436ms step_avg:34.45ms
step:391/1880 train_time:13470ms step_avg:34.45ms
step:392/1880 train_time:13504ms step_avg:34.45ms
step:393/1880 train_time:13538ms step_avg:34.45ms
step:394/1880 train_time:13572ms step_avg:34.45ms
step:395/1880 train_time:13606ms step_avg:34.45ms
step:396/1880 train_time:13640ms step_avg:34.44ms
step:397/1880 train_time:13674ms step_avg:34.44ms
step:398/1880 train_time:13708ms step_avg:34.44ms
step:399/1880 train_time:13742ms step_avg:34.44ms
step:400/1880 train_time:13776ms step_avg:34.44ms
step:401/1880 train_time:13810ms step_avg:34.44ms
step:402/1880 train_time:13844ms step_avg:34.44ms
step:403/1880 train_time:13878ms step_avg:34.44ms
step:404/1880 train_time:13912ms step_avg:34.44ms
step:405/1880 train_time:13946ms step_avg:34.43ms
step:406/1880 train_time:13980ms step_avg:34.43ms
step:407/1880 train_time:14014ms step_avg:34.43ms
step:408/1880 train_time:14048ms step_avg:34.43ms
step:409/1880 train_time:14082ms step_avg:34.43ms
step:410/1880 train_time:14116ms step_avg:34.43ms
step:411/1880 train_time:14150ms step_avg:34.43ms
step:412/1880 train_time:14184ms step_avg:34.43ms
step:413/1880 train_time:14218ms step_avg:34.43ms
step:414/1880 train_time:14253ms step_avg:34.43ms
step:415/1880 train_time:14286ms step_avg:34.42ms
step:416/1880 train_time:14320ms step_avg:34.42ms
step:417/1880 train_time:14354ms step_avg:34.42ms
step:418/1880 train_time:14388ms step_avg:34.42ms
step:419/1880 train_time:14422ms step_avg:34.42ms
step:420/1880 train_time:14456ms step_avg:34.42ms
step:421/1880 train_time:14490ms step_avg:34.42ms
step:422/1880 train_time:14524ms step_avg:34.42ms
step:423/1880 train_time:14558ms step_avg:34.42ms
step:424/1880 train_time:14592ms step_avg:34.42ms
step:425/1880 train_time:14626ms step_avg:34.41ms
step:426/1880 train_time:14660ms step_avg:34.41ms
step:427/1880 train_time:14694ms step_avg:34.41ms
step:428/1880 train_time:14728ms step_avg:34.41ms
step:429/1880 train_time:14762ms step_avg:34.41ms
step:430/1880 train_time:14796ms step_avg:34.41ms
step:431/1880 train_time:14830ms step_avg:34.41ms
step:432/1880 train_time:14864ms step_avg:34.41ms
step:433/1880 train_time:14898ms step_avg:34.41ms
step:434/1880 train_time:14932ms step_avg:34.41ms
step:435/1880 train_time:14966ms step_avg:34.40ms
step:436/1880 train_time:15000ms step_avg:34.40ms
step:437/1880 train_time:15034ms step_avg:34.40ms
step:438/1880 train_time:15068ms step_avg:34.40ms
step:439/1880 train_time:15102ms step_avg:34.40ms
step:440/1880 train_time:15136ms step_avg:34.40ms
step:441/1880 train_time:15170ms step_avg:34.40ms
step:442/1880 train_time:15205ms step_avg:34.40ms
step:443/1880 train_time:15239ms step_avg:34.40ms
step:444/1880 train_time:15273ms step_avg:34.40ms
step:445/1880 train_time:15306ms step_avg:34.40ms
step:446/1880 train_time:15341ms step_avg:34.40ms
step:447/1880 train_time:15374ms step_avg:34.39ms
step:448/1880 train_time:15409ms step_avg:34.39ms
step:449/1880 train_time:15443ms step_avg:34.39ms
step:450/1880 train_time:15477ms step_avg:34.39ms
step:451/1880 train_time:15511ms step_avg:34.39ms
step:452/1880 train_time:15545ms step_avg:34.39ms
step:453/1880 train_time:15579ms step_avg:34.39ms
step:454/1880 train_time:15613ms step_avg:34.39ms
step:455/1880 train_time:15647ms step_avg:34.39ms
step:456/1880 train_time:15681ms step_avg:34.39ms
step:457/1880 train_time:15715ms step_avg:34.39ms
step:458/1880 train_time:15749ms step_avg:34.39ms
step:459/1880 train_time:15782ms step_avg:34.38ms
step:460/1880 train_time:15816ms step_avg:34.38ms
step:461/1880 train_time:15851ms step_avg:34.38ms
step:462/1880 train_time:15885ms step_avg:34.38ms
step:463/1880 train_time:15919ms step_avg:34.38ms
step:464/1880 train_time:15953ms step_avg:34.38ms
step:465/1880 train_time:15986ms step_avg:34.38ms
step:466/1880 train_time:16020ms step_avg:34.38ms
step:467/1880 train_time:16054ms step_avg:34.38ms
step:468/1880 train_time:16088ms step_avg:34.38ms
step:469/1880 train_time:16122ms step_avg:34.37ms
step:470/1880 train_time:16156ms step_avg:34.37ms
step:471/1880 train_time:16190ms step_avg:34.37ms
step:472/1880 train_time:16224ms step_avg:34.37ms
step:473/1880 train_time:16258ms step_avg:34.37ms
step:474/1880 train_time:16292ms step_avg:34.37ms
step:475/1880 train_time:16326ms step_avg:34.37ms
step:476/1880 train_time:16360ms step_avg:34.37ms
step:477/1880 train_time:16394ms step_avg:34.37ms
step:478/1880 train_time:16428ms step_avg:34.37ms
step:479/1880 train_time:16462ms step_avg:34.37ms
step:480/1880 train_time:16496ms step_avg:34.37ms
step:481/1880 train_time:16530ms step_avg:34.37ms
step:482/1880 train_time:16564ms step_avg:34.37ms
step:483/1880 train_time:16598ms step_avg:34.36ms
step:484/1880 train_time:16632ms step_avg:34.36ms
step:485/1880 train_time:16666ms step_avg:34.36ms
step:486/1880 train_time:16700ms step_avg:34.36ms
step:487/1880 train_time:16734ms step_avg:34.36ms
step:488/1880 train_time:16768ms step_avg:34.36ms
step:489/1880 train_time:16802ms step_avg:34.36ms
step:490/1880 train_time:16836ms step_avg:34.36ms
step:491/1880 train_time:16870ms step_avg:34.36ms
step:492/1880 train_time:16905ms step_avg:34.36ms
step:493/1880 train_time:16939ms step_avg:34.36ms
step:494/1880 train_time:16973ms step_avg:34.36ms
step:495/1880 train_time:17007ms step_avg:34.36ms
step:496/1880 train_time:17041ms step_avg:34.36ms
step:497/1880 train_time:17074ms step_avg:34.35ms
step:498/1880 train_time:17108ms step_avg:34.35ms
step:499/1880 train_time:17142ms step_avg:34.35ms
step:500/1880 train_time:17177ms step_avg:34.35ms
step:500/1880 val_loss:4.2779 train_time:17214ms step_avg:34.43ms
step:501/1880 train_time:17235ms step_avg:34.40ms
step:502/1880 train_time:17254ms step_avg:34.37ms
step:503/1880 train_time:17282ms step_avg:34.36ms
step:504/1880 train_time:17318ms step_avg:34.36ms
step:505/1880 train_time:17353ms step_avg:34.36ms
step:506/1880 train_time:17388ms step_avg:34.36ms
step:507/1880 train_time:17422ms step_avg:34.36ms
step:508/1880 train_time:17456ms step_avg:34.36ms
step:509/1880 train_time:17490ms step_avg:34.36ms
step:510/1880 train_time:17525ms step_avg:34.36ms
step:511/1880 train_time:17558ms step_avg:34.36ms
step:512/1880 train_time:17592ms step_avg:34.36ms
step:513/1880 train_time:17626ms step_avg:34.36ms
step:514/1880 train_time:17660ms step_avg:34.36ms
step:515/1880 train_time:17694ms step_avg:34.36ms
step:516/1880 train_time:17728ms step_avg:34.36ms
step:517/1880 train_time:17761ms step_avg:34.35ms
step:518/1880 train_time:17795ms step_avg:34.35ms
step:519/1880 train_time:17829ms step_avg:34.35ms
step:520/1880 train_time:17863ms step_avg:34.35ms
step:521/1880 train_time:17897ms step_avg:34.35ms
step:522/1880 train_time:17930ms step_avg:34.35ms
step:523/1880 train_time:17964ms step_avg:34.35ms
step:524/1880 train_time:17998ms step_avg:34.35ms
step:525/1880 train_time:18032ms step_avg:34.35ms
step:526/1880 train_time:18066ms step_avg:34.35ms
step:527/1880 train_time:18099ms step_avg:34.34ms
step:528/1880 train_time:18134ms step_avg:34.34ms
step:529/1880 train_time:18168ms step_avg:34.34ms
step:530/1880 train_time:18202ms step_avg:34.34ms
step:531/1880 train_time:18236ms step_avg:34.34ms
step:532/1880 train_time:18270ms step_avg:34.34ms
step:533/1880 train_time:18304ms step_avg:34.34ms
step:534/1880 train_time:18339ms step_avg:34.34ms
step:535/1880 train_time:18373ms step_avg:34.34ms
step:536/1880 train_time:18407ms step_avg:34.34ms
step:537/1880 train_time:18441ms step_avg:34.34ms
step:538/1880 train_time:18476ms step_avg:34.34ms
step:539/1880 train_time:18510ms step_avg:34.34ms
step:540/1880 train_time:18544ms step_avg:34.34ms
step:541/1880 train_time:18577ms step_avg:34.34ms
step:542/1880 train_time:18611ms step_avg:34.34ms
step:543/1880 train_time:18645ms step_avg:34.34ms
step:544/1880 train_time:18679ms step_avg:34.34ms
step:545/1880 train_time:18713ms step_avg:34.34ms
step:546/1880 train_time:18748ms step_avg:34.34ms
step:547/1880 train_time:18781ms step_avg:34.34ms
step:548/1880 train_time:18816ms step_avg:34.33ms
step:549/1880 train_time:18849ms step_avg:34.33ms
step:550/1880 train_time:18883ms step_avg:34.33ms
step:551/1880 train_time:18917ms step_avg:34.33ms
step:552/1880 train_time:18951ms step_avg:34.33ms
step:553/1880 train_time:18985ms step_avg:34.33ms
step:554/1880 train_time:19019ms step_avg:34.33ms
step:555/1880 train_time:19053ms step_avg:34.33ms
step:556/1880 train_time:19087ms step_avg:34.33ms
step:557/1880 train_time:19121ms step_avg:34.33ms
step:558/1880 train_time:19155ms step_avg:34.33ms
step:559/1880 train_time:19189ms step_avg:34.33ms
step:560/1880 train_time:19223ms step_avg:34.33ms
step:561/1880 train_time:19257ms step_avg:34.33ms
step:562/1880 train_time:19291ms step_avg:34.33ms
step:563/1880 train_time:19326ms step_avg:34.33ms
step:564/1880 train_time:19360ms step_avg:34.33ms
step:565/1880 train_time:19394ms step_avg:34.33ms
step:566/1880 train_time:19429ms step_avg:34.33ms
step:567/1880 train_time:19463ms step_avg:34.33ms
step:568/1880 train_time:19497ms step_avg:34.33ms
step:569/1880 train_time:19531ms step_avg:34.32ms
step:570/1880 train_time:19565ms step_avg:34.32ms
step:571/1880 train_time:19599ms step_avg:34.32ms
step:572/1880 train_time:19633ms step_avg:34.32ms
step:573/1880 train_time:19666ms step_avg:34.32ms
step:574/1880 train_time:19701ms step_avg:34.32ms
step:575/1880 train_time:19734ms step_avg:34.32ms
step:576/1880 train_time:19768ms step_avg:34.32ms
step:577/1880 train_time:19802ms step_avg:34.32ms
step:578/1880 train_time:19836ms step_avg:34.32ms
step:579/1880 train_time:19870ms step_avg:34.32ms
step:580/1880 train_time:19904ms step_avg:34.32ms
step:581/1880 train_time:19938ms step_avg:34.32ms
step:582/1880 train_time:19972ms step_avg:34.32ms
step:583/1880 train_time:20006ms step_avg:34.32ms
step:584/1880 train_time:20040ms step_avg:34.32ms
step:585/1880 train_time:20074ms step_avg:34.31ms
step:586/1880 train_time:20108ms step_avg:34.31ms
step:587/1880 train_time:20142ms step_avg:34.31ms
step:588/1880 train_time:20176ms step_avg:34.31ms
step:589/1880 train_time:20210ms step_avg:34.31ms
step:590/1880 train_time:20244ms step_avg:34.31ms
step:591/1880 train_time:20278ms step_avg:34.31ms
step:592/1880 train_time:20312ms step_avg:34.31ms
step:593/1880 train_time:20346ms step_avg:34.31ms
step:594/1880 train_time:20381ms step_avg:34.31ms
step:595/1880 train_time:20414ms step_avg:34.31ms
step:596/1880 train_time:20449ms step_avg:34.31ms
step:597/1880 train_time:20483ms step_avg:34.31ms
step:598/1880 train_time:20517ms step_avg:34.31ms
step:599/1880 train_time:20551ms step_avg:34.31ms
step:600/1880 train_time:20585ms step_avg:34.31ms
step:601/1880 train_time:20619ms step_avg:34.31ms
step:602/1880 train_time:20653ms step_avg:34.31ms
step:603/1880 train_time:20687ms step_avg:34.31ms
step:604/1880 train_time:20721ms step_avg:34.31ms
step:605/1880 train_time:20755ms step_avg:34.31ms
step:606/1880 train_time:20789ms step_avg:34.31ms
step:607/1880 train_time:20823ms step_avg:34.30ms
step:608/1880 train_time:20857ms step_avg:34.30ms
step:609/1880 train_time:20891ms step_avg:34.30ms
step:610/1880 train_time:20925ms step_avg:34.30ms
step:611/1880 train_time:20959ms step_avg:34.30ms
step:612/1880 train_time:20993ms step_avg:34.30ms
step:613/1880 train_time:21026ms step_avg:34.30ms
step:614/1880 train_time:21061ms step_avg:34.30ms
step:615/1880 train_time:21095ms step_avg:34.30ms
step:616/1880 train_time:21155ms step_avg:34.34ms
step:617/1880 train_time:21217ms step_avg:34.39ms
step:618/1880 train_time:21278ms step_avg:34.43ms
step:619/1880 train_time:21341ms step_avg:34.48ms
step:620/1880 train_time:21402ms step_avg:34.52ms
step:621/1880 train_time:21464ms step_avg:34.56ms
step:622/1880 train_time:21525ms step_avg:34.61ms
step:623/1880 train_time:21586ms step_avg:34.65ms
step:624/1880 train_time:21647ms step_avg:34.69ms
step:625/1880 train_time:21709ms step_avg:34.73ms
step:626/1880 train_time:21771ms step_avg:34.78ms
step:627/1880 train_time:21833ms step_avg:34.82ms
step:628/1880 train_time:21895ms step_avg:34.87ms
step:629/1880 train_time:21957ms step_avg:34.91ms
step:630/1880 train_time:22018ms step_avg:34.95ms
step:631/1880 train_time:22079ms step_avg:34.99ms
step:632/1880 train_time:22139ms step_avg:35.03ms
step:633/1880 train_time:22201ms step_avg:35.07ms
step:634/1880 train_time:22262ms step_avg:35.11ms
step:635/1880 train_time:22324ms step_avg:35.16ms
step:636/1880 train_time:22387ms step_avg:35.20ms
step:637/1880 train_time:22449ms step_avg:35.24ms
step:638/1880 train_time:22511ms step_avg:35.28ms
step:639/1880 train_time:22573ms step_avg:35.33ms
step:640/1880 train_time:22635ms step_avg:35.37ms
step:641/1880 train_time:22697ms step_avg:35.41ms
step:642/1880 train_time:22758ms step_avg:35.45ms
step:643/1880 train_time:22821ms step_avg:35.49ms
step:644/1880 train_time:22882ms step_avg:35.53ms
step:645/1880 train_time:22943ms step_avg:35.57ms
step:646/1880 train_time:23005ms step_avg:35.61ms
step:647/1880 train_time:23066ms step_avg:35.65ms
step:648/1880 train_time:23128ms step_avg:35.69ms
step:649/1880 train_time:23190ms step_avg:35.73ms
step:650/1880 train_time:23252ms step_avg:35.77ms
step:651/1880 train_time:23314ms step_avg:35.81ms
step:652/1880 train_time:23376ms step_avg:35.85ms
step:653/1880 train_time:23437ms step_avg:35.89ms
step:654/1880 train_time:23498ms step_avg:35.93ms
step:655/1880 train_time:23561ms step_avg:35.97ms
step:656/1880 train_time:23622ms step_avg:36.01ms
step:657/1880 train_time:23684ms step_avg:36.05ms
step:658/1880 train_time:23745ms step_avg:36.09ms
step:659/1880 train_time:23807ms step_avg:36.13ms
step:660/1880 train_time:23869ms step_avg:36.16ms
step:661/1880 train_time:23931ms step_avg:36.20ms
step:662/1880 train_time:23992ms step_avg:36.24ms
step:663/1880 train_time:24054ms step_avg:36.28ms
step:664/1880 train_time:24116ms step_avg:36.32ms
step:665/1880 train_time:24178ms step_avg:36.36ms
step:666/1880 train_time:24239ms step_avg:36.40ms
step:667/1880 train_time:24301ms step_avg:36.43ms
step:668/1880 train_time:24363ms step_avg:36.47ms
step:669/1880 train_time:24424ms step_avg:36.51ms
step:670/1880 train_time:24487ms step_avg:36.55ms
step:671/1880 train_time:24548ms step_avg:36.58ms
step:672/1880 train_time:24610ms step_avg:36.62ms
step:673/1880 train_time:24671ms step_avg:36.66ms
step:674/1880 train_time:24733ms step_avg:36.70ms
step:675/1880 train_time:24795ms step_avg:36.73ms
step:676/1880 train_time:24857ms step_avg:36.77ms
step:677/1880 train_time:24918ms step_avg:36.81ms
step:678/1880 train_time:24979ms step_avg:36.84ms
step:679/1880 train_time:25041ms step_avg:36.88ms
step:680/1880 train_time:25102ms step_avg:36.91ms
step:681/1880 train_time:25164ms step_avg:36.95ms
step:682/1880 train_time:25226ms step_avg:36.99ms
step:683/1880 train_time:25286ms step_avg:37.02ms
step:684/1880 train_time:25348ms step_avg:37.06ms
step:685/1880 train_time:25410ms step_avg:37.09ms
step:686/1880 train_time:25473ms step_avg:37.13ms
step:687/1880 train_time:25534ms step_avg:37.17ms
step:688/1880 train_time:25595ms step_avg:37.20ms
step:689/1880 train_time:25657ms step_avg:37.24ms
step:690/1880 train_time:25718ms step_avg:37.27ms
step:691/1880 train_time:25780ms step_avg:37.31ms
step:692/1880 train_time:25841ms step_avg:37.34ms
step:693/1880 train_time:25903ms step_avg:37.38ms
step:694/1880 train_time:25964ms step_avg:37.41ms
step:695/1880 train_time:26026ms step_avg:37.45ms
step:696/1880 train_time:26089ms step_avg:37.48ms
step:697/1880 train_time:26150ms step_avg:37.52ms
step:698/1880 train_time:26212ms step_avg:37.55ms
step:699/1880 train_time:26274ms step_avg:37.59ms
step:700/1880 train_time:26335ms step_avg:37.62ms
step:701/1880 train_time:26398ms step_avg:37.66ms
step:702/1880 train_time:26459ms step_avg:37.69ms
step:703/1880 train_time:26521ms step_avg:37.73ms
step:704/1880 train_time:26583ms step_avg:37.76ms
step:705/1880 train_time:26644ms step_avg:37.79ms
step:706/1880 train_time:26705ms step_avg:37.83ms
step:707/1880 train_time:26767ms step_avg:37.86ms
step:708/1880 train_time:26827ms step_avg:37.89ms
step:709/1880 train_time:26890ms step_avg:37.93ms
step:710/1880 train_time:26952ms step_avg:37.96ms
step:711/1880 train_time:27014ms step_avg:37.99ms
step:712/1880 train_time:27075ms step_avg:38.03ms
step:713/1880 train_time:27137ms step_avg:38.06ms
step:714/1880 train_time:27198ms step_avg:38.09ms
step:715/1880 train_time:27260ms step_avg:38.13ms
step:716/1880 train_time:27321ms step_avg:38.16ms
step:717/1880 train_time:27382ms step_avg:38.19ms
step:718/1880 train_time:27443ms step_avg:38.22ms
step:719/1880 train_time:27506ms step_avg:38.26ms
step:720/1880 train_time:27567ms step_avg:38.29ms
step:721/1880 train_time:27629ms step_avg:38.32ms
step:722/1880 train_time:27691ms step_avg:38.35ms
step:723/1880 train_time:27753ms step_avg:38.39ms
step:724/1880 train_time:27814ms step_avg:38.42ms
step:725/1880 train_time:27877ms step_avg:38.45ms
step:726/1880 train_time:27937ms step_avg:38.48ms
step:727/1880 train_time:27999ms step_avg:38.51ms
step:728/1880 train_time:28061ms step_avg:38.54ms
step:729/1880 train_time:28123ms step_avg:38.58ms
step:730/1880 train_time:28185ms step_avg:38.61ms
step:731/1880 train_time:28247ms step_avg:38.64ms
step:732/1880 train_time:28308ms step_avg:38.67ms
step:733/1880 train_time:28370ms step_avg:38.70ms
step:734/1880 train_time:28432ms step_avg:38.74ms
step:735/1880 train_time:28494ms step_avg:38.77ms
step:736/1880 train_time:28555ms step_avg:38.80ms
step:737/1880 train_time:28617ms step_avg:38.83ms
step:738/1880 train_time:28678ms step_avg:38.86ms
step:739/1880 train_time:28740ms step_avg:38.89ms
step:740/1880 train_time:28801ms step_avg:38.92ms
step:741/1880 train_time:28862ms step_avg:38.95ms
step:742/1880 train_time:28923ms step_avg:38.98ms
step:743/1880 train_time:28985ms step_avg:39.01ms
step:744/1880 train_time:29046ms step_avg:39.04ms
step:745/1880 train_time:29108ms step_avg:39.07ms
step:746/1880 train_time:29169ms step_avg:39.10ms
step:747/1880 train_time:29231ms step_avg:39.13ms
step:748/1880 train_time:29293ms step_avg:39.16ms
step:749/1880 train_time:29355ms step_avg:39.19ms
step:750/1880 train_time:29416ms step_avg:39.22ms
step:750/1880 val_loss:4.0178 train_time:29480ms step_avg:39.31ms
step:751/1880 train_time:29501ms step_avg:39.28ms
step:752/1880 train_time:29542ms step_avg:39.28ms
step:753/1880 train_time:29606ms step_avg:39.32ms
step:754/1880 train_time:29669ms step_avg:39.35ms
step:755/1880 train_time:29731ms step_avg:39.38ms
step:756/1880 train_time:29793ms step_avg:39.41ms
step:757/1880 train_time:29854ms step_avg:39.44ms
step:758/1880 train_time:29915ms step_avg:39.47ms
step:759/1880 train_time:29976ms step_avg:39.49ms
step:760/1880 train_time:30036ms step_avg:39.52ms
step:761/1880 train_time:30097ms step_avg:39.55ms
step:762/1880 train_time:30158ms step_avg:39.58ms
step:763/1880 train_time:30219ms step_avg:39.61ms
step:764/1880 train_time:30279ms step_avg:39.63ms
step:765/1880 train_time:30340ms step_avg:39.66ms
step:766/1880 train_time:30402ms step_avg:39.69ms
step:767/1880 train_time:30465ms step_avg:39.72ms
step:768/1880 train_time:30528ms step_avg:39.75ms
step:769/1880 train_time:30591ms step_avg:39.78ms
step:770/1880 train_time:30653ms step_avg:39.81ms
step:771/1880 train_time:30715ms step_avg:39.84ms
step:772/1880 train_time:30777ms step_avg:39.87ms
step:773/1880 train_time:30838ms step_avg:39.89ms
step:774/1880 train_time:30900ms step_avg:39.92ms
step:775/1880 train_time:30961ms step_avg:39.95ms
step:776/1880 train_time:31022ms step_avg:39.98ms
step:777/1880 train_time:31082ms step_avg:40.00ms
step:778/1880 train_time:31142ms step_avg:40.03ms
step:779/1880 train_time:31204ms step_avg:40.06ms
step:780/1880 train_time:31265ms step_avg:40.08ms
step:781/1880 train_time:31326ms step_avg:40.11ms
step:782/1880 train_time:31387ms step_avg:40.14ms
step:783/1880 train_time:31449ms step_avg:40.16ms
step:784/1880 train_time:31510ms step_avg:40.19ms
step:785/1880 train_time:31573ms step_avg:40.22ms
step:786/1880 train_time:31636ms step_avg:40.25ms
step:787/1880 train_time:31698ms step_avg:40.28ms
step:788/1880 train_time:31759ms step_avg:40.30ms
step:789/1880 train_time:31820ms step_avg:40.33ms
step:790/1880 train_time:31881ms step_avg:40.36ms
step:791/1880 train_time:31943ms step_avg:40.38ms
step:792/1880 train_time:32004ms step_avg:40.41ms
step:793/1880 train_time:32065ms step_avg:40.43ms
step:794/1880 train_time:32125ms step_avg:40.46ms
step:795/1880 train_time:32187ms step_avg:40.49ms
step:796/1880 train_time:32248ms step_avg:40.51ms
step:797/1880 train_time:32309ms step_avg:40.54ms
step:798/1880 train_time:32370ms step_avg:40.56ms
step:799/1880 train_time:32433ms step_avg:40.59ms
step:800/1880 train_time:32495ms step_avg:40.62ms
step:801/1880 train_time:32556ms step_avg:40.64ms
step:802/1880 train_time:32617ms step_avg:40.67ms
step:803/1880 train_time:32679ms step_avg:40.70ms
step:804/1880 train_time:32740ms step_avg:40.72ms
step:805/1880 train_time:32802ms step_avg:40.75ms
step:806/1880 train_time:32863ms step_avg:40.77ms
step:807/1880 train_time:32926ms step_avg:40.80ms
step:808/1880 train_time:32987ms step_avg:40.83ms
step:809/1880 train_time:33049ms step_avg:40.85ms
step:810/1880 train_time:33111ms step_avg:40.88ms
step:811/1880 train_time:33172ms step_avg:40.90ms
step:812/1880 train_time:33233ms step_avg:40.93ms
step:813/1880 train_time:33294ms step_avg:40.95ms
step:814/1880 train_time:33355ms step_avg:40.98ms
step:815/1880 train_time:33418ms step_avg:41.00ms
step:816/1880 train_time:33479ms step_avg:41.03ms
step:817/1880 train_time:33540ms step_avg:41.05ms
step:818/1880 train_time:33601ms step_avg:41.08ms
step:819/1880 train_time:33663ms step_avg:41.10ms
step:820/1880 train_time:33725ms step_avg:41.13ms
step:821/1880 train_time:33786ms step_avg:41.15ms
step:822/1880 train_time:33847ms step_avg:41.18ms
step:823/1880 train_time:33909ms step_avg:41.20ms
step:824/1880 train_time:33970ms step_avg:41.23ms
step:825/1880 train_time:34032ms step_avg:41.25ms
step:826/1880 train_time:34093ms step_avg:41.27ms
step:827/1880 train_time:34154ms step_avg:41.30ms
step:828/1880 train_time:34216ms step_avg:41.32ms
step:829/1880 train_time:34277ms step_avg:41.35ms
step:830/1880 train_time:34338ms step_avg:41.37ms
step:831/1880 train_time:34399ms step_avg:41.39ms
step:832/1880 train_time:34459ms step_avg:41.42ms
step:833/1880 train_time:34521ms step_avg:41.44ms
step:834/1880 train_time:34583ms step_avg:41.47ms
step:835/1880 train_time:34644ms step_avg:41.49ms
step:836/1880 train_time:34705ms step_avg:41.51ms
step:837/1880 train_time:34767ms step_avg:41.54ms
step:838/1880 train_time:34828ms step_avg:41.56ms
step:839/1880 train_time:34890ms step_avg:41.58ms
step:840/1880 train_time:34951ms step_avg:41.61ms
step:841/1880 train_time:35013ms step_avg:41.63ms
step:842/1880 train_time:35074ms step_avg:41.66ms
step:843/1880 train_time:35136ms step_avg:41.68ms
step:844/1880 train_time:35198ms step_avg:41.70ms
step:845/1880 train_time:35259ms step_avg:41.73ms
step:846/1880 train_time:35320ms step_avg:41.75ms
step:847/1880 train_time:35382ms step_avg:41.77ms
step:848/1880 train_time:35443ms step_avg:41.80ms
step:849/1880 train_time:35505ms step_avg:41.82ms
step:850/1880 train_time:35566ms step_avg:41.84ms
step:851/1880 train_time:35627ms step_avg:41.86ms
step:852/1880 train_time:35688ms step_avg:41.89ms
step:853/1880 train_time:35750ms step_avg:41.91ms
step:854/1880 train_time:35812ms step_avg:41.93ms
step:855/1880 train_time:35874ms step_avg:41.96ms
step:856/1880 train_time:35935ms step_avg:41.98ms
step:857/1880 train_time:35997ms step_avg:42.00ms
step:858/1880 train_time:36058ms step_avg:42.03ms
step:859/1880 train_time:36121ms step_avg:42.05ms
step:860/1880 train_time:36181ms step_avg:42.07ms
step:861/1880 train_time:36243ms step_avg:42.09ms
step:862/1880 train_time:36304ms step_avg:42.12ms
step:863/1880 train_time:36365ms step_avg:42.14ms
step:864/1880 train_time:36427ms step_avg:42.16ms
step:865/1880 train_time:36488ms step_avg:42.18ms
step:866/1880 train_time:36549ms step_avg:42.20ms
step:867/1880 train_time:36611ms step_avg:42.23ms
step:868/1880 train_time:36674ms step_avg:42.25ms
step:869/1880 train_time:36735ms step_avg:42.27ms
step:870/1880 train_time:36797ms step_avg:42.30ms
step:871/1880 train_time:36859ms step_avg:42.32ms
step:872/1880 train_time:36920ms step_avg:42.34ms
step:873/1880 train_time:36981ms step_avg:42.36ms
step:874/1880 train_time:37042ms step_avg:42.38ms
step:875/1880 train_time:37104ms step_avg:42.40ms
step:876/1880 train_time:37165ms step_avg:42.43ms
step:877/1880 train_time:37227ms step_avg:42.45ms
step:878/1880 train_time:37288ms step_avg:42.47ms
step:879/1880 train_time:37349ms step_avg:42.49ms
step:880/1880 train_time:37410ms step_avg:42.51ms
step:881/1880 train_time:37473ms step_avg:42.53ms
step:882/1880 train_time:37534ms step_avg:42.56ms
step:883/1880 train_time:37596ms step_avg:42.58ms
step:884/1880 train_time:37657ms step_avg:42.60ms
step:885/1880 train_time:37719ms step_avg:42.62ms
step:886/1880 train_time:37779ms step_avg:42.64ms
step:887/1880 train_time:37841ms step_avg:42.66ms
step:888/1880 train_time:37902ms step_avg:42.68ms
step:889/1880 train_time:37964ms step_avg:42.70ms
step:890/1880 train_time:38025ms step_avg:42.73ms
step:891/1880 train_time:38087ms step_avg:42.75ms
step:892/1880 train_time:38148ms step_avg:42.77ms
step:893/1880 train_time:38210ms step_avg:42.79ms
step:894/1880 train_time:38271ms step_avg:42.81ms
step:895/1880 train_time:38333ms step_avg:42.83ms
step:896/1880 train_time:38394ms step_avg:42.85ms
step:897/1880 train_time:38456ms step_avg:42.87ms
step:898/1880 train_time:38517ms step_avg:42.89ms
step:899/1880 train_time:38579ms step_avg:42.91ms
step:900/1880 train_time:38640ms step_avg:42.93ms
step:901/1880 train_time:38702ms step_avg:42.95ms
step:902/1880 train_time:38763ms step_avg:42.97ms
step:903/1880 train_time:38824ms step_avg:42.99ms
step:904/1880 train_time:38886ms step_avg:43.01ms
step:905/1880 train_time:38947ms step_avg:43.04ms
step:906/1880 train_time:39008ms step_avg:43.06ms
step:907/1880 train_time:39070ms step_avg:43.08ms
step:908/1880 train_time:39131ms step_avg:43.10ms
step:909/1880 train_time:39192ms step_avg:43.12ms
step:910/1880 train_time:39254ms step_avg:43.14ms
step:911/1880 train_time:39316ms step_avg:43.16ms
step:912/1880 train_time:39377ms step_avg:43.18ms
step:913/1880 train_time:39438ms step_avg:43.20ms
step:914/1880 train_time:39500ms step_avg:43.22ms
step:915/1880 train_time:39561ms step_avg:43.24ms
step:916/1880 train_time:39623ms step_avg:43.26ms
step:917/1880 train_time:39684ms step_avg:43.28ms
step:918/1880 train_time:39746ms step_avg:43.30ms
step:919/1880 train_time:39808ms step_avg:43.32ms
step:920/1880 train_time:39869ms step_avg:43.34ms
step:921/1880 train_time:39931ms step_avg:43.36ms
step:922/1880 train_time:39993ms step_avg:43.38ms
step:923/1880 train_time:40055ms step_avg:43.40ms
step:924/1880 train_time:40116ms step_avg:43.42ms
step:925/1880 train_time:40177ms step_avg:43.44ms
step:926/1880 train_time:40239ms step_avg:43.45ms
step:927/1880 train_time:40301ms step_avg:43.47ms
step:928/1880 train_time:40362ms step_avg:43.49ms
step:929/1880 train_time:40424ms step_avg:43.51ms
step:930/1880 train_time:40484ms step_avg:43.53ms
step:931/1880 train_time:40547ms step_avg:43.55ms
step:932/1880 train_time:40608ms step_avg:43.57ms
step:933/1880 train_time:40669ms step_avg:43.59ms
step:934/1880 train_time:40731ms step_avg:43.61ms
step:935/1880 train_time:40793ms step_avg:43.63ms
step:936/1880 train_time:40855ms step_avg:43.65ms
step:937/1880 train_time:40917ms step_avg:43.67ms
step:938/1880 train_time:40979ms step_avg:43.69ms
step:939/1880 train_time:41039ms step_avg:43.71ms
step:940/1880 train_time:41100ms step_avg:43.72ms
step:941/1880 train_time:41161ms step_avg:43.74ms
step:942/1880 train_time:41222ms step_avg:43.76ms
step:943/1880 train_time:41284ms step_avg:43.78ms
step:944/1880 train_time:41345ms step_avg:43.80ms
step:945/1880 train_time:41407ms step_avg:43.82ms
step:946/1880 train_time:41468ms step_avg:43.83ms
step:947/1880 train_time:41530ms step_avg:43.85ms
step:948/1880 train_time:41591ms step_avg:43.87ms
step:949/1880 train_time:41653ms step_avg:43.89ms
step:950/1880 train_time:41716ms step_avg:43.91ms
step:951/1880 train_time:41777ms step_avg:43.93ms
step:952/1880 train_time:41838ms step_avg:43.95ms
step:953/1880 train_time:41900ms step_avg:43.97ms
step:954/1880 train_time:41961ms step_avg:43.98ms
step:955/1880 train_time:42022ms step_avg:44.00ms
step:956/1880 train_time:42083ms step_avg:44.02ms
step:957/1880 train_time:42145ms step_avg:44.04ms
step:958/1880 train_time:42205ms step_avg:44.06ms
step:959/1880 train_time:42267ms step_avg:44.07ms
step:960/1880 train_time:42328ms step_avg:44.09ms
step:961/1880 train_time:42389ms step_avg:44.11ms
step:962/1880 train_time:42451ms step_avg:44.13ms
step:963/1880 train_time:42512ms step_avg:44.15ms
step:964/1880 train_time:42574ms step_avg:44.16ms
step:965/1880 train_time:42636ms step_avg:44.18ms
step:966/1880 train_time:42698ms step_avg:44.20ms
step:967/1880 train_time:42759ms step_avg:44.22ms
step:968/1880 train_time:42821ms step_avg:44.24ms
step:969/1880 train_time:42883ms step_avg:44.25ms
step:970/1880 train_time:42944ms step_avg:44.27ms
step:971/1880 train_time:43005ms step_avg:44.29ms
step:972/1880 train_time:43066ms step_avg:44.31ms
step:973/1880 train_time:43128ms step_avg:44.32ms
step:974/1880 train_time:43189ms step_avg:44.34ms
step:975/1880 train_time:43251ms step_avg:44.36ms
step:976/1880 train_time:43313ms step_avg:44.38ms
step:977/1880 train_time:43374ms step_avg:44.39ms
step:978/1880 train_time:43435ms step_avg:44.41ms
step:979/1880 train_time:43496ms step_avg:44.43ms
step:980/1880 train_time:43558ms step_avg:44.45ms
step:981/1880 train_time:43620ms step_avg:44.47ms
step:982/1880 train_time:43681ms step_avg:44.48ms
step:983/1880 train_time:43743ms step_avg:44.50ms
step:984/1880 train_time:43805ms step_avg:44.52ms
step:985/1880 train_time:43867ms step_avg:44.53ms
step:986/1880 train_time:43928ms step_avg:44.55ms
step:987/1880 train_time:43990ms step_avg:44.57ms
step:988/1880 train_time:44051ms step_avg:44.59ms
step:989/1880 train_time:44113ms step_avg:44.60ms
step:990/1880 train_time:44174ms step_avg:44.62ms
step:991/1880 train_time:44236ms step_avg:44.64ms
step:992/1880 train_time:44297ms step_avg:44.65ms
step:993/1880 train_time:44359ms step_avg:44.67ms
step:994/1880 train_time:44419ms step_avg:44.69ms
step:995/1880 train_time:44481ms step_avg:44.70ms
step:996/1880 train_time:44542ms step_avg:44.72ms
step:997/1880 train_time:44604ms step_avg:44.74ms
step:998/1880 train_time:44665ms step_avg:44.75ms
step:999/1880 train_time:44727ms step_avg:44.77ms
step:1000/1880 train_time:44787ms step_avg:44.79ms
step:1000/1880 val_loss:3.7697 train_time:44852ms step_avg:44.85ms
step:1001/1880 train_time:44872ms step_avg:44.83ms
step:1002/1880 train_time:44912ms step_avg:44.82ms
step:1003/1880 train_time:44976ms step_avg:44.84ms
step:1004/1880 train_time:45040ms step_avg:44.86ms
step:1005/1880 train_time:45101ms step_avg:44.88ms
step:1006/1880 train_time:45162ms step_avg:44.89ms
step:1007/1880 train_time:45224ms step_avg:44.91ms
step:1008/1880 train_time:45284ms step_avg:44.92ms
step:1009/1880 train_time:45345ms step_avg:44.94ms
step:1010/1880 train_time:45406ms step_avg:44.96ms
step:1011/1880 train_time:45467ms step_avg:44.97ms
step:1012/1880 train_time:45527ms step_avg:44.99ms
step:1013/1880 train_time:45589ms step_avg:45.00ms
step:1014/1880 train_time:45650ms step_avg:45.02ms
step:1015/1880 train_time:45712ms step_avg:45.04ms
step:1016/1880 train_time:45774ms step_avg:45.05ms
step:1017/1880 train_time:45836ms step_avg:45.07ms
step:1018/1880 train_time:45898ms step_avg:45.09ms
step:1019/1880 train_time:45961ms step_avg:45.10ms
step:1020/1880 train_time:46023ms step_avg:45.12ms
step:1021/1880 train_time:46086ms step_avg:45.14ms
step:1022/1880 train_time:46148ms step_avg:45.15ms
step:1023/1880 train_time:46210ms step_avg:45.17ms
step:1024/1880 train_time:46271ms step_avg:45.19ms
step:1025/1880 train_time:46332ms step_avg:45.20ms
step:1026/1880 train_time:46394ms step_avg:45.22ms
step:1027/1880 train_time:46455ms step_avg:45.23ms
step:1028/1880 train_time:46516ms step_avg:45.25ms
step:1029/1880 train_time:46578ms step_avg:45.26ms
step:1030/1880 train_time:46638ms step_avg:45.28ms
step:1031/1880 train_time:46699ms step_avg:45.30ms
step:1032/1880 train_time:46761ms step_avg:45.31ms
step:1033/1880 train_time:46823ms step_avg:45.33ms
step:1034/1880 train_time:46884ms step_avg:45.34ms
step:1035/1880 train_time:46946ms step_avg:45.36ms
step:1036/1880 train_time:47008ms step_avg:45.37ms
step:1037/1880 train_time:47071ms step_avg:45.39ms
step:1038/1880 train_time:47133ms step_avg:45.41ms
step:1039/1880 train_time:47195ms step_avg:45.42ms
step:1040/1880 train_time:47256ms step_avg:45.44ms
step:1041/1880 train_time:47318ms step_avg:45.45ms
step:1042/1880 train_time:47379ms step_avg:45.47ms
step:1043/1880 train_time:47440ms step_avg:45.48ms
step:1044/1880 train_time:47501ms step_avg:45.50ms
step:1045/1880 train_time:47563ms step_avg:45.51ms
step:1046/1880 train_time:47623ms step_avg:45.53ms
step:1047/1880 train_time:47685ms step_avg:45.54ms
step:1048/1880 train_time:47745ms step_avg:45.56ms
step:1049/1880 train_time:47807ms step_avg:45.57ms
step:1050/1880 train_time:47868ms step_avg:45.59ms
step:1051/1880 train_time:47931ms step_avg:45.61ms
step:1052/1880 train_time:47993ms step_avg:45.62ms
step:1053/1880 train_time:48056ms step_avg:45.64ms
step:1054/1880 train_time:48118ms step_avg:45.65ms
step:1055/1880 train_time:48179ms step_avg:45.67ms
step:1056/1880 train_time:48240ms step_avg:45.68ms
step:1057/1880 train_time:48303ms step_avg:45.70ms
step:1058/1880 train_time:48364ms step_avg:45.71ms
step:1059/1880 train_time:48425ms step_avg:45.73ms
step:1060/1880 train_time:48487ms step_avg:45.74ms
step:1061/1880 train_time:48548ms step_avg:45.76ms
step:1062/1880 train_time:48609ms step_avg:45.77ms
step:1063/1880 train_time:48671ms step_avg:45.79ms
step:1064/1880 train_time:48733ms step_avg:45.80ms
step:1065/1880 train_time:48794ms step_avg:45.82ms
step:1066/1880 train_time:48855ms step_avg:45.83ms
step:1067/1880 train_time:48917ms step_avg:45.85ms
step:1068/1880 train_time:48978ms step_avg:45.86ms
step:1069/1880 train_time:49040ms step_avg:45.87ms
step:1070/1880 train_time:49101ms step_avg:45.89ms
step:1071/1880 train_time:49163ms step_avg:45.90ms
step:1072/1880 train_time:49224ms step_avg:45.92ms
step:1073/1880 train_time:49285ms step_avg:45.93ms
step:1074/1880 train_time:49346ms step_avg:45.95ms
step:1075/1880 train_time:49408ms step_avg:45.96ms
step:1076/1880 train_time:49470ms step_avg:45.98ms
step:1077/1880 train_time:49531ms step_avg:45.99ms
step:1078/1880 train_time:49592ms step_avg:46.00ms
step:1079/1880 train_time:49654ms step_avg:46.02ms
step:1080/1880 train_time:49715ms step_avg:46.03ms
step:1081/1880 train_time:49777ms step_avg:46.05ms
step:1082/1880 train_time:49838ms step_avg:46.06ms
step:1083/1880 train_time:49900ms step_avg:46.08ms
step:1084/1880 train_time:49961ms step_avg:46.09ms
step:1085/1880 train_time:50023ms step_avg:46.10ms
step:1086/1880 train_time:50084ms step_avg:46.12ms
step:1087/1880 train_time:50146ms step_avg:46.13ms
step:1088/1880 train_time:50208ms step_avg:46.15ms
step:1089/1880 train_time:50270ms step_avg:46.16ms
step:1090/1880 train_time:50331ms step_avg:46.18ms
step:1091/1880 train_time:50393ms step_avg:46.19ms
step:1092/1880 train_time:50454ms step_avg:46.20ms
step:1093/1880 train_time:50516ms step_avg:46.22ms
step:1094/1880 train_time:50577ms step_avg:46.23ms
step:1095/1880 train_time:50639ms step_avg:46.25ms
step:1096/1880 train_time:50700ms step_avg:46.26ms
step:1097/1880 train_time:50761ms step_avg:46.27ms
step:1098/1880 train_time:50821ms step_avg:46.29ms
step:1099/1880 train_time:50883ms step_avg:46.30ms
step:1100/1880 train_time:50943ms step_avg:46.31ms
step:1101/1880 train_time:51005ms step_avg:46.33ms
step:1102/1880 train_time:51066ms step_avg:46.34ms
step:1103/1880 train_time:51128ms step_avg:46.35ms
step:1104/1880 train_time:51189ms step_avg:46.37ms
step:1105/1880 train_time:51251ms step_avg:46.38ms
step:1106/1880 train_time:51312ms step_avg:46.39ms
step:1107/1880 train_time:51374ms step_avg:46.41ms
step:1108/1880 train_time:51435ms step_avg:46.42ms
step:1109/1880 train_time:51497ms step_avg:46.44ms
step:1110/1880 train_time:51558ms step_avg:46.45ms
step:1111/1880 train_time:51620ms step_avg:46.46ms
step:1112/1880 train_time:51680ms step_avg:46.48ms
step:1113/1880 train_time:51742ms step_avg:46.49ms
step:1114/1880 train_time:51803ms step_avg:46.50ms
step:1115/1880 train_time:51865ms step_avg:46.52ms
step:1116/1880 train_time:51926ms step_avg:46.53ms
step:1117/1880 train_time:51988ms step_avg:46.54ms
step:1118/1880 train_time:52049ms step_avg:46.56ms
step:1119/1880 train_time:52111ms step_avg:46.57ms
step:1120/1880 train_time:52173ms step_avg:46.58ms
step:1121/1880 train_time:52235ms step_avg:46.60ms
step:1122/1880 train_time:52296ms step_avg:46.61ms
step:1123/1880 train_time:52358ms step_avg:46.62ms
step:1124/1880 train_time:52419ms step_avg:46.64ms
step:1125/1880 train_time:52481ms step_avg:46.65ms
step:1126/1880 train_time:52542ms step_avg:46.66ms
step:1127/1880 train_time:52605ms step_avg:46.68ms
step:1128/1880 train_time:52666ms step_avg:46.69ms
step:1129/1880 train_time:52728ms step_avg:46.70ms
step:1130/1880 train_time:52790ms step_avg:46.72ms
step:1131/1880 train_time:52852ms step_avg:46.73ms
step:1132/1880 train_time:52913ms step_avg:46.74ms
step:1133/1880 train_time:52975ms step_avg:46.76ms
step:1134/1880 train_time:53036ms step_avg:46.77ms
step:1135/1880 train_time:53098ms step_avg:46.78ms
step:1136/1880 train_time:53159ms step_avg:46.79ms
step:1137/1880 train_time:53221ms step_avg:46.81ms
step:1138/1880 train_time:53282ms step_avg:46.82ms
step:1139/1880 train_time:53344ms step_avg:46.83ms
step:1140/1880 train_time:53405ms step_avg:46.85ms
step:1141/1880 train_time:53467ms step_avg:46.86ms
step:1142/1880 train_time:53528ms step_avg:46.87ms
step:1143/1880 train_time:53590ms step_avg:46.89ms
step:1144/1880 train_time:53652ms step_avg:46.90ms
step:1145/1880 train_time:53714ms step_avg:46.91ms
step:1146/1880 train_time:53775ms step_avg:46.92ms
step:1147/1880 train_time:53837ms step_avg:46.94ms
step:1148/1880 train_time:53898ms step_avg:46.95ms
step:1149/1880 train_time:53960ms step_avg:46.96ms
step:1150/1880 train_time:54021ms step_avg:46.98ms
step:1151/1880 train_time:54082ms step_avg:46.99ms
step:1152/1880 train_time:54143ms step_avg:47.00ms
step:1153/1880 train_time:54205ms step_avg:47.01ms
step:1154/1880 train_time:54266ms step_avg:47.02ms
step:1155/1880 train_time:54328ms step_avg:47.04ms
step:1156/1880 train_time:54389ms step_avg:47.05ms
step:1157/1880 train_time:54451ms step_avg:47.06ms
step:1158/1880 train_time:54512ms step_avg:47.07ms
step:1159/1880 train_time:54574ms step_avg:47.09ms
step:1160/1880 train_time:54635ms step_avg:47.10ms
step:1161/1880 train_time:54697ms step_avg:47.11ms
step:1162/1880 train_time:54759ms step_avg:47.12ms
step:1163/1880 train_time:54820ms step_avg:47.14ms
step:1164/1880 train_time:54881ms step_avg:47.15ms
step:1165/1880 train_time:54943ms step_avg:47.16ms
step:1166/1880 train_time:55003ms step_avg:47.17ms
step:1167/1880 train_time:55065ms step_avg:47.19ms
step:1168/1880 train_time:55126ms step_avg:47.20ms
step:1169/1880 train_time:55188ms step_avg:47.21ms
step:1170/1880 train_time:55249ms step_avg:47.22ms
step:1171/1880 train_time:55311ms step_avg:47.23ms
step:1172/1880 train_time:55372ms step_avg:47.25ms
step:1173/1880 train_time:55434ms step_avg:47.26ms
step:1174/1880 train_time:55495ms step_avg:47.27ms
step:1175/1880 train_time:55556ms step_avg:47.28ms
step:1176/1880 train_time:55617ms step_avg:47.29ms
step:1177/1880 train_time:55679ms step_avg:47.31ms
step:1178/1880 train_time:55741ms step_avg:47.32ms
step:1179/1880 train_time:55803ms step_avg:47.33ms
step:1180/1880 train_time:55864ms step_avg:47.34ms
step:1181/1880 train_time:55926ms step_avg:47.35ms
step:1182/1880 train_time:55987ms step_avg:47.37ms
step:1183/1880 train_time:56048ms step_avg:47.38ms
step:1184/1880 train_time:56110ms step_avg:47.39ms
step:1185/1880 train_time:56171ms step_avg:47.40ms
step:1186/1880 train_time:56233ms step_avg:47.41ms
step:1187/1880 train_time:56294ms step_avg:47.43ms
step:1188/1880 train_time:56356ms step_avg:47.44ms
step:1189/1880 train_time:56418ms step_avg:47.45ms
step:1190/1880 train_time:56479ms step_avg:47.46ms
step:1191/1880 train_time:56541ms step_avg:47.47ms
step:1192/1880 train_time:56601ms step_avg:47.48ms
step:1193/1880 train_time:56663ms step_avg:47.50ms
step:1194/1880 train_time:56725ms step_avg:47.51ms
step:1195/1880 train_time:56787ms step_avg:47.52ms
step:1196/1880 train_time:56848ms step_avg:47.53ms
step:1197/1880 train_time:56909ms step_avg:47.54ms
step:1198/1880 train_time:56971ms step_avg:47.56ms
step:1199/1880 train_time:57033ms step_avg:47.57ms
step:1200/1880 train_time:57094ms step_avg:47.58ms
step:1201/1880 train_time:57156ms step_avg:47.59ms
step:1202/1880 train_time:57217ms step_avg:47.60ms
step:1203/1880 train_time:57279ms step_avg:47.61ms
step:1204/1880 train_time:57339ms step_avg:47.62ms
step:1205/1880 train_time:57401ms step_avg:47.64ms
step:1206/1880 train_time:57462ms step_avg:47.65ms
step:1207/1880 train_time:57524ms step_avg:47.66ms
step:1208/1880 train_time:57584ms step_avg:47.67ms
step:1209/1880 train_time:57646ms step_avg:47.68ms
step:1210/1880 train_time:57707ms step_avg:47.69ms
step:1211/1880 train_time:57769ms step_avg:47.70ms
step:1212/1880 train_time:57830ms step_avg:47.71ms
step:1213/1880 train_time:57892ms step_avg:47.73ms
step:1214/1880 train_time:57953ms step_avg:47.74ms
step:1215/1880 train_time:58014ms step_avg:47.75ms
step:1216/1880 train_time:58076ms step_avg:47.76ms
step:1217/1880 train_time:58137ms step_avg:47.77ms
step:1218/1880 train_time:58198ms step_avg:47.78ms
step:1219/1880 train_time:58260ms step_avg:47.79ms
step:1220/1880 train_time:58321ms step_avg:47.80ms
step:1221/1880 train_time:58383ms step_avg:47.82ms
step:1222/1880 train_time:58444ms step_avg:47.83ms
step:1223/1880 train_time:58506ms step_avg:47.84ms
step:1224/1880 train_time:58567ms step_avg:47.85ms
step:1225/1880 train_time:58628ms step_avg:47.86ms
step:1226/1880 train_time:58690ms step_avg:47.87ms
step:1227/1880 train_time:58752ms step_avg:47.88ms
step:1228/1880 train_time:58813ms step_avg:47.89ms
step:1229/1880 train_time:58901ms step_avg:47.93ms
step:1230/1880 train_time:58988ms step_avg:47.96ms
step:1231/1880 train_time:59077ms step_avg:47.99ms
step:1232/1880 train_time:59165ms step_avg:48.02ms
step:1233/1880 train_time:59254ms step_avg:48.06ms
step:1234/1880 train_time:59342ms step_avg:48.09ms
step:1235/1880 train_time:59432ms step_avg:48.12ms
step:1236/1880 train_time:59520ms step_avg:48.16ms
step:1237/1880 train_time:59607ms step_avg:48.19ms
step:1238/1880 train_time:59695ms step_avg:48.22ms
step:1239/1880 train_time:59782ms step_avg:48.25ms
step:1240/1880 train_time:59871ms step_avg:48.28ms
step:1241/1880 train_time:59958ms step_avg:48.31ms
step:1242/1880 train_time:60046ms step_avg:48.35ms
step:1243/1880 train_time:60134ms step_avg:48.38ms
step:1244/1880 train_time:60222ms step_avg:48.41ms
step:1245/1880 train_time:60311ms step_avg:48.44ms
step:1246/1880 train_time:60400ms step_avg:48.48ms
step:1247/1880 train_time:60488ms step_avg:48.51ms
step:1248/1880 train_time:60575ms step_avg:48.54ms
step:1249/1880 train_time:60663ms step_avg:48.57ms
step:1250/1880 train_time:60750ms step_avg:48.60ms
step:1250/1880 val_loss:3.5341 train_time:60841ms step_avg:48.67ms
step:1251/1880 train_time:60865ms step_avg:48.65ms
step:1252/1880 train_time:60932ms step_avg:48.67ms
step:1253/1880 train_time:61026ms step_avg:48.70ms
step:1254/1880 train_time:61113ms step_avg:48.73ms
step:1255/1880 train_time:61199ms step_avg:48.76ms
step:1256/1880 train_time:61286ms step_avg:48.79ms
step:1257/1880 train_time:61372ms step_avg:48.82ms
step:1258/1880 train_time:61460ms step_avg:48.86ms
step:1259/1880 train_time:61547ms step_avg:48.89ms
step:1260/1880 train_time:61634ms step_avg:48.92ms
step:1261/1880 train_time:61721ms step_avg:48.95ms
step:1262/1880 train_time:61809ms step_avg:48.98ms
step:1263/1880 train_time:61901ms step_avg:49.01ms
step:1264/1880 train_time:61992ms step_avg:49.04ms
step:1265/1880 train_time:62081ms step_avg:49.08ms
step:1266/1880 train_time:62168ms step_avg:49.11ms
step:1267/1880 train_time:62256ms step_avg:49.14ms
step:1268/1880 train_time:62343ms step_avg:49.17ms
step:1269/1880 train_time:62430ms step_avg:49.20ms
step:1270/1880 train_time:62517ms step_avg:49.23ms
step:1271/1880 train_time:62604ms step_avg:49.26ms
step:1272/1880 train_time:62691ms step_avg:49.29ms
step:1273/1880 train_time:62780ms step_avg:49.32ms
step:1274/1880 train_time:62868ms step_avg:49.35ms
step:1275/1880 train_time:62960ms step_avg:49.38ms
step:1276/1880 train_time:63049ms step_avg:49.41ms
step:1277/1880 train_time:63137ms step_avg:49.44ms
step:1278/1880 train_time:63224ms step_avg:49.47ms
step:1279/1880 train_time:63311ms step_avg:49.50ms
step:1280/1880 train_time:63398ms step_avg:49.53ms
step:1281/1880 train_time:63484ms step_avg:49.56ms
step:1282/1880 train_time:63571ms step_avg:49.59ms
step:1283/1880 train_time:63660ms step_avg:49.62ms
step:1284/1880 train_time:63748ms step_avg:49.65ms
step:1285/1880 train_time:63837ms step_avg:49.68ms
step:1286/1880 train_time:63925ms step_avg:49.71ms
step:1287/1880 train_time:64015ms step_avg:49.74ms
step:1288/1880 train_time:64105ms step_avg:49.77ms
step:1289/1880 train_time:64193ms step_avg:49.80ms
step:1290/1880 train_time:64280ms step_avg:49.83ms
step:1291/1880 train_time:64369ms step_avg:49.86ms
step:1292/1880 train_time:64456ms step_avg:49.89ms
step:1293/1880 train_time:64543ms step_avg:49.92ms
step:1294/1880 train_time:64630ms step_avg:49.95ms
step:1295/1880 train_time:64719ms step_avg:49.98ms
step:1296/1880 train_time:64807ms step_avg:50.01ms
step:1297/1880 train_time:64895ms step_avg:50.03ms
step:1298/1880 train_time:64983ms step_avg:50.06ms
step:1299/1880 train_time:65072ms step_avg:50.09ms
step:1300/1880 train_time:65161ms step_avg:50.12ms
step:1301/1880 train_time:65248ms step_avg:50.15ms
step:1302/1880 train_time:65335ms step_avg:50.18ms
step:1303/1880 train_time:65423ms step_avg:50.21ms
step:1304/1880 train_time:65511ms step_avg:50.24ms
step:1305/1880 train_time:65599ms step_avg:50.27ms
step:1306/1880 train_time:65686ms step_avg:50.30ms
step:1307/1880 train_time:65775ms step_avg:50.32ms
step:1308/1880 train_time:65862ms step_avg:50.35ms
step:1309/1880 train_time:65952ms step_avg:50.38ms
step:1310/1880 train_time:66042ms step_avg:50.41ms
step:1311/1880 train_time:66131ms step_avg:50.44ms
step:1312/1880 train_time:66219ms step_avg:50.47ms
step:1313/1880 train_time:66307ms step_avg:50.50ms
step:1314/1880 train_time:66394ms step_avg:50.53ms
step:1315/1880 train_time:66482ms step_avg:50.56ms
step:1316/1880 train_time:66570ms step_avg:50.58ms
step:1317/1880 train_time:66658ms step_avg:50.61ms
step:1318/1880 train_time:66745ms step_avg:50.64ms
step:1319/1880 train_time:66833ms step_avg:50.67ms
step:1320/1880 train_time:66921ms step_avg:50.70ms
step:1321/1880 train_time:67010ms step_avg:50.73ms
step:1322/1880 train_time:67098ms step_avg:50.76ms
step:1323/1880 train_time:67187ms step_avg:50.78ms
step:1324/1880 train_time:67275ms step_avg:50.81ms
step:1325/1880 train_time:67362ms step_avg:50.84ms
step:1326/1880 train_time:67448ms step_avg:50.87ms
step:1327/1880 train_time:67537ms step_avg:50.89ms
step:1328/1880 train_time:67625ms step_avg:50.92ms
step:1329/1880 train_time:67714ms step_avg:50.95ms
step:1330/1880 train_time:67801ms step_avg:50.98ms
step:1331/1880 train_time:67888ms step_avg:51.01ms
step:1332/1880 train_time:67977ms step_avg:51.03ms
step:1333/1880 train_time:68065ms step_avg:51.06ms
step:1334/1880 train_time:68153ms step_avg:51.09ms
step:1335/1880 train_time:68241ms step_avg:51.12ms
step:1336/1880 train_time:68329ms step_avg:51.14ms
step:1337/1880 train_time:68417ms step_avg:51.17ms
step:1338/1880 train_time:68504ms step_avg:51.20ms
step:1339/1880 train_time:68593ms step_avg:51.23ms
step:1340/1880 train_time:68681ms step_avg:51.25ms
step:1341/1880 train_time:68768ms step_avg:51.28ms
step:1342/1880 train_time:68856ms step_avg:51.31ms
step:1343/1880 train_time:68944ms step_avg:51.34ms
step:1344/1880 train_time:69032ms step_avg:51.36ms
step:1345/1880 train_time:69120ms step_avg:51.39ms
step:1346/1880 train_time:69208ms step_avg:51.42ms
step:1347/1880 train_time:69296ms step_avg:51.44ms
step:1348/1880 train_time:69383ms step_avg:51.47ms
step:1349/1880 train_time:69471ms step_avg:51.50ms
step:1350/1880 train_time:69560ms step_avg:51.53ms
step:1351/1880 train_time:69647ms step_avg:51.55ms
step:1352/1880 train_time:69735ms step_avg:51.58ms
step:1353/1880 train_time:69823ms step_avg:51.61ms
step:1354/1880 train_time:69910ms step_avg:51.63ms
step:1355/1880 train_time:69998ms step_avg:51.66ms
step:1356/1880 train_time:70086ms step_avg:51.69ms
step:1357/1880 train_time:70173ms step_avg:51.71ms
step:1358/1880 train_time:70262ms step_avg:51.74ms
step:1359/1880 train_time:70350ms step_avg:51.77ms
step:1360/1880 train_time:70438ms step_avg:51.79ms
step:1361/1880 train_time:70526ms step_avg:51.82ms
step:1362/1880 train_time:70614ms step_avg:51.85ms
step:1363/1880 train_time:70703ms step_avg:51.87ms
step:1364/1880 train_time:70792ms step_avg:51.90ms
step:1365/1880 train_time:70880ms step_avg:51.93ms
step:1366/1880 train_time:70968ms step_avg:51.95ms
step:1367/1880 train_time:71056ms step_avg:51.98ms
step:1368/1880 train_time:71143ms step_avg:52.01ms
step:1369/1880 train_time:71231ms step_avg:52.03ms
step:1370/1880 train_time:71319ms step_avg:52.06ms
step:1371/1880 train_time:71407ms step_avg:52.08ms
step:1372/1880 train_time:71497ms step_avg:52.11ms
step:1373/1880 train_time:71584ms step_avg:52.14ms
step:1374/1880 train_time:71672ms step_avg:52.16ms
step:1375/1880 train_time:71760ms step_avg:52.19ms
step:1376/1880 train_time:71849ms step_avg:52.22ms
step:1377/1880 train_time:71938ms step_avg:52.24ms
step:1378/1880 train_time:72025ms step_avg:52.27ms
step:1379/1880 train_time:72113ms step_avg:52.29ms
step:1380/1880 train_time:72201ms step_avg:52.32ms
step:1381/1880 train_time:72289ms step_avg:52.35ms
step:1382/1880 train_time:72379ms step_avg:52.37ms
step:1383/1880 train_time:72466ms step_avg:52.40ms
step:1384/1880 train_time:72554ms step_avg:52.42ms
step:1385/1880 train_time:72643ms step_avg:52.45ms
step:1386/1880 train_time:72731ms step_avg:52.48ms
step:1387/1880 train_time:72821ms step_avg:52.50ms
step:1388/1880 train_time:72909ms step_avg:52.53ms
step:1389/1880 train_time:72997ms step_avg:52.55ms
step:1390/1880 train_time:73084ms step_avg:52.58ms
step:1391/1880 train_time:73173ms step_avg:52.60ms
step:1392/1880 train_time:73261ms step_avg:52.63ms
step:1393/1880 train_time:73349ms step_avg:52.66ms
step:1394/1880 train_time:73437ms step_avg:52.68ms
step:1395/1880 train_time:73525ms step_avg:52.71ms
step:1396/1880 train_time:73612ms step_avg:52.73ms
step:1397/1880 train_time:73701ms step_avg:52.76ms
step:1398/1880 train_time:73790ms step_avg:52.78ms
step:1399/1880 train_time:73878ms step_avg:52.81ms
step:1400/1880 train_time:73965ms step_avg:52.83ms
step:1401/1880 train_time:74054ms step_avg:52.86ms
step:1402/1880 train_time:74142ms step_avg:52.88ms
step:1403/1880 train_time:74229ms step_avg:52.91ms
step:1404/1880 train_time:74317ms step_avg:52.93ms
step:1405/1880 train_time:74404ms step_avg:52.96ms
step:1406/1880 train_time:74492ms step_avg:52.98ms
step:1407/1880 train_time:74581ms step_avg:53.01ms
step:1408/1880 train_time:74668ms step_avg:53.03ms
step:1409/1880 train_time:74758ms step_avg:53.06ms
step:1410/1880 train_time:74845ms step_avg:53.08ms
step:1411/1880 train_time:74932ms step_avg:53.11ms
step:1412/1880 train_time:75020ms step_avg:53.13ms
step:1413/1880 train_time:75107ms step_avg:53.15ms
step:1414/1880 train_time:75196ms step_avg:53.18ms
step:1415/1880 train_time:75283ms step_avg:53.20ms
step:1416/1880 train_time:75370ms step_avg:53.23ms
step:1417/1880 train_time:75459ms step_avg:53.25ms
step:1418/1880 train_time:75546ms step_avg:53.28ms
step:1419/1880 train_time:75634ms step_avg:53.30ms
step:1420/1880 train_time:75722ms step_avg:53.33ms
step:1421/1880 train_time:75809ms step_avg:53.35ms
step:1422/1880 train_time:75898ms step_avg:53.37ms
step:1423/1880 train_time:75986ms step_avg:53.40ms
step:1424/1880 train_time:76075ms step_avg:53.42ms
step:1425/1880 train_time:76162ms step_avg:53.45ms
step:1426/1880 train_time:76249ms step_avg:53.47ms
step:1427/1880 train_time:76336ms step_avg:53.49ms
step:1428/1880 train_time:76423ms step_avg:53.52ms
step:1429/1880 train_time:76511ms step_avg:53.54ms
step:1430/1880 train_time:76599ms step_avg:53.57ms
step:1431/1880 train_time:76687ms step_avg:53.59ms
step:1432/1880 train_time:76774ms step_avg:53.61ms
step:1433/1880 train_time:76863ms step_avg:53.64ms
step:1434/1880 train_time:76951ms step_avg:53.66ms
step:1435/1880 train_time:77041ms step_avg:53.69ms
step:1436/1880 train_time:77130ms step_avg:53.71ms
step:1437/1880 train_time:77217ms step_avg:53.74ms
step:1438/1880 train_time:77304ms step_avg:53.76ms
step:1439/1880 train_time:77392ms step_avg:53.78ms
step:1440/1880 train_time:77479ms step_avg:53.80ms
step:1441/1880 train_time:77568ms step_avg:53.83ms
step:1442/1880 train_time:77656ms step_avg:53.85ms
step:1443/1880 train_time:77743ms step_avg:53.88ms
step:1444/1880 train_time:77830ms step_avg:53.90ms
step:1445/1880 train_time:77919ms step_avg:53.92ms
step:1446/1880 train_time:78007ms step_avg:53.95ms
step:1447/1880 train_time:78096ms step_avg:53.97ms
step:1448/1880 train_time:78184ms step_avg:53.99ms
step:1449/1880 train_time:78272ms step_avg:54.02ms
step:1450/1880 train_time:78360ms step_avg:54.04ms
step:1451/1880 train_time:78447ms step_avg:54.06ms
step:1452/1880 train_time:78534ms step_avg:54.09ms
step:1453/1880 train_time:78623ms step_avg:54.11ms
step:1454/1880 train_time:78712ms step_avg:54.13ms
step:1455/1880 train_time:78801ms step_avg:54.16ms
step:1456/1880 train_time:78888ms step_avg:54.18ms
step:1457/1880 train_time:78976ms step_avg:54.20ms
step:1458/1880 train_time:79063ms step_avg:54.23ms
step:1459/1880 train_time:79153ms step_avg:54.25ms
step:1460/1880 train_time:79241ms step_avg:54.27ms
step:1461/1880 train_time:79330ms step_avg:54.30ms
step:1462/1880 train_time:79418ms step_avg:54.32ms
step:1463/1880 train_time:79505ms step_avg:54.34ms
step:1464/1880 train_time:79594ms step_avg:54.37ms
step:1465/1880 train_time:79682ms step_avg:54.39ms
step:1466/1880 train_time:79769ms step_avg:54.41ms
step:1467/1880 train_time:79857ms step_avg:54.44ms
step:1468/1880 train_time:79944ms step_avg:54.46ms
step:1469/1880 train_time:80033ms step_avg:54.48ms
step:1470/1880 train_time:80121ms step_avg:54.50ms
step:1471/1880 train_time:80209ms step_avg:54.53ms
step:1472/1880 train_time:80297ms step_avg:54.55ms
step:1473/1880 train_time:80386ms step_avg:54.57ms
step:1474/1880 train_time:80473ms step_avg:54.60ms
step:1475/1880 train_time:80560ms step_avg:54.62ms
step:1476/1880 train_time:80648ms step_avg:54.64ms
step:1477/1880 train_time:80737ms step_avg:54.66ms
step:1478/1880 train_time:80825ms step_avg:54.69ms
step:1479/1880 train_time:80912ms step_avg:54.71ms
step:1480/1880 train_time:80999ms step_avg:54.73ms
step:1481/1880 train_time:81088ms step_avg:54.75ms
step:1482/1880 train_time:81175ms step_avg:54.77ms
step:1483/1880 train_time:81264ms step_avg:54.80ms
step:1484/1880 train_time:81352ms step_avg:54.82ms
step:1485/1880 train_time:81440ms step_avg:54.84ms
step:1486/1880 train_time:81527ms step_avg:54.86ms
step:1487/1880 train_time:81616ms step_avg:54.89ms
step:1488/1880 train_time:81704ms step_avg:54.91ms
step:1489/1880 train_time:81792ms step_avg:54.93ms
step:1490/1880 train_time:81879ms step_avg:54.95ms
step:1491/1880 train_time:81967ms step_avg:54.97ms
step:1492/1880 train_time:82054ms step_avg:55.00ms
step:1493/1880 train_time:82142ms step_avg:55.02ms
step:1494/1880 train_time:82231ms step_avg:55.04ms
step:1495/1880 train_time:82320ms step_avg:55.06ms
step:1496/1880 train_time:82408ms step_avg:55.09ms
step:1497/1880 train_time:82496ms step_avg:55.11ms
step:1498/1880 train_time:82583ms step_avg:55.13ms
step:1499/1880 train_time:82671ms step_avg:55.15ms
step:1500/1880 train_time:82761ms step_avg:55.17ms
step:1500/1880 val_loss:3.4080 train_time:82850ms step_avg:55.23ms
step:1501/1880 train_time:82871ms step_avg:55.21ms
step:1502/1880 train_time:82940ms step_avg:55.22ms
step:1503/1880 train_time:83031ms step_avg:55.24ms
step:1504/1880 train_time:83121ms step_avg:55.27ms
step:1505/1880 train_time:83207ms step_avg:55.29ms
step:1506/1880 train_time:83293ms step_avg:55.31ms
step:1507/1880 train_time:83381ms step_avg:55.33ms
step:1508/1880 train_time:83468ms step_avg:55.35ms
step:1509/1880 train_time:83553ms step_avg:55.37ms
step:1510/1880 train_time:83641ms step_avg:55.39ms
step:1511/1880 train_time:83728ms step_avg:55.41ms
step:1512/1880 train_time:83816ms step_avg:55.43ms
step:1513/1880 train_time:83908ms step_avg:55.46ms
step:1514/1880 train_time:83997ms step_avg:55.48ms
step:1515/1880 train_time:84088ms step_avg:55.50ms
step:1516/1880 train_time:84175ms step_avg:55.52ms
step:1517/1880 train_time:84262ms step_avg:55.55ms
step:1518/1880 train_time:84349ms step_avg:55.57ms
step:1519/1880 train_time:84437ms step_avg:55.59ms
step:1520/1880 train_time:84523ms step_avg:55.61ms
step:1521/1880 train_time:84610ms step_avg:55.63ms
step:1522/1880 train_time:84697ms step_avg:55.65ms
step:1523/1880 train_time:84786ms step_avg:55.67ms
step:1524/1880 train_time:84876ms step_avg:55.69ms
step:1525/1880 train_time:84967ms step_avg:55.72ms
step:1526/1880 train_time:85057ms step_avg:55.74ms
step:1527/1880 train_time:85147ms step_avg:55.76ms
step:1528/1880 train_time:85235ms step_avg:55.78ms
step:1529/1880 train_time:85322ms step_avg:55.80ms
step:1530/1880 train_time:85408ms step_avg:55.82ms
step:1531/1880 train_time:85495ms step_avg:55.84ms
step:1532/1880 train_time:85583ms step_avg:55.86ms
step:1533/1880 train_time:85670ms step_avg:55.88ms
step:1534/1880 train_time:85758ms step_avg:55.90ms
step:1535/1880 train_time:85847ms step_avg:55.93ms
step:1536/1880 train_time:85937ms step_avg:55.95ms
step:1537/1880 train_time:86027ms step_avg:55.97ms
step:1538/1880 train_time:86116ms step_avg:55.99ms
step:1539/1880 train_time:86206ms step_avg:56.01ms
step:1540/1880 train_time:86293ms step_avg:56.03ms
step:1541/1880 train_time:86381ms step_avg:56.05ms
step:1542/1880 train_time:86467ms step_avg:56.07ms
step:1543/1880 train_time:86555ms step_avg:56.10ms
step:1544/1880 train_time:86642ms step_avg:56.12ms
step:1545/1880 train_time:86729ms step_avg:56.14ms
step:1546/1880 train_time:86818ms step_avg:56.16ms
step:1547/1880 train_time:86908ms step_avg:56.18ms
step:1548/1880 train_time:86996ms step_avg:56.20ms
step:1549/1880 train_time:87085ms step_avg:56.22ms
step:1550/1880 train_time:87173ms step_avg:56.24ms
step:1551/1880 train_time:87262ms step_avg:56.26ms
step:1552/1880 train_time:87349ms step_avg:56.28ms
step:1553/1880 train_time:87437ms step_avg:56.30ms
step:1554/1880 train_time:87525ms step_avg:56.32ms
step:1555/1880 train_time:87612ms step_avg:56.34ms
step:1556/1880 train_time:87700ms step_avg:56.36ms
step:1557/1880 train_time:87789ms step_avg:56.38ms
step:1558/1880 train_time:87878ms step_avg:56.40ms
step:1559/1880 train_time:87967ms step_avg:56.43ms
step:1560/1880 train_time:88055ms step_avg:56.45ms
step:1561/1880 train_time:88144ms step_avg:56.47ms
step:1562/1880 train_time:88231ms step_avg:56.49ms
step:1563/1880 train_time:88320ms step_avg:56.51ms
step:1564/1880 train_time:88407ms step_avg:56.53ms
step:1565/1880 train_time:88495ms step_avg:56.55ms
step:1566/1880 train_time:88582ms step_avg:56.57ms
step:1567/1880 train_time:88669ms step_avg:56.59ms
step:1568/1880 train_time:88757ms step_avg:56.61ms
step:1569/1880 train_time:88847ms step_avg:56.63ms
step:1570/1880 train_time:88934ms step_avg:56.65ms
step:1571/1880 train_time:89022ms step_avg:56.67ms
step:1572/1880 train_time:89110ms step_avg:56.69ms
step:1573/1880 train_time:89199ms step_avg:56.71ms
step:1574/1880 train_time:89287ms step_avg:56.73ms
step:1575/1880 train_time:89375ms step_avg:56.75ms
step:1576/1880 train_time:89463ms step_avg:56.77ms
step:1577/1880 train_time:89550ms step_avg:56.79ms
step:1578/1880 train_time:89637ms step_avg:56.80ms
step:1579/1880 train_time:89725ms step_avg:56.82ms
step:1580/1880 train_time:89813ms step_avg:56.84ms
step:1581/1880 train_time:89901ms step_avg:56.86ms
step:1582/1880 train_time:89989ms step_avg:56.88ms
step:1583/1880 train_time:90078ms step_avg:56.90ms
step:1584/1880 train_time:90166ms step_avg:56.92ms
step:1585/1880 train_time:90255ms step_avg:56.94ms
step:1586/1880 train_time:90344ms step_avg:56.96ms
step:1587/1880 train_time:90431ms step_avg:56.98ms
step:1588/1880 train_time:90520ms step_avg:57.00ms
step:1589/1880 train_time:90607ms step_avg:57.02ms
step:1590/1880 train_time:90694ms step_avg:57.04ms
step:1591/1880 train_time:90783ms step_avg:57.06ms
step:1592/1880 train_time:90870ms step_avg:57.08ms
step:1593/1880 train_time:90959ms step_avg:57.10ms
step:1594/1880 train_time:91047ms step_avg:57.12ms
step:1595/1880 train_time:91135ms step_avg:57.14ms
step:1596/1880 train_time:91223ms step_avg:57.16ms
step:1597/1880 train_time:91311ms step_avg:57.18ms
step:1598/1880 train_time:91399ms step_avg:57.20ms
step:1599/1880 train_time:91487ms step_avg:57.22ms
step:1600/1880 train_time:91576ms step_avg:57.23ms
step:1601/1880 train_time:91663ms step_avg:57.25ms
step:1602/1880 train_time:91751ms step_avg:57.27ms
step:1603/1880 train_time:91839ms step_avg:57.29ms
step:1604/1880 train_time:91927ms step_avg:57.31ms
step:1605/1880 train_time:92016ms step_avg:57.33ms
step:1606/1880 train_time:92104ms step_avg:57.35ms
step:1607/1880 train_time:92193ms step_avg:57.37ms
step:1608/1880 train_time:92281ms step_avg:57.39ms
step:1609/1880 train_time:92369ms step_avg:57.41ms
step:1610/1880 train_time:92457ms step_avg:57.43ms
step:1611/1880 train_time:92545ms step_avg:57.45ms
step:1612/1880 train_time:92632ms step_avg:57.46ms
step:1613/1880 train_time:92721ms step_avg:57.48ms
step:1614/1880 train_time:92808ms step_avg:57.50ms
step:1615/1880 train_time:92896ms step_avg:57.52ms
step:1616/1880 train_time:92985ms step_avg:57.54ms
step:1617/1880 train_time:93073ms step_avg:57.56ms
step:1618/1880 train_time:93160ms step_avg:57.58ms
step:1619/1880 train_time:93248ms step_avg:57.60ms
step:1620/1880 train_time:93336ms step_avg:57.61ms
step:1621/1880 train_time:93425ms step_avg:57.63ms
step:1622/1880 train_time:93511ms step_avg:57.65ms
step:1623/1880 train_time:93601ms step_avg:57.67ms
step:1624/1880 train_time:93688ms step_avg:57.69ms
step:1625/1880 train_time:93776ms step_avg:57.71ms
step:1626/1880 train_time:93864ms step_avg:57.73ms
step:1627/1880 train_time:93952ms step_avg:57.75ms
step:1628/1880 train_time:94040ms step_avg:57.76ms
step:1629/1880 train_time:94129ms step_avg:57.78ms
step:1630/1880 train_time:94217ms step_avg:57.80ms
step:1631/1880 train_time:94305ms step_avg:57.82ms
step:1632/1880 train_time:94393ms step_avg:57.84ms
step:1633/1880 train_time:94481ms step_avg:57.86ms
step:1634/1880 train_time:94569ms step_avg:57.88ms
step:1635/1880 train_time:94657ms step_avg:57.89ms
step:1636/1880 train_time:94745ms step_avg:57.91ms
step:1637/1880 train_time:94834ms step_avg:57.93ms
step:1638/1880 train_time:94922ms step_avg:57.95ms
step:1639/1880 train_time:95011ms step_avg:57.97ms
step:1640/1880 train_time:95098ms step_avg:57.99ms
step:1641/1880 train_time:95186ms step_avg:58.00ms
step:1642/1880 train_time:95274ms step_avg:58.02ms
step:1643/1880 train_time:95362ms step_avg:58.04ms
step:1644/1880 train_time:95450ms step_avg:58.06ms
step:1645/1880 train_time:95539ms step_avg:58.08ms
step:1646/1880 train_time:95626ms step_avg:58.10ms
step:1647/1880 train_time:95713ms step_avg:58.11ms
step:1648/1880 train_time:95801ms step_avg:58.13ms
step:1649/1880 train_time:95889ms step_avg:58.15ms
step:1650/1880 train_time:95978ms step_avg:58.17ms
step:1651/1880 train_time:96067ms step_avg:58.19ms
step:1652/1880 train_time:96155ms step_avg:58.21ms
step:1653/1880 train_time:96243ms step_avg:58.22ms
step:1654/1880 train_time:96330ms step_avg:58.24ms
step:1655/1880 train_time:96419ms step_avg:58.26ms
step:1656/1880 train_time:96507ms step_avg:58.28ms
step:1657/1880 train_time:96594ms step_avg:58.29ms
step:1658/1880 train_time:96683ms step_avg:58.31ms
step:1659/1880 train_time:96772ms step_avg:58.33ms
step:1660/1880 train_time:96858ms step_avg:58.35ms
step:1661/1880 train_time:96947ms step_avg:58.37ms
step:1662/1880 train_time:97035ms step_avg:58.38ms
step:1663/1880 train_time:97123ms step_avg:58.40ms
step:1664/1880 train_time:97210ms step_avg:58.42ms
step:1665/1880 train_time:97299ms step_avg:58.44ms
step:1666/1880 train_time:97387ms step_avg:58.46ms
step:1667/1880 train_time:97475ms step_avg:58.47ms
step:1668/1880 train_time:97563ms step_avg:58.49ms
step:1669/1880 train_time:97651ms step_avg:58.51ms
step:1670/1880 train_time:97740ms step_avg:58.53ms
step:1671/1880 train_time:97828ms step_avg:58.54ms
step:1672/1880 train_time:97916ms step_avg:58.56ms
step:1673/1880 train_time:98004ms step_avg:58.58ms
step:1674/1880 train_time:98090ms step_avg:58.60ms
step:1675/1880 train_time:98179ms step_avg:58.61ms
step:1676/1880 train_time:98268ms step_avg:58.63ms
step:1677/1880 train_time:98356ms step_avg:58.65ms
step:1678/1880 train_time:98445ms step_avg:58.67ms
step:1679/1880 train_time:98533ms step_avg:58.69ms
step:1680/1880 train_time:98621ms step_avg:58.70ms
step:1681/1880 train_time:98708ms step_avg:58.72ms
step:1682/1880 train_time:98796ms step_avg:58.74ms
step:1683/1880 train_time:98885ms step_avg:58.76ms
step:1684/1880 train_time:98972ms step_avg:58.77ms
step:1685/1880 train_time:99061ms step_avg:58.79ms
step:1686/1880 train_time:99148ms step_avg:58.81ms
step:1687/1880 train_time:99236ms step_avg:58.82ms
step:1688/1880 train_time:99325ms step_avg:58.84ms
step:1689/1880 train_time:99414ms step_avg:58.86ms
step:1690/1880 train_time:99501ms step_avg:58.88ms
step:1691/1880 train_time:99590ms step_avg:58.89ms
step:1692/1880 train_time:99679ms step_avg:58.91ms
step:1693/1880 train_time:99768ms step_avg:58.93ms
step:1694/1880 train_time:99856ms step_avg:58.95ms
step:1695/1880 train_time:99944ms step_avg:58.96ms
step:1696/1880 train_time:100031ms step_avg:58.98ms
step:1697/1880 train_time:100119ms step_avg:59.00ms
step:1698/1880 train_time:100207ms step_avg:59.01ms
step:1699/1880 train_time:100295ms step_avg:59.03ms
step:1700/1880 train_time:100384ms step_avg:59.05ms
step:1701/1880 train_time:100471ms step_avg:59.07ms
step:1702/1880 train_time:100559ms step_avg:59.08ms
step:1703/1880 train_time:100648ms step_avg:59.10ms
step:1704/1880 train_time:100736ms step_avg:59.12ms
step:1705/1880 train_time:100824ms step_avg:59.13ms
step:1706/1880 train_time:100912ms step_avg:59.15ms
step:1707/1880 train_time:101000ms step_avg:59.17ms
step:1708/1880 train_time:101087ms step_avg:59.18ms
step:1709/1880 train_time:101176ms step_avg:59.20ms
step:1710/1880 train_time:101264ms step_avg:59.22ms
step:1711/1880 train_time:101352ms step_avg:59.24ms
step:1712/1880 train_time:101440ms step_avg:59.25ms
step:1713/1880 train_time:101529ms step_avg:59.27ms
step:1714/1880 train_time:101618ms step_avg:59.29ms
step:1715/1880 train_time:101706ms step_avg:59.30ms
step:1716/1880 train_time:101794ms step_avg:59.32ms
step:1717/1880 train_time:101882ms step_avg:59.34ms
step:1718/1880 train_time:101969ms step_avg:59.35ms
step:1719/1880 train_time:102058ms step_avg:59.37ms
step:1720/1880 train_time:102146ms step_avg:59.39ms
step:1721/1880 train_time:102233ms step_avg:59.40ms
step:1722/1880 train_time:102322ms step_avg:59.42ms
step:1723/1880 train_time:102409ms step_avg:59.44ms
step:1724/1880 train_time:102498ms step_avg:59.45ms
step:1725/1880 train_time:102588ms step_avg:59.47ms
step:1726/1880 train_time:102675ms step_avg:59.49ms
step:1727/1880 train_time:102763ms step_avg:59.50ms
step:1728/1880 train_time:102851ms step_avg:59.52ms
step:1729/1880 train_time:102939ms step_avg:59.54ms
step:1730/1880 train_time:103027ms step_avg:59.55ms
step:1731/1880 train_time:103115ms step_avg:59.57ms
step:1732/1880 train_time:103202ms step_avg:59.59ms
step:1733/1880 train_time:103290ms step_avg:59.60ms
step:1734/1880 train_time:103378ms step_avg:59.62ms
step:1735/1880 train_time:103466ms step_avg:59.63ms
step:1736/1880 train_time:103555ms step_avg:59.65ms
step:1737/1880 train_time:103644ms step_avg:59.67ms
step:1738/1880 train_time:103731ms step_avg:59.68ms
step:1739/1880 train_time:103819ms step_avg:59.70ms
step:1740/1880 train_time:103907ms step_avg:59.72ms
step:1741/1880 train_time:103995ms step_avg:59.73ms
step:1742/1880 train_time:104084ms step_avg:59.75ms
step:1743/1880 train_time:104171ms step_avg:59.77ms
step:1744/1880 train_time:104258ms step_avg:59.78ms
step:1745/1880 train_time:104347ms step_avg:59.80ms
step:1746/1880 train_time:104435ms step_avg:59.81ms
step:1747/1880 train_time:104523ms step_avg:59.83ms
step:1748/1880 train_time:104611ms step_avg:59.85ms
step:1749/1880 train_time:104699ms step_avg:59.86ms
step:1750/1880 train_time:104788ms step_avg:59.88ms
step:1750/1880 val_loss:3.3138 train_time:104877ms step_avg:59.93ms
step:1751/1880 train_time:104898ms step_avg:59.91ms
step:1752/1880 train_time:104967ms step_avg:59.91ms
step:1753/1880 train_time:105061ms step_avg:59.93ms
step:1754/1880 train_time:105148ms step_avg:59.95ms
step:1755/1880 train_time:105236ms step_avg:59.96ms
step:1756/1880 train_time:105322ms step_avg:59.98ms
step:1757/1880 train_time:105410ms step_avg:59.99ms
step:1758/1880 train_time:105497ms step_avg:60.01ms
step:1759/1880 train_time:105584ms step_avg:60.03ms
step:1760/1880 train_time:105673ms step_avg:60.04ms
step:1761/1880 train_time:105760ms step_avg:60.06ms
step:1762/1880 train_time:105849ms step_avg:60.07ms
step:1763/1880 train_time:105939ms step_avg:60.09ms
step:1764/1880 train_time:106029ms step_avg:60.11ms
step:1765/1880 train_time:106118ms step_avg:60.12ms
step:1766/1880 train_time:106206ms step_avg:60.14ms
step:1767/1880 train_time:106294ms step_avg:60.15ms
step:1768/1880 train_time:106381ms step_avg:60.17ms
step:1769/1880 train_time:106469ms step_avg:60.19ms
step:1770/1880 train_time:106556ms step_avg:60.20ms
step:1771/1880 train_time:106643ms step_avg:60.22ms
step:1772/1880 train_time:106731ms step_avg:60.23ms
step:1773/1880 train_time:106819ms step_avg:60.25ms
step:1774/1880 train_time:106908ms step_avg:60.26ms
step:1775/1880 train_time:106999ms step_avg:60.28ms
step:1776/1880 train_time:107091ms step_avg:60.30ms
step:1777/1880 train_time:107179ms step_avg:60.31ms
step:1778/1880 train_time:107267ms step_avg:60.33ms
step:1779/1880 train_time:107355ms step_avg:60.35ms
step:1780/1880 train_time:107443ms step_avg:60.36ms
step:1781/1880 train_time:107531ms step_avg:60.38ms
step:1782/1880 train_time:107617ms step_avg:60.39ms
step:1783/1880 train_time:107705ms step_avg:60.41ms
step:1784/1880 train_time:107794ms step_avg:60.42ms
step:1785/1880 train_time:107883ms step_avg:60.44ms
step:1786/1880 train_time:107972ms step_avg:60.45ms
step:1787/1880 train_time:108060ms step_avg:60.47ms
step:1788/1880 train_time:108149ms step_avg:60.49ms
step:1789/1880 train_time:108236ms step_avg:60.50ms
step:1790/1880 train_time:108323ms step_avg:60.52ms
step:1791/1880 train_time:108412ms step_avg:60.53ms
step:1792/1880 train_time:108498ms step_avg:60.55ms
step:1793/1880 train_time:108586ms step_avg:60.56ms
step:1794/1880 train_time:108673ms step_avg:60.58ms
step:1795/1880 train_time:108761ms step_avg:60.59ms
step:1796/1880 train_time:108850ms step_avg:60.61ms
step:1797/1880 train_time:108939ms step_avg:60.62ms
step:1798/1880 train_time:109027ms step_avg:60.64ms
step:1799/1880 train_time:109117ms step_avg:60.65ms
step:1800/1880 train_time:109205ms step_avg:60.67ms
step:1801/1880 train_time:109294ms step_avg:60.69ms
step:1802/1880 train_time:109382ms step_avg:60.70ms
step:1803/1880 train_time:109470ms step_avg:60.72ms
step:1804/1880 train_time:109557ms step_avg:60.73ms
step:1805/1880 train_time:109644ms step_avg:60.74ms
step:1806/1880 train_time:109732ms step_avg:60.76ms
step:1807/1880 train_time:109819ms step_avg:60.77ms
step:1808/1880 train_time:109908ms step_avg:60.79ms
step:1809/1880 train_time:109998ms step_avg:60.81ms
step:1810/1880 train_time:110087ms step_avg:60.82ms
step:1811/1880 train_time:110176ms step_avg:60.84ms
step:1812/1880 train_time:110264ms step_avg:60.85ms
step:1813/1880 train_time:110352ms step_avg:60.87ms
step:1814/1880 train_time:110439ms step_avg:60.88ms
step:1815/1880 train_time:110527ms step_avg:60.90ms
step:1816/1880 train_time:110614ms step_avg:60.91ms
step:1817/1880 train_time:110701ms step_avg:60.93ms
step:1818/1880 train_time:110789ms step_avg:60.94ms
step:1819/1880 train_time:110878ms step_avg:60.96ms
step:1820/1880 train_time:110967ms step_avg:60.97ms
step:1821/1880 train_time:111056ms step_avg:60.99ms
step:1822/1880 train_time:111144ms step_avg:61.00ms
step:1823/1880 train_time:111234ms step_avg:61.02ms
step:1824/1880 train_time:111322ms step_avg:61.03ms
step:1825/1880 train_time:111410ms step_avg:61.05ms
step:1826/1880 train_time:111497ms step_avg:61.06ms
step:1827/1880 train_time:111585ms step_avg:61.08ms
step:1828/1880 train_time:111673ms step_avg:61.09ms
step:1829/1880 train_time:111761ms step_avg:61.10ms
step:1830/1880 train_time:111849ms step_avg:61.12ms
step:1831/1880 train_time:111937ms step_avg:61.13ms
step:1832/1880 train_time:112025ms step_avg:61.15ms
step:1833/1880 train_time:112115ms step_avg:61.16ms
step:1834/1880 train_time:112203ms step_avg:61.18ms
step:1835/1880 train_time:112292ms step_avg:61.19ms
step:1836/1880 train_time:112380ms step_avg:61.21ms
step:1837/1880 train_time:112467ms step_avg:61.22ms
step:1838/1880 train_time:112555ms step_avg:61.24ms
step:1839/1880 train_time:112643ms step_avg:61.25ms
step:1840/1880 train_time:112731ms step_avg:61.27ms
step:1841/1880 train_time:112818ms step_avg:61.28ms
step:1842/1880 train_time:112906ms step_avg:61.30ms
step:1843/1880 train_time:112995ms step_avg:61.31ms
step:1844/1880 train_time:113085ms step_avg:61.33ms
step:1845/1880 train_time:113175ms step_avg:61.34ms
step:1846/1880 train_time:113264ms step_avg:61.36ms
step:1847/1880 train_time:113352ms step_avg:61.37ms
step:1848/1880 train_time:113440ms step_avg:61.39ms
step:1849/1880 train_time:113528ms step_avg:61.40ms
step:1850/1880 train_time:113616ms step_avg:61.41ms
step:1851/1880 train_time:113703ms step_avg:61.43ms
step:1852/1880 train_time:113791ms step_avg:61.44ms
step:1853/1880 train_time:113879ms step_avg:61.46ms
step:1854/1880 train_time:113967ms step_avg:61.47ms
step:1855/1880 train_time:114056ms step_avg:61.49ms
step:1856/1880 train_time:114145ms step_avg:61.50ms
step:1857/1880 train_time:114236ms step_avg:61.52ms
step:1858/1880 train_time:114325ms step_avg:61.53ms
step:1859/1880 train_time:114414ms step_avg:61.55ms
step:1860/1880 train_time:114502ms step_avg:61.56ms
step:1861/1880 train_time:114591ms step_avg:61.58ms
step:1862/1880 train_time:114679ms step_avg:61.59ms
step:1863/1880 train_time:114767ms step_avg:61.60ms
step:1864/1880 train_time:114854ms step_avg:61.62ms
step:1865/1880 train_time:114943ms step_avg:61.63ms
step:1866/1880 train_time:115031ms step_avg:61.65ms
step:1867/1880 train_time:115120ms step_avg:61.66ms
step:1868/1880 train_time:115208ms step_avg:61.67ms
step:1869/1880 train_time:115297ms step_avg:61.69ms
step:1870/1880 train_time:115386ms step_avg:61.70ms
step:1871/1880 train_time:115475ms step_avg:61.72ms
step:1872/1880 train_time:115565ms step_avg:61.73ms
step:1873/1880 train_time:115652ms step_avg:61.75ms
step:1874/1880 train_time:115740ms step_avg:61.76ms
step:1875/1880 train_time:115827ms step_avg:61.77ms
step:1876/1880 train_time:115915ms step_avg:61.79ms
step:1877/1880 train_time:116002ms step_avg:61.80ms
step:1878/1880 train_time:116090ms step_avg:61.82ms
step:1879/1880 train_time:116178ms step_avg:61.83ms
step:1880/1880 train_time:116268ms step_avg:61.84ms
step:1880/1880 val_loss:3.2793 train_time:116360ms step_avg:61.89ms
peak memory allocated: 29709 MiB reserved: 44398 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits+5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1840  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec 26 23:29:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     78319      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     78320      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     78321      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     78322      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     78323      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     78324      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     78325      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     78326      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 613, 614, 615, 1226, 1227, 1228, 1839, 1840, 1841] for warmup
Resetting Model
step:0/1880 val_loss:10.8283 train_time:0ms step_avg:0.03ms
step:1/1880 train_time:81ms step_avg:80.82ms
step:2/1880 train_time:105ms step_avg:52.51ms
step:3/1880 train_time:126ms step_avg:41.94ms
step:4/1880 train_time:155ms step_avg:38.84ms
step:5/1880 train_time:189ms step_avg:37.83ms
step:6/1880 train_time:271ms step_avg:45.09ms
step:7/1880 train_time:384ms step_avg:54.89ms
step:8/1880 train_time:418ms step_avg:52.27ms
step:9/1880 train_time:452ms step_avg:50.20ms
step:10/1880 train_time:486ms step_avg:48.59ms
step:11/1880 train_time:520ms step_avg:47.24ms
step:12/1880 train_time:554ms step_avg:46.14ms
step:13/1880 train_time:588ms step_avg:45.20ms
step:14/1880 train_time:622ms step_avg:44.42ms
step:15/1880 train_time:656ms step_avg:43.71ms
step:16/1880 train_time:690ms step_avg:43.10ms
step:17/1880 train_time:724ms step_avg:42.56ms
step:18/1880 train_time:758ms step_avg:42.09ms
step:19/1880 train_time:791ms step_avg:41.66ms
step:20/1880 train_time:826ms step_avg:41.30ms
step:21/1880 train_time:860ms step_avg:40.95ms
step:22/1880 train_time:894ms step_avg:40.64ms
step:23/1880 train_time:928ms step_avg:40.34ms
step:24/1880 train_time:962ms step_avg:40.08ms
step:25/1880 train_time:996ms step_avg:39.83ms
step:26/1880 train_time:1030ms step_avg:39.60ms
step:27/1880 train_time:1064ms step_avg:39.39ms
step:28/1880 train_time:1098ms step_avg:39.21ms
step:29/1880 train_time:1132ms step_avg:39.02ms
step:30/1880 train_time:1166ms step_avg:38.86ms
step:31/1880 train_time:1200ms step_avg:38.69ms
step:32/1880 train_time:1234ms step_avg:38.55ms
step:33/1880 train_time:1267ms step_avg:38.40ms
step:34/1880 train_time:1302ms step_avg:38.29ms
step:35/1880 train_time:1337ms step_avg:38.19ms
step:36/1880 train_time:1372ms step_avg:38.10ms
step:37/1880 train_time:1406ms step_avg:38.00ms
step:38/1880 train_time:1441ms step_avg:37.91ms
step:39/1880 train_time:1475ms step_avg:37.81ms
step:40/1880 train_time:1510ms step_avg:37.74ms
step:41/1880 train_time:1544ms step_avg:37.67ms
step:42/1880 train_time:1579ms step_avg:37.59ms
step:43/1880 train_time:1613ms step_avg:37.51ms
step:44/1880 train_time:1647ms step_avg:37.44ms
step:45/1880 train_time:1681ms step_avg:37.36ms
step:46/1880 train_time:1716ms step_avg:37.29ms
step:47/1880 train_time:1750ms step_avg:37.22ms
step:48/1880 train_time:1784ms step_avg:37.17ms
step:49/1880 train_time:1818ms step_avg:37.10ms
step:50/1880 train_time:1852ms step_avg:37.05ms
step:51/1880 train_time:1887ms step_avg:36.99ms
step:52/1880 train_time:1921ms step_avg:36.94ms
step:53/1880 train_time:1954ms step_avg:36.88ms
step:54/1880 train_time:1988ms step_avg:36.82ms
step:55/1880 train_time:2022ms step_avg:36.77ms
step:56/1880 train_time:2056ms step_avg:36.72ms
step:57/1880 train_time:2090ms step_avg:36.67ms
step:58/1880 train_time:2125ms step_avg:36.63ms
step:59/1880 train_time:2158ms step_avg:36.58ms
step:60/1880 train_time:2193ms step_avg:36.54ms
step:61/1880 train_time:2226ms step_avg:36.50ms
step:62/1880 train_time:2261ms step_avg:36.46ms
step:63/1880 train_time:2294ms step_avg:36.42ms
step:64/1880 train_time:2329ms step_avg:36.39ms
step:65/1880 train_time:2363ms step_avg:36.35ms
step:66/1880 train_time:2397ms step_avg:36.32ms
step:67/1880 train_time:2431ms step_avg:36.29ms
step:68/1880 train_time:2466ms step_avg:36.26ms
step:69/1880 train_time:2500ms step_avg:36.23ms
step:70/1880 train_time:2534ms step_avg:36.20ms
step:71/1880 train_time:2568ms step_avg:36.17ms
step:72/1880 train_time:2602ms step_avg:36.14ms
step:73/1880 train_time:2636ms step_avg:36.11ms
step:74/1880 train_time:2671ms step_avg:36.09ms
step:75/1880 train_time:2705ms step_avg:36.06ms
step:76/1880 train_time:2739ms step_avg:36.04ms
step:77/1880 train_time:2773ms step_avg:36.01ms
step:78/1880 train_time:2807ms step_avg:35.98ms
step:79/1880 train_time:2841ms step_avg:35.96ms
step:80/1880 train_time:2875ms step_avg:35.94ms
step:81/1880 train_time:2909ms step_avg:35.91ms
step:82/1880 train_time:2943ms step_avg:35.89ms
step:83/1880 train_time:2976ms step_avg:35.86ms
step:84/1880 train_time:3011ms step_avg:35.84ms
step:85/1880 train_time:3045ms step_avg:35.82ms
step:86/1880 train_time:3079ms step_avg:35.80ms
step:87/1880 train_time:3113ms step_avg:35.78ms
step:88/1880 train_time:3147ms step_avg:35.76ms
step:89/1880 train_time:3180ms step_avg:35.73ms
step:90/1880 train_time:3215ms step_avg:35.72ms
step:91/1880 train_time:3249ms step_avg:35.70ms
step:92/1880 train_time:3283ms step_avg:35.68ms
step:93/1880 train_time:3317ms step_avg:35.66ms
step:94/1880 train_time:3351ms step_avg:35.65ms
step:95/1880 train_time:3385ms step_avg:35.64ms
step:96/1880 train_time:3420ms step_avg:35.62ms
step:97/1880 train_time:3454ms step_avg:35.60ms
step:98/1880 train_time:3488ms step_avg:35.59ms
step:99/1880 train_time:3522ms step_avg:35.58ms
step:100/1880 train_time:3557ms step_avg:35.57ms
step:101/1880 train_time:3591ms step_avg:35.55ms
step:102/1880 train_time:3625ms step_avg:35.54ms
step:103/1880 train_time:3659ms step_avg:35.52ms
step:104/1880 train_time:3693ms step_avg:35.51ms
step:105/1880 train_time:3727ms step_avg:35.50ms
step:106/1880 train_time:3761ms step_avg:35.48ms
step:107/1880 train_time:3795ms step_avg:35.47ms
step:108/1880 train_time:3829ms step_avg:35.46ms
step:109/1880 train_time:3863ms step_avg:35.44ms
step:110/1880 train_time:3898ms step_avg:35.43ms
step:111/1880 train_time:3931ms step_avg:35.42ms
step:112/1880 train_time:3966ms step_avg:35.41ms
step:113/1880 train_time:4000ms step_avg:35.39ms
step:114/1880 train_time:4034ms step_avg:35.38ms
step:115/1880 train_time:4068ms step_avg:35.37ms
step:116/1880 train_time:4102ms step_avg:35.36ms
step:117/1880 train_time:4136ms step_avg:35.35ms
step:118/1880 train_time:4170ms step_avg:35.34ms
step:119/1880 train_time:4204ms step_avg:35.32ms
step:120/1880 train_time:4238ms step_avg:35.32ms
step:121/1880 train_time:4272ms step_avg:35.30ms
step:122/1880 train_time:4306ms step_avg:35.29ms
step:123/1880 train_time:4340ms step_avg:35.28ms
step:124/1880 train_time:4374ms step_avg:35.27ms
step:125/1880 train_time:4408ms step_avg:35.26ms
step:126/1880 train_time:4442ms step_avg:35.25ms
step:127/1880 train_time:4476ms step_avg:35.24ms
step:128/1880 train_time:4510ms step_avg:35.23ms
step:129/1880 train_time:4544ms step_avg:35.22ms
step:130/1880 train_time:4578ms step_avg:35.22ms
step:131/1880 train_time:4612ms step_avg:35.21ms
step:132/1880 train_time:4647ms step_avg:35.20ms
step:133/1880 train_time:4681ms step_avg:35.20ms
step:134/1880 train_time:4715ms step_avg:35.19ms
step:135/1880 train_time:4749ms step_avg:35.18ms
step:136/1880 train_time:4784ms step_avg:35.17ms
step:137/1880 train_time:4818ms step_avg:35.16ms
step:138/1880 train_time:4852ms step_avg:35.16ms
step:139/1880 train_time:4886ms step_avg:35.15ms
step:140/1880 train_time:4921ms step_avg:35.15ms
step:141/1880 train_time:4954ms step_avg:35.14ms
step:142/1880 train_time:4989ms step_avg:35.13ms
step:143/1880 train_time:5023ms step_avg:35.12ms
step:144/1880 train_time:5057ms step_avg:35.12ms
step:145/1880 train_time:5091ms step_avg:35.11ms
step:146/1880 train_time:5125ms step_avg:35.10ms
step:147/1880 train_time:5159ms step_avg:35.09ms
step:148/1880 train_time:5193ms step_avg:35.09ms
step:149/1880 train_time:5227ms step_avg:35.08ms
step:150/1880 train_time:5261ms step_avg:35.07ms
step:151/1880 train_time:5295ms step_avg:35.06ms
step:152/1880 train_time:5330ms step_avg:35.06ms
step:153/1880 train_time:5363ms step_avg:35.05ms
step:154/1880 train_time:5397ms step_avg:35.05ms
step:155/1880 train_time:5431ms step_avg:35.04ms
step:156/1880 train_time:5465ms step_avg:35.03ms
step:157/1880 train_time:5499ms step_avg:35.02ms
step:158/1880 train_time:5533ms step_avg:35.02ms
step:159/1880 train_time:5566ms step_avg:35.01ms
step:160/1880 train_time:5601ms step_avg:35.00ms
step:161/1880 train_time:5635ms step_avg:35.00ms
step:162/1880 train_time:5669ms step_avg:34.99ms
step:163/1880 train_time:5703ms step_avg:34.99ms
step:164/1880 train_time:5737ms step_avg:34.98ms
step:165/1880 train_time:5771ms step_avg:34.98ms
step:166/1880 train_time:5806ms step_avg:34.97ms
step:167/1880 train_time:5839ms step_avg:34.97ms
step:168/1880 train_time:5873ms step_avg:34.96ms
step:169/1880 train_time:5907ms step_avg:34.95ms
step:170/1880 train_time:5942ms step_avg:34.95ms
step:171/1880 train_time:5975ms step_avg:34.94ms
step:172/1880 train_time:6009ms step_avg:34.94ms
step:173/1880 train_time:6043ms step_avg:34.93ms
step:174/1880 train_time:6078ms step_avg:34.93ms
step:175/1880 train_time:6111ms step_avg:34.92ms
step:176/1880 train_time:6146ms step_avg:34.92ms
step:177/1880 train_time:6180ms step_avg:34.91ms
step:178/1880 train_time:6214ms step_avg:34.91ms
step:179/1880 train_time:6248ms step_avg:34.90ms
step:180/1880 train_time:6282ms step_avg:34.90ms
step:181/1880 train_time:6315ms step_avg:34.89ms
step:182/1880 train_time:6350ms step_avg:34.89ms
step:183/1880 train_time:6384ms step_avg:34.88ms
step:184/1880 train_time:6418ms step_avg:34.88ms
step:185/1880 train_time:6452ms step_avg:34.87ms
step:186/1880 train_time:6486ms step_avg:34.87ms
step:187/1880 train_time:6519ms step_avg:34.86ms
step:188/1880 train_time:6553ms step_avg:34.86ms
step:189/1880 train_time:6587ms step_avg:34.85ms
step:190/1880 train_time:6621ms step_avg:34.85ms
step:191/1880 train_time:6655ms step_avg:34.84ms
step:192/1880 train_time:6689ms step_avg:34.84ms
step:193/1880 train_time:6723ms step_avg:34.84ms
step:194/1880 train_time:6758ms step_avg:34.83ms
step:195/1880 train_time:6792ms step_avg:34.83ms
step:196/1880 train_time:6827ms step_avg:34.83ms
step:197/1880 train_time:6861ms step_avg:34.82ms
step:198/1880 train_time:6895ms step_avg:34.82ms
step:199/1880 train_time:6929ms step_avg:34.82ms
step:200/1880 train_time:6963ms step_avg:34.81ms
step:201/1880 train_time:6997ms step_avg:34.81ms
step:202/1880 train_time:7031ms step_avg:34.81ms
step:203/1880 train_time:7065ms step_avg:34.80ms
step:204/1880 train_time:7099ms step_avg:34.80ms
step:205/1880 train_time:7133ms step_avg:34.79ms
step:206/1880 train_time:7167ms step_avg:34.79ms
step:207/1880 train_time:7201ms step_avg:34.79ms
step:208/1880 train_time:7235ms step_avg:34.78ms
step:209/1880 train_time:7269ms step_avg:34.78ms
step:210/1880 train_time:7303ms step_avg:34.78ms
step:211/1880 train_time:7337ms step_avg:34.77ms
step:212/1880 train_time:7371ms step_avg:34.77ms
step:213/1880 train_time:7405ms step_avg:34.77ms
step:214/1880 train_time:7439ms step_avg:34.76ms
step:215/1880 train_time:7473ms step_avg:34.76ms
step:216/1880 train_time:7507ms step_avg:34.75ms
step:217/1880 train_time:7541ms step_avg:34.75ms
step:218/1880 train_time:7575ms step_avg:34.75ms
step:219/1880 train_time:7609ms step_avg:34.75ms
step:220/1880 train_time:7643ms step_avg:34.74ms
step:221/1880 train_time:7677ms step_avg:34.74ms
step:222/1880 train_time:7711ms step_avg:34.74ms
step:223/1880 train_time:7745ms step_avg:34.73ms
step:224/1880 train_time:7779ms step_avg:34.73ms
step:225/1880 train_time:7813ms step_avg:34.72ms
step:226/1880 train_time:7847ms step_avg:34.72ms
step:227/1880 train_time:7881ms step_avg:34.72ms
step:228/1880 train_time:7915ms step_avg:34.72ms
step:229/1880 train_time:7949ms step_avg:34.71ms
step:230/1880 train_time:7983ms step_avg:34.71ms
step:231/1880 train_time:8017ms step_avg:34.70ms
step:232/1880 train_time:8051ms step_avg:34.70ms
step:233/1880 train_time:8085ms step_avg:34.70ms
step:234/1880 train_time:8119ms step_avg:34.70ms
step:235/1880 train_time:8153ms step_avg:34.69ms
step:236/1880 train_time:8187ms step_avg:34.69ms
step:237/1880 train_time:8221ms step_avg:34.69ms
step:238/1880 train_time:8255ms step_avg:34.68ms
step:239/1880 train_time:8289ms step_avg:34.68ms
step:240/1880 train_time:8323ms step_avg:34.68ms
step:241/1880 train_time:8357ms step_avg:34.67ms
step:242/1880 train_time:8391ms step_avg:34.67ms
step:243/1880 train_time:8425ms step_avg:34.67ms
step:244/1880 train_time:8459ms step_avg:34.67ms
step:245/1880 train_time:8492ms step_avg:34.66ms
step:246/1880 train_time:8527ms step_avg:34.66ms
step:247/1880 train_time:8560ms step_avg:34.66ms
step:248/1880 train_time:8594ms step_avg:34.66ms
step:249/1880 train_time:8628ms step_avg:34.65ms
step:250/1880 train_time:8662ms step_avg:34.65ms
step:250/1880 val_loss:4.5953 train_time:8699ms step_avg:34.80ms
step:251/1880 train_time:8722ms step_avg:34.75ms
step:252/1880 train_time:8741ms step_avg:34.69ms
step:253/1880 train_time:8767ms step_avg:34.65ms
step:254/1880 train_time:8801ms step_avg:34.65ms
step:255/1880 train_time:8836ms step_avg:34.65ms
step:256/1880 train_time:8871ms step_avg:34.65ms
step:257/1880 train_time:8905ms step_avg:34.65ms
step:258/1880 train_time:8940ms step_avg:34.65ms
step:259/1880 train_time:8973ms step_avg:34.65ms
step:260/1880 train_time:9008ms step_avg:34.65ms
step:261/1880 train_time:9041ms step_avg:34.64ms
step:262/1880 train_time:9075ms step_avg:34.64ms
step:263/1880 train_time:9109ms step_avg:34.64ms
step:264/1880 train_time:9143ms step_avg:34.63ms
step:265/1880 train_time:9177ms step_avg:34.63ms
step:266/1880 train_time:9211ms step_avg:34.63ms
step:267/1880 train_time:9245ms step_avg:34.62ms
step:268/1880 train_time:9279ms step_avg:34.62ms
step:269/1880 train_time:9312ms step_avg:34.62ms
step:270/1880 train_time:9346ms step_avg:34.62ms
step:271/1880 train_time:9380ms step_avg:34.61ms
step:272/1880 train_time:9414ms step_avg:34.61ms
step:273/1880 train_time:9448ms step_avg:34.61ms
step:274/1880 train_time:9482ms step_avg:34.61ms
step:275/1880 train_time:9516ms step_avg:34.60ms
step:276/1880 train_time:9550ms step_avg:34.60ms
step:277/1880 train_time:9583ms step_avg:34.60ms
step:278/1880 train_time:9617ms step_avg:34.59ms
step:279/1880 train_time:9651ms step_avg:34.59ms
step:280/1880 train_time:9686ms step_avg:34.59ms
step:281/1880 train_time:9720ms step_avg:34.59ms
step:282/1880 train_time:9754ms step_avg:34.59ms
step:283/1880 train_time:9788ms step_avg:34.59ms
step:284/1880 train_time:9823ms step_avg:34.59ms
step:285/1880 train_time:9857ms step_avg:34.59ms
step:286/1880 train_time:9892ms step_avg:34.59ms
step:287/1880 train_time:9926ms step_avg:34.58ms
step:288/1880 train_time:9960ms step_avg:34.58ms
step:289/1880 train_time:9993ms step_avg:34.58ms
step:290/1880 train_time:10028ms step_avg:34.58ms
step:291/1880 train_time:10061ms step_avg:34.58ms
step:292/1880 train_time:10095ms step_avg:34.57ms
step:293/1880 train_time:10129ms step_avg:34.57ms
step:294/1880 train_time:10163ms step_avg:34.57ms
step:295/1880 train_time:10197ms step_avg:34.57ms
step:296/1880 train_time:10231ms step_avg:34.57ms
step:297/1880 train_time:10265ms step_avg:34.56ms
step:298/1880 train_time:10299ms step_avg:34.56ms
step:299/1880 train_time:10333ms step_avg:34.56ms
step:300/1880 train_time:10367ms step_avg:34.56ms
step:301/1880 train_time:10400ms step_avg:34.55ms
step:302/1880 train_time:10434ms step_avg:34.55ms
step:303/1880 train_time:10468ms step_avg:34.55ms
step:304/1880 train_time:10502ms step_avg:34.55ms
step:305/1880 train_time:10536ms step_avg:34.54ms
step:306/1880 train_time:10570ms step_avg:34.54ms
step:307/1880 train_time:10604ms step_avg:34.54ms
step:308/1880 train_time:10638ms step_avg:34.54ms
step:309/1880 train_time:10672ms step_avg:34.54ms
step:310/1880 train_time:10706ms step_avg:34.54ms
step:311/1880 train_time:10740ms step_avg:34.53ms
step:312/1880 train_time:10774ms step_avg:34.53ms
step:313/1880 train_time:10808ms step_avg:34.53ms
step:314/1880 train_time:10842ms step_avg:34.53ms
step:315/1880 train_time:10877ms step_avg:34.53ms
step:316/1880 train_time:10911ms step_avg:34.53ms
step:317/1880 train_time:10945ms step_avg:34.53ms
step:318/1880 train_time:10979ms step_avg:34.53ms
step:319/1880 train_time:11013ms step_avg:34.52ms
step:320/1880 train_time:11047ms step_avg:34.52ms
step:321/1880 train_time:11081ms step_avg:34.52ms
step:322/1880 train_time:11115ms step_avg:34.52ms
step:323/1880 train_time:11149ms step_avg:34.52ms
step:324/1880 train_time:11183ms step_avg:34.52ms
step:325/1880 train_time:11217ms step_avg:34.51ms
step:326/1880 train_time:11252ms step_avg:34.51ms
step:327/1880 train_time:11285ms step_avg:34.51ms
step:328/1880 train_time:11319ms step_avg:34.51ms
step:329/1880 train_time:11353ms step_avg:34.51ms
step:330/1880 train_time:11387ms step_avg:34.51ms
step:331/1880 train_time:11421ms step_avg:34.50ms
step:332/1880 train_time:11455ms step_avg:34.50ms
step:333/1880 train_time:11489ms step_avg:34.50ms
step:334/1880 train_time:11523ms step_avg:34.50ms
step:335/1880 train_time:11556ms step_avg:34.50ms
step:336/1880 train_time:11591ms step_avg:34.50ms
step:337/1880 train_time:11624ms step_avg:34.49ms
step:338/1880 train_time:11659ms step_avg:34.49ms
step:339/1880 train_time:11693ms step_avg:34.49ms
step:340/1880 train_time:11727ms step_avg:34.49ms
step:341/1880 train_time:11760ms step_avg:34.49ms
step:342/1880 train_time:11794ms step_avg:34.49ms
step:343/1880 train_time:11828ms step_avg:34.48ms
step:344/1880 train_time:11862ms step_avg:34.48ms
step:345/1880 train_time:11897ms step_avg:34.48ms
step:346/1880 train_time:11931ms step_avg:34.48ms
step:347/1880 train_time:11965ms step_avg:34.48ms
step:348/1880 train_time:11999ms step_avg:34.48ms
step:349/1880 train_time:12033ms step_avg:34.48ms
step:350/1880 train_time:12067ms step_avg:34.48ms
step:351/1880 train_time:12101ms step_avg:34.48ms
step:352/1880 train_time:12135ms step_avg:34.47ms
step:353/1880 train_time:12168ms step_avg:34.47ms
step:354/1880 train_time:12203ms step_avg:34.47ms
step:355/1880 train_time:12237ms step_avg:34.47ms
step:356/1880 train_time:12271ms step_avg:34.47ms
step:357/1880 train_time:12305ms step_avg:34.47ms
step:358/1880 train_time:12339ms step_avg:34.47ms
step:359/1880 train_time:12373ms step_avg:34.46ms
step:360/1880 train_time:12406ms step_avg:34.46ms
step:361/1880 train_time:12440ms step_avg:34.46ms
step:362/1880 train_time:12474ms step_avg:34.46ms
step:363/1880 train_time:12508ms step_avg:34.46ms
step:364/1880 train_time:12542ms step_avg:34.46ms
step:365/1880 train_time:12575ms step_avg:34.45ms
step:366/1880 train_time:12610ms step_avg:34.45ms
step:367/1880 train_time:12644ms step_avg:34.45ms
step:368/1880 train_time:12678ms step_avg:34.45ms
step:369/1880 train_time:12712ms step_avg:34.45ms
step:370/1880 train_time:12746ms step_avg:34.45ms
step:371/1880 train_time:12780ms step_avg:34.45ms
step:372/1880 train_time:12814ms step_avg:34.45ms
step:373/1880 train_time:12848ms step_avg:34.44ms
step:374/1880 train_time:12882ms step_avg:34.44ms
step:375/1880 train_time:12916ms step_avg:34.44ms
step:376/1880 train_time:12951ms step_avg:34.44ms
step:377/1880 train_time:12985ms step_avg:34.44ms
step:378/1880 train_time:13019ms step_avg:34.44ms
step:379/1880 train_time:13053ms step_avg:34.44ms
step:380/1880 train_time:13088ms step_avg:34.44ms
step:381/1880 train_time:13121ms step_avg:34.44ms
step:382/1880 train_time:13155ms step_avg:34.44ms
step:383/1880 train_time:13189ms step_avg:34.44ms
step:384/1880 train_time:13224ms step_avg:34.44ms
step:385/1880 train_time:13258ms step_avg:34.44ms
step:386/1880 train_time:13292ms step_avg:34.44ms
step:387/1880 train_time:13326ms step_avg:34.43ms
step:388/1880 train_time:13360ms step_avg:34.43ms
step:389/1880 train_time:13394ms step_avg:34.43ms
step:390/1880 train_time:13428ms step_avg:34.43ms
step:391/1880 train_time:13461ms step_avg:34.43ms
step:392/1880 train_time:13496ms step_avg:34.43ms
step:393/1880 train_time:13529ms step_avg:34.43ms
step:394/1880 train_time:13564ms step_avg:34.43ms
step:395/1880 train_time:13598ms step_avg:34.42ms
step:396/1880 train_time:13632ms step_avg:34.42ms
step:397/1880 train_time:13666ms step_avg:34.42ms
step:398/1880 train_time:13700ms step_avg:34.42ms
step:399/1880 train_time:13734ms step_avg:34.42ms
step:400/1880 train_time:13768ms step_avg:34.42ms
step:401/1880 train_time:13802ms step_avg:34.42ms
step:402/1880 train_time:13836ms step_avg:34.42ms
step:403/1880 train_time:13869ms step_avg:34.42ms
step:404/1880 train_time:13903ms step_avg:34.41ms
step:405/1880 train_time:13937ms step_avg:34.41ms
step:406/1880 train_time:13972ms step_avg:34.41ms
step:407/1880 train_time:14006ms step_avg:34.41ms
step:408/1880 train_time:14040ms step_avg:34.41ms
step:409/1880 train_time:14073ms step_avg:34.41ms
step:410/1880 train_time:14107ms step_avg:34.41ms
step:411/1880 train_time:14141ms step_avg:34.41ms
step:412/1880 train_time:14175ms step_avg:34.41ms
step:413/1880 train_time:14209ms step_avg:34.41ms
step:414/1880 train_time:14244ms step_avg:34.40ms
step:415/1880 train_time:14278ms step_avg:34.40ms
step:416/1880 train_time:14312ms step_avg:34.40ms
step:417/1880 train_time:14346ms step_avg:34.40ms
step:418/1880 train_time:14379ms step_avg:34.40ms
step:419/1880 train_time:14413ms step_avg:34.40ms
step:420/1880 train_time:14447ms step_avg:34.40ms
step:421/1880 train_time:14481ms step_avg:34.40ms
step:422/1880 train_time:14515ms step_avg:34.39ms
step:423/1880 train_time:14548ms step_avg:34.39ms
step:424/1880 train_time:14583ms step_avg:34.39ms
step:425/1880 train_time:14617ms step_avg:34.39ms
step:426/1880 train_time:14651ms step_avg:34.39ms
step:427/1880 train_time:14685ms step_avg:34.39ms
step:428/1880 train_time:14720ms step_avg:34.39ms
step:429/1880 train_time:14753ms step_avg:34.39ms
step:430/1880 train_time:14787ms step_avg:34.39ms
step:431/1880 train_time:14821ms step_avg:34.39ms
step:432/1880 train_time:14855ms step_avg:34.39ms
step:433/1880 train_time:14888ms step_avg:34.38ms
step:434/1880 train_time:14922ms step_avg:34.38ms
step:435/1880 train_time:14956ms step_avg:34.38ms
step:436/1880 train_time:14991ms step_avg:34.38ms
step:437/1880 train_time:15024ms step_avg:34.38ms
step:438/1880 train_time:15059ms step_avg:34.38ms
step:439/1880 train_time:15092ms step_avg:34.38ms
step:440/1880 train_time:15127ms step_avg:34.38ms
step:441/1880 train_time:15160ms step_avg:34.38ms
step:442/1880 train_time:15195ms step_avg:34.38ms
step:443/1880 train_time:15228ms step_avg:34.38ms
step:444/1880 train_time:15262ms step_avg:34.37ms
step:445/1880 train_time:15297ms step_avg:34.37ms
step:446/1880 train_time:15331ms step_avg:34.37ms
step:447/1880 train_time:15365ms step_avg:34.37ms
step:448/1880 train_time:15399ms step_avg:34.37ms
step:449/1880 train_time:15433ms step_avg:34.37ms
step:450/1880 train_time:15467ms step_avg:34.37ms
step:451/1880 train_time:15501ms step_avg:34.37ms
step:452/1880 train_time:15535ms step_avg:34.37ms
step:453/1880 train_time:15569ms step_avg:34.37ms
step:454/1880 train_time:15603ms step_avg:34.37ms
step:455/1880 train_time:15637ms step_avg:34.37ms
step:456/1880 train_time:15672ms step_avg:34.37ms
step:457/1880 train_time:15705ms step_avg:34.37ms
step:458/1880 train_time:15739ms step_avg:34.37ms
step:459/1880 train_time:15773ms step_avg:34.36ms
step:460/1880 train_time:15807ms step_avg:34.36ms
step:461/1880 train_time:15841ms step_avg:34.36ms
step:462/1880 train_time:15875ms step_avg:34.36ms
step:463/1880 train_time:15908ms step_avg:34.36ms
step:464/1880 train_time:15942ms step_avg:34.36ms
step:465/1880 train_time:15976ms step_avg:34.36ms
step:466/1880 train_time:16011ms step_avg:34.36ms
step:467/1880 train_time:16044ms step_avg:34.36ms
step:468/1880 train_time:16079ms step_avg:34.36ms
step:469/1880 train_time:16112ms step_avg:34.35ms
step:470/1880 train_time:16146ms step_avg:34.35ms
step:471/1880 train_time:16180ms step_avg:34.35ms
step:472/1880 train_time:16215ms step_avg:34.35ms
step:473/1880 train_time:16248ms step_avg:34.35ms
step:474/1880 train_time:16282ms step_avg:34.35ms
step:475/1880 train_time:16316ms step_avg:34.35ms
step:476/1880 train_time:16351ms step_avg:34.35ms
step:477/1880 train_time:16385ms step_avg:34.35ms
step:478/1880 train_time:16419ms step_avg:34.35ms
step:479/1880 train_time:16453ms step_avg:34.35ms
step:480/1880 train_time:16487ms step_avg:34.35ms
step:481/1880 train_time:16521ms step_avg:34.35ms
step:482/1880 train_time:16555ms step_avg:34.35ms
step:483/1880 train_time:16589ms step_avg:34.35ms
step:484/1880 train_time:16623ms step_avg:34.35ms
step:485/1880 train_time:16657ms step_avg:34.34ms
step:486/1880 train_time:16691ms step_avg:34.34ms
step:487/1880 train_time:16725ms step_avg:34.34ms
step:488/1880 train_time:16759ms step_avg:34.34ms
step:489/1880 train_time:16793ms step_avg:34.34ms
step:490/1880 train_time:16827ms step_avg:34.34ms
step:491/1880 train_time:16861ms step_avg:34.34ms
step:492/1880 train_time:16895ms step_avg:34.34ms
step:493/1880 train_time:16929ms step_avg:34.34ms
step:494/1880 train_time:16963ms step_avg:34.34ms
step:495/1880 train_time:16997ms step_avg:34.34ms
step:496/1880 train_time:17031ms step_avg:34.34ms
step:497/1880 train_time:17065ms step_avg:34.34ms
step:498/1880 train_time:17099ms step_avg:34.34ms
step:499/1880 train_time:17133ms step_avg:34.33ms
step:500/1880 train_time:17167ms step_avg:34.33ms
step:500/1880 val_loss:4.2865 train_time:17204ms step_avg:34.41ms
step:501/1880 train_time:17227ms step_avg:34.38ms
step:502/1880 train_time:17246ms step_avg:34.35ms
step:503/1880 train_time:17272ms step_avg:34.34ms
step:504/1880 train_time:17307ms step_avg:34.34ms
step:505/1880 train_time:17342ms step_avg:34.34ms
step:506/1880 train_time:17377ms step_avg:34.34ms
step:507/1880 train_time:17412ms step_avg:34.34ms
step:508/1880 train_time:17447ms step_avg:34.34ms
step:509/1880 train_time:17481ms step_avg:34.34ms
step:510/1880 train_time:17515ms step_avg:34.34ms
step:511/1880 train_time:17549ms step_avg:34.34ms
step:512/1880 train_time:17584ms step_avg:34.34ms
step:513/1880 train_time:17618ms step_avg:34.34ms
step:514/1880 train_time:17652ms step_avg:34.34ms
step:515/1880 train_time:17686ms step_avg:34.34ms
step:516/1880 train_time:17720ms step_avg:34.34ms
step:517/1880 train_time:17754ms step_avg:34.34ms
step:518/1880 train_time:17788ms step_avg:34.34ms
step:519/1880 train_time:17822ms step_avg:34.34ms
step:520/1880 train_time:17856ms step_avg:34.34ms
step:521/1880 train_time:17889ms step_avg:34.34ms
step:522/1880 train_time:17923ms step_avg:34.34ms
step:523/1880 train_time:17957ms step_avg:34.33ms
step:524/1880 train_time:17991ms step_avg:34.33ms
step:525/1880 train_time:18025ms step_avg:34.33ms
step:526/1880 train_time:18059ms step_avg:34.33ms
step:527/1880 train_time:18093ms step_avg:34.33ms
step:528/1880 train_time:18127ms step_avg:34.33ms
step:529/1880 train_time:18160ms step_avg:34.33ms
step:530/1880 train_time:18194ms step_avg:34.33ms
step:531/1880 train_time:18228ms step_avg:34.33ms
step:532/1880 train_time:18263ms step_avg:34.33ms
step:533/1880 train_time:18297ms step_avg:34.33ms
step:534/1880 train_time:18331ms step_avg:34.33ms
step:535/1880 train_time:18365ms step_avg:34.33ms
step:536/1880 train_time:18399ms step_avg:34.33ms
step:537/1880 train_time:18433ms step_avg:34.33ms
step:538/1880 train_time:18468ms step_avg:34.33ms
step:539/1880 train_time:18502ms step_avg:34.33ms
step:540/1880 train_time:18537ms step_avg:34.33ms
step:541/1880 train_time:18571ms step_avg:34.33ms
step:542/1880 train_time:18605ms step_avg:34.33ms
step:543/1880 train_time:18639ms step_avg:34.33ms
step:544/1880 train_time:18673ms step_avg:34.32ms
step:545/1880 train_time:18707ms step_avg:34.32ms
step:546/1880 train_time:18741ms step_avg:34.32ms
step:547/1880 train_time:18775ms step_avg:34.32ms
step:548/1880 train_time:18809ms step_avg:34.32ms
step:549/1880 train_time:18842ms step_avg:34.32ms
step:550/1880 train_time:18876ms step_avg:34.32ms
step:551/1880 train_time:18910ms step_avg:34.32ms
step:552/1880 train_time:18944ms step_avg:34.32ms
step:553/1880 train_time:18978ms step_avg:34.32ms
step:554/1880 train_time:19012ms step_avg:34.32ms
step:555/1880 train_time:19046ms step_avg:34.32ms
step:556/1880 train_time:19081ms step_avg:34.32ms
step:557/1880 train_time:19115ms step_avg:34.32ms
step:558/1880 train_time:19149ms step_avg:34.32ms
step:559/1880 train_time:19183ms step_avg:34.32ms
step:560/1880 train_time:19217ms step_avg:34.32ms
step:561/1880 train_time:19251ms step_avg:34.31ms
step:562/1880 train_time:19285ms step_avg:34.32ms
step:563/1880 train_time:19319ms step_avg:34.31ms
step:564/1880 train_time:19354ms step_avg:34.31ms
step:565/1880 train_time:19388ms step_avg:34.31ms
step:566/1880 train_time:19422ms step_avg:34.31ms
step:567/1880 train_time:19456ms step_avg:34.31ms
step:568/1880 train_time:19490ms step_avg:34.31ms
step:569/1880 train_time:19524ms step_avg:34.31ms
step:570/1880 train_time:19559ms step_avg:34.31ms
step:571/1880 train_time:19592ms step_avg:34.31ms
step:572/1880 train_time:19626ms step_avg:34.31ms
step:573/1880 train_time:19660ms step_avg:34.31ms
step:574/1880 train_time:19694ms step_avg:34.31ms
step:575/1880 train_time:19728ms step_avg:34.31ms
step:576/1880 train_time:19763ms step_avg:34.31ms
step:577/1880 train_time:19796ms step_avg:34.31ms
step:578/1880 train_time:19830ms step_avg:34.31ms
step:579/1880 train_time:19864ms step_avg:34.31ms
step:580/1880 train_time:19898ms step_avg:34.31ms
step:581/1880 train_time:19932ms step_avg:34.31ms
step:582/1880 train_time:19966ms step_avg:34.31ms
step:583/1880 train_time:20000ms step_avg:34.31ms
step:584/1880 train_time:20034ms step_avg:34.30ms
step:585/1880 train_time:20068ms step_avg:34.30ms
step:586/1880 train_time:20102ms step_avg:34.30ms
step:587/1880 train_time:20136ms step_avg:34.30ms
step:588/1880 train_time:20169ms step_avg:34.30ms
step:589/1880 train_time:20204ms step_avg:34.30ms
step:590/1880 train_time:20238ms step_avg:34.30ms
step:591/1880 train_time:20272ms step_avg:34.30ms
step:592/1880 train_time:20306ms step_avg:34.30ms
step:593/1880 train_time:20339ms step_avg:34.30ms
step:594/1880 train_time:20373ms step_avg:34.30ms
step:595/1880 train_time:20407ms step_avg:34.30ms
step:596/1880 train_time:20441ms step_avg:34.30ms
step:597/1880 train_time:20475ms step_avg:34.30ms
step:598/1880 train_time:20510ms step_avg:34.30ms
step:599/1880 train_time:20544ms step_avg:34.30ms
step:600/1880 train_time:20579ms step_avg:34.30ms
step:601/1880 train_time:20612ms step_avg:34.30ms
step:602/1880 train_time:20647ms step_avg:34.30ms
step:603/1880 train_time:20681ms step_avg:34.30ms
step:604/1880 train_time:20714ms step_avg:34.30ms
step:605/1880 train_time:20749ms step_avg:34.30ms
step:606/1880 train_time:20783ms step_avg:34.29ms
step:607/1880 train_time:20816ms step_avg:34.29ms
step:608/1880 train_time:20850ms step_avg:34.29ms
step:609/1880 train_time:20884ms step_avg:34.29ms
step:610/1880 train_time:20918ms step_avg:34.29ms
step:611/1880 train_time:20953ms step_avg:34.29ms
step:612/1880 train_time:20987ms step_avg:34.29ms
step:613/1880 train_time:21021ms step_avg:34.29ms
step:614/1880 train_time:21055ms step_avg:34.29ms
step:615/1880 train_time:21090ms step_avg:34.29ms
step:616/1880 train_time:21150ms step_avg:34.33ms
step:617/1880 train_time:21212ms step_avg:34.38ms
step:618/1880 train_time:21274ms step_avg:34.42ms
step:619/1880 train_time:21337ms step_avg:34.47ms
step:620/1880 train_time:21398ms step_avg:34.51ms
step:621/1880 train_time:21460ms step_avg:34.56ms
step:622/1880 train_time:21522ms step_avg:34.60ms
step:623/1880 train_time:21583ms step_avg:34.64ms
step:624/1880 train_time:21644ms step_avg:34.69ms
step:625/1880 train_time:21706ms step_avg:34.73ms
step:626/1880 train_time:21767ms step_avg:34.77ms
step:627/1880 train_time:21829ms step_avg:34.81ms
step:628/1880 train_time:21890ms step_avg:34.86ms
step:629/1880 train_time:21952ms step_avg:34.90ms
step:630/1880 train_time:22013ms step_avg:34.94ms
step:631/1880 train_time:22075ms step_avg:34.98ms
step:632/1880 train_time:22136ms step_avg:35.03ms
step:633/1880 train_time:22198ms step_avg:35.07ms
step:634/1880 train_time:22260ms step_avg:35.11ms
step:635/1880 train_time:22322ms step_avg:35.15ms
step:636/1880 train_time:22383ms step_avg:35.19ms
step:637/1880 train_time:22445ms step_avg:35.23ms
step:638/1880 train_time:22505ms step_avg:35.27ms
step:639/1880 train_time:22567ms step_avg:35.32ms
step:640/1880 train_time:22628ms step_avg:35.36ms
step:641/1880 train_time:22690ms step_avg:35.40ms
step:642/1880 train_time:22751ms step_avg:35.44ms
step:643/1880 train_time:22813ms step_avg:35.48ms
step:644/1880 train_time:22875ms step_avg:35.52ms
step:645/1880 train_time:22937ms step_avg:35.56ms
step:646/1880 train_time:22998ms step_avg:35.60ms
step:647/1880 train_time:23060ms step_avg:35.64ms
step:648/1880 train_time:23121ms step_avg:35.68ms
step:649/1880 train_time:23182ms step_avg:35.72ms
step:650/1880 train_time:23243ms step_avg:35.76ms
step:651/1880 train_time:23305ms step_avg:35.80ms
step:652/1880 train_time:23366ms step_avg:35.84ms
step:653/1880 train_time:23428ms step_avg:35.88ms
step:654/1880 train_time:23491ms step_avg:35.92ms
step:655/1880 train_time:23553ms step_avg:35.96ms
step:656/1880 train_time:23615ms step_avg:36.00ms
step:657/1880 train_time:23678ms step_avg:36.04ms
step:658/1880 train_time:23739ms step_avg:36.08ms
step:659/1880 train_time:23801ms step_avg:36.12ms
step:660/1880 train_time:23862ms step_avg:36.15ms
step:661/1880 train_time:23924ms step_avg:36.19ms
step:662/1880 train_time:23985ms step_avg:36.23ms
step:663/1880 train_time:24047ms step_avg:36.27ms
step:664/1880 train_time:24108ms step_avg:36.31ms
step:665/1880 train_time:24170ms step_avg:36.35ms
step:666/1880 train_time:24231ms step_avg:36.38ms
step:667/1880 train_time:24293ms step_avg:36.42ms
step:668/1880 train_time:24355ms step_avg:36.46ms
step:669/1880 train_time:24418ms step_avg:36.50ms
step:670/1880 train_time:24479ms step_avg:36.54ms
step:671/1880 train_time:24540ms step_avg:36.57ms
step:672/1880 train_time:24601ms step_avg:36.61ms
step:673/1880 train_time:24663ms step_avg:36.65ms
step:674/1880 train_time:24724ms step_avg:36.68ms
step:675/1880 train_time:24785ms step_avg:36.72ms
step:676/1880 train_time:24846ms step_avg:36.75ms
step:677/1880 train_time:24908ms step_avg:36.79ms
step:678/1880 train_time:24969ms step_avg:36.83ms
step:679/1880 train_time:25030ms step_avg:36.86ms
step:680/1880 train_time:25092ms step_avg:36.90ms
step:681/1880 train_time:25153ms step_avg:36.94ms
step:682/1880 train_time:25214ms step_avg:36.97ms
step:683/1880 train_time:25276ms step_avg:37.01ms
step:684/1880 train_time:25337ms step_avg:37.04ms
step:685/1880 train_time:25399ms step_avg:37.08ms
step:686/1880 train_time:25461ms step_avg:37.12ms
step:687/1880 train_time:25522ms step_avg:37.15ms
step:688/1880 train_time:25584ms step_avg:37.19ms
step:689/1880 train_time:25645ms step_avg:37.22ms
step:690/1880 train_time:25707ms step_avg:37.26ms
step:691/1880 train_time:25769ms step_avg:37.29ms
step:692/1880 train_time:25830ms step_avg:37.33ms
step:693/1880 train_time:25892ms step_avg:37.36ms
step:694/1880 train_time:25954ms step_avg:37.40ms
step:695/1880 train_time:26015ms step_avg:37.43ms
step:696/1880 train_time:26077ms step_avg:37.47ms
step:697/1880 train_time:26139ms step_avg:37.50ms
step:698/1880 train_time:26200ms step_avg:37.54ms
step:699/1880 train_time:26262ms step_avg:37.57ms
step:700/1880 train_time:26324ms step_avg:37.61ms
step:701/1880 train_time:26386ms step_avg:37.64ms
step:702/1880 train_time:26447ms step_avg:37.67ms
step:703/1880 train_time:26508ms step_avg:37.71ms
step:704/1880 train_time:26569ms step_avg:37.74ms
step:705/1880 train_time:26631ms step_avg:37.77ms
step:706/1880 train_time:26693ms step_avg:37.81ms
step:707/1880 train_time:26755ms step_avg:37.84ms
step:708/1880 train_time:26816ms step_avg:37.88ms
step:709/1880 train_time:26878ms step_avg:37.91ms
step:710/1880 train_time:26939ms step_avg:37.94ms
step:711/1880 train_time:27001ms step_avg:37.98ms
step:712/1880 train_time:27062ms step_avg:38.01ms
step:713/1880 train_time:27124ms step_avg:38.04ms
step:714/1880 train_time:27185ms step_avg:38.07ms
step:715/1880 train_time:27246ms step_avg:38.11ms
step:716/1880 train_time:27308ms step_avg:38.14ms
step:717/1880 train_time:27370ms step_avg:38.17ms
step:718/1880 train_time:27431ms step_avg:38.20ms
step:719/1880 train_time:27494ms step_avg:38.24ms
step:720/1880 train_time:27555ms step_avg:38.27ms
step:721/1880 train_time:27617ms step_avg:38.30ms
step:722/1880 train_time:27678ms step_avg:38.34ms
step:723/1880 train_time:27740ms step_avg:38.37ms
step:724/1880 train_time:27801ms step_avg:38.40ms
step:725/1880 train_time:27863ms step_avg:38.43ms
step:726/1880 train_time:27924ms step_avg:38.46ms
step:727/1880 train_time:27986ms step_avg:38.49ms
step:728/1880 train_time:28046ms step_avg:38.53ms
step:729/1880 train_time:28108ms step_avg:38.56ms
step:730/1880 train_time:28169ms step_avg:38.59ms
step:731/1880 train_time:28231ms step_avg:38.62ms
step:732/1880 train_time:28294ms step_avg:38.65ms
step:733/1880 train_time:28355ms step_avg:38.68ms
step:734/1880 train_time:28417ms step_avg:38.72ms
step:735/1880 train_time:28479ms step_avg:38.75ms
step:736/1880 train_time:28540ms step_avg:38.78ms
step:737/1880 train_time:28602ms step_avg:38.81ms
step:738/1880 train_time:28664ms step_avg:38.84ms
step:739/1880 train_time:28725ms step_avg:38.87ms
step:740/1880 train_time:28787ms step_avg:38.90ms
step:741/1880 train_time:28848ms step_avg:38.93ms
step:742/1880 train_time:28910ms step_avg:38.96ms
step:743/1880 train_time:28973ms step_avg:38.99ms
step:744/1880 train_time:29034ms step_avg:39.02ms
step:745/1880 train_time:29096ms step_avg:39.05ms
step:746/1880 train_time:29157ms step_avg:39.08ms
step:747/1880 train_time:29219ms step_avg:39.12ms
step:748/1880 train_time:29280ms step_avg:39.14ms
step:749/1880 train_time:29341ms step_avg:39.17ms
step:750/1880 train_time:29403ms step_avg:39.20ms
step:750/1880 val_loss:4.0299 train_time:29467ms step_avg:39.29ms
step:751/1880 train_time:29489ms step_avg:39.27ms
step:752/1880 train_time:29528ms step_avg:39.27ms
step:753/1880 train_time:29591ms step_avg:39.30ms
step:754/1880 train_time:29654ms step_avg:39.33ms
step:755/1880 train_time:29716ms step_avg:39.36ms
step:756/1880 train_time:29778ms step_avg:39.39ms
step:757/1880 train_time:29839ms step_avg:39.42ms
step:758/1880 train_time:29900ms step_avg:39.45ms
step:759/1880 train_time:29962ms step_avg:39.48ms
step:760/1880 train_time:30022ms step_avg:39.50ms
step:761/1880 train_time:30084ms step_avg:39.53ms
step:762/1880 train_time:30144ms step_avg:39.56ms
step:763/1880 train_time:30206ms step_avg:39.59ms
step:764/1880 train_time:30267ms step_avg:39.62ms
step:765/1880 train_time:30328ms step_avg:39.64ms
step:766/1880 train_time:30390ms step_avg:39.67ms
step:767/1880 train_time:30455ms step_avg:39.71ms
step:768/1880 train_time:30517ms step_avg:39.74ms
step:769/1880 train_time:30579ms step_avg:39.77ms
step:770/1880 train_time:30641ms step_avg:39.79ms
step:771/1880 train_time:30704ms step_avg:39.82ms
step:772/1880 train_time:30765ms step_avg:39.85ms
step:773/1880 train_time:30827ms step_avg:39.88ms
step:774/1880 train_time:30887ms step_avg:39.91ms
step:775/1880 train_time:30949ms step_avg:39.93ms
step:776/1880 train_time:31009ms step_avg:39.96ms
step:777/1880 train_time:31070ms step_avg:39.99ms
step:778/1880 train_time:31132ms step_avg:40.01ms
step:779/1880 train_time:31193ms step_avg:40.04ms
step:780/1880 train_time:31254ms step_avg:40.07ms
step:781/1880 train_time:31315ms step_avg:40.10ms
step:782/1880 train_time:31377ms step_avg:40.12ms
step:783/1880 train_time:31439ms step_avg:40.15ms
step:784/1880 train_time:31501ms step_avg:40.18ms
step:785/1880 train_time:31563ms step_avg:40.21ms
step:786/1880 train_time:31625ms step_avg:40.23ms
step:787/1880 train_time:31687ms step_avg:40.26ms
step:788/1880 train_time:31748ms step_avg:40.29ms
step:789/1880 train_time:31810ms step_avg:40.32ms
step:790/1880 train_time:31870ms step_avg:40.34ms
step:791/1880 train_time:31932ms step_avg:40.37ms
step:792/1880 train_time:31992ms step_avg:40.39ms
step:793/1880 train_time:32054ms step_avg:40.42ms
step:794/1880 train_time:32114ms step_avg:40.45ms
step:795/1880 train_time:32176ms step_avg:40.47ms
step:796/1880 train_time:32237ms step_avg:40.50ms
step:797/1880 train_time:32298ms step_avg:40.52ms
step:798/1880 train_time:32359ms step_avg:40.55ms
step:799/1880 train_time:32421ms step_avg:40.58ms
step:800/1880 train_time:32483ms step_avg:40.60ms
step:801/1880 train_time:32545ms step_avg:40.63ms
step:802/1880 train_time:32606ms step_avg:40.66ms
step:803/1880 train_time:32669ms step_avg:40.68ms
step:804/1880 train_time:32730ms step_avg:40.71ms
step:805/1880 train_time:32792ms step_avg:40.74ms
step:806/1880 train_time:32853ms step_avg:40.76ms
step:807/1880 train_time:32915ms step_avg:40.79ms
step:808/1880 train_time:32977ms step_avg:40.81ms
step:809/1880 train_time:33038ms step_avg:40.84ms
step:810/1880 train_time:33100ms step_avg:40.86ms
step:811/1880 train_time:33162ms step_avg:40.89ms
step:812/1880 train_time:33222ms step_avg:40.91ms
step:813/1880 train_time:33284ms step_avg:40.94ms
step:814/1880 train_time:33345ms step_avg:40.96ms
step:815/1880 train_time:33407ms step_avg:40.99ms
step:816/1880 train_time:33468ms step_avg:41.01ms
step:817/1880 train_time:33530ms step_avg:41.04ms
step:818/1880 train_time:33591ms step_avg:41.07ms
step:819/1880 train_time:33654ms step_avg:41.09ms
step:820/1880 train_time:33716ms step_avg:41.12ms
step:821/1880 train_time:33777ms step_avg:41.14ms
step:822/1880 train_time:33838ms step_avg:41.17ms
step:823/1880 train_time:33899ms step_avg:41.19ms
step:824/1880 train_time:33960ms step_avg:41.21ms
step:825/1880 train_time:34022ms step_avg:41.24ms
step:826/1880 train_time:34083ms step_avg:41.26ms
step:827/1880 train_time:34144ms step_avg:41.29ms
step:828/1880 train_time:34205ms step_avg:41.31ms
step:829/1880 train_time:34267ms step_avg:41.33ms
step:830/1880 train_time:34328ms step_avg:41.36ms
step:831/1880 train_time:34390ms step_avg:41.38ms
step:832/1880 train_time:34450ms step_avg:41.41ms
step:833/1880 train_time:34513ms step_avg:41.43ms
step:834/1880 train_time:34575ms step_avg:41.46ms
step:835/1880 train_time:34637ms step_avg:41.48ms
step:836/1880 train_time:34699ms step_avg:41.51ms
step:837/1880 train_time:34761ms step_avg:41.53ms
step:838/1880 train_time:34822ms step_avg:41.55ms
step:839/1880 train_time:34884ms step_avg:41.58ms
step:840/1880 train_time:34945ms step_avg:41.60ms
step:841/1880 train_time:35007ms step_avg:41.63ms
step:842/1880 train_time:35068ms step_avg:41.65ms
step:843/1880 train_time:35129ms step_avg:41.67ms
step:844/1880 train_time:35190ms step_avg:41.69ms
step:845/1880 train_time:35252ms step_avg:41.72ms
step:846/1880 train_time:35313ms step_avg:41.74ms
step:847/1880 train_time:35375ms step_avg:41.77ms
step:848/1880 train_time:35437ms step_avg:41.79ms
step:849/1880 train_time:35499ms step_avg:41.81ms
step:850/1880 train_time:35560ms step_avg:41.84ms
step:851/1880 train_time:35623ms step_avg:41.86ms
step:852/1880 train_time:35684ms step_avg:41.88ms
step:853/1880 train_time:35747ms step_avg:41.91ms
step:854/1880 train_time:35808ms step_avg:41.93ms
step:855/1880 train_time:35870ms step_avg:41.95ms
step:856/1880 train_time:35931ms step_avg:41.98ms
step:857/1880 train_time:35992ms step_avg:42.00ms
step:858/1880 train_time:36053ms step_avg:42.02ms
step:859/1880 train_time:36115ms step_avg:42.04ms
step:860/1880 train_time:36176ms step_avg:42.06ms
step:861/1880 train_time:36237ms step_avg:42.09ms
step:862/1880 train_time:36299ms step_avg:42.11ms
step:863/1880 train_time:36361ms step_avg:42.13ms
step:864/1880 train_time:36422ms step_avg:42.16ms
step:865/1880 train_time:36484ms step_avg:42.18ms
step:866/1880 train_time:36545ms step_avg:42.20ms
step:867/1880 train_time:36607ms step_avg:42.22ms
step:868/1880 train_time:36668ms step_avg:42.24ms
step:869/1880 train_time:36731ms step_avg:42.27ms
step:870/1880 train_time:36792ms step_avg:42.29ms
step:871/1880 train_time:36854ms step_avg:42.31ms
step:872/1880 train_time:36915ms step_avg:42.33ms
step:873/1880 train_time:36977ms step_avg:42.36ms
step:874/1880 train_time:37039ms step_avg:42.38ms
step:875/1880 train_time:37101ms step_avg:42.40ms
step:876/1880 train_time:37162ms step_avg:42.42ms
step:877/1880 train_time:37224ms step_avg:42.44ms
step:878/1880 train_time:37285ms step_avg:42.47ms
step:879/1880 train_time:37347ms step_avg:42.49ms
step:880/1880 train_time:37409ms step_avg:42.51ms
step:881/1880 train_time:37470ms step_avg:42.53ms
step:882/1880 train_time:37532ms step_avg:42.55ms
step:883/1880 train_time:37594ms step_avg:42.58ms
step:884/1880 train_time:37656ms step_avg:42.60ms
step:885/1880 train_time:37717ms step_avg:42.62ms
step:886/1880 train_time:37779ms step_avg:42.64ms
step:887/1880 train_time:37840ms step_avg:42.66ms
step:888/1880 train_time:37901ms step_avg:42.68ms
step:889/1880 train_time:37963ms step_avg:42.70ms
step:890/1880 train_time:38024ms step_avg:42.72ms
step:891/1880 train_time:38086ms step_avg:42.75ms
step:892/1880 train_time:38147ms step_avg:42.77ms
step:893/1880 train_time:38210ms step_avg:42.79ms
step:894/1880 train_time:38271ms step_avg:42.81ms
step:895/1880 train_time:38333ms step_avg:42.83ms
step:896/1880 train_time:38394ms step_avg:42.85ms
step:897/1880 train_time:38456ms step_avg:42.87ms
step:898/1880 train_time:38517ms step_avg:42.89ms
step:899/1880 train_time:38580ms step_avg:42.91ms
step:900/1880 train_time:38642ms step_avg:42.94ms
step:901/1880 train_time:38703ms step_avg:42.96ms
step:902/1880 train_time:38764ms step_avg:42.98ms
step:903/1880 train_time:38826ms step_avg:43.00ms
step:904/1880 train_time:38888ms step_avg:43.02ms
step:905/1880 train_time:38949ms step_avg:43.04ms
step:906/1880 train_time:39010ms step_avg:43.06ms
step:907/1880 train_time:39072ms step_avg:43.08ms
step:908/1880 train_time:39134ms step_avg:43.10ms
step:909/1880 train_time:39196ms step_avg:43.12ms
step:910/1880 train_time:39257ms step_avg:43.14ms
step:911/1880 train_time:39319ms step_avg:43.16ms
step:912/1880 train_time:39380ms step_avg:43.18ms
step:913/1880 train_time:39442ms step_avg:43.20ms
step:914/1880 train_time:39503ms step_avg:43.22ms
step:915/1880 train_time:39565ms step_avg:43.24ms
step:916/1880 train_time:39626ms step_avg:43.26ms
step:917/1880 train_time:39688ms step_avg:43.28ms
step:918/1880 train_time:39749ms step_avg:43.30ms
step:919/1880 train_time:39811ms step_avg:43.32ms
step:920/1880 train_time:39872ms step_avg:43.34ms
step:921/1880 train_time:39934ms step_avg:43.36ms
step:922/1880 train_time:39995ms step_avg:43.38ms
step:923/1880 train_time:40057ms step_avg:43.40ms
step:924/1880 train_time:40119ms step_avg:43.42ms
step:925/1880 train_time:40181ms step_avg:43.44ms
step:926/1880 train_time:40242ms step_avg:43.46ms
step:927/1880 train_time:40304ms step_avg:43.48ms
step:928/1880 train_time:40365ms step_avg:43.50ms
step:929/1880 train_time:40427ms step_avg:43.52ms
step:930/1880 train_time:40487ms step_avg:43.53ms
step:931/1880 train_time:40549ms step_avg:43.55ms
step:932/1880 train_time:40610ms step_avg:43.57ms
step:933/1880 train_time:40672ms step_avg:43.59ms
step:934/1880 train_time:40733ms step_avg:43.61ms
step:935/1880 train_time:40795ms step_avg:43.63ms
step:936/1880 train_time:40857ms step_avg:43.65ms
step:937/1880 train_time:40919ms step_avg:43.67ms
step:938/1880 train_time:40981ms step_avg:43.69ms
step:939/1880 train_time:41042ms step_avg:43.71ms
step:940/1880 train_time:41104ms step_avg:43.73ms
step:941/1880 train_time:41165ms step_avg:43.75ms
step:942/1880 train_time:41227ms step_avg:43.77ms
step:943/1880 train_time:41288ms step_avg:43.78ms
step:944/1880 train_time:41350ms step_avg:43.80ms
step:945/1880 train_time:41412ms step_avg:43.82ms
step:946/1880 train_time:41473ms step_avg:43.84ms
step:947/1880 train_time:41535ms step_avg:43.86ms
step:948/1880 train_time:41597ms step_avg:43.88ms
step:949/1880 train_time:41659ms step_avg:43.90ms
step:950/1880 train_time:41720ms step_avg:43.92ms
step:951/1880 train_time:41782ms step_avg:43.93ms
step:952/1880 train_time:41843ms step_avg:43.95ms
step:953/1880 train_time:41905ms step_avg:43.97ms
step:954/1880 train_time:41966ms step_avg:43.99ms
step:955/1880 train_time:42029ms step_avg:44.01ms
step:956/1880 train_time:42090ms step_avg:44.03ms
step:957/1880 train_time:42152ms step_avg:44.05ms
step:958/1880 train_time:42213ms step_avg:44.06ms
step:959/1880 train_time:42275ms step_avg:44.08ms
step:960/1880 train_time:42337ms step_avg:44.10ms
step:961/1880 train_time:42399ms step_avg:44.12ms
step:962/1880 train_time:42460ms step_avg:44.14ms
step:963/1880 train_time:42523ms step_avg:44.16ms
step:964/1880 train_time:42584ms step_avg:44.17ms
step:965/1880 train_time:42646ms step_avg:44.19ms
step:966/1880 train_time:42707ms step_avg:44.21ms
step:967/1880 train_time:42769ms step_avg:44.23ms
step:968/1880 train_time:42830ms step_avg:44.25ms
step:969/1880 train_time:42892ms step_avg:44.26ms
step:970/1880 train_time:42953ms step_avg:44.28ms
step:971/1880 train_time:43015ms step_avg:44.30ms
step:972/1880 train_time:43077ms step_avg:44.32ms
step:973/1880 train_time:43139ms step_avg:44.34ms
step:974/1880 train_time:43200ms step_avg:44.35ms
step:975/1880 train_time:43262ms step_avg:44.37ms
step:976/1880 train_time:43323ms step_avg:44.39ms
step:977/1880 train_time:43385ms step_avg:44.41ms
step:978/1880 train_time:43446ms step_avg:44.42ms
step:979/1880 train_time:43508ms step_avg:44.44ms
step:980/1880 train_time:43569ms step_avg:44.46ms
step:981/1880 train_time:43631ms step_avg:44.48ms
step:982/1880 train_time:43692ms step_avg:44.49ms
step:983/1880 train_time:43754ms step_avg:44.51ms
step:984/1880 train_time:43815ms step_avg:44.53ms
step:985/1880 train_time:43877ms step_avg:44.54ms
step:986/1880 train_time:43938ms step_avg:44.56ms
step:987/1880 train_time:44000ms step_avg:44.58ms
step:988/1880 train_time:44061ms step_avg:44.60ms
step:989/1880 train_time:44124ms step_avg:44.61ms
step:990/1880 train_time:44184ms step_avg:44.63ms
step:991/1880 train_time:44246ms step_avg:44.65ms
step:992/1880 train_time:44308ms step_avg:44.66ms
step:993/1880 train_time:44369ms step_avg:44.68ms
step:994/1880 train_time:44430ms step_avg:44.70ms
step:995/1880 train_time:44492ms step_avg:44.72ms
step:996/1880 train_time:44553ms step_avg:44.73ms
step:997/1880 train_time:44616ms step_avg:44.75ms
step:998/1880 train_time:44677ms step_avg:44.77ms
step:999/1880 train_time:44739ms step_avg:44.78ms
step:1000/1880 train_time:44801ms step_avg:44.80ms
step:1000/1880 val_loss:3.7690 train_time:44865ms step_avg:44.87ms
step:1001/1880 train_time:44886ms step_avg:44.84ms
step:1002/1880 train_time:44927ms step_avg:44.84ms
step:1003/1880 train_time:44990ms step_avg:44.86ms
step:1004/1880 train_time:45053ms step_avg:44.87ms
step:1005/1880 train_time:45115ms step_avg:44.89ms
step:1006/1880 train_time:45176ms step_avg:44.91ms
step:1007/1880 train_time:45238ms step_avg:44.92ms
step:1008/1880 train_time:45298ms step_avg:44.94ms
step:1009/1880 train_time:45359ms step_avg:44.95ms
step:1010/1880 train_time:45420ms step_avg:44.97ms
step:1011/1880 train_time:45481ms step_avg:44.99ms
step:1012/1880 train_time:45543ms step_avg:45.00ms
step:1013/1880 train_time:45604ms step_avg:45.02ms
step:1014/1880 train_time:45664ms step_avg:45.03ms
step:1015/1880 train_time:45726ms step_avg:45.05ms
step:1016/1880 train_time:45787ms step_avg:45.07ms
step:1017/1880 train_time:45851ms step_avg:45.09ms
step:1018/1880 train_time:45913ms step_avg:45.10ms
step:1019/1880 train_time:45976ms step_avg:45.12ms
step:1020/1880 train_time:46039ms step_avg:45.14ms
step:1021/1880 train_time:46101ms step_avg:45.15ms
step:1022/1880 train_time:46162ms step_avg:45.17ms
step:1023/1880 train_time:46224ms step_avg:45.18ms
step:1024/1880 train_time:46284ms step_avg:45.20ms
step:1025/1880 train_time:46346ms step_avg:45.22ms
step:1026/1880 train_time:46406ms step_avg:45.23ms
step:1027/1880 train_time:46468ms step_avg:45.25ms
step:1028/1880 train_time:46528ms step_avg:45.26ms
step:1029/1880 train_time:46590ms step_avg:45.28ms
step:1030/1880 train_time:46651ms step_avg:45.29ms
step:1031/1880 train_time:46712ms step_avg:45.31ms
step:1032/1880 train_time:46774ms step_avg:45.32ms
step:1033/1880 train_time:46836ms step_avg:45.34ms
step:1034/1880 train_time:46898ms step_avg:45.36ms
step:1035/1880 train_time:46961ms step_avg:45.37ms
step:1036/1880 train_time:47022ms step_avg:45.39ms
step:1037/1880 train_time:47084ms step_avg:45.40ms
step:1038/1880 train_time:47145ms step_avg:45.42ms
step:1039/1880 train_time:47207ms step_avg:45.43ms
step:1040/1880 train_time:47268ms step_avg:45.45ms
step:1041/1880 train_time:47329ms step_avg:45.47ms
step:1042/1880 train_time:47390ms step_avg:45.48ms
step:1043/1880 train_time:47452ms step_avg:45.50ms
step:1044/1880 train_time:47514ms step_avg:45.51ms
step:1045/1880 train_time:47575ms step_avg:45.53ms
step:1046/1880 train_time:47636ms step_avg:45.54ms
step:1047/1880 train_time:47698ms step_avg:45.56ms
step:1048/1880 train_time:47759ms step_avg:45.57ms
step:1049/1880 train_time:47820ms step_avg:45.59ms
step:1050/1880 train_time:47881ms step_avg:45.60ms
step:1051/1880 train_time:47944ms step_avg:45.62ms
step:1052/1880 train_time:48005ms step_avg:45.63ms
step:1053/1880 train_time:48067ms step_avg:45.65ms
step:1054/1880 train_time:48128ms step_avg:45.66ms
step:1055/1880 train_time:48190ms step_avg:45.68ms
step:1056/1880 train_time:48251ms step_avg:45.69ms
step:1057/1880 train_time:48313ms step_avg:45.71ms
step:1058/1880 train_time:48375ms step_avg:45.72ms
step:1059/1880 train_time:48437ms step_avg:45.74ms
step:1060/1880 train_time:48498ms step_avg:45.75ms
step:1061/1880 train_time:48560ms step_avg:45.77ms
step:1062/1880 train_time:48620ms step_avg:45.78ms
step:1063/1880 train_time:48682ms step_avg:45.80ms
step:1064/1880 train_time:48743ms step_avg:45.81ms
step:1065/1880 train_time:48804ms step_avg:45.83ms
step:1066/1880 train_time:48866ms step_avg:45.84ms
step:1067/1880 train_time:48927ms step_avg:45.85ms
step:1068/1880 train_time:48988ms step_avg:45.87ms
step:1069/1880 train_time:49051ms step_avg:45.88ms
step:1070/1880 train_time:49112ms step_avg:45.90ms
step:1071/1880 train_time:49174ms step_avg:45.91ms
step:1072/1880 train_time:49235ms step_avg:45.93ms
step:1073/1880 train_time:49297ms step_avg:45.94ms
step:1074/1880 train_time:49358ms step_avg:45.96ms
step:1075/1880 train_time:49420ms step_avg:45.97ms
step:1076/1880 train_time:49481ms step_avg:45.99ms
step:1077/1880 train_time:49542ms step_avg:46.00ms
step:1078/1880 train_time:49603ms step_avg:46.01ms
step:1079/1880 train_time:49665ms step_avg:46.03ms
step:1080/1880 train_time:49726ms step_avg:46.04ms
step:1081/1880 train_time:49787ms step_avg:46.06ms
step:1082/1880 train_time:49848ms step_avg:46.07ms
step:1083/1880 train_time:49910ms step_avg:46.09ms
step:1084/1880 train_time:49972ms step_avg:46.10ms
step:1085/1880 train_time:50034ms step_avg:46.11ms
step:1086/1880 train_time:50095ms step_avg:46.13ms
step:1087/1880 train_time:50158ms step_avg:46.14ms
step:1088/1880 train_time:50220ms step_avg:46.16ms
step:1089/1880 train_time:50281ms step_avg:46.17ms
step:1090/1880 train_time:50342ms step_avg:46.19ms
step:1091/1880 train_time:50403ms step_avg:46.20ms
step:1092/1880 train_time:50464ms step_avg:46.21ms
step:1093/1880 train_time:50527ms step_avg:46.23ms
step:1094/1880 train_time:50587ms step_avg:46.24ms
step:1095/1880 train_time:50649ms step_avg:46.25ms
step:1096/1880 train_time:50710ms step_avg:46.27ms
step:1097/1880 train_time:50772ms step_avg:46.28ms
step:1098/1880 train_time:50835ms step_avg:46.30ms
step:1099/1880 train_time:50897ms step_avg:46.31ms
step:1100/1880 train_time:50958ms step_avg:46.33ms
step:1101/1880 train_time:51020ms step_avg:46.34ms
step:1102/1880 train_time:51082ms step_avg:46.35ms
step:1103/1880 train_time:51144ms step_avg:46.37ms
step:1104/1880 train_time:51205ms step_avg:46.38ms
step:1105/1880 train_time:51267ms step_avg:46.40ms
step:1106/1880 train_time:51328ms step_avg:46.41ms
step:1107/1880 train_time:51390ms step_avg:46.42ms
step:1108/1880 train_time:51451ms step_avg:46.44ms
step:1109/1880 train_time:51513ms step_avg:46.45ms
step:1110/1880 train_time:51575ms step_avg:46.46ms
step:1111/1880 train_time:51637ms step_avg:46.48ms
step:1112/1880 train_time:51698ms step_avg:46.49ms
step:1113/1880 train_time:51760ms step_avg:46.50ms
step:1114/1880 train_time:51821ms step_avg:46.52ms
step:1115/1880 train_time:51883ms step_avg:46.53ms
step:1116/1880 train_time:51944ms step_avg:46.54ms
step:1117/1880 train_time:52006ms step_avg:46.56ms
step:1118/1880 train_time:52067ms step_avg:46.57ms
step:1119/1880 train_time:52129ms step_avg:46.59ms
step:1120/1880 train_time:52190ms step_avg:46.60ms
step:1121/1880 train_time:52252ms step_avg:46.61ms
step:1122/1880 train_time:52314ms step_avg:46.63ms
step:1123/1880 train_time:52375ms step_avg:46.64ms
step:1124/1880 train_time:52437ms step_avg:46.65ms
step:1125/1880 train_time:52498ms step_avg:46.67ms
step:1126/1880 train_time:52560ms step_avg:46.68ms
step:1127/1880 train_time:52622ms step_avg:46.69ms
step:1128/1880 train_time:52683ms step_avg:46.70ms
step:1129/1880 train_time:52744ms step_avg:46.72ms
step:1130/1880 train_time:52806ms step_avg:46.73ms
step:1131/1880 train_time:52868ms step_avg:46.74ms
step:1132/1880 train_time:52930ms step_avg:46.76ms
step:1133/1880 train_time:52992ms step_avg:46.77ms
step:1134/1880 train_time:53054ms step_avg:46.79ms
step:1135/1880 train_time:53116ms step_avg:46.80ms
step:1136/1880 train_time:53177ms step_avg:46.81ms
step:1137/1880 train_time:53239ms step_avg:46.82ms
step:1138/1880 train_time:53300ms step_avg:46.84ms
step:1139/1880 train_time:53362ms step_avg:46.85ms
step:1140/1880 train_time:53423ms step_avg:46.86ms
step:1141/1880 train_time:53485ms step_avg:46.88ms
step:1142/1880 train_time:53546ms step_avg:46.89ms
step:1143/1880 train_time:53607ms step_avg:46.90ms
step:1144/1880 train_time:53668ms step_avg:46.91ms
step:1145/1880 train_time:53730ms step_avg:46.93ms
step:1146/1880 train_time:53791ms step_avg:46.94ms
step:1147/1880 train_time:53853ms step_avg:46.95ms
step:1148/1880 train_time:53915ms step_avg:46.96ms
step:1149/1880 train_time:53978ms step_avg:46.98ms
step:1150/1880 train_time:54040ms step_avg:46.99ms
step:1151/1880 train_time:54101ms step_avg:47.00ms
step:1152/1880 train_time:54163ms step_avg:47.02ms
step:1153/1880 train_time:54224ms step_avg:47.03ms
step:1154/1880 train_time:54285ms step_avg:47.04ms
step:1155/1880 train_time:54346ms step_avg:47.05ms
step:1156/1880 train_time:54407ms step_avg:47.07ms
step:1157/1880 train_time:54469ms step_avg:47.08ms
step:1158/1880 train_time:54530ms step_avg:47.09ms
step:1159/1880 train_time:54592ms step_avg:47.10ms
step:1160/1880 train_time:54654ms step_avg:47.12ms
step:1161/1880 train_time:54717ms step_avg:47.13ms
step:1162/1880 train_time:54778ms step_avg:47.14ms
step:1163/1880 train_time:54840ms step_avg:47.15ms
step:1164/1880 train_time:54901ms step_avg:47.17ms
step:1165/1880 train_time:54964ms step_avg:47.18ms
step:1166/1880 train_time:55025ms step_avg:47.19ms
step:1167/1880 train_time:55087ms step_avg:47.20ms
step:1168/1880 train_time:55149ms step_avg:47.22ms
step:1169/1880 train_time:55211ms step_avg:47.23ms
step:1170/1880 train_time:55272ms step_avg:47.24ms
step:1171/1880 train_time:55334ms step_avg:47.25ms
step:1172/1880 train_time:55396ms step_avg:47.27ms
step:1173/1880 train_time:55458ms step_avg:47.28ms
step:1174/1880 train_time:55518ms step_avg:47.29ms
step:1175/1880 train_time:55580ms step_avg:47.30ms
step:1176/1880 train_time:55641ms step_avg:47.31ms
step:1177/1880 train_time:55703ms step_avg:47.33ms
step:1178/1880 train_time:55764ms step_avg:47.34ms
step:1179/1880 train_time:55826ms step_avg:47.35ms
step:1180/1880 train_time:55887ms step_avg:47.36ms
step:1181/1880 train_time:55949ms step_avg:47.37ms
step:1182/1880 train_time:56010ms step_avg:47.39ms
step:1183/1880 train_time:56072ms step_avg:47.40ms
step:1184/1880 train_time:56133ms step_avg:47.41ms
step:1185/1880 train_time:56195ms step_avg:47.42ms
step:1186/1880 train_time:56257ms step_avg:47.43ms
step:1187/1880 train_time:56319ms step_avg:47.45ms
step:1188/1880 train_time:56380ms step_avg:47.46ms
step:1189/1880 train_time:56442ms step_avg:47.47ms
step:1190/1880 train_time:56503ms step_avg:47.48ms
step:1191/1880 train_time:56564ms step_avg:47.49ms
step:1192/1880 train_time:56625ms step_avg:47.50ms
step:1193/1880 train_time:56687ms step_avg:47.52ms
step:1194/1880 train_time:56748ms step_avg:47.53ms
step:1195/1880 train_time:56810ms step_avg:47.54ms
step:1196/1880 train_time:56871ms step_avg:47.55ms
step:1197/1880 train_time:56933ms step_avg:47.56ms
step:1198/1880 train_time:56995ms step_avg:47.57ms
step:1199/1880 train_time:57057ms step_avg:47.59ms
step:1200/1880 train_time:57118ms step_avg:47.60ms
step:1201/1880 train_time:57180ms step_avg:47.61ms
step:1202/1880 train_time:57241ms step_avg:47.62ms
step:1203/1880 train_time:57303ms step_avg:47.63ms
step:1204/1880 train_time:57364ms step_avg:47.64ms
step:1205/1880 train_time:57425ms step_avg:47.66ms
step:1206/1880 train_time:57486ms step_avg:47.67ms
step:1207/1880 train_time:57548ms step_avg:47.68ms
step:1208/1880 train_time:57609ms step_avg:47.69ms
step:1209/1880 train_time:57671ms step_avg:47.70ms
step:1210/1880 train_time:57733ms step_avg:47.71ms
step:1211/1880 train_time:57795ms step_avg:47.73ms
step:1212/1880 train_time:57857ms step_avg:47.74ms
step:1213/1880 train_time:57919ms step_avg:47.75ms
step:1214/1880 train_time:57980ms step_avg:47.76ms
step:1215/1880 train_time:58042ms step_avg:47.77ms
step:1216/1880 train_time:58103ms step_avg:47.78ms
step:1217/1880 train_time:58165ms step_avg:47.79ms
step:1218/1880 train_time:58225ms step_avg:47.80ms
step:1219/1880 train_time:58287ms step_avg:47.82ms
step:1220/1880 train_time:58349ms step_avg:47.83ms
step:1221/1880 train_time:58411ms step_avg:47.84ms
step:1222/1880 train_time:58472ms step_avg:47.85ms
step:1223/1880 train_time:58534ms step_avg:47.86ms
step:1224/1880 train_time:58595ms step_avg:47.87ms
step:1225/1880 train_time:58658ms step_avg:47.88ms
step:1226/1880 train_time:58719ms step_avg:47.89ms
step:1227/1880 train_time:58780ms step_avg:47.91ms
step:1228/1880 train_time:58842ms step_avg:47.92ms
step:1229/1880 train_time:58931ms step_avg:47.95ms
step:1230/1880 train_time:59019ms step_avg:47.98ms
step:1231/1880 train_time:59107ms step_avg:48.02ms
step:1232/1880 train_time:59195ms step_avg:48.05ms
step:1233/1880 train_time:59283ms step_avg:48.08ms
step:1234/1880 train_time:59371ms step_avg:48.11ms
step:1235/1880 train_time:59459ms step_avg:48.14ms
step:1236/1880 train_time:59546ms step_avg:48.18ms
step:1237/1880 train_time:59636ms step_avg:48.21ms
step:1238/1880 train_time:59724ms step_avg:48.24ms
step:1239/1880 train_time:59813ms step_avg:48.27ms
step:1240/1880 train_time:59900ms step_avg:48.31ms
step:1241/1880 train_time:59989ms step_avg:48.34ms
step:1242/1880 train_time:60076ms step_avg:48.37ms
step:1243/1880 train_time:60165ms step_avg:48.40ms
step:1244/1880 train_time:60253ms step_avg:48.44ms
step:1245/1880 train_time:60341ms step_avg:48.47ms
step:1246/1880 train_time:60430ms step_avg:48.50ms
step:1247/1880 train_time:60517ms step_avg:48.53ms
step:1248/1880 train_time:60606ms step_avg:48.56ms
step:1249/1880 train_time:60694ms step_avg:48.59ms
step:1250/1880 train_time:60782ms step_avg:48.63ms
step:1250/1880 val_loss:3.5341 train_time:60872ms step_avg:48.70ms
step:1251/1880 train_time:60893ms step_avg:48.68ms
step:1252/1880 train_time:60960ms step_avg:48.69ms
step:1253/1880 train_time:61051ms step_avg:48.72ms
step:1254/1880 train_time:61139ms step_avg:48.76ms
step:1255/1880 train_time:61227ms step_avg:48.79ms
step:1256/1880 train_time:61315ms step_avg:48.82ms
step:1257/1880 train_time:61402ms step_avg:48.85ms
step:1258/1880 train_time:61488ms step_avg:48.88ms
step:1259/1880 train_time:61575ms step_avg:48.91ms
step:1260/1880 train_time:61662ms step_avg:48.94ms
step:1261/1880 train_time:61750ms step_avg:48.97ms
step:1262/1880 train_time:61838ms step_avg:49.00ms
step:1263/1880 train_time:61930ms step_avg:49.03ms
step:1264/1880 train_time:62021ms step_avg:49.07ms
step:1265/1880 train_time:62110ms step_avg:49.10ms
step:1266/1880 train_time:62198ms step_avg:49.13ms
step:1267/1880 train_time:62286ms step_avg:49.16ms
step:1268/1880 train_time:62373ms step_avg:49.19ms
step:1269/1880 train_time:62460ms step_avg:49.22ms
step:1270/1880 train_time:62547ms step_avg:49.25ms
step:1271/1880 train_time:62634ms step_avg:49.28ms
step:1272/1880 train_time:62721ms step_avg:49.31ms
step:1273/1880 train_time:62809ms step_avg:49.34ms
step:1274/1880 train_time:62898ms step_avg:49.37ms
step:1275/1880 train_time:62988ms step_avg:49.40ms
step:1276/1880 train_time:63076ms step_avg:49.43ms
step:1277/1880 train_time:63165ms step_avg:49.46ms
step:1278/1880 train_time:63253ms step_avg:49.49ms
step:1279/1880 train_time:63341ms step_avg:49.52ms
step:1280/1880 train_time:63428ms step_avg:49.55ms
step:1281/1880 train_time:63515ms step_avg:49.58ms
step:1282/1880 train_time:63602ms step_avg:49.61ms
step:1283/1880 train_time:63689ms step_avg:49.64ms
step:1284/1880 train_time:63777ms step_avg:49.67ms
step:1285/1880 train_time:63866ms step_avg:49.70ms
step:1286/1880 train_time:63956ms step_avg:49.73ms
step:1287/1880 train_time:64045ms step_avg:49.76ms
step:1288/1880 train_time:64133ms step_avg:49.79ms
step:1289/1880 train_time:64222ms step_avg:49.82ms
step:1290/1880 train_time:64310ms step_avg:49.85ms
step:1291/1880 train_time:64397ms step_avg:49.88ms
step:1292/1880 train_time:64485ms step_avg:49.91ms
step:1293/1880 train_time:64572ms step_avg:49.94ms
step:1294/1880 train_time:64659ms step_avg:49.97ms
step:1295/1880 train_time:64747ms step_avg:50.00ms
step:1296/1880 train_time:64835ms step_avg:50.03ms
step:1297/1880 train_time:64924ms step_avg:50.06ms
step:1298/1880 train_time:65011ms step_avg:50.09ms
step:1299/1880 train_time:65100ms step_avg:50.12ms
step:1300/1880 train_time:65188ms step_avg:50.14ms
step:1301/1880 train_time:65276ms step_avg:50.17ms
step:1302/1880 train_time:65364ms step_avg:50.20ms
step:1303/1880 train_time:65452ms step_avg:50.23ms
step:1304/1880 train_time:65540ms step_avg:50.26ms
step:1305/1880 train_time:65628ms step_avg:50.29ms
step:1306/1880 train_time:65716ms step_avg:50.32ms
step:1307/1880 train_time:65804ms step_avg:50.35ms
step:1308/1880 train_time:65891ms step_avg:50.38ms
step:1309/1880 train_time:65980ms step_avg:50.40ms
step:1310/1880 train_time:66068ms step_avg:50.43ms
step:1311/1880 train_time:66156ms step_avg:50.46ms
step:1312/1880 train_time:66244ms step_avg:50.49ms
step:1313/1880 train_time:66332ms step_avg:50.52ms
step:1314/1880 train_time:66420ms step_avg:50.55ms
step:1315/1880 train_time:66507ms step_avg:50.58ms
step:1316/1880 train_time:66595ms step_avg:50.60ms
step:1317/1880 train_time:66683ms step_avg:50.63ms
step:1318/1880 train_time:66770ms step_avg:50.66ms
step:1319/1880 train_time:66858ms step_avg:50.69ms
step:1320/1880 train_time:66947ms step_avg:50.72ms
step:1321/1880 train_time:67035ms step_avg:50.75ms
step:1322/1880 train_time:67123ms step_avg:50.77ms
step:1323/1880 train_time:67211ms step_avg:50.80ms
step:1324/1880 train_time:67299ms step_avg:50.83ms
step:1325/1880 train_time:67387ms step_avg:50.86ms
step:1326/1880 train_time:67474ms step_avg:50.89ms
step:1327/1880 train_time:67563ms step_avg:50.91ms
step:1328/1880 train_time:67650ms step_avg:50.94ms
step:1329/1880 train_time:67738ms step_avg:50.97ms
step:1330/1880 train_time:67827ms step_avg:51.00ms
step:1331/1880 train_time:67915ms step_avg:51.03ms
step:1332/1880 train_time:68003ms step_avg:51.05ms
step:1333/1880 train_time:68091ms step_avg:51.08ms
step:1334/1880 train_time:68179ms step_avg:51.11ms
step:1335/1880 train_time:68267ms step_avg:51.14ms
step:1336/1880 train_time:68355ms step_avg:51.16ms
step:1337/1880 train_time:68443ms step_avg:51.19ms
step:1338/1880 train_time:68530ms step_avg:51.22ms
step:1339/1880 train_time:68620ms step_avg:51.25ms
step:1340/1880 train_time:68707ms step_avg:51.27ms
step:1341/1880 train_time:68795ms step_avg:51.30ms
step:1342/1880 train_time:68884ms step_avg:51.33ms
step:1343/1880 train_time:68972ms step_avg:51.36ms
step:1344/1880 train_time:69060ms step_avg:51.38ms
step:1345/1880 train_time:69148ms step_avg:51.41ms
step:1346/1880 train_time:69237ms step_avg:51.44ms
step:1347/1880 train_time:69326ms step_avg:51.47ms
step:1348/1880 train_time:69415ms step_avg:51.49ms
step:1349/1880 train_time:69503ms step_avg:51.52ms
step:1350/1880 train_time:69591ms step_avg:51.55ms
step:1351/1880 train_time:69678ms step_avg:51.58ms
step:1352/1880 train_time:69767ms step_avg:51.60ms
step:1353/1880 train_time:69855ms step_avg:51.63ms
step:1354/1880 train_time:69943ms step_avg:51.66ms
step:1355/1880 train_time:70031ms step_avg:51.68ms
step:1356/1880 train_time:70119ms step_avg:51.71ms
step:1357/1880 train_time:70207ms step_avg:51.74ms
step:1358/1880 train_time:70296ms step_avg:51.76ms
step:1359/1880 train_time:70386ms step_avg:51.79ms
step:1360/1880 train_time:70473ms step_avg:51.82ms
step:1361/1880 train_time:70561ms step_avg:51.85ms
step:1362/1880 train_time:70648ms step_avg:51.87ms
step:1363/1880 train_time:70737ms step_avg:51.90ms
step:1364/1880 train_time:70826ms step_avg:51.93ms
step:1365/1880 train_time:70913ms step_avg:51.95ms
step:1366/1880 train_time:71002ms step_avg:51.98ms
step:1367/1880 train_time:71089ms step_avg:52.00ms
step:1368/1880 train_time:71176ms step_avg:52.03ms
step:1369/1880 train_time:71266ms step_avg:52.06ms
step:1370/1880 train_time:71354ms step_avg:52.08ms
step:1371/1880 train_time:71442ms step_avg:52.11ms
step:1372/1880 train_time:71530ms step_avg:52.14ms
step:1373/1880 train_time:71617ms step_avg:52.16ms
step:1374/1880 train_time:71705ms step_avg:52.19ms
step:1375/1880 train_time:71794ms step_avg:52.21ms
step:1376/1880 train_time:71881ms step_avg:52.24ms
step:1377/1880 train_time:71969ms step_avg:52.26ms
step:1378/1880 train_time:72056ms step_avg:52.29ms
step:1379/1880 train_time:72145ms step_avg:52.32ms
step:1380/1880 train_time:72233ms step_avg:52.34ms
step:1381/1880 train_time:72321ms step_avg:52.37ms
step:1382/1880 train_time:72409ms step_avg:52.39ms
step:1383/1880 train_time:72497ms step_avg:52.42ms
step:1384/1880 train_time:72585ms step_avg:52.45ms
step:1385/1880 train_time:72673ms step_avg:52.47ms
step:1386/1880 train_time:72760ms step_avg:52.50ms
step:1387/1880 train_time:72847ms step_avg:52.52ms
step:1388/1880 train_time:72934ms step_avg:52.55ms
step:1389/1880 train_time:73022ms step_avg:52.57ms
step:1390/1880 train_time:73109ms step_avg:52.60ms
step:1391/1880 train_time:73197ms step_avg:52.62ms
step:1392/1880 train_time:73286ms step_avg:52.65ms
step:1393/1880 train_time:73374ms step_avg:52.67ms
step:1394/1880 train_time:73463ms step_avg:52.70ms
step:1395/1880 train_time:73551ms step_avg:52.72ms
step:1396/1880 train_time:73639ms step_avg:52.75ms
step:1397/1880 train_time:73728ms step_avg:52.78ms
step:1398/1880 train_time:73816ms step_avg:52.80ms
step:1399/1880 train_time:73903ms step_avg:52.83ms
step:1400/1880 train_time:73991ms step_avg:52.85ms
step:1401/1880 train_time:74079ms step_avg:52.88ms
step:1402/1880 train_time:74167ms step_avg:52.90ms
step:1403/1880 train_time:74255ms step_avg:52.93ms
step:1404/1880 train_time:74343ms step_avg:52.95ms
step:1405/1880 train_time:74432ms step_avg:52.98ms
step:1406/1880 train_time:74519ms step_avg:53.00ms
step:1407/1880 train_time:74608ms step_avg:53.03ms
step:1408/1880 train_time:74696ms step_avg:53.05ms
step:1409/1880 train_time:74784ms step_avg:53.08ms
step:1410/1880 train_time:74871ms step_avg:53.10ms
step:1411/1880 train_time:74960ms step_avg:53.13ms
step:1412/1880 train_time:75047ms step_avg:53.15ms
step:1413/1880 train_time:75135ms step_avg:53.17ms
step:1414/1880 train_time:75224ms step_avg:53.20ms
step:1415/1880 train_time:75312ms step_avg:53.22ms
step:1416/1880 train_time:75399ms step_avg:53.25ms
step:1417/1880 train_time:75488ms step_avg:53.27ms
step:1418/1880 train_time:75575ms step_avg:53.30ms
step:1419/1880 train_time:75664ms step_avg:53.32ms
step:1420/1880 train_time:75751ms step_avg:53.35ms
step:1421/1880 train_time:75838ms step_avg:53.37ms
step:1422/1880 train_time:75927ms step_avg:53.39ms
step:1423/1880 train_time:76015ms step_avg:53.42ms
step:1424/1880 train_time:76103ms step_avg:53.44ms
step:1425/1880 train_time:76191ms step_avg:53.47ms
step:1426/1880 train_time:76278ms step_avg:53.49ms
step:1427/1880 train_time:76366ms step_avg:53.52ms
step:1428/1880 train_time:76454ms step_avg:53.54ms
step:1429/1880 train_time:76542ms step_avg:53.56ms
step:1430/1880 train_time:76630ms step_avg:53.59ms
step:1431/1880 train_time:76718ms step_avg:53.61ms
step:1432/1880 train_time:76805ms step_avg:53.63ms
step:1433/1880 train_time:76892ms step_avg:53.66ms
step:1434/1880 train_time:76980ms step_avg:53.68ms
step:1435/1880 train_time:77069ms step_avg:53.71ms
step:1436/1880 train_time:77157ms step_avg:53.73ms
step:1437/1880 train_time:77246ms step_avg:53.75ms
step:1438/1880 train_time:77334ms step_avg:53.78ms
step:1439/1880 train_time:77422ms step_avg:53.80ms
step:1440/1880 train_time:77509ms step_avg:53.83ms
step:1441/1880 train_time:77597ms step_avg:53.85ms
step:1442/1880 train_time:77686ms step_avg:53.87ms
step:1443/1880 train_time:77773ms step_avg:53.90ms
step:1444/1880 train_time:77861ms step_avg:53.92ms
step:1445/1880 train_time:77950ms step_avg:53.94ms
step:1446/1880 train_time:78039ms step_avg:53.97ms
step:1447/1880 train_time:78127ms step_avg:53.99ms
step:1448/1880 train_time:78214ms step_avg:54.02ms
step:1449/1880 train_time:78303ms step_avg:54.04ms
step:1450/1880 train_time:78390ms step_avg:54.06ms
step:1451/1880 train_time:78478ms step_avg:54.09ms
step:1452/1880 train_time:78566ms step_avg:54.11ms
step:1453/1880 train_time:78654ms step_avg:54.13ms
step:1454/1880 train_time:78742ms step_avg:54.16ms
step:1455/1880 train_time:78830ms step_avg:54.18ms
step:1456/1880 train_time:78918ms step_avg:54.20ms
step:1457/1880 train_time:79007ms step_avg:54.23ms
step:1458/1880 train_time:79095ms step_avg:54.25ms
step:1459/1880 train_time:79183ms step_avg:54.27ms
step:1460/1880 train_time:79270ms step_avg:54.29ms
step:1461/1880 train_time:79359ms step_avg:54.32ms
step:1462/1880 train_time:79446ms step_avg:54.34ms
step:1463/1880 train_time:79534ms step_avg:54.36ms
step:1464/1880 train_time:79624ms step_avg:54.39ms
step:1465/1880 train_time:79711ms step_avg:54.41ms
step:1466/1880 train_time:79799ms step_avg:54.43ms
step:1467/1880 train_time:79888ms step_avg:54.46ms
step:1468/1880 train_time:79975ms step_avg:54.48ms
step:1469/1880 train_time:80064ms step_avg:54.50ms
step:1470/1880 train_time:80152ms step_avg:54.52ms
step:1471/1880 train_time:80240ms step_avg:54.55ms
step:1472/1880 train_time:80327ms step_avg:54.57ms
step:1473/1880 train_time:80415ms step_avg:54.59ms
step:1474/1880 train_time:80502ms step_avg:54.61ms
step:1475/1880 train_time:80590ms step_avg:54.64ms
step:1476/1880 train_time:80678ms step_avg:54.66ms
step:1477/1880 train_time:80767ms step_avg:54.68ms
step:1478/1880 train_time:80854ms step_avg:54.71ms
step:1479/1880 train_time:80943ms step_avg:54.73ms
step:1480/1880 train_time:81030ms step_avg:54.75ms
step:1481/1880 train_time:81119ms step_avg:54.77ms
step:1482/1880 train_time:81206ms step_avg:54.79ms
step:1483/1880 train_time:81294ms step_avg:54.82ms
step:1484/1880 train_time:81383ms step_avg:54.84ms
step:1485/1880 train_time:81470ms step_avg:54.86ms
step:1486/1880 train_time:81558ms step_avg:54.88ms
step:1487/1880 train_time:81647ms step_avg:54.91ms
step:1488/1880 train_time:81736ms step_avg:54.93ms
step:1489/1880 train_time:81824ms step_avg:54.95ms
step:1490/1880 train_time:81912ms step_avg:54.97ms
step:1491/1880 train_time:82000ms step_avg:55.00ms
step:1492/1880 train_time:82087ms step_avg:55.02ms
step:1493/1880 train_time:82176ms step_avg:55.04ms
step:1494/1880 train_time:82264ms step_avg:55.06ms
step:1495/1880 train_time:82352ms step_avg:55.08ms
step:1496/1880 train_time:82440ms step_avg:55.11ms
step:1497/1880 train_time:82528ms step_avg:55.13ms
step:1498/1880 train_time:82617ms step_avg:55.15ms
step:1499/1880 train_time:82706ms step_avg:55.17ms
step:1500/1880 train_time:82794ms step_avg:55.20ms
step:1500/1880 val_loss:3.4081 train_time:82884ms step_avg:55.26ms
step:1501/1880 train_time:82907ms step_avg:55.23ms
step:1502/1880 train_time:82974ms step_avg:55.24ms
step:1503/1880 train_time:83066ms step_avg:55.27ms
step:1504/1880 train_time:83154ms step_avg:55.29ms
step:1505/1880 train_time:83240ms step_avg:55.31ms
step:1506/1880 train_time:83327ms step_avg:55.33ms
step:1507/1880 train_time:83415ms step_avg:55.35ms
step:1508/1880 train_time:83503ms step_avg:55.37ms
step:1509/1880 train_time:83591ms step_avg:55.39ms
step:1510/1880 train_time:83678ms step_avg:55.42ms
step:1511/1880 train_time:83765ms step_avg:55.44ms
step:1512/1880 train_time:83854ms step_avg:55.46ms
step:1513/1880 train_time:83945ms step_avg:55.48ms
step:1514/1880 train_time:84035ms step_avg:55.51ms
step:1515/1880 train_time:84123ms step_avg:55.53ms
step:1516/1880 train_time:84211ms step_avg:55.55ms
step:1517/1880 train_time:84298ms step_avg:55.57ms
step:1518/1880 train_time:84385ms step_avg:55.59ms
step:1519/1880 train_time:84474ms step_avg:55.61ms
step:1520/1880 train_time:84561ms step_avg:55.63ms
step:1521/1880 train_time:84650ms step_avg:55.65ms
step:1522/1880 train_time:84737ms step_avg:55.68ms
step:1523/1880 train_time:84826ms step_avg:55.70ms
step:1524/1880 train_time:84915ms step_avg:55.72ms
step:1525/1880 train_time:85005ms step_avg:55.74ms
step:1526/1880 train_time:85093ms step_avg:55.76ms
step:1527/1880 train_time:85181ms step_avg:55.78ms
step:1528/1880 train_time:85269ms step_avg:55.80ms
step:1529/1880 train_time:85357ms step_avg:55.83ms
step:1530/1880 train_time:85446ms step_avg:55.85ms
step:1531/1880 train_time:85534ms step_avg:55.87ms
step:1532/1880 train_time:85621ms step_avg:55.89ms
step:1533/1880 train_time:85709ms step_avg:55.91ms
step:1534/1880 train_time:85796ms step_avg:55.93ms
step:1535/1880 train_time:85885ms step_avg:55.95ms
step:1536/1880 train_time:85974ms step_avg:55.97ms
step:1537/1880 train_time:86062ms step_avg:55.99ms
step:1538/1880 train_time:86150ms step_avg:56.01ms
step:1539/1880 train_time:86238ms step_avg:56.04ms
step:1540/1880 train_time:86326ms step_avg:56.06ms
step:1541/1880 train_time:86414ms step_avg:56.08ms
step:1542/1880 train_time:86501ms step_avg:56.10ms
step:1543/1880 train_time:86589ms step_avg:56.12ms
step:1544/1880 train_time:86676ms step_avg:56.14ms
step:1545/1880 train_time:86764ms step_avg:56.16ms
step:1546/1880 train_time:86853ms step_avg:56.18ms
step:1547/1880 train_time:86941ms step_avg:56.20ms
step:1548/1880 train_time:87029ms step_avg:56.22ms
step:1549/1880 train_time:87117ms step_avg:56.24ms
step:1550/1880 train_time:87205ms step_avg:56.26ms
step:1551/1880 train_time:87294ms step_avg:56.28ms
step:1552/1880 train_time:87382ms step_avg:56.30ms
step:1553/1880 train_time:87470ms step_avg:56.32ms
step:1554/1880 train_time:87558ms step_avg:56.34ms
step:1555/1880 train_time:87646ms step_avg:56.36ms
step:1556/1880 train_time:87733ms step_avg:56.38ms
step:1557/1880 train_time:87822ms step_avg:56.40ms
step:1558/1880 train_time:87911ms step_avg:56.43ms
step:1559/1880 train_time:88000ms step_avg:56.45ms
step:1560/1880 train_time:88089ms step_avg:56.47ms
step:1561/1880 train_time:88177ms step_avg:56.49ms
step:1562/1880 train_time:88265ms step_avg:56.51ms
step:1563/1880 train_time:88354ms step_avg:56.53ms
step:1564/1880 train_time:88442ms step_avg:56.55ms
step:1565/1880 train_time:88530ms step_avg:56.57ms
step:1566/1880 train_time:88618ms step_avg:56.59ms
step:1567/1880 train_time:88706ms step_avg:56.61ms
step:1568/1880 train_time:88795ms step_avg:56.63ms
step:1569/1880 train_time:88883ms step_avg:56.65ms
step:1570/1880 train_time:88971ms step_avg:56.67ms
step:1571/1880 train_time:89060ms step_avg:56.69ms
step:1572/1880 train_time:89148ms step_avg:56.71ms
step:1573/1880 train_time:89236ms step_avg:56.73ms
step:1574/1880 train_time:89325ms step_avg:56.75ms
step:1575/1880 train_time:89414ms step_avg:56.77ms
step:1576/1880 train_time:89501ms step_avg:56.79ms
step:1577/1880 train_time:89589ms step_avg:56.81ms
step:1578/1880 train_time:89676ms step_avg:56.83ms
step:1579/1880 train_time:89764ms step_avg:56.85ms
step:1580/1880 train_time:89853ms step_avg:56.87ms
step:1581/1880 train_time:89940ms step_avg:56.89ms
step:1582/1880 train_time:90028ms step_avg:56.91ms
step:1583/1880 train_time:90117ms step_avg:56.93ms
step:1584/1880 train_time:90204ms step_avg:56.95ms
step:1585/1880 train_time:90293ms step_avg:56.97ms
step:1586/1880 train_time:90381ms step_avg:56.99ms
step:1587/1880 train_time:90469ms step_avg:57.01ms
step:1588/1880 train_time:90557ms step_avg:57.03ms
step:1589/1880 train_time:90645ms step_avg:57.05ms
step:1590/1880 train_time:90734ms step_avg:57.07ms
step:1591/1880 train_time:90822ms step_avg:57.08ms
step:1592/1880 train_time:90910ms step_avg:57.10ms
step:1593/1880 train_time:90998ms step_avg:57.12ms
step:1594/1880 train_time:91086ms step_avg:57.14ms
step:1595/1880 train_time:91174ms step_avg:57.16ms
step:1596/1880 train_time:91262ms step_avg:57.18ms
step:1597/1880 train_time:91350ms step_avg:57.20ms
step:1598/1880 train_time:91437ms step_avg:57.22ms
step:1599/1880 train_time:91526ms step_avg:57.24ms
step:1600/1880 train_time:91614ms step_avg:57.26ms
step:1601/1880 train_time:91701ms step_avg:57.28ms
step:1602/1880 train_time:91790ms step_avg:57.30ms
step:1603/1880 train_time:91878ms step_avg:57.32ms
step:1604/1880 train_time:91966ms step_avg:57.34ms
step:1605/1880 train_time:92055ms step_avg:57.36ms
step:1606/1880 train_time:92143ms step_avg:57.37ms
step:1607/1880 train_time:92231ms step_avg:57.39ms
step:1608/1880 train_time:92318ms step_avg:57.41ms
step:1609/1880 train_time:92406ms step_avg:57.43ms
step:1610/1880 train_time:92494ms step_avg:57.45ms
step:1611/1880 train_time:92583ms step_avg:57.47ms
step:1612/1880 train_time:92671ms step_avg:57.49ms
step:1613/1880 train_time:92759ms step_avg:57.51ms
step:1614/1880 train_time:92849ms step_avg:57.53ms
step:1615/1880 train_time:92937ms step_avg:57.55ms
step:1616/1880 train_time:93025ms step_avg:57.56ms
step:1617/1880 train_time:93113ms step_avg:57.58ms
step:1618/1880 train_time:93200ms step_avg:57.60ms
step:1619/1880 train_time:93288ms step_avg:57.62ms
step:1620/1880 train_time:93376ms step_avg:57.64ms
step:1621/1880 train_time:93465ms step_avg:57.66ms
step:1622/1880 train_time:93553ms step_avg:57.68ms
step:1623/1880 train_time:93642ms step_avg:57.70ms
step:1624/1880 train_time:93730ms step_avg:57.72ms
step:1625/1880 train_time:93818ms step_avg:57.73ms
step:1626/1880 train_time:93906ms step_avg:57.75ms
step:1627/1880 train_time:93996ms step_avg:57.77ms
step:1628/1880 train_time:94084ms step_avg:57.79ms
step:1629/1880 train_time:94173ms step_avg:57.81ms
step:1630/1880 train_time:94260ms step_avg:57.83ms
step:1631/1880 train_time:94348ms step_avg:57.85ms
step:1632/1880 train_time:94436ms step_avg:57.86ms
step:1633/1880 train_time:94525ms step_avg:57.88ms
step:1634/1880 train_time:94613ms step_avg:57.90ms
step:1635/1880 train_time:94701ms step_avg:57.92ms
step:1636/1880 train_time:94789ms step_avg:57.94ms
step:1637/1880 train_time:94877ms step_avg:57.96ms
step:1638/1880 train_time:94964ms step_avg:57.98ms
step:1639/1880 train_time:95054ms step_avg:57.99ms
step:1640/1880 train_time:95142ms step_avg:58.01ms
step:1641/1880 train_time:95230ms step_avg:58.03ms
step:1642/1880 train_time:95317ms step_avg:58.05ms
step:1643/1880 train_time:95405ms step_avg:58.07ms
step:1644/1880 train_time:95493ms step_avg:58.09ms
step:1645/1880 train_time:95582ms step_avg:58.10ms
step:1646/1880 train_time:95671ms step_avg:58.12ms
step:1647/1880 train_time:95758ms step_avg:58.14ms
step:1648/1880 train_time:95846ms step_avg:58.16ms
step:1649/1880 train_time:95934ms step_avg:58.18ms
step:1650/1880 train_time:96022ms step_avg:58.20ms
step:1651/1880 train_time:96111ms step_avg:58.21ms
step:1652/1880 train_time:96199ms step_avg:58.23ms
step:1653/1880 train_time:96287ms step_avg:58.25ms
step:1654/1880 train_time:96375ms step_avg:58.27ms
step:1655/1880 train_time:96463ms step_avg:58.29ms
step:1656/1880 train_time:96551ms step_avg:58.30ms
step:1657/1880 train_time:96639ms step_avg:58.32ms
step:1658/1880 train_time:96728ms step_avg:58.34ms
step:1659/1880 train_time:96815ms step_avg:58.36ms
step:1660/1880 train_time:96904ms step_avg:58.38ms
step:1661/1880 train_time:96992ms step_avg:58.39ms
step:1662/1880 train_time:97080ms step_avg:58.41ms
step:1663/1880 train_time:97168ms step_avg:58.43ms
step:1664/1880 train_time:97256ms step_avg:58.45ms
step:1665/1880 train_time:97344ms step_avg:58.46ms
step:1666/1880 train_time:97432ms step_avg:58.48ms
step:1667/1880 train_time:97520ms step_avg:58.50ms
step:1668/1880 train_time:97609ms step_avg:58.52ms
step:1669/1880 train_time:97698ms step_avg:58.54ms
step:1670/1880 train_time:97785ms step_avg:58.55ms
step:1671/1880 train_time:97874ms step_avg:58.57ms
step:1672/1880 train_time:97962ms step_avg:58.59ms
step:1673/1880 train_time:98050ms step_avg:58.61ms
step:1674/1880 train_time:98138ms step_avg:58.62ms
step:1675/1880 train_time:98226ms step_avg:58.64ms
step:1676/1880 train_time:98315ms step_avg:58.66ms
step:1677/1880 train_time:98403ms step_avg:58.68ms
step:1678/1880 train_time:98491ms step_avg:58.70ms
step:1679/1880 train_time:98579ms step_avg:58.71ms
step:1680/1880 train_time:98668ms step_avg:58.73ms
step:1681/1880 train_time:98756ms step_avg:58.75ms
step:1682/1880 train_time:98844ms step_avg:58.77ms
step:1683/1880 train_time:98933ms step_avg:58.78ms
step:1684/1880 train_time:99021ms step_avg:58.80ms
step:1685/1880 train_time:99110ms step_avg:58.82ms
step:1686/1880 train_time:99197ms step_avg:58.84ms
step:1687/1880 train_time:99285ms step_avg:58.85ms
step:1688/1880 train_time:99373ms step_avg:58.87ms
step:1689/1880 train_time:99462ms step_avg:58.89ms
step:1690/1880 train_time:99550ms step_avg:58.91ms
step:1691/1880 train_time:99639ms step_avg:58.92ms
step:1692/1880 train_time:99727ms step_avg:58.94ms
step:1693/1880 train_time:99815ms step_avg:58.96ms
step:1694/1880 train_time:99903ms step_avg:58.97ms
step:1695/1880 train_time:99992ms step_avg:58.99ms
step:1696/1880 train_time:100080ms step_avg:59.01ms
step:1697/1880 train_time:100168ms step_avg:59.03ms
step:1698/1880 train_time:100256ms step_avg:59.04ms
step:1699/1880 train_time:100344ms step_avg:59.06ms
step:1700/1880 train_time:100432ms step_avg:59.08ms
step:1701/1880 train_time:100520ms step_avg:59.09ms
step:1702/1880 train_time:100607ms step_avg:59.11ms
step:1703/1880 train_time:100695ms step_avg:59.13ms
step:1704/1880 train_time:100782ms step_avg:59.14ms
step:1705/1880 train_time:100871ms step_avg:59.16ms
step:1706/1880 train_time:100959ms step_avg:59.18ms
step:1707/1880 train_time:101049ms step_avg:59.20ms
step:1708/1880 train_time:101137ms step_avg:59.21ms
step:1709/1880 train_time:101224ms step_avg:59.23ms
step:1710/1880 train_time:101313ms step_avg:59.25ms
step:1711/1880 train_time:101401ms step_avg:59.26ms
step:1712/1880 train_time:101490ms step_avg:59.28ms
step:1713/1880 train_time:101577ms step_avg:59.30ms
step:1714/1880 train_time:101665ms step_avg:59.31ms
step:1715/1880 train_time:101754ms step_avg:59.33ms
step:1716/1880 train_time:101842ms step_avg:59.35ms
step:1717/1880 train_time:101930ms step_avg:59.37ms
step:1718/1880 train_time:102018ms step_avg:59.38ms
step:1719/1880 train_time:102107ms step_avg:59.40ms
step:1720/1880 train_time:102194ms step_avg:59.42ms
step:1721/1880 train_time:102282ms step_avg:59.43ms
step:1722/1880 train_time:102370ms step_avg:59.45ms
step:1723/1880 train_time:102458ms step_avg:59.47ms
step:1724/1880 train_time:102546ms step_avg:59.48ms
step:1725/1880 train_time:102634ms step_avg:59.50ms
step:1726/1880 train_time:102722ms step_avg:59.51ms
step:1727/1880 train_time:102810ms step_avg:59.53ms
step:1728/1880 train_time:102897ms step_avg:59.55ms
step:1729/1880 train_time:102987ms step_avg:59.56ms
step:1730/1880 train_time:103074ms step_avg:59.58ms
step:1731/1880 train_time:103162ms step_avg:59.60ms
step:1732/1880 train_time:103250ms step_avg:59.61ms
step:1733/1880 train_time:103338ms step_avg:59.63ms
step:1734/1880 train_time:103426ms step_avg:59.65ms
step:1735/1880 train_time:103515ms step_avg:59.66ms
step:1736/1880 train_time:103602ms step_avg:59.68ms
step:1737/1880 train_time:103692ms step_avg:59.70ms
step:1738/1880 train_time:103779ms step_avg:59.71ms
step:1739/1880 train_time:103868ms step_avg:59.73ms
step:1740/1880 train_time:103956ms step_avg:59.75ms
step:1741/1880 train_time:104045ms step_avg:59.76ms
step:1742/1880 train_time:104133ms step_avg:59.78ms
step:1743/1880 train_time:104221ms step_avg:59.79ms
step:1744/1880 train_time:104309ms step_avg:59.81ms
step:1745/1880 train_time:104397ms step_avg:59.83ms
step:1746/1880 train_time:104485ms step_avg:59.84ms
step:1747/1880 train_time:104573ms step_avg:59.86ms
step:1748/1880 train_time:104662ms step_avg:59.88ms
step:1749/1880 train_time:104751ms step_avg:59.89ms
step:1750/1880 train_time:104839ms step_avg:59.91ms
step:1750/1880 val_loss:3.3140 train_time:104929ms step_avg:59.96ms
step:1751/1880 train_time:104949ms step_avg:59.94ms
step:1752/1880 train_time:105015ms step_avg:59.94ms
step:1753/1880 train_time:105108ms step_avg:59.96ms
step:1754/1880 train_time:105198ms step_avg:59.98ms
step:1755/1880 train_time:105286ms step_avg:59.99ms
step:1756/1880 train_time:105373ms step_avg:60.01ms
step:1757/1880 train_time:105461ms step_avg:60.02ms
step:1758/1880 train_time:105547ms step_avg:60.04ms
step:1759/1880 train_time:105634ms step_avg:60.05ms
step:1760/1880 train_time:105721ms step_avg:60.07ms
step:1761/1880 train_time:105810ms step_avg:60.08ms
step:1762/1880 train_time:105898ms step_avg:60.10ms
step:1763/1880 train_time:105989ms step_avg:60.12ms
step:1764/1880 train_time:106078ms step_avg:60.14ms
step:1765/1880 train_time:106169ms step_avg:60.15ms
step:1766/1880 train_time:106257ms step_avg:60.17ms
step:1767/1880 train_time:106345ms step_avg:60.18ms
step:1768/1880 train_time:106432ms step_avg:60.20ms
step:1769/1880 train_time:106520ms step_avg:60.21ms
step:1770/1880 train_time:106606ms step_avg:60.23ms
step:1771/1880 train_time:106694ms step_avg:60.24ms
step:1772/1880 train_time:106782ms step_avg:60.26ms
step:1773/1880 train_time:106871ms step_avg:60.28ms
step:1774/1880 train_time:106960ms step_avg:60.29ms
step:1775/1880 train_time:107049ms step_avg:60.31ms
step:1776/1880 train_time:107138ms step_avg:60.33ms
step:1777/1880 train_time:107227ms step_avg:60.34ms
step:1778/1880 train_time:107316ms step_avg:60.36ms
step:1779/1880 train_time:107404ms step_avg:60.37ms
step:1780/1880 train_time:107491ms step_avg:60.39ms
step:1781/1880 train_time:107578ms step_avg:60.40ms
step:1782/1880 train_time:107665ms step_avg:60.42ms
step:1783/1880 train_time:107754ms step_avg:60.43ms
step:1784/1880 train_time:107841ms step_avg:60.45ms
step:1785/1880 train_time:107930ms step_avg:60.46ms
step:1786/1880 train_time:108018ms step_avg:60.48ms
step:1787/1880 train_time:108107ms step_avg:60.50ms
step:1788/1880 train_time:108196ms step_avg:60.51ms
step:1789/1880 train_time:108286ms step_avg:60.53ms
step:1790/1880 train_time:108373ms step_avg:60.54ms
step:1791/1880 train_time:108460ms step_avg:60.56ms
step:1792/1880 train_time:108547ms step_avg:60.57ms
step:1793/1880 train_time:108635ms step_avg:60.59ms
step:1794/1880 train_time:108723ms step_avg:60.60ms
step:1795/1880 train_time:108811ms step_avg:60.62ms
step:1796/1880 train_time:108900ms step_avg:60.63ms
step:1797/1880 train_time:108989ms step_avg:60.65ms
step:1798/1880 train_time:109077ms step_avg:60.67ms
step:1799/1880 train_time:109166ms step_avg:60.68ms
step:1800/1880 train_time:109255ms step_avg:60.70ms
step:1801/1880 train_time:109343ms step_avg:60.71ms
step:1802/1880 train_time:109430ms step_avg:60.73ms
step:1803/1880 train_time:109518ms step_avg:60.74ms
step:1804/1880 train_time:109605ms step_avg:60.76ms
step:1805/1880 train_time:109693ms step_avg:60.77ms
step:1806/1880 train_time:109781ms step_avg:60.79ms
step:1807/1880 train_time:109868ms step_avg:60.80ms
step:1808/1880 train_time:109956ms step_avg:60.82ms
step:1809/1880 train_time:110046ms step_avg:60.83ms
step:1810/1880 train_time:110134ms step_avg:60.85ms
step:1811/1880 train_time:110223ms step_avg:60.86ms
step:1812/1880 train_time:110310ms step_avg:60.88ms
step:1813/1880 train_time:110398ms step_avg:60.89ms
step:1814/1880 train_time:110486ms step_avg:60.91ms
step:1815/1880 train_time:110573ms step_avg:60.92ms
step:1816/1880 train_time:110661ms step_avg:60.94ms
step:1817/1880 train_time:110749ms step_avg:60.95ms
step:1818/1880 train_time:110836ms step_avg:60.97ms
step:1819/1880 train_time:110926ms step_avg:60.98ms
step:1820/1880 train_time:111014ms step_avg:61.00ms
step:1821/1880 train_time:111103ms step_avg:61.01ms
step:1822/1880 train_time:111191ms step_avg:61.03ms
step:1823/1880 train_time:111279ms step_avg:61.04ms
step:1824/1880 train_time:111366ms step_avg:61.06ms
step:1825/1880 train_time:111454ms step_avg:61.07ms
step:1826/1880 train_time:111542ms step_avg:61.09ms
step:1827/1880 train_time:111630ms step_avg:61.10ms
step:1828/1880 train_time:111718ms step_avg:61.11ms
step:1829/1880 train_time:111806ms step_avg:61.13ms
step:1830/1880 train_time:111894ms step_avg:61.14ms
step:1831/1880 train_time:111982ms step_avg:61.16ms
step:1832/1880 train_time:112069ms step_avg:61.17ms
step:1833/1880 train_time:112159ms step_avg:61.19ms
step:1834/1880 train_time:112246ms step_avg:61.20ms
step:1835/1880 train_time:112335ms step_avg:61.22ms
step:1836/1880 train_time:112424ms step_avg:61.23ms
step:1837/1880 train_time:112511ms step_avg:61.25ms
step:1838/1880 train_time:112598ms step_avg:61.26ms
step:1839/1880 train_time:112686ms step_avg:61.28ms
step:1840/1880 train_time:112774ms step_avg:61.29ms
step:1841/1880 train_time:112862ms step_avg:61.30ms
step:1842/1880 train_time:112950ms step_avg:61.32ms
step:1843/1880 train_time:113039ms step_avg:61.33ms
step:1844/1880 train_time:113127ms step_avg:61.35ms
step:1845/1880 train_time:113216ms step_avg:61.36ms
step:1846/1880 train_time:113304ms step_avg:61.38ms
step:1847/1880 train_time:113392ms step_avg:61.39ms
step:1848/1880 train_time:113481ms step_avg:61.41ms
step:1849/1880 train_time:113569ms step_avg:61.42ms
step:1850/1880 train_time:113657ms step_avg:61.44ms
step:1851/1880 train_time:113745ms step_avg:61.45ms
step:1852/1880 train_time:113833ms step_avg:61.46ms
step:1853/1880 train_time:113921ms step_avg:61.48ms
step:1854/1880 train_time:114009ms step_avg:61.49ms
step:1855/1880 train_time:114098ms step_avg:61.51ms
step:1856/1880 train_time:114186ms step_avg:61.52ms
step:1857/1880 train_time:114275ms step_avg:61.54ms
step:1858/1880 train_time:114364ms step_avg:61.55ms
step:1859/1880 train_time:114453ms step_avg:61.57ms
step:1860/1880 train_time:114542ms step_avg:61.58ms
step:1861/1880 train_time:114629ms step_avg:61.60ms
step:1862/1880 train_time:114718ms step_avg:61.61ms
step:1863/1880 train_time:114808ms step_avg:61.63ms
step:1864/1880 train_time:114896ms step_avg:61.64ms
step:1865/1880 train_time:114986ms step_avg:61.65ms
step:1866/1880 train_time:115076ms step_avg:61.67ms
step:1867/1880 train_time:115164ms step_avg:61.68ms
step:1868/1880 train_time:115252ms step_avg:61.70ms
step:1869/1880 train_time:115342ms step_avg:61.71ms
step:1870/1880 train_time:115430ms step_avg:61.73ms
step:1871/1880 train_time:115518ms step_avg:61.74ms
step:1872/1880 train_time:115606ms step_avg:61.76ms
step:1873/1880 train_time:115693ms step_avg:61.77ms
step:1874/1880 train_time:115782ms step_avg:61.78ms
step:1875/1880 train_time:115870ms step_avg:61.80ms
step:1876/1880 train_time:115958ms step_avg:61.81ms
step:1877/1880 train_time:116047ms step_avg:61.83ms
step:1878/1880 train_time:116135ms step_avg:61.84ms
step:1879/1880 train_time:116225ms step_avg:61.85ms
step:1880/1880 train_time:116314ms step_avg:61.87ms
step:1880/1880 val_loss:3.2794 train_time:116405ms step_avg:61.92ms
peak memory allocated: 29709 MiB reserved: 44558 MiB

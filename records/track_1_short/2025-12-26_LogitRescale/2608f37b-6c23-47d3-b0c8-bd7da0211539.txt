import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits+5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1840  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec 26 23:26:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            130W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     76232      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     76233      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     76234      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     76235      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     76236      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     76237      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     76238      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     76239      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 613, 614, 615, 1226, 1227, 1228, 1839, 1840, 1841] for warmup
Resetting Model
step:0/1880 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/1880 train_time:83ms step_avg:83.08ms
step:2/1880 train_time:108ms step_avg:54.13ms
step:3/1880 train_time:130ms step_avg:43.48ms
step:4/1880 train_time:158ms step_avg:39.51ms
step:5/1880 train_time:189ms step_avg:37.81ms
step:6/1880 train_time:271ms step_avg:45.25ms
step:7/1880 train_time:295ms step_avg:42.09ms
step:8/1880 train_time:317ms step_avg:39.64ms
step:9/1880 train_time:351ms step_avg:38.98ms
step:10/1880 train_time:385ms step_avg:38.47ms
step:11/1880 train_time:419ms step_avg:38.05ms
step:12/1880 train_time:453ms step_avg:37.73ms
step:13/1880 train_time:487ms step_avg:37.43ms
step:14/1880 train_time:521ms step_avg:37.19ms
step:15/1880 train_time:554ms step_avg:36.96ms
step:16/1880 train_time:588ms step_avg:36.78ms
step:17/1880 train_time:622ms step_avg:36.62ms
step:18/1880 train_time:657ms step_avg:36.47ms
step:19/1880 train_time:690ms step_avg:36.33ms
step:20/1880 train_time:725ms step_avg:36.23ms
step:21/1880 train_time:758ms step_avg:36.12ms
step:22/1880 train_time:793ms step_avg:36.03ms
step:23/1880 train_time:826ms step_avg:35.93ms
step:24/1880 train_time:861ms step_avg:35.87ms
step:25/1880 train_time:894ms step_avg:35.78ms
step:26/1880 train_time:928ms step_avg:35.71ms
step:27/1880 train_time:962ms step_avg:35.65ms
step:28/1880 train_time:997ms step_avg:35.60ms
step:29/1880 train_time:1030ms step_avg:35.53ms
step:30/1880 train_time:1064ms step_avg:35.48ms
step:31/1880 train_time:1098ms step_avg:35.43ms
step:32/1880 train_time:1133ms step_avg:35.39ms
step:33/1880 train_time:1167ms step_avg:35.37ms
step:34/1880 train_time:1202ms step_avg:35.36ms
step:35/1880 train_time:1237ms step_avg:35.33ms
step:36/1880 train_time:1271ms step_avg:35.31ms
step:37/1880 train_time:1305ms step_avg:35.27ms
step:38/1880 train_time:1340ms step_avg:35.26ms
step:39/1880 train_time:1374ms step_avg:35.22ms
step:40/1880 train_time:1408ms step_avg:35.20ms
step:41/1880 train_time:1442ms step_avg:35.18ms
step:42/1880 train_time:1477ms step_avg:35.16ms
step:43/1880 train_time:1511ms step_avg:35.13ms
step:44/1880 train_time:1545ms step_avg:35.11ms
step:45/1880 train_time:1579ms step_avg:35.08ms
step:46/1880 train_time:1613ms step_avg:35.06ms
step:47/1880 train_time:1647ms step_avg:35.04ms
step:48/1880 train_time:1681ms step_avg:35.03ms
step:49/1880 train_time:1715ms step_avg:35.00ms
step:50/1880 train_time:1749ms step_avg:34.98ms
step:51/1880 train_time:1783ms step_avg:34.96ms
step:52/1880 train_time:1817ms step_avg:34.94ms
step:53/1880 train_time:1851ms step_avg:34.92ms
step:54/1880 train_time:1885ms step_avg:34.90ms
step:55/1880 train_time:1919ms step_avg:34.88ms
step:56/1880 train_time:1953ms step_avg:34.87ms
step:57/1880 train_time:1987ms step_avg:34.85ms
step:58/1880 train_time:2021ms step_avg:34.85ms
step:59/1880 train_time:2055ms step_avg:34.83ms
step:60/1880 train_time:2089ms step_avg:34.82ms
step:61/1880 train_time:2124ms step_avg:34.82ms
step:62/1880 train_time:2158ms step_avg:34.81ms
step:63/1880 train_time:2192ms step_avg:34.80ms
step:64/1880 train_time:2227ms step_avg:34.79ms
step:65/1880 train_time:2261ms step_avg:34.78ms
step:66/1880 train_time:2295ms step_avg:34.77ms
step:67/1880 train_time:2329ms step_avg:34.76ms
step:68/1880 train_time:2364ms step_avg:34.76ms
step:69/1880 train_time:2398ms step_avg:34.75ms
step:70/1880 train_time:2432ms step_avg:34.74ms
step:71/1880 train_time:2466ms step_avg:34.74ms
step:72/1880 train_time:2501ms step_avg:34.73ms
step:73/1880 train_time:2535ms step_avg:34.72ms
step:74/1880 train_time:2569ms step_avg:34.71ms
step:75/1880 train_time:2603ms step_avg:34.71ms
step:76/1880 train_time:2637ms step_avg:34.70ms
step:77/1880 train_time:2671ms step_avg:34.68ms
step:78/1880 train_time:2705ms step_avg:34.68ms
step:79/1880 train_time:2739ms step_avg:34.67ms
step:80/1880 train_time:2773ms step_avg:34.66ms
step:81/1880 train_time:2807ms step_avg:34.65ms
step:82/1880 train_time:2841ms step_avg:34.65ms
step:83/1880 train_time:2875ms step_avg:34.63ms
step:84/1880 train_time:2909ms step_avg:34.63ms
step:85/1880 train_time:2943ms step_avg:34.62ms
step:86/1880 train_time:2977ms step_avg:34.61ms
step:87/1880 train_time:3010ms step_avg:34.60ms
step:88/1880 train_time:3045ms step_avg:34.60ms
step:89/1880 train_time:3079ms step_avg:34.60ms
step:90/1880 train_time:3113ms step_avg:34.59ms
step:91/1880 train_time:3147ms step_avg:34.59ms
step:92/1880 train_time:3182ms step_avg:34.58ms
step:93/1880 train_time:3216ms step_avg:34.58ms
step:94/1880 train_time:3250ms step_avg:34.57ms
step:95/1880 train_time:3284ms step_avg:34.56ms
step:96/1880 train_time:3318ms step_avg:34.56ms
step:97/1880 train_time:3351ms step_avg:34.55ms
step:98/1880 train_time:3386ms step_avg:34.55ms
step:99/1880 train_time:3420ms step_avg:34.54ms
step:100/1880 train_time:3454ms step_avg:34.54ms
step:101/1880 train_time:3488ms step_avg:34.54ms
step:102/1880 train_time:3523ms step_avg:34.54ms
step:103/1880 train_time:3557ms step_avg:34.53ms
step:104/1880 train_time:3591ms step_avg:34.53ms
step:105/1880 train_time:3625ms step_avg:34.53ms
step:106/1880 train_time:3659ms step_avg:34.52ms
step:107/1880 train_time:3693ms step_avg:34.52ms
step:108/1880 train_time:3728ms step_avg:34.51ms
step:109/1880 train_time:3761ms step_avg:34.51ms
step:110/1880 train_time:3795ms step_avg:34.50ms
step:111/1880 train_time:3829ms step_avg:34.50ms
step:112/1880 train_time:3863ms step_avg:34.49ms
step:113/1880 train_time:3897ms step_avg:34.48ms
step:114/1880 train_time:3931ms step_avg:34.48ms
step:115/1880 train_time:3965ms step_avg:34.48ms
step:116/1880 train_time:3999ms step_avg:34.48ms
step:117/1880 train_time:4033ms step_avg:34.47ms
step:118/1880 train_time:4068ms step_avg:34.47ms
step:119/1880 train_time:4101ms step_avg:34.47ms
step:120/1880 train_time:4136ms step_avg:34.46ms
step:121/1880 train_time:4169ms step_avg:34.46ms
step:122/1880 train_time:4204ms step_avg:34.46ms
step:123/1880 train_time:4238ms step_avg:34.45ms
step:124/1880 train_time:4272ms step_avg:34.45ms
step:125/1880 train_time:4306ms step_avg:34.45ms
step:126/1880 train_time:4340ms step_avg:34.45ms
step:127/1880 train_time:4374ms step_avg:34.44ms
step:128/1880 train_time:4408ms step_avg:34.44ms
step:129/1880 train_time:4442ms step_avg:34.44ms
step:130/1880 train_time:4476ms step_avg:34.43ms
step:131/1880 train_time:4510ms step_avg:34.43ms
step:132/1880 train_time:4544ms step_avg:34.43ms
step:133/1880 train_time:4578ms step_avg:34.42ms
step:134/1880 train_time:4613ms step_avg:34.42ms
step:135/1880 train_time:4647ms step_avg:34.42ms
step:136/1880 train_time:4682ms step_avg:34.43ms
step:137/1880 train_time:4716ms step_avg:34.42ms
step:138/1880 train_time:4750ms step_avg:34.42ms
step:139/1880 train_time:4784ms step_avg:34.42ms
step:140/1880 train_time:4818ms step_avg:34.42ms
step:141/1880 train_time:4852ms step_avg:34.41ms
step:142/1880 train_time:4886ms step_avg:34.41ms
step:143/1880 train_time:4920ms step_avg:34.41ms
step:144/1880 train_time:4955ms step_avg:34.41ms
step:145/1880 train_time:4988ms step_avg:34.40ms
step:146/1880 train_time:5023ms step_avg:34.40ms
step:147/1880 train_time:5056ms step_avg:34.40ms
step:148/1880 train_time:5090ms step_avg:34.40ms
step:149/1880 train_time:5124ms step_avg:34.39ms
step:150/1880 train_time:5158ms step_avg:34.39ms
step:151/1880 train_time:5192ms step_avg:34.39ms
step:152/1880 train_time:5226ms step_avg:34.38ms
step:153/1880 train_time:5260ms step_avg:34.38ms
step:154/1880 train_time:5294ms step_avg:34.38ms
step:155/1880 train_time:5328ms step_avg:34.37ms
step:156/1880 train_time:5362ms step_avg:34.37ms
step:157/1880 train_time:5396ms step_avg:34.37ms
step:158/1880 train_time:5430ms step_avg:34.37ms
step:159/1880 train_time:5464ms step_avg:34.37ms
step:160/1880 train_time:5499ms step_avg:34.37ms
step:161/1880 train_time:5532ms step_avg:34.36ms
step:162/1880 train_time:5567ms step_avg:34.36ms
step:163/1880 train_time:5601ms step_avg:34.36ms
step:164/1880 train_time:5635ms step_avg:34.36ms
step:165/1880 train_time:5669ms step_avg:34.36ms
step:166/1880 train_time:5703ms step_avg:34.35ms
step:167/1880 train_time:5737ms step_avg:34.35ms
step:168/1880 train_time:5771ms step_avg:34.35ms
step:169/1880 train_time:5805ms step_avg:34.35ms
step:170/1880 train_time:5839ms step_avg:34.35ms
step:171/1880 train_time:5873ms step_avg:34.35ms
step:172/1880 train_time:5907ms step_avg:34.34ms
step:173/1880 train_time:5941ms step_avg:34.34ms
step:174/1880 train_time:5975ms step_avg:34.34ms
step:175/1880 train_time:6008ms step_avg:34.33ms
step:176/1880 train_time:6043ms step_avg:34.33ms
step:177/1880 train_time:6077ms step_avg:34.33ms
step:178/1880 train_time:6111ms step_avg:34.33ms
step:179/1880 train_time:6145ms step_avg:34.33ms
step:180/1880 train_time:6179ms step_avg:34.33ms
step:181/1880 train_time:6212ms step_avg:34.32ms
step:182/1880 train_time:6247ms step_avg:34.32ms
step:183/1880 train_time:6281ms step_avg:34.32ms
step:184/1880 train_time:6315ms step_avg:34.32ms
step:185/1880 train_time:6349ms step_avg:34.32ms
step:186/1880 train_time:6383ms step_avg:34.32ms
step:187/1880 train_time:6417ms step_avg:34.31ms
step:188/1880 train_time:6451ms step_avg:34.31ms
step:189/1880 train_time:6485ms step_avg:34.31ms
step:190/1880 train_time:6519ms step_avg:34.31ms
step:191/1880 train_time:6553ms step_avg:34.31ms
step:192/1880 train_time:6587ms step_avg:34.31ms
step:193/1880 train_time:6621ms step_avg:34.30ms
step:194/1880 train_time:6655ms step_avg:34.30ms
step:195/1880 train_time:6689ms step_avg:34.30ms
step:196/1880 train_time:6723ms step_avg:34.30ms
step:197/1880 train_time:6757ms step_avg:34.30ms
step:198/1880 train_time:6791ms step_avg:34.30ms
step:199/1880 train_time:6825ms step_avg:34.30ms
step:200/1880 train_time:6859ms step_avg:34.30ms
step:201/1880 train_time:6893ms step_avg:34.29ms
step:202/1880 train_time:6928ms step_avg:34.29ms
step:203/1880 train_time:6961ms step_avg:34.29ms
step:204/1880 train_time:6996ms step_avg:34.29ms
step:205/1880 train_time:7029ms step_avg:34.29ms
step:206/1880 train_time:7064ms step_avg:34.29ms
step:207/1880 train_time:7098ms step_avg:34.29ms
step:208/1880 train_time:7132ms step_avg:34.29ms
step:209/1880 train_time:7166ms step_avg:34.29ms
step:210/1880 train_time:7200ms step_avg:34.28ms
step:211/1880 train_time:7233ms step_avg:34.28ms
step:212/1880 train_time:7268ms step_avg:34.28ms
step:213/1880 train_time:7302ms step_avg:34.28ms
step:214/1880 train_time:7335ms step_avg:34.28ms
step:215/1880 train_time:7369ms step_avg:34.28ms
step:216/1880 train_time:7403ms step_avg:34.27ms
step:217/1880 train_time:7437ms step_avg:34.27ms
step:218/1880 train_time:7471ms step_avg:34.27ms
step:219/1880 train_time:7505ms step_avg:34.27ms
step:220/1880 train_time:7540ms step_avg:34.27ms
step:221/1880 train_time:7574ms step_avg:34.27ms
step:222/1880 train_time:7608ms step_avg:34.27ms
step:223/1880 train_time:7642ms step_avg:34.27ms
step:224/1880 train_time:7676ms step_avg:34.27ms
step:225/1880 train_time:7710ms step_avg:34.26ms
step:226/1880 train_time:7744ms step_avg:34.26ms
step:227/1880 train_time:7777ms step_avg:34.26ms
step:228/1880 train_time:7812ms step_avg:34.26ms
step:229/1880 train_time:7845ms step_avg:34.26ms
step:230/1880 train_time:7880ms step_avg:34.26ms
step:231/1880 train_time:7914ms step_avg:34.26ms
step:232/1880 train_time:7948ms step_avg:34.26ms
step:233/1880 train_time:7982ms step_avg:34.26ms
step:234/1880 train_time:8016ms step_avg:34.26ms
step:235/1880 train_time:8050ms step_avg:34.25ms
step:236/1880 train_time:8084ms step_avg:34.25ms
step:237/1880 train_time:8118ms step_avg:34.25ms
step:238/1880 train_time:8152ms step_avg:34.25ms
step:239/1880 train_time:8186ms step_avg:34.25ms
step:240/1880 train_time:8220ms step_avg:34.25ms
step:241/1880 train_time:8254ms step_avg:34.25ms
step:242/1880 train_time:8288ms step_avg:34.25ms
step:243/1880 train_time:8322ms step_avg:34.25ms
step:244/1880 train_time:8356ms step_avg:34.25ms
step:245/1880 train_time:8390ms step_avg:34.24ms
step:246/1880 train_time:8424ms step_avg:34.24ms
step:247/1880 train_time:8458ms step_avg:34.24ms
step:248/1880 train_time:8493ms step_avg:34.24ms
step:249/1880 train_time:8526ms step_avg:34.24ms
step:250/1880 train_time:8561ms step_avg:34.24ms
step:250/1880 val_loss:4.6338 train_time:8597ms step_avg:34.39ms
step:251/1880 train_time:8621ms step_avg:34.34ms
step:252/1880 train_time:8642ms step_avg:34.29ms
step:253/1880 train_time:8668ms step_avg:34.26ms
step:254/1880 train_time:8703ms step_avg:34.26ms
step:255/1880 train_time:8737ms step_avg:34.26ms
step:256/1880 train_time:8772ms step_avg:34.27ms
step:257/1880 train_time:8806ms step_avg:34.26ms
step:258/1880 train_time:8840ms step_avg:34.26ms
step:259/1880 train_time:8874ms step_avg:34.26ms
step:260/1880 train_time:8908ms step_avg:34.26ms
step:261/1880 train_time:8942ms step_avg:34.26ms
step:262/1880 train_time:8976ms step_avg:34.26ms
step:263/1880 train_time:9010ms step_avg:34.26ms
step:264/1880 train_time:9044ms step_avg:34.26ms
step:265/1880 train_time:9078ms step_avg:34.26ms
step:266/1880 train_time:9112ms step_avg:34.26ms
step:267/1880 train_time:9146ms step_avg:34.25ms
step:268/1880 train_time:9180ms step_avg:34.25ms
step:269/1880 train_time:9213ms step_avg:34.25ms
step:270/1880 train_time:9247ms step_avg:34.25ms
step:271/1880 train_time:9281ms step_avg:34.25ms
step:272/1880 train_time:9315ms step_avg:34.25ms
step:273/1880 train_time:9349ms step_avg:34.24ms
step:274/1880 train_time:9383ms step_avg:34.24ms
step:275/1880 train_time:9417ms step_avg:34.24ms
step:276/1880 train_time:9451ms step_avg:34.24ms
step:277/1880 train_time:9484ms step_avg:34.24ms
step:278/1880 train_time:9518ms step_avg:34.24ms
step:279/1880 train_time:9552ms step_avg:34.24ms
step:280/1880 train_time:9586ms step_avg:34.24ms
step:281/1880 train_time:9621ms step_avg:34.24ms
step:282/1880 train_time:9655ms step_avg:34.24ms
step:283/1880 train_time:9690ms step_avg:34.24ms
step:284/1880 train_time:9724ms step_avg:34.24ms
step:285/1880 train_time:9758ms step_avg:34.24ms
step:286/1880 train_time:9792ms step_avg:34.24ms
step:287/1880 train_time:9826ms step_avg:34.24ms
step:288/1880 train_time:9861ms step_avg:34.24ms
step:289/1880 train_time:9894ms step_avg:34.24ms
step:290/1880 train_time:9928ms step_avg:34.24ms
step:291/1880 train_time:9962ms step_avg:34.23ms
step:292/1880 train_time:9996ms step_avg:34.23ms
step:293/1880 train_time:10030ms step_avg:34.23ms
step:294/1880 train_time:10064ms step_avg:34.23ms
step:295/1880 train_time:10098ms step_avg:34.23ms
step:296/1880 train_time:10132ms step_avg:34.23ms
step:297/1880 train_time:10166ms step_avg:34.23ms
step:298/1880 train_time:10200ms step_avg:34.23ms
step:299/1880 train_time:10234ms step_avg:34.23ms
step:300/1880 train_time:10268ms step_avg:34.23ms
step:301/1880 train_time:10301ms step_avg:34.22ms
step:302/1880 train_time:10336ms step_avg:34.22ms
step:303/1880 train_time:10370ms step_avg:34.22ms
step:304/1880 train_time:10404ms step_avg:34.22ms
step:305/1880 train_time:10437ms step_avg:34.22ms
step:306/1880 train_time:10471ms step_avg:34.22ms
step:307/1880 train_time:10505ms step_avg:34.22ms
step:308/1880 train_time:10539ms step_avg:34.22ms
step:309/1880 train_time:10573ms step_avg:34.22ms
step:310/1880 train_time:10607ms step_avg:34.22ms
step:311/1880 train_time:10641ms step_avg:34.22ms
step:312/1880 train_time:10675ms step_avg:34.21ms
step:313/1880 train_time:10709ms step_avg:34.21ms
step:314/1880 train_time:10743ms step_avg:34.21ms
step:315/1880 train_time:10777ms step_avg:34.21ms
step:316/1880 train_time:10812ms step_avg:34.21ms
step:317/1880 train_time:10845ms step_avg:34.21ms
step:318/1880 train_time:10880ms step_avg:34.21ms
step:319/1880 train_time:10913ms step_avg:34.21ms
step:320/1880 train_time:10947ms step_avg:34.21ms
step:321/1880 train_time:10981ms step_avg:34.21ms
step:322/1880 train_time:11015ms step_avg:34.21ms
step:323/1880 train_time:11048ms step_avg:34.21ms
step:324/1880 train_time:11083ms step_avg:34.21ms
step:325/1880 train_time:11117ms step_avg:34.20ms
step:326/1880 train_time:11151ms step_avg:34.20ms
step:327/1880 train_time:11184ms step_avg:34.20ms
step:328/1880 train_time:11219ms step_avg:34.20ms
step:329/1880 train_time:11252ms step_avg:34.20ms
step:330/1880 train_time:11286ms step_avg:34.20ms
step:331/1880 train_time:11320ms step_avg:34.20ms
step:332/1880 train_time:11354ms step_avg:34.20ms
step:333/1880 train_time:11388ms step_avg:34.20ms
step:334/1880 train_time:11422ms step_avg:34.20ms
step:335/1880 train_time:11455ms step_avg:34.20ms
step:336/1880 train_time:11489ms step_avg:34.19ms
step:337/1880 train_time:11523ms step_avg:34.19ms
step:338/1880 train_time:11557ms step_avg:34.19ms
step:339/1880 train_time:11590ms step_avg:34.19ms
step:340/1880 train_time:11624ms step_avg:34.19ms
step:341/1880 train_time:11658ms step_avg:34.19ms
step:342/1880 train_time:11692ms step_avg:34.19ms
step:343/1880 train_time:11726ms step_avg:34.19ms
step:344/1880 train_time:11760ms step_avg:34.19ms
step:345/1880 train_time:11795ms step_avg:34.19ms
step:346/1880 train_time:11829ms step_avg:34.19ms
step:347/1880 train_time:11862ms step_avg:34.19ms
step:348/1880 train_time:11897ms step_avg:34.19ms
step:349/1880 train_time:11931ms step_avg:34.18ms
step:350/1880 train_time:11965ms step_avg:34.18ms
step:351/1880 train_time:11998ms step_avg:34.18ms
step:352/1880 train_time:12032ms step_avg:34.18ms
step:353/1880 train_time:12066ms step_avg:34.18ms
step:354/1880 train_time:12100ms step_avg:34.18ms
step:355/1880 train_time:12134ms step_avg:34.18ms
step:356/1880 train_time:12168ms step_avg:34.18ms
step:357/1880 train_time:12202ms step_avg:34.18ms
step:358/1880 train_time:12237ms step_avg:34.18ms
step:359/1880 train_time:12270ms step_avg:34.18ms
step:360/1880 train_time:12304ms step_avg:34.18ms
step:361/1880 train_time:12338ms step_avg:34.18ms
step:362/1880 train_time:12373ms step_avg:34.18ms
step:363/1880 train_time:12406ms step_avg:34.18ms
step:364/1880 train_time:12440ms step_avg:34.18ms
step:365/1880 train_time:12474ms step_avg:34.18ms
step:366/1880 train_time:12508ms step_avg:34.18ms
step:367/1880 train_time:12542ms step_avg:34.17ms
step:368/1880 train_time:12576ms step_avg:34.17ms
step:369/1880 train_time:12610ms step_avg:34.17ms
step:370/1880 train_time:12644ms step_avg:34.17ms
step:371/1880 train_time:12678ms step_avg:34.17ms
step:372/1880 train_time:12712ms step_avg:34.17ms
step:373/1880 train_time:12746ms step_avg:34.17ms
step:374/1880 train_time:12780ms step_avg:34.17ms
step:375/1880 train_time:12814ms step_avg:34.17ms
step:376/1880 train_time:12848ms step_avg:34.17ms
step:377/1880 train_time:12882ms step_avg:34.17ms
step:378/1880 train_time:12916ms step_avg:34.17ms
step:379/1880 train_time:12950ms step_avg:34.17ms
step:380/1880 train_time:12984ms step_avg:34.17ms
step:381/1880 train_time:13018ms step_avg:34.17ms
step:382/1880 train_time:13052ms step_avg:34.17ms
step:383/1880 train_time:13086ms step_avg:34.17ms
step:384/1880 train_time:13120ms step_avg:34.17ms
step:385/1880 train_time:13154ms step_avg:34.17ms
step:386/1880 train_time:13188ms step_avg:34.17ms
step:387/1880 train_time:13222ms step_avg:34.17ms
step:388/1880 train_time:13256ms step_avg:34.17ms
step:389/1880 train_time:13290ms step_avg:34.16ms
step:390/1880 train_time:13324ms step_avg:34.16ms
step:391/1880 train_time:13358ms step_avg:34.16ms
step:392/1880 train_time:13392ms step_avg:34.16ms
step:393/1880 train_time:13426ms step_avg:34.16ms
step:394/1880 train_time:13460ms step_avg:34.16ms
step:395/1880 train_time:13494ms step_avg:34.16ms
step:396/1880 train_time:13528ms step_avg:34.16ms
step:397/1880 train_time:13561ms step_avg:34.16ms
step:398/1880 train_time:13596ms step_avg:34.16ms
step:399/1880 train_time:13629ms step_avg:34.16ms
step:400/1880 train_time:13663ms step_avg:34.16ms
step:401/1880 train_time:13697ms step_avg:34.16ms
step:402/1880 train_time:13731ms step_avg:34.16ms
step:403/1880 train_time:13765ms step_avg:34.16ms
step:404/1880 train_time:13799ms step_avg:34.16ms
step:405/1880 train_time:13833ms step_avg:34.16ms
step:406/1880 train_time:13868ms step_avg:34.16ms
step:407/1880 train_time:13901ms step_avg:34.16ms
step:408/1880 train_time:13935ms step_avg:34.15ms
step:409/1880 train_time:13969ms step_avg:34.15ms
step:410/1880 train_time:14003ms step_avg:34.15ms
step:411/1880 train_time:14037ms step_avg:34.15ms
step:412/1880 train_time:14072ms step_avg:34.15ms
step:413/1880 train_time:14106ms step_avg:34.15ms
step:414/1880 train_time:14140ms step_avg:34.15ms
step:415/1880 train_time:14174ms step_avg:34.15ms
step:416/1880 train_time:14208ms step_avg:34.15ms
step:417/1880 train_time:14242ms step_avg:34.15ms
step:418/1880 train_time:14276ms step_avg:34.15ms
step:419/1880 train_time:14310ms step_avg:34.15ms
step:420/1880 train_time:14344ms step_avg:34.15ms
step:421/1880 train_time:14378ms step_avg:34.15ms
step:422/1880 train_time:14412ms step_avg:34.15ms
step:423/1880 train_time:14446ms step_avg:34.15ms
step:424/1880 train_time:14480ms step_avg:34.15ms
step:425/1880 train_time:14513ms step_avg:34.15ms
step:426/1880 train_time:14547ms step_avg:34.15ms
step:427/1880 train_time:14581ms step_avg:34.15ms
step:428/1880 train_time:14615ms step_avg:34.15ms
step:429/1880 train_time:14648ms step_avg:34.15ms
step:430/1880 train_time:14683ms step_avg:34.15ms
step:431/1880 train_time:14716ms step_avg:34.14ms
step:432/1880 train_time:14750ms step_avg:34.14ms
step:433/1880 train_time:14784ms step_avg:34.14ms
step:434/1880 train_time:14818ms step_avg:34.14ms
step:435/1880 train_time:14852ms step_avg:34.14ms
step:436/1880 train_time:14886ms step_avg:34.14ms
step:437/1880 train_time:14920ms step_avg:34.14ms
step:438/1880 train_time:14954ms step_avg:34.14ms
step:439/1880 train_time:14988ms step_avg:34.14ms
step:440/1880 train_time:15022ms step_avg:34.14ms
step:441/1880 train_time:15056ms step_avg:34.14ms
step:442/1880 train_time:15090ms step_avg:34.14ms
step:443/1880 train_time:15124ms step_avg:34.14ms
step:444/1880 train_time:15158ms step_avg:34.14ms
step:445/1880 train_time:15192ms step_avg:34.14ms
step:446/1880 train_time:15227ms step_avg:34.14ms
step:447/1880 train_time:15261ms step_avg:34.14ms
step:448/1880 train_time:15295ms step_avg:34.14ms
step:449/1880 train_time:15329ms step_avg:34.14ms
step:450/1880 train_time:15363ms step_avg:34.14ms
step:451/1880 train_time:15397ms step_avg:34.14ms
step:452/1880 train_time:15431ms step_avg:34.14ms
step:453/1880 train_time:15465ms step_avg:34.14ms
step:454/1880 train_time:15499ms step_avg:34.14ms
step:455/1880 train_time:15533ms step_avg:34.14ms
step:456/1880 train_time:15567ms step_avg:34.14ms
step:457/1880 train_time:15601ms step_avg:34.14ms
step:458/1880 train_time:15635ms step_avg:34.14ms
step:459/1880 train_time:15669ms step_avg:34.14ms
step:460/1880 train_time:15703ms step_avg:34.14ms
step:461/1880 train_time:15737ms step_avg:34.14ms
step:462/1880 train_time:15771ms step_avg:34.14ms
step:463/1880 train_time:15804ms step_avg:34.13ms
step:464/1880 train_time:15839ms step_avg:34.14ms
step:465/1880 train_time:15873ms step_avg:34.13ms
step:466/1880 train_time:15907ms step_avg:34.13ms
step:467/1880 train_time:15941ms step_avg:34.13ms
step:468/1880 train_time:15975ms step_avg:34.13ms
step:469/1880 train_time:16009ms step_avg:34.13ms
step:470/1880 train_time:16043ms step_avg:34.13ms
step:471/1880 train_time:16077ms step_avg:34.13ms
step:472/1880 train_time:16111ms step_avg:34.13ms
step:473/1880 train_time:16145ms step_avg:34.13ms
step:474/1880 train_time:16179ms step_avg:34.13ms
step:475/1880 train_time:16213ms step_avg:34.13ms
step:476/1880 train_time:16247ms step_avg:34.13ms
step:477/1880 train_time:16281ms step_avg:34.13ms
step:478/1880 train_time:16315ms step_avg:34.13ms
step:479/1880 train_time:16349ms step_avg:34.13ms
step:480/1880 train_time:16383ms step_avg:34.13ms
step:481/1880 train_time:16417ms step_avg:34.13ms
step:482/1880 train_time:16451ms step_avg:34.13ms
step:483/1880 train_time:16485ms step_avg:34.13ms
step:484/1880 train_time:16519ms step_avg:34.13ms
step:485/1880 train_time:16553ms step_avg:34.13ms
step:486/1880 train_time:16587ms step_avg:34.13ms
step:487/1880 train_time:16621ms step_avg:34.13ms
step:488/1880 train_time:16655ms step_avg:34.13ms
step:489/1880 train_time:16689ms step_avg:34.13ms
step:490/1880 train_time:16723ms step_avg:34.13ms
step:491/1880 train_time:16756ms step_avg:34.13ms
step:492/1880 train_time:16791ms step_avg:34.13ms
step:493/1880 train_time:16824ms step_avg:34.13ms
step:494/1880 train_time:16859ms step_avg:34.13ms
step:495/1880 train_time:16893ms step_avg:34.13ms
step:496/1880 train_time:16927ms step_avg:34.13ms
step:497/1880 train_time:16961ms step_avg:34.13ms
step:498/1880 train_time:16995ms step_avg:34.13ms
step:499/1880 train_time:17029ms step_avg:34.13ms
step:500/1880 train_time:17063ms step_avg:34.13ms
step:500/1880 val_loss:4.3007 train_time:17100ms step_avg:34.20ms
step:501/1880 train_time:17122ms step_avg:34.17ms
step:502/1880 train_time:17143ms step_avg:34.15ms
step:503/1880 train_time:17169ms step_avg:34.13ms
step:504/1880 train_time:17201ms step_avg:34.13ms
step:505/1880 train_time:17237ms step_avg:34.13ms
step:506/1880 train_time:17272ms step_avg:34.14ms
step:507/1880 train_time:17307ms step_avg:34.14ms
step:508/1880 train_time:17342ms step_avg:34.14ms
step:509/1880 train_time:17375ms step_avg:34.14ms
step:510/1880 train_time:17410ms step_avg:34.14ms
step:511/1880 train_time:17443ms step_avg:34.14ms
step:512/1880 train_time:17477ms step_avg:34.14ms
step:513/1880 train_time:17511ms step_avg:34.13ms
step:514/1880 train_time:17545ms step_avg:34.13ms
step:515/1880 train_time:17579ms step_avg:34.13ms
step:516/1880 train_time:17612ms step_avg:34.13ms
step:517/1880 train_time:17646ms step_avg:34.13ms
step:518/1880 train_time:17680ms step_avg:34.13ms
step:519/1880 train_time:17713ms step_avg:34.13ms
step:520/1880 train_time:17747ms step_avg:34.13ms
step:521/1880 train_time:17781ms step_avg:34.13ms
step:522/1880 train_time:17815ms step_avg:34.13ms
step:523/1880 train_time:17849ms step_avg:34.13ms
step:524/1880 train_time:17883ms step_avg:34.13ms
step:525/1880 train_time:17916ms step_avg:34.13ms
step:526/1880 train_time:17950ms step_avg:34.13ms
step:527/1880 train_time:17984ms step_avg:34.13ms
step:528/1880 train_time:18018ms step_avg:34.13ms
step:529/1880 train_time:18052ms step_avg:34.13ms
step:530/1880 train_time:18087ms step_avg:34.13ms
step:531/1880 train_time:18121ms step_avg:34.13ms
step:532/1880 train_time:18156ms step_avg:34.13ms
step:533/1880 train_time:18190ms step_avg:34.13ms
step:534/1880 train_time:18224ms step_avg:34.13ms
step:535/1880 train_time:18258ms step_avg:34.13ms
step:536/1880 train_time:18293ms step_avg:34.13ms
step:537/1880 train_time:18327ms step_avg:34.13ms
step:538/1880 train_time:18361ms step_avg:34.13ms
step:539/1880 train_time:18395ms step_avg:34.13ms
step:540/1880 train_time:18429ms step_avg:34.13ms
step:541/1880 train_time:18462ms step_avg:34.13ms
step:542/1880 train_time:18497ms step_avg:34.13ms
step:543/1880 train_time:18531ms step_avg:34.13ms
step:544/1880 train_time:18565ms step_avg:34.13ms
step:545/1880 train_time:18599ms step_avg:34.13ms
step:546/1880 train_time:18633ms step_avg:34.13ms
step:547/1880 train_time:18667ms step_avg:34.13ms
step:548/1880 train_time:18701ms step_avg:34.13ms
step:549/1880 train_time:18734ms step_avg:34.12ms
step:550/1880 train_time:18769ms step_avg:34.12ms
step:551/1880 train_time:18802ms step_avg:34.12ms
step:552/1880 train_time:18836ms step_avg:34.12ms
step:553/1880 train_time:18870ms step_avg:34.12ms
step:554/1880 train_time:18904ms step_avg:34.12ms
step:555/1880 train_time:18938ms step_avg:34.12ms
step:556/1880 train_time:18972ms step_avg:34.12ms
step:557/1880 train_time:19006ms step_avg:34.12ms
step:558/1880 train_time:19040ms step_avg:34.12ms
step:559/1880 train_time:19074ms step_avg:34.12ms
step:560/1880 train_time:19108ms step_avg:34.12ms
step:561/1880 train_time:19142ms step_avg:34.12ms
step:562/1880 train_time:19176ms step_avg:34.12ms
step:563/1880 train_time:19210ms step_avg:34.12ms
step:564/1880 train_time:19244ms step_avg:34.12ms
step:565/1880 train_time:19278ms step_avg:34.12ms
step:566/1880 train_time:19313ms step_avg:34.12ms
step:567/1880 train_time:19347ms step_avg:34.12ms
step:568/1880 train_time:19381ms step_avg:34.12ms
step:569/1880 train_time:19415ms step_avg:34.12ms
step:570/1880 train_time:19449ms step_avg:34.12ms
step:571/1880 train_time:19482ms step_avg:34.12ms
step:572/1880 train_time:19517ms step_avg:34.12ms
step:573/1880 train_time:19550ms step_avg:34.12ms
step:574/1880 train_time:19585ms step_avg:34.12ms
step:575/1880 train_time:19618ms step_avg:34.12ms
step:576/1880 train_time:19653ms step_avg:34.12ms
step:577/1880 train_time:19687ms step_avg:34.12ms
step:578/1880 train_time:19721ms step_avg:34.12ms
step:579/1880 train_time:19755ms step_avg:34.12ms
step:580/1880 train_time:19789ms step_avg:34.12ms
step:581/1880 train_time:19823ms step_avg:34.12ms
step:582/1880 train_time:19858ms step_avg:34.12ms
step:583/1880 train_time:19890ms step_avg:34.12ms
step:584/1880 train_time:19924ms step_avg:34.12ms
step:585/1880 train_time:19958ms step_avg:34.12ms
step:586/1880 train_time:19992ms step_avg:34.12ms
step:587/1880 train_time:20026ms step_avg:34.12ms
step:588/1880 train_time:20060ms step_avg:34.12ms
step:589/1880 train_time:20094ms step_avg:34.11ms
step:590/1880 train_time:20128ms step_avg:34.12ms
step:591/1880 train_time:20162ms step_avg:34.11ms
step:592/1880 train_time:20196ms step_avg:34.11ms
step:593/1880 train_time:20230ms step_avg:34.11ms
step:594/1880 train_time:20264ms step_avg:34.11ms
step:595/1880 train_time:20298ms step_avg:34.11ms
step:596/1880 train_time:20332ms step_avg:34.11ms
step:597/1880 train_time:20366ms step_avg:34.11ms
step:598/1880 train_time:20400ms step_avg:34.11ms
step:599/1880 train_time:20434ms step_avg:34.11ms
step:600/1880 train_time:20468ms step_avg:34.11ms
step:601/1880 train_time:20502ms step_avg:34.11ms
step:602/1880 train_time:20536ms step_avg:34.11ms
step:603/1880 train_time:20570ms step_avg:34.11ms
step:604/1880 train_time:20604ms step_avg:34.11ms
step:605/1880 train_time:20638ms step_avg:34.11ms
step:606/1880 train_time:20672ms step_avg:34.11ms
step:607/1880 train_time:20706ms step_avg:34.11ms
step:608/1880 train_time:20740ms step_avg:34.11ms
step:609/1880 train_time:20774ms step_avg:34.11ms
step:610/1880 train_time:20808ms step_avg:34.11ms
step:611/1880 train_time:20842ms step_avg:34.11ms
step:612/1880 train_time:20876ms step_avg:34.11ms
step:613/1880 train_time:20910ms step_avg:34.11ms
step:614/1880 train_time:20944ms step_avg:34.11ms
step:615/1880 train_time:20978ms step_avg:34.11ms
step:616/1880 train_time:21039ms step_avg:34.15ms
step:617/1880 train_time:21100ms step_avg:34.20ms
step:618/1880 train_time:21161ms step_avg:34.24ms
step:619/1880 train_time:21224ms step_avg:34.29ms
step:620/1880 train_time:21284ms step_avg:34.33ms
step:621/1880 train_time:21347ms step_avg:34.37ms
step:622/1880 train_time:21408ms step_avg:34.42ms
step:623/1880 train_time:21469ms step_avg:34.46ms
step:624/1880 train_time:21531ms step_avg:34.50ms
step:625/1880 train_time:21592ms step_avg:34.55ms
step:626/1880 train_time:21654ms step_avg:34.59ms
step:627/1880 train_time:21716ms step_avg:34.64ms
step:628/1880 train_time:21779ms step_avg:34.68ms
step:629/1880 train_time:21841ms step_avg:34.72ms
step:630/1880 train_time:21903ms step_avg:34.77ms
step:631/1880 train_time:21964ms step_avg:34.81ms
step:632/1880 train_time:22024ms step_avg:34.85ms
step:633/1880 train_time:22086ms step_avg:34.89ms
step:634/1880 train_time:22147ms step_avg:34.93ms
step:635/1880 train_time:22208ms step_avg:34.97ms
step:636/1880 train_time:22270ms step_avg:35.02ms
step:637/1880 train_time:22332ms step_avg:35.06ms
step:638/1880 train_time:22394ms step_avg:35.10ms
step:639/1880 train_time:22455ms step_avg:35.14ms
step:640/1880 train_time:22517ms step_avg:35.18ms
step:641/1880 train_time:22579ms step_avg:35.22ms
step:642/1880 train_time:22641ms step_avg:35.27ms
step:643/1880 train_time:22703ms step_avg:35.31ms
step:644/1880 train_time:22764ms step_avg:35.35ms
step:645/1880 train_time:22825ms step_avg:35.39ms
step:646/1880 train_time:22886ms step_avg:35.43ms
step:647/1880 train_time:22948ms step_avg:35.47ms
step:648/1880 train_time:23009ms step_avg:35.51ms
step:649/1880 train_time:23070ms step_avg:35.55ms
step:650/1880 train_time:23132ms step_avg:35.59ms
step:651/1880 train_time:23194ms step_avg:35.63ms
step:652/1880 train_time:23256ms step_avg:35.67ms
step:653/1880 train_time:23318ms step_avg:35.71ms
step:654/1880 train_time:23379ms step_avg:35.75ms
step:655/1880 train_time:23441ms step_avg:35.79ms
step:656/1880 train_time:23502ms step_avg:35.83ms
step:657/1880 train_time:23564ms step_avg:35.87ms
step:658/1880 train_time:23626ms step_avg:35.91ms
step:659/1880 train_time:23688ms step_avg:35.95ms
step:660/1880 train_time:23749ms step_avg:35.98ms
step:661/1880 train_time:23811ms step_avg:36.02ms
step:662/1880 train_time:23872ms step_avg:36.06ms
step:663/1880 train_time:23934ms step_avg:36.10ms
step:664/1880 train_time:23995ms step_avg:36.14ms
step:665/1880 train_time:24056ms step_avg:36.18ms
step:666/1880 train_time:24118ms step_avg:36.21ms
step:667/1880 train_time:24180ms step_avg:36.25ms
step:668/1880 train_time:24241ms step_avg:36.29ms
step:669/1880 train_time:24303ms step_avg:36.33ms
step:670/1880 train_time:24363ms step_avg:36.36ms
step:671/1880 train_time:24425ms step_avg:36.40ms
step:672/1880 train_time:24486ms step_avg:36.44ms
step:673/1880 train_time:24548ms step_avg:36.47ms
step:674/1880 train_time:24609ms step_avg:36.51ms
step:675/1880 train_time:24671ms step_avg:36.55ms
step:676/1880 train_time:24732ms step_avg:36.59ms
step:677/1880 train_time:24794ms step_avg:36.62ms
step:678/1880 train_time:24856ms step_avg:36.66ms
step:679/1880 train_time:24918ms step_avg:36.70ms
step:680/1880 train_time:24979ms step_avg:36.73ms
step:681/1880 train_time:25041ms step_avg:36.77ms
step:682/1880 train_time:25102ms step_avg:36.81ms
step:683/1880 train_time:25163ms step_avg:36.84ms
step:684/1880 train_time:25224ms step_avg:36.88ms
step:685/1880 train_time:25286ms step_avg:36.91ms
step:686/1880 train_time:25348ms step_avg:36.95ms
step:687/1880 train_time:25409ms step_avg:36.99ms
step:688/1880 train_time:25470ms step_avg:37.02ms
step:689/1880 train_time:25532ms step_avg:37.06ms
step:690/1880 train_time:25593ms step_avg:37.09ms
step:691/1880 train_time:25656ms step_avg:37.13ms
step:692/1880 train_time:25718ms step_avg:37.16ms
step:693/1880 train_time:25779ms step_avg:37.20ms
step:694/1880 train_time:25840ms step_avg:37.23ms
step:695/1880 train_time:25902ms step_avg:37.27ms
step:696/1880 train_time:25964ms step_avg:37.30ms
step:697/1880 train_time:26025ms step_avg:37.34ms
step:698/1880 train_time:26086ms step_avg:37.37ms
step:699/1880 train_time:26148ms step_avg:37.41ms
step:700/1880 train_time:26209ms step_avg:37.44ms
step:701/1880 train_time:26271ms step_avg:37.48ms
step:702/1880 train_time:26333ms step_avg:37.51ms
step:703/1880 train_time:26395ms step_avg:37.55ms
step:704/1880 train_time:26458ms step_avg:37.58ms
step:705/1880 train_time:26520ms step_avg:37.62ms
step:706/1880 train_time:26581ms step_avg:37.65ms
step:707/1880 train_time:26642ms step_avg:37.68ms
step:708/1880 train_time:26703ms step_avg:37.72ms
step:709/1880 train_time:26765ms step_avg:37.75ms
step:710/1880 train_time:26826ms step_avg:37.78ms
step:711/1880 train_time:26888ms step_avg:37.82ms
step:712/1880 train_time:26950ms step_avg:37.85ms
step:713/1880 train_time:27011ms step_avg:37.88ms
step:714/1880 train_time:27072ms step_avg:37.92ms
step:715/1880 train_time:27134ms step_avg:37.95ms
step:716/1880 train_time:27196ms step_avg:37.98ms
step:717/1880 train_time:27258ms step_avg:38.02ms
step:718/1880 train_time:27319ms step_avg:38.05ms
step:719/1880 train_time:27381ms step_avg:38.08ms
step:720/1880 train_time:27442ms step_avg:38.11ms
step:721/1880 train_time:27504ms step_avg:38.15ms
step:722/1880 train_time:27565ms step_avg:38.18ms
step:723/1880 train_time:27627ms step_avg:38.21ms
step:724/1880 train_time:27688ms step_avg:38.24ms
step:725/1880 train_time:27750ms step_avg:38.28ms
step:726/1880 train_time:27810ms step_avg:38.31ms
step:727/1880 train_time:27872ms step_avg:38.34ms
step:728/1880 train_time:27934ms step_avg:38.37ms
step:729/1880 train_time:27996ms step_avg:38.40ms
step:730/1880 train_time:28058ms step_avg:38.44ms
step:731/1880 train_time:28120ms step_avg:38.47ms
step:732/1880 train_time:28181ms step_avg:38.50ms
step:733/1880 train_time:28243ms step_avg:38.53ms
step:734/1880 train_time:28304ms step_avg:38.56ms
step:735/1880 train_time:28365ms step_avg:38.59ms
step:736/1880 train_time:28426ms step_avg:38.62ms
step:737/1880 train_time:28489ms step_avg:38.65ms
step:738/1880 train_time:28550ms step_avg:38.69ms
step:739/1880 train_time:28611ms step_avg:38.72ms
step:740/1880 train_time:28672ms step_avg:38.75ms
step:741/1880 train_time:28734ms step_avg:38.78ms
step:742/1880 train_time:28796ms step_avg:38.81ms
step:743/1880 train_time:28857ms step_avg:38.84ms
step:744/1880 train_time:28919ms step_avg:38.87ms
step:745/1880 train_time:28980ms step_avg:38.90ms
step:746/1880 train_time:29041ms step_avg:38.93ms
step:747/1880 train_time:29103ms step_avg:38.96ms
step:748/1880 train_time:29164ms step_avg:38.99ms
step:749/1880 train_time:29226ms step_avg:39.02ms
step:750/1880 train_time:29286ms step_avg:39.05ms
step:750/1880 val_loss:4.0278 train_time:29351ms step_avg:39.13ms
step:751/1880 train_time:29373ms step_avg:39.11ms
step:752/1880 train_time:29412ms step_avg:39.11ms
step:753/1880 train_time:29476ms step_avg:39.15ms
step:754/1880 train_time:29539ms step_avg:39.18ms
step:755/1880 train_time:29601ms step_avg:39.21ms
step:756/1880 train_time:29662ms step_avg:39.24ms
step:757/1880 train_time:29723ms step_avg:39.26ms
step:758/1880 train_time:29784ms step_avg:39.29ms
step:759/1880 train_time:29845ms step_avg:39.32ms
step:760/1880 train_time:29906ms step_avg:39.35ms
step:761/1880 train_time:29967ms step_avg:39.38ms
step:762/1880 train_time:30028ms step_avg:39.41ms
step:763/1880 train_time:30089ms step_avg:39.44ms
step:764/1880 train_time:30151ms step_avg:39.46ms
step:765/1880 train_time:30212ms step_avg:39.49ms
step:766/1880 train_time:30274ms step_avg:39.52ms
step:767/1880 train_time:30337ms step_avg:39.55ms
step:768/1880 train_time:30400ms step_avg:39.58ms
step:769/1880 train_time:30463ms step_avg:39.61ms
step:770/1880 train_time:30524ms step_avg:39.64ms
step:771/1880 train_time:30587ms step_avg:39.67ms
step:772/1880 train_time:30649ms step_avg:39.70ms
step:773/1880 train_time:30711ms step_avg:39.73ms
step:774/1880 train_time:30773ms step_avg:39.76ms
step:775/1880 train_time:30834ms step_avg:39.79ms
step:776/1880 train_time:30896ms step_avg:39.81ms
step:777/1880 train_time:30957ms step_avg:39.84ms
step:778/1880 train_time:31018ms step_avg:39.87ms
step:779/1880 train_time:31079ms step_avg:39.90ms
step:780/1880 train_time:31140ms step_avg:39.92ms
step:781/1880 train_time:31202ms step_avg:39.95ms
step:782/1880 train_time:31263ms step_avg:39.98ms
step:783/1880 train_time:31325ms step_avg:40.01ms
step:784/1880 train_time:31387ms step_avg:40.03ms
step:785/1880 train_time:31449ms step_avg:40.06ms
step:786/1880 train_time:31511ms step_avg:40.09ms
step:787/1880 train_time:31573ms step_avg:40.12ms
step:788/1880 train_time:31635ms step_avg:40.15ms
step:789/1880 train_time:31698ms step_avg:40.17ms
step:790/1880 train_time:31759ms step_avg:40.20ms
step:791/1880 train_time:31820ms step_avg:40.23ms
step:792/1880 train_time:31881ms step_avg:40.25ms
step:793/1880 train_time:31943ms step_avg:40.28ms
step:794/1880 train_time:32004ms step_avg:40.31ms
step:795/1880 train_time:32065ms step_avg:40.33ms
step:796/1880 train_time:32126ms step_avg:40.36ms
step:797/1880 train_time:32187ms step_avg:40.39ms
step:798/1880 train_time:32248ms step_avg:40.41ms
step:799/1880 train_time:32310ms step_avg:40.44ms
step:800/1880 train_time:32372ms step_avg:40.46ms
step:801/1880 train_time:32434ms step_avg:40.49ms
step:802/1880 train_time:32496ms step_avg:40.52ms
step:803/1880 train_time:32558ms step_avg:40.55ms
step:804/1880 train_time:32619ms step_avg:40.57ms
step:805/1880 train_time:32681ms step_avg:40.60ms
step:806/1880 train_time:32742ms step_avg:40.62ms
step:807/1880 train_time:32804ms step_avg:40.65ms
step:808/1880 train_time:32865ms step_avg:40.67ms
step:809/1880 train_time:32926ms step_avg:40.70ms
step:810/1880 train_time:32986ms step_avg:40.72ms
step:811/1880 train_time:33048ms step_avg:40.75ms
step:812/1880 train_time:33109ms step_avg:40.77ms
step:813/1880 train_time:33170ms step_avg:40.80ms
step:814/1880 train_time:33232ms step_avg:40.83ms
step:815/1880 train_time:33293ms step_avg:40.85ms
step:816/1880 train_time:33355ms step_avg:40.88ms
step:817/1880 train_time:33417ms step_avg:40.90ms
step:818/1880 train_time:33479ms step_avg:40.93ms
step:819/1880 train_time:33541ms step_avg:40.95ms
step:820/1880 train_time:33602ms step_avg:40.98ms
step:821/1880 train_time:33664ms step_avg:41.00ms
step:822/1880 train_time:33725ms step_avg:41.03ms
step:823/1880 train_time:33787ms step_avg:41.05ms
step:824/1880 train_time:33847ms step_avg:41.08ms
step:825/1880 train_time:33909ms step_avg:41.10ms
step:826/1880 train_time:33971ms step_avg:41.13ms
step:827/1880 train_time:34033ms step_avg:41.15ms
step:828/1880 train_time:34094ms step_avg:41.18ms
step:829/1880 train_time:34156ms step_avg:41.20ms
step:830/1880 train_time:34218ms step_avg:41.23ms
step:831/1880 train_time:34279ms step_avg:41.25ms
step:832/1880 train_time:34340ms step_avg:41.27ms
step:833/1880 train_time:34402ms step_avg:41.30ms
step:834/1880 train_time:34465ms step_avg:41.32ms
step:835/1880 train_time:34526ms step_avg:41.35ms
step:836/1880 train_time:34588ms step_avg:41.37ms
step:837/1880 train_time:34650ms step_avg:41.40ms
step:838/1880 train_time:34713ms step_avg:41.42ms
step:839/1880 train_time:34774ms step_avg:41.45ms
step:840/1880 train_time:34836ms step_avg:41.47ms
step:841/1880 train_time:34898ms step_avg:41.50ms
step:842/1880 train_time:34960ms step_avg:41.52ms
step:843/1880 train_time:35021ms step_avg:41.54ms
step:844/1880 train_time:35082ms step_avg:41.57ms
step:845/1880 train_time:35145ms step_avg:41.59ms
step:846/1880 train_time:35205ms step_avg:41.61ms
step:847/1880 train_time:35267ms step_avg:41.64ms
step:848/1880 train_time:35329ms step_avg:41.66ms
step:849/1880 train_time:35390ms step_avg:41.68ms
step:850/1880 train_time:35452ms step_avg:41.71ms
step:851/1880 train_time:35514ms step_avg:41.73ms
step:852/1880 train_time:35576ms step_avg:41.76ms
step:853/1880 train_time:35637ms step_avg:41.78ms
step:854/1880 train_time:35699ms step_avg:41.80ms
step:855/1880 train_time:35761ms step_avg:41.83ms
step:856/1880 train_time:35821ms step_avg:41.85ms
step:857/1880 train_time:35883ms step_avg:41.87ms
step:858/1880 train_time:35944ms step_avg:41.89ms
step:859/1880 train_time:36006ms step_avg:41.92ms
step:860/1880 train_time:36067ms step_avg:41.94ms
step:861/1880 train_time:36129ms step_avg:41.96ms
step:862/1880 train_time:36191ms step_avg:41.98ms
step:863/1880 train_time:36253ms step_avg:42.01ms
step:864/1880 train_time:36315ms step_avg:42.03ms
step:865/1880 train_time:36377ms step_avg:42.05ms
step:866/1880 train_time:36438ms step_avg:42.08ms
step:867/1880 train_time:36499ms step_avg:42.10ms
step:868/1880 train_time:36561ms step_avg:42.12ms
step:869/1880 train_time:36623ms step_avg:42.14ms
step:870/1880 train_time:36684ms step_avg:42.17ms
step:871/1880 train_time:36746ms step_avg:42.19ms
step:872/1880 train_time:36807ms step_avg:42.21ms
step:873/1880 train_time:36869ms step_avg:42.23ms
step:874/1880 train_time:36931ms step_avg:42.26ms
step:875/1880 train_time:36994ms step_avg:42.28ms
step:876/1880 train_time:37056ms step_avg:42.30ms
step:877/1880 train_time:37118ms step_avg:42.32ms
step:878/1880 train_time:37179ms step_avg:42.35ms
step:879/1880 train_time:37241ms step_avg:42.37ms
step:880/1880 train_time:37303ms step_avg:42.39ms
step:881/1880 train_time:37364ms step_avg:42.41ms
step:882/1880 train_time:37425ms step_avg:42.43ms
step:883/1880 train_time:37487ms step_avg:42.45ms
step:884/1880 train_time:37548ms step_avg:42.47ms
step:885/1880 train_time:37610ms step_avg:42.50ms
step:886/1880 train_time:37672ms step_avg:42.52ms
step:887/1880 train_time:37734ms step_avg:42.54ms
step:888/1880 train_time:37796ms step_avg:42.56ms
step:889/1880 train_time:37857ms step_avg:42.58ms
step:890/1880 train_time:37919ms step_avg:42.61ms
step:891/1880 train_time:37981ms step_avg:42.63ms
step:892/1880 train_time:38042ms step_avg:42.65ms
step:893/1880 train_time:38104ms step_avg:42.67ms
step:894/1880 train_time:38165ms step_avg:42.69ms
step:895/1880 train_time:38228ms step_avg:42.71ms
step:896/1880 train_time:38289ms step_avg:42.73ms
step:897/1880 train_time:38352ms step_avg:42.76ms
step:898/1880 train_time:38414ms step_avg:42.78ms
step:899/1880 train_time:38476ms step_avg:42.80ms
step:900/1880 train_time:38537ms step_avg:42.82ms
step:901/1880 train_time:38599ms step_avg:42.84ms
step:902/1880 train_time:38660ms step_avg:42.86ms
step:903/1880 train_time:38722ms step_avg:42.88ms
step:904/1880 train_time:38783ms step_avg:42.90ms
step:905/1880 train_time:38845ms step_avg:42.92ms
step:906/1880 train_time:38906ms step_avg:42.94ms
step:907/1880 train_time:38969ms step_avg:42.96ms
step:908/1880 train_time:39030ms step_avg:42.98ms
step:909/1880 train_time:39092ms step_avg:43.01ms
step:910/1880 train_time:39155ms step_avg:43.03ms
step:911/1880 train_time:39217ms step_avg:43.05ms
step:912/1880 train_time:39278ms step_avg:43.07ms
step:913/1880 train_time:39340ms step_avg:43.09ms
step:914/1880 train_time:39401ms step_avg:43.11ms
step:915/1880 train_time:39463ms step_avg:43.13ms
step:916/1880 train_time:39524ms step_avg:43.15ms
step:917/1880 train_time:39586ms step_avg:43.17ms
step:918/1880 train_time:39648ms step_avg:43.19ms
step:919/1880 train_time:39710ms step_avg:43.21ms
step:920/1880 train_time:39772ms step_avg:43.23ms
step:921/1880 train_time:39833ms step_avg:43.25ms
step:922/1880 train_time:39896ms step_avg:43.27ms
step:923/1880 train_time:39958ms step_avg:43.29ms
step:924/1880 train_time:40019ms step_avg:43.31ms
step:925/1880 train_time:40081ms step_avg:43.33ms
step:926/1880 train_time:40142ms step_avg:43.35ms
step:927/1880 train_time:40204ms step_avg:43.37ms
step:928/1880 train_time:40265ms step_avg:43.39ms
step:929/1880 train_time:40327ms step_avg:43.41ms
step:930/1880 train_time:40388ms step_avg:43.43ms
step:931/1880 train_time:40449ms step_avg:43.45ms
step:932/1880 train_time:40511ms step_avg:43.47ms
step:933/1880 train_time:40573ms step_avg:43.49ms
step:934/1880 train_time:40634ms step_avg:43.51ms
step:935/1880 train_time:40696ms step_avg:43.53ms
step:936/1880 train_time:40759ms step_avg:43.55ms
step:937/1880 train_time:40820ms step_avg:43.56ms
step:938/1880 train_time:40881ms step_avg:43.58ms
step:939/1880 train_time:40943ms step_avg:43.60ms
step:940/1880 train_time:41005ms step_avg:43.62ms
step:941/1880 train_time:41066ms step_avg:43.64ms
step:942/1880 train_time:41128ms step_avg:43.66ms
step:943/1880 train_time:41190ms step_avg:43.68ms
step:944/1880 train_time:41251ms step_avg:43.70ms
step:945/1880 train_time:41313ms step_avg:43.72ms
step:946/1880 train_time:41375ms step_avg:43.74ms
step:947/1880 train_time:41436ms step_avg:43.76ms
step:948/1880 train_time:41498ms step_avg:43.77ms
step:949/1880 train_time:41560ms step_avg:43.79ms
step:950/1880 train_time:41621ms step_avg:43.81ms
step:951/1880 train_time:41683ms step_avg:43.83ms
step:952/1880 train_time:41744ms step_avg:43.85ms
step:953/1880 train_time:41806ms step_avg:43.87ms
step:954/1880 train_time:41867ms step_avg:43.89ms
step:955/1880 train_time:41929ms step_avg:43.90ms
step:956/1880 train_time:41990ms step_avg:43.92ms
step:957/1880 train_time:42053ms step_avg:43.94ms
step:958/1880 train_time:42115ms step_avg:43.96ms
step:959/1880 train_time:42176ms step_avg:43.98ms
step:960/1880 train_time:42237ms step_avg:44.00ms
step:961/1880 train_time:42300ms step_avg:44.02ms
step:962/1880 train_time:42360ms step_avg:44.03ms
step:963/1880 train_time:42422ms step_avg:44.05ms
step:964/1880 train_time:42484ms step_avg:44.07ms
step:965/1880 train_time:42546ms step_avg:44.09ms
step:966/1880 train_time:42607ms step_avg:44.11ms
step:967/1880 train_time:42670ms step_avg:44.13ms
step:968/1880 train_time:42731ms step_avg:44.14ms
step:969/1880 train_time:42794ms step_avg:44.16ms
step:970/1880 train_time:42855ms step_avg:44.18ms
step:971/1880 train_time:42917ms step_avg:44.20ms
step:972/1880 train_time:42978ms step_avg:44.22ms
step:973/1880 train_time:43040ms step_avg:44.23ms
step:974/1880 train_time:43101ms step_avg:44.25ms
step:975/1880 train_time:43163ms step_avg:44.27ms
step:976/1880 train_time:43224ms step_avg:44.29ms
step:977/1880 train_time:43286ms step_avg:44.31ms
step:978/1880 train_time:43347ms step_avg:44.32ms
step:979/1880 train_time:43409ms step_avg:44.34ms
step:980/1880 train_time:43470ms step_avg:44.36ms
step:981/1880 train_time:43532ms step_avg:44.38ms
step:982/1880 train_time:43594ms step_avg:44.39ms
step:983/1880 train_time:43656ms step_avg:44.41ms
step:984/1880 train_time:43718ms step_avg:44.43ms
step:985/1880 train_time:43780ms step_avg:44.45ms
step:986/1880 train_time:43841ms step_avg:44.46ms
step:987/1880 train_time:43903ms step_avg:44.48ms
step:988/1880 train_time:43964ms step_avg:44.50ms
step:989/1880 train_time:44027ms step_avg:44.52ms
step:990/1880 train_time:44088ms step_avg:44.53ms
step:991/1880 train_time:44149ms step_avg:44.55ms
step:992/1880 train_time:44210ms step_avg:44.57ms
step:993/1880 train_time:44273ms step_avg:44.58ms
step:994/1880 train_time:44335ms step_avg:44.60ms
step:995/1880 train_time:44396ms step_avg:44.62ms
step:996/1880 train_time:44458ms step_avg:44.64ms
step:997/1880 train_time:44520ms step_avg:44.65ms
step:998/1880 train_time:44581ms step_avg:44.67ms
step:999/1880 train_time:44643ms step_avg:44.69ms
step:1000/1880 train_time:44705ms step_avg:44.70ms
step:1000/1880 val_loss:3.7733 train_time:44770ms step_avg:44.77ms
step:1001/1880 train_time:44791ms step_avg:44.75ms
step:1002/1880 train_time:44832ms step_avg:44.74ms
step:1003/1880 train_time:44897ms step_avg:44.76ms
step:1004/1880 train_time:44959ms step_avg:44.78ms
step:1005/1880 train_time:45021ms step_avg:44.80ms
step:1006/1880 train_time:45082ms step_avg:44.81ms
step:1007/1880 train_time:45144ms step_avg:44.83ms
step:1008/1880 train_time:45205ms step_avg:44.85ms
step:1009/1880 train_time:45266ms step_avg:44.86ms
step:1010/1880 train_time:45327ms step_avg:44.88ms
step:1011/1880 train_time:45388ms step_avg:44.89ms
step:1012/1880 train_time:45449ms step_avg:44.91ms
step:1013/1880 train_time:45511ms step_avg:44.93ms
step:1014/1880 train_time:45572ms step_avg:44.94ms
step:1015/1880 train_time:45633ms step_avg:44.96ms
step:1016/1880 train_time:45694ms step_avg:44.97ms
step:1017/1880 train_time:45757ms step_avg:44.99ms
step:1018/1880 train_time:45819ms step_avg:45.01ms
step:1019/1880 train_time:45883ms step_avg:45.03ms
step:1020/1880 train_time:45946ms step_avg:45.04ms
step:1021/1880 train_time:46008ms step_avg:45.06ms
step:1022/1880 train_time:46070ms step_avg:45.08ms
step:1023/1880 train_time:46132ms step_avg:45.09ms
step:1024/1880 train_time:46193ms step_avg:45.11ms
step:1025/1880 train_time:46255ms step_avg:45.13ms
step:1026/1880 train_time:46316ms step_avg:45.14ms
step:1027/1880 train_time:46378ms step_avg:45.16ms
step:1028/1880 train_time:46439ms step_avg:45.17ms
step:1029/1880 train_time:46500ms step_avg:45.19ms
step:1030/1880 train_time:46561ms step_avg:45.20ms
step:1031/1880 train_time:46622ms step_avg:45.22ms
step:1032/1880 train_time:46683ms step_avg:45.24ms
step:1033/1880 train_time:46746ms step_avg:45.25ms
step:1034/1880 train_time:46808ms step_avg:45.27ms
step:1035/1880 train_time:46871ms step_avg:45.29ms
step:1036/1880 train_time:46933ms step_avg:45.30ms
step:1037/1880 train_time:46995ms step_avg:45.32ms
step:1038/1880 train_time:47057ms step_avg:45.33ms
step:1039/1880 train_time:47119ms step_avg:45.35ms
step:1040/1880 train_time:47180ms step_avg:45.37ms
step:1041/1880 train_time:47241ms step_avg:45.38ms
step:1042/1880 train_time:47302ms step_avg:45.40ms
step:1043/1880 train_time:47364ms step_avg:45.41ms
step:1044/1880 train_time:47425ms step_avg:45.43ms
step:1045/1880 train_time:47487ms step_avg:45.44ms
step:1046/1880 train_time:47548ms step_avg:45.46ms
step:1047/1880 train_time:47609ms step_avg:45.47ms
step:1048/1880 train_time:47671ms step_avg:45.49ms
step:1049/1880 train_time:47733ms step_avg:45.50ms
step:1050/1880 train_time:47794ms step_avg:45.52ms
step:1051/1880 train_time:47857ms step_avg:45.53ms
step:1052/1880 train_time:47919ms step_avg:45.55ms
step:1053/1880 train_time:47982ms step_avg:45.57ms
step:1054/1880 train_time:48043ms step_avg:45.58ms
step:1055/1880 train_time:48105ms step_avg:45.60ms
step:1056/1880 train_time:48167ms step_avg:45.61ms
step:1057/1880 train_time:48229ms step_avg:45.63ms
step:1058/1880 train_time:48291ms step_avg:45.64ms
step:1059/1880 train_time:48352ms step_avg:45.66ms
step:1060/1880 train_time:48413ms step_avg:45.67ms
step:1061/1880 train_time:48475ms step_avg:45.69ms
step:1062/1880 train_time:48537ms step_avg:45.70ms
step:1063/1880 train_time:48599ms step_avg:45.72ms
step:1064/1880 train_time:48660ms step_avg:45.73ms
step:1065/1880 train_time:48722ms step_avg:45.75ms
step:1066/1880 train_time:48783ms step_avg:45.76ms
step:1067/1880 train_time:48846ms step_avg:45.78ms
step:1068/1880 train_time:48909ms step_avg:45.79ms
step:1069/1880 train_time:48971ms step_avg:45.81ms
step:1070/1880 train_time:49033ms step_avg:45.83ms
step:1071/1880 train_time:49095ms step_avg:45.84ms
step:1072/1880 train_time:49156ms step_avg:45.85ms
step:1073/1880 train_time:49217ms step_avg:45.87ms
step:1074/1880 train_time:49278ms step_avg:45.88ms
step:1075/1880 train_time:49341ms step_avg:45.90ms
step:1076/1880 train_time:49402ms step_avg:45.91ms
step:1077/1880 train_time:49464ms step_avg:45.93ms
step:1078/1880 train_time:49525ms step_avg:45.94ms
step:1079/1880 train_time:49587ms step_avg:45.96ms
step:1080/1880 train_time:49649ms step_avg:45.97ms
step:1081/1880 train_time:49712ms step_avg:45.99ms
step:1082/1880 train_time:49773ms step_avg:46.00ms
step:1083/1880 train_time:49835ms step_avg:46.02ms
step:1084/1880 train_time:49897ms step_avg:46.03ms
step:1085/1880 train_time:49959ms step_avg:46.05ms
step:1086/1880 train_time:50021ms step_avg:46.06ms
step:1087/1880 train_time:50083ms step_avg:46.07ms
step:1088/1880 train_time:50146ms step_avg:46.09ms
step:1089/1880 train_time:50208ms step_avg:46.10ms
step:1090/1880 train_time:50270ms step_avg:46.12ms
step:1091/1880 train_time:50332ms step_avg:46.13ms
step:1092/1880 train_time:50393ms step_avg:46.15ms
step:1093/1880 train_time:50455ms step_avg:46.16ms
step:1094/1880 train_time:50516ms step_avg:46.18ms
step:1095/1880 train_time:50578ms step_avg:46.19ms
step:1096/1880 train_time:50639ms step_avg:46.20ms
step:1097/1880 train_time:50701ms step_avg:46.22ms
step:1098/1880 train_time:50763ms step_avg:46.23ms
step:1099/1880 train_time:50825ms step_avg:46.25ms
step:1100/1880 train_time:50887ms step_avg:46.26ms
step:1101/1880 train_time:50949ms step_avg:46.28ms
step:1102/1880 train_time:51011ms step_avg:46.29ms
step:1103/1880 train_time:51074ms step_avg:46.30ms
step:1104/1880 train_time:51135ms step_avg:46.32ms
step:1105/1880 train_time:51198ms step_avg:46.33ms
step:1106/1880 train_time:51259ms step_avg:46.35ms
step:1107/1880 train_time:51321ms step_avg:46.36ms
step:1108/1880 train_time:51382ms step_avg:46.37ms
step:1109/1880 train_time:51444ms step_avg:46.39ms
step:1110/1880 train_time:51506ms step_avg:46.40ms
step:1111/1880 train_time:51568ms step_avg:46.42ms
step:1112/1880 train_time:51630ms step_avg:46.43ms
step:1113/1880 train_time:51692ms step_avg:46.44ms
step:1114/1880 train_time:51753ms step_avg:46.46ms
step:1115/1880 train_time:51815ms step_avg:46.47ms
step:1116/1880 train_time:51877ms step_avg:46.48ms
step:1117/1880 train_time:51939ms step_avg:46.50ms
step:1118/1880 train_time:52000ms step_avg:46.51ms
step:1119/1880 train_time:52062ms step_avg:46.53ms
step:1120/1880 train_time:52123ms step_avg:46.54ms
step:1121/1880 train_time:52185ms step_avg:46.55ms
step:1122/1880 train_time:52247ms step_avg:46.57ms
step:1123/1880 train_time:52309ms step_avg:46.58ms
step:1124/1880 train_time:52371ms step_avg:46.59ms
step:1125/1880 train_time:52433ms step_avg:46.61ms
step:1126/1880 train_time:52495ms step_avg:46.62ms
step:1127/1880 train_time:52556ms step_avg:46.63ms
step:1128/1880 train_time:52617ms step_avg:46.65ms
step:1129/1880 train_time:52679ms step_avg:46.66ms
step:1130/1880 train_time:52740ms step_avg:46.67ms
step:1131/1880 train_time:52803ms step_avg:46.69ms
step:1132/1880 train_time:52866ms step_avg:46.70ms
step:1133/1880 train_time:52928ms step_avg:46.71ms
step:1134/1880 train_time:52990ms step_avg:46.73ms
step:1135/1880 train_time:53052ms step_avg:46.74ms
step:1136/1880 train_time:53113ms step_avg:46.75ms
step:1137/1880 train_time:53175ms step_avg:46.77ms
step:1138/1880 train_time:53236ms step_avg:46.78ms
step:1139/1880 train_time:53297ms step_avg:46.79ms
step:1140/1880 train_time:53359ms step_avg:46.81ms
step:1141/1880 train_time:53421ms step_avg:46.82ms
step:1142/1880 train_time:53482ms step_avg:46.83ms
step:1143/1880 train_time:53545ms step_avg:46.85ms
step:1144/1880 train_time:53606ms step_avg:46.86ms
step:1145/1880 train_time:53668ms step_avg:46.87ms
step:1146/1880 train_time:53729ms step_avg:46.88ms
step:1147/1880 train_time:53791ms step_avg:46.90ms
step:1148/1880 train_time:53853ms step_avg:46.91ms
step:1149/1880 train_time:53915ms step_avg:46.92ms
step:1150/1880 train_time:53977ms step_avg:46.94ms
step:1151/1880 train_time:54039ms step_avg:46.95ms
step:1152/1880 train_time:54100ms step_avg:46.96ms
step:1153/1880 train_time:54162ms step_avg:46.97ms
step:1154/1880 train_time:54223ms step_avg:46.99ms
step:1155/1880 train_time:54284ms step_avg:47.00ms
step:1156/1880 train_time:54346ms step_avg:47.01ms
step:1157/1880 train_time:54408ms step_avg:47.03ms
step:1158/1880 train_time:54469ms step_avg:47.04ms
step:1159/1880 train_time:54532ms step_avg:47.05ms
step:1160/1880 train_time:54593ms step_avg:47.06ms
step:1161/1880 train_time:54655ms step_avg:47.08ms
step:1162/1880 train_time:54716ms step_avg:47.09ms
step:1163/1880 train_time:54778ms step_avg:47.10ms
step:1164/1880 train_time:54840ms step_avg:47.11ms
step:1165/1880 train_time:54902ms step_avg:47.13ms
step:1166/1880 train_time:54964ms step_avg:47.14ms
step:1167/1880 train_time:55026ms step_avg:47.15ms
step:1168/1880 train_time:55088ms step_avg:47.16ms
step:1169/1880 train_time:55150ms step_avg:47.18ms
step:1170/1880 train_time:55211ms step_avg:47.19ms
step:1171/1880 train_time:55273ms step_avg:47.20ms
step:1172/1880 train_time:55334ms step_avg:47.21ms
step:1173/1880 train_time:55396ms step_avg:47.23ms
step:1174/1880 train_time:55457ms step_avg:47.24ms
step:1175/1880 train_time:55519ms step_avg:47.25ms
step:1176/1880 train_time:55580ms step_avg:47.26ms
step:1177/1880 train_time:55642ms step_avg:47.27ms
step:1178/1880 train_time:55703ms step_avg:47.29ms
step:1179/1880 train_time:55764ms step_avg:47.30ms
step:1180/1880 train_time:55826ms step_avg:47.31ms
step:1181/1880 train_time:55888ms step_avg:47.32ms
step:1182/1880 train_time:55950ms step_avg:47.33ms
step:1183/1880 train_time:56013ms step_avg:47.35ms
step:1184/1880 train_time:56074ms step_avg:47.36ms
step:1185/1880 train_time:56136ms step_avg:47.37ms
step:1186/1880 train_time:56196ms step_avg:47.38ms
step:1187/1880 train_time:56258ms step_avg:47.40ms
step:1188/1880 train_time:56319ms step_avg:47.41ms
step:1189/1880 train_time:56382ms step_avg:47.42ms
step:1190/1880 train_time:56444ms step_avg:47.43ms
step:1191/1880 train_time:56506ms step_avg:47.44ms
step:1192/1880 train_time:56567ms step_avg:47.46ms
step:1193/1880 train_time:56629ms step_avg:47.47ms
step:1194/1880 train_time:56691ms step_avg:47.48ms
step:1195/1880 train_time:56753ms step_avg:47.49ms
step:1196/1880 train_time:56814ms step_avg:47.50ms
step:1197/1880 train_time:56876ms step_avg:47.52ms
step:1198/1880 train_time:56938ms step_avg:47.53ms
step:1199/1880 train_time:57000ms step_avg:47.54ms
step:1200/1880 train_time:57061ms step_avg:47.55ms
step:1201/1880 train_time:57123ms step_avg:47.56ms
step:1202/1880 train_time:57185ms step_avg:47.57ms
step:1203/1880 train_time:57247ms step_avg:47.59ms
step:1204/1880 train_time:57308ms step_avg:47.60ms
step:1205/1880 train_time:57370ms step_avg:47.61ms
step:1206/1880 train_time:57432ms step_avg:47.62ms
step:1207/1880 train_time:57494ms step_avg:47.63ms
step:1208/1880 train_time:57555ms step_avg:47.64ms
step:1209/1880 train_time:57617ms step_avg:47.66ms
step:1210/1880 train_time:57678ms step_avg:47.67ms
step:1211/1880 train_time:57741ms step_avg:47.68ms
step:1212/1880 train_time:57802ms step_avg:47.69ms
step:1213/1880 train_time:57864ms step_avg:47.70ms
step:1214/1880 train_time:57925ms step_avg:47.71ms
step:1215/1880 train_time:57988ms step_avg:47.73ms
step:1216/1880 train_time:58049ms step_avg:47.74ms
step:1217/1880 train_time:58111ms step_avg:47.75ms
step:1218/1880 train_time:58173ms step_avg:47.76ms
step:1219/1880 train_time:58234ms step_avg:47.77ms
step:1220/1880 train_time:58295ms step_avg:47.78ms
step:1221/1880 train_time:58357ms step_avg:47.79ms
step:1222/1880 train_time:58418ms step_avg:47.81ms
step:1223/1880 train_time:58480ms step_avg:47.82ms
step:1224/1880 train_time:58542ms step_avg:47.83ms
step:1225/1880 train_time:58604ms step_avg:47.84ms
step:1226/1880 train_time:58665ms step_avg:47.85ms
step:1227/1880 train_time:58727ms step_avg:47.86ms
step:1228/1880 train_time:58790ms step_avg:47.87ms
step:1229/1880 train_time:58878ms step_avg:47.91ms
step:1230/1880 train_time:58967ms step_avg:47.94ms
step:1231/1880 train_time:59054ms step_avg:47.97ms
step:1232/1880 train_time:59143ms step_avg:48.01ms
step:1233/1880 train_time:59231ms step_avg:48.04ms
step:1234/1880 train_time:59319ms step_avg:48.07ms
step:1235/1880 train_time:59409ms step_avg:48.10ms
step:1236/1880 train_time:59497ms step_avg:48.14ms
step:1237/1880 train_time:59586ms step_avg:48.17ms
step:1238/1880 train_time:59673ms step_avg:48.20ms
step:1239/1880 train_time:59761ms step_avg:48.23ms
step:1240/1880 train_time:59849ms step_avg:48.27ms
step:1241/1880 train_time:59938ms step_avg:48.30ms
step:1242/1880 train_time:60026ms step_avg:48.33ms
step:1243/1880 train_time:60114ms step_avg:48.36ms
step:1244/1880 train_time:60204ms step_avg:48.40ms
step:1245/1880 train_time:60290ms step_avg:48.43ms
step:1246/1880 train_time:60380ms step_avg:48.46ms
step:1247/1880 train_time:60469ms step_avg:48.49ms
step:1248/1880 train_time:60558ms step_avg:48.52ms
step:1249/1880 train_time:60646ms step_avg:48.56ms
step:1250/1880 train_time:60734ms step_avg:48.59ms
step:1250/1880 val_loss:3.5356 train_time:60825ms step_avg:48.66ms
step:1251/1880 train_time:60847ms step_avg:48.64ms
step:1252/1880 train_time:60912ms step_avg:48.65ms
step:1253/1880 train_time:61000ms step_avg:48.68ms
step:1254/1880 train_time:61088ms step_avg:48.71ms
step:1255/1880 train_time:61177ms step_avg:48.75ms
step:1256/1880 train_time:61264ms step_avg:48.78ms
step:1257/1880 train_time:61350ms step_avg:48.81ms
step:1258/1880 train_time:61438ms step_avg:48.84ms
step:1259/1880 train_time:61525ms step_avg:48.87ms
step:1260/1880 train_time:61613ms step_avg:48.90ms
step:1261/1880 train_time:61701ms step_avg:48.93ms
step:1262/1880 train_time:61792ms step_avg:48.96ms
step:1263/1880 train_time:61883ms step_avg:49.00ms
step:1264/1880 train_time:61972ms step_avg:49.03ms
step:1265/1880 train_time:62062ms step_avg:49.06ms
step:1266/1880 train_time:62150ms step_avg:49.09ms
step:1267/1880 train_time:62238ms step_avg:49.12ms
step:1268/1880 train_time:62325ms step_avg:49.15ms
step:1269/1880 train_time:62412ms step_avg:49.18ms
step:1270/1880 train_time:62499ms step_avg:49.21ms
step:1271/1880 train_time:62587ms step_avg:49.24ms
step:1272/1880 train_time:62674ms step_avg:49.27ms
step:1273/1880 train_time:62763ms step_avg:49.30ms
step:1274/1880 train_time:62852ms step_avg:49.33ms
step:1275/1880 train_time:62942ms step_avg:49.37ms
step:1276/1880 train_time:63031ms step_avg:49.40ms
step:1277/1880 train_time:63119ms step_avg:49.43ms
step:1278/1880 train_time:63206ms step_avg:49.46ms
step:1279/1880 train_time:63295ms step_avg:49.49ms
step:1280/1880 train_time:63382ms step_avg:49.52ms
step:1281/1880 train_time:63469ms step_avg:49.55ms
step:1282/1880 train_time:63556ms step_avg:49.58ms
step:1283/1880 train_time:63644ms step_avg:49.61ms
step:1284/1880 train_time:63732ms step_avg:49.64ms
step:1285/1880 train_time:63821ms step_avg:49.67ms
step:1286/1880 train_time:63912ms step_avg:49.70ms
step:1287/1880 train_time:64001ms step_avg:49.73ms
step:1288/1880 train_time:64089ms step_avg:49.76ms
step:1289/1880 train_time:64177ms step_avg:49.79ms
step:1290/1880 train_time:64265ms step_avg:49.82ms
step:1291/1880 train_time:64354ms step_avg:49.85ms
step:1292/1880 train_time:64441ms step_avg:49.88ms
step:1293/1880 train_time:64528ms step_avg:49.91ms
step:1294/1880 train_time:64615ms step_avg:49.93ms
step:1295/1880 train_time:64704ms step_avg:49.96ms
step:1296/1880 train_time:64792ms step_avg:49.99ms
step:1297/1880 train_time:64882ms step_avg:50.02ms
step:1298/1880 train_time:64971ms step_avg:50.05ms
step:1299/1880 train_time:65059ms step_avg:50.08ms
step:1300/1880 train_time:65148ms step_avg:50.11ms
step:1301/1880 train_time:65236ms step_avg:50.14ms
step:1302/1880 train_time:65323ms step_avg:50.17ms
step:1303/1880 train_time:65411ms step_avg:50.20ms
step:1304/1880 train_time:65498ms step_avg:50.23ms
step:1305/1880 train_time:65586ms step_avg:50.26ms
step:1306/1880 train_time:65674ms step_avg:50.29ms
step:1307/1880 train_time:65762ms step_avg:50.32ms
step:1308/1880 train_time:65851ms step_avg:50.34ms
step:1309/1880 train_time:65939ms step_avg:50.37ms
step:1310/1880 train_time:66027ms step_avg:50.40ms
step:1311/1880 train_time:66116ms step_avg:50.43ms
step:1312/1880 train_time:66203ms step_avg:50.46ms
step:1313/1880 train_time:66291ms step_avg:50.49ms
step:1314/1880 train_time:66378ms step_avg:50.52ms
step:1315/1880 train_time:66467ms step_avg:50.55ms
step:1316/1880 train_time:66554ms step_avg:50.57ms
step:1317/1880 train_time:66643ms step_avg:50.60ms
step:1318/1880 train_time:66731ms step_avg:50.63ms
step:1319/1880 train_time:66820ms step_avg:50.66ms
step:1320/1880 train_time:66908ms step_avg:50.69ms
step:1321/1880 train_time:66996ms step_avg:50.72ms
step:1322/1880 train_time:67084ms step_avg:50.74ms
step:1323/1880 train_time:67172ms step_avg:50.77ms
step:1324/1880 train_time:67260ms step_avg:50.80ms
step:1325/1880 train_time:67349ms step_avg:50.83ms
step:1326/1880 train_time:67437ms step_avg:50.86ms
step:1327/1880 train_time:67525ms step_avg:50.89ms
step:1328/1880 train_time:67612ms step_avg:50.91ms
step:1329/1880 train_time:67700ms step_avg:50.94ms
step:1330/1880 train_time:67788ms step_avg:50.97ms
step:1331/1880 train_time:67876ms step_avg:51.00ms
step:1332/1880 train_time:67965ms step_avg:51.02ms
step:1333/1880 train_time:68053ms step_avg:51.05ms
step:1334/1880 train_time:68142ms step_avg:51.08ms
step:1335/1880 train_time:68230ms step_avg:51.11ms
step:1336/1880 train_time:68317ms step_avg:51.14ms
step:1337/1880 train_time:68406ms step_avg:51.16ms
step:1338/1880 train_time:68493ms step_avg:51.19ms
step:1339/1880 train_time:68582ms step_avg:51.22ms
step:1340/1880 train_time:68670ms step_avg:51.25ms
step:1341/1880 train_time:68759ms step_avg:51.27ms
step:1342/1880 train_time:68847ms step_avg:51.30ms
step:1343/1880 train_time:68936ms step_avg:51.33ms
step:1344/1880 train_time:69024ms step_avg:51.36ms
step:1345/1880 train_time:69112ms step_avg:51.38ms
step:1346/1880 train_time:69201ms step_avg:51.41ms
step:1347/1880 train_time:69290ms step_avg:51.44ms
step:1348/1880 train_time:69377ms step_avg:51.47ms
step:1349/1880 train_time:69465ms step_avg:51.49ms
step:1350/1880 train_time:69553ms step_avg:51.52ms
step:1351/1880 train_time:69640ms step_avg:51.55ms
step:1352/1880 train_time:69729ms step_avg:51.57ms
step:1353/1880 train_time:69817ms step_avg:51.60ms
step:1354/1880 train_time:69905ms step_avg:51.63ms
step:1355/1880 train_time:69992ms step_avg:51.65ms
step:1356/1880 train_time:70081ms step_avg:51.68ms
step:1357/1880 train_time:70170ms step_avg:51.71ms
step:1358/1880 train_time:70259ms step_avg:51.74ms
step:1359/1880 train_time:70348ms step_avg:51.76ms
step:1360/1880 train_time:70435ms step_avg:51.79ms
step:1361/1880 train_time:70524ms step_avg:51.82ms
step:1362/1880 train_time:70611ms step_avg:51.84ms
step:1363/1880 train_time:70699ms step_avg:51.87ms
step:1364/1880 train_time:70789ms step_avg:51.90ms
step:1365/1880 train_time:70876ms step_avg:51.92ms
step:1366/1880 train_time:70964ms step_avg:51.95ms
step:1367/1880 train_time:71052ms step_avg:51.98ms
step:1368/1880 train_time:71139ms step_avg:52.00ms
step:1369/1880 train_time:71228ms step_avg:52.03ms
step:1370/1880 train_time:71315ms step_avg:52.05ms
step:1371/1880 train_time:71404ms step_avg:52.08ms
step:1372/1880 train_time:71492ms step_avg:52.11ms
step:1373/1880 train_time:71580ms step_avg:52.13ms
step:1374/1880 train_time:71669ms step_avg:52.16ms
step:1375/1880 train_time:71757ms step_avg:52.19ms
step:1376/1880 train_time:71844ms step_avg:52.21ms
step:1377/1880 train_time:71932ms step_avg:52.24ms
step:1378/1880 train_time:72020ms step_avg:52.26ms
step:1379/1880 train_time:72108ms step_avg:52.29ms
step:1380/1880 train_time:72195ms step_avg:52.32ms
step:1381/1880 train_time:72283ms step_avg:52.34ms
step:1382/1880 train_time:72371ms step_avg:52.37ms
step:1383/1880 train_time:72460ms step_avg:52.39ms
step:1384/1880 train_time:72548ms step_avg:52.42ms
step:1385/1880 train_time:72636ms step_avg:52.44ms
step:1386/1880 train_time:72724ms step_avg:52.47ms
step:1387/1880 train_time:72812ms step_avg:52.50ms
step:1388/1880 train_time:72900ms step_avg:52.52ms
step:1389/1880 train_time:72988ms step_avg:52.55ms
step:1390/1880 train_time:73075ms step_avg:52.57ms
step:1391/1880 train_time:73164ms step_avg:52.60ms
step:1392/1880 train_time:73252ms step_avg:52.62ms
step:1393/1880 train_time:73340ms step_avg:52.65ms
step:1394/1880 train_time:73427ms step_avg:52.67ms
step:1395/1880 train_time:73515ms step_avg:52.70ms
step:1396/1880 train_time:73603ms step_avg:52.72ms
step:1397/1880 train_time:73692ms step_avg:52.75ms
step:1398/1880 train_time:73780ms step_avg:52.78ms
step:1399/1880 train_time:73868ms step_avg:52.80ms
step:1400/1880 train_time:73955ms step_avg:52.82ms
step:1401/1880 train_time:74043ms step_avg:52.85ms
step:1402/1880 train_time:74131ms step_avg:52.88ms
step:1403/1880 train_time:74218ms step_avg:52.90ms
step:1404/1880 train_time:74306ms step_avg:52.92ms
step:1405/1880 train_time:74395ms step_avg:52.95ms
step:1406/1880 train_time:74483ms step_avg:52.98ms
step:1407/1880 train_time:74571ms step_avg:53.00ms
step:1408/1880 train_time:74659ms step_avg:53.03ms
step:1409/1880 train_time:74747ms step_avg:53.05ms
step:1410/1880 train_time:74835ms step_avg:53.07ms
step:1411/1880 train_time:74922ms step_avg:53.10ms
step:1412/1880 train_time:75010ms step_avg:53.12ms
step:1413/1880 train_time:75098ms step_avg:53.15ms
step:1414/1880 train_time:75187ms step_avg:53.17ms
step:1415/1880 train_time:75275ms step_avg:53.20ms
step:1416/1880 train_time:75363ms step_avg:53.22ms
step:1417/1880 train_time:75451ms step_avg:53.25ms
step:1418/1880 train_time:75539ms step_avg:53.27ms
step:1419/1880 train_time:75626ms step_avg:53.30ms
step:1420/1880 train_time:75714ms step_avg:53.32ms
step:1421/1880 train_time:75801ms step_avg:53.34ms
step:1422/1880 train_time:75891ms step_avg:53.37ms
step:1423/1880 train_time:75979ms step_avg:53.39ms
step:1424/1880 train_time:76067ms step_avg:53.42ms
step:1425/1880 train_time:76155ms step_avg:53.44ms
step:1426/1880 train_time:76243ms step_avg:53.47ms
step:1427/1880 train_time:76331ms step_avg:53.49ms
step:1428/1880 train_time:76419ms step_avg:53.51ms
step:1429/1880 train_time:76507ms step_avg:53.54ms
step:1430/1880 train_time:76595ms step_avg:53.56ms
step:1431/1880 train_time:76684ms step_avg:53.59ms
step:1432/1880 train_time:76771ms step_avg:53.61ms
step:1433/1880 train_time:76859ms step_avg:53.64ms
step:1434/1880 train_time:76949ms step_avg:53.66ms
step:1435/1880 train_time:77037ms step_avg:53.68ms
step:1436/1880 train_time:77125ms step_avg:53.71ms
step:1437/1880 train_time:77213ms step_avg:53.73ms
step:1438/1880 train_time:77301ms step_avg:53.76ms
step:1439/1880 train_time:77389ms step_avg:53.78ms
step:1440/1880 train_time:77476ms step_avg:53.80ms
step:1441/1880 train_time:77565ms step_avg:53.83ms
step:1442/1880 train_time:77653ms step_avg:53.85ms
step:1443/1880 train_time:77740ms step_avg:53.87ms
step:1444/1880 train_time:77828ms step_avg:53.90ms
step:1445/1880 train_time:77917ms step_avg:53.92ms
step:1446/1880 train_time:78004ms step_avg:53.94ms
step:1447/1880 train_time:78093ms step_avg:53.97ms
step:1448/1880 train_time:78180ms step_avg:53.99ms
step:1449/1880 train_time:78268ms step_avg:54.02ms
step:1450/1880 train_time:78356ms step_avg:54.04ms
step:1451/1880 train_time:78444ms step_avg:54.06ms
step:1452/1880 train_time:78532ms step_avg:54.09ms
step:1453/1880 train_time:78631ms step_avg:54.12ms
step:1454/1880 train_time:78709ms step_avg:54.13ms
step:1455/1880 train_time:78797ms step_avg:54.16ms
step:1456/1880 train_time:78884ms step_avg:54.18ms
step:1457/1880 train_time:78973ms step_avg:54.20ms
step:1458/1880 train_time:79061ms step_avg:54.23ms
step:1459/1880 train_time:79150ms step_avg:54.25ms
step:1460/1880 train_time:79237ms step_avg:54.27ms
step:1461/1880 train_time:79325ms step_avg:54.29ms
step:1462/1880 train_time:79412ms step_avg:54.32ms
step:1463/1880 train_time:79500ms step_avg:54.34ms
step:1464/1880 train_time:79590ms step_avg:54.36ms
step:1465/1880 train_time:79678ms step_avg:54.39ms
step:1466/1880 train_time:79766ms step_avg:54.41ms
step:1467/1880 train_time:79854ms step_avg:54.43ms
step:1468/1880 train_time:79943ms step_avg:54.46ms
step:1469/1880 train_time:80031ms step_avg:54.48ms
step:1470/1880 train_time:80119ms step_avg:54.50ms
step:1471/1880 train_time:80207ms step_avg:54.53ms
step:1472/1880 train_time:80295ms step_avg:54.55ms
step:1473/1880 train_time:80384ms step_avg:54.57ms
step:1474/1880 train_time:80471ms step_avg:54.59ms
step:1475/1880 train_time:80560ms step_avg:54.62ms
step:1476/1880 train_time:80649ms step_avg:54.64ms
step:1477/1880 train_time:80736ms step_avg:54.66ms
step:1478/1880 train_time:80824ms step_avg:54.68ms
step:1479/1880 train_time:80913ms step_avg:54.71ms
step:1480/1880 train_time:81001ms step_avg:54.73ms
step:1481/1880 train_time:81090ms step_avg:54.75ms
step:1482/1880 train_time:81178ms step_avg:54.78ms
step:1483/1880 train_time:81266ms step_avg:54.80ms
step:1484/1880 train_time:81353ms step_avg:54.82ms
step:1485/1880 train_time:81442ms step_avg:54.84ms
step:1486/1880 train_time:81530ms step_avg:54.87ms
step:1487/1880 train_time:81619ms step_avg:54.89ms
step:1488/1880 train_time:81706ms step_avg:54.91ms
step:1489/1880 train_time:81794ms step_avg:54.93ms
step:1490/1880 train_time:81882ms step_avg:54.95ms
step:1491/1880 train_time:81971ms step_avg:54.98ms
step:1492/1880 train_time:82060ms step_avg:55.00ms
step:1493/1880 train_time:82149ms step_avg:55.02ms
step:1494/1880 train_time:82236ms step_avg:55.04ms
step:1495/1880 train_time:82326ms step_avg:55.07ms
step:1496/1880 train_time:82413ms step_avg:55.09ms
step:1497/1880 train_time:82501ms step_avg:55.11ms
step:1498/1880 train_time:82591ms step_avg:55.13ms
step:1499/1880 train_time:82680ms step_avg:55.16ms
step:1500/1880 train_time:82766ms step_avg:55.18ms
step:1500/1880 val_loss:3.4089 train_time:82857ms step_avg:55.24ms
step:1501/1880 train_time:82877ms step_avg:55.21ms
step:1502/1880 train_time:82943ms step_avg:55.22ms
step:1503/1880 train_time:83038ms step_avg:55.25ms
step:1504/1880 train_time:83125ms step_avg:55.27ms
step:1505/1880 train_time:83214ms step_avg:55.29ms
step:1506/1880 train_time:83301ms step_avg:55.31ms
step:1507/1880 train_time:83388ms step_avg:55.33ms
step:1508/1880 train_time:83476ms step_avg:55.36ms
step:1509/1880 train_time:83562ms step_avg:55.38ms
step:1510/1880 train_time:83650ms step_avg:55.40ms
step:1511/1880 train_time:83738ms step_avg:55.42ms
step:1512/1880 train_time:83826ms step_avg:55.44ms
step:1513/1880 train_time:83917ms step_avg:55.46ms
step:1514/1880 train_time:84007ms step_avg:55.49ms
step:1515/1880 train_time:84096ms step_avg:55.51ms
step:1516/1880 train_time:84184ms step_avg:55.53ms
step:1517/1880 train_time:84272ms step_avg:55.55ms
step:1518/1880 train_time:84359ms step_avg:55.57ms
step:1519/1880 train_time:84446ms step_avg:55.59ms
step:1520/1880 train_time:84533ms step_avg:55.61ms
step:1521/1880 train_time:84622ms step_avg:55.64ms
step:1522/1880 train_time:84709ms step_avg:55.66ms
step:1523/1880 train_time:84797ms step_avg:55.68ms
step:1524/1880 train_time:84886ms step_avg:55.70ms
step:1525/1880 train_time:84974ms step_avg:55.72ms
step:1526/1880 train_time:85063ms step_avg:55.74ms
step:1527/1880 train_time:85151ms step_avg:55.76ms
step:1528/1880 train_time:85239ms step_avg:55.78ms
step:1529/1880 train_time:85328ms step_avg:55.81ms
step:1530/1880 train_time:85416ms step_avg:55.83ms
step:1531/1880 train_time:85503ms step_avg:55.85ms
step:1532/1880 train_time:85591ms step_avg:55.87ms
step:1533/1880 train_time:85678ms step_avg:55.89ms
step:1534/1880 train_time:85765ms step_avg:55.91ms
step:1535/1880 train_time:85854ms step_avg:55.93ms
step:1536/1880 train_time:85942ms step_avg:55.95ms
step:1537/1880 train_time:86030ms step_avg:55.97ms
step:1538/1880 train_time:86120ms step_avg:55.99ms
step:1539/1880 train_time:86208ms step_avg:56.02ms
step:1540/1880 train_time:86297ms step_avg:56.04ms
step:1541/1880 train_time:86385ms step_avg:56.06ms
step:1542/1880 train_time:86472ms step_avg:56.08ms
step:1543/1880 train_time:86560ms step_avg:56.10ms
step:1544/1880 train_time:86648ms step_avg:56.12ms
step:1545/1880 train_time:86735ms step_avg:56.14ms
step:1546/1880 train_time:86823ms step_avg:56.16ms
step:1547/1880 train_time:86911ms step_avg:56.18ms
step:1548/1880 train_time:87001ms step_avg:56.20ms
step:1549/1880 train_time:87090ms step_avg:56.22ms
step:1550/1880 train_time:87179ms step_avg:56.24ms
step:1551/1880 train_time:87266ms step_avg:56.26ms
step:1552/1880 train_time:87353ms step_avg:56.28ms
step:1553/1880 train_time:87442ms step_avg:56.30ms
step:1554/1880 train_time:87530ms step_avg:56.33ms
step:1555/1880 train_time:87619ms step_avg:56.35ms
step:1556/1880 train_time:87706ms step_avg:56.37ms
step:1557/1880 train_time:87794ms step_avg:56.39ms
step:1558/1880 train_time:87882ms step_avg:56.41ms
step:1559/1880 train_time:87970ms step_avg:56.43ms
step:1560/1880 train_time:88059ms step_avg:56.45ms
step:1561/1880 train_time:88147ms step_avg:56.47ms
step:1562/1880 train_time:88235ms step_avg:56.49ms
step:1563/1880 train_time:88322ms step_avg:56.51ms
step:1564/1880 train_time:88409ms step_avg:56.53ms
step:1565/1880 train_time:88498ms step_avg:56.55ms
step:1566/1880 train_time:88585ms step_avg:56.57ms
step:1567/1880 train_time:88674ms step_avg:56.59ms
step:1568/1880 train_time:88761ms step_avg:56.61ms
step:1569/1880 train_time:88849ms step_avg:56.63ms
step:1570/1880 train_time:88938ms step_avg:56.65ms
step:1571/1880 train_time:89025ms step_avg:56.67ms
step:1572/1880 train_time:89113ms step_avg:56.69ms
step:1573/1880 train_time:89202ms step_avg:56.71ms
step:1574/1880 train_time:89290ms step_avg:56.73ms
step:1575/1880 train_time:89378ms step_avg:56.75ms
step:1576/1880 train_time:89467ms step_avg:56.77ms
step:1577/1880 train_time:89554ms step_avg:56.79ms
step:1578/1880 train_time:89642ms step_avg:56.81ms
step:1579/1880 train_time:89730ms step_avg:56.83ms
step:1580/1880 train_time:89818ms step_avg:56.85ms
step:1581/1880 train_time:89906ms step_avg:56.87ms
step:1582/1880 train_time:89994ms step_avg:56.89ms
step:1583/1880 train_time:90083ms step_avg:56.91ms
step:1584/1880 train_time:90171ms step_avg:56.93ms
step:1585/1880 train_time:90260ms step_avg:56.95ms
step:1586/1880 train_time:90348ms step_avg:56.97ms
step:1587/1880 train_time:90436ms step_avg:56.99ms
step:1588/1880 train_time:90523ms step_avg:57.00ms
step:1589/1880 train_time:90611ms step_avg:57.02ms
step:1590/1880 train_time:90700ms step_avg:57.04ms
step:1591/1880 train_time:90788ms step_avg:57.06ms
step:1592/1880 train_time:90876ms step_avg:57.08ms
step:1593/1880 train_time:90964ms step_avg:57.10ms
step:1594/1880 train_time:91052ms step_avg:57.12ms
step:1595/1880 train_time:91140ms step_avg:57.14ms
step:1596/1880 train_time:91227ms step_avg:57.16ms
step:1597/1880 train_time:91316ms step_avg:57.18ms
step:1598/1880 train_time:91403ms step_avg:57.20ms
step:1599/1880 train_time:91490ms step_avg:57.22ms
step:1600/1880 train_time:91579ms step_avg:57.24ms
step:1601/1880 train_time:91666ms step_avg:57.26ms
step:1602/1880 train_time:91755ms step_avg:57.28ms
step:1603/1880 train_time:91843ms step_avg:57.29ms
step:1604/1880 train_time:91930ms step_avg:57.31ms
step:1605/1880 train_time:92019ms step_avg:57.33ms
step:1606/1880 train_time:92106ms step_avg:57.35ms
step:1607/1880 train_time:92194ms step_avg:57.37ms
step:1608/1880 train_time:92282ms step_avg:57.39ms
step:1609/1880 train_time:92370ms step_avg:57.41ms
step:1610/1880 train_time:92458ms step_avg:57.43ms
step:1611/1880 train_time:92545ms step_avg:57.45ms
step:1612/1880 train_time:92633ms step_avg:57.46ms
step:1613/1880 train_time:92722ms step_avg:57.48ms
step:1614/1880 train_time:92811ms step_avg:57.50ms
step:1615/1880 train_time:92899ms step_avg:57.52ms
step:1616/1880 train_time:92986ms step_avg:57.54ms
step:1617/1880 train_time:93076ms step_avg:57.56ms
step:1618/1880 train_time:93163ms step_avg:57.58ms
step:1619/1880 train_time:93251ms step_avg:57.60ms
step:1620/1880 train_time:93339ms step_avg:57.62ms
step:1621/1880 train_time:93427ms step_avg:57.64ms
step:1622/1880 train_time:93515ms step_avg:57.65ms
step:1623/1880 train_time:93602ms step_avg:57.67ms
step:1624/1880 train_time:93691ms step_avg:57.69ms
step:1625/1880 train_time:93780ms step_avg:57.71ms
step:1626/1880 train_time:93868ms step_avg:57.73ms
step:1627/1880 train_time:93956ms step_avg:57.75ms
step:1628/1880 train_time:94043ms step_avg:57.77ms
step:1629/1880 train_time:94131ms step_avg:57.78ms
step:1630/1880 train_time:94219ms step_avg:57.80ms
step:1631/1880 train_time:94308ms step_avg:57.82ms
step:1632/1880 train_time:94396ms step_avg:57.84ms
step:1633/1880 train_time:94484ms step_avg:57.86ms
step:1634/1880 train_time:94573ms step_avg:57.88ms
step:1635/1880 train_time:94661ms step_avg:57.90ms
step:1636/1880 train_time:94748ms step_avg:57.91ms
step:1637/1880 train_time:94837ms step_avg:57.93ms
step:1638/1880 train_time:94924ms step_avg:57.95ms
step:1639/1880 train_time:95013ms step_avg:57.97ms
step:1640/1880 train_time:95101ms step_avg:57.99ms
step:1641/1880 train_time:95189ms step_avg:58.01ms
step:1642/1880 train_time:95277ms step_avg:58.03ms
step:1643/1880 train_time:95365ms step_avg:58.04ms
step:1644/1880 train_time:95453ms step_avg:58.06ms
step:1645/1880 train_time:95540ms step_avg:58.08ms
step:1646/1880 train_time:95628ms step_avg:58.10ms
step:1647/1880 train_time:95717ms step_avg:58.12ms
step:1648/1880 train_time:95804ms step_avg:58.13ms
step:1649/1880 train_time:95893ms step_avg:58.15ms
step:1650/1880 train_time:95981ms step_avg:58.17ms
step:1651/1880 train_time:96068ms step_avg:58.19ms
step:1652/1880 train_time:96157ms step_avg:58.21ms
step:1653/1880 train_time:96245ms step_avg:58.22ms
step:1654/1880 train_time:96333ms step_avg:58.24ms
step:1655/1880 train_time:96423ms step_avg:58.26ms
step:1656/1880 train_time:96510ms step_avg:58.28ms
step:1657/1880 train_time:96597ms step_avg:58.30ms
step:1658/1880 train_time:96685ms step_avg:58.31ms
step:1659/1880 train_time:96773ms step_avg:58.33ms
step:1660/1880 train_time:96860ms step_avg:58.35ms
step:1661/1880 train_time:96948ms step_avg:58.37ms
step:1662/1880 train_time:97036ms step_avg:58.39ms
step:1663/1880 train_time:97124ms step_avg:58.40ms
step:1664/1880 train_time:97212ms step_avg:58.42ms
step:1665/1880 train_time:97302ms step_avg:58.44ms
step:1666/1880 train_time:97391ms step_avg:58.46ms
step:1667/1880 train_time:97480ms step_avg:58.48ms
step:1668/1880 train_time:97567ms step_avg:58.49ms
step:1669/1880 train_time:97656ms step_avg:58.51ms
step:1670/1880 train_time:97743ms step_avg:58.53ms
step:1671/1880 train_time:97831ms step_avg:58.55ms
step:1672/1880 train_time:97920ms step_avg:58.56ms
step:1673/1880 train_time:98008ms step_avg:58.58ms
step:1674/1880 train_time:98096ms step_avg:58.60ms
step:1675/1880 train_time:98184ms step_avg:58.62ms
step:1676/1880 train_time:98274ms step_avg:58.64ms
step:1677/1880 train_time:98363ms step_avg:58.65ms
step:1678/1880 train_time:98452ms step_avg:58.67ms
step:1679/1880 train_time:98540ms step_avg:58.69ms
step:1680/1880 train_time:98628ms step_avg:58.71ms
step:1681/1880 train_time:98716ms step_avg:58.72ms
step:1682/1880 train_time:98803ms step_avg:58.74ms
step:1683/1880 train_time:98892ms step_avg:58.76ms
step:1684/1880 train_time:98980ms step_avg:58.78ms
step:1685/1880 train_time:99068ms step_avg:58.79ms
step:1686/1880 train_time:99156ms step_avg:58.81ms
step:1687/1880 train_time:99244ms step_avg:58.83ms
step:1688/1880 train_time:99333ms step_avg:58.85ms
step:1689/1880 train_time:99423ms step_avg:58.86ms
step:1690/1880 train_time:99511ms step_avg:58.88ms
step:1691/1880 train_time:99600ms step_avg:58.90ms
step:1692/1880 train_time:99686ms step_avg:58.92ms
step:1693/1880 train_time:99775ms step_avg:58.93ms
step:1694/1880 train_time:99864ms step_avg:58.95ms
step:1695/1880 train_time:99951ms step_avg:58.97ms
step:1696/1880 train_time:100039ms step_avg:58.99ms
step:1697/1880 train_time:100127ms step_avg:59.00ms
step:1698/1880 train_time:100215ms step_avg:59.02ms
step:1699/1880 train_time:100304ms step_avg:59.04ms
step:1700/1880 train_time:100392ms step_avg:59.05ms
step:1701/1880 train_time:100481ms step_avg:59.07ms
step:1702/1880 train_time:100569ms step_avg:59.09ms
step:1703/1880 train_time:100656ms step_avg:59.11ms
step:1704/1880 train_time:100743ms step_avg:59.12ms
step:1705/1880 train_time:100832ms step_avg:59.14ms
step:1706/1880 train_time:100920ms step_avg:59.16ms
step:1707/1880 train_time:101008ms step_avg:59.17ms
step:1708/1880 train_time:101096ms step_avg:59.19ms
step:1709/1880 train_time:101184ms step_avg:59.21ms
step:1710/1880 train_time:101271ms step_avg:59.22ms
step:1711/1880 train_time:101360ms step_avg:59.24ms
step:1712/1880 train_time:101448ms step_avg:59.26ms
step:1713/1880 train_time:101536ms step_avg:59.27ms
step:1714/1880 train_time:101624ms step_avg:59.29ms
step:1715/1880 train_time:101711ms step_avg:59.31ms
step:1716/1880 train_time:101799ms step_avg:59.32ms
step:1717/1880 train_time:101886ms step_avg:59.34ms
step:1718/1880 train_time:101975ms step_avg:59.36ms
step:1719/1880 train_time:102063ms step_avg:59.37ms
step:1720/1880 train_time:102151ms step_avg:59.39ms
step:1721/1880 train_time:102239ms step_avg:59.41ms
step:1722/1880 train_time:102327ms step_avg:59.42ms
step:1723/1880 train_time:102416ms step_avg:59.44ms
step:1724/1880 train_time:102505ms step_avg:59.46ms
step:1725/1880 train_time:102593ms step_avg:59.47ms
step:1726/1880 train_time:102681ms step_avg:59.49ms
step:1727/1880 train_time:102768ms step_avg:59.51ms
step:1728/1880 train_time:102856ms step_avg:59.52ms
step:1729/1880 train_time:102944ms step_avg:59.54ms
step:1730/1880 train_time:103032ms step_avg:59.56ms
step:1731/1880 train_time:103120ms step_avg:59.57ms
step:1732/1880 train_time:103209ms step_avg:59.59ms
step:1733/1880 train_time:103297ms step_avg:59.61ms
step:1734/1880 train_time:103384ms step_avg:59.62ms
step:1735/1880 train_time:103473ms step_avg:59.64ms
step:1736/1880 train_time:103561ms step_avg:59.65ms
step:1737/1880 train_time:103650ms step_avg:59.67ms
step:1738/1880 train_time:103737ms step_avg:59.69ms
step:1739/1880 train_time:103826ms step_avg:59.70ms
step:1740/1880 train_time:103914ms step_avg:59.72ms
step:1741/1880 train_time:104003ms step_avg:59.74ms
step:1742/1880 train_time:104091ms step_avg:59.75ms
step:1743/1880 train_time:104179ms step_avg:59.77ms
step:1744/1880 train_time:104267ms step_avg:59.79ms
step:1745/1880 train_time:104355ms step_avg:59.80ms
step:1746/1880 train_time:104443ms step_avg:59.82ms
step:1747/1880 train_time:104532ms step_avg:59.84ms
step:1748/1880 train_time:104620ms step_avg:59.85ms
step:1749/1880 train_time:104707ms step_avg:59.87ms
step:1750/1880 train_time:104794ms step_avg:59.88ms
step:1750/1880 val_loss:3.3139 train_time:104885ms step_avg:59.93ms
step:1751/1880 train_time:104906ms step_avg:59.91ms
step:1752/1880 train_time:104974ms step_avg:59.92ms
step:1753/1880 train_time:105063ms step_avg:59.93ms
step:1754/1880 train_time:105150ms step_avg:59.95ms
step:1755/1880 train_time:105237ms step_avg:59.96ms
step:1756/1880 train_time:105324ms step_avg:59.98ms
step:1757/1880 train_time:105410ms step_avg:59.99ms
step:1758/1880 train_time:105497ms step_avg:60.01ms
step:1759/1880 train_time:105584ms step_avg:60.03ms
step:1760/1880 train_time:105671ms step_avg:60.04ms
step:1761/1880 train_time:105759ms step_avg:60.06ms
step:1762/1880 train_time:105849ms step_avg:60.07ms
step:1763/1880 train_time:105940ms step_avg:60.09ms
step:1764/1880 train_time:106030ms step_avg:60.11ms
step:1765/1880 train_time:106120ms step_avg:60.12ms
step:1766/1880 train_time:106208ms step_avg:60.14ms
step:1767/1880 train_time:106295ms step_avg:60.16ms
step:1768/1880 train_time:106382ms step_avg:60.17ms
step:1769/1880 train_time:106470ms step_avg:60.19ms
step:1770/1880 train_time:106557ms step_avg:60.20ms
step:1771/1880 train_time:106646ms step_avg:60.22ms
step:1772/1880 train_time:106733ms step_avg:60.23ms
step:1773/1880 train_time:106822ms step_avg:60.25ms
step:1774/1880 train_time:106911ms step_avg:60.27ms
step:1775/1880 train_time:107001ms step_avg:60.28ms
step:1776/1880 train_time:107089ms step_avg:60.30ms
step:1777/1880 train_time:107177ms step_avg:60.31ms
step:1778/1880 train_time:107264ms step_avg:60.33ms
step:1779/1880 train_time:107352ms step_avg:60.34ms
step:1780/1880 train_time:107440ms step_avg:60.36ms
step:1781/1880 train_time:107528ms step_avg:60.38ms
step:1782/1880 train_time:107615ms step_avg:60.39ms
step:1783/1880 train_time:107704ms step_avg:60.41ms
step:1784/1880 train_time:107791ms step_avg:60.42ms
step:1785/1880 train_time:107880ms step_avg:60.44ms
step:1786/1880 train_time:107968ms step_avg:60.45ms
step:1787/1880 train_time:108057ms step_avg:60.47ms
step:1788/1880 train_time:108146ms step_avg:60.48ms
step:1789/1880 train_time:108235ms step_avg:60.50ms
step:1790/1880 train_time:108322ms step_avg:60.51ms
step:1791/1880 train_time:108410ms step_avg:60.53ms
step:1792/1880 train_time:108497ms step_avg:60.55ms
step:1793/1880 train_time:108585ms step_avg:60.56ms
step:1794/1880 train_time:108673ms step_avg:60.58ms
step:1795/1880 train_time:108761ms step_avg:60.59ms
step:1796/1880 train_time:108850ms step_avg:60.61ms
step:1797/1880 train_time:108939ms step_avg:60.62ms
step:1798/1880 train_time:109027ms step_avg:60.64ms
step:1799/1880 train_time:109116ms step_avg:60.65ms
step:1800/1880 train_time:109203ms step_avg:60.67ms
step:1801/1880 train_time:109291ms step_avg:60.68ms
step:1802/1880 train_time:109378ms step_avg:60.70ms
step:1803/1880 train_time:109466ms step_avg:60.71ms
step:1804/1880 train_time:109553ms step_avg:60.73ms
step:1805/1880 train_time:109641ms step_avg:60.74ms
step:1806/1880 train_time:109729ms step_avg:60.76ms
step:1807/1880 train_time:109818ms step_avg:60.77ms
step:1808/1880 train_time:109909ms step_avg:60.79ms
step:1809/1880 train_time:109997ms step_avg:60.81ms
step:1810/1880 train_time:110085ms step_avg:60.82ms
step:1811/1880 train_time:110173ms step_avg:60.84ms
step:1812/1880 train_time:110260ms step_avg:60.85ms
step:1813/1880 train_time:110349ms step_avg:60.87ms
step:1814/1880 train_time:110436ms step_avg:60.88ms
step:1815/1880 train_time:110525ms step_avg:60.90ms
step:1816/1880 train_time:110612ms step_avg:60.91ms
step:1817/1880 train_time:110699ms step_avg:60.92ms
step:1818/1880 train_time:110788ms step_avg:60.94ms
step:1819/1880 train_time:110877ms step_avg:60.96ms
step:1820/1880 train_time:110966ms step_avg:60.97ms
step:1821/1880 train_time:111055ms step_avg:60.99ms
step:1822/1880 train_time:111143ms step_avg:61.00ms
step:1823/1880 train_time:111232ms step_avg:61.02ms
step:1824/1880 train_time:111320ms step_avg:61.03ms
step:1825/1880 train_time:111408ms step_avg:61.05ms
step:1826/1880 train_time:111496ms step_avg:61.06ms
step:1827/1880 train_time:111584ms step_avg:61.07ms
step:1828/1880 train_time:111671ms step_avg:61.09ms
step:1829/1880 train_time:111759ms step_avg:61.10ms
step:1830/1880 train_time:111847ms step_avg:61.12ms
step:1831/1880 train_time:111935ms step_avg:61.13ms
step:1832/1880 train_time:112023ms step_avg:61.15ms
step:1833/1880 train_time:112112ms step_avg:61.16ms
step:1834/1880 train_time:112200ms step_avg:61.18ms
step:1835/1880 train_time:112289ms step_avg:61.19ms
step:1836/1880 train_time:112377ms step_avg:61.21ms
step:1837/1880 train_time:112465ms step_avg:61.22ms
step:1838/1880 train_time:112552ms step_avg:61.24ms
step:1839/1880 train_time:112640ms step_avg:61.25ms
step:1840/1880 train_time:112729ms step_avg:61.27ms
step:1841/1880 train_time:112818ms step_avg:61.28ms
step:1842/1880 train_time:112907ms step_avg:61.30ms
step:1843/1880 train_time:112995ms step_avg:61.31ms
step:1844/1880 train_time:113083ms step_avg:61.32ms
step:1845/1880 train_time:113171ms step_avg:61.34ms
step:1846/1880 train_time:113259ms step_avg:61.35ms
step:1847/1880 train_time:113349ms step_avg:61.37ms
step:1848/1880 train_time:113437ms step_avg:61.38ms
step:1849/1880 train_time:113526ms step_avg:61.40ms
step:1850/1880 train_time:113613ms step_avg:61.41ms
step:1851/1880 train_time:113702ms step_avg:61.43ms
step:1852/1880 train_time:113791ms step_avg:61.44ms
step:1853/1880 train_time:113880ms step_avg:61.46ms
step:1854/1880 train_time:113967ms step_avg:61.47ms
step:1855/1880 train_time:114055ms step_avg:61.49ms
step:1856/1880 train_time:114143ms step_avg:61.50ms
step:1857/1880 train_time:114232ms step_avg:61.51ms
step:1858/1880 train_time:114321ms step_avg:61.53ms
step:1859/1880 train_time:114410ms step_avg:61.54ms
step:1860/1880 train_time:114499ms step_avg:61.56ms
step:1861/1880 train_time:114588ms step_avg:61.57ms
step:1862/1880 train_time:114677ms step_avg:61.59ms
step:1863/1880 train_time:114766ms step_avg:61.60ms
step:1864/1880 train_time:114854ms step_avg:61.62ms
step:1865/1880 train_time:114943ms step_avg:61.63ms
step:1866/1880 train_time:115031ms step_avg:61.65ms
step:1867/1880 train_time:115120ms step_avg:61.66ms
step:1868/1880 train_time:115208ms step_avg:61.67ms
step:1869/1880 train_time:115296ms step_avg:61.69ms
step:1870/1880 train_time:115385ms step_avg:61.70ms
step:1871/1880 train_time:115473ms step_avg:61.72ms
step:1872/1880 train_time:115561ms step_avg:61.73ms
step:1873/1880 train_time:115650ms step_avg:61.75ms
step:1874/1880 train_time:115738ms step_avg:61.76ms
step:1875/1880 train_time:115828ms step_avg:61.77ms
step:1876/1880 train_time:115916ms step_avg:61.79ms
step:1877/1880 train_time:116004ms step_avg:61.80ms
step:1878/1880 train_time:116091ms step_avg:61.82ms
step:1879/1880 train_time:116180ms step_avg:61.83ms
step:1880/1880 train_time:116268ms step_avg:61.84ms
step:1880/1880 val_loss:3.2799 train_time:116359ms step_avg:61.89ms
peak memory allocated: 29709 MiB reserved: 43938 MiB

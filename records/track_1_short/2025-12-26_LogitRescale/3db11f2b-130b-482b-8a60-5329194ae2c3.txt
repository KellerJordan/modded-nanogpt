import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits+5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1840  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec 26 23:11:02 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            129W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     67890      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     67891      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     67892      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     67893      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     67894      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     67895      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     67896      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     67897      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 613, 614, 615, 1226, 1227, 1228, 1839, 1840, 1841] for warmup
Resetting Model
step:0/1880 val_loss:10.8302 train_time:0ms step_avg:0.04ms
step:1/1880 train_time:79ms step_avg:78.51ms
step:2/1880 train_time:102ms step_avg:50.98ms
step:3/1880 train_time:122ms step_avg:40.81ms
step:4/1880 train_time:151ms step_avg:37.82ms
step:5/1880 train_time:185ms step_avg:37.00ms
step:6/1880 train_time:264ms step_avg:44.02ms
step:7/1880 train_time:388ms step_avg:55.48ms
step:8/1880 train_time:422ms step_avg:52.81ms
step:9/1880 train_time:456ms step_avg:50.68ms
step:10/1880 train_time:490ms step_avg:49.01ms
step:11/1880 train_time:524ms step_avg:47.61ms
step:12/1880 train_time:558ms step_avg:46.48ms
step:13/1880 train_time:592ms step_avg:45.51ms
step:14/1880 train_time:626ms step_avg:44.71ms
step:15/1880 train_time:660ms step_avg:43.99ms
step:16/1880 train_time:694ms step_avg:43.39ms
step:17/1880 train_time:728ms step_avg:42.83ms
step:18/1880 train_time:762ms step_avg:42.35ms
step:19/1880 train_time:796ms step_avg:41.90ms
step:20/1880 train_time:830ms step_avg:41.50ms
step:21/1880 train_time:864ms step_avg:41.14ms
step:22/1880 train_time:898ms step_avg:40.83ms
step:23/1880 train_time:932ms step_avg:40.52ms
step:24/1880 train_time:966ms step_avg:40.26ms
step:25/1880 train_time:1000ms step_avg:40.00ms
step:26/1880 train_time:1034ms step_avg:39.78ms
step:27/1880 train_time:1068ms step_avg:39.55ms
step:28/1880 train_time:1102ms step_avg:39.36ms
step:29/1880 train_time:1136ms step_avg:39.17ms
step:30/1880 train_time:1170ms step_avg:39.00ms
step:31/1880 train_time:1204ms step_avg:38.83ms
step:32/1880 train_time:1238ms step_avg:38.68ms
step:33/1880 train_time:1272ms step_avg:38.54ms
step:34/1880 train_time:1306ms step_avg:38.43ms
step:35/1880 train_time:1341ms step_avg:38.31ms
step:36/1880 train_time:1376ms step_avg:38.22ms
step:37/1880 train_time:1410ms step_avg:38.11ms
step:38/1880 train_time:1444ms step_avg:38.01ms
step:39/1880 train_time:1478ms step_avg:37.91ms
step:40/1880 train_time:1512ms step_avg:37.81ms
step:41/1880 train_time:1547ms step_avg:37.72ms
step:42/1880 train_time:1581ms step_avg:37.65ms
step:43/1880 train_time:1615ms step_avg:37.56ms
step:44/1880 train_time:1650ms step_avg:37.49ms
step:45/1880 train_time:1684ms step_avg:37.41ms
step:46/1880 train_time:1718ms step_avg:37.35ms
step:47/1880 train_time:1752ms step_avg:37.28ms
step:48/1880 train_time:1787ms step_avg:37.23ms
step:49/1880 train_time:1821ms step_avg:37.16ms
step:50/1880 train_time:1855ms step_avg:37.10ms
step:51/1880 train_time:1889ms step_avg:37.04ms
step:52/1880 train_time:1923ms step_avg:36.99ms
step:53/1880 train_time:1957ms step_avg:36.93ms
step:54/1880 train_time:1992ms step_avg:36.88ms
step:55/1880 train_time:2026ms step_avg:36.83ms
step:56/1880 train_time:2060ms step_avg:36.79ms
step:57/1880 train_time:2094ms step_avg:36.74ms
step:58/1880 train_time:2128ms step_avg:36.70ms
step:59/1880 train_time:2162ms step_avg:36.65ms
step:60/1880 train_time:2196ms step_avg:36.60ms
step:61/1880 train_time:2230ms step_avg:36.56ms
step:62/1880 train_time:2264ms step_avg:36.52ms
step:63/1880 train_time:2299ms step_avg:36.49ms
step:64/1880 train_time:2334ms step_avg:36.47ms
step:65/1880 train_time:2368ms step_avg:36.44ms
step:66/1880 train_time:2403ms step_avg:36.41ms
step:67/1880 train_time:2437ms step_avg:36.38ms
step:68/1880 train_time:2472ms step_avg:36.35ms
step:69/1880 train_time:2506ms step_avg:36.31ms
step:70/1880 train_time:2540ms step_avg:36.28ms
step:71/1880 train_time:2574ms step_avg:36.25ms
step:72/1880 train_time:2608ms step_avg:36.23ms
step:73/1880 train_time:2643ms step_avg:36.20ms
step:74/1880 train_time:2677ms step_avg:36.17ms
step:75/1880 train_time:2711ms step_avg:36.14ms
step:76/1880 train_time:2745ms step_avg:36.12ms
step:77/1880 train_time:2779ms step_avg:36.09ms
step:78/1880 train_time:2813ms step_avg:36.06ms
step:79/1880 train_time:2847ms step_avg:36.04ms
step:80/1880 train_time:2881ms step_avg:36.02ms
step:81/1880 train_time:2915ms step_avg:35.99ms
step:82/1880 train_time:2949ms step_avg:35.97ms
step:83/1880 train_time:2983ms step_avg:35.94ms
step:84/1880 train_time:3017ms step_avg:35.92ms
step:85/1880 train_time:3051ms step_avg:35.90ms
step:86/1880 train_time:3086ms step_avg:35.88ms
step:87/1880 train_time:3119ms step_avg:35.86ms
step:88/1880 train_time:3153ms step_avg:35.83ms
step:89/1880 train_time:3187ms step_avg:35.81ms
step:90/1880 train_time:3222ms step_avg:35.80ms
step:91/1880 train_time:3256ms step_avg:35.78ms
step:92/1880 train_time:3290ms step_avg:35.76ms
step:93/1880 train_time:3324ms step_avg:35.75ms
step:94/1880 train_time:3359ms step_avg:35.73ms
step:95/1880 train_time:3393ms step_avg:35.72ms
step:96/1880 train_time:3428ms step_avg:35.71ms
step:97/1880 train_time:3462ms step_avg:35.69ms
step:98/1880 train_time:3497ms step_avg:35.68ms
step:99/1880 train_time:3531ms step_avg:35.67ms
step:100/1880 train_time:3565ms step_avg:35.65ms
step:101/1880 train_time:3599ms step_avg:35.64ms
step:102/1880 train_time:3634ms step_avg:35.62ms
step:103/1880 train_time:3668ms step_avg:35.61ms
step:104/1880 train_time:3702ms step_avg:35.60ms
step:105/1880 train_time:3736ms step_avg:35.58ms
step:106/1880 train_time:3770ms step_avg:35.57ms
step:107/1880 train_time:3804ms step_avg:35.55ms
step:108/1880 train_time:3838ms step_avg:35.54ms
step:109/1880 train_time:3872ms step_avg:35.52ms
step:110/1880 train_time:3906ms step_avg:35.51ms
step:111/1880 train_time:3940ms step_avg:35.50ms
step:112/1880 train_time:3974ms step_avg:35.48ms
step:113/1880 train_time:4008ms step_avg:35.47ms
step:114/1880 train_time:4042ms step_avg:35.46ms
step:115/1880 train_time:4076ms step_avg:35.44ms
step:116/1880 train_time:4110ms step_avg:35.43ms
step:117/1880 train_time:4144ms step_avg:35.42ms
step:118/1880 train_time:4178ms step_avg:35.41ms
step:119/1880 train_time:4212ms step_avg:35.40ms
step:120/1880 train_time:4246ms step_avg:35.39ms
step:121/1880 train_time:4280ms step_avg:35.37ms
step:122/1880 train_time:4315ms step_avg:35.37ms
step:123/1880 train_time:4349ms step_avg:35.36ms
step:124/1880 train_time:4383ms step_avg:35.35ms
step:125/1880 train_time:4418ms step_avg:35.34ms
step:126/1880 train_time:4452ms step_avg:35.33ms
step:127/1880 train_time:4486ms step_avg:35.32ms
step:128/1880 train_time:4521ms step_avg:35.32ms
step:129/1880 train_time:4555ms step_avg:35.31ms
step:130/1880 train_time:4589ms step_avg:35.30ms
step:131/1880 train_time:4623ms step_avg:35.29ms
step:132/1880 train_time:4657ms step_avg:35.28ms
step:133/1880 train_time:4691ms step_avg:35.27ms
step:134/1880 train_time:4726ms step_avg:35.27ms
step:135/1880 train_time:4760ms step_avg:35.26ms
step:136/1880 train_time:4794ms step_avg:35.25ms
step:137/1880 train_time:4829ms step_avg:35.24ms
step:138/1880 train_time:4863ms step_avg:35.24ms
step:139/1880 train_time:4897ms step_avg:35.23ms
step:140/1880 train_time:4931ms step_avg:35.22ms
step:141/1880 train_time:4965ms step_avg:35.21ms
step:142/1880 train_time:4999ms step_avg:35.21ms
step:143/1880 train_time:5033ms step_avg:35.20ms
step:144/1880 train_time:5067ms step_avg:35.19ms
step:145/1880 train_time:5101ms step_avg:35.18ms
step:146/1880 train_time:5135ms step_avg:35.17ms
step:147/1880 train_time:5169ms step_avg:35.16ms
step:148/1880 train_time:5203ms step_avg:35.16ms
step:149/1880 train_time:5237ms step_avg:35.15ms
step:150/1880 train_time:5271ms step_avg:35.14ms
step:151/1880 train_time:5305ms step_avg:35.13ms
step:152/1880 train_time:5339ms step_avg:35.13ms
step:153/1880 train_time:5373ms step_avg:35.12ms
step:154/1880 train_time:5407ms step_avg:35.11ms
step:155/1880 train_time:5441ms step_avg:35.10ms
step:156/1880 train_time:5476ms step_avg:35.10ms
step:157/1880 train_time:5510ms step_avg:35.09ms
step:158/1880 train_time:5544ms step_avg:35.09ms
step:159/1880 train_time:5577ms step_avg:35.08ms
step:160/1880 train_time:5612ms step_avg:35.07ms
step:161/1880 train_time:5646ms step_avg:35.07ms
step:162/1880 train_time:5680ms step_avg:35.06ms
step:163/1880 train_time:5714ms step_avg:35.06ms
step:164/1880 train_time:5749ms step_avg:35.05ms
step:165/1880 train_time:5782ms step_avg:35.05ms
step:166/1880 train_time:5817ms step_avg:35.04ms
step:167/1880 train_time:5851ms step_avg:35.03ms
step:168/1880 train_time:5885ms step_avg:35.03ms
step:169/1880 train_time:5919ms step_avg:35.02ms
step:170/1880 train_time:5953ms step_avg:35.02ms
step:171/1880 train_time:5987ms step_avg:35.01ms
step:172/1880 train_time:6022ms step_avg:35.01ms
step:173/1880 train_time:6055ms step_avg:35.00ms
step:174/1880 train_time:6090ms step_avg:35.00ms
step:175/1880 train_time:6124ms step_avg:34.99ms
step:176/1880 train_time:6158ms step_avg:34.99ms
step:177/1880 train_time:6192ms step_avg:34.98ms
step:178/1880 train_time:6226ms step_avg:34.98ms
step:179/1880 train_time:6260ms step_avg:34.97ms
step:180/1880 train_time:6295ms step_avg:34.97ms
step:181/1880 train_time:6329ms step_avg:34.97ms
step:182/1880 train_time:6363ms step_avg:34.96ms
step:183/1880 train_time:6397ms step_avg:34.95ms
step:184/1880 train_time:6431ms step_avg:34.95ms
step:185/1880 train_time:6465ms step_avg:34.94ms
step:186/1880 train_time:6499ms step_avg:34.94ms
step:187/1880 train_time:6533ms step_avg:34.93ms
step:188/1880 train_time:6567ms step_avg:34.93ms
step:189/1880 train_time:6601ms step_avg:34.92ms
step:190/1880 train_time:6635ms step_avg:34.92ms
step:191/1880 train_time:6669ms step_avg:34.92ms
step:192/1880 train_time:6703ms step_avg:34.91ms
step:193/1880 train_time:6737ms step_avg:34.91ms
step:194/1880 train_time:6772ms step_avg:34.91ms
step:195/1880 train_time:6806ms step_avg:34.90ms
step:196/1880 train_time:6840ms step_avg:34.90ms
step:197/1880 train_time:6874ms step_avg:34.89ms
step:198/1880 train_time:6909ms step_avg:34.89ms
step:199/1880 train_time:6943ms step_avg:34.89ms
step:200/1880 train_time:6977ms step_avg:34.88ms
step:201/1880 train_time:7011ms step_avg:34.88ms
step:202/1880 train_time:7045ms step_avg:34.88ms
step:203/1880 train_time:7079ms step_avg:34.87ms
step:204/1880 train_time:7113ms step_avg:34.87ms
step:205/1880 train_time:7147ms step_avg:34.87ms
step:206/1880 train_time:7182ms step_avg:34.86ms
step:207/1880 train_time:7216ms step_avg:34.86ms
step:208/1880 train_time:7250ms step_avg:34.86ms
step:209/1880 train_time:7284ms step_avg:34.85ms
step:210/1880 train_time:7318ms step_avg:34.85ms
step:211/1880 train_time:7352ms step_avg:34.85ms
step:212/1880 train_time:7387ms step_avg:34.84ms
step:213/1880 train_time:7421ms step_avg:34.84ms
step:214/1880 train_time:7455ms step_avg:34.84ms
step:215/1880 train_time:7489ms step_avg:34.83ms
step:216/1880 train_time:7523ms step_avg:34.83ms
step:217/1880 train_time:7557ms step_avg:34.82ms
step:218/1880 train_time:7591ms step_avg:34.82ms
step:219/1880 train_time:7625ms step_avg:34.82ms
step:220/1880 train_time:7659ms step_avg:34.82ms
step:221/1880 train_time:7694ms step_avg:34.81ms
step:222/1880 train_time:7728ms step_avg:34.81ms
step:223/1880 train_time:7762ms step_avg:34.81ms
step:224/1880 train_time:7796ms step_avg:34.81ms
step:225/1880 train_time:7830ms step_avg:34.80ms
step:226/1880 train_time:7865ms step_avg:34.80ms
step:227/1880 train_time:7899ms step_avg:34.80ms
step:228/1880 train_time:7933ms step_avg:34.79ms
step:229/1880 train_time:7967ms step_avg:34.79ms
step:230/1880 train_time:8001ms step_avg:34.79ms
step:231/1880 train_time:8035ms step_avg:34.78ms
step:232/1880 train_time:8069ms step_avg:34.78ms
step:233/1880 train_time:8103ms step_avg:34.78ms
step:234/1880 train_time:8137ms step_avg:34.77ms
step:235/1880 train_time:8171ms step_avg:34.77ms
step:236/1880 train_time:8205ms step_avg:34.77ms
step:237/1880 train_time:8239ms step_avg:34.76ms
step:238/1880 train_time:8273ms step_avg:34.76ms
step:239/1880 train_time:8307ms step_avg:34.76ms
step:240/1880 train_time:8341ms step_avg:34.76ms
step:241/1880 train_time:8375ms step_avg:34.75ms
step:242/1880 train_time:8409ms step_avg:34.75ms
step:243/1880 train_time:8443ms step_avg:34.74ms
step:244/1880 train_time:8477ms step_avg:34.74ms
step:245/1880 train_time:8511ms step_avg:34.74ms
step:246/1880 train_time:8545ms step_avg:34.74ms
step:247/1880 train_time:8579ms step_avg:34.73ms
step:248/1880 train_time:8613ms step_avg:34.73ms
step:249/1880 train_time:8647ms step_avg:34.73ms
step:250/1880 train_time:8681ms step_avg:34.73ms
step:250/1880 val_loss:4.6095 train_time:8718ms step_avg:34.87ms
step:251/1880 train_time:8738ms step_avg:34.81ms
step:252/1880 train_time:8757ms step_avg:34.75ms
step:253/1880 train_time:8786ms step_avg:34.73ms
step:254/1880 train_time:8821ms step_avg:34.73ms
step:255/1880 train_time:8856ms step_avg:34.73ms
step:256/1880 train_time:8891ms step_avg:34.73ms
step:257/1880 train_time:8925ms step_avg:34.73ms
step:258/1880 train_time:8960ms step_avg:34.73ms
step:259/1880 train_time:8994ms step_avg:34.72ms
step:260/1880 train_time:9028ms step_avg:34.72ms
step:261/1880 train_time:9062ms step_avg:34.72ms
step:262/1880 train_time:9096ms step_avg:34.72ms
step:263/1880 train_time:9129ms step_avg:34.71ms
step:264/1880 train_time:9164ms step_avg:34.71ms
step:265/1880 train_time:9197ms step_avg:34.71ms
step:266/1880 train_time:9231ms step_avg:34.70ms
step:267/1880 train_time:9265ms step_avg:34.70ms
step:268/1880 train_time:9299ms step_avg:34.70ms
step:269/1880 train_time:9333ms step_avg:34.70ms
step:270/1880 train_time:9367ms step_avg:34.69ms
step:271/1880 train_time:9401ms step_avg:34.69ms
step:272/1880 train_time:9435ms step_avg:34.69ms
step:273/1880 train_time:9468ms step_avg:34.68ms
step:274/1880 train_time:9502ms step_avg:34.68ms
step:275/1880 train_time:9536ms step_avg:34.68ms
step:276/1880 train_time:9570ms step_avg:34.68ms
step:277/1880 train_time:9604ms step_avg:34.67ms
step:278/1880 train_time:9638ms step_avg:34.67ms
step:279/1880 train_time:9672ms step_avg:34.66ms
step:280/1880 train_time:9706ms step_avg:34.66ms
step:281/1880 train_time:9740ms step_avg:34.66ms
step:282/1880 train_time:9775ms step_avg:34.66ms
step:283/1880 train_time:9809ms step_avg:34.66ms
step:284/1880 train_time:9844ms step_avg:34.66ms
step:285/1880 train_time:9878ms step_avg:34.66ms
step:286/1880 train_time:9913ms step_avg:34.66ms
step:287/1880 train_time:9947ms step_avg:34.66ms
step:288/1880 train_time:9982ms step_avg:34.66ms
step:289/1880 train_time:10016ms step_avg:34.66ms
step:290/1880 train_time:10050ms step_avg:34.66ms
step:291/1880 train_time:10084ms step_avg:34.65ms
step:292/1880 train_time:10119ms step_avg:34.65ms
step:293/1880 train_time:10153ms step_avg:34.65ms
step:294/1880 train_time:10187ms step_avg:34.65ms
step:295/1880 train_time:10221ms step_avg:34.65ms
step:296/1880 train_time:10255ms step_avg:34.64ms
step:297/1880 train_time:10288ms step_avg:34.64ms
step:298/1880 train_time:10322ms step_avg:34.64ms
step:299/1880 train_time:10356ms step_avg:34.64ms
step:300/1880 train_time:10390ms step_avg:34.63ms
step:301/1880 train_time:10424ms step_avg:34.63ms
step:302/1880 train_time:10458ms step_avg:34.63ms
step:303/1880 train_time:10492ms step_avg:34.63ms
step:304/1880 train_time:10526ms step_avg:34.62ms
step:305/1880 train_time:10560ms step_avg:34.62ms
step:306/1880 train_time:10594ms step_avg:34.62ms
step:307/1880 train_time:10627ms step_avg:34.62ms
step:308/1880 train_time:10662ms step_avg:34.62ms
step:309/1880 train_time:10696ms step_avg:34.61ms
step:310/1880 train_time:10730ms step_avg:34.61ms
step:311/1880 train_time:10764ms step_avg:34.61ms
step:312/1880 train_time:10799ms step_avg:34.61ms
step:313/1880 train_time:10833ms step_avg:34.61ms
step:314/1880 train_time:10867ms step_avg:34.61ms
step:315/1880 train_time:10901ms step_avg:34.61ms
step:316/1880 train_time:10935ms step_avg:34.60ms
step:317/1880 train_time:10968ms step_avg:34.60ms
step:318/1880 train_time:11003ms step_avg:34.60ms
step:319/1880 train_time:11037ms step_avg:34.60ms
step:320/1880 train_time:11071ms step_avg:34.60ms
step:321/1880 train_time:11105ms step_avg:34.60ms
step:322/1880 train_time:11140ms step_avg:34.60ms
step:323/1880 train_time:11173ms step_avg:34.59ms
step:324/1880 train_time:11208ms step_avg:34.59ms
step:325/1880 train_time:11241ms step_avg:34.59ms
step:326/1880 train_time:11275ms step_avg:34.59ms
step:327/1880 train_time:11309ms step_avg:34.58ms
step:328/1880 train_time:11344ms step_avg:34.58ms
step:329/1880 train_time:11378ms step_avg:34.58ms
step:330/1880 train_time:11412ms step_avg:34.58ms
step:331/1880 train_time:11445ms step_avg:34.58ms
step:332/1880 train_time:11480ms step_avg:34.58ms
step:333/1880 train_time:11513ms step_avg:34.57ms
step:334/1880 train_time:11547ms step_avg:34.57ms
step:335/1880 train_time:11581ms step_avg:34.57ms
step:336/1880 train_time:11615ms step_avg:34.57ms
step:337/1880 train_time:11649ms step_avg:34.57ms
step:338/1880 train_time:11683ms step_avg:34.57ms
step:339/1880 train_time:11717ms step_avg:34.56ms
step:340/1880 train_time:11752ms step_avg:34.56ms
step:341/1880 train_time:11786ms step_avg:34.56ms
step:342/1880 train_time:11820ms step_avg:34.56ms
step:343/1880 train_time:11854ms step_avg:34.56ms
step:344/1880 train_time:11888ms step_avg:34.56ms
step:345/1880 train_time:11922ms step_avg:34.56ms
step:346/1880 train_time:11957ms step_avg:34.56ms
step:347/1880 train_time:11991ms step_avg:34.55ms
step:348/1880 train_time:12025ms step_avg:34.55ms
step:349/1880 train_time:12059ms step_avg:34.55ms
step:350/1880 train_time:12094ms step_avg:34.55ms
step:351/1880 train_time:12128ms step_avg:34.55ms
step:352/1880 train_time:12162ms step_avg:34.55ms
step:353/1880 train_time:12196ms step_avg:34.55ms
step:354/1880 train_time:12230ms step_avg:34.55ms
step:355/1880 train_time:12264ms step_avg:34.55ms
step:356/1880 train_time:12298ms step_avg:34.54ms
step:357/1880 train_time:12332ms step_avg:34.54ms
step:358/1880 train_time:12366ms step_avg:34.54ms
step:359/1880 train_time:12400ms step_avg:34.54ms
step:360/1880 train_time:12434ms step_avg:34.54ms
step:361/1880 train_time:12468ms step_avg:34.54ms
step:362/1880 train_time:12502ms step_avg:34.54ms
step:363/1880 train_time:12536ms step_avg:34.53ms
step:364/1880 train_time:12570ms step_avg:34.53ms
step:365/1880 train_time:12604ms step_avg:34.53ms
step:366/1880 train_time:12638ms step_avg:34.53ms
step:367/1880 train_time:12672ms step_avg:34.53ms
step:368/1880 train_time:12706ms step_avg:34.53ms
step:369/1880 train_time:12740ms step_avg:34.53ms
step:370/1880 train_time:12774ms step_avg:34.52ms
step:371/1880 train_time:12808ms step_avg:34.52ms
step:372/1880 train_time:12842ms step_avg:34.52ms
step:373/1880 train_time:12876ms step_avg:34.52ms
step:374/1880 train_time:12910ms step_avg:34.52ms
step:375/1880 train_time:12944ms step_avg:34.52ms
step:376/1880 train_time:12978ms step_avg:34.52ms
step:377/1880 train_time:13012ms step_avg:34.51ms
step:378/1880 train_time:13047ms step_avg:34.51ms
step:379/1880 train_time:13081ms step_avg:34.51ms
step:380/1880 train_time:13115ms step_avg:34.51ms
step:381/1880 train_time:13149ms step_avg:34.51ms
step:382/1880 train_time:13183ms step_avg:34.51ms
step:383/1880 train_time:13217ms step_avg:34.51ms
step:384/1880 train_time:13251ms step_avg:34.51ms
step:385/1880 train_time:13285ms step_avg:34.51ms
step:386/1880 train_time:13319ms step_avg:34.51ms
step:387/1880 train_time:13353ms step_avg:34.50ms
step:388/1880 train_time:13387ms step_avg:34.50ms
step:389/1880 train_time:13421ms step_avg:34.50ms
step:390/1880 train_time:13455ms step_avg:34.50ms
step:391/1880 train_time:13489ms step_avg:34.50ms
step:392/1880 train_time:13523ms step_avg:34.50ms
step:393/1880 train_time:13557ms step_avg:34.50ms
step:394/1880 train_time:13591ms step_avg:34.50ms
step:395/1880 train_time:13625ms step_avg:34.49ms
step:396/1880 train_time:13659ms step_avg:34.49ms
step:397/1880 train_time:13693ms step_avg:34.49ms
step:398/1880 train_time:13727ms step_avg:34.49ms
step:399/1880 train_time:13761ms step_avg:34.49ms
step:400/1880 train_time:13795ms step_avg:34.49ms
step:401/1880 train_time:13829ms step_avg:34.49ms
step:402/1880 train_time:13863ms step_avg:34.49ms
step:403/1880 train_time:13897ms step_avg:34.48ms
step:404/1880 train_time:13931ms step_avg:34.48ms
step:405/1880 train_time:13965ms step_avg:34.48ms
step:406/1880 train_time:14000ms step_avg:34.48ms
step:407/1880 train_time:14034ms step_avg:34.48ms
step:408/1880 train_time:14068ms step_avg:34.48ms
step:409/1880 train_time:14102ms step_avg:34.48ms
step:410/1880 train_time:14136ms step_avg:34.48ms
step:411/1880 train_time:14170ms step_avg:34.48ms
step:412/1880 train_time:14204ms step_avg:34.48ms
step:413/1880 train_time:14238ms step_avg:34.47ms
step:414/1880 train_time:14272ms step_avg:34.47ms
step:415/1880 train_time:14306ms step_avg:34.47ms
step:416/1880 train_time:14341ms step_avg:34.47ms
step:417/1880 train_time:14375ms step_avg:34.47ms
step:418/1880 train_time:14409ms step_avg:34.47ms
step:419/1880 train_time:14443ms step_avg:34.47ms
step:420/1880 train_time:14477ms step_avg:34.47ms
step:421/1880 train_time:14511ms step_avg:34.47ms
step:422/1880 train_time:14545ms step_avg:34.47ms
step:423/1880 train_time:14579ms step_avg:34.47ms
step:424/1880 train_time:14613ms step_avg:34.47ms
step:425/1880 train_time:14647ms step_avg:34.46ms
step:426/1880 train_time:14681ms step_avg:34.46ms
step:427/1880 train_time:14715ms step_avg:34.46ms
step:428/1880 train_time:14749ms step_avg:34.46ms
step:429/1880 train_time:14783ms step_avg:34.46ms
step:430/1880 train_time:14817ms step_avg:34.46ms
step:431/1880 train_time:14851ms step_avg:34.46ms
step:432/1880 train_time:14885ms step_avg:34.46ms
step:433/1880 train_time:14919ms step_avg:34.45ms
step:434/1880 train_time:14953ms step_avg:34.45ms
step:435/1880 train_time:14987ms step_avg:34.45ms
step:436/1880 train_time:15021ms step_avg:34.45ms
step:437/1880 train_time:15055ms step_avg:34.45ms
step:438/1880 train_time:15089ms step_avg:34.45ms
step:439/1880 train_time:15123ms step_avg:34.45ms
step:440/1880 train_time:15157ms step_avg:34.45ms
step:441/1880 train_time:15191ms step_avg:34.45ms
step:442/1880 train_time:15226ms step_avg:34.45ms
step:443/1880 train_time:15260ms step_avg:34.45ms
step:444/1880 train_time:15294ms step_avg:34.45ms
step:445/1880 train_time:15328ms step_avg:34.45ms
step:446/1880 train_time:15362ms step_avg:34.45ms
step:447/1880 train_time:15397ms step_avg:34.44ms
step:448/1880 train_time:15431ms step_avg:34.44ms
step:449/1880 train_time:15465ms step_avg:34.44ms
step:450/1880 train_time:15499ms step_avg:34.44ms
step:451/1880 train_time:15533ms step_avg:34.44ms
step:452/1880 train_time:15567ms step_avg:34.44ms
step:453/1880 train_time:15601ms step_avg:34.44ms
step:454/1880 train_time:15635ms step_avg:34.44ms
step:455/1880 train_time:15669ms step_avg:34.44ms
step:456/1880 train_time:15703ms step_avg:34.44ms
step:457/1880 train_time:15737ms step_avg:34.44ms
step:458/1880 train_time:15772ms step_avg:34.44ms
step:459/1880 train_time:15806ms step_avg:34.44ms
step:460/1880 train_time:15840ms step_avg:34.44ms
step:461/1880 train_time:15874ms step_avg:34.43ms
step:462/1880 train_time:15908ms step_avg:34.43ms
step:463/1880 train_time:15942ms step_avg:34.43ms
step:464/1880 train_time:15976ms step_avg:34.43ms
step:465/1880 train_time:16010ms step_avg:34.43ms
step:466/1880 train_time:16044ms step_avg:34.43ms
step:467/1880 train_time:16078ms step_avg:34.43ms
step:468/1880 train_time:16112ms step_avg:34.43ms
step:469/1880 train_time:16147ms step_avg:34.43ms
step:470/1880 train_time:16181ms step_avg:34.43ms
step:471/1880 train_time:16216ms step_avg:34.43ms
step:472/1880 train_time:16250ms step_avg:34.43ms
step:473/1880 train_time:16284ms step_avg:34.43ms
step:474/1880 train_time:16318ms step_avg:34.43ms
step:475/1880 train_time:16352ms step_avg:34.43ms
step:476/1880 train_time:16386ms step_avg:34.43ms
step:477/1880 train_time:16420ms step_avg:34.42ms
step:478/1880 train_time:16454ms step_avg:34.42ms
step:479/1880 train_time:16488ms step_avg:34.42ms
step:480/1880 train_time:16523ms step_avg:34.42ms
step:481/1880 train_time:16557ms step_avg:34.42ms
step:482/1880 train_time:16591ms step_avg:34.42ms
step:483/1880 train_time:16625ms step_avg:34.42ms
step:484/1880 train_time:16659ms step_avg:34.42ms
step:485/1880 train_time:16692ms step_avg:34.42ms
step:486/1880 train_time:16727ms step_avg:34.42ms
step:487/1880 train_time:16760ms step_avg:34.42ms
step:488/1880 train_time:16795ms step_avg:34.42ms
step:489/1880 train_time:16828ms step_avg:34.41ms
step:490/1880 train_time:16862ms step_avg:34.41ms
step:491/1880 train_time:16896ms step_avg:34.41ms
step:492/1880 train_time:16930ms step_avg:34.41ms
step:493/1880 train_time:16964ms step_avg:34.41ms
step:494/1880 train_time:16998ms step_avg:34.41ms
step:495/1880 train_time:17032ms step_avg:34.41ms
step:496/1880 train_time:17066ms step_avg:34.41ms
step:497/1880 train_time:17100ms step_avg:34.41ms
step:498/1880 train_time:17135ms step_avg:34.41ms
step:499/1880 train_time:17168ms step_avg:34.41ms
step:500/1880 train_time:17203ms step_avg:34.41ms
step:500/1880 val_loss:4.2758 train_time:17240ms step_avg:34.48ms
step:501/1880 train_time:17259ms step_avg:34.45ms
step:502/1880 train_time:17280ms step_avg:34.42ms
step:503/1880 train_time:17307ms step_avg:34.41ms
step:504/1880 train_time:17342ms step_avg:34.41ms
step:505/1880 train_time:17377ms step_avg:34.41ms
step:506/1880 train_time:17412ms step_avg:34.41ms
step:507/1880 train_time:17446ms step_avg:34.41ms
step:508/1880 train_time:17481ms step_avg:34.41ms
step:509/1880 train_time:17515ms step_avg:34.41ms
step:510/1880 train_time:17550ms step_avg:34.41ms
step:511/1880 train_time:17584ms step_avg:34.41ms
step:512/1880 train_time:17618ms step_avg:34.41ms
step:513/1880 train_time:17651ms step_avg:34.41ms
step:514/1880 train_time:17685ms step_avg:34.41ms
step:515/1880 train_time:17719ms step_avg:34.41ms
step:516/1880 train_time:17753ms step_avg:34.41ms
step:517/1880 train_time:17787ms step_avg:34.40ms
step:518/1880 train_time:17821ms step_avg:34.40ms
step:519/1880 train_time:17854ms step_avg:34.40ms
step:520/1880 train_time:17889ms step_avg:34.40ms
step:521/1880 train_time:17922ms step_avg:34.40ms
step:522/1880 train_time:17956ms step_avg:34.40ms
step:523/1880 train_time:17990ms step_avg:34.40ms
step:524/1880 train_time:18024ms step_avg:34.40ms
step:525/1880 train_time:18058ms step_avg:34.40ms
step:526/1880 train_time:18092ms step_avg:34.40ms
step:527/1880 train_time:18126ms step_avg:34.39ms
step:528/1880 train_time:18160ms step_avg:34.39ms
step:529/1880 train_time:18194ms step_avg:34.39ms
step:530/1880 train_time:18228ms step_avg:34.39ms
step:531/1880 train_time:18262ms step_avg:34.39ms
step:532/1880 train_time:18296ms step_avg:34.39ms
step:533/1880 train_time:18330ms step_avg:34.39ms
step:534/1880 train_time:18365ms step_avg:34.39ms
step:535/1880 train_time:18399ms step_avg:34.39ms
step:536/1880 train_time:18433ms step_avg:34.39ms
step:537/1880 train_time:18467ms step_avg:34.39ms
step:538/1880 train_time:18502ms step_avg:34.39ms
step:539/1880 train_time:18536ms step_avg:34.39ms
step:540/1880 train_time:18570ms step_avg:34.39ms
step:541/1880 train_time:18604ms step_avg:34.39ms
step:542/1880 train_time:18638ms step_avg:34.39ms
step:543/1880 train_time:18672ms step_avg:34.39ms
step:544/1880 train_time:18706ms step_avg:34.39ms
step:545/1880 train_time:18740ms step_avg:34.39ms
step:546/1880 train_time:18774ms step_avg:34.39ms
step:547/1880 train_time:18808ms step_avg:34.38ms
step:548/1880 train_time:18843ms step_avg:34.38ms
step:549/1880 train_time:18876ms step_avg:34.38ms
step:550/1880 train_time:18910ms step_avg:34.38ms
step:551/1880 train_time:18944ms step_avg:34.38ms
step:552/1880 train_time:18978ms step_avg:34.38ms
step:553/1880 train_time:19012ms step_avg:34.38ms
step:554/1880 train_time:19046ms step_avg:34.38ms
step:555/1880 train_time:19079ms step_avg:34.38ms
step:556/1880 train_time:19114ms step_avg:34.38ms
step:557/1880 train_time:19148ms step_avg:34.38ms
step:558/1880 train_time:19182ms step_avg:34.38ms
step:559/1880 train_time:19216ms step_avg:34.38ms
step:560/1880 train_time:19250ms step_avg:34.38ms
step:561/1880 train_time:19284ms step_avg:34.37ms
step:562/1880 train_time:19318ms step_avg:34.37ms
step:563/1880 train_time:19352ms step_avg:34.37ms
step:564/1880 train_time:19387ms step_avg:34.37ms
step:565/1880 train_time:19421ms step_avg:34.37ms
step:566/1880 train_time:19455ms step_avg:34.37ms
step:567/1880 train_time:19490ms step_avg:34.37ms
step:568/1880 train_time:19524ms step_avg:34.37ms
step:569/1880 train_time:19558ms step_avg:34.37ms
step:570/1880 train_time:19592ms step_avg:34.37ms
step:571/1880 train_time:19626ms step_avg:34.37ms
step:572/1880 train_time:19660ms step_avg:34.37ms
step:573/1880 train_time:19694ms step_avg:34.37ms
step:574/1880 train_time:19728ms step_avg:34.37ms
step:575/1880 train_time:19762ms step_avg:34.37ms
step:576/1880 train_time:19797ms step_avg:34.37ms
step:577/1880 train_time:19831ms step_avg:34.37ms
step:578/1880 train_time:19865ms step_avg:34.37ms
step:579/1880 train_time:19899ms step_avg:34.37ms
step:580/1880 train_time:19933ms step_avg:34.37ms
step:581/1880 train_time:19967ms step_avg:34.37ms
step:582/1880 train_time:20001ms step_avg:34.37ms
step:583/1880 train_time:20035ms step_avg:34.37ms
step:584/1880 train_time:20069ms step_avg:34.37ms
step:585/1880 train_time:20103ms step_avg:34.36ms
step:586/1880 train_time:20137ms step_avg:34.36ms
step:587/1880 train_time:20171ms step_avg:34.36ms
step:588/1880 train_time:20205ms step_avg:34.36ms
step:589/1880 train_time:20239ms step_avg:34.36ms
step:590/1880 train_time:20274ms step_avg:34.36ms
step:591/1880 train_time:20308ms step_avg:34.36ms
step:592/1880 train_time:20342ms step_avg:34.36ms
step:593/1880 train_time:20376ms step_avg:34.36ms
step:594/1880 train_time:20410ms step_avg:34.36ms
step:595/1880 train_time:20444ms step_avg:34.36ms
step:596/1880 train_time:20478ms step_avg:34.36ms
step:597/1880 train_time:20513ms step_avg:34.36ms
step:598/1880 train_time:20547ms step_avg:34.36ms
step:599/1880 train_time:20581ms step_avg:34.36ms
step:600/1880 train_time:20615ms step_avg:34.36ms
step:601/1880 train_time:20649ms step_avg:34.36ms
step:602/1880 train_time:20684ms step_avg:34.36ms
step:603/1880 train_time:20717ms step_avg:34.36ms
step:604/1880 train_time:20751ms step_avg:34.36ms
step:605/1880 train_time:20785ms step_avg:34.36ms
step:606/1880 train_time:20819ms step_avg:34.35ms
step:607/1880 train_time:20853ms step_avg:34.35ms
step:608/1880 train_time:20888ms step_avg:34.35ms
step:609/1880 train_time:20922ms step_avg:34.35ms
step:610/1880 train_time:20956ms step_avg:34.35ms
step:611/1880 train_time:20990ms step_avg:34.35ms
step:612/1880 train_time:21025ms step_avg:34.35ms
step:613/1880 train_time:21058ms step_avg:34.35ms
step:614/1880 train_time:21093ms step_avg:34.35ms
step:615/1880 train_time:21128ms step_avg:34.35ms
step:616/1880 train_time:21188ms step_avg:34.40ms
step:617/1880 train_time:21251ms step_avg:34.44ms
step:618/1880 train_time:21312ms step_avg:34.49ms
step:619/1880 train_time:21374ms step_avg:34.53ms
step:620/1880 train_time:21435ms step_avg:34.57ms
step:621/1880 train_time:21497ms step_avg:34.62ms
step:622/1880 train_time:21560ms step_avg:34.66ms
step:623/1880 train_time:21623ms step_avg:34.71ms
step:624/1880 train_time:21685ms step_avg:34.75ms
step:625/1880 train_time:21746ms step_avg:34.79ms
step:626/1880 train_time:21807ms step_avg:34.84ms
step:627/1880 train_time:21868ms step_avg:34.88ms
step:628/1880 train_time:21930ms step_avg:34.92ms
step:629/1880 train_time:21991ms step_avg:34.96ms
step:630/1880 train_time:22052ms step_avg:35.00ms
step:631/1880 train_time:22114ms step_avg:35.05ms
step:632/1880 train_time:22175ms step_avg:35.09ms
step:633/1880 train_time:22237ms step_avg:35.13ms
step:634/1880 train_time:22299ms step_avg:35.17ms
step:635/1880 train_time:22360ms step_avg:35.21ms
step:636/1880 train_time:22422ms step_avg:35.25ms
step:637/1880 train_time:22483ms step_avg:35.30ms
step:638/1880 train_time:22545ms step_avg:35.34ms
step:639/1880 train_time:22607ms step_avg:35.38ms
step:640/1880 train_time:22668ms step_avg:35.42ms
step:641/1880 train_time:22730ms step_avg:35.46ms
step:642/1880 train_time:22791ms step_avg:35.50ms
step:643/1880 train_time:22853ms step_avg:35.54ms
step:644/1880 train_time:22915ms step_avg:35.58ms
step:645/1880 train_time:22976ms step_avg:35.62ms
step:646/1880 train_time:23038ms step_avg:35.66ms
step:647/1880 train_time:23100ms step_avg:35.70ms
step:648/1880 train_time:23161ms step_avg:35.74ms
step:649/1880 train_time:23223ms step_avg:35.78ms
step:650/1880 train_time:23284ms step_avg:35.82ms
step:651/1880 train_time:23346ms step_avg:35.86ms
step:652/1880 train_time:23407ms step_avg:35.90ms
step:653/1880 train_time:23468ms step_avg:35.94ms
step:654/1880 train_time:23530ms step_avg:35.98ms
step:655/1880 train_time:23592ms step_avg:36.02ms
step:656/1880 train_time:23654ms step_avg:36.06ms
step:657/1880 train_time:23717ms step_avg:36.10ms
step:658/1880 train_time:23779ms step_avg:36.14ms
step:659/1880 train_time:23841ms step_avg:36.18ms
step:660/1880 train_time:23903ms step_avg:36.22ms
step:661/1880 train_time:23965ms step_avg:36.26ms
step:662/1880 train_time:24026ms step_avg:36.29ms
step:663/1880 train_time:24087ms step_avg:36.33ms
step:664/1880 train_time:24148ms step_avg:36.37ms
step:665/1880 train_time:24210ms step_avg:36.41ms
step:666/1880 train_time:24272ms step_avg:36.44ms
step:667/1880 train_time:24334ms step_avg:36.48ms
step:668/1880 train_time:24395ms step_avg:36.52ms
step:669/1880 train_time:24457ms step_avg:36.56ms
step:670/1880 train_time:24519ms step_avg:36.60ms
step:671/1880 train_time:24581ms step_avg:36.63ms
step:672/1880 train_time:24643ms step_avg:36.67ms
step:673/1880 train_time:24705ms step_avg:36.71ms
step:674/1880 train_time:24767ms step_avg:36.75ms
step:675/1880 train_time:24828ms step_avg:36.78ms
step:676/1880 train_time:24889ms step_avg:36.82ms
step:677/1880 train_time:24951ms step_avg:36.86ms
step:678/1880 train_time:25013ms step_avg:36.89ms
step:679/1880 train_time:25075ms step_avg:36.93ms
step:680/1880 train_time:25136ms step_avg:36.96ms
step:681/1880 train_time:25198ms step_avg:37.00ms
step:682/1880 train_time:25260ms step_avg:37.04ms
step:683/1880 train_time:25322ms step_avg:37.07ms
step:684/1880 train_time:25383ms step_avg:37.11ms
step:685/1880 train_time:25445ms step_avg:37.15ms
step:686/1880 train_time:25507ms step_avg:37.18ms
step:687/1880 train_time:25568ms step_avg:37.22ms
step:688/1880 train_time:25630ms step_avg:37.25ms
step:689/1880 train_time:25692ms step_avg:37.29ms
step:690/1880 train_time:25753ms step_avg:37.32ms
step:691/1880 train_time:25815ms step_avg:37.36ms
step:692/1880 train_time:25878ms step_avg:37.40ms
step:693/1880 train_time:25940ms step_avg:37.43ms
step:694/1880 train_time:26002ms step_avg:37.47ms
step:695/1880 train_time:26063ms step_avg:37.50ms
step:696/1880 train_time:26125ms step_avg:37.54ms
step:697/1880 train_time:26186ms step_avg:37.57ms
step:698/1880 train_time:26248ms step_avg:37.60ms
step:699/1880 train_time:26310ms step_avg:37.64ms
step:700/1880 train_time:26371ms step_avg:37.67ms
step:701/1880 train_time:26433ms step_avg:37.71ms
step:702/1880 train_time:26495ms step_avg:37.74ms
step:703/1880 train_time:26558ms step_avg:37.78ms
step:704/1880 train_time:26619ms step_avg:37.81ms
step:705/1880 train_time:26681ms step_avg:37.84ms
step:706/1880 train_time:26742ms step_avg:37.88ms
step:707/1880 train_time:26803ms step_avg:37.91ms
step:708/1880 train_time:26864ms step_avg:37.94ms
step:709/1880 train_time:26926ms step_avg:37.98ms
step:710/1880 train_time:26987ms step_avg:38.01ms
step:711/1880 train_time:27049ms step_avg:38.04ms
step:712/1880 train_time:27110ms step_avg:38.08ms
step:713/1880 train_time:27171ms step_avg:38.11ms
step:714/1880 train_time:27233ms step_avg:38.14ms
step:715/1880 train_time:27295ms step_avg:38.17ms
step:716/1880 train_time:27356ms step_avg:38.21ms
step:717/1880 train_time:27418ms step_avg:38.24ms
step:718/1880 train_time:27480ms step_avg:38.27ms
step:719/1880 train_time:27542ms step_avg:38.31ms
step:720/1880 train_time:27603ms step_avg:38.34ms
step:721/1880 train_time:27666ms step_avg:38.37ms
step:722/1880 train_time:27727ms step_avg:38.40ms
step:723/1880 train_time:27788ms step_avg:38.43ms
step:724/1880 train_time:27849ms step_avg:38.47ms
step:725/1880 train_time:27911ms step_avg:38.50ms
step:726/1880 train_time:27972ms step_avg:38.53ms
step:727/1880 train_time:28033ms step_avg:38.56ms
step:728/1880 train_time:28095ms step_avg:38.59ms
step:729/1880 train_time:28157ms step_avg:38.62ms
step:730/1880 train_time:28220ms step_avg:38.66ms
step:731/1880 train_time:28282ms step_avg:38.69ms
step:732/1880 train_time:28344ms step_avg:38.72ms
step:733/1880 train_time:28406ms step_avg:38.75ms
step:734/1880 train_time:28467ms step_avg:38.78ms
step:735/1880 train_time:28529ms step_avg:38.82ms
step:736/1880 train_time:28590ms step_avg:38.85ms
step:737/1880 train_time:28653ms step_avg:38.88ms
step:738/1880 train_time:28714ms step_avg:38.91ms
step:739/1880 train_time:28776ms step_avg:38.94ms
step:740/1880 train_time:28837ms step_avg:38.97ms
step:741/1880 train_time:28900ms step_avg:39.00ms
step:742/1880 train_time:28961ms step_avg:39.03ms
step:743/1880 train_time:29022ms step_avg:39.06ms
step:744/1880 train_time:29083ms step_avg:39.09ms
step:745/1880 train_time:29145ms step_avg:39.12ms
step:746/1880 train_time:29206ms step_avg:39.15ms
step:747/1880 train_time:29268ms step_avg:39.18ms
step:748/1880 train_time:29329ms step_avg:39.21ms
step:749/1880 train_time:29391ms step_avg:39.24ms
step:750/1880 train_time:29453ms step_avg:39.27ms
step:750/1880 val_loss:4.0254 train_time:29517ms step_avg:39.36ms
step:751/1880 train_time:29537ms step_avg:39.33ms
step:752/1880 train_time:29578ms step_avg:39.33ms
step:753/1880 train_time:29642ms step_avg:39.37ms
step:754/1880 train_time:29706ms step_avg:39.40ms
step:755/1880 train_time:29768ms step_avg:39.43ms
step:756/1880 train_time:29829ms step_avg:39.46ms
step:757/1880 train_time:29890ms step_avg:39.49ms
step:758/1880 train_time:29951ms step_avg:39.51ms
step:759/1880 train_time:30013ms step_avg:39.54ms
step:760/1880 train_time:30074ms step_avg:39.57ms
step:761/1880 train_time:30135ms step_avg:39.60ms
step:762/1880 train_time:30196ms step_avg:39.63ms
step:763/1880 train_time:30257ms step_avg:39.66ms
step:764/1880 train_time:30318ms step_avg:39.68ms
step:765/1880 train_time:30379ms step_avg:39.71ms
step:766/1880 train_time:30441ms step_avg:39.74ms
step:767/1880 train_time:30504ms step_avg:39.77ms
step:768/1880 train_time:30566ms step_avg:39.80ms
step:769/1880 train_time:30628ms step_avg:39.83ms
step:770/1880 train_time:30690ms step_avg:39.86ms
step:771/1880 train_time:30752ms step_avg:39.89ms
step:772/1880 train_time:30814ms step_avg:39.91ms
step:773/1880 train_time:30876ms step_avg:39.94ms
step:774/1880 train_time:30937ms step_avg:39.97ms
step:775/1880 train_time:30998ms step_avg:40.00ms
step:776/1880 train_time:31060ms step_avg:40.03ms
step:777/1880 train_time:31121ms step_avg:40.05ms
step:778/1880 train_time:31182ms step_avg:40.08ms
step:779/1880 train_time:31244ms step_avg:40.11ms
step:780/1880 train_time:31304ms step_avg:40.13ms
step:781/1880 train_time:31366ms step_avg:40.16ms
step:782/1880 train_time:31427ms step_avg:40.19ms
step:783/1880 train_time:31489ms step_avg:40.22ms
step:784/1880 train_time:31550ms step_avg:40.24ms
step:785/1880 train_time:31613ms step_avg:40.27ms
step:786/1880 train_time:31675ms step_avg:40.30ms
step:787/1880 train_time:31737ms step_avg:40.33ms
step:788/1880 train_time:31799ms step_avg:40.35ms
step:789/1880 train_time:31862ms step_avg:40.38ms
step:790/1880 train_time:31923ms step_avg:40.41ms
step:791/1880 train_time:31984ms step_avg:40.43ms
step:792/1880 train_time:32044ms step_avg:40.46ms
step:793/1880 train_time:32106ms step_avg:40.49ms
step:794/1880 train_time:32167ms step_avg:40.51ms
step:795/1880 train_time:32228ms step_avg:40.54ms
step:796/1880 train_time:32289ms step_avg:40.56ms
step:797/1880 train_time:32351ms step_avg:40.59ms
step:798/1880 train_time:32412ms step_avg:40.62ms
step:799/1880 train_time:32474ms step_avg:40.64ms
step:800/1880 train_time:32535ms step_avg:40.67ms
step:801/1880 train_time:32598ms step_avg:40.70ms
step:802/1880 train_time:32660ms step_avg:40.72ms
step:803/1880 train_time:32722ms step_avg:40.75ms
step:804/1880 train_time:32784ms step_avg:40.78ms
step:805/1880 train_time:32846ms step_avg:40.80ms
step:806/1880 train_time:32907ms step_avg:40.83ms
step:807/1880 train_time:32968ms step_avg:40.85ms
step:808/1880 train_time:33029ms step_avg:40.88ms
step:809/1880 train_time:33090ms step_avg:40.90ms
step:810/1880 train_time:33151ms step_avg:40.93ms
step:811/1880 train_time:33213ms step_avg:40.95ms
step:812/1880 train_time:33274ms step_avg:40.98ms
step:813/1880 train_time:33335ms step_avg:41.00ms
step:814/1880 train_time:33396ms step_avg:41.03ms
step:815/1880 train_time:33459ms step_avg:41.05ms
step:816/1880 train_time:33521ms step_avg:41.08ms
step:817/1880 train_time:33582ms step_avg:41.10ms
step:818/1880 train_time:33644ms step_avg:41.13ms
step:819/1880 train_time:33706ms step_avg:41.15ms
step:820/1880 train_time:33768ms step_avg:41.18ms
step:821/1880 train_time:33830ms step_avg:41.21ms
step:822/1880 train_time:33890ms step_avg:41.23ms
step:823/1880 train_time:33952ms step_avg:41.25ms
step:824/1880 train_time:34013ms step_avg:41.28ms
step:825/1880 train_time:34075ms step_avg:41.30ms
step:826/1880 train_time:34136ms step_avg:41.33ms
step:827/1880 train_time:34198ms step_avg:41.35ms
step:828/1880 train_time:34260ms step_avg:41.38ms
step:829/1880 train_time:34321ms step_avg:41.40ms
step:830/1880 train_time:34383ms step_avg:41.42ms
step:831/1880 train_time:34444ms step_avg:41.45ms
step:832/1880 train_time:34505ms step_avg:41.47ms
step:833/1880 train_time:34567ms step_avg:41.50ms
step:834/1880 train_time:34628ms step_avg:41.52ms
step:835/1880 train_time:34691ms step_avg:41.55ms
step:836/1880 train_time:34752ms step_avg:41.57ms
step:837/1880 train_time:34814ms step_avg:41.59ms
step:838/1880 train_time:34876ms step_avg:41.62ms
step:839/1880 train_time:34937ms step_avg:41.64ms
step:840/1880 train_time:35000ms step_avg:41.67ms
step:841/1880 train_time:35062ms step_avg:41.69ms
step:842/1880 train_time:35123ms step_avg:41.71ms
step:843/1880 train_time:35184ms step_avg:41.74ms
step:844/1880 train_time:35246ms step_avg:41.76ms
step:845/1880 train_time:35307ms step_avg:41.78ms
step:846/1880 train_time:35369ms step_avg:41.81ms
step:847/1880 train_time:35431ms step_avg:41.83ms
step:848/1880 train_time:35492ms step_avg:41.85ms
step:849/1880 train_time:35554ms step_avg:41.88ms
step:850/1880 train_time:35616ms step_avg:41.90ms
step:851/1880 train_time:35677ms step_avg:41.92ms
step:852/1880 train_time:35739ms step_avg:41.95ms
step:853/1880 train_time:35802ms step_avg:41.97ms
step:854/1880 train_time:35863ms step_avg:41.99ms
step:855/1880 train_time:35925ms step_avg:42.02ms
step:856/1880 train_time:35986ms step_avg:42.04ms
step:857/1880 train_time:36048ms step_avg:42.06ms
step:858/1880 train_time:36109ms step_avg:42.08ms
step:859/1880 train_time:36171ms step_avg:42.11ms
step:860/1880 train_time:36232ms step_avg:42.13ms
step:861/1880 train_time:36293ms step_avg:42.15ms
step:862/1880 train_time:36354ms step_avg:42.17ms
step:863/1880 train_time:36416ms step_avg:42.20ms
step:864/1880 train_time:36479ms step_avg:42.22ms
step:865/1880 train_time:36540ms step_avg:42.24ms
step:866/1880 train_time:36602ms step_avg:42.27ms
step:867/1880 train_time:36663ms step_avg:42.29ms
step:868/1880 train_time:36724ms step_avg:42.31ms
step:869/1880 train_time:36786ms step_avg:42.33ms
step:870/1880 train_time:36847ms step_avg:42.35ms
step:871/1880 train_time:36910ms step_avg:42.38ms
step:872/1880 train_time:36972ms step_avg:42.40ms
step:873/1880 train_time:37033ms step_avg:42.42ms
step:874/1880 train_time:37095ms step_avg:42.44ms
step:875/1880 train_time:37156ms step_avg:42.46ms
step:876/1880 train_time:37218ms step_avg:42.49ms
step:877/1880 train_time:37279ms step_avg:42.51ms
step:878/1880 train_time:37341ms step_avg:42.53ms
step:879/1880 train_time:37402ms step_avg:42.55ms
step:880/1880 train_time:37464ms step_avg:42.57ms
step:881/1880 train_time:37526ms step_avg:42.59ms
step:882/1880 train_time:37587ms step_avg:42.62ms
step:883/1880 train_time:37649ms step_avg:42.64ms
step:884/1880 train_time:37710ms step_avg:42.66ms
step:885/1880 train_time:37771ms step_avg:42.68ms
step:886/1880 train_time:37832ms step_avg:42.70ms
step:887/1880 train_time:37895ms step_avg:42.72ms
step:888/1880 train_time:37956ms step_avg:42.74ms
step:889/1880 train_time:38019ms step_avg:42.77ms
step:890/1880 train_time:38080ms step_avg:42.79ms
step:891/1880 train_time:38142ms step_avg:42.81ms
step:892/1880 train_time:38203ms step_avg:42.83ms
step:893/1880 train_time:38265ms step_avg:42.85ms
step:894/1880 train_time:38326ms step_avg:42.87ms
step:895/1880 train_time:38388ms step_avg:42.89ms
step:896/1880 train_time:38449ms step_avg:42.91ms
step:897/1880 train_time:38511ms step_avg:42.93ms
step:898/1880 train_time:38572ms step_avg:42.95ms
step:899/1880 train_time:38634ms step_avg:42.97ms
step:900/1880 train_time:38695ms step_avg:42.99ms
step:901/1880 train_time:38758ms step_avg:43.02ms
step:902/1880 train_time:38820ms step_avg:43.04ms
step:903/1880 train_time:38881ms step_avg:43.06ms
step:904/1880 train_time:38943ms step_avg:43.08ms
step:905/1880 train_time:39005ms step_avg:43.10ms
step:906/1880 train_time:39066ms step_avg:43.12ms
step:907/1880 train_time:39128ms step_avg:43.14ms
step:908/1880 train_time:39189ms step_avg:43.16ms
step:909/1880 train_time:39251ms step_avg:43.18ms
step:910/1880 train_time:39312ms step_avg:43.20ms
step:911/1880 train_time:39374ms step_avg:43.22ms
step:912/1880 train_time:39435ms step_avg:43.24ms
step:913/1880 train_time:39497ms step_avg:43.26ms
step:914/1880 train_time:39558ms step_avg:43.28ms
step:915/1880 train_time:39620ms step_avg:43.30ms
step:916/1880 train_time:39682ms step_avg:43.32ms
step:917/1880 train_time:39744ms step_avg:43.34ms
step:918/1880 train_time:39805ms step_avg:43.36ms
step:919/1880 train_time:39867ms step_avg:43.38ms
step:920/1880 train_time:39927ms step_avg:43.40ms
step:921/1880 train_time:39989ms step_avg:43.42ms
step:922/1880 train_time:40050ms step_avg:43.44ms
step:923/1880 train_time:40112ms step_avg:43.46ms
step:924/1880 train_time:40174ms step_avg:43.48ms
step:925/1880 train_time:40236ms step_avg:43.50ms
step:926/1880 train_time:40299ms step_avg:43.52ms
step:927/1880 train_time:40361ms step_avg:43.54ms
step:928/1880 train_time:40423ms step_avg:43.56ms
step:929/1880 train_time:40484ms step_avg:43.58ms
step:930/1880 train_time:40544ms step_avg:43.60ms
step:931/1880 train_time:40606ms step_avg:43.62ms
step:932/1880 train_time:40668ms step_avg:43.64ms
step:933/1880 train_time:40729ms step_avg:43.65ms
step:934/1880 train_time:40790ms step_avg:43.67ms
step:935/1880 train_time:40852ms step_avg:43.69ms
step:936/1880 train_time:40913ms step_avg:43.71ms
step:937/1880 train_time:40975ms step_avg:43.73ms
step:938/1880 train_time:41036ms step_avg:43.75ms
step:939/1880 train_time:41098ms step_avg:43.77ms
step:940/1880 train_time:41161ms step_avg:43.79ms
step:941/1880 train_time:41223ms step_avg:43.81ms
step:942/1880 train_time:41284ms step_avg:43.83ms
step:943/1880 train_time:41346ms step_avg:43.84ms
step:944/1880 train_time:41407ms step_avg:43.86ms
step:945/1880 train_time:41468ms step_avg:43.88ms
step:946/1880 train_time:41529ms step_avg:43.90ms
step:947/1880 train_time:41591ms step_avg:43.92ms
step:948/1880 train_time:41652ms step_avg:43.94ms
step:949/1880 train_time:41714ms step_avg:43.96ms
step:950/1880 train_time:41775ms step_avg:43.97ms
step:951/1880 train_time:41838ms step_avg:43.99ms
step:952/1880 train_time:41900ms step_avg:44.01ms
step:953/1880 train_time:41961ms step_avg:44.03ms
step:954/1880 train_time:42022ms step_avg:44.05ms
step:955/1880 train_time:42084ms step_avg:44.07ms
step:956/1880 train_time:42145ms step_avg:44.09ms
step:957/1880 train_time:42207ms step_avg:44.10ms
step:958/1880 train_time:42268ms step_avg:44.12ms
step:959/1880 train_time:42330ms step_avg:44.14ms
step:960/1880 train_time:42391ms step_avg:44.16ms
step:961/1880 train_time:42453ms step_avg:44.18ms
step:962/1880 train_time:42516ms step_avg:44.20ms
step:963/1880 train_time:42578ms step_avg:44.21ms
step:964/1880 train_time:42639ms step_avg:44.23ms
step:965/1880 train_time:42701ms step_avg:44.25ms
step:966/1880 train_time:42762ms step_avg:44.27ms
step:967/1880 train_time:42824ms step_avg:44.29ms
step:968/1880 train_time:42885ms step_avg:44.30ms
step:969/1880 train_time:42946ms step_avg:44.32ms
step:970/1880 train_time:43008ms step_avg:44.34ms
step:971/1880 train_time:43069ms step_avg:44.36ms
step:972/1880 train_time:43131ms step_avg:44.37ms
step:973/1880 train_time:43193ms step_avg:44.39ms
step:974/1880 train_time:43254ms step_avg:44.41ms
step:975/1880 train_time:43317ms step_avg:44.43ms
step:976/1880 train_time:43379ms step_avg:44.45ms
step:977/1880 train_time:43441ms step_avg:44.46ms
step:978/1880 train_time:43503ms step_avg:44.48ms
step:979/1880 train_time:43565ms step_avg:44.50ms
step:980/1880 train_time:43625ms step_avg:44.52ms
step:981/1880 train_time:43687ms step_avg:44.53ms
step:982/1880 train_time:43749ms step_avg:44.55ms
step:983/1880 train_time:43810ms step_avg:44.57ms
step:984/1880 train_time:43871ms step_avg:44.58ms
step:985/1880 train_time:43933ms step_avg:44.60ms
step:986/1880 train_time:43995ms step_avg:44.62ms
step:987/1880 train_time:44056ms step_avg:44.64ms
step:988/1880 train_time:44118ms step_avg:44.65ms
step:989/1880 train_time:44180ms step_avg:44.67ms
step:990/1880 train_time:44242ms step_avg:44.69ms
step:991/1880 train_time:44304ms step_avg:44.71ms
step:992/1880 train_time:44365ms step_avg:44.72ms
step:993/1880 train_time:44428ms step_avg:44.74ms
step:994/1880 train_time:44489ms step_avg:44.76ms
step:995/1880 train_time:44550ms step_avg:44.77ms
step:996/1880 train_time:44612ms step_avg:44.79ms
step:997/1880 train_time:44673ms step_avg:44.81ms
step:998/1880 train_time:44735ms step_avg:44.82ms
step:999/1880 train_time:44797ms step_avg:44.84ms
step:1000/1880 train_time:44858ms step_avg:44.86ms
step:1000/1880 val_loss:3.7680 train_time:44923ms step_avg:44.92ms
step:1001/1880 train_time:44945ms step_avg:44.90ms
step:1002/1880 train_time:44984ms step_avg:44.89ms
step:1003/1880 train_time:45049ms step_avg:44.91ms
step:1004/1880 train_time:45111ms step_avg:44.93ms
step:1005/1880 train_time:45172ms step_avg:44.95ms
step:1006/1880 train_time:45235ms step_avg:44.96ms
step:1007/1880 train_time:45296ms step_avg:44.98ms
step:1008/1880 train_time:45357ms step_avg:45.00ms
step:1009/1880 train_time:45418ms step_avg:45.01ms
step:1010/1880 train_time:45479ms step_avg:45.03ms
step:1011/1880 train_time:45540ms step_avg:45.04ms
step:1012/1880 train_time:45601ms step_avg:45.06ms
step:1013/1880 train_time:45662ms step_avg:45.08ms
step:1014/1880 train_time:45723ms step_avg:45.09ms
step:1015/1880 train_time:45785ms step_avg:45.11ms
step:1016/1880 train_time:45846ms step_avg:45.12ms
step:1017/1880 train_time:45909ms step_avg:45.14ms
step:1018/1880 train_time:45971ms step_avg:45.16ms
step:1019/1880 train_time:46034ms step_avg:45.18ms
step:1020/1880 train_time:46096ms step_avg:45.19ms
step:1021/1880 train_time:46158ms step_avg:45.21ms
step:1022/1880 train_time:46220ms step_avg:45.23ms
step:1023/1880 train_time:46282ms step_avg:45.24ms
step:1024/1880 train_time:46343ms step_avg:45.26ms
step:1025/1880 train_time:46404ms step_avg:45.27ms
step:1026/1880 train_time:46464ms step_avg:45.29ms
step:1027/1880 train_time:46525ms step_avg:45.30ms
step:1028/1880 train_time:46586ms step_avg:45.32ms
step:1029/1880 train_time:46647ms step_avg:45.33ms
step:1030/1880 train_time:46708ms step_avg:45.35ms
step:1031/1880 train_time:46770ms step_avg:45.36ms
step:1032/1880 train_time:46831ms step_avg:45.38ms
step:1033/1880 train_time:46894ms step_avg:45.40ms
step:1034/1880 train_time:46956ms step_avg:45.41ms
step:1035/1880 train_time:47019ms step_avg:45.43ms
step:1036/1880 train_time:47080ms step_avg:45.44ms
step:1037/1880 train_time:47143ms step_avg:45.46ms
step:1038/1880 train_time:47204ms step_avg:45.48ms
step:1039/1880 train_time:47266ms step_avg:45.49ms
step:1040/1880 train_time:47327ms step_avg:45.51ms
step:1041/1880 train_time:47388ms step_avg:45.52ms
step:1042/1880 train_time:47449ms step_avg:45.54ms
step:1043/1880 train_time:47511ms step_avg:45.55ms
step:1044/1880 train_time:47572ms step_avg:45.57ms
step:1045/1880 train_time:47633ms step_avg:45.58ms
step:1046/1880 train_time:47693ms step_avg:45.60ms
step:1047/1880 train_time:47755ms step_avg:45.61ms
step:1048/1880 train_time:47817ms step_avg:45.63ms
step:1049/1880 train_time:47879ms step_avg:45.64ms
step:1050/1880 train_time:47941ms step_avg:45.66ms
step:1051/1880 train_time:48004ms step_avg:45.67ms
step:1052/1880 train_time:48065ms step_avg:45.69ms
step:1053/1880 train_time:48127ms step_avg:45.70ms
step:1054/1880 train_time:48188ms step_avg:45.72ms
step:1055/1880 train_time:48250ms step_avg:45.73ms
step:1056/1880 train_time:48312ms step_avg:45.75ms
step:1057/1880 train_time:48374ms step_avg:45.77ms
step:1058/1880 train_time:48435ms step_avg:45.78ms
step:1059/1880 train_time:48497ms step_avg:45.79ms
step:1060/1880 train_time:48558ms step_avg:45.81ms
step:1061/1880 train_time:48619ms step_avg:45.82ms
step:1062/1880 train_time:48680ms step_avg:45.84ms
step:1063/1880 train_time:48742ms step_avg:45.85ms
step:1064/1880 train_time:48803ms step_avg:45.87ms
step:1065/1880 train_time:48865ms step_avg:45.88ms
step:1066/1880 train_time:48926ms step_avg:45.90ms
step:1067/1880 train_time:48988ms step_avg:45.91ms
step:1068/1880 train_time:49049ms step_avg:45.93ms
step:1069/1880 train_time:49111ms step_avg:45.94ms
step:1070/1880 train_time:49172ms step_avg:45.95ms
step:1071/1880 train_time:49233ms step_avg:45.97ms
step:1072/1880 train_time:49295ms step_avg:45.98ms
step:1073/1880 train_time:49357ms step_avg:46.00ms
step:1074/1880 train_time:49418ms step_avg:46.01ms
step:1075/1880 train_time:49480ms step_avg:46.03ms
step:1076/1880 train_time:49542ms step_avg:46.04ms
step:1077/1880 train_time:49603ms step_avg:46.06ms
step:1078/1880 train_time:49664ms step_avg:46.07ms
step:1079/1880 train_time:49725ms step_avg:46.08ms
step:1080/1880 train_time:49786ms step_avg:46.10ms
step:1081/1880 train_time:49847ms step_avg:46.11ms
step:1082/1880 train_time:49909ms step_avg:46.13ms
step:1083/1880 train_time:49970ms step_avg:46.14ms
step:1084/1880 train_time:50031ms step_avg:46.15ms
step:1085/1880 train_time:50093ms step_avg:46.17ms
step:1086/1880 train_time:50154ms step_avg:46.18ms
step:1087/1880 train_time:50216ms step_avg:46.20ms
step:1088/1880 train_time:50278ms step_avg:46.21ms
step:1089/1880 train_time:50340ms step_avg:46.23ms
step:1090/1880 train_time:50402ms step_avg:46.24ms
step:1091/1880 train_time:50463ms step_avg:46.25ms
step:1092/1880 train_time:50524ms step_avg:46.27ms
step:1093/1880 train_time:50586ms step_avg:46.28ms
step:1094/1880 train_time:50647ms step_avg:46.30ms
step:1095/1880 train_time:50708ms step_avg:46.31ms
step:1096/1880 train_time:50769ms step_avg:46.32ms
step:1097/1880 train_time:50831ms step_avg:46.34ms
step:1098/1880 train_time:50892ms step_avg:46.35ms
step:1099/1880 train_time:50954ms step_avg:46.36ms
step:1100/1880 train_time:51015ms step_avg:46.38ms
step:1101/1880 train_time:51077ms step_avg:46.39ms
step:1102/1880 train_time:51139ms step_avg:46.41ms
step:1103/1880 train_time:51201ms step_avg:46.42ms
step:1104/1880 train_time:51262ms step_avg:46.43ms
step:1105/1880 train_time:51324ms step_avg:46.45ms
step:1106/1880 train_time:51385ms step_avg:46.46ms
step:1107/1880 train_time:51446ms step_avg:46.47ms
step:1108/1880 train_time:51508ms step_avg:46.49ms
step:1109/1880 train_time:51570ms step_avg:46.50ms
step:1110/1880 train_time:51631ms step_avg:46.51ms
step:1111/1880 train_time:51693ms step_avg:46.53ms
step:1112/1880 train_time:51755ms step_avg:46.54ms
step:1113/1880 train_time:51817ms step_avg:46.56ms
step:1114/1880 train_time:51878ms step_avg:46.57ms
step:1115/1880 train_time:51941ms step_avg:46.58ms
step:1116/1880 train_time:52003ms step_avg:46.60ms
step:1117/1880 train_time:52064ms step_avg:46.61ms
step:1118/1880 train_time:52125ms step_avg:46.62ms
step:1119/1880 train_time:52187ms step_avg:46.64ms
step:1120/1880 train_time:52248ms step_avg:46.65ms
step:1121/1880 train_time:52310ms step_avg:46.66ms
step:1122/1880 train_time:52371ms step_avg:46.68ms
step:1123/1880 train_time:52433ms step_avg:46.69ms
step:1124/1880 train_time:52495ms step_avg:46.70ms
step:1125/1880 train_time:52557ms step_avg:46.72ms
step:1126/1880 train_time:52618ms step_avg:46.73ms
step:1127/1880 train_time:52680ms step_avg:46.74ms
step:1128/1880 train_time:52741ms step_avg:46.76ms
step:1129/1880 train_time:52803ms step_avg:46.77ms
step:1130/1880 train_time:52864ms step_avg:46.78ms
step:1131/1880 train_time:52926ms step_avg:46.80ms
step:1132/1880 train_time:52988ms step_avg:46.81ms
step:1133/1880 train_time:53050ms step_avg:46.82ms
step:1134/1880 train_time:53111ms step_avg:46.83ms
step:1135/1880 train_time:53173ms step_avg:46.85ms
step:1136/1880 train_time:53234ms step_avg:46.86ms
step:1137/1880 train_time:53296ms step_avg:46.87ms
step:1138/1880 train_time:53357ms step_avg:46.89ms
step:1139/1880 train_time:53419ms step_avg:46.90ms
step:1140/1880 train_time:53481ms step_avg:46.91ms
step:1141/1880 train_time:53543ms step_avg:46.93ms
step:1142/1880 train_time:53604ms step_avg:46.94ms
step:1143/1880 train_time:53665ms step_avg:46.95ms
step:1144/1880 train_time:53726ms step_avg:46.96ms
step:1145/1880 train_time:53788ms step_avg:46.98ms
step:1146/1880 train_time:53849ms step_avg:46.99ms
step:1147/1880 train_time:53911ms step_avg:47.00ms
step:1148/1880 train_time:53972ms step_avg:47.01ms
step:1149/1880 train_time:54035ms step_avg:47.03ms
step:1150/1880 train_time:54097ms step_avg:47.04ms
step:1151/1880 train_time:54159ms step_avg:47.05ms
step:1152/1880 train_time:54221ms step_avg:47.07ms
step:1153/1880 train_time:54282ms step_avg:47.08ms
step:1154/1880 train_time:54344ms step_avg:47.09ms
step:1155/1880 train_time:54406ms step_avg:47.10ms
step:1156/1880 train_time:54466ms step_avg:47.12ms
step:1157/1880 train_time:54528ms step_avg:47.13ms
step:1158/1880 train_time:54589ms step_avg:47.14ms
step:1159/1880 train_time:54651ms step_avg:47.15ms
step:1160/1880 train_time:54712ms step_avg:47.17ms
step:1161/1880 train_time:54774ms step_avg:47.18ms
step:1162/1880 train_time:54836ms step_avg:47.19ms
step:1163/1880 train_time:54897ms step_avg:47.20ms
step:1164/1880 train_time:54959ms step_avg:47.22ms
step:1165/1880 train_time:55021ms step_avg:47.23ms
step:1166/1880 train_time:55082ms step_avg:47.24ms
step:1167/1880 train_time:55143ms step_avg:47.25ms
step:1168/1880 train_time:55204ms step_avg:47.26ms
step:1169/1880 train_time:55266ms step_avg:47.28ms
step:1170/1880 train_time:55327ms step_avg:47.29ms
step:1171/1880 train_time:55389ms step_avg:47.30ms
step:1172/1880 train_time:55450ms step_avg:47.31ms
step:1173/1880 train_time:55511ms step_avg:47.32ms
step:1174/1880 train_time:55573ms step_avg:47.34ms
step:1175/1880 train_time:55634ms step_avg:47.35ms
step:1176/1880 train_time:55696ms step_avg:47.36ms
step:1177/1880 train_time:55759ms step_avg:47.37ms
step:1178/1880 train_time:55820ms step_avg:47.39ms
step:1179/1880 train_time:55881ms step_avg:47.40ms
step:1180/1880 train_time:55942ms step_avg:47.41ms
step:1181/1880 train_time:56004ms step_avg:47.42ms
step:1182/1880 train_time:56065ms step_avg:47.43ms
step:1183/1880 train_time:56127ms step_avg:47.44ms
step:1184/1880 train_time:56188ms step_avg:47.46ms
step:1185/1880 train_time:56250ms step_avg:47.47ms
step:1186/1880 train_time:56312ms step_avg:47.48ms
step:1187/1880 train_time:56373ms step_avg:47.49ms
step:1188/1880 train_time:56435ms step_avg:47.50ms
step:1189/1880 train_time:56497ms step_avg:47.52ms
step:1190/1880 train_time:56559ms step_avg:47.53ms
step:1191/1880 train_time:56620ms step_avg:47.54ms
step:1192/1880 train_time:56682ms step_avg:47.55ms
step:1193/1880 train_time:56744ms step_avg:47.56ms
step:1194/1880 train_time:56805ms step_avg:47.58ms
step:1195/1880 train_time:56866ms step_avg:47.59ms
step:1196/1880 train_time:56927ms step_avg:47.60ms
step:1197/1880 train_time:56989ms step_avg:47.61ms
step:1198/1880 train_time:57050ms step_avg:47.62ms
step:1199/1880 train_time:57112ms step_avg:47.63ms
step:1200/1880 train_time:57173ms step_avg:47.64ms
step:1201/1880 train_time:57236ms step_avg:47.66ms
step:1202/1880 train_time:57297ms step_avg:47.67ms
step:1203/1880 train_time:57359ms step_avg:47.68ms
step:1204/1880 train_time:57420ms step_avg:47.69ms
step:1205/1880 train_time:57482ms step_avg:47.70ms
step:1206/1880 train_time:57543ms step_avg:47.71ms
step:1207/1880 train_time:57605ms step_avg:47.73ms
step:1208/1880 train_time:57666ms step_avg:47.74ms
step:1209/1880 train_time:57727ms step_avg:47.75ms
step:1210/1880 train_time:57788ms step_avg:47.76ms
step:1211/1880 train_time:57850ms step_avg:47.77ms
step:1212/1880 train_time:57911ms step_avg:47.78ms
step:1213/1880 train_time:57973ms step_avg:47.79ms
step:1214/1880 train_time:58035ms step_avg:47.80ms
step:1215/1880 train_time:58097ms step_avg:47.82ms
step:1216/1880 train_time:58158ms step_avg:47.83ms
step:1217/1880 train_time:58220ms step_avg:47.84ms
step:1218/1880 train_time:58281ms step_avg:47.85ms
step:1219/1880 train_time:58342ms step_avg:47.86ms
step:1220/1880 train_time:58403ms step_avg:47.87ms
step:1221/1880 train_time:58465ms step_avg:47.88ms
step:1222/1880 train_time:58527ms step_avg:47.89ms
step:1223/1880 train_time:58589ms step_avg:47.91ms
step:1224/1880 train_time:58650ms step_avg:47.92ms
step:1225/1880 train_time:58712ms step_avg:47.93ms
step:1226/1880 train_time:58772ms step_avg:47.94ms
step:1227/1880 train_time:58834ms step_avg:47.95ms
step:1228/1880 train_time:58895ms step_avg:47.96ms
step:1229/1880 train_time:58984ms step_avg:47.99ms
step:1230/1880 train_time:59072ms step_avg:48.03ms
step:1231/1880 train_time:59160ms step_avg:48.06ms
step:1232/1880 train_time:59248ms step_avg:48.09ms
step:1233/1880 train_time:59336ms step_avg:48.12ms
step:1234/1880 train_time:59423ms step_avg:48.16ms
step:1235/1880 train_time:59513ms step_avg:48.19ms
step:1236/1880 train_time:59600ms step_avg:48.22ms
step:1237/1880 train_time:59689ms step_avg:48.25ms
step:1238/1880 train_time:59776ms step_avg:48.28ms
step:1239/1880 train_time:59864ms step_avg:48.32ms
step:1240/1880 train_time:59953ms step_avg:48.35ms
step:1241/1880 train_time:60040ms step_avg:48.38ms
step:1242/1880 train_time:60128ms step_avg:48.41ms
step:1243/1880 train_time:60217ms step_avg:48.44ms
step:1244/1880 train_time:60304ms step_avg:48.48ms
step:1245/1880 train_time:60393ms step_avg:48.51ms
step:1246/1880 train_time:60480ms step_avg:48.54ms
step:1247/1880 train_time:60568ms step_avg:48.57ms
step:1248/1880 train_time:60656ms step_avg:48.60ms
step:1249/1880 train_time:60744ms step_avg:48.63ms
step:1250/1880 train_time:60832ms step_avg:48.67ms
step:1250/1880 val_loss:3.5320 train_time:60923ms step_avg:48.74ms
step:1251/1880 train_time:60943ms step_avg:48.72ms
step:1252/1880 train_time:61008ms step_avg:48.73ms
step:1253/1880 train_time:61105ms step_avg:48.77ms
step:1254/1880 train_time:61195ms step_avg:48.80ms
step:1255/1880 train_time:61283ms step_avg:48.83ms
step:1256/1880 train_time:61370ms step_avg:48.86ms
step:1257/1880 train_time:61457ms step_avg:48.89ms
step:1258/1880 train_time:61544ms step_avg:48.92ms
step:1259/1880 train_time:61630ms step_avg:48.95ms
step:1260/1880 train_time:61717ms step_avg:48.98ms
step:1261/1880 train_time:61804ms step_avg:49.01ms
step:1262/1880 train_time:61891ms step_avg:49.04ms
step:1263/1880 train_time:61981ms step_avg:49.07ms
step:1264/1880 train_time:62072ms step_avg:49.11ms
step:1265/1880 train_time:62163ms step_avg:49.14ms
step:1266/1880 train_time:62250ms step_avg:49.17ms
step:1267/1880 train_time:62338ms step_avg:49.20ms
step:1268/1880 train_time:62425ms step_avg:49.23ms
step:1269/1880 train_time:62512ms step_avg:49.26ms
step:1270/1880 train_time:62599ms step_avg:49.29ms
step:1271/1880 train_time:62687ms step_avg:49.32ms
step:1272/1880 train_time:62773ms step_avg:49.35ms
step:1273/1880 train_time:62861ms step_avg:49.38ms
step:1274/1880 train_time:62948ms step_avg:49.41ms
step:1275/1880 train_time:63040ms step_avg:49.44ms
step:1276/1880 train_time:63128ms step_avg:49.47ms
step:1277/1880 train_time:63217ms step_avg:49.50ms
step:1278/1880 train_time:63305ms step_avg:49.53ms
step:1279/1880 train_time:63392ms step_avg:49.56ms
step:1280/1880 train_time:63481ms step_avg:49.59ms
step:1281/1880 train_time:63568ms step_avg:49.62ms
step:1282/1880 train_time:63655ms step_avg:49.65ms
step:1283/1880 train_time:63743ms step_avg:49.68ms
step:1284/1880 train_time:63830ms step_avg:49.71ms
step:1285/1880 train_time:63918ms step_avg:49.74ms
step:1286/1880 train_time:64007ms step_avg:49.77ms
step:1287/1880 train_time:64097ms step_avg:49.80ms
step:1288/1880 train_time:64185ms step_avg:49.83ms
step:1289/1880 train_time:64274ms step_avg:49.86ms
step:1290/1880 train_time:64362ms step_avg:49.89ms
step:1291/1880 train_time:64449ms step_avg:49.92ms
step:1292/1880 train_time:64537ms step_avg:49.95ms
step:1293/1880 train_time:64624ms step_avg:49.98ms
step:1294/1880 train_time:64712ms step_avg:50.01ms
step:1295/1880 train_time:64799ms step_avg:50.04ms
step:1296/1880 train_time:64886ms step_avg:50.07ms
step:1297/1880 train_time:64975ms step_avg:50.10ms
step:1298/1880 train_time:65065ms step_avg:50.13ms
step:1299/1880 train_time:65153ms step_avg:50.16ms
step:1300/1880 train_time:65242ms step_avg:50.19ms
step:1301/1880 train_time:65329ms step_avg:50.21ms
step:1302/1880 train_time:65417ms step_avg:50.24ms
step:1303/1880 train_time:65505ms step_avg:50.27ms
step:1304/1880 train_time:65593ms step_avg:50.30ms
step:1305/1880 train_time:65681ms step_avg:50.33ms
step:1306/1880 train_time:65768ms step_avg:50.36ms
step:1307/1880 train_time:65857ms step_avg:50.39ms
step:1308/1880 train_time:65944ms step_avg:50.42ms
step:1309/1880 train_time:66032ms step_avg:50.44ms
step:1310/1880 train_time:66122ms step_avg:50.47ms
step:1311/1880 train_time:66209ms step_avg:50.50ms
step:1312/1880 train_time:66297ms step_avg:50.53ms
step:1313/1880 train_time:66385ms step_avg:50.56ms
step:1314/1880 train_time:66473ms step_avg:50.59ms
step:1315/1880 train_time:66562ms step_avg:50.62ms
step:1316/1880 train_time:66650ms step_avg:50.65ms
step:1317/1880 train_time:66737ms step_avg:50.67ms
step:1318/1880 train_time:66824ms step_avg:50.70ms
step:1319/1880 train_time:66912ms step_avg:50.73ms
step:1320/1880 train_time:67001ms step_avg:50.76ms
step:1321/1880 train_time:67089ms step_avg:50.79ms
step:1322/1880 train_time:67177ms step_avg:50.81ms
step:1323/1880 train_time:67265ms step_avg:50.84ms
step:1324/1880 train_time:67353ms step_avg:50.87ms
step:1325/1880 train_time:67441ms step_avg:50.90ms
step:1326/1880 train_time:67528ms step_avg:50.93ms
step:1327/1880 train_time:67616ms step_avg:50.95ms
step:1328/1880 train_time:67704ms step_avg:50.98ms
step:1329/1880 train_time:67792ms step_avg:51.01ms
step:1330/1880 train_time:67879ms step_avg:51.04ms
step:1331/1880 train_time:67968ms step_avg:51.07ms
step:1332/1880 train_time:68056ms step_avg:51.09ms
step:1333/1880 train_time:68145ms step_avg:51.12ms
step:1334/1880 train_time:68233ms step_avg:51.15ms
step:1335/1880 train_time:68321ms step_avg:51.18ms
step:1336/1880 train_time:68408ms step_avg:51.20ms
step:1337/1880 train_time:68496ms step_avg:51.23ms
step:1338/1880 train_time:68584ms step_avg:51.26ms
step:1339/1880 train_time:68672ms step_avg:51.29ms
step:1340/1880 train_time:68760ms step_avg:51.31ms
step:1341/1880 train_time:68847ms step_avg:51.34ms
step:1342/1880 train_time:68936ms step_avg:51.37ms
step:1343/1880 train_time:69026ms step_avg:51.40ms
step:1344/1880 train_time:69113ms step_avg:51.42ms
step:1345/1880 train_time:69201ms step_avg:51.45ms
step:1346/1880 train_time:69289ms step_avg:51.48ms
step:1347/1880 train_time:69377ms step_avg:51.51ms
step:1348/1880 train_time:69466ms step_avg:51.53ms
step:1349/1880 train_time:69554ms step_avg:51.56ms
step:1350/1880 train_time:69642ms step_avg:51.59ms
step:1351/1880 train_time:69729ms step_avg:51.61ms
step:1352/1880 train_time:69817ms step_avg:51.64ms
step:1353/1880 train_time:69906ms step_avg:51.67ms
step:1354/1880 train_time:69994ms step_avg:51.69ms
step:1355/1880 train_time:70083ms step_avg:51.72ms
step:1356/1880 train_time:70170ms step_avg:51.75ms
step:1357/1880 train_time:70259ms step_avg:51.78ms
step:1358/1880 train_time:70346ms step_avg:51.80ms
step:1359/1880 train_time:70434ms step_avg:51.83ms
step:1360/1880 train_time:70523ms step_avg:51.85ms
step:1361/1880 train_time:70610ms step_avg:51.88ms
step:1362/1880 train_time:70698ms step_avg:51.91ms
step:1363/1880 train_time:70787ms step_avg:51.93ms
step:1364/1880 train_time:70876ms step_avg:51.96ms
step:1365/1880 train_time:70964ms step_avg:51.99ms
step:1366/1880 train_time:71053ms step_avg:52.02ms
step:1367/1880 train_time:71141ms step_avg:52.04ms
step:1368/1880 train_time:71229ms step_avg:52.07ms
step:1369/1880 train_time:71317ms step_avg:52.09ms
step:1370/1880 train_time:71405ms step_avg:52.12ms
step:1371/1880 train_time:71494ms step_avg:52.15ms
step:1372/1880 train_time:71582ms step_avg:52.17ms
step:1373/1880 train_time:71669ms step_avg:52.20ms
step:1374/1880 train_time:71758ms step_avg:52.23ms
step:1375/1880 train_time:71846ms step_avg:52.25ms
step:1376/1880 train_time:71936ms step_avg:52.28ms
step:1377/1880 train_time:72025ms step_avg:52.31ms
step:1378/1880 train_time:72112ms step_avg:52.33ms
step:1379/1880 train_time:72201ms step_avg:52.36ms
step:1380/1880 train_time:72288ms step_avg:52.38ms
step:1381/1880 train_time:72376ms step_avg:52.41ms
step:1382/1880 train_time:72465ms step_avg:52.43ms
step:1383/1880 train_time:72553ms step_avg:52.46ms
step:1384/1880 train_time:72641ms step_avg:52.49ms
step:1385/1880 train_time:72729ms step_avg:52.51ms
step:1386/1880 train_time:72817ms step_avg:52.54ms
step:1387/1880 train_time:72906ms step_avg:52.56ms
step:1388/1880 train_time:72994ms step_avg:52.59ms
step:1389/1880 train_time:73082ms step_avg:52.61ms
step:1390/1880 train_time:73169ms step_avg:52.64ms
step:1391/1880 train_time:73256ms step_avg:52.66ms
step:1392/1880 train_time:73345ms step_avg:52.69ms
step:1393/1880 train_time:73433ms step_avg:52.72ms
step:1394/1880 train_time:73521ms step_avg:52.74ms
step:1395/1880 train_time:73608ms step_avg:52.77ms
step:1396/1880 train_time:73696ms step_avg:52.79ms
step:1397/1880 train_time:73785ms step_avg:52.82ms
step:1398/1880 train_time:73873ms step_avg:52.84ms
step:1399/1880 train_time:73961ms step_avg:52.87ms
step:1400/1880 train_time:74049ms step_avg:52.89ms
step:1401/1880 train_time:74137ms step_avg:52.92ms
step:1402/1880 train_time:74224ms step_avg:52.94ms
step:1403/1880 train_time:74312ms step_avg:52.97ms
step:1404/1880 train_time:74401ms step_avg:52.99ms
step:1405/1880 train_time:74488ms step_avg:53.02ms
step:1406/1880 train_time:74576ms step_avg:53.04ms
step:1407/1880 train_time:74665ms step_avg:53.07ms
step:1408/1880 train_time:74753ms step_avg:53.09ms
step:1409/1880 train_time:74841ms step_avg:53.12ms
step:1410/1880 train_time:74928ms step_avg:53.14ms
step:1411/1880 train_time:75016ms step_avg:53.17ms
step:1412/1880 train_time:75105ms step_avg:53.19ms
step:1413/1880 train_time:75193ms step_avg:53.22ms
step:1414/1880 train_time:75282ms step_avg:53.24ms
step:1415/1880 train_time:75370ms step_avg:53.26ms
step:1416/1880 train_time:75458ms step_avg:53.29ms
step:1417/1880 train_time:75546ms step_avg:53.31ms
step:1418/1880 train_time:75635ms step_avg:53.34ms
step:1419/1880 train_time:75722ms step_avg:53.36ms
step:1420/1880 train_time:75809ms step_avg:53.39ms
step:1421/1880 train_time:75896ms step_avg:53.41ms
step:1422/1880 train_time:75984ms step_avg:53.43ms
step:1423/1880 train_time:76073ms step_avg:53.46ms
step:1424/1880 train_time:76162ms step_avg:53.48ms
step:1425/1880 train_time:76250ms step_avg:53.51ms
step:1426/1880 train_time:76339ms step_avg:53.53ms
step:1427/1880 train_time:76427ms step_avg:53.56ms
step:1428/1880 train_time:76515ms step_avg:53.58ms
step:1429/1880 train_time:76603ms step_avg:53.61ms
step:1430/1880 train_time:76691ms step_avg:53.63ms
step:1431/1880 train_time:76778ms step_avg:53.65ms
step:1432/1880 train_time:76866ms step_avg:53.68ms
step:1433/1880 train_time:76954ms step_avg:53.70ms
step:1434/1880 train_time:77042ms step_avg:53.73ms
step:1435/1880 train_time:77130ms step_avg:53.75ms
step:1436/1880 train_time:77218ms step_avg:53.77ms
step:1437/1880 train_time:77306ms step_avg:53.80ms
step:1438/1880 train_time:77396ms step_avg:53.82ms
step:1439/1880 train_time:77484ms step_avg:53.85ms
step:1440/1880 train_time:77571ms step_avg:53.87ms
step:1441/1880 train_time:77660ms step_avg:53.89ms
step:1442/1880 train_time:77747ms step_avg:53.92ms
step:1443/1880 train_time:77835ms step_avg:53.94ms
step:1444/1880 train_time:77923ms step_avg:53.96ms
step:1445/1880 train_time:78011ms step_avg:53.99ms
step:1446/1880 train_time:78100ms step_avg:54.01ms
step:1447/1880 train_time:78188ms step_avg:54.03ms
step:1448/1880 train_time:78276ms step_avg:54.06ms
step:1449/1880 train_time:78365ms step_avg:54.08ms
step:1450/1880 train_time:78454ms step_avg:54.11ms
step:1451/1880 train_time:78543ms step_avg:54.13ms
step:1452/1880 train_time:78630ms step_avg:54.15ms
step:1453/1880 train_time:78718ms step_avg:54.18ms
step:1454/1880 train_time:78806ms step_avg:54.20ms
step:1455/1880 train_time:78895ms step_avg:54.22ms
step:1456/1880 train_time:78982ms step_avg:54.25ms
step:1457/1880 train_time:79070ms step_avg:54.27ms
step:1458/1880 train_time:79159ms step_avg:54.29ms
step:1459/1880 train_time:79247ms step_avg:54.32ms
step:1460/1880 train_time:79336ms step_avg:54.34ms
step:1461/1880 train_time:79425ms step_avg:54.36ms
step:1462/1880 train_time:79512ms step_avg:54.39ms
step:1463/1880 train_time:79600ms step_avg:54.41ms
step:1464/1880 train_time:79688ms step_avg:54.43ms
step:1465/1880 train_time:79776ms step_avg:54.45ms
step:1466/1880 train_time:79864ms step_avg:54.48ms
step:1467/1880 train_time:79952ms step_avg:54.50ms
step:1468/1880 train_time:80041ms step_avg:54.52ms
step:1469/1880 train_time:80128ms step_avg:54.55ms
step:1470/1880 train_time:80217ms step_avg:54.57ms
step:1471/1880 train_time:80306ms step_avg:54.59ms
step:1472/1880 train_time:80396ms step_avg:54.62ms
step:1473/1880 train_time:80485ms step_avg:54.64ms
step:1474/1880 train_time:80573ms step_avg:54.66ms
step:1475/1880 train_time:80660ms step_avg:54.68ms
step:1476/1880 train_time:80747ms step_avg:54.71ms
step:1477/1880 train_time:80834ms step_avg:54.73ms
step:1478/1880 train_time:80921ms step_avg:54.75ms
step:1479/1880 train_time:81009ms step_avg:54.77ms
step:1480/1880 train_time:81096ms step_avg:54.79ms
step:1481/1880 train_time:81186ms step_avg:54.82ms
step:1482/1880 train_time:81274ms step_avg:54.84ms
step:1483/1880 train_time:81363ms step_avg:54.86ms
step:1484/1880 train_time:81451ms step_avg:54.89ms
step:1485/1880 train_time:81539ms step_avg:54.91ms
step:1486/1880 train_time:81626ms step_avg:54.93ms
step:1487/1880 train_time:81714ms step_avg:54.95ms
step:1488/1880 train_time:81803ms step_avg:54.98ms
step:1489/1880 train_time:81890ms step_avg:55.00ms
step:1490/1880 train_time:81979ms step_avg:55.02ms
step:1491/1880 train_time:82067ms step_avg:55.04ms
step:1492/1880 train_time:82155ms step_avg:55.06ms
step:1493/1880 train_time:82245ms step_avg:55.09ms
step:1494/1880 train_time:82332ms step_avg:55.11ms
step:1495/1880 train_time:82421ms step_avg:55.13ms
step:1496/1880 train_time:82508ms step_avg:55.15ms
step:1497/1880 train_time:82597ms step_avg:55.17ms
step:1498/1880 train_time:82685ms step_avg:55.20ms
step:1499/1880 train_time:82773ms step_avg:55.22ms
step:1500/1880 train_time:82862ms step_avg:55.24ms
step:1500/1880 val_loss:3.4074 train_time:82952ms step_avg:55.30ms
step:1501/1880 train_time:82972ms step_avg:55.28ms
step:1502/1880 train_time:83039ms step_avg:55.29ms
step:1503/1880 train_time:83135ms step_avg:55.31ms
step:1504/1880 train_time:83225ms step_avg:55.34ms
step:1505/1880 train_time:83313ms step_avg:55.36ms
step:1506/1880 train_time:83400ms step_avg:55.38ms
step:1507/1880 train_time:83487ms step_avg:55.40ms
step:1508/1880 train_time:83574ms step_avg:55.42ms
step:1509/1880 train_time:83661ms step_avg:55.44ms
step:1510/1880 train_time:83747ms step_avg:55.46ms
step:1511/1880 train_time:83836ms step_avg:55.48ms
step:1512/1880 train_time:83924ms step_avg:55.51ms
step:1513/1880 train_time:84013ms step_avg:55.53ms
step:1514/1880 train_time:84103ms step_avg:55.55ms
step:1515/1880 train_time:84193ms step_avg:55.57ms
step:1516/1880 train_time:84281ms step_avg:55.59ms
step:1517/1880 train_time:84370ms step_avg:55.62ms
step:1518/1880 train_time:84456ms step_avg:55.64ms
step:1519/1880 train_time:84544ms step_avg:55.66ms
step:1520/1880 train_time:84631ms step_avg:55.68ms
step:1521/1880 train_time:84718ms step_avg:55.70ms
step:1522/1880 train_time:84806ms step_avg:55.72ms
step:1523/1880 train_time:84895ms step_avg:55.74ms
step:1524/1880 train_time:84984ms step_avg:55.76ms
step:1525/1880 train_time:85074ms step_avg:55.79ms
step:1526/1880 train_time:85163ms step_avg:55.81ms
step:1527/1880 train_time:85252ms step_avg:55.83ms
step:1528/1880 train_time:85339ms step_avg:55.85ms
step:1529/1880 train_time:85428ms step_avg:55.87ms
step:1530/1880 train_time:85515ms step_avg:55.89ms
step:1531/1880 train_time:85602ms step_avg:55.91ms
step:1532/1880 train_time:85690ms step_avg:55.93ms
step:1533/1880 train_time:85777ms step_avg:55.95ms
step:1534/1880 train_time:85865ms step_avg:55.97ms
step:1535/1880 train_time:85955ms step_avg:56.00ms
step:1536/1880 train_time:86044ms step_avg:56.02ms
step:1537/1880 train_time:86134ms step_avg:56.04ms
step:1538/1880 train_time:86223ms step_avg:56.06ms
step:1539/1880 train_time:86311ms step_avg:56.08ms
step:1540/1880 train_time:86399ms step_avg:56.10ms
step:1541/1880 train_time:86487ms step_avg:56.12ms
step:1542/1880 train_time:86574ms step_avg:56.14ms
step:1543/1880 train_time:86661ms step_avg:56.16ms
step:1544/1880 train_time:86748ms step_avg:56.18ms
step:1545/1880 train_time:86836ms step_avg:56.20ms
step:1546/1880 train_time:86924ms step_avg:56.23ms
step:1547/1880 train_time:87013ms step_avg:56.25ms
step:1548/1880 train_time:87101ms step_avg:56.27ms
step:1549/1880 train_time:87191ms step_avg:56.29ms
step:1550/1880 train_time:87278ms step_avg:56.31ms
step:1551/1880 train_time:87367ms step_avg:56.33ms
step:1552/1880 train_time:87455ms step_avg:56.35ms
step:1553/1880 train_time:87543ms step_avg:56.37ms
step:1554/1880 train_time:87631ms step_avg:56.39ms
step:1555/1880 train_time:87718ms step_avg:56.41ms
step:1556/1880 train_time:87806ms step_avg:56.43ms
step:1557/1880 train_time:87894ms step_avg:56.45ms
step:1558/1880 train_time:87982ms step_avg:56.47ms
step:1559/1880 train_time:88071ms step_avg:56.49ms
step:1560/1880 train_time:88159ms step_avg:56.51ms
step:1561/1880 train_time:88248ms step_avg:56.53ms
step:1562/1880 train_time:88336ms step_avg:56.55ms
step:1563/1880 train_time:88423ms step_avg:56.57ms
step:1564/1880 train_time:88511ms step_avg:56.59ms
step:1565/1880 train_time:88599ms step_avg:56.61ms
step:1566/1880 train_time:88687ms step_avg:56.63ms
step:1567/1880 train_time:88774ms step_avg:56.65ms
step:1568/1880 train_time:88862ms step_avg:56.67ms
step:1569/1880 train_time:88950ms step_avg:56.69ms
step:1570/1880 train_time:89037ms step_avg:56.71ms
step:1571/1880 train_time:89126ms step_avg:56.73ms
step:1572/1880 train_time:89214ms step_avg:56.75ms
step:1573/1880 train_time:89302ms step_avg:56.77ms
step:1574/1880 train_time:89391ms step_avg:56.79ms
step:1575/1880 train_time:89479ms step_avg:56.81ms
step:1576/1880 train_time:89567ms step_avg:56.83ms
step:1577/1880 train_time:89655ms step_avg:56.85ms
step:1578/1880 train_time:89743ms step_avg:56.87ms
step:1579/1880 train_time:89831ms step_avg:56.89ms
step:1580/1880 train_time:89919ms step_avg:56.91ms
step:1581/1880 train_time:90006ms step_avg:56.93ms
step:1582/1880 train_time:90096ms step_avg:56.95ms
step:1583/1880 train_time:90184ms step_avg:56.97ms
step:1584/1880 train_time:90272ms step_avg:56.99ms
step:1585/1880 train_time:90360ms step_avg:57.01ms
step:1586/1880 train_time:90448ms step_avg:57.03ms
step:1587/1880 train_time:90537ms step_avg:57.05ms
step:1588/1880 train_time:90624ms step_avg:57.07ms
step:1589/1880 train_time:90712ms step_avg:57.09ms
step:1590/1880 train_time:90800ms step_avg:57.11ms
step:1591/1880 train_time:90889ms step_avg:57.13ms
step:1592/1880 train_time:90976ms step_avg:57.15ms
step:1593/1880 train_time:91064ms step_avg:57.17ms
step:1594/1880 train_time:91152ms step_avg:57.18ms
step:1595/1880 train_time:91240ms step_avg:57.20ms
step:1596/1880 train_time:91329ms step_avg:57.22ms
step:1597/1880 train_time:91418ms step_avg:57.24ms
step:1598/1880 train_time:91507ms step_avg:57.26ms
step:1599/1880 train_time:91596ms step_avg:57.28ms
step:1600/1880 train_time:91684ms step_avg:57.30ms
step:1601/1880 train_time:91772ms step_avg:57.32ms
step:1602/1880 train_time:91859ms step_avg:57.34ms
step:1603/1880 train_time:91947ms step_avg:57.36ms
step:1604/1880 train_time:92036ms step_avg:57.38ms
step:1605/1880 train_time:92123ms step_avg:57.40ms
step:1606/1880 train_time:92211ms step_avg:57.42ms
step:1607/1880 train_time:92299ms step_avg:57.44ms
step:1608/1880 train_time:92389ms step_avg:57.46ms
step:1609/1880 train_time:92477ms step_avg:57.47ms
step:1610/1880 train_time:92564ms step_avg:57.49ms
step:1611/1880 train_time:92652ms step_avg:57.51ms
step:1612/1880 train_time:92740ms step_avg:57.53ms
step:1613/1880 train_time:92828ms step_avg:57.55ms
step:1614/1880 train_time:92916ms step_avg:57.57ms
step:1615/1880 train_time:93003ms step_avg:57.59ms
step:1616/1880 train_time:93091ms step_avg:57.61ms
step:1617/1880 train_time:93179ms step_avg:57.62ms
step:1618/1880 train_time:93266ms step_avg:57.64ms
step:1619/1880 train_time:93355ms step_avg:57.66ms
step:1620/1880 train_time:93443ms step_avg:57.68ms
step:1621/1880 train_time:93530ms step_avg:57.70ms
step:1622/1880 train_time:93618ms step_avg:57.72ms
step:1623/1880 train_time:93705ms step_avg:57.74ms
step:1624/1880 train_time:93794ms step_avg:57.76ms
step:1625/1880 train_time:93882ms step_avg:57.77ms
step:1626/1880 train_time:93972ms step_avg:57.79ms
step:1627/1880 train_time:94059ms step_avg:57.81ms
step:1628/1880 train_time:94147ms step_avg:57.83ms
step:1629/1880 train_time:94235ms step_avg:57.85ms
step:1630/1880 train_time:94324ms step_avg:57.87ms
step:1631/1880 train_time:94412ms step_avg:57.89ms
step:1632/1880 train_time:94499ms step_avg:57.90ms
step:1633/1880 train_time:94587ms step_avg:57.92ms
step:1634/1880 train_time:94675ms step_avg:57.94ms
step:1635/1880 train_time:94763ms step_avg:57.96ms
step:1636/1880 train_time:94851ms step_avg:57.98ms
step:1637/1880 train_time:94939ms step_avg:58.00ms
step:1638/1880 train_time:95028ms step_avg:58.01ms
step:1639/1880 train_time:95117ms step_avg:58.03ms
step:1640/1880 train_time:95205ms step_avg:58.05ms
step:1641/1880 train_time:95294ms step_avg:58.07ms
step:1642/1880 train_time:95381ms step_avg:58.09ms
step:1643/1880 train_time:95469ms step_avg:58.11ms
step:1644/1880 train_time:95556ms step_avg:58.12ms
step:1645/1880 train_time:95644ms step_avg:58.14ms
step:1646/1880 train_time:95733ms step_avg:58.16ms
step:1647/1880 train_time:95821ms step_avg:58.18ms
step:1648/1880 train_time:95909ms step_avg:58.20ms
step:1649/1880 train_time:95999ms step_avg:58.22ms
step:1650/1880 train_time:96087ms step_avg:58.23ms
step:1651/1880 train_time:96175ms step_avg:58.25ms
step:1652/1880 train_time:96263ms step_avg:58.27ms
step:1653/1880 train_time:96351ms step_avg:58.29ms
step:1654/1880 train_time:96438ms step_avg:58.31ms
step:1655/1880 train_time:96527ms step_avg:58.32ms
step:1656/1880 train_time:96614ms step_avg:58.34ms
step:1657/1880 train_time:96702ms step_avg:58.36ms
step:1658/1880 train_time:96790ms step_avg:58.38ms
step:1659/1880 train_time:96878ms step_avg:58.40ms
step:1660/1880 train_time:96967ms step_avg:58.41ms
step:1661/1880 train_time:97055ms step_avg:58.43ms
step:1662/1880 train_time:97142ms step_avg:58.45ms
step:1663/1880 train_time:97231ms step_avg:58.47ms
step:1664/1880 train_time:97319ms step_avg:58.48ms
step:1665/1880 train_time:97407ms step_avg:58.50ms
step:1666/1880 train_time:97496ms step_avg:58.52ms
step:1667/1880 train_time:97584ms step_avg:58.54ms
step:1668/1880 train_time:97672ms step_avg:58.56ms
step:1669/1880 train_time:97759ms step_avg:58.57ms
step:1670/1880 train_time:97847ms step_avg:58.59ms
step:1671/1880 train_time:97935ms step_avg:58.61ms
step:1672/1880 train_time:98024ms step_avg:58.63ms
step:1673/1880 train_time:98111ms step_avg:58.64ms
step:1674/1880 train_time:98199ms step_avg:58.66ms
step:1675/1880 train_time:98288ms step_avg:58.68ms
step:1676/1880 train_time:98376ms step_avg:58.70ms
step:1677/1880 train_time:98465ms step_avg:58.71ms
step:1678/1880 train_time:98553ms step_avg:58.73ms
step:1679/1880 train_time:98641ms step_avg:58.75ms
step:1680/1880 train_time:98728ms step_avg:58.77ms
step:1681/1880 train_time:98816ms step_avg:58.78ms
step:1682/1880 train_time:98905ms step_avg:58.80ms
step:1683/1880 train_time:98994ms step_avg:58.82ms
step:1684/1880 train_time:99081ms step_avg:58.84ms
step:1685/1880 train_time:99170ms step_avg:58.85ms
step:1686/1880 train_time:99258ms step_avg:58.87ms
step:1687/1880 train_time:99346ms step_avg:58.89ms
step:1688/1880 train_time:99436ms step_avg:58.91ms
step:1689/1880 train_time:99524ms step_avg:58.92ms
step:1690/1880 train_time:99613ms step_avg:58.94ms
step:1691/1880 train_time:99701ms step_avg:58.96ms
step:1692/1880 train_time:99789ms step_avg:58.98ms
step:1693/1880 train_time:99877ms step_avg:58.99ms
step:1694/1880 train_time:99965ms step_avg:59.01ms
step:1695/1880 train_time:100053ms step_avg:59.03ms
step:1696/1880 train_time:100141ms step_avg:59.05ms
step:1697/1880 train_time:100230ms step_avg:59.06ms
step:1698/1880 train_time:100318ms step_avg:59.08ms
step:1699/1880 train_time:100407ms step_avg:59.10ms
step:1700/1880 train_time:100496ms step_avg:59.12ms
step:1701/1880 train_time:100583ms step_avg:59.13ms
step:1702/1880 train_time:100672ms step_avg:59.15ms
step:1703/1880 train_time:100759ms step_avg:59.17ms
step:1704/1880 train_time:100846ms step_avg:59.18ms
step:1705/1880 train_time:100935ms step_avg:59.20ms
step:1706/1880 train_time:101024ms step_avg:59.22ms
step:1707/1880 train_time:101113ms step_avg:59.23ms
step:1708/1880 train_time:101200ms step_avg:59.25ms
step:1709/1880 train_time:101288ms step_avg:59.27ms
step:1710/1880 train_time:101376ms step_avg:59.28ms
step:1711/1880 train_time:101464ms step_avg:59.30ms
step:1712/1880 train_time:101553ms step_avg:59.32ms
step:1713/1880 train_time:101640ms step_avg:59.33ms
step:1714/1880 train_time:101727ms step_avg:59.35ms
step:1715/1880 train_time:101815ms step_avg:59.37ms
step:1716/1880 train_time:101903ms step_avg:59.38ms
step:1717/1880 train_time:101990ms step_avg:59.40ms
step:1718/1880 train_time:102078ms step_avg:59.42ms
step:1719/1880 train_time:102166ms step_avg:59.43ms
step:1720/1880 train_time:102254ms step_avg:59.45ms
step:1721/1880 train_time:102342ms step_avg:59.47ms
step:1722/1880 train_time:102431ms step_avg:59.48ms
step:1723/1880 train_time:102518ms step_avg:59.50ms
step:1724/1880 train_time:102606ms step_avg:59.52ms
step:1725/1880 train_time:102695ms step_avg:59.53ms
step:1726/1880 train_time:102783ms step_avg:59.55ms
step:1727/1880 train_time:102871ms step_avg:59.57ms
step:1728/1880 train_time:102958ms step_avg:59.58ms
step:1729/1880 train_time:103046ms step_avg:59.60ms
step:1730/1880 train_time:103135ms step_avg:59.62ms
step:1731/1880 train_time:103222ms step_avg:59.63ms
step:1732/1880 train_time:103311ms step_avg:59.65ms
step:1733/1880 train_time:103399ms step_avg:59.66ms
step:1734/1880 train_time:103487ms step_avg:59.68ms
step:1735/1880 train_time:103575ms step_avg:59.70ms
step:1736/1880 train_time:103664ms step_avg:59.71ms
step:1737/1880 train_time:103752ms step_avg:59.73ms
step:1738/1880 train_time:103840ms step_avg:59.75ms
step:1739/1880 train_time:103928ms step_avg:59.76ms
step:1740/1880 train_time:104017ms step_avg:59.78ms
step:1741/1880 train_time:104105ms step_avg:59.80ms
step:1742/1880 train_time:104194ms step_avg:59.81ms
step:1743/1880 train_time:104281ms step_avg:59.83ms
step:1744/1880 train_time:104370ms step_avg:59.85ms
step:1745/1880 train_time:104458ms step_avg:59.86ms
step:1746/1880 train_time:104546ms step_avg:59.88ms
step:1747/1880 train_time:104635ms step_avg:59.89ms
step:1748/1880 train_time:104724ms step_avg:59.91ms
step:1749/1880 train_time:104811ms step_avg:59.93ms
step:1750/1880 train_time:104899ms step_avg:59.94ms
step:1750/1880 val_loss:3.3132 train_time:104990ms step_avg:59.99ms
step:1751/1880 train_time:105010ms step_avg:59.97ms
step:1752/1880 train_time:105078ms step_avg:59.98ms
step:1753/1880 train_time:105171ms step_avg:59.99ms
step:1754/1880 train_time:105259ms step_avg:60.01ms
step:1755/1880 train_time:105348ms step_avg:60.03ms
step:1756/1880 train_time:105435ms step_avg:60.04ms
step:1757/1880 train_time:105522ms step_avg:60.06ms
step:1758/1880 train_time:105609ms step_avg:60.07ms
step:1759/1880 train_time:105696ms step_avg:60.09ms
step:1760/1880 train_time:105785ms step_avg:60.11ms
step:1761/1880 train_time:105872ms step_avg:60.12ms
step:1762/1880 train_time:105960ms step_avg:60.14ms
step:1763/1880 train_time:106051ms step_avg:60.15ms
step:1764/1880 train_time:106141ms step_avg:60.17ms
step:1765/1880 train_time:106231ms step_avg:60.19ms
step:1766/1880 train_time:106319ms step_avg:60.20ms
step:1767/1880 train_time:106407ms step_avg:60.22ms
step:1768/1880 train_time:106495ms step_avg:60.23ms
step:1769/1880 train_time:106583ms step_avg:60.25ms
step:1770/1880 train_time:106670ms step_avg:60.27ms
step:1771/1880 train_time:106757ms step_avg:60.28ms
step:1772/1880 train_time:106844ms step_avg:60.30ms
step:1773/1880 train_time:106933ms step_avg:60.31ms
step:1774/1880 train_time:107022ms step_avg:60.33ms
step:1775/1880 train_time:107111ms step_avg:60.34ms
step:1776/1880 train_time:107201ms step_avg:60.36ms
step:1777/1880 train_time:107290ms step_avg:60.38ms
step:1778/1880 train_time:107377ms step_avg:60.39ms
step:1779/1880 train_time:107465ms step_avg:60.41ms
step:1780/1880 train_time:107552ms step_avg:60.42ms
step:1781/1880 train_time:107640ms step_avg:60.44ms
step:1782/1880 train_time:107729ms step_avg:60.45ms
step:1783/1880 train_time:107816ms step_avg:60.47ms
step:1784/1880 train_time:107905ms step_avg:60.48ms
step:1785/1880 train_time:107992ms step_avg:60.50ms
step:1786/1880 train_time:108081ms step_avg:60.52ms
step:1787/1880 train_time:108171ms step_avg:60.53ms
step:1788/1880 train_time:108260ms step_avg:60.55ms
step:1789/1880 train_time:108350ms step_avg:60.56ms
step:1790/1880 train_time:108438ms step_avg:60.58ms
step:1791/1880 train_time:108527ms step_avg:60.60ms
step:1792/1880 train_time:108614ms step_avg:60.61ms
step:1793/1880 train_time:108702ms step_avg:60.63ms
step:1794/1880 train_time:108790ms step_avg:60.64ms
step:1795/1880 train_time:108877ms step_avg:60.66ms
step:1796/1880 train_time:108966ms step_avg:60.67ms
step:1797/1880 train_time:109054ms step_avg:60.69ms
step:1798/1880 train_time:109142ms step_avg:60.70ms
step:1799/1880 train_time:109232ms step_avg:60.72ms
step:1800/1880 train_time:109321ms step_avg:60.73ms
step:1801/1880 train_time:109410ms step_avg:60.75ms
step:1802/1880 train_time:109497ms step_avg:60.76ms
step:1803/1880 train_time:109585ms step_avg:60.78ms
step:1804/1880 train_time:109672ms step_avg:60.79ms
step:1805/1880 train_time:109760ms step_avg:60.81ms
step:1806/1880 train_time:109848ms step_avg:60.82ms
step:1807/1880 train_time:109935ms step_avg:60.84ms
step:1808/1880 train_time:110024ms step_avg:60.85ms
step:1809/1880 train_time:110112ms step_avg:60.87ms
step:1810/1880 train_time:110200ms step_avg:60.88ms
step:1811/1880 train_time:110289ms step_avg:60.90ms
step:1812/1880 train_time:110377ms step_avg:60.91ms
step:1813/1880 train_time:110466ms step_avg:60.93ms
step:1814/1880 train_time:110553ms step_avg:60.94ms
step:1815/1880 train_time:110641ms step_avg:60.96ms
step:1816/1880 train_time:110729ms step_avg:60.97ms
step:1817/1880 train_time:110817ms step_avg:60.99ms
step:1818/1880 train_time:110906ms step_avg:61.00ms
step:1819/1880 train_time:110994ms step_avg:61.02ms
step:1820/1880 train_time:111082ms step_avg:61.03ms
step:1821/1880 train_time:111172ms step_avg:61.05ms
step:1822/1880 train_time:111262ms step_avg:61.07ms
step:1823/1880 train_time:111352ms step_avg:61.08ms
step:1824/1880 train_time:111440ms step_avg:61.10ms
step:1825/1880 train_time:111529ms step_avg:61.11ms
step:1826/1880 train_time:111616ms step_avg:61.13ms
step:1827/1880 train_time:111704ms step_avg:61.14ms
step:1828/1880 train_time:111791ms step_avg:61.15ms
step:1829/1880 train_time:111879ms step_avg:61.17ms
step:1830/1880 train_time:111967ms step_avg:61.18ms
step:1831/1880 train_time:112055ms step_avg:61.20ms
step:1832/1880 train_time:112144ms step_avg:61.21ms
step:1833/1880 train_time:112233ms step_avg:61.23ms
step:1834/1880 train_time:112321ms step_avg:61.24ms
step:1835/1880 train_time:112410ms step_avg:61.26ms
step:1836/1880 train_time:112498ms step_avg:61.27ms
step:1837/1880 train_time:112587ms step_avg:61.29ms
step:1838/1880 train_time:112674ms step_avg:61.30ms
step:1839/1880 train_time:112761ms step_avg:61.32ms
step:1840/1880 train_time:112849ms step_avg:61.33ms
step:1841/1880 train_time:112938ms step_avg:61.35ms
step:1842/1880 train_time:113026ms step_avg:61.36ms
step:1843/1880 train_time:113114ms step_avg:61.37ms
step:1844/1880 train_time:113203ms step_avg:61.39ms
step:1845/1880 train_time:113292ms step_avg:61.40ms
step:1846/1880 train_time:113380ms step_avg:61.42ms
step:1847/1880 train_time:113470ms step_avg:61.44ms
step:1848/1880 train_time:113559ms step_avg:61.45ms
step:1849/1880 train_time:113648ms step_avg:61.46ms
step:1850/1880 train_time:113735ms step_avg:61.48ms
step:1851/1880 train_time:113824ms step_avg:61.49ms
step:1852/1880 train_time:113911ms step_avg:61.51ms
step:1853/1880 train_time:113999ms step_avg:61.52ms
step:1854/1880 train_time:114088ms step_avg:61.54ms
step:1855/1880 train_time:114176ms step_avg:61.55ms
step:1856/1880 train_time:114264ms step_avg:61.56ms
step:1857/1880 train_time:114353ms step_avg:61.58ms
step:1858/1880 train_time:114443ms step_avg:61.59ms
step:1859/1880 train_time:114532ms step_avg:61.61ms
step:1860/1880 train_time:114620ms step_avg:61.62ms
step:1861/1880 train_time:114709ms step_avg:61.64ms
step:1862/1880 train_time:114797ms step_avg:61.65ms
step:1863/1880 train_time:114886ms step_avg:61.67ms
step:1864/1880 train_time:114973ms step_avg:61.68ms
step:1865/1880 train_time:115062ms step_avg:61.70ms
step:1866/1880 train_time:115151ms step_avg:61.71ms
step:1867/1880 train_time:115239ms step_avg:61.72ms
step:1868/1880 train_time:115328ms step_avg:61.74ms
step:1869/1880 train_time:115416ms step_avg:61.75ms
step:1870/1880 train_time:115505ms step_avg:61.77ms
step:1871/1880 train_time:115593ms step_avg:61.78ms
step:1872/1880 train_time:115682ms step_avg:61.80ms
step:1873/1880 train_time:115770ms step_avg:61.81ms
step:1874/1880 train_time:115860ms step_avg:61.83ms
step:1875/1880 train_time:115950ms step_avg:61.84ms
step:1876/1880 train_time:116038ms step_avg:61.85ms
step:1877/1880 train_time:116127ms step_avg:61.87ms
step:1878/1880 train_time:116214ms step_avg:61.88ms
step:1879/1880 train_time:116303ms step_avg:61.90ms
step:1880/1880 train_time:116392ms step_avg:61.91ms
step:1880/1880 val_loss:3.2793 train_time:116483ms step_avg:61.96ms
peak memory allocated: 29709 MiB reserved: 44458 MiB

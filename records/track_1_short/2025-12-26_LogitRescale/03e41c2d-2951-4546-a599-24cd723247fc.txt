import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits+5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1840  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec 26 23:07:17 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            129W /  700W |    1520MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     65799      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     65800      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     65801      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     65802      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     65803      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     65804      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     65805      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     65806      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 613, 614, 615, 1226, 1227, 1228, 1839, 1840, 1841] for warmup
Resetting Model
step:0/1880 val_loss:10.8283 train_time:0ms step_avg:0.03ms
step:1/1880 train_time:79ms step_avg:79.41ms
step:2/1880 train_time:102ms step_avg:51.20ms
step:3/1880 train_time:123ms step_avg:40.98ms
step:4/1880 train_time:157ms step_avg:39.23ms
step:5/1880 train_time:190ms step_avg:38.10ms
step:6/1880 train_time:278ms step_avg:46.33ms
step:7/1880 train_time:400ms step_avg:57.09ms
step:8/1880 train_time:434ms step_avg:54.21ms
step:9/1880 train_time:467ms step_avg:51.93ms
step:10/1880 train_time:501ms step_avg:50.14ms
step:11/1880 train_time:535ms step_avg:48.67ms
step:12/1880 train_time:569ms step_avg:47.45ms
step:13/1880 train_time:603ms step_avg:46.42ms
step:14/1880 train_time:638ms step_avg:45.55ms
step:15/1880 train_time:671ms step_avg:44.76ms
step:16/1880 train_time:705ms step_avg:44.09ms
step:17/1880 train_time:739ms step_avg:43.49ms
step:18/1880 train_time:773ms step_avg:42.96ms
step:19/1880 train_time:807ms step_avg:42.50ms
step:20/1880 train_time:842ms step_avg:42.09ms
step:21/1880 train_time:876ms step_avg:41.70ms
step:22/1880 train_time:910ms step_avg:41.38ms
step:23/1880 train_time:944ms step_avg:41.05ms
step:24/1880 train_time:978ms step_avg:40.77ms
step:25/1880 train_time:1012ms step_avg:40.49ms
step:26/1880 train_time:1046ms step_avg:40.24ms
step:27/1880 train_time:1080ms step_avg:40.00ms
step:28/1880 train_time:1114ms step_avg:39.79ms
step:29/1880 train_time:1148ms step_avg:39.59ms
step:30/1880 train_time:1182ms step_avg:39.41ms
step:31/1880 train_time:1216ms step_avg:39.23ms
step:32/1880 train_time:1250ms step_avg:39.08ms
step:33/1880 train_time:1284ms step_avg:38.92ms
step:34/1880 train_time:1319ms step_avg:38.80ms
step:35/1880 train_time:1354ms step_avg:38.67ms
step:36/1880 train_time:1388ms step_avg:38.57ms
step:37/1880 train_time:1423ms step_avg:38.46ms
step:38/1880 train_time:1457ms step_avg:38.35ms
step:39/1880 train_time:1491ms step_avg:38.24ms
step:40/1880 train_time:1526ms step_avg:38.14ms
step:41/1880 train_time:1560ms step_avg:38.05ms
step:42/1880 train_time:1594ms step_avg:37.96ms
step:43/1880 train_time:1628ms step_avg:37.87ms
step:44/1880 train_time:1663ms step_avg:37.79ms
step:45/1880 train_time:1697ms step_avg:37.71ms
step:46/1880 train_time:1731ms step_avg:37.64ms
step:47/1880 train_time:1765ms step_avg:37.56ms
step:48/1880 train_time:1800ms step_avg:37.49ms
step:49/1880 train_time:1834ms step_avg:37.42ms
step:50/1880 train_time:1868ms step_avg:37.36ms
step:51/1880 train_time:1902ms step_avg:37.30ms
step:52/1880 train_time:1936ms step_avg:37.24ms
step:53/1880 train_time:1971ms step_avg:37.18ms
step:54/1880 train_time:2005ms step_avg:37.13ms
step:55/1880 train_time:2039ms step_avg:37.08ms
step:56/1880 train_time:2073ms step_avg:37.03ms
step:57/1880 train_time:2108ms step_avg:36.98ms
step:58/1880 train_time:2142ms step_avg:36.93ms
step:59/1880 train_time:2176ms step_avg:36.87ms
step:60/1880 train_time:2210ms step_avg:36.83ms
step:61/1880 train_time:2244ms step_avg:36.78ms
step:62/1880 train_time:2278ms step_avg:36.74ms
step:63/1880 train_time:2312ms step_avg:36.70ms
step:64/1880 train_time:2347ms step_avg:36.67ms
step:65/1880 train_time:2381ms step_avg:36.63ms
step:66/1880 train_time:2416ms step_avg:36.60ms
step:67/1880 train_time:2450ms step_avg:36.56ms
step:68/1880 train_time:2484ms step_avg:36.53ms
step:69/1880 train_time:2518ms step_avg:36.49ms
step:70/1880 train_time:2552ms step_avg:36.46ms
step:71/1880 train_time:2586ms step_avg:36.42ms
step:72/1880 train_time:2620ms step_avg:36.39ms
step:73/1880 train_time:2654ms step_avg:36.36ms
step:74/1880 train_time:2689ms step_avg:36.34ms
step:75/1880 train_time:2723ms step_avg:36.31ms
step:76/1880 train_time:2757ms step_avg:36.28ms
step:77/1880 train_time:2791ms step_avg:36.25ms
step:78/1880 train_time:2826ms step_avg:36.23ms
step:79/1880 train_time:2860ms step_avg:36.20ms
step:80/1880 train_time:2894ms step_avg:36.18ms
step:81/1880 train_time:2928ms step_avg:36.15ms
step:82/1880 train_time:2963ms step_avg:36.13ms
step:83/1880 train_time:2998ms step_avg:36.12ms
step:84/1880 train_time:3031ms step_avg:36.08ms
step:85/1880 train_time:3065ms step_avg:36.06ms
step:86/1880 train_time:3100ms step_avg:36.04ms
step:87/1880 train_time:3134ms step_avg:36.02ms
step:88/1880 train_time:3168ms step_avg:36.00ms
step:89/1880 train_time:3202ms step_avg:35.98ms
step:90/1880 train_time:3237ms step_avg:35.96ms
step:91/1880 train_time:3271ms step_avg:35.94ms
step:92/1880 train_time:3306ms step_avg:35.93ms
step:93/1880 train_time:3339ms step_avg:35.91ms
step:94/1880 train_time:3373ms step_avg:35.89ms
step:95/1880 train_time:3408ms step_avg:35.87ms
step:96/1880 train_time:3442ms step_avg:35.86ms
step:97/1880 train_time:3476ms step_avg:35.84ms
step:98/1880 train_time:3511ms step_avg:35.83ms
step:99/1880 train_time:3545ms step_avg:35.81ms
step:100/1880 train_time:3579ms step_avg:35.79ms
step:101/1880 train_time:3613ms step_avg:35.77ms
step:102/1880 train_time:3648ms step_avg:35.76ms
step:103/1880 train_time:3682ms step_avg:35.74ms
step:104/1880 train_time:3716ms step_avg:35.73ms
step:105/1880 train_time:3750ms step_avg:35.71ms
step:106/1880 train_time:3784ms step_avg:35.70ms
step:107/1880 train_time:3818ms step_avg:35.69ms
step:108/1880 train_time:3853ms step_avg:35.67ms
step:109/1880 train_time:3887ms step_avg:35.66ms
step:110/1880 train_time:3921ms step_avg:35.65ms
step:111/1880 train_time:3955ms step_avg:35.63ms
step:112/1880 train_time:3989ms step_avg:35.62ms
step:113/1880 train_time:4023ms step_avg:35.60ms
step:114/1880 train_time:4057ms step_avg:35.59ms
step:115/1880 train_time:4091ms step_avg:35.58ms
step:116/1880 train_time:4126ms step_avg:35.57ms
step:117/1880 train_time:4160ms step_avg:35.55ms
step:118/1880 train_time:4194ms step_avg:35.54ms
step:119/1880 train_time:4228ms step_avg:35.53ms
step:120/1880 train_time:4262ms step_avg:35.52ms
step:121/1880 train_time:4296ms step_avg:35.50ms
step:122/1880 train_time:4330ms step_avg:35.49ms
step:123/1880 train_time:4364ms step_avg:35.48ms
step:124/1880 train_time:4398ms step_avg:35.47ms
step:125/1880 train_time:4433ms step_avg:35.46ms
step:126/1880 train_time:4467ms step_avg:35.45ms
step:127/1880 train_time:4501ms step_avg:35.44ms
step:128/1880 train_time:4536ms step_avg:35.44ms
step:129/1880 train_time:4569ms step_avg:35.42ms
step:130/1880 train_time:4604ms step_avg:35.41ms
step:131/1880 train_time:4637ms step_avg:35.40ms
step:132/1880 train_time:4672ms step_avg:35.39ms
step:133/1880 train_time:4706ms step_avg:35.38ms
step:134/1880 train_time:4740ms step_avg:35.37ms
step:135/1880 train_time:4774ms step_avg:35.36ms
step:136/1880 train_time:4808ms step_avg:35.36ms
step:137/1880 train_time:4842ms step_avg:35.34ms
step:138/1880 train_time:4876ms step_avg:35.34ms
step:139/1880 train_time:4910ms step_avg:35.33ms
step:140/1880 train_time:4945ms step_avg:35.32ms
step:141/1880 train_time:4979ms step_avg:35.31ms
step:142/1880 train_time:5013ms step_avg:35.30ms
step:143/1880 train_time:5047ms step_avg:35.29ms
step:144/1880 train_time:5081ms step_avg:35.28ms
step:145/1880 train_time:5115ms step_avg:35.27ms
step:146/1880 train_time:5149ms step_avg:35.27ms
step:147/1880 train_time:5183ms step_avg:35.26ms
step:148/1880 train_time:5217ms step_avg:35.25ms
step:149/1880 train_time:5251ms step_avg:35.24ms
step:150/1880 train_time:5286ms step_avg:35.24ms
step:151/1880 train_time:5319ms step_avg:35.23ms
step:152/1880 train_time:5353ms step_avg:35.22ms
step:153/1880 train_time:5387ms step_avg:35.21ms
step:154/1880 train_time:5422ms step_avg:35.21ms
step:155/1880 train_time:5455ms step_avg:35.20ms
step:156/1880 train_time:5489ms step_avg:35.19ms
step:157/1880 train_time:5523ms step_avg:35.18ms
step:158/1880 train_time:5558ms step_avg:35.17ms
step:159/1880 train_time:5591ms step_avg:35.17ms
step:160/1880 train_time:5625ms step_avg:35.16ms
step:161/1880 train_time:5659ms step_avg:35.15ms
step:162/1880 train_time:5693ms step_avg:35.14ms
step:163/1880 train_time:5727ms step_avg:35.14ms
step:164/1880 train_time:5761ms step_avg:35.13ms
step:165/1880 train_time:5795ms step_avg:35.12ms
step:166/1880 train_time:5829ms step_avg:35.12ms
step:167/1880 train_time:5864ms step_avg:35.11ms
step:168/1880 train_time:5898ms step_avg:35.11ms
step:169/1880 train_time:5932ms step_avg:35.10ms
step:170/1880 train_time:5967ms step_avg:35.10ms
step:171/1880 train_time:6001ms step_avg:35.09ms
step:172/1880 train_time:6035ms step_avg:35.09ms
step:173/1880 train_time:6069ms step_avg:35.08ms
step:174/1880 train_time:6103ms step_avg:35.08ms
step:175/1880 train_time:6137ms step_avg:35.07ms
step:176/1880 train_time:6171ms step_avg:35.06ms
step:177/1880 train_time:6205ms step_avg:35.06ms
step:178/1880 train_time:6239ms step_avg:35.05ms
step:179/1880 train_time:6273ms step_avg:35.04ms
step:180/1880 train_time:6307ms step_avg:35.04ms
step:181/1880 train_time:6341ms step_avg:35.03ms
step:182/1880 train_time:6375ms step_avg:35.03ms
step:183/1880 train_time:6409ms step_avg:35.02ms
step:184/1880 train_time:6443ms step_avg:35.02ms
step:185/1880 train_time:6477ms step_avg:35.01ms
step:186/1880 train_time:6511ms step_avg:35.01ms
step:187/1880 train_time:6545ms step_avg:35.00ms
step:188/1880 train_time:6579ms step_avg:35.00ms
step:189/1880 train_time:6613ms step_avg:34.99ms
step:190/1880 train_time:6648ms step_avg:34.99ms
step:191/1880 train_time:6682ms step_avg:34.98ms
step:192/1880 train_time:6716ms step_avg:34.98ms
step:193/1880 train_time:6750ms step_avg:34.97ms
step:194/1880 train_time:6784ms step_avg:34.97ms
step:195/1880 train_time:6818ms step_avg:34.96ms
step:196/1880 train_time:6852ms step_avg:34.96ms
step:197/1880 train_time:6886ms step_avg:34.95ms
step:198/1880 train_time:6920ms step_avg:34.95ms
step:199/1880 train_time:6953ms step_avg:34.94ms
step:200/1880 train_time:6988ms step_avg:34.94ms
step:201/1880 train_time:7021ms step_avg:34.93ms
step:202/1880 train_time:7055ms step_avg:34.93ms
step:203/1880 train_time:7090ms step_avg:34.92ms
step:204/1880 train_time:7124ms step_avg:34.92ms
step:205/1880 train_time:7158ms step_avg:34.92ms
step:206/1880 train_time:7192ms step_avg:34.91ms
step:207/1880 train_time:7226ms step_avg:34.91ms
step:208/1880 train_time:7260ms step_avg:34.91ms
step:209/1880 train_time:7294ms step_avg:34.90ms
step:210/1880 train_time:7328ms step_avg:34.90ms
step:211/1880 train_time:7362ms step_avg:34.89ms
step:212/1880 train_time:7396ms step_avg:34.89ms
step:213/1880 train_time:7430ms step_avg:34.88ms
step:214/1880 train_time:7464ms step_avg:34.88ms
step:215/1880 train_time:7498ms step_avg:34.88ms
step:216/1880 train_time:7532ms step_avg:34.87ms
step:217/1880 train_time:7566ms step_avg:34.87ms
step:218/1880 train_time:7601ms step_avg:34.87ms
step:219/1880 train_time:7634ms step_avg:34.86ms
step:220/1880 train_time:7668ms step_avg:34.86ms
step:221/1880 train_time:7702ms step_avg:34.85ms
step:222/1880 train_time:7737ms step_avg:34.85ms
step:223/1880 train_time:7770ms step_avg:34.84ms
step:224/1880 train_time:7806ms step_avg:34.85ms
step:225/1880 train_time:7838ms step_avg:34.84ms
step:226/1880 train_time:7873ms step_avg:34.84ms
step:227/1880 train_time:7907ms step_avg:34.83ms
step:228/1880 train_time:7941ms step_avg:34.83ms
step:229/1880 train_time:7975ms step_avg:34.82ms
step:230/1880 train_time:8009ms step_avg:34.82ms
step:231/1880 train_time:8043ms step_avg:34.82ms
step:232/1880 train_time:8078ms step_avg:34.82ms
step:233/1880 train_time:8112ms step_avg:34.82ms
step:234/1880 train_time:8146ms step_avg:34.81ms
step:235/1880 train_time:8180ms step_avg:34.81ms
step:236/1880 train_time:8215ms step_avg:34.81ms
step:237/1880 train_time:8249ms step_avg:34.80ms
step:238/1880 train_time:8283ms step_avg:34.80ms
step:239/1880 train_time:8316ms step_avg:34.80ms
step:240/1880 train_time:8351ms step_avg:34.79ms
step:241/1880 train_time:8385ms step_avg:34.79ms
step:242/1880 train_time:8419ms step_avg:34.79ms
step:243/1880 train_time:8453ms step_avg:34.78ms
step:244/1880 train_time:8487ms step_avg:34.78ms
step:245/1880 train_time:8520ms step_avg:34.78ms
step:246/1880 train_time:8555ms step_avg:34.77ms
step:247/1880 train_time:8588ms step_avg:34.77ms
step:248/1880 train_time:8622ms step_avg:34.77ms
step:249/1880 train_time:8656ms step_avg:34.76ms
step:250/1880 train_time:8690ms step_avg:34.76ms
step:250/1880 val_loss:4.6170 train_time:8728ms step_avg:34.91ms
step:251/1880 train_time:8749ms step_avg:34.86ms
step:252/1880 train_time:8776ms step_avg:34.82ms
step:253/1880 train_time:8795ms step_avg:34.76ms
step:254/1880 train_time:8830ms step_avg:34.76ms
step:255/1880 train_time:8865ms step_avg:34.76ms
step:256/1880 train_time:8900ms step_avg:34.77ms
step:257/1880 train_time:8935ms step_avg:34.76ms
step:258/1880 train_time:8969ms step_avg:34.76ms
step:259/1880 train_time:9003ms step_avg:34.76ms
step:260/1880 train_time:9037ms step_avg:34.76ms
step:261/1880 train_time:9071ms step_avg:34.75ms
step:262/1880 train_time:9105ms step_avg:34.75ms
step:263/1880 train_time:9139ms step_avg:34.75ms
step:264/1880 train_time:9173ms step_avg:34.75ms
step:265/1880 train_time:9207ms step_avg:34.74ms
step:266/1880 train_time:9241ms step_avg:34.74ms
step:267/1880 train_time:9275ms step_avg:34.74ms
step:268/1880 train_time:9309ms step_avg:34.73ms
step:269/1880 train_time:9343ms step_avg:34.73ms
step:270/1880 train_time:9377ms step_avg:34.73ms
step:271/1880 train_time:9410ms step_avg:34.72ms
step:272/1880 train_time:9444ms step_avg:34.72ms
step:273/1880 train_time:9478ms step_avg:34.72ms
step:274/1880 train_time:9512ms step_avg:34.72ms
step:275/1880 train_time:9546ms step_avg:34.71ms
step:276/1880 train_time:9580ms step_avg:34.71ms
step:277/1880 train_time:9614ms step_avg:34.71ms
step:278/1880 train_time:9648ms step_avg:34.70ms
step:279/1880 train_time:9682ms step_avg:34.70ms
step:280/1880 train_time:9716ms step_avg:34.70ms
step:281/1880 train_time:9750ms step_avg:34.70ms
step:282/1880 train_time:9784ms step_avg:34.70ms
step:283/1880 train_time:9818ms step_avg:34.69ms
step:284/1880 train_time:9852ms step_avg:34.69ms
step:285/1880 train_time:9887ms step_avg:34.69ms
step:286/1880 train_time:9921ms step_avg:34.69ms
step:287/1880 train_time:9956ms step_avg:34.69ms
step:288/1880 train_time:9990ms step_avg:34.69ms
step:289/1880 train_time:10024ms step_avg:34.68ms
step:290/1880 train_time:10058ms step_avg:34.68ms
step:291/1880 train_time:10092ms step_avg:34.68ms
step:292/1880 train_time:10126ms step_avg:34.68ms
step:293/1880 train_time:10160ms step_avg:34.68ms
step:294/1880 train_time:10194ms step_avg:34.67ms
step:295/1880 train_time:10228ms step_avg:34.67ms
step:296/1880 train_time:10262ms step_avg:34.67ms
step:297/1880 train_time:10296ms step_avg:34.67ms
step:298/1880 train_time:10330ms step_avg:34.67ms
step:299/1880 train_time:10364ms step_avg:34.66ms
step:300/1880 train_time:10398ms step_avg:34.66ms
step:301/1880 train_time:10432ms step_avg:34.66ms
step:302/1880 train_time:10466ms step_avg:34.66ms
step:303/1880 train_time:10500ms step_avg:34.65ms
step:304/1880 train_time:10534ms step_avg:34.65ms
step:305/1880 train_time:10568ms step_avg:34.65ms
step:306/1880 train_time:10602ms step_avg:34.65ms
step:307/1880 train_time:10636ms step_avg:34.65ms
step:308/1880 train_time:10670ms step_avg:34.64ms
step:309/1880 train_time:10704ms step_avg:34.64ms
step:310/1880 train_time:10738ms step_avg:34.64ms
step:311/1880 train_time:10772ms step_avg:34.64ms
step:312/1880 train_time:10806ms step_avg:34.64ms
step:313/1880 train_time:10840ms step_avg:34.63ms
step:314/1880 train_time:10875ms step_avg:34.63ms
step:315/1880 train_time:10909ms step_avg:34.63ms
step:316/1880 train_time:10943ms step_avg:34.63ms
step:317/1880 train_time:10977ms step_avg:34.63ms
step:318/1880 train_time:11011ms step_avg:34.63ms
step:319/1880 train_time:11046ms step_avg:34.63ms
step:320/1880 train_time:11079ms step_avg:34.62ms
step:321/1880 train_time:11113ms step_avg:34.62ms
step:322/1880 train_time:11147ms step_avg:34.62ms
step:323/1880 train_time:11181ms step_avg:34.62ms
step:324/1880 train_time:11216ms step_avg:34.62ms
step:325/1880 train_time:11249ms step_avg:34.61ms
step:326/1880 train_time:11284ms step_avg:34.61ms
step:327/1880 train_time:11318ms step_avg:34.61ms
step:328/1880 train_time:11352ms step_avg:34.61ms
step:329/1880 train_time:11386ms step_avg:34.61ms
step:330/1880 train_time:11420ms step_avg:34.61ms
step:331/1880 train_time:11454ms step_avg:34.60ms
step:332/1880 train_time:11488ms step_avg:34.60ms
step:333/1880 train_time:11522ms step_avg:34.60ms
step:334/1880 train_time:11556ms step_avg:34.60ms
step:335/1880 train_time:11590ms step_avg:34.60ms
step:336/1880 train_time:11624ms step_avg:34.59ms
step:337/1880 train_time:11658ms step_avg:34.59ms
step:338/1880 train_time:11692ms step_avg:34.59ms
step:339/1880 train_time:11725ms step_avg:34.59ms
step:340/1880 train_time:11760ms step_avg:34.59ms
step:341/1880 train_time:11794ms step_avg:34.59ms
step:342/1880 train_time:11828ms step_avg:34.59ms
step:343/1880 train_time:11862ms step_avg:34.58ms
step:344/1880 train_time:11897ms step_avg:34.58ms
step:345/1880 train_time:11931ms step_avg:34.58ms
step:346/1880 train_time:11965ms step_avg:34.58ms
step:347/1880 train_time:11999ms step_avg:34.58ms
step:348/1880 train_time:12034ms step_avg:34.58ms
step:349/1880 train_time:12068ms step_avg:34.58ms
step:350/1880 train_time:12102ms step_avg:34.58ms
step:351/1880 train_time:12136ms step_avg:34.58ms
step:352/1880 train_time:12171ms step_avg:34.58ms
step:353/1880 train_time:12204ms step_avg:34.57ms
step:354/1880 train_time:12238ms step_avg:34.57ms
step:355/1880 train_time:12272ms step_avg:34.57ms
step:356/1880 train_time:12307ms step_avg:34.57ms
step:357/1880 train_time:12340ms step_avg:34.57ms
step:358/1880 train_time:12374ms step_avg:34.57ms
step:359/1880 train_time:12408ms step_avg:34.56ms
step:360/1880 train_time:12442ms step_avg:34.56ms
step:361/1880 train_time:12476ms step_avg:34.56ms
step:362/1880 train_time:12511ms step_avg:34.56ms
step:363/1880 train_time:12544ms step_avg:34.56ms
step:364/1880 train_time:12578ms step_avg:34.56ms
step:365/1880 train_time:12612ms step_avg:34.55ms
step:366/1880 train_time:12648ms step_avg:34.56ms
step:367/1880 train_time:12680ms step_avg:34.55ms
step:368/1880 train_time:12714ms step_avg:34.55ms
step:369/1880 train_time:12748ms step_avg:34.55ms
step:370/1880 train_time:12782ms step_avg:34.55ms
step:371/1880 train_time:12816ms step_avg:34.54ms
step:372/1880 train_time:12850ms step_avg:34.54ms
step:373/1880 train_time:12884ms step_avg:34.54ms
step:374/1880 train_time:12918ms step_avg:34.54ms
step:375/1880 train_time:12952ms step_avg:34.54ms
step:376/1880 train_time:12987ms step_avg:34.54ms
step:377/1880 train_time:13020ms step_avg:34.54ms
step:378/1880 train_time:13055ms step_avg:34.54ms
step:379/1880 train_time:13088ms step_avg:34.53ms
step:380/1880 train_time:13123ms step_avg:34.53ms
step:381/1880 train_time:13157ms step_avg:34.53ms
step:382/1880 train_time:13191ms step_avg:34.53ms
step:383/1880 train_time:13225ms step_avg:34.53ms
step:384/1880 train_time:13259ms step_avg:34.53ms
step:385/1880 train_time:13293ms step_avg:34.53ms
step:386/1880 train_time:13327ms step_avg:34.53ms
step:387/1880 train_time:13361ms step_avg:34.52ms
step:388/1880 train_time:13395ms step_avg:34.52ms
step:389/1880 train_time:13429ms step_avg:34.52ms
step:390/1880 train_time:13463ms step_avg:34.52ms
step:391/1880 train_time:13497ms step_avg:34.52ms
step:392/1880 train_time:13531ms step_avg:34.52ms
step:393/1880 train_time:13564ms step_avg:34.51ms
step:394/1880 train_time:13599ms step_avg:34.51ms
step:395/1880 train_time:13633ms step_avg:34.51ms
step:396/1880 train_time:13667ms step_avg:34.51ms
step:397/1880 train_time:13701ms step_avg:34.51ms
step:398/1880 train_time:13735ms step_avg:34.51ms
step:399/1880 train_time:13769ms step_avg:34.51ms
step:400/1880 train_time:13803ms step_avg:34.51ms
step:401/1880 train_time:13837ms step_avg:34.51ms
step:402/1880 train_time:13871ms step_avg:34.51ms
step:403/1880 train_time:13905ms step_avg:34.50ms
step:404/1880 train_time:13940ms step_avg:34.50ms
step:405/1880 train_time:13974ms step_avg:34.50ms
step:406/1880 train_time:14008ms step_avg:34.50ms
step:407/1880 train_time:14042ms step_avg:34.50ms
step:408/1880 train_time:14076ms step_avg:34.50ms
step:409/1880 train_time:14110ms step_avg:34.50ms
step:410/1880 train_time:14146ms step_avg:34.50ms
step:411/1880 train_time:14178ms step_avg:34.50ms
step:412/1880 train_time:14212ms step_avg:34.50ms
step:413/1880 train_time:14246ms step_avg:34.49ms
step:414/1880 train_time:14280ms step_avg:34.49ms
step:415/1880 train_time:14314ms step_avg:34.49ms
step:416/1880 train_time:14349ms step_avg:34.49ms
step:417/1880 train_time:14383ms step_avg:34.49ms
step:418/1880 train_time:14417ms step_avg:34.49ms
step:419/1880 train_time:14450ms step_avg:34.49ms
step:420/1880 train_time:14485ms step_avg:34.49ms
step:421/1880 train_time:14518ms step_avg:34.49ms
step:422/1880 train_time:14552ms step_avg:34.48ms
step:423/1880 train_time:14586ms step_avg:34.48ms
step:424/1880 train_time:14620ms step_avg:34.48ms
step:425/1880 train_time:14654ms step_avg:34.48ms
step:426/1880 train_time:14688ms step_avg:34.48ms
step:427/1880 train_time:14722ms step_avg:34.48ms
step:428/1880 train_time:14756ms step_avg:34.48ms
step:429/1880 train_time:14790ms step_avg:34.48ms
step:430/1880 train_time:14824ms step_avg:34.48ms
step:431/1880 train_time:14858ms step_avg:34.47ms
step:432/1880 train_time:14892ms step_avg:34.47ms
step:433/1880 train_time:14926ms step_avg:34.47ms
step:434/1880 train_time:14961ms step_avg:34.47ms
step:435/1880 train_time:14995ms step_avg:34.47ms
step:436/1880 train_time:15029ms step_avg:34.47ms
step:437/1880 train_time:15063ms step_avg:34.47ms
step:438/1880 train_time:15097ms step_avg:34.47ms
step:439/1880 train_time:15131ms step_avg:34.47ms
step:440/1880 train_time:15165ms step_avg:34.47ms
step:441/1880 train_time:15199ms step_avg:34.46ms
step:442/1880 train_time:15233ms step_avg:34.46ms
step:443/1880 train_time:15267ms step_avg:34.46ms
step:444/1880 train_time:15301ms step_avg:34.46ms
step:445/1880 train_time:15335ms step_avg:34.46ms
step:446/1880 train_time:15369ms step_avg:34.46ms
step:447/1880 train_time:15403ms step_avg:34.46ms
step:448/1880 train_time:15437ms step_avg:34.46ms
step:449/1880 train_time:15471ms step_avg:34.46ms
step:450/1880 train_time:15505ms step_avg:34.46ms
step:451/1880 train_time:15539ms step_avg:34.45ms
step:452/1880 train_time:15573ms step_avg:34.45ms
step:453/1880 train_time:15607ms step_avg:34.45ms
step:454/1880 train_time:15641ms step_avg:34.45ms
step:455/1880 train_time:15675ms step_avg:34.45ms
step:456/1880 train_time:15709ms step_avg:34.45ms
step:457/1880 train_time:15743ms step_avg:34.45ms
step:458/1880 train_time:15777ms step_avg:34.45ms
step:459/1880 train_time:15811ms step_avg:34.45ms
step:460/1880 train_time:15845ms step_avg:34.44ms
step:461/1880 train_time:15878ms step_avg:34.44ms
step:462/1880 train_time:15912ms step_avg:34.44ms
step:463/1880 train_time:15946ms step_avg:34.44ms
step:464/1880 train_time:15980ms step_avg:34.44ms
step:465/1880 train_time:16014ms step_avg:34.44ms
step:466/1880 train_time:16049ms step_avg:34.44ms
step:467/1880 train_time:16083ms step_avg:34.44ms
step:468/1880 train_time:16117ms step_avg:34.44ms
step:469/1880 train_time:16151ms step_avg:34.44ms
step:470/1880 train_time:16185ms step_avg:34.44ms
step:471/1880 train_time:16219ms step_avg:34.44ms
step:472/1880 train_time:16253ms step_avg:34.43ms
step:473/1880 train_time:16287ms step_avg:34.43ms
step:474/1880 train_time:16322ms step_avg:34.43ms
step:475/1880 train_time:16356ms step_avg:34.43ms
step:476/1880 train_time:16390ms step_avg:34.43ms
step:477/1880 train_time:16424ms step_avg:34.43ms
step:478/1880 train_time:16459ms step_avg:34.43ms
step:479/1880 train_time:16493ms step_avg:34.43ms
step:480/1880 train_time:16527ms step_avg:34.43ms
step:481/1880 train_time:16560ms step_avg:34.43ms
step:482/1880 train_time:16595ms step_avg:34.43ms
step:483/1880 train_time:16628ms step_avg:34.43ms
step:484/1880 train_time:16663ms step_avg:34.43ms
step:485/1880 train_time:16696ms step_avg:34.43ms
step:486/1880 train_time:16731ms step_avg:34.43ms
step:487/1880 train_time:16764ms step_avg:34.42ms
step:488/1880 train_time:16798ms step_avg:34.42ms
step:489/1880 train_time:16832ms step_avg:34.42ms
step:490/1880 train_time:16866ms step_avg:34.42ms
step:491/1880 train_time:16900ms step_avg:34.42ms
step:492/1880 train_time:16935ms step_avg:34.42ms
step:493/1880 train_time:16969ms step_avg:34.42ms
step:494/1880 train_time:17003ms step_avg:34.42ms
step:495/1880 train_time:17037ms step_avg:34.42ms
step:496/1880 train_time:17071ms step_avg:34.42ms
step:497/1880 train_time:17105ms step_avg:34.42ms
step:498/1880 train_time:17139ms step_avg:34.42ms
step:499/1880 train_time:17173ms step_avg:34.42ms
step:500/1880 train_time:17208ms step_avg:34.42ms
step:500/1880 val_loss:4.2989 train_time:17245ms step_avg:34.49ms
step:501/1880 train_time:17264ms step_avg:34.46ms
step:502/1880 train_time:17283ms step_avg:34.43ms
step:503/1880 train_time:17313ms step_avg:34.42ms
step:504/1880 train_time:17348ms step_avg:34.42ms
step:505/1880 train_time:17382ms step_avg:34.42ms
step:506/1880 train_time:17417ms step_avg:34.42ms
step:507/1880 train_time:17451ms step_avg:34.42ms
step:508/1880 train_time:17485ms step_avg:34.42ms
step:509/1880 train_time:17519ms step_avg:34.42ms
step:510/1880 train_time:17554ms step_avg:34.42ms
step:511/1880 train_time:17587ms step_avg:34.42ms
step:512/1880 train_time:17622ms step_avg:34.42ms
step:513/1880 train_time:17655ms step_avg:34.42ms
step:514/1880 train_time:17689ms step_avg:34.41ms
step:515/1880 train_time:17723ms step_avg:34.41ms
step:516/1880 train_time:17757ms step_avg:34.41ms
step:517/1880 train_time:17790ms step_avg:34.41ms
step:518/1880 train_time:17824ms step_avg:34.41ms
step:519/1880 train_time:17858ms step_avg:34.41ms
step:520/1880 train_time:17892ms step_avg:34.41ms
step:521/1880 train_time:17926ms step_avg:34.41ms
step:522/1880 train_time:17960ms step_avg:34.41ms
step:523/1880 train_time:17993ms step_avg:34.40ms
step:524/1880 train_time:18027ms step_avg:34.40ms
step:525/1880 train_time:18061ms step_avg:34.40ms
step:526/1880 train_time:18095ms step_avg:34.40ms
step:527/1880 train_time:18129ms step_avg:34.40ms
step:528/1880 train_time:18163ms step_avg:34.40ms
step:529/1880 train_time:18198ms step_avg:34.40ms
step:530/1880 train_time:18232ms step_avg:34.40ms
step:531/1880 train_time:18267ms step_avg:34.40ms
step:532/1880 train_time:18304ms step_avg:34.41ms
step:533/1880 train_time:18336ms step_avg:34.40ms
step:534/1880 train_time:18370ms step_avg:34.40ms
step:535/1880 train_time:18404ms step_avg:34.40ms
step:536/1880 train_time:18438ms step_avg:34.40ms
step:537/1880 train_time:18472ms step_avg:34.40ms
step:538/1880 train_time:18507ms step_avg:34.40ms
step:539/1880 train_time:18541ms step_avg:34.40ms
step:540/1880 train_time:18575ms step_avg:34.40ms
step:541/1880 train_time:18609ms step_avg:34.40ms
step:542/1880 train_time:18644ms step_avg:34.40ms
step:543/1880 train_time:18678ms step_avg:34.40ms
step:544/1880 train_time:18712ms step_avg:34.40ms
step:545/1880 train_time:18745ms step_avg:34.40ms
step:546/1880 train_time:18779ms step_avg:34.39ms
step:547/1880 train_time:18813ms step_avg:34.39ms
step:548/1880 train_time:18847ms step_avg:34.39ms
step:549/1880 train_time:18881ms step_avg:34.39ms
step:550/1880 train_time:18915ms step_avg:34.39ms
step:551/1880 train_time:18949ms step_avg:34.39ms
step:552/1880 train_time:18983ms step_avg:34.39ms
step:553/1880 train_time:19017ms step_avg:34.39ms
step:554/1880 train_time:19051ms step_avg:34.39ms
step:555/1880 train_time:19084ms step_avg:34.39ms
step:556/1880 train_time:19118ms step_avg:34.39ms
step:557/1880 train_time:19152ms step_avg:34.38ms
step:558/1880 train_time:19186ms step_avg:34.38ms
step:559/1880 train_time:19221ms step_avg:34.38ms
step:560/1880 train_time:19255ms step_avg:34.38ms
step:561/1880 train_time:19289ms step_avg:34.38ms
step:562/1880 train_time:19323ms step_avg:34.38ms
step:563/1880 train_time:19358ms step_avg:34.38ms
step:564/1880 train_time:19392ms step_avg:34.38ms
step:565/1880 train_time:19426ms step_avg:34.38ms
step:566/1880 train_time:19461ms step_avg:34.38ms
step:567/1880 train_time:19495ms step_avg:34.38ms
step:568/1880 train_time:19529ms step_avg:34.38ms
step:569/1880 train_time:19563ms step_avg:34.38ms
step:570/1880 train_time:19597ms step_avg:34.38ms
step:571/1880 train_time:19631ms step_avg:34.38ms
step:572/1880 train_time:19666ms step_avg:34.38ms
step:573/1880 train_time:19699ms step_avg:34.38ms
step:574/1880 train_time:19734ms step_avg:34.38ms
step:575/1880 train_time:19768ms step_avg:34.38ms
step:576/1880 train_time:19802ms step_avg:34.38ms
step:577/1880 train_time:19836ms step_avg:34.38ms
step:578/1880 train_time:19870ms step_avg:34.38ms
step:579/1880 train_time:19904ms step_avg:34.38ms
step:580/1880 train_time:19938ms step_avg:34.38ms
step:581/1880 train_time:19972ms step_avg:34.37ms
step:582/1880 train_time:20006ms step_avg:34.37ms
step:583/1880 train_time:20040ms step_avg:34.37ms
step:584/1880 train_time:20074ms step_avg:34.37ms
step:585/1880 train_time:20107ms step_avg:34.37ms
step:586/1880 train_time:20142ms step_avg:34.37ms
step:587/1880 train_time:20176ms step_avg:34.37ms
step:588/1880 train_time:20210ms step_avg:34.37ms
step:589/1880 train_time:20244ms step_avg:34.37ms
step:590/1880 train_time:20278ms step_avg:34.37ms
step:591/1880 train_time:20311ms step_avg:34.37ms
step:592/1880 train_time:20346ms step_avg:34.37ms
step:593/1880 train_time:20380ms step_avg:34.37ms
step:594/1880 train_time:20414ms step_avg:34.37ms
step:595/1880 train_time:20448ms step_avg:34.37ms
step:596/1880 train_time:20483ms step_avg:34.37ms
step:597/1880 train_time:20517ms step_avg:34.37ms
step:598/1880 train_time:20551ms step_avg:34.37ms
step:599/1880 train_time:20585ms step_avg:34.37ms
step:600/1880 train_time:20620ms step_avg:34.37ms
step:601/1880 train_time:20654ms step_avg:34.37ms
step:602/1880 train_time:20688ms step_avg:34.36ms
step:603/1880 train_time:20722ms step_avg:34.36ms
step:604/1880 train_time:20756ms step_avg:34.36ms
step:605/1880 train_time:20789ms step_avg:34.36ms
step:606/1880 train_time:20823ms step_avg:34.36ms
step:607/1880 train_time:20857ms step_avg:34.36ms
step:608/1880 train_time:20892ms step_avg:34.36ms
step:609/1880 train_time:20925ms step_avg:34.36ms
step:610/1880 train_time:20960ms step_avg:34.36ms
step:611/1880 train_time:20993ms step_avg:34.36ms
step:612/1880 train_time:21027ms step_avg:34.36ms
step:613/1880 train_time:21061ms step_avg:34.36ms
step:614/1880 train_time:21096ms step_avg:34.36ms
step:615/1880 train_time:21130ms step_avg:34.36ms
step:616/1880 train_time:21191ms step_avg:34.40ms
step:617/1880 train_time:21254ms step_avg:34.45ms
step:618/1880 train_time:21316ms step_avg:34.49ms
step:619/1880 train_time:21378ms step_avg:34.54ms
step:620/1880 train_time:21439ms step_avg:34.58ms
step:621/1880 train_time:21501ms step_avg:34.62ms
step:622/1880 train_time:21562ms step_avg:34.67ms
step:623/1880 train_time:21624ms step_avg:34.71ms
step:624/1880 train_time:21685ms step_avg:34.75ms
step:625/1880 train_time:21747ms step_avg:34.80ms
step:626/1880 train_time:21808ms step_avg:34.84ms
step:627/1880 train_time:21871ms step_avg:34.88ms
step:628/1880 train_time:21932ms step_avg:34.92ms
step:629/1880 train_time:21994ms step_avg:34.97ms
step:630/1880 train_time:22055ms step_avg:35.01ms
step:631/1880 train_time:22117ms step_avg:35.05ms
step:632/1880 train_time:22178ms step_avg:35.09ms
step:633/1880 train_time:22240ms step_avg:35.13ms
step:634/1880 train_time:22302ms step_avg:35.18ms
step:635/1880 train_time:22363ms step_avg:35.22ms
step:636/1880 train_time:22424ms step_avg:35.26ms
step:637/1880 train_time:22486ms step_avg:35.30ms
step:638/1880 train_time:22548ms step_avg:35.34ms
step:639/1880 train_time:22610ms step_avg:35.38ms
step:640/1880 train_time:22672ms step_avg:35.42ms
step:641/1880 train_time:22734ms step_avg:35.47ms
step:642/1880 train_time:22795ms step_avg:35.51ms
step:643/1880 train_time:22857ms step_avg:35.55ms
step:644/1880 train_time:22918ms step_avg:35.59ms
step:645/1880 train_time:22980ms step_avg:35.63ms
step:646/1880 train_time:23041ms step_avg:35.67ms
step:647/1880 train_time:23103ms step_avg:35.71ms
step:648/1880 train_time:23164ms step_avg:35.75ms
step:649/1880 train_time:23225ms step_avg:35.79ms
step:650/1880 train_time:23286ms step_avg:35.82ms
step:651/1880 train_time:23349ms step_avg:35.87ms
step:652/1880 train_time:23410ms step_avg:35.91ms
step:653/1880 train_time:23472ms step_avg:35.94ms
step:654/1880 train_time:23533ms step_avg:35.98ms
step:655/1880 train_time:23596ms step_avg:36.03ms
step:656/1880 train_time:23657ms step_avg:36.06ms
step:657/1880 train_time:23719ms step_avg:36.10ms
step:658/1880 train_time:23780ms step_avg:36.14ms
step:659/1880 train_time:23842ms step_avg:36.18ms
step:660/1880 train_time:23903ms step_avg:36.22ms
step:661/1880 train_time:23965ms step_avg:36.26ms
step:662/1880 train_time:24027ms step_avg:36.29ms
step:663/1880 train_time:24089ms step_avg:36.33ms
step:664/1880 train_time:24150ms step_avg:36.37ms
step:665/1880 train_time:24212ms step_avg:36.41ms
step:666/1880 train_time:24274ms step_avg:36.45ms
step:667/1880 train_time:24336ms step_avg:36.49ms
step:668/1880 train_time:24396ms step_avg:36.52ms
step:669/1880 train_time:24457ms step_avg:36.56ms
step:670/1880 train_time:24519ms step_avg:36.60ms
step:671/1880 train_time:24581ms step_avg:36.63ms
step:672/1880 train_time:24643ms step_avg:36.67ms
step:673/1880 train_time:24704ms step_avg:36.71ms
step:674/1880 train_time:24765ms step_avg:36.74ms
step:675/1880 train_time:24827ms step_avg:36.78ms
step:676/1880 train_time:24889ms step_avg:36.82ms
step:677/1880 train_time:24951ms step_avg:36.85ms
step:678/1880 train_time:25012ms step_avg:36.89ms
step:679/1880 train_time:25074ms step_avg:36.93ms
step:680/1880 train_time:25135ms step_avg:36.96ms
step:681/1880 train_time:25199ms step_avg:37.00ms
step:682/1880 train_time:25258ms step_avg:37.04ms
step:683/1880 train_time:25319ms step_avg:37.07ms
step:684/1880 train_time:25381ms step_avg:37.11ms
step:685/1880 train_time:25442ms step_avg:37.14ms
step:686/1880 train_time:25504ms step_avg:37.18ms
step:687/1880 train_time:25565ms step_avg:37.21ms
step:688/1880 train_time:25627ms step_avg:37.25ms
step:689/1880 train_time:25690ms step_avg:37.29ms
step:690/1880 train_time:25751ms step_avg:37.32ms
step:691/1880 train_time:25813ms step_avg:37.36ms
step:692/1880 train_time:25874ms step_avg:37.39ms
step:693/1880 train_time:25936ms step_avg:37.43ms
step:694/1880 train_time:25997ms step_avg:37.46ms
step:695/1880 train_time:26059ms step_avg:37.50ms
step:696/1880 train_time:26121ms step_avg:37.53ms
step:697/1880 train_time:26182ms step_avg:37.56ms
step:698/1880 train_time:26244ms step_avg:37.60ms
step:699/1880 train_time:26306ms step_avg:37.63ms
step:700/1880 train_time:26368ms step_avg:37.67ms
step:701/1880 train_time:26430ms step_avg:37.70ms
step:702/1880 train_time:26491ms step_avg:37.74ms
step:703/1880 train_time:26553ms step_avg:37.77ms
step:704/1880 train_time:26614ms step_avg:37.80ms
step:705/1880 train_time:26676ms step_avg:37.84ms
step:706/1880 train_time:26738ms step_avg:37.87ms
step:707/1880 train_time:26800ms step_avg:37.91ms
step:708/1880 train_time:26860ms step_avg:37.94ms
step:709/1880 train_time:26922ms step_avg:37.97ms
step:710/1880 train_time:26983ms step_avg:38.00ms
step:711/1880 train_time:27045ms step_avg:38.04ms
step:712/1880 train_time:27108ms step_avg:38.07ms
step:713/1880 train_time:27169ms step_avg:38.11ms
step:714/1880 train_time:27232ms step_avg:38.14ms
step:715/1880 train_time:27294ms step_avg:38.17ms
step:716/1880 train_time:27356ms step_avg:38.21ms
step:717/1880 train_time:27417ms step_avg:38.24ms
step:718/1880 train_time:27478ms step_avg:38.27ms
step:719/1880 train_time:27540ms step_avg:38.30ms
step:720/1880 train_time:27600ms step_avg:38.33ms
step:721/1880 train_time:27662ms step_avg:38.37ms
step:722/1880 train_time:27724ms step_avg:38.40ms
step:723/1880 train_time:27785ms step_avg:38.43ms
step:724/1880 train_time:27847ms step_avg:38.46ms
step:725/1880 train_time:27909ms step_avg:38.50ms
step:726/1880 train_time:27971ms step_avg:38.53ms
step:727/1880 train_time:28033ms step_avg:38.56ms
step:728/1880 train_time:28094ms step_avg:38.59ms
step:729/1880 train_time:28156ms step_avg:38.62ms
step:730/1880 train_time:28218ms step_avg:38.65ms
step:731/1880 train_time:28280ms step_avg:38.69ms
step:732/1880 train_time:28341ms step_avg:38.72ms
step:733/1880 train_time:28403ms step_avg:38.75ms
step:734/1880 train_time:28465ms step_avg:38.78ms
step:735/1880 train_time:28527ms step_avg:38.81ms
step:736/1880 train_time:28589ms step_avg:38.84ms
step:737/1880 train_time:28651ms step_avg:38.88ms
step:738/1880 train_time:28713ms step_avg:38.91ms
step:739/1880 train_time:28774ms step_avg:38.94ms
step:740/1880 train_time:28835ms step_avg:38.97ms
step:741/1880 train_time:28898ms step_avg:39.00ms
step:742/1880 train_time:28958ms step_avg:39.03ms
step:743/1880 train_time:29021ms step_avg:39.06ms
step:744/1880 train_time:29083ms step_avg:39.09ms
step:745/1880 train_time:29145ms step_avg:39.12ms
step:746/1880 train_time:29206ms step_avg:39.15ms
step:747/1880 train_time:29269ms step_avg:39.18ms
step:748/1880 train_time:29331ms step_avg:39.21ms
step:749/1880 train_time:29393ms step_avg:39.24ms
step:750/1880 train_time:29454ms step_avg:39.27ms
step:750/1880 val_loss:4.0196 train_time:29518ms step_avg:39.36ms
step:751/1880 train_time:29541ms step_avg:39.34ms
step:752/1880 train_time:29580ms step_avg:39.33ms
step:753/1880 train_time:29643ms step_avg:39.37ms
step:754/1880 train_time:29707ms step_avg:39.40ms
step:755/1880 train_time:29768ms step_avg:39.43ms
step:756/1880 train_time:29830ms step_avg:39.46ms
step:757/1880 train_time:29891ms step_avg:39.49ms
step:758/1880 train_time:29952ms step_avg:39.51ms
step:759/1880 train_time:30014ms step_avg:39.54ms
step:760/1880 train_time:30075ms step_avg:39.57ms
step:761/1880 train_time:30137ms step_avg:39.60ms
step:762/1880 train_time:30198ms step_avg:39.63ms
step:763/1880 train_time:30258ms step_avg:39.66ms
step:764/1880 train_time:30319ms step_avg:39.68ms
step:765/1880 train_time:30381ms step_avg:39.71ms
step:766/1880 train_time:30443ms step_avg:39.74ms
step:767/1880 train_time:30507ms step_avg:39.77ms
step:768/1880 train_time:30570ms step_avg:39.80ms
step:769/1880 train_time:30632ms step_avg:39.83ms
step:770/1880 train_time:30694ms step_avg:39.86ms
step:771/1880 train_time:30756ms step_avg:39.89ms
step:772/1880 train_time:30817ms step_avg:39.92ms
step:773/1880 train_time:30879ms step_avg:39.95ms
step:774/1880 train_time:30941ms step_avg:39.98ms
step:775/1880 train_time:31003ms step_avg:40.00ms
step:776/1880 train_time:31064ms step_avg:40.03ms
step:777/1880 train_time:31126ms step_avg:40.06ms
step:778/1880 train_time:31187ms step_avg:40.09ms
step:779/1880 train_time:31249ms step_avg:40.11ms
step:780/1880 train_time:31311ms step_avg:40.14ms
step:781/1880 train_time:31372ms step_avg:40.17ms
step:782/1880 train_time:31434ms step_avg:40.20ms
step:783/1880 train_time:31496ms step_avg:40.22ms
step:784/1880 train_time:31557ms step_avg:40.25ms
step:785/1880 train_time:31619ms step_avg:40.28ms
step:786/1880 train_time:31681ms step_avg:40.31ms
step:787/1880 train_time:31743ms step_avg:40.33ms
step:788/1880 train_time:31805ms step_avg:40.36ms
step:789/1880 train_time:31868ms step_avg:40.39ms
step:790/1880 train_time:31929ms step_avg:40.42ms
step:791/1880 train_time:31990ms step_avg:40.44ms
step:792/1880 train_time:32052ms step_avg:40.47ms
step:793/1880 train_time:32113ms step_avg:40.50ms
step:794/1880 train_time:32174ms step_avg:40.52ms
step:795/1880 train_time:32236ms step_avg:40.55ms
step:796/1880 train_time:32297ms step_avg:40.57ms
step:797/1880 train_time:32359ms step_avg:40.60ms
step:798/1880 train_time:32420ms step_avg:40.63ms
step:799/1880 train_time:32482ms step_avg:40.65ms
step:800/1880 train_time:32544ms step_avg:40.68ms
step:801/1880 train_time:32606ms step_avg:40.71ms
step:802/1880 train_time:32668ms step_avg:40.73ms
step:803/1880 train_time:32730ms step_avg:40.76ms
step:804/1880 train_time:32792ms step_avg:40.79ms
step:805/1880 train_time:32854ms step_avg:40.81ms
step:806/1880 train_time:32915ms step_avg:40.84ms
step:807/1880 train_time:32977ms step_avg:40.86ms
step:808/1880 train_time:33040ms step_avg:40.89ms
step:809/1880 train_time:33099ms step_avg:40.91ms
step:810/1880 train_time:33160ms step_avg:40.94ms
step:811/1880 train_time:33222ms step_avg:40.96ms
step:812/1880 train_time:33284ms step_avg:40.99ms
step:813/1880 train_time:33346ms step_avg:41.02ms
step:814/1880 train_time:33408ms step_avg:41.04ms
step:815/1880 train_time:33470ms step_avg:41.07ms
step:816/1880 train_time:33531ms step_avg:41.09ms
step:817/1880 train_time:33593ms step_avg:41.12ms
step:818/1880 train_time:33655ms step_avg:41.14ms
step:819/1880 train_time:33717ms step_avg:41.17ms
step:820/1880 train_time:33779ms step_avg:41.19ms
step:821/1880 train_time:33841ms step_avg:41.22ms
step:822/1880 train_time:33902ms step_avg:41.24ms
step:823/1880 train_time:33964ms step_avg:41.27ms
step:824/1880 train_time:34025ms step_avg:41.29ms
step:825/1880 train_time:34087ms step_avg:41.32ms
step:826/1880 train_time:34149ms step_avg:41.34ms
step:827/1880 train_time:34211ms step_avg:41.37ms
step:828/1880 train_time:34273ms step_avg:41.39ms
step:829/1880 train_time:34334ms step_avg:41.42ms
step:830/1880 train_time:34394ms step_avg:41.44ms
step:831/1880 train_time:34456ms step_avg:41.46ms
step:832/1880 train_time:34518ms step_avg:41.49ms
step:833/1880 train_time:34580ms step_avg:41.51ms
step:834/1880 train_time:34641ms step_avg:41.54ms
step:835/1880 train_time:34703ms step_avg:41.56ms
step:836/1880 train_time:34765ms step_avg:41.59ms
step:837/1880 train_time:34827ms step_avg:41.61ms
step:838/1880 train_time:34889ms step_avg:41.63ms
step:839/1880 train_time:34951ms step_avg:41.66ms
step:840/1880 train_time:35012ms step_avg:41.68ms
step:841/1880 train_time:35074ms step_avg:41.70ms
step:842/1880 train_time:35135ms step_avg:41.73ms
step:843/1880 train_time:35197ms step_avg:41.75ms
step:844/1880 train_time:35258ms step_avg:41.77ms
step:845/1880 train_time:35319ms step_avg:41.80ms
step:846/1880 train_time:35381ms step_avg:41.82ms
step:847/1880 train_time:35444ms step_avg:41.85ms
step:848/1880 train_time:35505ms step_avg:41.87ms
step:849/1880 train_time:35566ms step_avg:41.89ms
step:850/1880 train_time:35628ms step_avg:41.92ms
step:851/1880 train_time:35691ms step_avg:41.94ms
step:852/1880 train_time:35752ms step_avg:41.96ms
step:853/1880 train_time:35814ms step_avg:41.99ms
step:854/1880 train_time:35875ms step_avg:42.01ms
step:855/1880 train_time:35937ms step_avg:42.03ms
step:856/1880 train_time:35998ms step_avg:42.05ms
step:857/1880 train_time:36060ms step_avg:42.08ms
step:858/1880 train_time:36121ms step_avg:42.10ms
step:859/1880 train_time:36182ms step_avg:42.12ms
step:860/1880 train_time:36243ms step_avg:42.14ms
step:861/1880 train_time:36305ms step_avg:42.17ms
step:862/1880 train_time:36367ms step_avg:42.19ms
step:863/1880 train_time:36430ms step_avg:42.21ms
step:864/1880 train_time:36492ms step_avg:42.24ms
step:865/1880 train_time:36554ms step_avg:42.26ms
step:866/1880 train_time:36615ms step_avg:42.28ms
step:867/1880 train_time:36677ms step_avg:42.30ms
step:868/1880 train_time:36738ms step_avg:42.33ms
step:869/1880 train_time:36800ms step_avg:42.35ms
step:870/1880 train_time:36862ms step_avg:42.37ms
step:871/1880 train_time:36924ms step_avg:42.39ms
step:872/1880 train_time:36985ms step_avg:42.41ms
step:873/1880 train_time:37048ms step_avg:42.44ms
step:874/1880 train_time:37110ms step_avg:42.46ms
step:875/1880 train_time:37172ms step_avg:42.48ms
step:876/1880 train_time:37233ms step_avg:42.50ms
step:877/1880 train_time:37295ms step_avg:42.53ms
step:878/1880 train_time:37357ms step_avg:42.55ms
step:879/1880 train_time:37419ms step_avg:42.57ms
step:880/1880 train_time:37480ms step_avg:42.59ms
step:881/1880 train_time:37543ms step_avg:42.61ms
step:882/1880 train_time:37605ms step_avg:42.64ms
step:883/1880 train_time:37667ms step_avg:42.66ms
step:884/1880 train_time:37728ms step_avg:42.68ms
step:885/1880 train_time:37790ms step_avg:42.70ms
step:886/1880 train_time:37852ms step_avg:42.72ms
step:887/1880 train_time:37913ms step_avg:42.74ms
step:888/1880 train_time:37974ms step_avg:42.76ms
step:889/1880 train_time:38036ms step_avg:42.79ms
step:890/1880 train_time:38097ms step_avg:42.81ms
step:891/1880 train_time:38159ms step_avg:42.83ms
step:892/1880 train_time:38220ms step_avg:42.85ms
step:893/1880 train_time:38282ms step_avg:42.87ms
step:894/1880 train_time:38343ms step_avg:42.89ms
step:895/1880 train_time:38406ms step_avg:42.91ms
step:896/1880 train_time:38467ms step_avg:42.93ms
step:897/1880 train_time:38529ms step_avg:42.95ms
step:898/1880 train_time:38591ms step_avg:42.97ms
step:899/1880 train_time:38653ms step_avg:43.00ms
step:900/1880 train_time:38714ms step_avg:43.02ms
step:901/1880 train_time:38776ms step_avg:43.04ms
step:902/1880 train_time:38836ms step_avg:43.06ms
step:903/1880 train_time:38898ms step_avg:43.08ms
step:904/1880 train_time:38959ms step_avg:43.10ms
step:905/1880 train_time:39021ms step_avg:43.12ms
step:906/1880 train_time:39082ms step_avg:43.14ms
step:907/1880 train_time:39145ms step_avg:43.16ms
step:908/1880 train_time:39207ms step_avg:43.18ms
step:909/1880 train_time:39268ms step_avg:43.20ms
step:910/1880 train_time:39331ms step_avg:43.22ms
step:911/1880 train_time:39392ms step_avg:43.24ms
step:912/1880 train_time:39453ms step_avg:43.26ms
step:913/1880 train_time:39515ms step_avg:43.28ms
step:914/1880 train_time:39576ms step_avg:43.30ms
step:915/1880 train_time:39637ms step_avg:43.32ms
step:916/1880 train_time:39699ms step_avg:43.34ms
step:917/1880 train_time:39761ms step_avg:43.36ms
step:918/1880 train_time:39823ms step_avg:43.38ms
step:919/1880 train_time:39886ms step_avg:43.40ms
step:920/1880 train_time:39947ms step_avg:43.42ms
step:921/1880 train_time:40009ms step_avg:43.44ms
step:922/1880 train_time:40071ms step_avg:43.46ms
step:923/1880 train_time:40132ms step_avg:43.48ms
step:924/1880 train_time:40193ms step_avg:43.50ms
step:925/1880 train_time:40255ms step_avg:43.52ms
step:926/1880 train_time:40315ms step_avg:43.54ms
step:927/1880 train_time:40377ms step_avg:43.56ms
step:928/1880 train_time:40438ms step_avg:43.58ms
step:929/1880 train_time:40500ms step_avg:43.60ms
step:930/1880 train_time:40560ms step_avg:43.61ms
step:931/1880 train_time:40622ms step_avg:43.63ms
step:932/1880 train_time:40684ms step_avg:43.65ms
step:933/1880 train_time:40746ms step_avg:43.67ms
step:934/1880 train_time:40808ms step_avg:43.69ms
step:935/1880 train_time:40870ms step_avg:43.71ms
step:936/1880 train_time:40932ms step_avg:43.73ms
step:937/1880 train_time:40994ms step_avg:43.75ms
step:938/1880 train_time:41056ms step_avg:43.77ms
step:939/1880 train_time:41117ms step_avg:43.79ms
step:940/1880 train_time:41179ms step_avg:43.81ms
step:941/1880 train_time:41241ms step_avg:43.83ms
step:942/1880 train_time:41303ms step_avg:43.85ms
step:943/1880 train_time:41364ms step_avg:43.86ms
step:944/1880 train_time:41427ms step_avg:43.88ms
step:945/1880 train_time:41489ms step_avg:43.90ms
step:946/1880 train_time:41550ms step_avg:43.92ms
step:947/1880 train_time:41612ms step_avg:43.94ms
step:948/1880 train_time:41673ms step_avg:43.96ms
step:949/1880 train_time:41734ms step_avg:43.98ms
step:950/1880 train_time:41795ms step_avg:44.00ms
step:951/1880 train_time:41857ms step_avg:44.01ms
step:952/1880 train_time:41918ms step_avg:44.03ms
step:953/1880 train_time:41980ms step_avg:44.05ms
step:954/1880 train_time:42042ms step_avg:44.07ms
step:955/1880 train_time:42104ms step_avg:44.09ms
step:956/1880 train_time:42166ms step_avg:44.11ms
step:957/1880 train_time:42228ms step_avg:44.13ms
step:958/1880 train_time:42290ms step_avg:44.14ms
step:959/1880 train_time:42351ms step_avg:44.16ms
step:960/1880 train_time:42413ms step_avg:44.18ms
step:961/1880 train_time:42475ms step_avg:44.20ms
step:962/1880 train_time:42536ms step_avg:44.22ms
step:963/1880 train_time:42598ms step_avg:44.23ms
step:964/1880 train_time:42659ms step_avg:44.25ms
step:965/1880 train_time:42720ms step_avg:44.27ms
step:966/1880 train_time:42782ms step_avg:44.29ms
step:967/1880 train_time:42844ms step_avg:44.31ms
step:968/1880 train_time:42906ms step_avg:44.32ms
step:969/1880 train_time:42968ms step_avg:44.34ms
step:970/1880 train_time:43030ms step_avg:44.36ms
step:971/1880 train_time:43092ms step_avg:44.38ms
step:972/1880 train_time:43153ms step_avg:44.40ms
step:973/1880 train_time:43215ms step_avg:44.41ms
step:974/1880 train_time:43276ms step_avg:44.43ms
step:975/1880 train_time:43338ms step_avg:44.45ms
step:976/1880 train_time:43399ms step_avg:44.47ms
step:977/1880 train_time:43461ms step_avg:44.48ms
step:978/1880 train_time:43523ms step_avg:44.50ms
step:979/1880 train_time:43585ms step_avg:44.52ms
step:980/1880 train_time:43649ms step_avg:44.54ms
step:981/1880 train_time:43709ms step_avg:44.56ms
step:982/1880 train_time:43771ms step_avg:44.57ms
step:983/1880 train_time:43832ms step_avg:44.59ms
step:984/1880 train_time:43893ms step_avg:44.61ms
step:985/1880 train_time:43955ms step_avg:44.62ms
step:986/1880 train_time:44016ms step_avg:44.64ms
step:987/1880 train_time:44078ms step_avg:44.66ms
step:988/1880 train_time:44140ms step_avg:44.68ms
step:989/1880 train_time:44201ms step_avg:44.69ms
step:990/1880 train_time:44262ms step_avg:44.71ms
step:991/1880 train_time:44324ms step_avg:44.73ms
step:992/1880 train_time:44386ms step_avg:44.74ms
step:993/1880 train_time:44447ms step_avg:44.76ms
step:994/1880 train_time:44509ms step_avg:44.78ms
step:995/1880 train_time:44572ms step_avg:44.80ms
step:996/1880 train_time:44633ms step_avg:44.81ms
step:997/1880 train_time:44694ms step_avg:44.83ms
step:998/1880 train_time:44755ms step_avg:44.84ms
step:999/1880 train_time:44816ms step_avg:44.86ms
step:1000/1880 train_time:44877ms step_avg:44.88ms
step:1000/1880 val_loss:3.7774 train_time:44941ms step_avg:44.94ms
step:1001/1880 train_time:44961ms step_avg:44.92ms
step:1002/1880 train_time:45004ms step_avg:44.91ms
step:1003/1880 train_time:45066ms step_avg:44.93ms
step:1004/1880 train_time:45128ms step_avg:44.95ms
step:1005/1880 train_time:45190ms step_avg:44.96ms
step:1006/1880 train_time:45251ms step_avg:44.98ms
step:1007/1880 train_time:45312ms step_avg:45.00ms
step:1008/1880 train_time:45373ms step_avg:45.01ms
step:1009/1880 train_time:45434ms step_avg:45.03ms
step:1010/1880 train_time:45495ms step_avg:45.04ms
step:1011/1880 train_time:45556ms step_avg:45.06ms
step:1012/1880 train_time:45617ms step_avg:45.08ms
step:1013/1880 train_time:45679ms step_avg:45.09ms
step:1014/1880 train_time:45741ms step_avg:45.11ms
step:1015/1880 train_time:45803ms step_avg:45.13ms
step:1016/1880 train_time:45865ms step_avg:45.14ms
step:1017/1880 train_time:45927ms step_avg:45.16ms
step:1018/1880 train_time:45990ms step_avg:45.18ms
step:1019/1880 train_time:46053ms step_avg:45.19ms
step:1020/1880 train_time:46115ms step_avg:45.21ms
step:1021/1880 train_time:46177ms step_avg:45.23ms
step:1022/1880 train_time:46239ms step_avg:45.24ms
step:1023/1880 train_time:46301ms step_avg:45.26ms
step:1024/1880 train_time:46362ms step_avg:45.28ms
step:1025/1880 train_time:46422ms step_avg:45.29ms
step:1026/1880 train_time:46484ms step_avg:45.31ms
step:1027/1880 train_time:46545ms step_avg:45.32ms
step:1028/1880 train_time:46606ms step_avg:45.34ms
step:1029/1880 train_time:46667ms step_avg:45.35ms
step:1030/1880 train_time:46728ms step_avg:45.37ms
step:1031/1880 train_time:46790ms step_avg:45.38ms
step:1032/1880 train_time:46851ms step_avg:45.40ms
step:1033/1880 train_time:46915ms step_avg:45.42ms
step:1034/1880 train_time:46977ms step_avg:45.43ms
step:1035/1880 train_time:47040ms step_avg:45.45ms
step:1036/1880 train_time:47102ms step_avg:45.47ms
step:1037/1880 train_time:47165ms step_avg:45.48ms
step:1038/1880 train_time:47226ms step_avg:45.50ms
step:1039/1880 train_time:47288ms step_avg:45.51ms
step:1040/1880 train_time:47349ms step_avg:45.53ms
step:1041/1880 train_time:47411ms step_avg:45.54ms
step:1042/1880 train_time:47472ms step_avg:45.56ms
step:1043/1880 train_time:47533ms step_avg:45.57ms
step:1044/1880 train_time:47595ms step_avg:45.59ms
step:1045/1880 train_time:47657ms step_avg:45.60ms
step:1046/1880 train_time:47718ms step_avg:45.62ms
step:1047/1880 train_time:47780ms step_avg:45.64ms
step:1048/1880 train_time:47842ms step_avg:45.65ms
step:1049/1880 train_time:47904ms step_avg:45.67ms
step:1050/1880 train_time:47965ms step_avg:45.68ms
step:1051/1880 train_time:48027ms step_avg:45.70ms
step:1052/1880 train_time:48089ms step_avg:45.71ms
step:1053/1880 train_time:48150ms step_avg:45.73ms
step:1054/1880 train_time:48213ms step_avg:45.74ms
step:1055/1880 train_time:48275ms step_avg:45.76ms
step:1056/1880 train_time:48337ms step_avg:45.77ms
step:1057/1880 train_time:48399ms step_avg:45.79ms
step:1058/1880 train_time:48461ms step_avg:45.80ms
step:1059/1880 train_time:48523ms step_avg:45.82ms
step:1060/1880 train_time:48584ms step_avg:45.83ms
step:1061/1880 train_time:48645ms step_avg:45.85ms
step:1062/1880 train_time:48706ms step_avg:45.86ms
step:1063/1880 train_time:48768ms step_avg:45.88ms
step:1064/1880 train_time:48829ms step_avg:45.89ms
step:1065/1880 train_time:48890ms step_avg:45.91ms
step:1066/1880 train_time:48952ms step_avg:45.92ms
step:1067/1880 train_time:49014ms step_avg:45.94ms
step:1068/1880 train_time:49077ms step_avg:45.95ms
step:1069/1880 train_time:49139ms step_avg:45.97ms
step:1070/1880 train_time:49202ms step_avg:45.98ms
step:1071/1880 train_time:49263ms step_avg:46.00ms
step:1072/1880 train_time:49324ms step_avg:46.01ms
step:1073/1880 train_time:49386ms step_avg:46.03ms
step:1074/1880 train_time:49447ms step_avg:46.04ms
step:1075/1880 train_time:49509ms step_avg:46.05ms
step:1076/1880 train_time:49571ms step_avg:46.07ms
step:1077/1880 train_time:49632ms step_avg:46.08ms
step:1078/1880 train_time:49693ms step_avg:46.10ms
step:1079/1880 train_time:49756ms step_avg:46.11ms
step:1080/1880 train_time:49817ms step_avg:46.13ms
step:1081/1880 train_time:49879ms step_avg:46.14ms
step:1082/1880 train_time:49941ms step_avg:46.16ms
step:1083/1880 train_time:50003ms step_avg:46.17ms
step:1084/1880 train_time:50064ms step_avg:46.18ms
step:1085/1880 train_time:50126ms step_avg:46.20ms
step:1086/1880 train_time:50187ms step_avg:46.21ms
step:1087/1880 train_time:50248ms step_avg:46.23ms
step:1088/1880 train_time:50310ms step_avg:46.24ms
step:1089/1880 train_time:50372ms step_avg:46.25ms
step:1090/1880 train_time:50433ms step_avg:46.27ms
step:1091/1880 train_time:50495ms step_avg:46.28ms
step:1092/1880 train_time:50558ms step_avg:46.30ms
step:1093/1880 train_time:50620ms step_avg:46.31ms
step:1094/1880 train_time:50684ms step_avg:46.33ms
step:1095/1880 train_time:50743ms step_avg:46.34ms
step:1096/1880 train_time:50805ms step_avg:46.36ms
step:1097/1880 train_time:50866ms step_avg:46.37ms
step:1098/1880 train_time:50927ms step_avg:46.38ms
step:1099/1880 train_time:50990ms step_avg:46.40ms
step:1100/1880 train_time:51052ms step_avg:46.41ms
step:1101/1880 train_time:51114ms step_avg:46.43ms
step:1102/1880 train_time:51177ms step_avg:46.44ms
step:1103/1880 train_time:51239ms step_avg:46.45ms
step:1104/1880 train_time:51301ms step_avg:46.47ms
step:1105/1880 train_time:51362ms step_avg:46.48ms
step:1106/1880 train_time:51424ms step_avg:46.50ms
step:1107/1880 train_time:51485ms step_avg:46.51ms
step:1108/1880 train_time:51547ms step_avg:46.52ms
step:1109/1880 train_time:51608ms step_avg:46.54ms
step:1110/1880 train_time:51670ms step_avg:46.55ms
step:1111/1880 train_time:51732ms step_avg:46.56ms
step:1112/1880 train_time:51793ms step_avg:46.58ms
step:1113/1880 train_time:51855ms step_avg:46.59ms
step:1114/1880 train_time:51917ms step_avg:46.60ms
step:1115/1880 train_time:51979ms step_avg:46.62ms
step:1116/1880 train_time:52042ms step_avg:46.63ms
step:1117/1880 train_time:52103ms step_avg:46.65ms
step:1118/1880 train_time:52164ms step_avg:46.66ms
step:1119/1880 train_time:52226ms step_avg:46.67ms
step:1120/1880 train_time:52288ms step_avg:46.69ms
step:1121/1880 train_time:52349ms step_avg:46.70ms
step:1122/1880 train_time:52411ms step_avg:46.71ms
step:1123/1880 train_time:52473ms step_avg:46.73ms
step:1124/1880 train_time:52535ms step_avg:46.74ms
step:1125/1880 train_time:52597ms step_avg:46.75ms
step:1126/1880 train_time:52659ms step_avg:46.77ms
step:1127/1880 train_time:52720ms step_avg:46.78ms
step:1128/1880 train_time:52781ms step_avg:46.79ms
step:1129/1880 train_time:52843ms step_avg:46.81ms
step:1130/1880 train_time:52905ms step_avg:46.82ms
step:1131/1880 train_time:52966ms step_avg:46.83ms
step:1132/1880 train_time:53027ms step_avg:46.84ms
step:1133/1880 train_time:53088ms step_avg:46.86ms
step:1134/1880 train_time:53149ms step_avg:46.87ms
step:1135/1880 train_time:53211ms step_avg:46.88ms
step:1136/1880 train_time:53273ms step_avg:46.89ms
step:1137/1880 train_time:53335ms step_avg:46.91ms
step:1138/1880 train_time:53398ms step_avg:46.92ms
step:1139/1880 train_time:53459ms step_avg:46.94ms
step:1140/1880 train_time:53521ms step_avg:46.95ms
step:1141/1880 train_time:53584ms step_avg:46.96ms
step:1142/1880 train_time:53644ms step_avg:46.97ms
step:1143/1880 train_time:53706ms step_avg:46.99ms
step:1144/1880 train_time:53767ms step_avg:47.00ms
step:1145/1880 train_time:53829ms step_avg:47.01ms
step:1146/1880 train_time:53891ms step_avg:47.03ms
step:1147/1880 train_time:53953ms step_avg:47.04ms
step:1148/1880 train_time:54014ms step_avg:47.05ms
step:1149/1880 train_time:54077ms step_avg:47.06ms
step:1150/1880 train_time:54138ms step_avg:47.08ms
step:1151/1880 train_time:54201ms step_avg:47.09ms
step:1152/1880 train_time:54262ms step_avg:47.10ms
step:1153/1880 train_time:54324ms step_avg:47.12ms
step:1154/1880 train_time:54385ms step_avg:47.13ms
step:1155/1880 train_time:54447ms step_avg:47.14ms
step:1156/1880 train_time:54508ms step_avg:47.15ms
step:1157/1880 train_time:54570ms step_avg:47.17ms
step:1158/1880 train_time:54631ms step_avg:47.18ms
step:1159/1880 train_time:54694ms step_avg:47.19ms
step:1160/1880 train_time:54755ms step_avg:47.20ms
step:1161/1880 train_time:54818ms step_avg:47.22ms
step:1162/1880 train_time:54879ms step_avg:47.23ms
step:1163/1880 train_time:54941ms step_avg:47.24ms
step:1164/1880 train_time:55002ms step_avg:47.25ms
step:1165/1880 train_time:55064ms step_avg:47.27ms
step:1166/1880 train_time:55126ms step_avg:47.28ms
step:1167/1880 train_time:55187ms step_avg:47.29ms
step:1168/1880 train_time:55248ms step_avg:47.30ms
step:1169/1880 train_time:55310ms step_avg:47.31ms
step:1170/1880 train_time:55371ms step_avg:47.33ms
step:1171/1880 train_time:55433ms step_avg:47.34ms
step:1172/1880 train_time:55495ms step_avg:47.35ms
step:1173/1880 train_time:55557ms step_avg:47.36ms
step:1174/1880 train_time:55618ms step_avg:47.37ms
step:1175/1880 train_time:55681ms step_avg:47.39ms
step:1176/1880 train_time:55742ms step_avg:47.40ms
step:1177/1880 train_time:55803ms step_avg:47.41ms
step:1178/1880 train_time:55864ms step_avg:47.42ms
step:1179/1880 train_time:55926ms step_avg:47.43ms
step:1180/1880 train_time:55987ms step_avg:47.45ms
step:1181/1880 train_time:56049ms step_avg:47.46ms
step:1182/1880 train_time:56111ms step_avg:47.47ms
step:1183/1880 train_time:56173ms step_avg:47.48ms
step:1184/1880 train_time:56234ms step_avg:47.49ms
step:1185/1880 train_time:56296ms step_avg:47.51ms
step:1186/1880 train_time:56357ms step_avg:47.52ms
step:1187/1880 train_time:56420ms step_avg:47.53ms
step:1188/1880 train_time:56482ms step_avg:47.54ms
step:1189/1880 train_time:56543ms step_avg:47.56ms
step:1190/1880 train_time:56605ms step_avg:47.57ms
step:1191/1880 train_time:56667ms step_avg:47.58ms
step:1192/1880 train_time:56728ms step_avg:47.59ms
step:1193/1880 train_time:56790ms step_avg:47.60ms
step:1194/1880 train_time:56851ms step_avg:47.61ms
step:1195/1880 train_time:56913ms step_avg:47.63ms
step:1196/1880 train_time:56974ms step_avg:47.64ms
step:1197/1880 train_time:57037ms step_avg:47.65ms
step:1198/1880 train_time:57099ms step_avg:47.66ms
step:1199/1880 train_time:57161ms step_avg:47.67ms
step:1200/1880 train_time:57223ms step_avg:47.69ms
step:1201/1880 train_time:57284ms step_avg:47.70ms
step:1202/1880 train_time:57346ms step_avg:47.71ms
step:1203/1880 train_time:57407ms step_avg:47.72ms
step:1204/1880 train_time:57468ms step_avg:47.73ms
step:1205/1880 train_time:57530ms step_avg:47.74ms
step:1206/1880 train_time:57591ms step_avg:47.75ms
step:1207/1880 train_time:57653ms step_avg:47.77ms
step:1208/1880 train_time:57716ms step_avg:47.78ms
step:1209/1880 train_time:57777ms step_avg:47.79ms
step:1210/1880 train_time:57840ms step_avg:47.80ms
step:1211/1880 train_time:57901ms step_avg:47.81ms
step:1212/1880 train_time:57963ms step_avg:47.82ms
step:1213/1880 train_time:58024ms step_avg:47.84ms
step:1214/1880 train_time:58086ms step_avg:47.85ms
step:1215/1880 train_time:58147ms step_avg:47.86ms
step:1216/1880 train_time:58209ms step_avg:47.87ms
step:1217/1880 train_time:58271ms step_avg:47.88ms
step:1218/1880 train_time:58332ms step_avg:47.89ms
step:1219/1880 train_time:58395ms step_avg:47.90ms
step:1220/1880 train_time:58456ms step_avg:47.92ms
step:1221/1880 train_time:58518ms step_avg:47.93ms
step:1222/1880 train_time:58580ms step_avg:47.94ms
step:1223/1880 train_time:58642ms step_avg:47.95ms
step:1224/1880 train_time:58704ms step_avg:47.96ms
step:1225/1880 train_time:58765ms step_avg:47.97ms
step:1226/1880 train_time:58826ms step_avg:47.98ms
step:1227/1880 train_time:58888ms step_avg:47.99ms
step:1228/1880 train_time:58949ms step_avg:48.00ms
step:1229/1880 train_time:59037ms step_avg:48.04ms
step:1230/1880 train_time:59127ms step_avg:48.07ms
step:1231/1880 train_time:59216ms step_avg:48.10ms
step:1232/1880 train_time:59305ms step_avg:48.14ms
step:1233/1880 train_time:59393ms step_avg:48.17ms
step:1234/1880 train_time:59480ms step_avg:48.20ms
step:1235/1880 train_time:59569ms step_avg:48.23ms
step:1236/1880 train_time:59656ms step_avg:48.27ms
step:1237/1880 train_time:59745ms step_avg:48.30ms
step:1238/1880 train_time:59834ms step_avg:48.33ms
step:1239/1880 train_time:59922ms step_avg:48.36ms
step:1240/1880 train_time:60010ms step_avg:48.40ms
step:1241/1880 train_time:60098ms step_avg:48.43ms
step:1242/1880 train_time:60186ms step_avg:48.46ms
step:1243/1880 train_time:60275ms step_avg:48.49ms
step:1244/1880 train_time:60364ms step_avg:48.52ms
step:1245/1880 train_time:60452ms step_avg:48.56ms
step:1246/1880 train_time:60540ms step_avg:48.59ms
step:1247/1880 train_time:60629ms step_avg:48.62ms
step:1248/1880 train_time:60716ms step_avg:48.65ms
step:1249/1880 train_time:60805ms step_avg:48.68ms
step:1250/1880 train_time:60894ms step_avg:48.71ms
step:1250/1880 val_loss:3.5374 train_time:60984ms step_avg:48.79ms
step:1251/1880 train_time:61005ms step_avg:48.77ms
step:1252/1880 train_time:61073ms step_avg:48.78ms
step:1253/1880 train_time:61164ms step_avg:48.81ms
step:1254/1880 train_time:61252ms step_avg:48.84ms
step:1255/1880 train_time:61339ms step_avg:48.88ms
step:1256/1880 train_time:61425ms step_avg:48.91ms
step:1257/1880 train_time:61513ms step_avg:48.94ms
step:1258/1880 train_time:61600ms step_avg:48.97ms
step:1259/1880 train_time:61688ms step_avg:49.00ms
step:1260/1880 train_time:61776ms step_avg:49.03ms
step:1261/1880 train_time:61864ms step_avg:49.06ms
step:1262/1880 train_time:61953ms step_avg:49.09ms
step:1263/1880 train_time:62043ms step_avg:49.12ms
step:1264/1880 train_time:62133ms step_avg:49.16ms
step:1265/1880 train_time:62223ms step_avg:49.19ms
step:1266/1880 train_time:62309ms step_avg:49.22ms
step:1267/1880 train_time:62396ms step_avg:49.25ms
step:1268/1880 train_time:62484ms step_avg:49.28ms
step:1269/1880 train_time:62570ms step_avg:49.31ms
step:1270/1880 train_time:62658ms step_avg:49.34ms
step:1271/1880 train_time:62745ms step_avg:49.37ms
step:1272/1880 train_time:62834ms step_avg:49.40ms
step:1273/1880 train_time:62924ms step_avg:49.43ms
step:1274/1880 train_time:63012ms step_avg:49.46ms
step:1275/1880 train_time:63102ms step_avg:49.49ms
step:1276/1880 train_time:63190ms step_avg:49.52ms
step:1277/1880 train_time:63279ms step_avg:49.55ms
step:1278/1880 train_time:63367ms step_avg:49.58ms
step:1279/1880 train_time:63454ms step_avg:49.61ms
step:1280/1880 train_time:63542ms step_avg:49.64ms
step:1281/1880 train_time:63629ms step_avg:49.67ms
step:1282/1880 train_time:63716ms step_avg:49.70ms
step:1283/1880 train_time:63805ms step_avg:49.73ms
step:1284/1880 train_time:63893ms step_avg:49.76ms
step:1285/1880 train_time:63982ms step_avg:49.79ms
step:1286/1880 train_time:64071ms step_avg:49.82ms
step:1287/1880 train_time:64160ms step_avg:49.85ms
step:1288/1880 train_time:64248ms step_avg:49.88ms
step:1289/1880 train_time:64337ms step_avg:49.91ms
step:1290/1880 train_time:64426ms step_avg:49.94ms
step:1291/1880 train_time:64512ms step_avg:49.97ms
step:1292/1880 train_time:64599ms step_avg:50.00ms
step:1293/1880 train_time:64687ms step_avg:50.03ms
step:1294/1880 train_time:64774ms step_avg:50.06ms
step:1295/1880 train_time:64863ms step_avg:50.09ms
step:1296/1880 train_time:64950ms step_avg:50.12ms
step:1297/1880 train_time:65039ms step_avg:50.15ms
step:1298/1880 train_time:65127ms step_avg:50.17ms
step:1299/1880 train_time:65215ms step_avg:50.20ms
step:1300/1880 train_time:65305ms step_avg:50.23ms
step:1301/1880 train_time:65393ms step_avg:50.26ms
step:1302/1880 train_time:65480ms step_avg:50.29ms
step:1303/1880 train_time:65568ms step_avg:50.32ms
step:1304/1880 train_time:65656ms step_avg:50.35ms
step:1305/1880 train_time:65744ms step_avg:50.38ms
step:1306/1880 train_time:65831ms step_avg:50.41ms
step:1307/1880 train_time:65920ms step_avg:50.44ms
step:1308/1880 train_time:66008ms step_avg:50.47ms
step:1309/1880 train_time:66097ms step_avg:50.49ms
step:1310/1880 train_time:66185ms step_avg:50.52ms
step:1311/1880 train_time:66273ms step_avg:50.55ms
step:1312/1880 train_time:66361ms step_avg:50.58ms
step:1313/1880 train_time:66448ms step_avg:50.61ms
step:1314/1880 train_time:66536ms step_avg:50.64ms
step:1315/1880 train_time:66624ms step_avg:50.66ms
step:1316/1880 train_time:66711ms step_avg:50.69ms
step:1317/1880 train_time:66800ms step_avg:50.72ms
step:1318/1880 train_time:66888ms step_avg:50.75ms
step:1319/1880 train_time:66976ms step_avg:50.78ms
step:1320/1880 train_time:67065ms step_avg:50.81ms
step:1321/1880 train_time:67154ms step_avg:50.84ms
step:1322/1880 train_time:67242ms step_avg:50.86ms
step:1323/1880 train_time:67330ms step_avg:50.89ms
step:1324/1880 train_time:67417ms step_avg:50.92ms
step:1325/1880 train_time:67505ms step_avg:50.95ms
step:1326/1880 train_time:67592ms step_avg:50.97ms
step:1327/1880 train_time:67681ms step_avg:51.00ms
step:1328/1880 train_time:67769ms step_avg:51.03ms
step:1329/1880 train_time:67857ms step_avg:51.06ms
step:1330/1880 train_time:67944ms step_avg:51.09ms
step:1331/1880 train_time:68032ms step_avg:51.11ms
step:1332/1880 train_time:68122ms step_avg:51.14ms
step:1333/1880 train_time:68209ms step_avg:51.17ms
step:1334/1880 train_time:68298ms step_avg:51.20ms
step:1335/1880 train_time:68386ms step_avg:51.23ms
step:1336/1880 train_time:68473ms step_avg:51.25ms
step:1337/1880 train_time:68561ms step_avg:51.28ms
step:1338/1880 train_time:68648ms step_avg:51.31ms
step:1339/1880 train_time:68736ms step_avg:51.33ms
step:1340/1880 train_time:68824ms step_avg:51.36ms
step:1341/1880 train_time:68912ms step_avg:51.39ms
step:1342/1880 train_time:69000ms step_avg:51.42ms
step:1343/1880 train_time:69089ms step_avg:51.44ms
step:1344/1880 train_time:69178ms step_avg:51.47ms
step:1345/1880 train_time:69267ms step_avg:51.50ms
step:1346/1880 train_time:69355ms step_avg:51.53ms
step:1347/1880 train_time:69443ms step_avg:51.55ms
step:1348/1880 train_time:69530ms step_avg:51.58ms
step:1349/1880 train_time:69619ms step_avg:51.61ms
step:1350/1880 train_time:69706ms step_avg:51.63ms
step:1351/1880 train_time:69792ms step_avg:51.66ms
step:1352/1880 train_time:69882ms step_avg:51.69ms
step:1353/1880 train_time:69970ms step_avg:51.71ms
step:1354/1880 train_time:70058ms step_avg:51.74ms
step:1355/1880 train_time:70146ms step_avg:51.77ms
step:1356/1880 train_time:70234ms step_avg:51.80ms
step:1357/1880 train_time:70323ms step_avg:51.82ms
step:1358/1880 train_time:70411ms step_avg:51.85ms
step:1359/1880 train_time:70499ms step_avg:51.88ms
step:1360/1880 train_time:70586ms step_avg:51.90ms
step:1361/1880 train_time:70674ms step_avg:51.93ms
step:1362/1880 train_time:70762ms step_avg:51.95ms
step:1363/1880 train_time:70850ms step_avg:51.98ms
step:1364/1880 train_time:70938ms step_avg:52.01ms
step:1365/1880 train_time:71026ms step_avg:52.03ms
step:1366/1880 train_time:71113ms step_avg:52.06ms
step:1367/1880 train_time:71202ms step_avg:52.09ms
step:1368/1880 train_time:71289ms step_avg:52.11ms
step:1369/1880 train_time:71378ms step_avg:52.14ms
step:1370/1880 train_time:71466ms step_avg:52.16ms
step:1371/1880 train_time:71554ms step_avg:52.19ms
step:1372/1880 train_time:71642ms step_avg:52.22ms
step:1373/1880 train_time:71729ms step_avg:52.24ms
step:1374/1880 train_time:71818ms step_avg:52.27ms
step:1375/1880 train_time:71907ms step_avg:52.30ms
step:1376/1880 train_time:71996ms step_avg:52.32ms
step:1377/1880 train_time:72084ms step_avg:52.35ms
step:1378/1880 train_time:72172ms step_avg:52.37ms
step:1379/1880 train_time:72260ms step_avg:52.40ms
step:1380/1880 train_time:72347ms step_avg:52.43ms
step:1381/1880 train_time:72435ms step_avg:52.45ms
step:1382/1880 train_time:72525ms step_avg:52.48ms
step:1383/1880 train_time:72612ms step_avg:52.50ms
step:1384/1880 train_time:72699ms step_avg:52.53ms
step:1385/1880 train_time:72787ms step_avg:52.55ms
step:1386/1880 train_time:72875ms step_avg:52.58ms
step:1387/1880 train_time:72964ms step_avg:52.61ms
step:1388/1880 train_time:73051ms step_avg:52.63ms
step:1389/1880 train_time:73140ms step_avg:52.66ms
step:1390/1880 train_time:73227ms step_avg:52.68ms
step:1391/1880 train_time:73316ms step_avg:52.71ms
step:1392/1880 train_time:73403ms step_avg:52.73ms
step:1393/1880 train_time:73491ms step_avg:52.76ms
step:1394/1880 train_time:73579ms step_avg:52.78ms
step:1395/1880 train_time:73667ms step_avg:52.81ms
step:1396/1880 train_time:73754ms step_avg:52.83ms
step:1397/1880 train_time:73843ms step_avg:52.86ms
step:1398/1880 train_time:73930ms step_avg:52.88ms
step:1399/1880 train_time:74018ms step_avg:52.91ms
step:1400/1880 train_time:74107ms step_avg:52.93ms
step:1401/1880 train_time:74195ms step_avg:52.96ms
step:1402/1880 train_time:74283ms step_avg:52.98ms
step:1403/1880 train_time:74371ms step_avg:53.01ms
step:1404/1880 train_time:74458ms step_avg:53.03ms
step:1405/1880 train_time:74546ms step_avg:53.06ms
step:1406/1880 train_time:74634ms step_avg:53.08ms
step:1407/1880 train_time:74722ms step_avg:53.11ms
step:1408/1880 train_time:74809ms step_avg:53.13ms
step:1409/1880 train_time:74896ms step_avg:53.16ms
step:1410/1880 train_time:74985ms step_avg:53.18ms
step:1411/1880 train_time:75073ms step_avg:53.21ms
step:1412/1880 train_time:75161ms step_avg:53.23ms
step:1413/1880 train_time:75249ms step_avg:53.25ms
step:1414/1880 train_time:75338ms step_avg:53.28ms
step:1415/1880 train_time:75426ms step_avg:53.30ms
step:1416/1880 train_time:75514ms step_avg:53.33ms
step:1417/1880 train_time:75602ms step_avg:53.35ms
step:1418/1880 train_time:75689ms step_avg:53.38ms
step:1419/1880 train_time:75776ms step_avg:53.40ms
step:1420/1880 train_time:75865ms step_avg:53.43ms
step:1421/1880 train_time:75952ms step_avg:53.45ms
step:1422/1880 train_time:76041ms step_avg:53.47ms
step:1423/1880 train_time:76130ms step_avg:53.50ms
step:1424/1880 train_time:76218ms step_avg:53.52ms
step:1425/1880 train_time:76307ms step_avg:53.55ms
step:1426/1880 train_time:76394ms step_avg:53.57ms
step:1427/1880 train_time:76482ms step_avg:53.60ms
step:1428/1880 train_time:76570ms step_avg:53.62ms
step:1429/1880 train_time:76658ms step_avg:53.64ms
step:1430/1880 train_time:76745ms step_avg:53.67ms
step:1431/1880 train_time:76832ms step_avg:53.69ms
step:1432/1880 train_time:76920ms step_avg:53.72ms
step:1433/1880 train_time:77008ms step_avg:53.74ms
step:1434/1880 train_time:77097ms step_avg:53.76ms
step:1435/1880 train_time:77187ms step_avg:53.79ms
step:1436/1880 train_time:77275ms step_avg:53.81ms
step:1437/1880 train_time:77362ms step_avg:53.84ms
step:1438/1880 train_time:77449ms step_avg:53.86ms
step:1439/1880 train_time:77537ms step_avg:53.88ms
step:1440/1880 train_time:77625ms step_avg:53.91ms
step:1441/1880 train_time:77712ms step_avg:53.93ms
step:1442/1880 train_time:77800ms step_avg:53.95ms
step:1443/1880 train_time:77888ms step_avg:53.98ms
step:1444/1880 train_time:77977ms step_avg:54.00ms
step:1445/1880 train_time:78066ms step_avg:54.02ms
step:1446/1880 train_time:78154ms step_avg:54.05ms
step:1447/1880 train_time:78243ms step_avg:54.07ms
step:1448/1880 train_time:78330ms step_avg:54.10ms
step:1449/1880 train_time:78418ms step_avg:54.12ms
step:1450/1880 train_time:78505ms step_avg:54.14ms
step:1451/1880 train_time:78592ms step_avg:54.16ms
step:1452/1880 train_time:78681ms step_avg:54.19ms
step:1453/1880 train_time:78769ms step_avg:54.21ms
step:1454/1880 train_time:78857ms step_avg:54.23ms
step:1455/1880 train_time:78946ms step_avg:54.26ms
step:1456/1880 train_time:79034ms step_avg:54.28ms
step:1457/1880 train_time:79123ms step_avg:54.31ms
step:1458/1880 train_time:79210ms step_avg:54.33ms
step:1459/1880 train_time:79299ms step_avg:54.35ms
step:1460/1880 train_time:79387ms step_avg:54.37ms
step:1461/1880 train_time:79475ms step_avg:54.40ms
step:1462/1880 train_time:79562ms step_avg:54.42ms
step:1463/1880 train_time:79650ms step_avg:54.44ms
step:1464/1880 train_time:79739ms step_avg:54.47ms
step:1465/1880 train_time:79826ms step_avg:54.49ms
step:1466/1880 train_time:79914ms step_avg:54.51ms
step:1467/1880 train_time:80002ms step_avg:54.53ms
step:1468/1880 train_time:80089ms step_avg:54.56ms
step:1469/1880 train_time:80177ms step_avg:54.58ms
step:1470/1880 train_time:80266ms step_avg:54.60ms
step:1471/1880 train_time:80354ms step_avg:54.63ms
step:1472/1880 train_time:80443ms step_avg:54.65ms
step:1473/1880 train_time:80530ms step_avg:54.67ms
step:1474/1880 train_time:80618ms step_avg:54.69ms
step:1475/1880 train_time:80705ms step_avg:54.72ms
step:1476/1880 train_time:80792ms step_avg:54.74ms
step:1477/1880 train_time:80881ms step_avg:54.76ms
step:1478/1880 train_time:80968ms step_avg:54.78ms
step:1479/1880 train_time:81056ms step_avg:54.80ms
step:1480/1880 train_time:81145ms step_avg:54.83ms
step:1481/1880 train_time:81233ms step_avg:54.85ms
step:1482/1880 train_time:81321ms step_avg:54.87ms
step:1483/1880 train_time:81410ms step_avg:54.90ms
step:1484/1880 train_time:81498ms step_avg:54.92ms
step:1485/1880 train_time:81586ms step_avg:54.94ms
step:1486/1880 train_time:81675ms step_avg:54.96ms
step:1487/1880 train_time:81763ms step_avg:54.99ms
step:1488/1880 train_time:81850ms step_avg:55.01ms
step:1489/1880 train_time:81937ms step_avg:55.03ms
step:1490/1880 train_time:82025ms step_avg:55.05ms
step:1491/1880 train_time:82113ms step_avg:55.07ms
step:1492/1880 train_time:82201ms step_avg:55.09ms
step:1493/1880 train_time:82290ms step_avg:55.12ms
step:1494/1880 train_time:82377ms step_avg:55.14ms
step:1495/1880 train_time:82466ms step_avg:55.16ms
step:1496/1880 train_time:82553ms step_avg:55.18ms
step:1497/1880 train_time:82642ms step_avg:55.21ms
step:1498/1880 train_time:82730ms step_avg:55.23ms
step:1499/1880 train_time:82817ms step_avg:55.25ms
step:1500/1880 train_time:82906ms step_avg:55.27ms
step:1500/1880 val_loss:3.4077 train_time:82996ms step_avg:55.33ms
step:1501/1880 train_time:83016ms step_avg:55.31ms
step:1502/1880 train_time:83086ms step_avg:55.32ms
step:1503/1880 train_time:83177ms step_avg:55.34ms
step:1504/1880 train_time:83265ms step_avg:55.36ms
step:1505/1880 train_time:83352ms step_avg:55.38ms
step:1506/1880 train_time:83438ms step_avg:55.40ms
step:1507/1880 train_time:83526ms step_avg:55.43ms
step:1508/1880 train_time:83613ms step_avg:55.45ms
step:1509/1880 train_time:83700ms step_avg:55.47ms
step:1510/1880 train_time:83788ms step_avg:55.49ms
step:1511/1880 train_time:83875ms step_avg:55.51ms
step:1512/1880 train_time:83964ms step_avg:55.53ms
step:1513/1880 train_time:84056ms step_avg:55.56ms
step:1514/1880 train_time:84148ms step_avg:55.58ms
step:1515/1880 train_time:84237ms step_avg:55.60ms
step:1516/1880 train_time:84324ms step_avg:55.62ms
step:1517/1880 train_time:84412ms step_avg:55.64ms
step:1518/1880 train_time:84500ms step_avg:55.67ms
step:1519/1880 train_time:84588ms step_avg:55.69ms
step:1520/1880 train_time:84675ms step_avg:55.71ms
step:1521/1880 train_time:84763ms step_avg:55.73ms
step:1522/1880 train_time:84851ms step_avg:55.75ms
step:1523/1880 train_time:84938ms step_avg:55.77ms
step:1524/1880 train_time:85028ms step_avg:55.79ms
step:1525/1880 train_time:85118ms step_avg:55.82ms
step:1526/1880 train_time:85207ms step_avg:55.84ms
step:1527/1880 train_time:85295ms step_avg:55.86ms
step:1528/1880 train_time:85382ms step_avg:55.88ms
step:1529/1880 train_time:85470ms step_avg:55.90ms
step:1530/1880 train_time:85557ms step_avg:55.92ms
step:1531/1880 train_time:85645ms step_avg:55.94ms
step:1532/1880 train_time:85733ms step_avg:55.96ms
step:1533/1880 train_time:85820ms step_avg:55.98ms
step:1534/1880 train_time:85908ms step_avg:56.00ms
step:1535/1880 train_time:85997ms step_avg:56.02ms
step:1536/1880 train_time:86086ms step_avg:56.05ms
step:1537/1880 train_time:86176ms step_avg:56.07ms
step:1538/1880 train_time:86265ms step_avg:56.09ms
step:1539/1880 train_time:86353ms step_avg:56.11ms
step:1540/1880 train_time:86440ms step_avg:56.13ms
step:1541/1880 train_time:86528ms step_avg:56.15ms
step:1542/1880 train_time:86615ms step_avg:56.17ms
step:1543/1880 train_time:86703ms step_avg:56.19ms
step:1544/1880 train_time:86791ms step_avg:56.21ms
step:1545/1880 train_time:86879ms step_avg:56.23ms
step:1546/1880 train_time:86967ms step_avg:56.25ms
step:1547/1880 train_time:87055ms step_avg:56.27ms
step:1548/1880 train_time:87145ms step_avg:56.29ms
step:1549/1880 train_time:87235ms step_avg:56.32ms
step:1550/1880 train_time:87323ms step_avg:56.34ms
step:1551/1880 train_time:87411ms step_avg:56.36ms
step:1552/1880 train_time:87498ms step_avg:56.38ms
step:1553/1880 train_time:87585ms step_avg:56.40ms
step:1554/1880 train_time:87673ms step_avg:56.42ms
step:1555/1880 train_time:87761ms step_avg:56.44ms
step:1556/1880 train_time:87850ms step_avg:56.46ms
step:1557/1880 train_time:87938ms step_avg:56.48ms
step:1558/1880 train_time:88026ms step_avg:56.50ms
step:1559/1880 train_time:88115ms step_avg:56.52ms
step:1560/1880 train_time:88203ms step_avg:56.54ms
step:1561/1880 train_time:88294ms step_avg:56.56ms
step:1562/1880 train_time:88381ms step_avg:56.58ms
step:1563/1880 train_time:88469ms step_avg:56.60ms
step:1564/1880 train_time:88556ms step_avg:56.62ms
step:1565/1880 train_time:88643ms step_avg:56.64ms
step:1566/1880 train_time:88732ms step_avg:56.66ms
step:1567/1880 train_time:88820ms step_avg:56.68ms
step:1568/1880 train_time:88908ms step_avg:56.70ms
step:1569/1880 train_time:88995ms step_avg:56.72ms
step:1570/1880 train_time:89084ms step_avg:56.74ms
step:1571/1880 train_time:89172ms step_avg:56.76ms
step:1572/1880 train_time:89261ms step_avg:56.78ms
step:1573/1880 train_time:89350ms step_avg:56.80ms
step:1574/1880 train_time:89437ms step_avg:56.82ms
step:1575/1880 train_time:89525ms step_avg:56.84ms
step:1576/1880 train_time:89613ms step_avg:56.86ms
step:1577/1880 train_time:89701ms step_avg:56.88ms
step:1578/1880 train_time:89789ms step_avg:56.90ms
step:1579/1880 train_time:89876ms step_avg:56.92ms
step:1580/1880 train_time:89965ms step_avg:56.94ms
step:1581/1880 train_time:90053ms step_avg:56.96ms
step:1582/1880 train_time:90141ms step_avg:56.98ms
step:1583/1880 train_time:90230ms step_avg:57.00ms
step:1584/1880 train_time:90317ms step_avg:57.02ms
step:1585/1880 train_time:90405ms step_avg:57.04ms
step:1586/1880 train_time:90493ms step_avg:57.06ms
step:1587/1880 train_time:90581ms step_avg:57.08ms
step:1588/1880 train_time:90669ms step_avg:57.10ms
step:1589/1880 train_time:90758ms step_avg:57.12ms
step:1590/1880 train_time:90846ms step_avg:57.14ms
step:1591/1880 train_time:90935ms step_avg:57.16ms
step:1592/1880 train_time:91023ms step_avg:57.18ms
step:1593/1880 train_time:91112ms step_avg:57.20ms
step:1594/1880 train_time:91201ms step_avg:57.22ms
step:1595/1880 train_time:91289ms step_avg:57.23ms
step:1596/1880 train_time:91377ms step_avg:57.25ms
step:1597/1880 train_time:91466ms step_avg:57.27ms
step:1598/1880 train_time:91554ms step_avg:57.29ms
step:1599/1880 train_time:91641ms step_avg:57.31ms
step:1600/1880 train_time:91730ms step_avg:57.33ms
step:1601/1880 train_time:91817ms step_avg:57.35ms
step:1602/1880 train_time:91905ms step_avg:57.37ms
step:1603/1880 train_time:91993ms step_avg:57.39ms
step:1604/1880 train_time:92081ms step_avg:57.41ms
step:1605/1880 train_time:92170ms step_avg:57.43ms
step:1606/1880 train_time:92258ms step_avg:57.45ms
step:1607/1880 train_time:92349ms step_avg:57.47ms
step:1608/1880 train_time:92434ms step_avg:57.48ms
step:1609/1880 train_time:92522ms step_avg:57.50ms
step:1610/1880 train_time:92610ms step_avg:57.52ms
step:1611/1880 train_time:92697ms step_avg:57.54ms
step:1612/1880 train_time:92786ms step_avg:57.56ms
step:1613/1880 train_time:92874ms step_avg:57.58ms
step:1614/1880 train_time:92964ms step_avg:57.60ms
step:1615/1880 train_time:93053ms step_avg:57.62ms
step:1616/1880 train_time:93140ms step_avg:57.64ms
step:1617/1880 train_time:93229ms step_avg:57.66ms
step:1618/1880 train_time:93317ms step_avg:57.67ms
step:1619/1880 train_time:93405ms step_avg:57.69ms
step:1620/1880 train_time:93493ms step_avg:57.71ms
step:1621/1880 train_time:93581ms step_avg:57.73ms
step:1622/1880 train_time:93669ms step_avg:57.75ms
step:1623/1880 train_time:93758ms step_avg:57.77ms
step:1624/1880 train_time:93848ms step_avg:57.79ms
step:1625/1880 train_time:93934ms step_avg:57.81ms
step:1626/1880 train_time:94022ms step_avg:57.82ms
step:1627/1880 train_time:94111ms step_avg:57.84ms
step:1628/1880 train_time:94198ms step_avg:57.86ms
step:1629/1880 train_time:94287ms step_avg:57.88ms
step:1630/1880 train_time:94374ms step_avg:57.90ms
step:1631/1880 train_time:94462ms step_avg:57.92ms
step:1632/1880 train_time:94552ms step_avg:57.94ms
step:1633/1880 train_time:94639ms step_avg:57.95ms
step:1634/1880 train_time:94727ms step_avg:57.97ms
step:1635/1880 train_time:94815ms step_avg:57.99ms
step:1636/1880 train_time:94904ms step_avg:58.01ms
step:1637/1880 train_time:94993ms step_avg:58.03ms
step:1638/1880 train_time:95081ms step_avg:58.05ms
step:1639/1880 train_time:95169ms step_avg:58.07ms
step:1640/1880 train_time:95257ms step_avg:58.08ms
step:1641/1880 train_time:95344ms step_avg:58.10ms
step:1642/1880 train_time:95432ms step_avg:58.12ms
step:1643/1880 train_time:95520ms step_avg:58.14ms
step:1644/1880 train_time:95608ms step_avg:58.16ms
step:1645/1880 train_time:95696ms step_avg:58.17ms
step:1646/1880 train_time:95784ms step_avg:58.19ms
step:1647/1880 train_time:95873ms step_avg:58.21ms
step:1648/1880 train_time:95961ms step_avg:58.23ms
step:1649/1880 train_time:96049ms step_avg:58.25ms
step:1650/1880 train_time:96137ms step_avg:58.26ms
step:1651/1880 train_time:96226ms step_avg:58.28ms
step:1652/1880 train_time:96314ms step_avg:58.30ms
step:1653/1880 train_time:96402ms step_avg:58.32ms
step:1654/1880 train_time:96490ms step_avg:58.34ms
step:1655/1880 train_time:96578ms step_avg:58.36ms
step:1656/1880 train_time:96665ms step_avg:58.37ms
step:1657/1880 train_time:96754ms step_avg:58.39ms
step:1658/1880 train_time:96843ms step_avg:58.41ms
step:1659/1880 train_time:96931ms step_avg:58.43ms
step:1660/1880 train_time:97018ms step_avg:58.44ms
step:1661/1880 train_time:97107ms step_avg:58.46ms
step:1662/1880 train_time:97195ms step_avg:58.48ms
step:1663/1880 train_time:97283ms step_avg:58.50ms
step:1664/1880 train_time:97372ms step_avg:58.52ms
step:1665/1880 train_time:97460ms step_avg:58.53ms
step:1666/1880 train_time:97549ms step_avg:58.55ms
step:1667/1880 train_time:97637ms step_avg:58.57ms
step:1668/1880 train_time:97725ms step_avg:58.59ms
step:1669/1880 train_time:97814ms step_avg:58.61ms
step:1670/1880 train_time:97902ms step_avg:58.62ms
step:1671/1880 train_time:97991ms step_avg:58.64ms
step:1672/1880 train_time:98078ms step_avg:58.66ms
step:1673/1880 train_time:98167ms step_avg:58.68ms
step:1674/1880 train_time:98254ms step_avg:58.69ms
step:1675/1880 train_time:98342ms step_avg:58.71ms
step:1676/1880 train_time:98431ms step_avg:58.73ms
step:1677/1880 train_time:98519ms step_avg:58.75ms
step:1678/1880 train_time:98606ms step_avg:58.76ms
step:1679/1880 train_time:98695ms step_avg:58.78ms
step:1680/1880 train_time:98784ms step_avg:58.80ms
step:1681/1880 train_time:98873ms step_avg:58.82ms
step:1682/1880 train_time:98962ms step_avg:58.84ms
step:1683/1880 train_time:99050ms step_avg:58.85ms
step:1684/1880 train_time:99137ms step_avg:58.87ms
step:1685/1880 train_time:99226ms step_avg:58.89ms
step:1686/1880 train_time:99314ms step_avg:58.90ms
step:1687/1880 train_time:99402ms step_avg:58.92ms
step:1688/1880 train_time:99490ms step_avg:58.94ms
step:1689/1880 train_time:99578ms step_avg:58.96ms
step:1690/1880 train_time:99666ms step_avg:58.97ms
step:1691/1880 train_time:99754ms step_avg:58.99ms
step:1692/1880 train_time:99843ms step_avg:59.01ms
step:1693/1880 train_time:99931ms step_avg:59.03ms
step:1694/1880 train_time:100019ms step_avg:59.04ms
step:1695/1880 train_time:100107ms step_avg:59.06ms
step:1696/1880 train_time:100195ms step_avg:59.08ms
step:1697/1880 train_time:100283ms step_avg:59.09ms
step:1698/1880 train_time:100372ms step_avg:59.11ms
step:1699/1880 train_time:100460ms step_avg:59.13ms
step:1700/1880 train_time:100548ms step_avg:59.15ms
step:1701/1880 train_time:100635ms step_avg:59.16ms
step:1702/1880 train_time:100724ms step_avg:59.18ms
step:1703/1880 train_time:100812ms step_avg:59.20ms
step:1704/1880 train_time:100900ms step_avg:59.21ms
step:1705/1880 train_time:100989ms step_avg:59.23ms
step:1706/1880 train_time:101077ms step_avg:59.25ms
step:1707/1880 train_time:101165ms step_avg:59.26ms
step:1708/1880 train_time:101253ms step_avg:59.28ms
step:1709/1880 train_time:101342ms step_avg:59.30ms
step:1710/1880 train_time:101429ms step_avg:59.32ms
step:1711/1880 train_time:101517ms step_avg:59.33ms
step:1712/1880 train_time:101605ms step_avg:59.35ms
step:1713/1880 train_time:101694ms step_avg:59.37ms
step:1714/1880 train_time:101783ms step_avg:59.38ms
step:1715/1880 train_time:101870ms step_avg:59.40ms
step:1716/1880 train_time:101958ms step_avg:59.42ms
step:1717/1880 train_time:102046ms step_avg:59.43ms
step:1718/1880 train_time:102133ms step_avg:59.45ms
step:1719/1880 train_time:102222ms step_avg:59.47ms
step:1720/1880 train_time:102310ms step_avg:59.48ms
step:1721/1880 train_time:102398ms step_avg:59.50ms
step:1722/1880 train_time:102487ms step_avg:59.52ms
step:1723/1880 train_time:102575ms step_avg:59.53ms
step:1724/1880 train_time:102664ms step_avg:59.55ms
step:1725/1880 train_time:102753ms step_avg:59.57ms
step:1726/1880 train_time:102840ms step_avg:59.58ms
step:1727/1880 train_time:102929ms step_avg:59.60ms
step:1728/1880 train_time:103016ms step_avg:59.62ms
step:1729/1880 train_time:103104ms step_avg:59.63ms
step:1730/1880 train_time:103192ms step_avg:59.65ms
step:1731/1880 train_time:103280ms step_avg:59.67ms
step:1732/1880 train_time:103368ms step_avg:59.68ms
step:1733/1880 train_time:103456ms step_avg:59.70ms
step:1734/1880 train_time:103544ms step_avg:59.71ms
step:1735/1880 train_time:103633ms step_avg:59.73ms
step:1736/1880 train_time:103721ms step_avg:59.75ms
step:1737/1880 train_time:103809ms step_avg:59.76ms
step:1738/1880 train_time:103896ms step_avg:59.78ms
step:1739/1880 train_time:103985ms step_avg:59.80ms
step:1740/1880 train_time:104073ms step_avg:59.81ms
step:1741/1880 train_time:104162ms step_avg:59.83ms
step:1742/1880 train_time:104250ms step_avg:59.84ms
step:1743/1880 train_time:104338ms step_avg:59.86ms
step:1744/1880 train_time:104426ms step_avg:59.88ms
step:1745/1880 train_time:104514ms step_avg:59.89ms
step:1746/1880 train_time:104604ms step_avg:59.91ms
step:1747/1880 train_time:104693ms step_avg:59.93ms
step:1748/1880 train_time:104781ms step_avg:59.94ms
step:1749/1880 train_time:104869ms step_avg:59.96ms
step:1750/1880 train_time:104956ms step_avg:59.97ms
step:1750/1880 val_loss:3.3127 train_time:105047ms step_avg:60.03ms
step:1751/1880 train_time:105067ms step_avg:60.00ms
step:1752/1880 train_time:105136ms step_avg:60.01ms
step:1753/1880 train_time:105227ms step_avg:60.03ms
step:1754/1880 train_time:105316ms step_avg:60.04ms
step:1755/1880 train_time:105403ms step_avg:60.06ms
step:1756/1880 train_time:105489ms step_avg:60.07ms
step:1757/1880 train_time:105576ms step_avg:60.09ms
step:1758/1880 train_time:105663ms step_avg:60.10ms
step:1759/1880 train_time:105752ms step_avg:60.12ms
step:1760/1880 train_time:105840ms step_avg:60.14ms
step:1761/1880 train_time:105927ms step_avg:60.15ms
step:1762/1880 train_time:106016ms step_avg:60.17ms
step:1763/1880 train_time:106106ms step_avg:60.18ms
step:1764/1880 train_time:106196ms step_avg:60.20ms
step:1765/1880 train_time:106285ms step_avg:60.22ms
step:1766/1880 train_time:106372ms step_avg:60.23ms
step:1767/1880 train_time:106462ms step_avg:60.25ms
step:1768/1880 train_time:106548ms step_avg:60.26ms
step:1769/1880 train_time:106637ms step_avg:60.28ms
step:1770/1880 train_time:106724ms step_avg:60.30ms
step:1771/1880 train_time:106811ms step_avg:60.31ms
step:1772/1880 train_time:106899ms step_avg:60.33ms
step:1773/1880 train_time:106987ms step_avg:60.34ms
step:1774/1880 train_time:107077ms step_avg:60.36ms
step:1775/1880 train_time:107165ms step_avg:60.37ms
step:1776/1880 train_time:107254ms step_avg:60.39ms
step:1777/1880 train_time:107342ms step_avg:60.41ms
step:1778/1880 train_time:107431ms step_avg:60.42ms
step:1779/1880 train_time:107520ms step_avg:60.44ms
step:1780/1880 train_time:107607ms step_avg:60.45ms
step:1781/1880 train_time:107695ms step_avg:60.47ms
step:1782/1880 train_time:107782ms step_avg:60.48ms
step:1783/1880 train_time:107870ms step_avg:60.50ms
step:1784/1880 train_time:107959ms step_avg:60.51ms
step:1785/1880 train_time:108047ms step_avg:60.53ms
step:1786/1880 train_time:108136ms step_avg:60.55ms
step:1787/1880 train_time:108225ms step_avg:60.56ms
step:1788/1880 train_time:108314ms step_avg:60.58ms
step:1789/1880 train_time:108403ms step_avg:60.59ms
step:1790/1880 train_time:108491ms step_avg:60.61ms
step:1791/1880 train_time:108578ms step_avg:60.62ms
step:1792/1880 train_time:108667ms step_avg:60.64ms
step:1793/1880 train_time:108756ms step_avg:60.66ms
step:1794/1880 train_time:108843ms step_avg:60.67ms
step:1795/1880 train_time:108931ms step_avg:60.69ms
step:1796/1880 train_time:109020ms step_avg:60.70ms
step:1797/1880 train_time:109109ms step_avg:60.72ms
step:1798/1880 train_time:109197ms step_avg:60.73ms
step:1799/1880 train_time:109285ms step_avg:60.75ms
step:1800/1880 train_time:109374ms step_avg:60.76ms
step:1801/1880 train_time:109464ms step_avg:60.78ms
step:1802/1880 train_time:109553ms step_avg:60.80ms
step:1803/1880 train_time:109642ms step_avg:60.81ms
step:1804/1880 train_time:109729ms step_avg:60.83ms
step:1805/1880 train_time:109817ms step_avg:60.84ms
step:1806/1880 train_time:109904ms step_avg:60.85ms
step:1807/1880 train_time:109993ms step_avg:60.87ms
step:1808/1880 train_time:110082ms step_avg:60.89ms
step:1809/1880 train_time:110170ms step_avg:60.90ms
step:1810/1880 train_time:110258ms step_avg:60.92ms
step:1811/1880 train_time:110345ms step_avg:60.93ms
step:1812/1880 train_time:110434ms step_avg:60.95ms
step:1813/1880 train_time:110523ms step_avg:60.96ms
step:1814/1880 train_time:110611ms step_avg:60.98ms
step:1815/1880 train_time:110698ms step_avg:60.99ms
step:1816/1880 train_time:110786ms step_avg:61.01ms
step:1817/1880 train_time:110874ms step_avg:61.02ms
step:1818/1880 train_time:110962ms step_avg:61.04ms
step:1819/1880 train_time:111051ms step_avg:61.05ms
step:1820/1880 train_time:111140ms step_avg:61.07ms
step:1821/1880 train_time:111228ms step_avg:61.08ms
step:1822/1880 train_time:111316ms step_avg:61.10ms
step:1823/1880 train_time:111405ms step_avg:61.11ms
step:1824/1880 train_time:111494ms step_avg:61.13ms
step:1825/1880 train_time:111584ms step_avg:61.14ms
step:1826/1880 train_time:111670ms step_avg:61.16ms
step:1827/1880 train_time:111758ms step_avg:61.17ms
step:1828/1880 train_time:111846ms step_avg:61.18ms
step:1829/1880 train_time:111934ms step_avg:61.20ms
step:1830/1880 train_time:112022ms step_avg:61.21ms
step:1831/1880 train_time:112110ms step_avg:61.23ms
step:1832/1880 train_time:112199ms step_avg:61.24ms
step:1833/1880 train_time:112287ms step_avg:61.26ms
step:1834/1880 train_time:112375ms step_avg:61.27ms
step:1835/1880 train_time:112464ms step_avg:61.29ms
step:1836/1880 train_time:112552ms step_avg:61.30ms
step:1837/1880 train_time:112640ms step_avg:61.32ms
step:1838/1880 train_time:112728ms step_avg:61.33ms
step:1839/1880 train_time:112817ms step_avg:61.35ms
step:1840/1880 train_time:112904ms step_avg:61.36ms
step:1841/1880 train_time:112994ms step_avg:61.38ms
step:1842/1880 train_time:113081ms step_avg:61.39ms
step:1843/1880 train_time:113170ms step_avg:61.41ms
step:1844/1880 train_time:113258ms step_avg:61.42ms
step:1845/1880 train_time:113346ms step_avg:61.43ms
step:1846/1880 train_time:113435ms step_avg:61.45ms
step:1847/1880 train_time:113525ms step_avg:61.46ms
step:1848/1880 train_time:113613ms step_avg:61.48ms
step:1849/1880 train_time:113702ms step_avg:61.49ms
step:1850/1880 train_time:113790ms step_avg:61.51ms
step:1851/1880 train_time:113881ms step_avg:61.52ms
step:1852/1880 train_time:113967ms step_avg:61.54ms
step:1853/1880 train_time:114056ms step_avg:61.55ms
step:1854/1880 train_time:114144ms step_avg:61.57ms
step:1855/1880 train_time:114232ms step_avg:61.58ms
step:1856/1880 train_time:114320ms step_avg:61.59ms
step:1857/1880 train_time:114408ms step_avg:61.61ms
step:1858/1880 train_time:114496ms step_avg:61.62ms
step:1859/1880 train_time:114584ms step_avg:61.64ms
step:1860/1880 train_time:114673ms step_avg:61.65ms
step:1861/1880 train_time:114763ms step_avg:61.67ms
step:1862/1880 train_time:114851ms step_avg:61.68ms
step:1863/1880 train_time:114940ms step_avg:61.70ms
step:1864/1880 train_time:115028ms step_avg:61.71ms
step:1865/1880 train_time:115118ms step_avg:61.73ms
step:1866/1880 train_time:115206ms step_avg:61.74ms
step:1867/1880 train_time:115295ms step_avg:61.75ms
step:1868/1880 train_time:115382ms step_avg:61.77ms
step:1869/1880 train_time:115469ms step_avg:61.78ms
step:1870/1880 train_time:115558ms step_avg:61.80ms
step:1871/1880 train_time:115647ms step_avg:61.81ms
step:1872/1880 train_time:115736ms step_avg:61.82ms
step:1873/1880 train_time:115824ms step_avg:61.84ms
step:1874/1880 train_time:115914ms step_avg:61.85ms
step:1875/1880 train_time:116003ms step_avg:61.87ms
step:1876/1880 train_time:116091ms step_avg:61.88ms
step:1877/1880 train_time:116181ms step_avg:61.90ms
step:1878/1880 train_time:116269ms step_avg:61.91ms
step:1879/1880 train_time:116358ms step_avg:61.93ms
step:1880/1880 train_time:116445ms step_avg:61.94ms
step:1880/1880 val_loss:3.2778 train_time:116536ms step_avg:61.99ms
peak memory allocated: 29709 MiB reserved: 44058 MiB

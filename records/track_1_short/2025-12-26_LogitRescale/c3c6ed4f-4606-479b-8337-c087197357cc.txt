import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits+5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1840  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec 26 23:18:32 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     72073      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     72074      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     72075      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     72076      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     72077      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     72078      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     72079      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     72080      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 613, 614, 615, 1226, 1227, 1228, 1839, 1840, 1841] for warmup
Resetting Model
step:0/1880 val_loss:10.8309 train_time:0ms step_avg:0.03ms
step:1/1880 train_time:70ms step_avg:69.89ms
step:2/1880 train_time:95ms step_avg:47.37ms
step:3/1880 train_time:116ms step_avg:38.71ms
step:4/1880 train_time:150ms step_avg:37.51ms
step:5/1880 train_time:184ms step_avg:36.78ms
step:6/1880 train_time:261ms step_avg:43.44ms
step:7/1880 train_time:374ms step_avg:53.46ms
step:8/1880 train_time:408ms step_avg:51.03ms
step:9/1880 train_time:442ms step_avg:49.10ms
step:10/1880 train_time:476ms step_avg:47.59ms
step:11/1880 train_time:510ms step_avg:46.32ms
step:12/1880 train_time:543ms step_avg:45.28ms
step:13/1880 train_time:577ms step_avg:44.42ms
step:14/1880 train_time:612ms step_avg:43.70ms
step:15/1880 train_time:646ms step_avg:43.05ms
step:16/1880 train_time:680ms step_avg:42.49ms
step:17/1880 train_time:714ms step_avg:41.98ms
step:18/1880 train_time:748ms step_avg:41.55ms
step:19/1880 train_time:782ms step_avg:41.14ms
step:20/1880 train_time:816ms step_avg:40.79ms
step:21/1880 train_time:850ms step_avg:40.46ms
step:22/1880 train_time:884ms step_avg:40.16ms
step:23/1880 train_time:917ms step_avg:39.88ms
step:24/1880 train_time:951ms step_avg:39.64ms
step:25/1880 train_time:985ms step_avg:39.40ms
step:26/1880 train_time:1019ms step_avg:39.20ms
step:27/1880 train_time:1053ms step_avg:39.00ms
step:28/1880 train_time:1087ms step_avg:38.83ms
step:29/1880 train_time:1121ms step_avg:38.65ms
step:30/1880 train_time:1155ms step_avg:38.50ms
step:31/1880 train_time:1189ms step_avg:38.36ms
step:32/1880 train_time:1223ms step_avg:38.23ms
step:33/1880 train_time:1257ms step_avg:38.10ms
step:34/1880 train_time:1292ms step_avg:38.01ms
step:35/1880 train_time:1327ms step_avg:37.92ms
step:36/1880 train_time:1362ms step_avg:37.85ms
step:37/1880 train_time:1397ms step_avg:37.75ms
step:38/1880 train_time:1432ms step_avg:37.67ms
step:39/1880 train_time:1466ms step_avg:37.58ms
step:40/1880 train_time:1500ms step_avg:37.50ms
step:41/1880 train_time:1534ms step_avg:37.41ms
step:42/1880 train_time:1568ms step_avg:37.34ms
step:43/1880 train_time:1602ms step_avg:37.26ms
step:44/1880 train_time:1636ms step_avg:37.19ms
step:45/1880 train_time:1670ms step_avg:37.11ms
step:46/1880 train_time:1704ms step_avg:37.05ms
step:47/1880 train_time:1738ms step_avg:36.98ms
step:48/1880 train_time:1772ms step_avg:36.93ms
step:49/1880 train_time:1806ms step_avg:36.87ms
step:50/1880 train_time:1841ms step_avg:36.82ms
step:51/1880 train_time:1875ms step_avg:36.76ms
step:52/1880 train_time:1909ms step_avg:36.71ms
step:53/1880 train_time:1943ms step_avg:36.66ms
step:54/1880 train_time:1977ms step_avg:36.61ms
step:55/1880 train_time:2011ms step_avg:36.56ms
step:56/1880 train_time:2045ms step_avg:36.52ms
step:57/1880 train_time:2079ms step_avg:36.47ms
step:58/1880 train_time:2113ms step_avg:36.43ms
step:59/1880 train_time:2147ms step_avg:36.39ms
step:60/1880 train_time:2181ms step_avg:36.35ms
step:61/1880 train_time:2215ms step_avg:36.31ms
step:62/1880 train_time:2250ms step_avg:36.28ms
step:63/1880 train_time:2284ms step_avg:36.25ms
step:64/1880 train_time:2318ms step_avg:36.22ms
step:65/1880 train_time:2352ms step_avg:36.19ms
step:66/1880 train_time:2387ms step_avg:36.16ms
step:67/1880 train_time:2421ms step_avg:36.13ms
step:68/1880 train_time:2455ms step_avg:36.10ms
step:69/1880 train_time:2489ms step_avg:36.07ms
step:70/1880 train_time:2523ms step_avg:36.04ms
step:71/1880 train_time:2557ms step_avg:36.01ms
step:72/1880 train_time:2591ms step_avg:35.99ms
step:73/1880 train_time:2625ms step_avg:35.96ms
step:74/1880 train_time:2659ms step_avg:35.94ms
step:75/1880 train_time:2694ms step_avg:35.91ms
step:76/1880 train_time:2728ms step_avg:35.89ms
step:77/1880 train_time:2762ms step_avg:35.87ms
step:78/1880 train_time:2796ms step_avg:35.85ms
step:79/1880 train_time:2830ms step_avg:35.83ms
step:80/1880 train_time:2864ms step_avg:35.81ms
step:81/1880 train_time:2899ms step_avg:35.79ms
step:82/1880 train_time:2933ms step_avg:35.77ms
step:83/1880 train_time:2967ms step_avg:35.75ms
step:84/1880 train_time:3001ms step_avg:35.73ms
step:85/1880 train_time:3035ms step_avg:35.70ms
step:86/1880 train_time:3069ms step_avg:35.68ms
step:87/1880 train_time:3103ms step_avg:35.66ms
step:88/1880 train_time:3137ms step_avg:35.64ms
step:89/1880 train_time:3171ms step_avg:35.62ms
step:90/1880 train_time:3205ms step_avg:35.61ms
step:91/1880 train_time:3239ms step_avg:35.59ms
step:92/1880 train_time:3273ms step_avg:35.57ms
step:93/1880 train_time:3307ms step_avg:35.56ms
step:94/1880 train_time:3342ms step_avg:35.55ms
step:95/1880 train_time:3376ms step_avg:35.53ms
step:96/1880 train_time:3410ms step_avg:35.52ms
step:97/1880 train_time:3444ms step_avg:35.50ms
step:98/1880 train_time:3478ms step_avg:35.49ms
step:99/1880 train_time:3512ms step_avg:35.48ms
step:100/1880 train_time:3546ms step_avg:35.46ms
step:101/1880 train_time:3580ms step_avg:35.44ms
step:102/1880 train_time:3614ms step_avg:35.44ms
step:103/1880 train_time:3648ms step_avg:35.42ms
step:104/1880 train_time:3682ms step_avg:35.41ms
step:105/1880 train_time:3716ms step_avg:35.39ms
step:106/1880 train_time:3751ms step_avg:35.38ms
step:107/1880 train_time:3785ms step_avg:35.37ms
step:108/1880 train_time:3819ms step_avg:35.36ms
step:109/1880 train_time:3852ms step_avg:35.34ms
step:110/1880 train_time:3887ms step_avg:35.33ms
step:111/1880 train_time:3921ms step_avg:35.32ms
step:112/1880 train_time:3955ms step_avg:35.32ms
step:113/1880 train_time:3989ms step_avg:35.30ms
step:114/1880 train_time:4023ms step_avg:35.29ms
step:115/1880 train_time:4057ms step_avg:35.28ms
step:116/1880 train_time:4091ms step_avg:35.27ms
step:117/1880 train_time:4125ms step_avg:35.26ms
step:118/1880 train_time:4160ms step_avg:35.25ms
step:119/1880 train_time:4193ms step_avg:35.24ms
step:120/1880 train_time:4228ms step_avg:35.23ms
step:121/1880 train_time:4261ms step_avg:35.22ms
step:122/1880 train_time:4295ms step_avg:35.21ms
step:123/1880 train_time:4329ms step_avg:35.19ms
step:124/1880 train_time:4363ms step_avg:35.19ms
step:125/1880 train_time:4397ms step_avg:35.18ms
step:126/1880 train_time:4431ms step_avg:35.17ms
step:127/1880 train_time:4465ms step_avg:35.16ms
step:128/1880 train_time:4499ms step_avg:35.15ms
step:129/1880 train_time:4533ms step_avg:35.14ms
step:130/1880 train_time:4567ms step_avg:35.13ms
step:131/1880 train_time:4601ms step_avg:35.12ms
step:132/1880 train_time:4635ms step_avg:35.11ms
step:133/1880 train_time:4669ms step_avg:35.11ms
step:134/1880 train_time:4704ms step_avg:35.10ms
step:135/1880 train_time:4738ms step_avg:35.09ms
step:136/1880 train_time:4772ms step_avg:35.09ms
step:137/1880 train_time:4806ms step_avg:35.08ms
step:138/1880 train_time:4841ms step_avg:35.08ms
step:139/1880 train_time:4875ms step_avg:35.07ms
step:140/1880 train_time:4909ms step_avg:35.06ms
step:141/1880 train_time:4942ms step_avg:35.05ms
step:142/1880 train_time:4976ms step_avg:35.05ms
step:143/1880 train_time:5011ms step_avg:35.04ms
step:144/1880 train_time:5045ms step_avg:35.03ms
step:145/1880 train_time:5078ms step_avg:35.02ms
step:146/1880 train_time:5113ms step_avg:35.02ms
step:147/1880 train_time:5146ms step_avg:35.01ms
step:148/1880 train_time:5181ms step_avg:35.00ms
step:149/1880 train_time:5214ms step_avg:35.00ms
step:150/1880 train_time:5249ms step_avg:34.99ms
step:151/1880 train_time:5282ms step_avg:34.98ms
step:152/1880 train_time:5316ms step_avg:34.98ms
step:153/1880 train_time:5351ms step_avg:34.97ms
step:154/1880 train_time:5385ms step_avg:34.97ms
step:155/1880 train_time:5419ms step_avg:34.96ms
step:156/1880 train_time:5453ms step_avg:34.95ms
step:157/1880 train_time:5487ms step_avg:34.95ms
step:158/1880 train_time:5521ms step_avg:34.94ms
step:159/1880 train_time:5554ms step_avg:34.93ms
step:160/1880 train_time:5588ms step_avg:34.93ms
step:161/1880 train_time:5622ms step_avg:34.92ms
step:162/1880 train_time:5656ms step_avg:34.91ms
step:163/1880 train_time:5690ms step_avg:34.91ms
step:164/1880 train_time:5724ms step_avg:34.90ms
step:165/1880 train_time:5758ms step_avg:34.90ms
step:166/1880 train_time:5793ms step_avg:34.90ms
step:167/1880 train_time:5827ms step_avg:34.89ms
step:168/1880 train_time:5861ms step_avg:34.89ms
step:169/1880 train_time:5895ms step_avg:34.88ms
step:170/1880 train_time:5929ms step_avg:34.88ms
step:171/1880 train_time:5963ms step_avg:34.87ms
step:172/1880 train_time:5997ms step_avg:34.87ms
step:173/1880 train_time:6031ms step_avg:34.86ms
step:174/1880 train_time:6065ms step_avg:34.86ms
step:175/1880 train_time:6099ms step_avg:34.85ms
step:176/1880 train_time:6133ms step_avg:34.85ms
step:177/1880 train_time:6167ms step_avg:34.84ms
step:178/1880 train_time:6201ms step_avg:34.84ms
step:179/1880 train_time:6235ms step_avg:34.83ms
step:180/1880 train_time:6269ms step_avg:34.83ms
step:181/1880 train_time:6303ms step_avg:34.82ms
step:182/1880 train_time:6337ms step_avg:34.82ms
step:183/1880 train_time:6371ms step_avg:34.82ms
step:184/1880 train_time:6405ms step_avg:34.81ms
step:185/1880 train_time:6439ms step_avg:34.81ms
step:186/1880 train_time:6473ms step_avg:34.80ms
step:187/1880 train_time:6507ms step_avg:34.80ms
step:188/1880 train_time:6542ms step_avg:34.80ms
step:189/1880 train_time:6575ms step_avg:34.79ms
step:190/1880 train_time:6610ms step_avg:34.79ms
step:191/1880 train_time:6643ms step_avg:34.78ms
step:192/1880 train_time:6677ms step_avg:34.78ms
step:193/1880 train_time:6711ms step_avg:34.77ms
step:194/1880 train_time:6745ms step_avg:34.77ms
step:195/1880 train_time:6779ms step_avg:34.76ms
step:196/1880 train_time:6813ms step_avg:34.76ms
step:197/1880 train_time:6847ms step_avg:34.76ms
step:198/1880 train_time:6881ms step_avg:34.75ms
step:199/1880 train_time:6915ms step_avg:34.75ms
step:200/1880 train_time:6949ms step_avg:34.75ms
step:201/1880 train_time:6983ms step_avg:34.74ms
step:202/1880 train_time:7017ms step_avg:34.74ms
step:203/1880 train_time:7051ms step_avg:34.73ms
step:204/1880 train_time:7085ms step_avg:34.73ms
step:205/1880 train_time:7119ms step_avg:34.72ms
step:206/1880 train_time:7153ms step_avg:34.72ms
step:207/1880 train_time:7187ms step_avg:34.72ms
step:208/1880 train_time:7221ms step_avg:34.72ms
step:209/1880 train_time:7255ms step_avg:34.71ms
step:210/1880 train_time:7290ms step_avg:34.71ms
step:211/1880 train_time:7324ms step_avg:34.71ms
step:212/1880 train_time:7358ms step_avg:34.71ms
step:213/1880 train_time:7392ms step_avg:34.70ms
step:214/1880 train_time:7426ms step_avg:34.70ms
step:215/1880 train_time:7460ms step_avg:34.70ms
step:216/1880 train_time:7494ms step_avg:34.69ms
step:217/1880 train_time:7528ms step_avg:34.69ms
step:218/1880 train_time:7562ms step_avg:34.69ms
step:219/1880 train_time:7596ms step_avg:34.68ms
step:220/1880 train_time:7630ms step_avg:34.68ms
step:221/1880 train_time:7663ms step_avg:34.68ms
step:222/1880 train_time:7698ms step_avg:34.67ms
step:223/1880 train_time:7731ms step_avg:34.67ms
step:224/1880 train_time:7766ms step_avg:34.67ms
step:225/1880 train_time:7799ms step_avg:34.66ms
step:226/1880 train_time:7833ms step_avg:34.66ms
step:227/1880 train_time:7868ms step_avg:34.66ms
step:228/1880 train_time:7902ms step_avg:34.66ms
step:229/1880 train_time:7936ms step_avg:34.65ms
step:230/1880 train_time:7970ms step_avg:34.65ms
step:231/1880 train_time:8004ms step_avg:34.65ms
step:232/1880 train_time:8038ms step_avg:34.65ms
step:233/1880 train_time:8071ms step_avg:34.64ms
step:234/1880 train_time:8106ms step_avg:34.64ms
step:235/1880 train_time:8140ms step_avg:34.64ms
step:236/1880 train_time:8174ms step_avg:34.63ms
step:237/1880 train_time:8207ms step_avg:34.63ms
step:238/1880 train_time:8241ms step_avg:34.63ms
step:239/1880 train_time:8275ms step_avg:34.62ms
step:240/1880 train_time:8309ms step_avg:34.62ms
step:241/1880 train_time:8343ms step_avg:34.62ms
step:242/1880 train_time:8377ms step_avg:34.62ms
step:243/1880 train_time:8411ms step_avg:34.61ms
step:244/1880 train_time:8445ms step_avg:34.61ms
step:245/1880 train_time:8479ms step_avg:34.61ms
step:246/1880 train_time:8513ms step_avg:34.61ms
step:247/1880 train_time:8547ms step_avg:34.61ms
step:248/1880 train_time:8582ms step_avg:34.60ms
step:249/1880 train_time:8615ms step_avg:34.60ms
step:250/1880 train_time:8650ms step_avg:34.60ms
step:250/1880 val_loss:4.5975 train_time:8686ms step_avg:34.75ms
step:251/1880 train_time:8706ms step_avg:34.68ms
step:252/1880 train_time:8726ms step_avg:34.63ms
step:253/1880 train_time:8755ms step_avg:34.60ms
step:254/1880 train_time:8789ms step_avg:34.60ms
step:255/1880 train_time:8823ms step_avg:34.60ms
step:256/1880 train_time:8858ms step_avg:34.60ms
step:257/1880 train_time:8892ms step_avg:34.60ms
step:258/1880 train_time:8926ms step_avg:34.60ms
step:259/1880 train_time:8960ms step_avg:34.59ms
step:260/1880 train_time:8994ms step_avg:34.59ms
step:261/1880 train_time:9028ms step_avg:34.59ms
step:262/1880 train_time:9062ms step_avg:34.59ms
step:263/1880 train_time:9095ms step_avg:34.58ms
step:264/1880 train_time:9130ms step_avg:34.58ms
step:265/1880 train_time:9163ms step_avg:34.58ms
step:266/1880 train_time:9197ms step_avg:34.58ms
step:267/1880 train_time:9231ms step_avg:34.57ms
step:268/1880 train_time:9265ms step_avg:34.57ms
step:269/1880 train_time:9299ms step_avg:34.57ms
step:270/1880 train_time:9333ms step_avg:34.57ms
step:271/1880 train_time:9367ms step_avg:34.56ms
step:272/1880 train_time:9401ms step_avg:34.56ms
step:273/1880 train_time:9434ms step_avg:34.56ms
step:274/1880 train_time:9468ms step_avg:34.55ms
step:275/1880 train_time:9502ms step_avg:34.55ms
step:276/1880 train_time:9536ms step_avg:34.55ms
step:277/1880 train_time:9570ms step_avg:34.55ms
step:278/1880 train_time:9604ms step_avg:34.55ms
step:279/1880 train_time:9638ms step_avg:34.54ms
step:280/1880 train_time:9672ms step_avg:34.54ms
step:281/1880 train_time:9707ms step_avg:34.54ms
step:282/1880 train_time:9741ms step_avg:34.54ms
step:283/1880 train_time:9775ms step_avg:34.54ms
step:284/1880 train_time:9810ms step_avg:34.54ms
step:285/1880 train_time:9844ms step_avg:34.54ms
step:286/1880 train_time:9878ms step_avg:34.54ms
step:287/1880 train_time:9912ms step_avg:34.54ms
step:288/1880 train_time:9946ms step_avg:34.54ms
step:289/1880 train_time:9980ms step_avg:34.53ms
step:290/1880 train_time:10014ms step_avg:34.53ms
step:291/1880 train_time:10048ms step_avg:34.53ms
step:292/1880 train_time:10082ms step_avg:34.53ms
step:293/1880 train_time:10116ms step_avg:34.53ms
step:294/1880 train_time:10150ms step_avg:34.52ms
step:295/1880 train_time:10184ms step_avg:34.52ms
step:296/1880 train_time:10217ms step_avg:34.52ms
step:297/1880 train_time:10251ms step_avg:34.52ms
step:298/1880 train_time:10286ms step_avg:34.52ms
step:299/1880 train_time:10320ms step_avg:34.51ms
step:300/1880 train_time:10354ms step_avg:34.51ms
step:301/1880 train_time:10387ms step_avg:34.51ms
step:302/1880 train_time:10421ms step_avg:34.51ms
step:303/1880 train_time:10455ms step_avg:34.51ms
step:304/1880 train_time:10489ms step_avg:34.50ms
step:305/1880 train_time:10523ms step_avg:34.50ms
step:306/1880 train_time:10557ms step_avg:34.50ms
step:307/1880 train_time:10591ms step_avg:34.50ms
step:308/1880 train_time:10625ms step_avg:34.50ms
step:309/1880 train_time:10659ms step_avg:34.50ms
step:310/1880 train_time:10693ms step_avg:34.49ms
step:311/1880 train_time:10727ms step_avg:34.49ms
step:312/1880 train_time:10762ms step_avg:34.49ms
step:313/1880 train_time:10795ms step_avg:34.49ms
step:314/1880 train_time:10829ms step_avg:34.49ms
step:315/1880 train_time:10863ms step_avg:34.49ms
step:316/1880 train_time:10898ms step_avg:34.49ms
step:317/1880 train_time:10931ms step_avg:34.48ms
step:318/1880 train_time:10966ms step_avg:34.48ms
step:319/1880 train_time:11000ms step_avg:34.48ms
step:320/1880 train_time:11034ms step_avg:34.48ms
step:321/1880 train_time:11068ms step_avg:34.48ms
step:322/1880 train_time:11102ms step_avg:34.48ms
step:323/1880 train_time:11136ms step_avg:34.48ms
step:324/1880 train_time:11170ms step_avg:34.47ms
step:325/1880 train_time:11204ms step_avg:34.47ms
step:326/1880 train_time:11238ms step_avg:34.47ms
step:327/1880 train_time:11271ms step_avg:34.47ms
step:328/1880 train_time:11306ms step_avg:34.47ms
step:329/1880 train_time:11339ms step_avg:34.47ms
step:330/1880 train_time:11373ms step_avg:34.46ms
step:331/1880 train_time:11407ms step_avg:34.46ms
step:332/1880 train_time:11442ms step_avg:34.46ms
step:333/1880 train_time:11475ms step_avg:34.46ms
step:334/1880 train_time:11509ms step_avg:34.46ms
step:335/1880 train_time:11544ms step_avg:34.46ms
step:336/1880 train_time:11578ms step_avg:34.46ms
step:337/1880 train_time:11611ms step_avg:34.46ms
step:338/1880 train_time:11645ms step_avg:34.45ms
step:339/1880 train_time:11680ms step_avg:34.45ms
step:340/1880 train_time:11714ms step_avg:34.45ms
step:341/1880 train_time:11748ms step_avg:34.45ms
step:342/1880 train_time:11782ms step_avg:34.45ms
step:343/1880 train_time:11816ms step_avg:34.45ms
step:344/1880 train_time:11850ms step_avg:34.45ms
step:345/1880 train_time:11884ms step_avg:34.45ms
step:346/1880 train_time:11918ms step_avg:34.45ms
step:347/1880 train_time:11952ms step_avg:34.44ms
step:348/1880 train_time:11986ms step_avg:34.44ms
step:349/1880 train_time:12020ms step_avg:34.44ms
step:350/1880 train_time:12055ms step_avg:34.44ms
step:351/1880 train_time:12088ms step_avg:34.44ms
step:352/1880 train_time:12122ms step_avg:34.44ms
step:353/1880 train_time:12156ms step_avg:34.44ms
step:354/1880 train_time:12190ms step_avg:34.44ms
step:355/1880 train_time:12224ms step_avg:34.43ms
step:356/1880 train_time:12258ms step_avg:34.43ms
step:357/1880 train_time:12292ms step_avg:34.43ms
step:358/1880 train_time:12326ms step_avg:34.43ms
step:359/1880 train_time:12360ms step_avg:34.43ms
step:360/1880 train_time:12394ms step_avg:34.43ms
step:361/1880 train_time:12428ms step_avg:34.43ms
step:362/1880 train_time:12463ms step_avg:34.43ms
step:363/1880 train_time:12496ms step_avg:34.43ms
step:364/1880 train_time:12530ms step_avg:34.42ms
step:365/1880 train_time:12564ms step_avg:34.42ms
step:366/1880 train_time:12598ms step_avg:34.42ms
step:367/1880 train_time:12632ms step_avg:34.42ms
step:368/1880 train_time:12666ms step_avg:34.42ms
step:369/1880 train_time:12700ms step_avg:34.42ms
step:370/1880 train_time:12734ms step_avg:34.42ms
step:371/1880 train_time:12768ms step_avg:34.42ms
step:372/1880 train_time:12802ms step_avg:34.41ms
step:373/1880 train_time:12836ms step_avg:34.41ms
step:374/1880 train_time:12870ms step_avg:34.41ms
step:375/1880 train_time:12904ms step_avg:34.41ms
step:376/1880 train_time:12938ms step_avg:34.41ms
step:377/1880 train_time:12972ms step_avg:34.41ms
step:378/1880 train_time:13006ms step_avg:34.41ms
step:379/1880 train_time:13040ms step_avg:34.41ms
step:380/1880 train_time:13075ms step_avg:34.41ms
step:381/1880 train_time:13108ms step_avg:34.40ms
step:382/1880 train_time:13142ms step_avg:34.40ms
step:383/1880 train_time:13176ms step_avg:34.40ms
step:384/1880 train_time:13210ms step_avg:34.40ms
step:385/1880 train_time:13244ms step_avg:34.40ms
step:386/1880 train_time:13278ms step_avg:34.40ms
step:387/1880 train_time:13312ms step_avg:34.40ms
step:388/1880 train_time:13346ms step_avg:34.40ms
step:389/1880 train_time:13380ms step_avg:34.39ms
step:390/1880 train_time:13414ms step_avg:34.39ms
step:391/1880 train_time:13447ms step_avg:34.39ms
step:392/1880 train_time:13482ms step_avg:34.39ms
step:393/1880 train_time:13515ms step_avg:34.39ms
step:394/1880 train_time:13550ms step_avg:34.39ms
step:395/1880 train_time:13583ms step_avg:34.39ms
step:396/1880 train_time:13617ms step_avg:34.39ms
step:397/1880 train_time:13651ms step_avg:34.39ms
step:398/1880 train_time:13685ms step_avg:34.38ms
step:399/1880 train_time:13719ms step_avg:34.38ms
step:400/1880 train_time:13753ms step_avg:34.38ms
step:401/1880 train_time:13787ms step_avg:34.38ms
step:402/1880 train_time:13821ms step_avg:34.38ms
step:403/1880 train_time:13855ms step_avg:34.38ms
step:404/1880 train_time:13889ms step_avg:34.38ms
step:405/1880 train_time:13923ms step_avg:34.38ms
step:406/1880 train_time:13957ms step_avg:34.38ms
step:407/1880 train_time:13991ms step_avg:34.38ms
step:408/1880 train_time:14025ms step_avg:34.38ms
step:409/1880 train_time:14059ms step_avg:34.37ms
step:410/1880 train_time:14094ms step_avg:34.37ms
step:411/1880 train_time:14127ms step_avg:34.37ms
step:412/1880 train_time:14161ms step_avg:34.37ms
step:413/1880 train_time:14195ms step_avg:34.37ms
step:414/1880 train_time:14229ms step_avg:34.37ms
step:415/1880 train_time:14263ms step_avg:34.37ms
step:416/1880 train_time:14297ms step_avg:34.37ms
step:417/1880 train_time:14331ms step_avg:34.37ms
step:418/1880 train_time:14365ms step_avg:34.37ms
step:419/1880 train_time:14399ms step_avg:34.37ms
step:420/1880 train_time:14434ms step_avg:34.37ms
step:421/1880 train_time:14468ms step_avg:34.36ms
step:422/1880 train_time:14502ms step_avg:34.37ms
step:423/1880 train_time:14536ms step_avg:34.36ms
step:424/1880 train_time:14570ms step_avg:34.36ms
step:425/1880 train_time:14604ms step_avg:34.36ms
step:426/1880 train_time:14638ms step_avg:34.36ms
step:427/1880 train_time:14672ms step_avg:34.36ms
step:428/1880 train_time:14706ms step_avg:34.36ms
step:429/1880 train_time:14740ms step_avg:34.36ms
step:430/1880 train_time:14774ms step_avg:34.36ms
step:431/1880 train_time:14808ms step_avg:34.36ms
step:432/1880 train_time:14842ms step_avg:34.36ms
step:433/1880 train_time:14876ms step_avg:34.35ms
step:434/1880 train_time:14910ms step_avg:34.35ms
step:435/1880 train_time:14944ms step_avg:34.35ms
step:436/1880 train_time:14978ms step_avg:34.35ms
step:437/1880 train_time:15012ms step_avg:34.35ms
step:438/1880 train_time:15046ms step_avg:34.35ms
step:439/1880 train_time:15080ms step_avg:34.35ms
step:440/1880 train_time:15114ms step_avg:34.35ms
step:441/1880 train_time:15148ms step_avg:34.35ms
step:442/1880 train_time:15182ms step_avg:34.35ms
step:443/1880 train_time:15216ms step_avg:34.35ms
step:444/1880 train_time:15250ms step_avg:34.35ms
step:445/1880 train_time:15284ms step_avg:34.35ms
step:446/1880 train_time:15318ms step_avg:34.35ms
step:447/1880 train_time:15352ms step_avg:34.34ms
step:448/1880 train_time:15386ms step_avg:34.34ms
step:449/1880 train_time:15420ms step_avg:34.34ms
step:450/1880 train_time:15454ms step_avg:34.34ms
step:451/1880 train_time:15487ms step_avg:34.34ms
step:452/1880 train_time:15522ms step_avg:34.34ms
step:453/1880 train_time:15555ms step_avg:34.34ms
step:454/1880 train_time:15590ms step_avg:34.34ms
step:455/1880 train_time:15624ms step_avg:34.34ms
step:456/1880 train_time:15658ms step_avg:34.34ms
step:457/1880 train_time:15692ms step_avg:34.34ms
step:458/1880 train_time:15726ms step_avg:34.34ms
step:459/1880 train_time:15760ms step_avg:34.34ms
step:460/1880 train_time:15794ms step_avg:34.33ms
step:461/1880 train_time:15828ms step_avg:34.33ms
step:462/1880 train_time:15862ms step_avg:34.33ms
step:463/1880 train_time:15895ms step_avg:34.33ms
step:464/1880 train_time:15929ms step_avg:34.33ms
step:465/1880 train_time:15963ms step_avg:34.33ms
step:466/1880 train_time:15997ms step_avg:34.33ms
step:467/1880 train_time:16031ms step_avg:34.33ms
step:468/1880 train_time:16065ms step_avg:34.33ms
step:469/1880 train_time:16099ms step_avg:34.33ms
step:470/1880 train_time:16133ms step_avg:34.33ms
step:471/1880 train_time:16167ms step_avg:34.32ms
step:472/1880 train_time:16201ms step_avg:34.32ms
step:473/1880 train_time:16235ms step_avg:34.32ms
step:474/1880 train_time:16269ms step_avg:34.32ms
step:475/1880 train_time:16303ms step_avg:34.32ms
step:476/1880 train_time:16337ms step_avg:34.32ms
step:477/1880 train_time:16370ms step_avg:34.32ms
step:478/1880 train_time:16405ms step_avg:34.32ms
step:479/1880 train_time:16439ms step_avg:34.32ms
step:480/1880 train_time:16473ms step_avg:34.32ms
step:481/1880 train_time:16507ms step_avg:34.32ms
step:482/1880 train_time:16541ms step_avg:34.32ms
step:483/1880 train_time:16575ms step_avg:34.32ms
step:484/1880 train_time:16608ms step_avg:34.32ms
step:485/1880 train_time:16643ms step_avg:34.31ms
step:486/1880 train_time:16677ms step_avg:34.31ms
step:487/1880 train_time:16710ms step_avg:34.31ms
step:488/1880 train_time:16745ms step_avg:34.31ms
step:489/1880 train_time:16779ms step_avg:34.31ms
step:490/1880 train_time:16813ms step_avg:34.31ms
step:491/1880 train_time:16847ms step_avg:34.31ms
step:492/1880 train_time:16882ms step_avg:34.31ms
step:493/1880 train_time:16915ms step_avg:34.31ms
step:494/1880 train_time:16949ms step_avg:34.31ms
step:495/1880 train_time:16983ms step_avg:34.31ms
step:496/1880 train_time:17017ms step_avg:34.31ms
step:497/1880 train_time:17051ms step_avg:34.31ms
step:498/1880 train_time:17085ms step_avg:34.31ms
step:499/1880 train_time:17120ms step_avg:34.31ms
step:500/1880 train_time:17154ms step_avg:34.31ms
step:500/1880 val_loss:4.2781 train_time:17191ms step_avg:34.38ms
step:501/1880 train_time:17211ms step_avg:34.35ms
step:502/1880 train_time:17231ms step_avg:34.33ms
step:503/1880 train_time:17258ms step_avg:34.31ms
step:504/1880 train_time:17293ms step_avg:34.31ms
step:505/1880 train_time:17328ms step_avg:34.31ms
step:506/1880 train_time:17363ms step_avg:34.31ms
step:507/1880 train_time:17397ms step_avg:34.31ms
step:508/1880 train_time:17432ms step_avg:34.31ms
step:509/1880 train_time:17466ms step_avg:34.31ms
step:510/1880 train_time:17500ms step_avg:34.31ms
step:511/1880 train_time:17534ms step_avg:34.31ms
step:512/1880 train_time:17567ms step_avg:34.31ms
step:513/1880 train_time:17601ms step_avg:34.31ms
step:514/1880 train_time:17635ms step_avg:34.31ms
step:515/1880 train_time:17669ms step_avg:34.31ms
step:516/1880 train_time:17703ms step_avg:34.31ms
step:517/1880 train_time:17737ms step_avg:34.31ms
step:518/1880 train_time:17770ms step_avg:34.31ms
step:519/1880 train_time:17804ms step_avg:34.30ms
step:520/1880 train_time:17838ms step_avg:34.30ms
step:521/1880 train_time:17872ms step_avg:34.30ms
step:522/1880 train_time:17906ms step_avg:34.30ms
step:523/1880 train_time:17939ms step_avg:34.30ms
step:524/1880 train_time:17973ms step_avg:34.30ms
step:525/1880 train_time:18007ms step_avg:34.30ms
step:526/1880 train_time:18040ms step_avg:34.30ms
step:527/1880 train_time:18074ms step_avg:34.30ms
step:528/1880 train_time:18108ms step_avg:34.30ms
step:529/1880 train_time:18142ms step_avg:34.29ms
step:530/1880 train_time:18176ms step_avg:34.29ms
step:531/1880 train_time:18210ms step_avg:34.29ms
step:532/1880 train_time:18245ms step_avg:34.29ms
step:533/1880 train_time:18279ms step_avg:34.29ms
step:534/1880 train_time:18314ms step_avg:34.30ms
step:535/1880 train_time:18348ms step_avg:34.29ms
step:536/1880 train_time:18382ms step_avg:34.29ms
step:537/1880 train_time:18416ms step_avg:34.29ms
step:538/1880 train_time:18451ms step_avg:34.30ms
step:539/1880 train_time:18485ms step_avg:34.29ms
step:540/1880 train_time:18519ms step_avg:34.29ms
step:541/1880 train_time:18553ms step_avg:34.29ms
step:542/1880 train_time:18586ms step_avg:34.29ms
step:543/1880 train_time:18621ms step_avg:34.29ms
step:544/1880 train_time:18655ms step_avg:34.29ms
step:545/1880 train_time:18689ms step_avg:34.29ms
step:546/1880 train_time:18723ms step_avg:34.29ms
step:547/1880 train_time:18756ms step_avg:34.29ms
step:548/1880 train_time:18791ms step_avg:34.29ms
step:549/1880 train_time:18824ms step_avg:34.29ms
step:550/1880 train_time:18858ms step_avg:34.29ms
step:551/1880 train_time:18892ms step_avg:34.29ms
step:552/1880 train_time:18926ms step_avg:34.29ms
step:553/1880 train_time:18960ms step_avg:34.29ms
step:554/1880 train_time:18994ms step_avg:34.28ms
step:555/1880 train_time:19027ms step_avg:34.28ms
step:556/1880 train_time:19061ms step_avg:34.28ms
step:557/1880 train_time:19095ms step_avg:34.28ms
step:558/1880 train_time:19129ms step_avg:34.28ms
step:559/1880 train_time:19163ms step_avg:34.28ms
step:560/1880 train_time:19197ms step_avg:34.28ms
step:561/1880 train_time:19231ms step_avg:34.28ms
step:562/1880 train_time:19265ms step_avg:34.28ms
step:563/1880 train_time:19299ms step_avg:34.28ms
step:564/1880 train_time:19333ms step_avg:34.28ms
step:565/1880 train_time:19367ms step_avg:34.28ms
step:566/1880 train_time:19401ms step_avg:34.28ms
step:567/1880 train_time:19435ms step_avg:34.28ms
step:568/1880 train_time:19469ms step_avg:34.28ms
step:569/1880 train_time:19503ms step_avg:34.28ms
step:570/1880 train_time:19538ms step_avg:34.28ms
step:571/1880 train_time:19572ms step_avg:34.28ms
step:572/1880 train_time:19606ms step_avg:34.28ms
step:573/1880 train_time:19640ms step_avg:34.28ms
step:574/1880 train_time:19674ms step_avg:34.27ms
step:575/1880 train_time:19707ms step_avg:34.27ms
step:576/1880 train_time:19741ms step_avg:34.27ms
step:577/1880 train_time:19775ms step_avg:34.27ms
step:578/1880 train_time:19809ms step_avg:34.27ms
step:579/1880 train_time:19843ms step_avg:34.27ms
step:580/1880 train_time:19877ms step_avg:34.27ms
step:581/1880 train_time:19911ms step_avg:34.27ms
step:582/1880 train_time:19945ms step_avg:34.27ms
step:583/1880 train_time:19979ms step_avg:34.27ms
step:584/1880 train_time:20013ms step_avg:34.27ms
step:585/1880 train_time:20046ms step_avg:34.27ms
step:586/1880 train_time:20080ms step_avg:34.27ms
step:587/1880 train_time:20114ms step_avg:34.27ms
step:588/1880 train_time:20148ms step_avg:34.27ms
step:589/1880 train_time:20182ms step_avg:34.26ms
step:590/1880 train_time:20216ms step_avg:34.26ms
step:591/1880 train_time:20250ms step_avg:34.26ms
step:592/1880 train_time:20284ms step_avg:34.26ms
step:593/1880 train_time:20318ms step_avg:34.26ms
step:594/1880 train_time:20352ms step_avg:34.26ms
step:595/1880 train_time:20386ms step_avg:34.26ms
step:596/1880 train_time:20420ms step_avg:34.26ms
step:597/1880 train_time:20454ms step_avg:34.26ms
step:598/1880 train_time:20488ms step_avg:34.26ms
step:599/1880 train_time:20522ms step_avg:34.26ms
step:600/1880 train_time:20556ms step_avg:34.26ms
step:601/1880 train_time:20590ms step_avg:34.26ms
step:602/1880 train_time:20624ms step_avg:34.26ms
step:603/1880 train_time:20658ms step_avg:34.26ms
step:604/1880 train_time:20693ms step_avg:34.26ms
step:605/1880 train_time:20727ms step_avg:34.26ms
step:606/1880 train_time:20761ms step_avg:34.26ms
step:607/1880 train_time:20795ms step_avg:34.26ms
step:608/1880 train_time:20829ms step_avg:34.26ms
step:609/1880 train_time:20863ms step_avg:34.26ms
step:610/1880 train_time:20897ms step_avg:34.26ms
step:611/1880 train_time:20931ms step_avg:34.26ms
step:612/1880 train_time:20965ms step_avg:34.26ms
step:613/1880 train_time:20999ms step_avg:34.26ms
step:614/1880 train_time:21034ms step_avg:34.26ms
step:615/1880 train_time:21068ms step_avg:34.26ms
step:616/1880 train_time:21128ms step_avg:34.30ms
step:617/1880 train_time:21189ms step_avg:34.34ms
step:618/1880 train_time:21250ms step_avg:34.39ms
step:619/1880 train_time:21313ms step_avg:34.43ms
step:620/1880 train_time:21374ms step_avg:34.47ms
step:621/1880 train_time:21435ms step_avg:34.52ms
step:622/1880 train_time:21497ms step_avg:34.56ms
step:623/1880 train_time:21559ms step_avg:34.61ms
step:624/1880 train_time:21621ms step_avg:34.65ms
step:625/1880 train_time:21682ms step_avg:34.69ms
step:626/1880 train_time:21744ms step_avg:34.74ms
step:627/1880 train_time:21806ms step_avg:34.78ms
step:628/1880 train_time:21866ms step_avg:34.82ms
step:629/1880 train_time:21929ms step_avg:34.86ms
step:630/1880 train_time:21990ms step_avg:34.90ms
step:631/1880 train_time:22052ms step_avg:34.95ms
step:632/1880 train_time:22113ms step_avg:34.99ms
step:633/1880 train_time:22174ms step_avg:35.03ms
step:634/1880 train_time:22234ms step_avg:35.07ms
step:635/1880 train_time:22296ms step_avg:35.11ms
step:636/1880 train_time:22358ms step_avg:35.15ms
step:637/1880 train_time:22420ms step_avg:35.20ms
step:638/1880 train_time:22481ms step_avg:35.24ms
step:639/1880 train_time:22543ms step_avg:35.28ms
step:640/1880 train_time:22604ms step_avg:35.32ms
step:641/1880 train_time:22666ms step_avg:35.36ms
step:642/1880 train_time:22727ms step_avg:35.40ms
step:643/1880 train_time:22789ms step_avg:35.44ms
step:644/1880 train_time:22850ms step_avg:35.48ms
step:645/1880 train_time:22913ms step_avg:35.52ms
step:646/1880 train_time:22974ms step_avg:35.56ms
step:647/1880 train_time:23036ms step_avg:35.60ms
step:648/1880 train_time:23097ms step_avg:35.64ms
step:649/1880 train_time:23159ms step_avg:35.68ms
step:650/1880 train_time:23221ms step_avg:35.72ms
step:651/1880 train_time:23282ms step_avg:35.76ms
step:652/1880 train_time:23343ms step_avg:35.80ms
step:653/1880 train_time:23405ms step_avg:35.84ms
step:654/1880 train_time:23466ms step_avg:35.88ms
step:655/1880 train_time:23528ms step_avg:35.92ms
step:656/1880 train_time:23589ms step_avg:35.96ms
step:657/1880 train_time:23652ms step_avg:36.00ms
step:658/1880 train_time:23713ms step_avg:36.04ms
step:659/1880 train_time:23775ms step_avg:36.08ms
step:660/1880 train_time:23836ms step_avg:36.12ms
step:661/1880 train_time:23899ms step_avg:36.16ms
step:662/1880 train_time:23961ms step_avg:36.20ms
step:663/1880 train_time:24023ms step_avg:36.23ms
step:664/1880 train_time:24084ms step_avg:36.27ms
step:665/1880 train_time:24146ms step_avg:36.31ms
step:666/1880 train_time:24207ms step_avg:36.35ms
step:667/1880 train_time:24269ms step_avg:36.38ms
step:668/1880 train_time:24330ms step_avg:36.42ms
step:669/1880 train_time:24392ms step_avg:36.46ms
step:670/1880 train_time:24453ms step_avg:36.50ms
step:671/1880 train_time:24515ms step_avg:36.53ms
step:672/1880 train_time:24576ms step_avg:36.57ms
step:673/1880 train_time:24639ms step_avg:36.61ms
step:674/1880 train_time:24701ms step_avg:36.65ms
step:675/1880 train_time:24762ms step_avg:36.69ms
step:676/1880 train_time:24824ms step_avg:36.72ms
step:677/1880 train_time:24886ms step_avg:36.76ms
step:678/1880 train_time:24947ms step_avg:36.80ms
step:679/1880 train_time:25010ms step_avg:36.83ms
step:680/1880 train_time:25071ms step_avg:36.87ms
step:681/1880 train_time:25133ms step_avg:36.91ms
step:682/1880 train_time:25195ms step_avg:36.94ms
step:683/1880 train_time:25256ms step_avg:36.98ms
step:684/1880 train_time:25318ms step_avg:37.01ms
step:685/1880 train_time:25380ms step_avg:37.05ms
step:686/1880 train_time:25442ms step_avg:37.09ms
step:687/1880 train_time:25504ms step_avg:37.12ms
step:688/1880 train_time:25566ms step_avg:37.16ms
step:689/1880 train_time:25627ms step_avg:37.19ms
step:690/1880 train_time:25689ms step_avg:37.23ms
step:691/1880 train_time:25751ms step_avg:37.27ms
step:692/1880 train_time:25812ms step_avg:37.30ms
step:693/1880 train_time:25873ms step_avg:37.34ms
step:694/1880 train_time:25935ms step_avg:37.37ms
step:695/1880 train_time:25996ms step_avg:37.40ms
step:696/1880 train_time:26058ms step_avg:37.44ms
step:697/1880 train_time:26121ms step_avg:37.48ms
step:698/1880 train_time:26183ms step_avg:37.51ms
step:699/1880 train_time:26246ms step_avg:37.55ms
step:700/1880 train_time:26307ms step_avg:37.58ms
step:701/1880 train_time:26368ms step_avg:37.61ms
step:702/1880 train_time:26429ms step_avg:37.65ms
step:703/1880 train_time:26492ms step_avg:37.68ms
step:704/1880 train_time:26552ms step_avg:37.72ms
step:705/1880 train_time:26614ms step_avg:37.75ms
step:706/1880 train_time:26675ms step_avg:37.78ms
step:707/1880 train_time:26737ms step_avg:37.82ms
step:708/1880 train_time:26798ms step_avg:37.85ms
step:709/1880 train_time:26860ms step_avg:37.88ms
step:710/1880 train_time:26921ms step_avg:37.92ms
step:711/1880 train_time:26984ms step_avg:37.95ms
step:712/1880 train_time:27045ms step_avg:37.98ms
step:713/1880 train_time:27107ms step_avg:38.02ms
step:714/1880 train_time:27169ms step_avg:38.05ms
step:715/1880 train_time:27230ms step_avg:38.08ms
step:716/1880 train_time:27291ms step_avg:38.12ms
step:717/1880 train_time:27353ms step_avg:38.15ms
step:718/1880 train_time:27414ms step_avg:38.18ms
step:719/1880 train_time:27476ms step_avg:38.21ms
step:720/1880 train_time:27538ms step_avg:38.25ms
step:721/1880 train_time:27600ms step_avg:38.28ms
step:722/1880 train_time:27662ms step_avg:38.31ms
step:723/1880 train_time:27723ms step_avg:38.34ms
step:724/1880 train_time:27785ms step_avg:38.38ms
step:725/1880 train_time:27847ms step_avg:38.41ms
step:726/1880 train_time:27908ms step_avg:38.44ms
step:727/1880 train_time:27970ms step_avg:38.47ms
step:728/1880 train_time:28031ms step_avg:38.50ms
step:729/1880 train_time:28093ms step_avg:38.54ms
step:730/1880 train_time:28154ms step_avg:38.57ms
step:731/1880 train_time:28216ms step_avg:38.60ms
step:732/1880 train_time:28277ms step_avg:38.63ms
step:733/1880 train_time:28338ms step_avg:38.66ms
step:734/1880 train_time:28400ms step_avg:38.69ms
step:735/1880 train_time:28461ms step_avg:38.72ms
step:736/1880 train_time:28523ms step_avg:38.75ms
step:737/1880 train_time:28585ms step_avg:38.79ms
step:738/1880 train_time:28646ms step_avg:38.82ms
step:739/1880 train_time:28708ms step_avg:38.85ms
step:740/1880 train_time:28768ms step_avg:38.88ms
step:741/1880 train_time:28831ms step_avg:38.91ms
step:742/1880 train_time:28892ms step_avg:38.94ms
step:743/1880 train_time:28954ms step_avg:38.97ms
step:744/1880 train_time:29015ms step_avg:39.00ms
step:745/1880 train_time:29077ms step_avg:39.03ms
step:746/1880 train_time:29138ms step_avg:39.06ms
step:747/1880 train_time:29200ms step_avg:39.09ms
step:748/1880 train_time:29261ms step_avg:39.12ms
step:749/1880 train_time:29323ms step_avg:39.15ms
step:750/1880 train_time:29384ms step_avg:39.18ms
step:750/1880 val_loss:4.0227 train_time:29449ms step_avg:39.27ms
step:751/1880 train_time:29469ms step_avg:39.24ms
step:752/1880 train_time:29509ms step_avg:39.24ms
step:753/1880 train_time:29572ms step_avg:39.27ms
step:754/1880 train_time:29634ms step_avg:39.30ms
step:755/1880 train_time:29696ms step_avg:39.33ms
step:756/1880 train_time:29757ms step_avg:39.36ms
step:757/1880 train_time:29819ms step_avg:39.39ms
step:758/1880 train_time:29880ms step_avg:39.42ms
step:759/1880 train_time:29941ms step_avg:39.45ms
step:760/1880 train_time:30001ms step_avg:39.48ms
step:761/1880 train_time:30063ms step_avg:39.50ms
step:762/1880 train_time:30124ms step_avg:39.53ms
step:763/1880 train_time:30185ms step_avg:39.56ms
step:764/1880 train_time:30247ms step_avg:39.59ms
step:765/1880 train_time:30308ms step_avg:39.62ms
step:766/1880 train_time:30369ms step_avg:39.65ms
step:767/1880 train_time:30432ms step_avg:39.68ms
step:768/1880 train_time:30494ms step_avg:39.71ms
step:769/1880 train_time:30556ms step_avg:39.73ms
step:770/1880 train_time:30617ms step_avg:39.76ms
step:771/1880 train_time:30679ms step_avg:39.79ms
step:772/1880 train_time:30741ms step_avg:39.82ms
step:773/1880 train_time:30803ms step_avg:39.85ms
step:774/1880 train_time:30864ms step_avg:39.88ms
step:775/1880 train_time:30925ms step_avg:39.90ms
step:776/1880 train_time:30986ms step_avg:39.93ms
step:777/1880 train_time:31046ms step_avg:39.96ms
step:778/1880 train_time:31108ms step_avg:39.98ms
step:779/1880 train_time:31169ms step_avg:40.01ms
step:780/1880 train_time:31230ms step_avg:40.04ms
step:781/1880 train_time:31291ms step_avg:40.07ms
step:782/1880 train_time:31352ms step_avg:40.09ms
step:783/1880 train_time:31414ms step_avg:40.12ms
step:784/1880 train_time:31475ms step_avg:40.15ms
step:785/1880 train_time:31538ms step_avg:40.18ms
step:786/1880 train_time:31600ms step_avg:40.20ms
step:787/1880 train_time:31663ms step_avg:40.23ms
step:788/1880 train_time:31725ms step_avg:40.26ms
step:789/1880 train_time:31786ms step_avg:40.29ms
step:790/1880 train_time:31847ms step_avg:40.31ms
step:791/1880 train_time:31909ms step_avg:40.34ms
step:792/1880 train_time:31969ms step_avg:40.36ms
step:793/1880 train_time:32031ms step_avg:40.39ms
step:794/1880 train_time:32092ms step_avg:40.42ms
step:795/1880 train_time:32153ms step_avg:40.44ms
step:796/1880 train_time:32214ms step_avg:40.47ms
step:797/1880 train_time:32275ms step_avg:40.50ms
step:798/1880 train_time:32337ms step_avg:40.52ms
step:799/1880 train_time:32398ms step_avg:40.55ms
step:800/1880 train_time:32459ms step_avg:40.57ms
step:801/1880 train_time:32522ms step_avg:40.60ms
step:802/1880 train_time:32584ms step_avg:40.63ms
step:803/1880 train_time:32646ms step_avg:40.65ms
step:804/1880 train_time:32707ms step_avg:40.68ms
step:805/1880 train_time:32768ms step_avg:40.71ms
step:806/1880 train_time:32829ms step_avg:40.73ms
step:807/1880 train_time:32891ms step_avg:40.76ms
step:808/1880 train_time:32952ms step_avg:40.78ms
step:809/1880 train_time:33014ms step_avg:40.81ms
step:810/1880 train_time:33074ms step_avg:40.83ms
step:811/1880 train_time:33136ms step_avg:40.86ms
step:812/1880 train_time:33196ms step_avg:40.88ms
step:813/1880 train_time:33258ms step_avg:40.91ms
step:814/1880 train_time:33319ms step_avg:40.93ms
step:815/1880 train_time:33381ms step_avg:40.96ms
step:816/1880 train_time:33442ms step_avg:40.98ms
step:817/1880 train_time:33503ms step_avg:41.01ms
step:818/1880 train_time:33565ms step_avg:41.03ms
step:819/1880 train_time:33627ms step_avg:41.06ms
step:820/1880 train_time:33689ms step_avg:41.08ms
step:821/1880 train_time:33750ms step_avg:41.11ms
step:822/1880 train_time:33811ms step_avg:41.13ms
step:823/1880 train_time:33872ms step_avg:41.16ms
step:824/1880 train_time:33933ms step_avg:41.18ms
step:825/1880 train_time:33995ms step_avg:41.21ms
step:826/1880 train_time:34056ms step_avg:41.23ms
step:827/1880 train_time:34117ms step_avg:41.25ms
step:828/1880 train_time:34178ms step_avg:41.28ms
step:829/1880 train_time:34239ms step_avg:41.30ms
step:830/1880 train_time:34300ms step_avg:41.33ms
step:831/1880 train_time:34362ms step_avg:41.35ms
step:832/1880 train_time:34423ms step_avg:41.37ms
step:833/1880 train_time:34484ms step_avg:41.40ms
step:834/1880 train_time:34546ms step_avg:41.42ms
step:835/1880 train_time:34607ms step_avg:41.45ms
step:836/1880 train_time:34669ms step_avg:41.47ms
step:837/1880 train_time:34731ms step_avg:41.49ms
step:838/1880 train_time:34791ms step_avg:41.52ms
step:839/1880 train_time:34854ms step_avg:41.54ms
step:840/1880 train_time:34915ms step_avg:41.57ms
step:841/1880 train_time:34976ms step_avg:41.59ms
step:842/1880 train_time:35037ms step_avg:41.61ms
step:843/1880 train_time:35098ms step_avg:41.63ms
step:844/1880 train_time:35160ms step_avg:41.66ms
step:845/1880 train_time:35221ms step_avg:41.68ms
step:846/1880 train_time:35282ms step_avg:41.70ms
step:847/1880 train_time:35343ms step_avg:41.73ms
step:848/1880 train_time:35405ms step_avg:41.75ms
step:849/1880 train_time:35466ms step_avg:41.77ms
step:850/1880 train_time:35527ms step_avg:41.80ms
step:851/1880 train_time:35589ms step_avg:41.82ms
step:852/1880 train_time:35650ms step_avg:41.84ms
step:853/1880 train_time:35712ms step_avg:41.87ms
step:854/1880 train_time:35774ms step_avg:41.89ms
step:855/1880 train_time:35836ms step_avg:41.91ms
step:856/1880 train_time:35897ms step_avg:41.94ms
step:857/1880 train_time:35958ms step_avg:41.96ms
step:858/1880 train_time:36021ms step_avg:41.98ms
step:859/1880 train_time:36082ms step_avg:42.00ms
step:860/1880 train_time:36143ms step_avg:42.03ms
step:861/1880 train_time:36204ms step_avg:42.05ms
step:862/1880 train_time:36266ms step_avg:42.07ms
step:863/1880 train_time:36328ms step_avg:42.10ms
step:864/1880 train_time:36389ms step_avg:42.12ms
step:865/1880 train_time:36451ms step_avg:42.14ms
step:866/1880 train_time:36512ms step_avg:42.16ms
step:867/1880 train_time:36574ms step_avg:42.19ms
step:868/1880 train_time:36636ms step_avg:42.21ms
step:869/1880 train_time:36698ms step_avg:42.23ms
step:870/1880 train_time:36759ms step_avg:42.25ms
step:871/1880 train_time:36822ms step_avg:42.28ms
step:872/1880 train_time:36884ms step_avg:42.30ms
step:873/1880 train_time:36945ms step_avg:42.32ms
step:874/1880 train_time:37007ms step_avg:42.34ms
step:875/1880 train_time:37068ms step_avg:42.36ms
step:876/1880 train_time:37129ms step_avg:42.38ms
step:877/1880 train_time:37191ms step_avg:42.41ms
step:878/1880 train_time:37252ms step_avg:42.43ms
step:879/1880 train_time:37314ms step_avg:42.45ms
step:880/1880 train_time:37375ms step_avg:42.47ms
step:881/1880 train_time:37436ms step_avg:42.49ms
step:882/1880 train_time:37498ms step_avg:42.51ms
step:883/1880 train_time:37560ms step_avg:42.54ms
step:884/1880 train_time:37621ms step_avg:42.56ms
step:885/1880 train_time:37682ms step_avg:42.58ms
step:886/1880 train_time:37743ms step_avg:42.60ms
step:887/1880 train_time:37805ms step_avg:42.62ms
step:888/1880 train_time:37867ms step_avg:42.64ms
step:889/1880 train_time:37929ms step_avg:42.66ms
step:890/1880 train_time:37989ms step_avg:42.68ms
step:891/1880 train_time:38051ms step_avg:42.71ms
step:892/1880 train_time:38112ms step_avg:42.73ms
step:893/1880 train_time:38174ms step_avg:42.75ms
step:894/1880 train_time:38235ms step_avg:42.77ms
step:895/1880 train_time:38297ms step_avg:42.79ms
step:896/1880 train_time:38357ms step_avg:42.81ms
step:897/1880 train_time:38419ms step_avg:42.83ms
step:898/1880 train_time:38480ms step_avg:42.85ms
step:899/1880 train_time:38541ms step_avg:42.87ms
step:900/1880 train_time:38603ms step_avg:42.89ms
step:901/1880 train_time:38665ms step_avg:42.91ms
step:902/1880 train_time:38726ms step_avg:42.93ms
step:903/1880 train_time:38788ms step_avg:42.95ms
step:904/1880 train_time:38849ms step_avg:42.97ms
step:905/1880 train_time:38911ms step_avg:43.00ms
step:906/1880 train_time:38972ms step_avg:43.02ms
step:907/1880 train_time:39034ms step_avg:43.04ms
step:908/1880 train_time:39095ms step_avg:43.06ms
step:909/1880 train_time:39157ms step_avg:43.08ms
step:910/1880 train_time:39218ms step_avg:43.10ms
step:911/1880 train_time:39280ms step_avg:43.12ms
step:912/1880 train_time:39341ms step_avg:43.14ms
step:913/1880 train_time:39403ms step_avg:43.16ms
step:914/1880 train_time:39464ms step_avg:43.18ms
step:915/1880 train_time:39525ms step_avg:43.20ms
step:916/1880 train_time:39586ms step_avg:43.22ms
step:917/1880 train_time:39648ms step_avg:43.24ms
step:918/1880 train_time:39709ms step_avg:43.26ms
step:919/1880 train_time:39771ms step_avg:43.28ms
step:920/1880 train_time:39832ms step_avg:43.30ms
step:921/1880 train_time:39894ms step_avg:43.32ms
step:922/1880 train_time:39955ms step_avg:43.34ms
step:923/1880 train_time:40017ms step_avg:43.36ms
step:924/1880 train_time:40079ms step_avg:43.38ms
step:925/1880 train_time:40141ms step_avg:43.40ms
step:926/1880 train_time:40202ms step_avg:43.41ms
step:927/1880 train_time:40264ms step_avg:43.43ms
step:928/1880 train_time:40326ms step_avg:43.45ms
step:929/1880 train_time:40388ms step_avg:43.47ms
step:930/1880 train_time:40449ms step_avg:43.49ms
step:931/1880 train_time:40511ms step_avg:43.51ms
step:932/1880 train_time:40572ms step_avg:43.53ms
step:933/1880 train_time:40634ms step_avg:43.55ms
step:934/1880 train_time:40695ms step_avg:43.57ms
step:935/1880 train_time:40756ms step_avg:43.59ms
step:936/1880 train_time:40818ms step_avg:43.61ms
step:937/1880 train_time:40879ms step_avg:43.63ms
step:938/1880 train_time:40941ms step_avg:43.65ms
step:939/1880 train_time:41003ms step_avg:43.67ms
step:940/1880 train_time:41064ms step_avg:43.69ms
step:941/1880 train_time:41126ms step_avg:43.70ms
step:942/1880 train_time:41187ms step_avg:43.72ms
step:943/1880 train_time:41249ms step_avg:43.74ms
step:944/1880 train_time:41310ms step_avg:43.76ms
step:945/1880 train_time:41371ms step_avg:43.78ms
step:946/1880 train_time:41433ms step_avg:43.80ms
step:947/1880 train_time:41495ms step_avg:43.82ms
step:948/1880 train_time:41556ms step_avg:43.84ms
step:949/1880 train_time:41618ms step_avg:43.85ms
step:950/1880 train_time:41680ms step_avg:43.87ms
step:951/1880 train_time:41741ms step_avg:43.89ms
step:952/1880 train_time:41802ms step_avg:43.91ms
step:953/1880 train_time:41864ms step_avg:43.93ms
step:954/1880 train_time:41926ms step_avg:43.95ms
step:955/1880 train_time:41988ms step_avg:43.97ms
step:956/1880 train_time:42048ms step_avg:43.98ms
step:957/1880 train_time:42110ms step_avg:44.00ms
step:958/1880 train_time:42171ms step_avg:44.02ms
step:959/1880 train_time:42232ms step_avg:44.04ms
step:960/1880 train_time:42293ms step_avg:44.06ms
step:961/1880 train_time:42355ms step_avg:44.07ms
step:962/1880 train_time:42415ms step_avg:44.09ms
step:963/1880 train_time:42477ms step_avg:44.11ms
step:964/1880 train_time:42539ms step_avg:44.13ms
step:965/1880 train_time:42600ms step_avg:44.15ms
step:966/1880 train_time:42661ms step_avg:44.16ms
step:967/1880 train_time:42724ms step_avg:44.18ms
step:968/1880 train_time:42785ms step_avg:44.20ms
step:969/1880 train_time:42847ms step_avg:44.22ms
step:970/1880 train_time:42908ms step_avg:44.24ms
step:971/1880 train_time:42969ms step_avg:44.25ms
step:972/1880 train_time:43031ms step_avg:44.27ms
step:973/1880 train_time:43092ms step_avg:44.29ms
step:974/1880 train_time:43153ms step_avg:44.31ms
step:975/1880 train_time:43215ms step_avg:44.32ms
step:976/1880 train_time:43276ms step_avg:44.34ms
step:977/1880 train_time:43338ms step_avg:44.36ms
step:978/1880 train_time:43400ms step_avg:44.38ms
step:979/1880 train_time:43461ms step_avg:44.39ms
step:980/1880 train_time:43523ms step_avg:44.41ms
step:981/1880 train_time:43584ms step_avg:44.43ms
step:982/1880 train_time:43645ms step_avg:44.45ms
step:983/1880 train_time:43706ms step_avg:44.46ms
step:984/1880 train_time:43768ms step_avg:44.48ms
step:985/1880 train_time:43829ms step_avg:44.50ms
step:986/1880 train_time:43891ms step_avg:44.51ms
step:987/1880 train_time:43952ms step_avg:44.53ms
step:988/1880 train_time:44013ms step_avg:44.55ms
step:989/1880 train_time:44075ms step_avg:44.56ms
step:990/1880 train_time:44136ms step_avg:44.58ms
step:991/1880 train_time:44197ms step_avg:44.60ms
step:992/1880 train_time:44259ms step_avg:44.62ms
step:993/1880 train_time:44320ms step_avg:44.63ms
step:994/1880 train_time:44382ms step_avg:44.65ms
step:995/1880 train_time:44444ms step_avg:44.67ms
step:996/1880 train_time:44505ms step_avg:44.68ms
step:997/1880 train_time:44566ms step_avg:44.70ms
step:998/1880 train_time:44628ms step_avg:44.72ms
step:999/1880 train_time:44689ms step_avg:44.73ms
step:1000/1880 train_time:44750ms step_avg:44.75ms
step:1000/1880 val_loss:3.7719 train_time:44814ms step_avg:44.81ms
step:1001/1880 train_time:44841ms step_avg:44.80ms
step:1002/1880 train_time:44877ms step_avg:44.79ms
step:1003/1880 train_time:44941ms step_avg:44.81ms
step:1004/1880 train_time:45004ms step_avg:44.82ms
step:1005/1880 train_time:45065ms step_avg:44.84ms
step:1006/1880 train_time:45126ms step_avg:44.86ms
step:1007/1880 train_time:45187ms step_avg:44.87ms
step:1008/1880 train_time:45248ms step_avg:44.89ms
step:1009/1880 train_time:45309ms step_avg:44.91ms
step:1010/1880 train_time:45370ms step_avg:44.92ms
step:1011/1880 train_time:45431ms step_avg:44.94ms
step:1012/1880 train_time:45491ms step_avg:44.95ms
step:1013/1880 train_time:45553ms step_avg:44.97ms
step:1014/1880 train_time:45613ms step_avg:44.98ms
step:1015/1880 train_time:45674ms step_avg:45.00ms
step:1016/1880 train_time:45735ms step_avg:45.01ms
step:1017/1880 train_time:45799ms step_avg:45.03ms
step:1018/1880 train_time:45861ms step_avg:45.05ms
step:1019/1880 train_time:45924ms step_avg:45.07ms
step:1020/1880 train_time:45986ms step_avg:45.08ms
step:1021/1880 train_time:46048ms step_avg:45.10ms
step:1022/1880 train_time:46110ms step_avg:45.12ms
step:1023/1880 train_time:46171ms step_avg:45.13ms
step:1024/1880 train_time:46231ms step_avg:45.15ms
step:1025/1880 train_time:46294ms step_avg:45.16ms
step:1026/1880 train_time:46354ms step_avg:45.18ms
step:1027/1880 train_time:46416ms step_avg:45.20ms
step:1028/1880 train_time:46476ms step_avg:45.21ms
step:1029/1880 train_time:46537ms step_avg:45.23ms
step:1030/1880 train_time:46598ms step_avg:45.24ms
step:1031/1880 train_time:46659ms step_avg:45.26ms
step:1032/1880 train_time:46721ms step_avg:45.27ms
step:1033/1880 train_time:46783ms step_avg:45.29ms
step:1034/1880 train_time:46844ms step_avg:45.30ms
step:1035/1880 train_time:46907ms step_avg:45.32ms
step:1036/1880 train_time:46968ms step_avg:45.34ms
step:1037/1880 train_time:47030ms step_avg:45.35ms
step:1038/1880 train_time:47091ms step_avg:45.37ms
step:1039/1880 train_time:47153ms step_avg:45.38ms
step:1040/1880 train_time:47214ms step_avg:45.40ms
step:1041/1880 train_time:47276ms step_avg:45.41ms
step:1042/1880 train_time:47337ms step_avg:45.43ms
step:1043/1880 train_time:47399ms step_avg:45.44ms
step:1044/1880 train_time:47460ms step_avg:45.46ms
step:1045/1880 train_time:47522ms step_avg:45.48ms
step:1046/1880 train_time:47582ms step_avg:45.49ms
step:1047/1880 train_time:47644ms step_avg:45.51ms
step:1048/1880 train_time:47705ms step_avg:45.52ms
step:1049/1880 train_time:47766ms step_avg:45.54ms
step:1050/1880 train_time:47827ms step_avg:45.55ms
step:1051/1880 train_time:47889ms step_avg:45.57ms
step:1052/1880 train_time:47950ms step_avg:45.58ms
step:1053/1880 train_time:48012ms step_avg:45.60ms
step:1054/1880 train_time:48074ms step_avg:45.61ms
step:1055/1880 train_time:48135ms step_avg:45.63ms
step:1056/1880 train_time:48195ms step_avg:45.64ms
step:1057/1880 train_time:48257ms step_avg:45.65ms
step:1058/1880 train_time:48318ms step_avg:45.67ms
step:1059/1880 train_time:48380ms step_avg:45.68ms
step:1060/1880 train_time:48441ms step_avg:45.70ms
step:1061/1880 train_time:48502ms step_avg:45.71ms
step:1062/1880 train_time:48563ms step_avg:45.73ms
step:1063/1880 train_time:48624ms step_avg:45.74ms
step:1064/1880 train_time:48686ms step_avg:45.76ms
step:1065/1880 train_time:48747ms step_avg:45.77ms
step:1066/1880 train_time:48808ms step_avg:45.79ms
step:1067/1880 train_time:48870ms step_avg:45.80ms
step:1068/1880 train_time:48931ms step_avg:45.82ms
step:1069/1880 train_time:48993ms step_avg:45.83ms
step:1070/1880 train_time:49055ms step_avg:45.85ms
step:1071/1880 train_time:49116ms step_avg:45.86ms
step:1072/1880 train_time:49177ms step_avg:45.87ms
step:1073/1880 train_time:49239ms step_avg:45.89ms
step:1074/1880 train_time:49300ms step_avg:45.90ms
step:1075/1880 train_time:49362ms step_avg:45.92ms
step:1076/1880 train_time:49423ms step_avg:45.93ms
step:1077/1880 train_time:49484ms step_avg:45.95ms
step:1078/1880 train_time:49545ms step_avg:45.96ms
step:1079/1880 train_time:49607ms step_avg:45.97ms
step:1080/1880 train_time:49668ms step_avg:45.99ms
step:1081/1880 train_time:49729ms step_avg:46.00ms
step:1082/1880 train_time:49790ms step_avg:46.02ms
step:1083/1880 train_time:49852ms step_avg:46.03ms
step:1084/1880 train_time:49914ms step_avg:46.05ms
step:1085/1880 train_time:49976ms step_avg:46.06ms
step:1086/1880 train_time:50038ms step_avg:46.08ms
step:1087/1880 train_time:50099ms step_avg:46.09ms
step:1088/1880 train_time:50161ms step_avg:46.10ms
step:1089/1880 train_time:50223ms step_avg:46.12ms
step:1090/1880 train_time:50284ms step_avg:46.13ms
step:1091/1880 train_time:50346ms step_avg:46.15ms
step:1092/1880 train_time:50407ms step_avg:46.16ms
step:1093/1880 train_time:50468ms step_avg:46.17ms
step:1094/1880 train_time:50529ms step_avg:46.19ms
step:1095/1880 train_time:50591ms step_avg:46.20ms
step:1096/1880 train_time:50652ms step_avg:46.22ms
step:1097/1880 train_time:50713ms step_avg:46.23ms
step:1098/1880 train_time:50774ms step_avg:46.24ms
step:1099/1880 train_time:50836ms step_avg:46.26ms
step:1100/1880 train_time:50897ms step_avg:46.27ms
step:1101/1880 train_time:50959ms step_avg:46.28ms
step:1102/1880 train_time:51021ms step_avg:46.30ms
step:1103/1880 train_time:51083ms step_avg:46.31ms
step:1104/1880 train_time:51145ms step_avg:46.33ms
step:1105/1880 train_time:51207ms step_avg:46.34ms
step:1106/1880 train_time:51268ms step_avg:46.35ms
step:1107/1880 train_time:51330ms step_avg:46.37ms
step:1108/1880 train_time:51391ms step_avg:46.38ms
step:1109/1880 train_time:51453ms step_avg:46.40ms
step:1110/1880 train_time:51513ms step_avg:46.41ms
step:1111/1880 train_time:51575ms step_avg:46.42ms
step:1112/1880 train_time:51635ms step_avg:46.43ms
step:1113/1880 train_time:51698ms step_avg:46.45ms
step:1114/1880 train_time:51759ms step_avg:46.46ms
step:1115/1880 train_time:51821ms step_avg:46.48ms
step:1116/1880 train_time:51882ms step_avg:46.49ms
step:1117/1880 train_time:51944ms step_avg:46.50ms
step:1118/1880 train_time:52005ms step_avg:46.52ms
step:1119/1880 train_time:52066ms step_avg:46.53ms
step:1120/1880 train_time:52128ms step_avg:46.54ms
step:1121/1880 train_time:52189ms step_avg:46.56ms
step:1122/1880 train_time:52251ms step_avg:46.57ms
step:1123/1880 train_time:52313ms step_avg:46.58ms
step:1124/1880 train_time:52374ms step_avg:46.60ms
step:1125/1880 train_time:52436ms step_avg:46.61ms
step:1126/1880 train_time:52497ms step_avg:46.62ms
step:1127/1880 train_time:52559ms step_avg:46.64ms
step:1128/1880 train_time:52620ms step_avg:46.65ms
step:1129/1880 train_time:52682ms step_avg:46.66ms
step:1130/1880 train_time:52744ms step_avg:46.68ms
step:1131/1880 train_time:52805ms step_avg:46.69ms
step:1132/1880 train_time:52867ms step_avg:46.70ms
step:1133/1880 train_time:52928ms step_avg:46.72ms
step:1134/1880 train_time:52990ms step_avg:46.73ms
step:1135/1880 train_time:53051ms step_avg:46.74ms
step:1136/1880 train_time:53112ms step_avg:46.75ms
step:1137/1880 train_time:53174ms step_avg:46.77ms
step:1138/1880 train_time:53235ms step_avg:46.78ms
step:1139/1880 train_time:53297ms step_avg:46.79ms
step:1140/1880 train_time:53358ms step_avg:46.81ms
step:1141/1880 train_time:53420ms step_avg:46.82ms
step:1142/1880 train_time:53482ms step_avg:46.83ms
step:1143/1880 train_time:53543ms step_avg:46.84ms
step:1144/1880 train_time:53604ms step_avg:46.86ms
step:1145/1880 train_time:53666ms step_avg:46.87ms
step:1146/1880 train_time:53727ms step_avg:46.88ms
step:1147/1880 train_time:53789ms step_avg:46.89ms
step:1148/1880 train_time:53849ms step_avg:46.91ms
step:1149/1880 train_time:53911ms step_avg:46.92ms
step:1150/1880 train_time:53972ms step_avg:46.93ms
step:1151/1880 train_time:54034ms step_avg:46.95ms
step:1152/1880 train_time:54095ms step_avg:46.96ms
step:1153/1880 train_time:54157ms step_avg:46.97ms
step:1154/1880 train_time:54218ms step_avg:46.98ms
step:1155/1880 train_time:54280ms step_avg:47.00ms
step:1156/1880 train_time:54341ms step_avg:47.01ms
step:1157/1880 train_time:54403ms step_avg:47.02ms
step:1158/1880 train_time:54465ms step_avg:47.03ms
step:1159/1880 train_time:54526ms step_avg:47.05ms
step:1160/1880 train_time:54587ms step_avg:47.06ms
step:1161/1880 train_time:54649ms step_avg:47.07ms
step:1162/1880 train_time:54710ms step_avg:47.08ms
step:1163/1880 train_time:54772ms step_avg:47.10ms
step:1164/1880 train_time:54833ms step_avg:47.11ms
step:1165/1880 train_time:54895ms step_avg:47.12ms
step:1166/1880 train_time:54957ms step_avg:47.13ms
step:1167/1880 train_time:55018ms step_avg:47.14ms
step:1168/1880 train_time:55079ms step_avg:47.16ms
step:1169/1880 train_time:55141ms step_avg:47.17ms
step:1170/1880 train_time:55202ms step_avg:47.18ms
step:1171/1880 train_time:55263ms step_avg:47.19ms
step:1172/1880 train_time:55325ms step_avg:47.21ms
step:1173/1880 train_time:55386ms step_avg:47.22ms
step:1174/1880 train_time:55447ms step_avg:47.23ms
step:1175/1880 train_time:55509ms step_avg:47.24ms
step:1176/1880 train_time:55571ms step_avg:47.25ms
step:1177/1880 train_time:55632ms step_avg:47.27ms
step:1178/1880 train_time:55693ms step_avg:47.28ms
step:1179/1880 train_time:55755ms step_avg:47.29ms
step:1180/1880 train_time:55815ms step_avg:47.30ms
step:1181/1880 train_time:55877ms step_avg:47.31ms
step:1182/1880 train_time:55938ms step_avg:47.33ms
step:1183/1880 train_time:56001ms step_avg:47.34ms
step:1184/1880 train_time:56062ms step_avg:47.35ms
step:1185/1880 train_time:56125ms step_avg:47.36ms
step:1186/1880 train_time:56185ms step_avg:47.37ms
step:1187/1880 train_time:56247ms step_avg:47.39ms
step:1188/1880 train_time:56308ms step_avg:47.40ms
step:1189/1880 train_time:56370ms step_avg:47.41ms
step:1190/1880 train_time:56431ms step_avg:47.42ms
step:1191/1880 train_time:56493ms step_avg:47.43ms
step:1192/1880 train_time:56553ms step_avg:47.44ms
step:1193/1880 train_time:56616ms step_avg:47.46ms
step:1194/1880 train_time:56677ms step_avg:47.47ms
step:1195/1880 train_time:56738ms step_avg:47.48ms
step:1196/1880 train_time:56800ms step_avg:47.49ms
step:1197/1880 train_time:56861ms step_avg:47.50ms
step:1198/1880 train_time:56922ms step_avg:47.51ms
step:1199/1880 train_time:56983ms step_avg:47.53ms
step:1200/1880 train_time:57045ms step_avg:47.54ms
step:1201/1880 train_time:57106ms step_avg:47.55ms
step:1202/1880 train_time:57168ms step_avg:47.56ms
step:1203/1880 train_time:57230ms step_avg:47.57ms
step:1204/1880 train_time:57291ms step_avg:47.58ms
step:1205/1880 train_time:57353ms step_avg:47.60ms
step:1206/1880 train_time:57414ms step_avg:47.61ms
step:1207/1880 train_time:57475ms step_avg:47.62ms
step:1208/1880 train_time:57536ms step_avg:47.63ms
step:1209/1880 train_time:57598ms step_avg:47.64ms
step:1210/1880 train_time:57659ms step_avg:47.65ms
step:1211/1880 train_time:57721ms step_avg:47.66ms
step:1212/1880 train_time:57782ms step_avg:47.68ms
step:1213/1880 train_time:57844ms step_avg:47.69ms
step:1214/1880 train_time:57905ms step_avg:47.70ms
step:1215/1880 train_time:57967ms step_avg:47.71ms
step:1216/1880 train_time:58028ms step_avg:47.72ms
step:1217/1880 train_time:58089ms step_avg:47.73ms
step:1218/1880 train_time:58150ms step_avg:47.74ms
step:1219/1880 train_time:58211ms step_avg:47.75ms
step:1220/1880 train_time:58273ms step_avg:47.76ms
step:1221/1880 train_time:58335ms step_avg:47.78ms
step:1222/1880 train_time:58395ms step_avg:47.79ms
step:1223/1880 train_time:58458ms step_avg:47.80ms
step:1224/1880 train_time:58519ms step_avg:47.81ms
step:1225/1880 train_time:58580ms step_avg:47.82ms
step:1226/1880 train_time:58641ms step_avg:47.83ms
step:1227/1880 train_time:58702ms step_avg:47.84ms
step:1228/1880 train_time:58765ms step_avg:47.85ms
step:1229/1880 train_time:58853ms step_avg:47.89ms
step:1230/1880 train_time:58940ms step_avg:47.92ms
step:1231/1880 train_time:59028ms step_avg:47.95ms
step:1232/1880 train_time:59116ms step_avg:47.98ms
step:1233/1880 train_time:59204ms step_avg:48.02ms
step:1234/1880 train_time:59292ms step_avg:48.05ms
step:1235/1880 train_time:59380ms step_avg:48.08ms
step:1236/1880 train_time:59467ms step_avg:48.11ms
step:1237/1880 train_time:59556ms step_avg:48.15ms
step:1238/1880 train_time:59643ms step_avg:48.18ms
step:1239/1880 train_time:59732ms step_avg:48.21ms
step:1240/1880 train_time:59820ms step_avg:48.24ms
step:1241/1880 train_time:59908ms step_avg:48.27ms
step:1242/1880 train_time:59995ms step_avg:48.31ms
step:1243/1880 train_time:60083ms step_avg:48.34ms
step:1244/1880 train_time:60171ms step_avg:48.37ms
step:1245/1880 train_time:60259ms step_avg:48.40ms
step:1246/1880 train_time:60347ms step_avg:48.43ms
step:1247/1880 train_time:60435ms step_avg:48.46ms
step:1248/1880 train_time:60522ms step_avg:48.50ms
step:1249/1880 train_time:60610ms step_avg:48.53ms
step:1250/1880 train_time:60698ms step_avg:48.56ms
step:1250/1880 val_loss:3.5313 train_time:60788ms step_avg:48.63ms
step:1251/1880 train_time:60811ms step_avg:48.61ms
step:1252/1880 train_time:60879ms step_avg:48.63ms
step:1253/1880 train_time:60968ms step_avg:48.66ms
step:1254/1880 train_time:61055ms step_avg:48.69ms
step:1255/1880 train_time:61143ms step_avg:48.72ms
step:1256/1880 train_time:61230ms step_avg:48.75ms
step:1257/1880 train_time:61317ms step_avg:48.78ms
step:1258/1880 train_time:61405ms step_avg:48.81ms
step:1259/1880 train_time:61492ms step_avg:48.84ms
step:1260/1880 train_time:61580ms step_avg:48.87ms
step:1261/1880 train_time:61668ms step_avg:48.90ms
step:1262/1880 train_time:61757ms step_avg:48.94ms
step:1263/1880 train_time:61849ms step_avg:48.97ms
step:1264/1880 train_time:61937ms step_avg:49.00ms
step:1265/1880 train_time:62026ms step_avg:49.03ms
step:1266/1880 train_time:62113ms step_avg:49.06ms
step:1267/1880 train_time:62201ms step_avg:49.09ms
step:1268/1880 train_time:62287ms step_avg:49.12ms
step:1269/1880 train_time:62374ms step_avg:49.15ms
step:1270/1880 train_time:62462ms step_avg:49.18ms
step:1271/1880 train_time:62549ms step_avg:49.21ms
step:1272/1880 train_time:62636ms step_avg:49.24ms
step:1273/1880 train_time:62725ms step_avg:49.27ms
step:1274/1880 train_time:62812ms step_avg:49.30ms
step:1275/1880 train_time:62903ms step_avg:49.34ms
step:1276/1880 train_time:62991ms step_avg:49.37ms
step:1277/1880 train_time:63079ms step_avg:49.40ms
step:1278/1880 train_time:63167ms step_avg:49.43ms
step:1279/1880 train_time:63254ms step_avg:49.46ms
step:1280/1880 train_time:63341ms step_avg:49.48ms
step:1281/1880 train_time:63428ms step_avg:49.51ms
step:1282/1880 train_time:63514ms step_avg:49.54ms
step:1283/1880 train_time:63603ms step_avg:49.57ms
step:1284/1880 train_time:63692ms step_avg:49.60ms
step:1285/1880 train_time:63780ms step_avg:49.63ms
step:1286/1880 train_time:63869ms step_avg:49.66ms
step:1287/1880 train_time:63958ms step_avg:49.70ms
step:1288/1880 train_time:64046ms step_avg:49.72ms
step:1289/1880 train_time:64134ms step_avg:49.76ms
step:1290/1880 train_time:64222ms step_avg:49.78ms
step:1291/1880 train_time:64309ms step_avg:49.81ms
step:1292/1880 train_time:64396ms step_avg:49.84ms
step:1293/1880 train_time:64483ms step_avg:49.87ms
step:1294/1880 train_time:64572ms step_avg:49.90ms
step:1295/1880 train_time:64661ms step_avg:49.93ms
step:1296/1880 train_time:64749ms step_avg:49.96ms
step:1297/1880 train_time:64837ms step_avg:49.99ms
step:1298/1880 train_time:64925ms step_avg:50.02ms
step:1299/1880 train_time:65013ms step_avg:50.05ms
step:1300/1880 train_time:65103ms step_avg:50.08ms
step:1301/1880 train_time:65191ms step_avg:50.11ms
step:1302/1880 train_time:65278ms step_avg:50.14ms
step:1303/1880 train_time:65366ms step_avg:50.17ms
step:1304/1880 train_time:65453ms step_avg:50.19ms
step:1305/1880 train_time:65541ms step_avg:50.22ms
step:1306/1880 train_time:65629ms step_avg:50.25ms
step:1307/1880 train_time:65717ms step_avg:50.28ms
step:1308/1880 train_time:65804ms step_avg:50.31ms
step:1309/1880 train_time:65892ms step_avg:50.34ms
step:1310/1880 train_time:65980ms step_avg:50.37ms
step:1311/1880 train_time:66070ms step_avg:50.40ms
step:1312/1880 train_time:66157ms step_avg:50.42ms
step:1313/1880 train_time:66245ms step_avg:50.45ms
step:1314/1880 train_time:66333ms step_avg:50.48ms
step:1315/1880 train_time:66421ms step_avg:50.51ms
step:1316/1880 train_time:66509ms step_avg:50.54ms
step:1317/1880 train_time:66596ms step_avg:50.57ms
step:1318/1880 train_time:66683ms step_avg:50.59ms
step:1319/1880 train_time:66772ms step_avg:50.62ms
step:1320/1880 train_time:66860ms step_avg:50.65ms
step:1321/1880 train_time:66949ms step_avg:50.68ms
step:1322/1880 train_time:67037ms step_avg:50.71ms
step:1323/1880 train_time:67126ms step_avg:50.74ms
step:1324/1880 train_time:67213ms step_avg:50.76ms
step:1325/1880 train_time:67301ms step_avg:50.79ms
step:1326/1880 train_time:67388ms step_avg:50.82ms
step:1327/1880 train_time:67476ms step_avg:50.85ms
step:1328/1880 train_time:67564ms step_avg:50.88ms
step:1329/1880 train_time:67652ms step_avg:50.90ms
step:1330/1880 train_time:67740ms step_avg:50.93ms
step:1331/1880 train_time:67828ms step_avg:50.96ms
step:1332/1880 train_time:67915ms step_avg:50.99ms
step:1333/1880 train_time:68004ms step_avg:51.02ms
step:1334/1880 train_time:68093ms step_avg:51.04ms
step:1335/1880 train_time:68181ms step_avg:51.07ms
step:1336/1880 train_time:68269ms step_avg:51.10ms
step:1337/1880 train_time:68357ms step_avg:51.13ms
step:1338/1880 train_time:68444ms step_avg:51.15ms
step:1339/1880 train_time:68533ms step_avg:51.18ms
step:1340/1880 train_time:68621ms step_avg:51.21ms
step:1341/1880 train_time:68709ms step_avg:51.24ms
step:1342/1880 train_time:68796ms step_avg:51.26ms
step:1343/1880 train_time:68885ms step_avg:51.29ms
step:1344/1880 train_time:68973ms step_avg:51.32ms
step:1345/1880 train_time:69061ms step_avg:51.35ms
step:1346/1880 train_time:69149ms step_avg:51.37ms
step:1347/1880 train_time:69237ms step_avg:51.40ms
step:1348/1880 train_time:69324ms step_avg:51.43ms
step:1349/1880 train_time:69412ms step_avg:51.45ms
step:1350/1880 train_time:69500ms step_avg:51.48ms
step:1351/1880 train_time:69588ms step_avg:51.51ms
step:1352/1880 train_time:69675ms step_avg:51.53ms
step:1353/1880 train_time:69764ms step_avg:51.56ms
step:1354/1880 train_time:69852ms step_avg:51.59ms
step:1355/1880 train_time:69941ms step_avg:51.62ms
step:1356/1880 train_time:70030ms step_avg:51.64ms
step:1357/1880 train_time:70117ms step_avg:51.67ms
step:1358/1880 train_time:70206ms step_avg:51.70ms
step:1359/1880 train_time:70294ms step_avg:51.72ms
step:1360/1880 train_time:70381ms step_avg:51.75ms
step:1361/1880 train_time:70470ms step_avg:51.78ms
step:1362/1880 train_time:70558ms step_avg:51.80ms
step:1363/1880 train_time:70647ms step_avg:51.83ms
step:1364/1880 train_time:70735ms step_avg:51.86ms
step:1365/1880 train_time:70822ms step_avg:51.88ms
step:1366/1880 train_time:70911ms step_avg:51.91ms
step:1367/1880 train_time:70999ms step_avg:51.94ms
step:1368/1880 train_time:71086ms step_avg:51.96ms
step:1369/1880 train_time:71175ms step_avg:51.99ms
step:1370/1880 train_time:71264ms step_avg:52.02ms
step:1371/1880 train_time:71352ms step_avg:52.04ms
step:1372/1880 train_time:71440ms step_avg:52.07ms
step:1373/1880 train_time:71529ms step_avg:52.10ms
step:1374/1880 train_time:71616ms step_avg:52.12ms
step:1375/1880 train_time:71704ms step_avg:52.15ms
step:1376/1880 train_time:71792ms step_avg:52.17ms
step:1377/1880 train_time:71879ms step_avg:52.20ms
step:1378/1880 train_time:71967ms step_avg:52.23ms
step:1379/1880 train_time:72055ms step_avg:52.25ms
step:1380/1880 train_time:72143ms step_avg:52.28ms
step:1381/1880 train_time:72232ms step_avg:52.30ms
step:1382/1880 train_time:72320ms step_avg:52.33ms
step:1383/1880 train_time:72409ms step_avg:52.36ms
step:1384/1880 train_time:72496ms step_avg:52.38ms
step:1385/1880 train_time:72585ms step_avg:52.41ms
step:1386/1880 train_time:72673ms step_avg:52.43ms
step:1387/1880 train_time:72760ms step_avg:52.46ms
step:1388/1880 train_time:72848ms step_avg:52.48ms
step:1389/1880 train_time:72936ms step_avg:52.51ms
step:1390/1880 train_time:73024ms step_avg:52.54ms
step:1391/1880 train_time:73112ms step_avg:52.56ms
step:1392/1880 train_time:73200ms step_avg:52.59ms
step:1393/1880 train_time:73288ms step_avg:52.61ms
step:1394/1880 train_time:73376ms step_avg:52.64ms
step:1395/1880 train_time:73465ms step_avg:52.66ms
step:1396/1880 train_time:73553ms step_avg:52.69ms
step:1397/1880 train_time:73641ms step_avg:52.71ms
step:1398/1880 train_time:73730ms step_avg:52.74ms
step:1399/1880 train_time:73817ms step_avg:52.76ms
step:1400/1880 train_time:73905ms step_avg:52.79ms
step:1401/1880 train_time:73993ms step_avg:52.81ms
step:1402/1880 train_time:74081ms step_avg:52.84ms
step:1403/1880 train_time:74169ms step_avg:52.86ms
step:1404/1880 train_time:74257ms step_avg:52.89ms
step:1405/1880 train_time:74346ms step_avg:52.92ms
step:1406/1880 train_time:74433ms step_avg:52.94ms
step:1407/1880 train_time:74521ms step_avg:52.96ms
step:1408/1880 train_time:74609ms step_avg:52.99ms
step:1409/1880 train_time:74697ms step_avg:53.01ms
step:1410/1880 train_time:74784ms step_avg:53.04ms
step:1411/1880 train_time:74872ms step_avg:53.06ms
step:1412/1880 train_time:74960ms step_avg:53.09ms
step:1413/1880 train_time:75048ms step_avg:53.11ms
step:1414/1880 train_time:75136ms step_avg:53.14ms
step:1415/1880 train_time:75225ms step_avg:53.16ms
step:1416/1880 train_time:75313ms step_avg:53.19ms
step:1417/1880 train_time:75401ms step_avg:53.21ms
step:1418/1880 train_time:75488ms step_avg:53.24ms
step:1419/1880 train_time:75575ms step_avg:53.26ms
step:1420/1880 train_time:75663ms step_avg:53.28ms
step:1421/1880 train_time:75751ms step_avg:53.31ms
step:1422/1880 train_time:75839ms step_avg:53.33ms
step:1423/1880 train_time:75928ms step_avg:53.36ms
step:1424/1880 train_time:76015ms step_avg:53.38ms
step:1425/1880 train_time:76104ms step_avg:53.41ms
step:1426/1880 train_time:76191ms step_avg:53.43ms
step:1427/1880 train_time:76280ms step_avg:53.45ms
step:1428/1880 train_time:76368ms step_avg:53.48ms
step:1429/1880 train_time:76456ms step_avg:53.50ms
step:1430/1880 train_time:76543ms step_avg:53.53ms
step:1431/1880 train_time:76633ms step_avg:53.55ms
step:1432/1880 train_time:76720ms step_avg:53.58ms
step:1433/1880 train_time:76807ms step_avg:53.60ms
step:1434/1880 train_time:76895ms step_avg:53.62ms
step:1435/1880 train_time:76983ms step_avg:53.65ms
step:1436/1880 train_time:77071ms step_avg:53.67ms
step:1437/1880 train_time:77159ms step_avg:53.69ms
step:1438/1880 train_time:77247ms step_avg:53.72ms
step:1439/1880 train_time:77335ms step_avg:53.74ms
step:1440/1880 train_time:77423ms step_avg:53.77ms
step:1441/1880 train_time:77511ms step_avg:53.79ms
step:1442/1880 train_time:77600ms step_avg:53.81ms
step:1443/1880 train_time:77688ms step_avg:53.84ms
step:1444/1880 train_time:77774ms step_avg:53.86ms
step:1445/1880 train_time:77862ms step_avg:53.88ms
step:1446/1880 train_time:77951ms step_avg:53.91ms
step:1447/1880 train_time:78038ms step_avg:53.93ms
step:1448/1880 train_time:78127ms step_avg:53.96ms
step:1449/1880 train_time:78215ms step_avg:53.98ms
step:1450/1880 train_time:78303ms step_avg:54.00ms
step:1451/1880 train_time:78391ms step_avg:54.03ms
step:1452/1880 train_time:78478ms step_avg:54.05ms
step:1453/1880 train_time:78566ms step_avg:54.07ms
step:1454/1880 train_time:78654ms step_avg:54.10ms
step:1455/1880 train_time:78742ms step_avg:54.12ms
step:1456/1880 train_time:78831ms step_avg:54.14ms
step:1457/1880 train_time:78920ms step_avg:54.17ms
step:1458/1880 train_time:79007ms step_avg:54.19ms
step:1459/1880 train_time:79095ms step_avg:54.21ms
step:1460/1880 train_time:79183ms step_avg:54.24ms
step:1461/1880 train_time:79273ms step_avg:54.26ms
step:1462/1880 train_time:79360ms step_avg:54.28ms
step:1463/1880 train_time:79447ms step_avg:54.30ms
step:1464/1880 train_time:79535ms step_avg:54.33ms
step:1465/1880 train_time:79623ms step_avg:54.35ms
step:1466/1880 train_time:79711ms step_avg:54.37ms
step:1467/1880 train_time:79799ms step_avg:54.40ms
step:1468/1880 train_time:79887ms step_avg:54.42ms
step:1469/1880 train_time:79975ms step_avg:54.44ms
step:1470/1880 train_time:80063ms step_avg:54.46ms
step:1471/1880 train_time:80151ms step_avg:54.49ms
step:1472/1880 train_time:80239ms step_avg:54.51ms
step:1473/1880 train_time:80328ms step_avg:54.53ms
step:1474/1880 train_time:80415ms step_avg:54.56ms
step:1475/1880 train_time:80503ms step_avg:54.58ms
step:1476/1880 train_time:80590ms step_avg:54.60ms
step:1477/1880 train_time:80678ms step_avg:54.62ms
step:1478/1880 train_time:80766ms step_avg:54.65ms
step:1479/1880 train_time:80854ms step_avg:54.67ms
step:1480/1880 train_time:80943ms step_avg:54.69ms
step:1481/1880 train_time:81031ms step_avg:54.71ms
step:1482/1880 train_time:81118ms step_avg:54.74ms
step:1483/1880 train_time:81207ms step_avg:54.76ms
step:1484/1880 train_time:81295ms step_avg:54.78ms
step:1485/1880 train_time:81383ms step_avg:54.80ms
step:1486/1880 train_time:81472ms step_avg:54.83ms
step:1487/1880 train_time:81560ms step_avg:54.85ms
step:1488/1880 train_time:81649ms step_avg:54.87ms
step:1489/1880 train_time:81736ms step_avg:54.89ms
step:1490/1880 train_time:81825ms step_avg:54.92ms
step:1491/1880 train_time:81913ms step_avg:54.94ms
step:1492/1880 train_time:82001ms step_avg:54.96ms
step:1493/1880 train_time:82089ms step_avg:54.98ms
step:1494/1880 train_time:82176ms step_avg:55.00ms
step:1495/1880 train_time:82265ms step_avg:55.03ms
step:1496/1880 train_time:82353ms step_avg:55.05ms
step:1497/1880 train_time:82441ms step_avg:55.07ms
step:1498/1880 train_time:82529ms step_avg:55.09ms
step:1499/1880 train_time:82617ms step_avg:55.11ms
step:1500/1880 train_time:82705ms step_avg:55.14ms
step:1500/1880 val_loss:3.4056 train_time:82796ms step_avg:55.20ms
step:1501/1880 train_time:82819ms step_avg:55.18ms
step:1502/1880 train_time:82885ms step_avg:55.18ms
step:1503/1880 train_time:82977ms step_avg:55.21ms
step:1504/1880 train_time:83063ms step_avg:55.23ms
step:1505/1880 train_time:83150ms step_avg:55.25ms
step:1506/1880 train_time:83238ms step_avg:55.27ms
step:1507/1880 train_time:83324ms step_avg:55.29ms
step:1508/1880 train_time:83411ms step_avg:55.31ms
step:1509/1880 train_time:83498ms step_avg:55.33ms
step:1510/1880 train_time:83586ms step_avg:55.35ms
step:1511/1880 train_time:83675ms step_avg:55.38ms
step:1512/1880 train_time:83764ms step_avg:55.40ms
step:1513/1880 train_time:83856ms step_avg:55.42ms
step:1514/1880 train_time:83947ms step_avg:55.45ms
step:1515/1880 train_time:84035ms step_avg:55.47ms
step:1516/1880 train_time:84122ms step_avg:55.49ms
step:1517/1880 train_time:84209ms step_avg:55.51ms
step:1518/1880 train_time:84296ms step_avg:55.53ms
step:1519/1880 train_time:84383ms step_avg:55.55ms
step:1520/1880 train_time:84470ms step_avg:55.57ms
step:1521/1880 train_time:84557ms step_avg:55.59ms
step:1522/1880 train_time:84645ms step_avg:55.61ms
step:1523/1880 train_time:84734ms step_avg:55.64ms
step:1524/1880 train_time:84824ms step_avg:55.66ms
step:1525/1880 train_time:84913ms step_avg:55.68ms
step:1526/1880 train_time:85002ms step_avg:55.70ms
step:1527/1880 train_time:85090ms step_avg:55.72ms
step:1528/1880 train_time:85177ms step_avg:55.74ms
step:1529/1880 train_time:85264ms step_avg:55.76ms
step:1530/1880 train_time:85352ms step_avg:55.79ms
step:1531/1880 train_time:85440ms step_avg:55.81ms
step:1532/1880 train_time:85527ms step_avg:55.83ms
step:1533/1880 train_time:85615ms step_avg:55.85ms
step:1534/1880 train_time:85703ms step_avg:55.87ms
step:1535/1880 train_time:85793ms step_avg:55.89ms
step:1536/1880 train_time:85883ms step_avg:55.91ms
step:1537/1880 train_time:85973ms step_avg:55.94ms
step:1538/1880 train_time:86061ms step_avg:55.96ms
step:1539/1880 train_time:86150ms step_avg:55.98ms
step:1540/1880 train_time:86237ms step_avg:56.00ms
step:1541/1880 train_time:86324ms step_avg:56.02ms
step:1542/1880 train_time:86411ms step_avg:56.04ms
step:1543/1880 train_time:86499ms step_avg:56.06ms
step:1544/1880 train_time:86587ms step_avg:56.08ms
step:1545/1880 train_time:86676ms step_avg:56.10ms
step:1546/1880 train_time:86764ms step_avg:56.12ms
step:1547/1880 train_time:86854ms step_avg:56.14ms
step:1548/1880 train_time:86942ms step_avg:56.16ms
step:1549/1880 train_time:87031ms step_avg:56.19ms
step:1550/1880 train_time:87119ms step_avg:56.21ms
step:1551/1880 train_time:87207ms step_avg:56.23ms
step:1552/1880 train_time:87295ms step_avg:56.25ms
step:1553/1880 train_time:87382ms step_avg:56.27ms
step:1554/1880 train_time:87471ms step_avg:56.29ms
step:1555/1880 train_time:87560ms step_avg:56.31ms
step:1556/1880 train_time:87648ms step_avg:56.33ms
step:1557/1880 train_time:87736ms step_avg:56.35ms
step:1558/1880 train_time:87824ms step_avg:56.37ms
step:1559/1880 train_time:87914ms step_avg:56.39ms
step:1560/1880 train_time:88002ms step_avg:56.41ms
step:1561/1880 train_time:88090ms step_avg:56.43ms
step:1562/1880 train_time:88178ms step_avg:56.45ms
step:1563/1880 train_time:88266ms step_avg:56.47ms
step:1564/1880 train_time:88354ms step_avg:56.49ms
step:1565/1880 train_time:88441ms step_avg:56.51ms
step:1566/1880 train_time:88528ms step_avg:56.53ms
step:1567/1880 train_time:88616ms step_avg:56.55ms
step:1568/1880 train_time:88704ms step_avg:56.57ms
step:1569/1880 train_time:88792ms step_avg:56.59ms
step:1570/1880 train_time:88880ms step_avg:56.61ms
step:1571/1880 train_time:88969ms step_avg:56.63ms
step:1572/1880 train_time:89057ms step_avg:56.65ms
step:1573/1880 train_time:89145ms step_avg:56.67ms
step:1574/1880 train_time:89232ms step_avg:56.69ms
step:1575/1880 train_time:89320ms step_avg:56.71ms
step:1576/1880 train_time:89408ms step_avg:56.73ms
step:1577/1880 train_time:89496ms step_avg:56.75ms
step:1578/1880 train_time:89583ms step_avg:56.77ms
step:1579/1880 train_time:89671ms step_avg:56.79ms
step:1580/1880 train_time:89760ms step_avg:56.81ms
step:1581/1880 train_time:89848ms step_avg:56.83ms
step:1582/1880 train_time:89938ms step_avg:56.85ms
step:1583/1880 train_time:90026ms step_avg:56.87ms
step:1584/1880 train_time:90114ms step_avg:56.89ms
step:1585/1880 train_time:90202ms step_avg:56.91ms
step:1586/1880 train_time:90290ms step_avg:56.93ms
step:1587/1880 train_time:90379ms step_avg:56.95ms
step:1588/1880 train_time:90466ms step_avg:56.97ms
step:1589/1880 train_time:90554ms step_avg:56.99ms
step:1590/1880 train_time:90641ms step_avg:57.01ms
step:1591/1880 train_time:90730ms step_avg:57.03ms
step:1592/1880 train_time:90818ms step_avg:57.05ms
step:1593/1880 train_time:90906ms step_avg:57.07ms
step:1594/1880 train_time:90994ms step_avg:57.09ms
step:1595/1880 train_time:91082ms step_avg:57.10ms
step:1596/1880 train_time:91169ms step_avg:57.12ms
step:1597/1880 train_time:91258ms step_avg:57.14ms
step:1598/1880 train_time:91346ms step_avg:57.16ms
step:1599/1880 train_time:91434ms step_avg:57.18ms
step:1600/1880 train_time:91522ms step_avg:57.20ms
step:1601/1880 train_time:91609ms step_avg:57.22ms
step:1602/1880 train_time:91699ms step_avg:57.24ms
step:1603/1880 train_time:91786ms step_avg:57.26ms
step:1604/1880 train_time:91875ms step_avg:57.28ms
step:1605/1880 train_time:91963ms step_avg:57.30ms
step:1606/1880 train_time:92051ms step_avg:57.32ms
step:1607/1880 train_time:92139ms step_avg:57.34ms
step:1608/1880 train_time:92227ms step_avg:57.36ms
step:1609/1880 train_time:92316ms step_avg:57.37ms
step:1610/1880 train_time:92403ms step_avg:57.39ms
step:1611/1880 train_time:92491ms step_avg:57.41ms
step:1612/1880 train_time:92579ms step_avg:57.43ms
step:1613/1880 train_time:92668ms step_avg:57.45ms
step:1614/1880 train_time:92756ms step_avg:57.47ms
step:1615/1880 train_time:92844ms step_avg:57.49ms
step:1616/1880 train_time:92933ms step_avg:57.51ms
step:1617/1880 train_time:93021ms step_avg:57.53ms
step:1618/1880 train_time:93109ms step_avg:57.55ms
step:1619/1880 train_time:93197ms step_avg:57.56ms
step:1620/1880 train_time:93284ms step_avg:57.58ms
step:1621/1880 train_time:93372ms step_avg:57.60ms
step:1622/1880 train_time:93460ms step_avg:57.62ms
step:1623/1880 train_time:93548ms step_avg:57.64ms
step:1624/1880 train_time:93637ms step_avg:57.66ms
step:1625/1880 train_time:93725ms step_avg:57.68ms
step:1626/1880 train_time:93813ms step_avg:57.70ms
step:1627/1880 train_time:93902ms step_avg:57.72ms
step:1628/1880 train_time:93990ms step_avg:57.73ms
step:1629/1880 train_time:94078ms step_avg:57.75ms
step:1630/1880 train_time:94166ms step_avg:57.77ms
step:1631/1880 train_time:94255ms step_avg:57.79ms
step:1632/1880 train_time:94343ms step_avg:57.81ms
step:1633/1880 train_time:94431ms step_avg:57.83ms
step:1634/1880 train_time:94520ms step_avg:57.85ms
step:1635/1880 train_time:94608ms step_avg:57.86ms
step:1636/1880 train_time:94697ms step_avg:57.88ms
step:1637/1880 train_time:94784ms step_avg:57.90ms
step:1638/1880 train_time:94873ms step_avg:57.92ms
step:1639/1880 train_time:94963ms step_avg:57.94ms
step:1640/1880 train_time:95051ms step_avg:57.96ms
step:1641/1880 train_time:95139ms step_avg:57.98ms
step:1642/1880 train_time:95227ms step_avg:57.99ms
step:1643/1880 train_time:95314ms step_avg:58.01ms
step:1644/1880 train_time:95402ms step_avg:58.03ms
step:1645/1880 train_time:95490ms step_avg:58.05ms
step:1646/1880 train_time:95578ms step_avg:58.07ms
step:1647/1880 train_time:95666ms step_avg:58.09ms
step:1648/1880 train_time:95753ms step_avg:58.10ms
step:1649/1880 train_time:95842ms step_avg:58.12ms
step:1650/1880 train_time:95930ms step_avg:58.14ms
step:1651/1880 train_time:96018ms step_avg:58.16ms
step:1652/1880 train_time:96105ms step_avg:58.18ms
step:1653/1880 train_time:96194ms step_avg:58.19ms
step:1654/1880 train_time:96281ms step_avg:58.21ms
step:1655/1880 train_time:96369ms step_avg:58.23ms
step:1656/1880 train_time:96456ms step_avg:58.25ms
step:1657/1880 train_time:96544ms step_avg:58.26ms
step:1658/1880 train_time:96632ms step_avg:58.28ms
step:1659/1880 train_time:96721ms step_avg:58.30ms
step:1660/1880 train_time:96809ms step_avg:58.32ms
step:1661/1880 train_time:96898ms step_avg:58.34ms
step:1662/1880 train_time:96985ms step_avg:58.35ms
step:1663/1880 train_time:97074ms step_avg:58.37ms
step:1664/1880 train_time:97162ms step_avg:58.39ms
step:1665/1880 train_time:97250ms step_avg:58.41ms
step:1666/1880 train_time:97339ms step_avg:58.43ms
step:1667/1880 train_time:97427ms step_avg:58.44ms
step:1668/1880 train_time:97514ms step_avg:58.46ms
step:1669/1880 train_time:97603ms step_avg:58.48ms
step:1670/1880 train_time:97691ms step_avg:58.50ms
step:1671/1880 train_time:97778ms step_avg:58.51ms
step:1672/1880 train_time:97866ms step_avg:58.53ms
step:1673/1880 train_time:97954ms step_avg:58.55ms
step:1674/1880 train_time:98042ms step_avg:58.57ms
step:1675/1880 train_time:98130ms step_avg:58.59ms
step:1676/1880 train_time:98219ms step_avg:58.60ms
step:1677/1880 train_time:98307ms step_avg:58.62ms
step:1678/1880 train_time:98397ms step_avg:58.64ms
step:1679/1880 train_time:98484ms step_avg:58.66ms
step:1680/1880 train_time:98573ms step_avg:58.67ms
step:1681/1880 train_time:98661ms step_avg:58.69ms
step:1682/1880 train_time:98749ms step_avg:58.71ms
step:1683/1880 train_time:98837ms step_avg:58.73ms
step:1684/1880 train_time:98925ms step_avg:58.74ms
step:1685/1880 train_time:99013ms step_avg:58.76ms
step:1686/1880 train_time:99101ms step_avg:58.78ms
step:1687/1880 train_time:99189ms step_avg:58.80ms
step:1688/1880 train_time:99279ms step_avg:58.81ms
step:1689/1880 train_time:99367ms step_avg:58.83ms
step:1690/1880 train_time:99455ms step_avg:58.85ms
step:1691/1880 train_time:99543ms step_avg:58.87ms
step:1692/1880 train_time:99631ms step_avg:58.88ms
step:1693/1880 train_time:99719ms step_avg:58.90ms
step:1694/1880 train_time:99807ms step_avg:58.92ms
step:1695/1880 train_time:99895ms step_avg:58.94ms
step:1696/1880 train_time:99983ms step_avg:58.95ms
step:1697/1880 train_time:100072ms step_avg:58.97ms
step:1698/1880 train_time:100160ms step_avg:58.99ms
step:1699/1880 train_time:100248ms step_avg:59.00ms
step:1700/1880 train_time:100336ms step_avg:59.02ms
step:1701/1880 train_time:100423ms step_avg:59.04ms
step:1702/1880 train_time:100512ms step_avg:59.06ms
step:1703/1880 train_time:100600ms step_avg:59.07ms
step:1704/1880 train_time:100688ms step_avg:59.09ms
step:1705/1880 train_time:100777ms step_avg:59.11ms
step:1706/1880 train_time:100865ms step_avg:59.12ms
step:1707/1880 train_time:100953ms step_avg:59.14ms
step:1708/1880 train_time:101041ms step_avg:59.16ms
step:1709/1880 train_time:101129ms step_avg:59.17ms
step:1710/1880 train_time:101218ms step_avg:59.19ms
step:1711/1880 train_time:101305ms step_avg:59.21ms
step:1712/1880 train_time:101393ms step_avg:59.23ms
step:1713/1880 train_time:101483ms step_avg:59.24ms
step:1714/1880 train_time:101571ms step_avg:59.26ms
step:1715/1880 train_time:101660ms step_avg:59.28ms
step:1716/1880 train_time:101748ms step_avg:59.29ms
step:1717/1880 train_time:101837ms step_avg:59.31ms
step:1718/1880 train_time:101923ms step_avg:59.33ms
step:1719/1880 train_time:102012ms step_avg:59.34ms
step:1720/1880 train_time:102101ms step_avg:59.36ms
step:1721/1880 train_time:102189ms step_avg:59.38ms
step:1722/1880 train_time:102277ms step_avg:59.39ms
step:1723/1880 train_time:102364ms step_avg:59.41ms
step:1724/1880 train_time:102453ms step_avg:59.43ms
step:1725/1880 train_time:102541ms step_avg:59.44ms
step:1726/1880 train_time:102629ms step_avg:59.46ms
step:1727/1880 train_time:102718ms step_avg:59.48ms
step:1728/1880 train_time:102806ms step_avg:59.49ms
step:1729/1880 train_time:102894ms step_avg:59.51ms
step:1730/1880 train_time:102982ms step_avg:59.53ms
step:1731/1880 train_time:103070ms step_avg:59.54ms
step:1732/1880 train_time:103158ms step_avg:59.56ms
step:1733/1880 train_time:103246ms step_avg:59.58ms
step:1734/1880 train_time:103333ms step_avg:59.59ms
step:1735/1880 train_time:103421ms step_avg:59.61ms
step:1736/1880 train_time:103510ms step_avg:59.63ms
step:1737/1880 train_time:103598ms step_avg:59.64ms
step:1738/1880 train_time:103686ms step_avg:59.66ms
step:1739/1880 train_time:103775ms step_avg:59.67ms
step:1740/1880 train_time:103862ms step_avg:59.69ms
step:1741/1880 train_time:103951ms step_avg:59.71ms
step:1742/1880 train_time:104039ms step_avg:59.72ms
step:1743/1880 train_time:104126ms step_avg:59.74ms
step:1744/1880 train_time:104215ms step_avg:59.76ms
step:1745/1880 train_time:104302ms step_avg:59.77ms
step:1746/1880 train_time:104390ms step_avg:59.79ms
step:1747/1880 train_time:104478ms step_avg:59.80ms
step:1748/1880 train_time:104565ms step_avg:59.82ms
step:1749/1880 train_time:104653ms step_avg:59.84ms
step:1750/1880 train_time:104741ms step_avg:59.85ms
step:1750/1880 val_loss:3.3115 train_time:104832ms step_avg:59.90ms
step:1751/1880 train_time:104857ms step_avg:59.88ms
step:1752/1880 train_time:104922ms step_avg:59.89ms
step:1753/1880 train_time:105012ms step_avg:59.90ms
step:1754/1880 train_time:105100ms step_avg:59.92ms
step:1755/1880 train_time:105187ms step_avg:59.94ms
step:1756/1880 train_time:105274ms step_avg:59.95ms
step:1757/1880 train_time:105361ms step_avg:59.97ms
step:1758/1880 train_time:105448ms step_avg:59.98ms
step:1759/1880 train_time:105535ms step_avg:60.00ms
step:1760/1880 train_time:105624ms step_avg:60.01ms
step:1761/1880 train_time:105710ms step_avg:60.03ms
step:1762/1880 train_time:105800ms step_avg:60.05ms
step:1763/1880 train_time:105890ms step_avg:60.06ms
step:1764/1880 train_time:105979ms step_avg:60.08ms
step:1765/1880 train_time:106067ms step_avg:60.09ms
step:1766/1880 train_time:106155ms step_avg:60.11ms
step:1767/1880 train_time:106243ms step_avg:60.13ms
step:1768/1880 train_time:106330ms step_avg:60.14ms
step:1769/1880 train_time:106417ms step_avg:60.16ms
step:1770/1880 train_time:106504ms step_avg:60.17ms
step:1771/1880 train_time:106591ms step_avg:60.19ms
step:1772/1880 train_time:106679ms step_avg:60.20ms
step:1773/1880 train_time:106768ms step_avg:60.22ms
step:1774/1880 train_time:106858ms step_avg:60.24ms
step:1775/1880 train_time:106947ms step_avg:60.25ms
step:1776/1880 train_time:107035ms step_avg:60.27ms
step:1777/1880 train_time:107123ms step_avg:60.28ms
step:1778/1880 train_time:107210ms step_avg:60.30ms
step:1779/1880 train_time:107298ms step_avg:60.31ms
step:1780/1880 train_time:107386ms step_avg:60.33ms
step:1781/1880 train_time:107474ms step_avg:60.34ms
step:1782/1880 train_time:107561ms step_avg:60.36ms
step:1783/1880 train_time:107649ms step_avg:60.38ms
step:1784/1880 train_time:107737ms step_avg:60.39ms
step:1785/1880 train_time:107826ms step_avg:60.41ms
step:1786/1880 train_time:107914ms step_avg:60.42ms
step:1787/1880 train_time:108003ms step_avg:60.44ms
step:1788/1880 train_time:108091ms step_avg:60.45ms
step:1789/1880 train_time:108179ms step_avg:60.47ms
step:1790/1880 train_time:108266ms step_avg:60.48ms
step:1791/1880 train_time:108353ms step_avg:60.50ms
step:1792/1880 train_time:108441ms step_avg:60.51ms
step:1793/1880 train_time:108528ms step_avg:60.53ms
step:1794/1880 train_time:108618ms step_avg:60.55ms
step:1795/1880 train_time:108707ms step_avg:60.56ms
step:1796/1880 train_time:108796ms step_avg:60.58ms
step:1797/1880 train_time:108884ms step_avg:60.59ms
step:1798/1880 train_time:108972ms step_avg:60.61ms
step:1799/1880 train_time:109061ms step_avg:60.62ms
step:1800/1880 train_time:109148ms step_avg:60.64ms
step:1801/1880 train_time:109236ms step_avg:60.65ms
step:1802/1880 train_time:109325ms step_avg:60.67ms
step:1803/1880 train_time:109412ms step_avg:60.68ms
step:1804/1880 train_time:109499ms step_avg:60.70ms
step:1805/1880 train_time:109587ms step_avg:60.71ms
step:1806/1880 train_time:109677ms step_avg:60.73ms
step:1807/1880 train_time:109765ms step_avg:60.74ms
step:1808/1880 train_time:109854ms step_avg:60.76ms
step:1809/1880 train_time:109943ms step_avg:60.78ms
step:1810/1880 train_time:110029ms step_avg:60.79ms
step:1811/1880 train_time:110118ms step_avg:60.81ms
step:1812/1880 train_time:110206ms step_avg:60.82ms
step:1813/1880 train_time:110294ms step_avg:60.84ms
step:1814/1880 train_time:110382ms step_avg:60.85ms
step:1815/1880 train_time:110470ms step_avg:60.86ms
step:1816/1880 train_time:110558ms step_avg:60.88ms
step:1817/1880 train_time:110647ms step_avg:60.90ms
step:1818/1880 train_time:110735ms step_avg:60.91ms
step:1819/1880 train_time:110824ms step_avg:60.93ms
step:1820/1880 train_time:110913ms step_avg:60.94ms
step:1821/1880 train_time:111000ms step_avg:60.96ms
step:1822/1880 train_time:111088ms step_avg:60.97ms
step:1823/1880 train_time:111177ms step_avg:60.99ms
step:1824/1880 train_time:111265ms step_avg:61.00ms
step:1825/1880 train_time:111352ms step_avg:61.01ms
step:1826/1880 train_time:111439ms step_avg:61.03ms
step:1827/1880 train_time:111527ms step_avg:61.04ms
step:1828/1880 train_time:111615ms step_avg:61.06ms
step:1829/1880 train_time:111704ms step_avg:61.07ms
step:1830/1880 train_time:111793ms step_avg:61.09ms
step:1831/1880 train_time:111882ms step_avg:61.10ms
step:1832/1880 train_time:111969ms step_avg:61.12ms
step:1833/1880 train_time:112059ms step_avg:61.13ms
step:1834/1880 train_time:112146ms step_avg:61.15ms
step:1835/1880 train_time:112233ms step_avg:61.16ms
step:1836/1880 train_time:112321ms step_avg:61.18ms
step:1837/1880 train_time:112409ms step_avg:61.19ms
step:1838/1880 train_time:112497ms step_avg:61.21ms
step:1839/1880 train_time:112586ms step_avg:61.22ms
step:1840/1880 train_time:112675ms step_avg:61.24ms
step:1841/1880 train_time:112764ms step_avg:61.25ms
step:1842/1880 train_time:112852ms step_avg:61.27ms
step:1843/1880 train_time:112940ms step_avg:61.28ms
step:1844/1880 train_time:113028ms step_avg:61.30ms
step:1845/1880 train_time:113117ms step_avg:61.31ms
step:1846/1880 train_time:113205ms step_avg:61.32ms
step:1847/1880 train_time:113293ms step_avg:61.34ms
step:1848/1880 train_time:113381ms step_avg:61.35ms
step:1849/1880 train_time:113468ms step_avg:61.37ms
step:1850/1880 train_time:113556ms step_avg:61.38ms
step:1851/1880 train_time:113646ms step_avg:61.40ms
step:1852/1880 train_time:113737ms step_avg:61.41ms
step:1853/1880 train_time:113823ms step_avg:61.43ms
step:1854/1880 train_time:113910ms step_avg:61.44ms
step:1855/1880 train_time:113999ms step_avg:61.45ms
step:1856/1880 train_time:114086ms step_avg:61.47ms
step:1857/1880 train_time:114174ms step_avg:61.48ms
step:1858/1880 train_time:114263ms step_avg:61.50ms
step:1859/1880 train_time:114351ms step_avg:61.51ms
step:1860/1880 train_time:114440ms step_avg:61.53ms
step:1861/1880 train_time:114528ms step_avg:61.54ms
step:1862/1880 train_time:114617ms step_avg:61.56ms
step:1863/1880 train_time:114706ms step_avg:61.57ms
step:1864/1880 train_time:114795ms step_avg:61.59ms
step:1865/1880 train_time:114884ms step_avg:61.60ms
step:1866/1880 train_time:114974ms step_avg:61.62ms
step:1867/1880 train_time:115062ms step_avg:61.63ms
step:1868/1880 train_time:115150ms step_avg:61.64ms
step:1869/1880 train_time:115238ms step_avg:61.66ms
step:1870/1880 train_time:115326ms step_avg:61.67ms
step:1871/1880 train_time:115414ms step_avg:61.69ms
step:1872/1880 train_time:115502ms step_avg:61.70ms
step:1873/1880 train_time:115591ms step_avg:61.71ms
step:1874/1880 train_time:115679ms step_avg:61.73ms
step:1875/1880 train_time:115768ms step_avg:61.74ms
step:1876/1880 train_time:115857ms step_avg:61.76ms
step:1877/1880 train_time:115946ms step_avg:61.77ms
step:1878/1880 train_time:116034ms step_avg:61.79ms
step:1879/1880 train_time:116122ms step_avg:61.80ms
step:1880/1880 train_time:116210ms step_avg:61.81ms
step:1880/1880 val_loss:3.2770 train_time:116300ms step_avg:61.86ms
peak memory allocated: 29709 MiB reserved: 44338 MiB

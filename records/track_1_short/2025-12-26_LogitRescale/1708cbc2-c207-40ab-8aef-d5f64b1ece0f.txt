import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))


@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn_gate (10 params 6 padding params)
        2. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        3. reduce scatter attn/mlp round 2 (16 mlp params)
        4. wait on step 1, then compute update of 1 and schedule all gather
        5. wait on step 2, then compute update of 2 and schedule all gather
        6. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        7. wait on 4, then compute update of 4 and schedule all gather
        8. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn_gate', 'attn', 'mlp']
        group_sizes = [10, 16, 16]
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':

                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'

                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['attn_gate']

            if "second_momentum_buffer" not in group:
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str],lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.bfloat16, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-1]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                # cautious weight decay
                mask = (update * p_slice) > 0
                # lr as weight decay schedule
                eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                update.addcmul_(p_slice, mask, value=eff_weight_decay * lr)

                p_slice.add_(other=update, alpha=-1.0)

                if not is_small:
                    all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_offset: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.5/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_offset = key_offset[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits+5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0
            loss = (cross_entropy * mtp_weights).sum()
        elif self.training:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
        else:
            loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management

def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model

        adam_labels = ['lm_head', 'value_embed', 'smear_gate', 'x0_lambdas', 'embed']
        scalar_labels = ['scalars']
        muon_labels = ['attn_gate', 'attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        scalar_params = [p for p in model.parameters() if getattr(p, 'label', None) in scalar_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + scalar_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, lr=0.008, betas=(0.65, 0.95), eps=1e-8, weight_decay=0.005)
        self.scalar_opt = DistAdam(scalar_params, scalar_labels, lr=0.008, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.scalar_opt, self.muon_opt]
        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.freeze_timer = 0
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True
        self.scalar_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = [0]
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            if opt.freeze_timer > 0:
                opt.freeze_timer -= 1
                opt.zero_grad(set_to_none=True)
            else:
                if self._is_active_step(opt, step):
                    for group in opt.param_groups:
                        group["lr"] = group["initial_lr"] * step_lr
                    opt.step()
                    opt.zero_grad(set_to_none=True)
                    if opt.odd_step_only:
                        opt.should_sync = False
        
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True

    def start_transition(self, freeze_count=40):
        # freeze scalar weights during transition
        self.scalar_opt.freeze_timer = freeze_count
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1840  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
warmup_steps = sorted(set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0))
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        val_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args()) / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec 26 23:03:30 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            124W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   24C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   23C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   25C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     63708      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     63709      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     63710      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     63711      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     63712      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     63713      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     63714      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     63715      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 613, 614, 615, 1226, 1227, 1228, 1839, 1840, 1841] for warmup
Resetting Model
step:0/1880 val_loss:10.8295 train_time:0ms step_avg:0.04ms
step:1/1880 train_time:88ms step_avg:88.44ms
step:2/1880 train_time:117ms step_avg:58.50ms
step:3/1880 train_time:142ms step_avg:47.22ms
step:4/1880 train_time:165ms step_avg:41.29ms
step:5/1880 train_time:195ms step_avg:38.90ms
step:6/1880 train_time:384ms step_avg:64.08ms
step:7/1880 train_time:407ms step_avg:58.20ms
step:8/1880 train_time:429ms step_avg:53.59ms
step:9/1880 train_time:462ms step_avg:51.39ms
step:10/1880 train_time:497ms step_avg:49.67ms
step:11/1880 train_time:530ms step_avg:48.21ms
step:12/1880 train_time:565ms step_avg:47.05ms
step:13/1880 train_time:598ms step_avg:46.04ms
step:14/1880 train_time:633ms step_avg:45.19ms
step:15/1880 train_time:666ms step_avg:44.43ms
step:16/1880 train_time:701ms step_avg:43.80ms
step:17/1880 train_time:735ms step_avg:43.22ms
step:18/1880 train_time:769ms step_avg:42.71ms
step:19/1880 train_time:802ms step_avg:42.23ms
step:20/1880 train_time:836ms step_avg:41.82ms
step:21/1880 train_time:870ms step_avg:41.44ms
step:22/1880 train_time:905ms step_avg:41.12ms
step:23/1880 train_time:939ms step_avg:40.81ms
step:24/1880 train_time:973ms step_avg:40.55ms
step:25/1880 train_time:1007ms step_avg:40.28ms
step:26/1880 train_time:1041ms step_avg:40.06ms
step:27/1880 train_time:1075ms step_avg:39.82ms
step:28/1880 train_time:1109ms step_avg:39.62ms
step:29/1880 train_time:1143ms step_avg:39.42ms
step:30/1880 train_time:1177ms step_avg:39.24ms
step:31/1880 train_time:1211ms step_avg:39.07ms
step:32/1880 train_time:1245ms step_avg:38.91ms
step:33/1880 train_time:1280ms step_avg:38.78ms
step:34/1880 train_time:1315ms step_avg:38.68ms
step:35/1880 train_time:1350ms step_avg:38.58ms
step:36/1880 train_time:1386ms step_avg:38.49ms
step:37/1880 train_time:1420ms step_avg:38.39ms
step:38/1880 train_time:1455ms step_avg:38.30ms
step:39/1880 train_time:1489ms step_avg:38.19ms
step:40/1880 train_time:1523ms step_avg:38.09ms
step:41/1880 train_time:1557ms step_avg:37.98ms
step:42/1880 train_time:1592ms step_avg:37.90ms
step:43/1880 train_time:1626ms step_avg:37.81ms
step:44/1880 train_time:1660ms step_avg:37.73ms
step:45/1880 train_time:1694ms step_avg:37.65ms
step:46/1880 train_time:1729ms step_avg:37.58ms
step:47/1880 train_time:1763ms step_avg:37.50ms
step:48/1880 train_time:1797ms step_avg:37.44ms
step:49/1880 train_time:1831ms step_avg:37.36ms
step:50/1880 train_time:1865ms step_avg:37.30ms
step:51/1880 train_time:1899ms step_avg:37.23ms
step:52/1880 train_time:1933ms step_avg:37.17ms
step:53/1880 train_time:1967ms step_avg:37.11ms
step:54/1880 train_time:2001ms step_avg:37.06ms
step:55/1880 train_time:2035ms step_avg:37.00ms
step:56/1880 train_time:2069ms step_avg:36.95ms
step:57/1880 train_time:2103ms step_avg:36.90ms
step:58/1880 train_time:2138ms step_avg:36.86ms
step:59/1880 train_time:2171ms step_avg:36.80ms
step:60/1880 train_time:2206ms step_avg:36.76ms
step:61/1880 train_time:2240ms step_avg:36.72ms
step:62/1880 train_time:2274ms step_avg:36.68ms
step:63/1880 train_time:2308ms step_avg:36.64ms
step:64/1880 train_time:2343ms step_avg:36.61ms
step:65/1880 train_time:2377ms step_avg:36.57ms
step:66/1880 train_time:2412ms step_avg:36.54ms
step:67/1880 train_time:2446ms step_avg:36.51ms
step:68/1880 train_time:2482ms step_avg:36.49ms
step:69/1880 train_time:2514ms step_avg:36.44ms
step:70/1880 train_time:2549ms step_avg:36.41ms
step:71/1880 train_time:2583ms step_avg:36.38ms
step:72/1880 train_time:2617ms step_avg:36.35ms
step:73/1880 train_time:2651ms step_avg:36.31ms
step:74/1880 train_time:2686ms step_avg:36.29ms
step:75/1880 train_time:2720ms step_avg:36.27ms
step:76/1880 train_time:2754ms step_avg:36.24ms
step:77/1880 train_time:2788ms step_avg:36.21ms
step:78/1880 train_time:2823ms step_avg:36.19ms
step:79/1880 train_time:2856ms step_avg:36.16ms
step:80/1880 train_time:2891ms step_avg:36.13ms
step:81/1880 train_time:2925ms step_avg:36.11ms
step:82/1880 train_time:2959ms step_avg:36.08ms
step:83/1880 train_time:2992ms step_avg:36.05ms
step:84/1880 train_time:3027ms step_avg:36.04ms
step:85/1880 train_time:3061ms step_avg:36.01ms
step:86/1880 train_time:3095ms step_avg:35.99ms
step:87/1880 train_time:3129ms step_avg:35.97ms
step:88/1880 train_time:3164ms step_avg:35.95ms
step:89/1880 train_time:3197ms step_avg:35.93ms
step:90/1880 train_time:3232ms step_avg:35.91ms
step:91/1880 train_time:3266ms step_avg:35.89ms
step:92/1880 train_time:3301ms step_avg:35.88ms
step:93/1880 train_time:3335ms step_avg:35.86ms
step:94/1880 train_time:3369ms step_avg:35.84ms
step:95/1880 train_time:3404ms step_avg:35.83ms
step:96/1880 train_time:3438ms step_avg:35.82ms
step:97/1880 train_time:3472ms step_avg:35.79ms
step:98/1880 train_time:3506ms step_avg:35.78ms
step:99/1880 train_time:3540ms step_avg:35.76ms
step:100/1880 train_time:3575ms step_avg:35.75ms
step:101/1880 train_time:3609ms step_avg:35.73ms
step:102/1880 train_time:3643ms step_avg:35.72ms
step:103/1880 train_time:3677ms step_avg:35.70ms
step:104/1880 train_time:3711ms step_avg:35.69ms
step:105/1880 train_time:3746ms step_avg:35.67ms
step:106/1880 train_time:3780ms step_avg:35.66ms
step:107/1880 train_time:3814ms step_avg:35.65ms
step:108/1880 train_time:3849ms step_avg:35.64ms
step:109/1880 train_time:3883ms step_avg:35.62ms
step:110/1880 train_time:3917ms step_avg:35.61ms
step:111/1880 train_time:3951ms step_avg:35.59ms
step:112/1880 train_time:3985ms step_avg:35.58ms
step:113/1880 train_time:4019ms step_avg:35.57ms
step:114/1880 train_time:4053ms step_avg:35.56ms
step:115/1880 train_time:4087ms step_avg:35.54ms
step:116/1880 train_time:4122ms step_avg:35.53ms
step:117/1880 train_time:4155ms step_avg:35.52ms
step:118/1880 train_time:4190ms step_avg:35.51ms
step:119/1880 train_time:4223ms step_avg:35.49ms
step:120/1880 train_time:4257ms step_avg:35.48ms
step:121/1880 train_time:4291ms step_avg:35.46ms
step:122/1880 train_time:4325ms step_avg:35.45ms
step:123/1880 train_time:4359ms step_avg:35.44ms
step:124/1880 train_time:4394ms step_avg:35.43ms
step:125/1880 train_time:4428ms step_avg:35.42ms
step:126/1880 train_time:4462ms step_avg:35.41ms
step:127/1880 train_time:4496ms step_avg:35.40ms
step:128/1880 train_time:4531ms step_avg:35.40ms
step:129/1880 train_time:4564ms step_avg:35.38ms
step:130/1880 train_time:4599ms step_avg:35.37ms
step:131/1880 train_time:4632ms step_avg:35.36ms
step:132/1880 train_time:4667ms step_avg:35.36ms
step:133/1880 train_time:4701ms step_avg:35.35ms
step:134/1880 train_time:4735ms step_avg:35.34ms
step:135/1880 train_time:4769ms step_avg:35.33ms
step:136/1880 train_time:4804ms step_avg:35.32ms
step:137/1880 train_time:4838ms step_avg:35.31ms
step:138/1880 train_time:4872ms step_avg:35.31ms
step:139/1880 train_time:4906ms step_avg:35.30ms
step:140/1880 train_time:4941ms step_avg:35.29ms
step:141/1880 train_time:4975ms step_avg:35.28ms
step:142/1880 train_time:5009ms step_avg:35.28ms
step:143/1880 train_time:5043ms step_avg:35.27ms
step:144/1880 train_time:5078ms step_avg:35.26ms
step:145/1880 train_time:5111ms step_avg:35.25ms
step:146/1880 train_time:5146ms step_avg:35.24ms
step:147/1880 train_time:5179ms step_avg:35.23ms
step:148/1880 train_time:5213ms step_avg:35.23ms
step:149/1880 train_time:5247ms step_avg:35.21ms
step:150/1880 train_time:5281ms step_avg:35.21ms
step:151/1880 train_time:5315ms step_avg:35.20ms
step:152/1880 train_time:5349ms step_avg:35.19ms
step:153/1880 train_time:5383ms step_avg:35.18ms
step:154/1880 train_time:5417ms step_avg:35.17ms
step:155/1880 train_time:5451ms step_avg:35.17ms
step:156/1880 train_time:5485ms step_avg:35.16ms
step:157/1880 train_time:5519ms step_avg:35.15ms
step:158/1880 train_time:5553ms step_avg:35.15ms
step:159/1880 train_time:5587ms step_avg:35.14ms
step:160/1880 train_time:5621ms step_avg:35.13ms
step:161/1880 train_time:5655ms step_avg:35.12ms
step:162/1880 train_time:5689ms step_avg:35.12ms
step:163/1880 train_time:5723ms step_avg:35.11ms
step:164/1880 train_time:5758ms step_avg:35.11ms
step:165/1880 train_time:5792ms step_avg:35.10ms
step:166/1880 train_time:5826ms step_avg:35.10ms
step:167/1880 train_time:5860ms step_avg:35.09ms
step:168/1880 train_time:5894ms step_avg:35.09ms
step:169/1880 train_time:5928ms step_avg:35.08ms
step:170/1880 train_time:5963ms step_avg:35.08ms
step:171/1880 train_time:5997ms step_avg:35.07ms
step:172/1880 train_time:6031ms step_avg:35.06ms
step:173/1880 train_time:6065ms step_avg:35.06ms
step:174/1880 train_time:6100ms step_avg:35.06ms
step:175/1880 train_time:6134ms step_avg:35.05ms
step:176/1880 train_time:6168ms step_avg:35.05ms
step:177/1880 train_time:6202ms step_avg:35.04ms
step:178/1880 train_time:6236ms step_avg:35.03ms
step:179/1880 train_time:6270ms step_avg:35.03ms
step:180/1880 train_time:6304ms step_avg:35.02ms
step:181/1880 train_time:6338ms step_avg:35.02ms
step:182/1880 train_time:6373ms step_avg:35.01ms
step:183/1880 train_time:6406ms step_avg:35.01ms
step:184/1880 train_time:6441ms step_avg:35.00ms
step:185/1880 train_time:6475ms step_avg:35.00ms
step:186/1880 train_time:6509ms step_avg:34.99ms
step:187/1880 train_time:6543ms step_avg:34.99ms
step:188/1880 train_time:6577ms step_avg:34.98ms
step:189/1880 train_time:6611ms step_avg:34.98ms
step:190/1880 train_time:6645ms step_avg:34.97ms
step:191/1880 train_time:6679ms step_avg:34.97ms
step:192/1880 train_time:6713ms step_avg:34.96ms
step:193/1880 train_time:6747ms step_avg:34.96ms
step:194/1880 train_time:6781ms step_avg:34.95ms
step:195/1880 train_time:6814ms step_avg:34.95ms
step:196/1880 train_time:6848ms step_avg:34.94ms
step:197/1880 train_time:6882ms step_avg:34.93ms
step:198/1880 train_time:6916ms step_avg:34.93ms
step:199/1880 train_time:6950ms step_avg:34.92ms
step:200/1880 train_time:6984ms step_avg:34.92ms
step:201/1880 train_time:7018ms step_avg:34.92ms
step:202/1880 train_time:7052ms step_avg:34.91ms
step:203/1880 train_time:7086ms step_avg:34.91ms
step:204/1880 train_time:7121ms step_avg:34.90ms
step:205/1880 train_time:7154ms step_avg:34.90ms
step:206/1880 train_time:7189ms step_avg:34.90ms
step:207/1880 train_time:7223ms step_avg:34.89ms
step:208/1880 train_time:7257ms step_avg:34.89ms
step:209/1880 train_time:7291ms step_avg:34.88ms
step:210/1880 train_time:7325ms step_avg:34.88ms
step:211/1880 train_time:7359ms step_avg:34.88ms
step:212/1880 train_time:7394ms step_avg:34.88ms
step:213/1880 train_time:7428ms step_avg:34.87ms
step:214/1880 train_time:7462ms step_avg:34.87ms
step:215/1880 train_time:7495ms step_avg:34.86ms
step:216/1880 train_time:7529ms step_avg:34.86ms
step:217/1880 train_time:7563ms step_avg:34.85ms
step:218/1880 train_time:7597ms step_avg:34.85ms
step:219/1880 train_time:7631ms step_avg:34.85ms
step:220/1880 train_time:7665ms step_avg:34.84ms
step:221/1880 train_time:7699ms step_avg:34.84ms
step:222/1880 train_time:7734ms step_avg:34.84ms
step:223/1880 train_time:7768ms step_avg:34.83ms
step:224/1880 train_time:7802ms step_avg:34.83ms
step:225/1880 train_time:7836ms step_avg:34.83ms
step:226/1880 train_time:7870ms step_avg:34.82ms
step:227/1880 train_time:7904ms step_avg:34.82ms
step:228/1880 train_time:7938ms step_avg:34.81ms
step:229/1880 train_time:7971ms step_avg:34.81ms
step:230/1880 train_time:8006ms step_avg:34.81ms
step:231/1880 train_time:8040ms step_avg:34.81ms
step:232/1880 train_time:8074ms step_avg:34.80ms
step:233/1880 train_time:8108ms step_avg:34.80ms
step:234/1880 train_time:8142ms step_avg:34.80ms
step:235/1880 train_time:8176ms step_avg:34.79ms
step:236/1880 train_time:8210ms step_avg:34.79ms
step:237/1880 train_time:8244ms step_avg:34.78ms
step:238/1880 train_time:8278ms step_avg:34.78ms
step:239/1880 train_time:8312ms step_avg:34.78ms
step:240/1880 train_time:8346ms step_avg:34.78ms
step:241/1880 train_time:8380ms step_avg:34.77ms
step:242/1880 train_time:8414ms step_avg:34.77ms
step:243/1880 train_time:8448ms step_avg:34.77ms
step:244/1880 train_time:8483ms step_avg:34.76ms
step:245/1880 train_time:8516ms step_avg:34.76ms
step:246/1880 train_time:8551ms step_avg:34.76ms
step:247/1880 train_time:8585ms step_avg:34.76ms
step:248/1880 train_time:8619ms step_avg:34.75ms
step:249/1880 train_time:8653ms step_avg:34.75ms
step:250/1880 train_time:8687ms step_avg:34.75ms
step:250/1880 val_loss:4.6103 train_time:8724ms step_avg:34.89ms
step:251/1880 train_time:8750ms step_avg:34.86ms
step:252/1880 train_time:8772ms step_avg:34.81ms
step:253/1880 train_time:8792ms step_avg:34.75ms
step:254/1880 train_time:8828ms step_avg:34.75ms
step:255/1880 train_time:8862ms step_avg:34.75ms
step:256/1880 train_time:8897ms step_avg:34.76ms
step:257/1880 train_time:8931ms step_avg:34.75ms
step:258/1880 train_time:8966ms step_avg:34.75ms
step:259/1880 train_time:9000ms step_avg:34.75ms
step:260/1880 train_time:9034ms step_avg:34.75ms
step:261/1880 train_time:9068ms step_avg:34.74ms
step:262/1880 train_time:9102ms step_avg:34.74ms
step:263/1880 train_time:9136ms step_avg:34.74ms
step:264/1880 train_time:9170ms step_avg:34.74ms
step:265/1880 train_time:9204ms step_avg:34.73ms
step:266/1880 train_time:9238ms step_avg:34.73ms
step:267/1880 train_time:9272ms step_avg:34.73ms
step:268/1880 train_time:9306ms step_avg:34.72ms
step:269/1880 train_time:9340ms step_avg:34.72ms
step:270/1880 train_time:9374ms step_avg:34.72ms
step:271/1880 train_time:9408ms step_avg:34.71ms
step:272/1880 train_time:9442ms step_avg:34.71ms
step:273/1880 train_time:9475ms step_avg:34.71ms
step:274/1880 train_time:9509ms step_avg:34.71ms
step:275/1880 train_time:9543ms step_avg:34.70ms
step:276/1880 train_time:9577ms step_avg:34.70ms
step:277/1880 train_time:9611ms step_avg:34.70ms
step:278/1880 train_time:9645ms step_avg:34.70ms
step:279/1880 train_time:9680ms step_avg:34.69ms
step:280/1880 train_time:9714ms step_avg:34.69ms
step:281/1880 train_time:9749ms step_avg:34.69ms
step:282/1880 train_time:9784ms step_avg:34.69ms
step:283/1880 train_time:9818ms step_avg:34.69ms
step:284/1880 train_time:9852ms step_avg:34.69ms
step:285/1880 train_time:9886ms step_avg:34.69ms
step:286/1880 train_time:9920ms step_avg:34.69ms
step:287/1880 train_time:9954ms step_avg:34.68ms
step:288/1880 train_time:9989ms step_avg:34.68ms
step:289/1880 train_time:10023ms step_avg:34.68ms
step:290/1880 train_time:10057ms step_avg:34.68ms
step:291/1880 train_time:10091ms step_avg:34.68ms
step:292/1880 train_time:10126ms step_avg:34.68ms
step:293/1880 train_time:10160ms step_avg:34.67ms
step:294/1880 train_time:10194ms step_avg:34.67ms
step:295/1880 train_time:10228ms step_avg:34.67ms
step:296/1880 train_time:10262ms step_avg:34.67ms
step:297/1880 train_time:10295ms step_avg:34.66ms
step:298/1880 train_time:10329ms step_avg:34.66ms
step:299/1880 train_time:10363ms step_avg:34.66ms
step:300/1880 train_time:10397ms step_avg:34.66ms
step:301/1880 train_time:10431ms step_avg:34.65ms
step:302/1880 train_time:10465ms step_avg:34.65ms
step:303/1880 train_time:10499ms step_avg:34.65ms
step:304/1880 train_time:10533ms step_avg:34.65ms
step:305/1880 train_time:10567ms step_avg:34.65ms
step:306/1880 train_time:10601ms step_avg:34.64ms
step:307/1880 train_time:10635ms step_avg:34.64ms
step:308/1880 train_time:10669ms step_avg:34.64ms
step:309/1880 train_time:10703ms step_avg:34.64ms
step:310/1880 train_time:10738ms step_avg:34.64ms
step:311/1880 train_time:10772ms step_avg:34.64ms
step:312/1880 train_time:10806ms step_avg:34.63ms
step:313/1880 train_time:10840ms step_avg:34.63ms
step:314/1880 train_time:10874ms step_avg:34.63ms
step:315/1880 train_time:10908ms step_avg:34.63ms
step:316/1880 train_time:10943ms step_avg:34.63ms
step:317/1880 train_time:10977ms step_avg:34.63ms
step:318/1880 train_time:11011ms step_avg:34.63ms
step:319/1880 train_time:11045ms step_avg:34.62ms
step:320/1880 train_time:11080ms step_avg:34.62ms
step:321/1880 train_time:11114ms step_avg:34.62ms
step:322/1880 train_time:11148ms step_avg:34.62ms
step:323/1880 train_time:11182ms step_avg:34.62ms
step:324/1880 train_time:11216ms step_avg:34.62ms
step:325/1880 train_time:11250ms step_avg:34.61ms
step:326/1880 train_time:11284ms step_avg:34.61ms
step:327/1880 train_time:11317ms step_avg:34.61ms
step:328/1880 train_time:11351ms step_avg:34.61ms
step:329/1880 train_time:11385ms step_avg:34.61ms
step:330/1880 train_time:11419ms step_avg:34.60ms
step:331/1880 train_time:11453ms step_avg:34.60ms
step:332/1880 train_time:11487ms step_avg:34.60ms
step:333/1880 train_time:11521ms step_avg:34.60ms
step:334/1880 train_time:11555ms step_avg:34.60ms
step:335/1880 train_time:11588ms step_avg:34.59ms
step:336/1880 train_time:11623ms step_avg:34.59ms
step:337/1880 train_time:11656ms step_avg:34.59ms
step:338/1880 train_time:11690ms step_avg:34.59ms
step:339/1880 train_time:11725ms step_avg:34.59ms
step:340/1880 train_time:11759ms step_avg:34.58ms
step:341/1880 train_time:11793ms step_avg:34.58ms
step:342/1880 train_time:11827ms step_avg:34.58ms
step:343/1880 train_time:11860ms step_avg:34.58ms
step:344/1880 train_time:11895ms step_avg:34.58ms
step:345/1880 train_time:11929ms step_avg:34.58ms
step:346/1880 train_time:11963ms step_avg:34.58ms
step:347/1880 train_time:11997ms step_avg:34.57ms
step:348/1880 train_time:12031ms step_avg:34.57ms
step:349/1880 train_time:12066ms step_avg:34.57ms
step:350/1880 train_time:12100ms step_avg:34.57ms
step:351/1880 train_time:12134ms step_avg:34.57ms
step:352/1880 train_time:12169ms step_avg:34.57ms
step:353/1880 train_time:12202ms step_avg:34.57ms
step:354/1880 train_time:12237ms step_avg:34.57ms
step:355/1880 train_time:12270ms step_avg:34.56ms
step:356/1880 train_time:12305ms step_avg:34.56ms
step:357/1880 train_time:12339ms step_avg:34.56ms
step:358/1880 train_time:12373ms step_avg:34.56ms
step:359/1880 train_time:12407ms step_avg:34.56ms
step:360/1880 train_time:12441ms step_avg:34.56ms
step:361/1880 train_time:12474ms step_avg:34.56ms
step:362/1880 train_time:12509ms step_avg:34.55ms
step:363/1880 train_time:12543ms step_avg:34.55ms
step:364/1880 train_time:12577ms step_avg:34.55ms
step:365/1880 train_time:12611ms step_avg:34.55ms
step:366/1880 train_time:12645ms step_avg:34.55ms
step:367/1880 train_time:12679ms step_avg:34.55ms
step:368/1880 train_time:12713ms step_avg:34.55ms
step:369/1880 train_time:12747ms step_avg:34.54ms
step:370/1880 train_time:12781ms step_avg:34.54ms
step:371/1880 train_time:12815ms step_avg:34.54ms
step:372/1880 train_time:12849ms step_avg:34.54ms
step:373/1880 train_time:12883ms step_avg:34.54ms
step:374/1880 train_time:12917ms step_avg:34.54ms
step:375/1880 train_time:12951ms step_avg:34.54ms
step:376/1880 train_time:12985ms step_avg:34.53ms
step:377/1880 train_time:13019ms step_avg:34.53ms
step:378/1880 train_time:13053ms step_avg:34.53ms
step:379/1880 train_time:13087ms step_avg:34.53ms
step:380/1880 train_time:13121ms step_avg:34.53ms
step:381/1880 train_time:13155ms step_avg:34.53ms
step:382/1880 train_time:13189ms step_avg:34.53ms
step:383/1880 train_time:13224ms step_avg:34.53ms
step:384/1880 train_time:13258ms step_avg:34.53ms
step:385/1880 train_time:13292ms step_avg:34.52ms
step:386/1880 train_time:13326ms step_avg:34.52ms
step:387/1880 train_time:13360ms step_avg:34.52ms
step:388/1880 train_time:13394ms step_avg:34.52ms
step:389/1880 train_time:13428ms step_avg:34.52ms
step:390/1880 train_time:13462ms step_avg:34.52ms
step:391/1880 train_time:13495ms step_avg:34.51ms
step:392/1880 train_time:13531ms step_avg:34.52ms
step:393/1880 train_time:13564ms step_avg:34.51ms
step:394/1880 train_time:13598ms step_avg:34.51ms
step:395/1880 train_time:13631ms step_avg:34.51ms
step:396/1880 train_time:13665ms step_avg:34.51ms
step:397/1880 train_time:13699ms step_avg:34.51ms
step:398/1880 train_time:13733ms step_avg:34.51ms
step:399/1880 train_time:13767ms step_avg:34.50ms
step:400/1880 train_time:13802ms step_avg:34.50ms
step:401/1880 train_time:13836ms step_avg:34.50ms
step:402/1880 train_time:13870ms step_avg:34.50ms
step:403/1880 train_time:13904ms step_avg:34.50ms
step:404/1880 train_time:13938ms step_avg:34.50ms
step:405/1880 train_time:13972ms step_avg:34.50ms
step:406/1880 train_time:14006ms step_avg:34.50ms
step:407/1880 train_time:14040ms step_avg:34.50ms
step:408/1880 train_time:14075ms step_avg:34.50ms
step:409/1880 train_time:14109ms step_avg:34.50ms
step:410/1880 train_time:14143ms step_avg:34.50ms
step:411/1880 train_time:14177ms step_avg:34.49ms
step:412/1880 train_time:14211ms step_avg:34.49ms
step:413/1880 train_time:14245ms step_avg:34.49ms
step:414/1880 train_time:14279ms step_avg:34.49ms
step:415/1880 train_time:14313ms step_avg:34.49ms
step:416/1880 train_time:14347ms step_avg:34.49ms
step:417/1880 train_time:14381ms step_avg:34.49ms
step:418/1880 train_time:14416ms step_avg:34.49ms
step:419/1880 train_time:14449ms step_avg:34.49ms
step:420/1880 train_time:14484ms step_avg:34.49ms
step:421/1880 train_time:14518ms step_avg:34.48ms
step:422/1880 train_time:14552ms step_avg:34.48ms
step:423/1880 train_time:14585ms step_avg:34.48ms
step:424/1880 train_time:14620ms step_avg:34.48ms
step:425/1880 train_time:14653ms step_avg:34.48ms
step:426/1880 train_time:14688ms step_avg:34.48ms
step:427/1880 train_time:14722ms step_avg:34.48ms
step:428/1880 train_time:14756ms step_avg:34.48ms
step:429/1880 train_time:14791ms step_avg:34.48ms
step:430/1880 train_time:14825ms step_avg:34.48ms
step:431/1880 train_time:14859ms step_avg:34.48ms
step:432/1880 train_time:14893ms step_avg:34.47ms
step:433/1880 train_time:14927ms step_avg:34.47ms
step:434/1880 train_time:14961ms step_avg:34.47ms
step:435/1880 train_time:14995ms step_avg:34.47ms
step:436/1880 train_time:15029ms step_avg:34.47ms
step:437/1880 train_time:15063ms step_avg:34.47ms
step:438/1880 train_time:15097ms step_avg:34.47ms
step:439/1880 train_time:15131ms step_avg:34.47ms
step:440/1880 train_time:15165ms step_avg:34.47ms
step:441/1880 train_time:15198ms step_avg:34.46ms
step:442/1880 train_time:15233ms step_avg:34.46ms
step:443/1880 train_time:15266ms step_avg:34.46ms
step:444/1880 train_time:15301ms step_avg:34.46ms
step:445/1880 train_time:15334ms step_avg:34.46ms
step:446/1880 train_time:15369ms step_avg:34.46ms
step:447/1880 train_time:15403ms step_avg:34.46ms
step:448/1880 train_time:15437ms step_avg:34.46ms
step:449/1880 train_time:15471ms step_avg:34.46ms
step:450/1880 train_time:15505ms step_avg:34.45ms
step:451/1880 train_time:15538ms step_avg:34.45ms
step:452/1880 train_time:15573ms step_avg:34.45ms
step:453/1880 train_time:15606ms step_avg:34.45ms
step:454/1880 train_time:15641ms step_avg:34.45ms
step:455/1880 train_time:15674ms step_avg:34.45ms
step:456/1880 train_time:15709ms step_avg:34.45ms
step:457/1880 train_time:15742ms step_avg:34.45ms
step:458/1880 train_time:15776ms step_avg:34.45ms
step:459/1880 train_time:15810ms step_avg:34.45ms
step:460/1880 train_time:15845ms step_avg:34.44ms
step:461/1880 train_time:15878ms step_avg:34.44ms
step:462/1880 train_time:15913ms step_avg:34.44ms
step:463/1880 train_time:15946ms step_avg:34.44ms
step:464/1880 train_time:15981ms step_avg:34.44ms
step:465/1880 train_time:16014ms step_avg:34.44ms
step:466/1880 train_time:16049ms step_avg:34.44ms
step:467/1880 train_time:16083ms step_avg:34.44ms
step:468/1880 train_time:16117ms step_avg:34.44ms
step:469/1880 train_time:16151ms step_avg:34.44ms
step:470/1880 train_time:16185ms step_avg:34.44ms
step:471/1880 train_time:16219ms step_avg:34.43ms
step:472/1880 train_time:16253ms step_avg:34.43ms
step:473/1880 train_time:16287ms step_avg:34.43ms
step:474/1880 train_time:16322ms step_avg:34.43ms
step:475/1880 train_time:16356ms step_avg:34.43ms
step:476/1880 train_time:16390ms step_avg:34.43ms
step:477/1880 train_time:16424ms step_avg:34.43ms
step:478/1880 train_time:16458ms step_avg:34.43ms
step:479/1880 train_time:16492ms step_avg:34.43ms
step:480/1880 train_time:16526ms step_avg:34.43ms
step:481/1880 train_time:16560ms step_avg:34.43ms
step:482/1880 train_time:16594ms step_avg:34.43ms
step:483/1880 train_time:16628ms step_avg:34.43ms
step:484/1880 train_time:16663ms step_avg:34.43ms
step:485/1880 train_time:16696ms step_avg:34.43ms
step:486/1880 train_time:16731ms step_avg:34.42ms
step:487/1880 train_time:16764ms step_avg:34.42ms
step:488/1880 train_time:16799ms step_avg:34.42ms
step:489/1880 train_time:16832ms step_avg:34.42ms
step:490/1880 train_time:16866ms step_avg:34.42ms
step:491/1880 train_time:16900ms step_avg:34.42ms
step:492/1880 train_time:16935ms step_avg:34.42ms
step:493/1880 train_time:16969ms step_avg:34.42ms
step:494/1880 train_time:17003ms step_avg:34.42ms
step:495/1880 train_time:17037ms step_avg:34.42ms
step:496/1880 train_time:17071ms step_avg:34.42ms
step:497/1880 train_time:17105ms step_avg:34.42ms
step:498/1880 train_time:17139ms step_avg:34.42ms
step:499/1880 train_time:17173ms step_avg:34.42ms
step:500/1880 train_time:17208ms step_avg:34.42ms
step:500/1880 val_loss:4.2834 train_time:17245ms step_avg:34.49ms
step:501/1880 train_time:17268ms step_avg:34.47ms
step:502/1880 train_time:17290ms step_avg:34.44ms
step:503/1880 train_time:17313ms step_avg:34.42ms
step:504/1880 train_time:17347ms step_avg:34.42ms
step:505/1880 train_time:17383ms step_avg:34.42ms
step:506/1880 train_time:17418ms step_avg:34.42ms
step:507/1880 train_time:17453ms step_avg:34.42ms
step:508/1880 train_time:17487ms step_avg:34.42ms
step:509/1880 train_time:17521ms step_avg:34.42ms
step:510/1880 train_time:17556ms step_avg:34.42ms
step:511/1880 train_time:17590ms step_avg:34.42ms
step:512/1880 train_time:17624ms step_avg:34.42ms
step:513/1880 train_time:17657ms step_avg:34.42ms
step:514/1880 train_time:17691ms step_avg:34.42ms
step:515/1880 train_time:17725ms step_avg:34.42ms
step:516/1880 train_time:17759ms step_avg:34.42ms
step:517/1880 train_time:17792ms step_avg:34.41ms
step:518/1880 train_time:17827ms step_avg:34.41ms
step:519/1880 train_time:17860ms step_avg:34.41ms
step:520/1880 train_time:17894ms step_avg:34.41ms
step:521/1880 train_time:17928ms step_avg:34.41ms
step:522/1880 train_time:17962ms step_avg:34.41ms
step:523/1880 train_time:17996ms step_avg:34.41ms
step:524/1880 train_time:18030ms step_avg:34.41ms
step:525/1880 train_time:18064ms step_avg:34.41ms
step:526/1880 train_time:18098ms step_avg:34.41ms
step:527/1880 train_time:18131ms step_avg:34.40ms
step:528/1880 train_time:18166ms step_avg:34.40ms
step:529/1880 train_time:18199ms step_avg:34.40ms
step:530/1880 train_time:18234ms step_avg:34.40ms
step:531/1880 train_time:18268ms step_avg:34.40ms
step:532/1880 train_time:18303ms step_avg:34.40ms
step:533/1880 train_time:18337ms step_avg:34.40ms
step:534/1880 train_time:18372ms step_avg:34.40ms
step:535/1880 train_time:18406ms step_avg:34.40ms
step:536/1880 train_time:18440ms step_avg:34.40ms
step:537/1880 train_time:18474ms step_avg:34.40ms
step:538/1880 train_time:18509ms step_avg:34.40ms
step:539/1880 train_time:18543ms step_avg:34.40ms
step:540/1880 train_time:18577ms step_avg:34.40ms
step:541/1880 train_time:18611ms step_avg:34.40ms
step:542/1880 train_time:18645ms step_avg:34.40ms
step:543/1880 train_time:18679ms step_avg:34.40ms
step:544/1880 train_time:18714ms step_avg:34.40ms
step:545/1880 train_time:18747ms step_avg:34.40ms
step:546/1880 train_time:18781ms step_avg:34.40ms
step:547/1880 train_time:18815ms step_avg:34.40ms
step:548/1880 train_time:18849ms step_avg:34.40ms
step:549/1880 train_time:18883ms step_avg:34.39ms
step:550/1880 train_time:18917ms step_avg:34.39ms
step:551/1880 train_time:18951ms step_avg:34.39ms
step:552/1880 train_time:18985ms step_avg:34.39ms
step:553/1880 train_time:19019ms step_avg:34.39ms
step:554/1880 train_time:19053ms step_avg:34.39ms
step:555/1880 train_time:19087ms step_avg:34.39ms
step:556/1880 train_time:19121ms step_avg:34.39ms
step:557/1880 train_time:19154ms step_avg:34.39ms
step:558/1880 train_time:19189ms step_avg:34.39ms
step:559/1880 train_time:19223ms step_avg:34.39ms
step:560/1880 train_time:19257ms step_avg:34.39ms
step:561/1880 train_time:19291ms step_avg:34.39ms
step:562/1880 train_time:19325ms step_avg:34.39ms
step:563/1880 train_time:19359ms step_avg:34.39ms
step:564/1880 train_time:19394ms step_avg:34.39ms
step:565/1880 train_time:19428ms step_avg:34.39ms
step:566/1880 train_time:19462ms step_avg:34.39ms
step:567/1880 train_time:19496ms step_avg:34.38ms
step:568/1880 train_time:19531ms step_avg:34.39ms
step:569/1880 train_time:19565ms step_avg:34.39ms
step:570/1880 train_time:19599ms step_avg:34.38ms
step:571/1880 train_time:19633ms step_avg:34.38ms
step:572/1880 train_time:19668ms step_avg:34.38ms
step:573/1880 train_time:19702ms step_avg:34.38ms
step:574/1880 train_time:19736ms step_avg:34.38ms
step:575/1880 train_time:19770ms step_avg:34.38ms
step:576/1880 train_time:19805ms step_avg:34.38ms
step:577/1880 train_time:19838ms step_avg:34.38ms
step:578/1880 train_time:19872ms step_avg:34.38ms
step:579/1880 train_time:19906ms step_avg:34.38ms
step:580/1880 train_time:19940ms step_avg:34.38ms
step:581/1880 train_time:19974ms step_avg:34.38ms
step:582/1880 train_time:20008ms step_avg:34.38ms
step:583/1880 train_time:20042ms step_avg:34.38ms
step:584/1880 train_time:20077ms step_avg:34.38ms
step:585/1880 train_time:20111ms step_avg:34.38ms
step:586/1880 train_time:20145ms step_avg:34.38ms
step:587/1880 train_time:20179ms step_avg:34.38ms
step:588/1880 train_time:20213ms step_avg:34.38ms
step:589/1880 train_time:20247ms step_avg:34.38ms
step:590/1880 train_time:20281ms step_avg:34.38ms
step:591/1880 train_time:20315ms step_avg:34.37ms
step:592/1880 train_time:20349ms step_avg:34.37ms
step:593/1880 train_time:20383ms step_avg:34.37ms
step:594/1880 train_time:20418ms step_avg:34.37ms
step:595/1880 train_time:20452ms step_avg:34.37ms
step:596/1880 train_time:20486ms step_avg:34.37ms
step:597/1880 train_time:20520ms step_avg:34.37ms
step:598/1880 train_time:20555ms step_avg:34.37ms
step:599/1880 train_time:20589ms step_avg:34.37ms
step:600/1880 train_time:20624ms step_avg:34.37ms
step:601/1880 train_time:20658ms step_avg:34.37ms
step:602/1880 train_time:20693ms step_avg:34.37ms
step:603/1880 train_time:20727ms step_avg:34.37ms
step:604/1880 train_time:20761ms step_avg:34.37ms
step:605/1880 train_time:20795ms step_avg:34.37ms
step:606/1880 train_time:20829ms step_avg:34.37ms
step:607/1880 train_time:20863ms step_avg:34.37ms
step:608/1880 train_time:20897ms step_avg:34.37ms
step:609/1880 train_time:20931ms step_avg:34.37ms
step:610/1880 train_time:20966ms step_avg:34.37ms
step:611/1880 train_time:20999ms step_avg:34.37ms
step:612/1880 train_time:21034ms step_avg:34.37ms
step:613/1880 train_time:21067ms step_avg:34.37ms
step:614/1880 train_time:21101ms step_avg:34.37ms
step:615/1880 train_time:21136ms step_avg:34.37ms
step:616/1880 train_time:21197ms step_avg:34.41ms
step:617/1880 train_time:21258ms step_avg:34.45ms
step:618/1880 train_time:21319ms step_avg:34.50ms
step:619/1880 train_time:21380ms step_avg:34.54ms
step:620/1880 train_time:21442ms step_avg:34.58ms
step:621/1880 train_time:21504ms step_avg:34.63ms
step:622/1880 train_time:21566ms step_avg:34.67ms
step:623/1880 train_time:21627ms step_avg:34.71ms
step:624/1880 train_time:21689ms step_avg:34.76ms
step:625/1880 train_time:21751ms step_avg:34.80ms
step:626/1880 train_time:21812ms step_avg:34.84ms
step:627/1880 train_time:21875ms step_avg:34.89ms
step:628/1880 train_time:21937ms step_avg:34.93ms
step:629/1880 train_time:21998ms step_avg:34.97ms
step:630/1880 train_time:22059ms step_avg:35.01ms
step:631/1880 train_time:22121ms step_avg:35.06ms
step:632/1880 train_time:22182ms step_avg:35.10ms
step:633/1880 train_time:22243ms step_avg:35.14ms
step:634/1880 train_time:22304ms step_avg:35.18ms
step:635/1880 train_time:22365ms step_avg:35.22ms
step:636/1880 train_time:22427ms step_avg:35.26ms
step:637/1880 train_time:22488ms step_avg:35.30ms
step:638/1880 train_time:22550ms step_avg:35.34ms
step:639/1880 train_time:22612ms step_avg:35.39ms
step:640/1880 train_time:22674ms step_avg:35.43ms
step:641/1880 train_time:22736ms step_avg:35.47ms
step:642/1880 train_time:22798ms step_avg:35.51ms
step:643/1880 train_time:22860ms step_avg:35.55ms
step:644/1880 train_time:22921ms step_avg:35.59ms
step:645/1880 train_time:22984ms step_avg:35.63ms
step:646/1880 train_time:23046ms step_avg:35.67ms
step:647/1880 train_time:23107ms step_avg:35.71ms
step:648/1880 train_time:23169ms step_avg:35.75ms
step:649/1880 train_time:23232ms step_avg:35.80ms
step:650/1880 train_time:23293ms step_avg:35.84ms
step:651/1880 train_time:23355ms step_avg:35.88ms
step:652/1880 train_time:23417ms step_avg:35.92ms
step:653/1880 train_time:23478ms step_avg:35.95ms
step:654/1880 train_time:23540ms step_avg:35.99ms
step:655/1880 train_time:23601ms step_avg:36.03ms
step:656/1880 train_time:23663ms step_avg:36.07ms
step:657/1880 train_time:23724ms step_avg:36.11ms
step:658/1880 train_time:23786ms step_avg:36.15ms
step:659/1880 train_time:23847ms step_avg:36.19ms
step:660/1880 train_time:23909ms step_avg:36.23ms
step:661/1880 train_time:23970ms step_avg:36.26ms
step:662/1880 train_time:24032ms step_avg:36.30ms
step:663/1880 train_time:24094ms step_avg:36.34ms
step:664/1880 train_time:24156ms step_avg:36.38ms
step:665/1880 train_time:24217ms step_avg:36.42ms
step:666/1880 train_time:24278ms step_avg:36.45ms
step:667/1880 train_time:24340ms step_avg:36.49ms
step:668/1880 train_time:24401ms step_avg:36.53ms
step:669/1880 train_time:24464ms step_avg:36.57ms
step:670/1880 train_time:24525ms step_avg:36.60ms
step:671/1880 train_time:24587ms step_avg:36.64ms
step:672/1880 train_time:24648ms step_avg:36.68ms
step:673/1880 train_time:24711ms step_avg:36.72ms
step:674/1880 train_time:24773ms step_avg:36.75ms
step:675/1880 train_time:24834ms step_avg:36.79ms
step:676/1880 train_time:24896ms step_avg:36.83ms
step:677/1880 train_time:24958ms step_avg:36.87ms
step:678/1880 train_time:25019ms step_avg:36.90ms
step:679/1880 train_time:25080ms step_avg:36.94ms
step:680/1880 train_time:25141ms step_avg:36.97ms
step:681/1880 train_time:25203ms step_avg:37.01ms
step:682/1880 train_time:25265ms step_avg:37.05ms
step:683/1880 train_time:25325ms step_avg:37.08ms
step:684/1880 train_time:25387ms step_avg:37.11ms
step:685/1880 train_time:25448ms step_avg:37.15ms
step:686/1880 train_time:25510ms step_avg:37.19ms
step:687/1880 train_time:25572ms step_avg:37.22ms
step:688/1880 train_time:25633ms step_avg:37.26ms
step:689/1880 train_time:25696ms step_avg:37.29ms
step:690/1880 train_time:25757ms step_avg:37.33ms
step:691/1880 train_time:25819ms step_avg:37.36ms
step:692/1880 train_time:25880ms step_avg:37.40ms
step:693/1880 train_time:25942ms step_avg:37.43ms
step:694/1880 train_time:26004ms step_avg:37.47ms
step:695/1880 train_time:26066ms step_avg:37.50ms
step:696/1880 train_time:26127ms step_avg:37.54ms
step:697/1880 train_time:26188ms step_avg:37.57ms
step:698/1880 train_time:26250ms step_avg:37.61ms
step:699/1880 train_time:26312ms step_avg:37.64ms
step:700/1880 train_time:26375ms step_avg:37.68ms
step:701/1880 train_time:26437ms step_avg:37.71ms
step:702/1880 train_time:26498ms step_avg:37.75ms
step:703/1880 train_time:26559ms step_avg:37.78ms
step:704/1880 train_time:26620ms step_avg:37.81ms
step:705/1880 train_time:26681ms step_avg:37.85ms
step:706/1880 train_time:26743ms step_avg:37.88ms
step:707/1880 train_time:26804ms step_avg:37.91ms
step:708/1880 train_time:26866ms step_avg:37.95ms
step:709/1880 train_time:26928ms step_avg:37.98ms
step:710/1880 train_time:26989ms step_avg:38.01ms
step:711/1880 train_time:27052ms step_avg:38.05ms
step:712/1880 train_time:27114ms step_avg:38.08ms
step:713/1880 train_time:27176ms step_avg:38.11ms
step:714/1880 train_time:27237ms step_avg:38.15ms
step:715/1880 train_time:27298ms step_avg:38.18ms
step:716/1880 train_time:27360ms step_avg:38.21ms
step:717/1880 train_time:27422ms step_avg:38.25ms
step:718/1880 train_time:27484ms step_avg:38.28ms
step:719/1880 train_time:27546ms step_avg:38.31ms
step:720/1880 train_time:27607ms step_avg:38.34ms
step:721/1880 train_time:27669ms step_avg:38.38ms
step:722/1880 train_time:27730ms step_avg:38.41ms
step:723/1880 train_time:27793ms step_avg:38.44ms
step:724/1880 train_time:27856ms step_avg:38.47ms
step:725/1880 train_time:27918ms step_avg:38.51ms
step:726/1880 train_time:27979ms step_avg:38.54ms
step:727/1880 train_time:28041ms step_avg:38.57ms
step:728/1880 train_time:28102ms step_avg:38.60ms
step:729/1880 train_time:28164ms step_avg:38.63ms
step:730/1880 train_time:28225ms step_avg:38.66ms
step:731/1880 train_time:28286ms step_avg:38.70ms
step:732/1880 train_time:28348ms step_avg:38.73ms
step:733/1880 train_time:28411ms step_avg:38.76ms
step:734/1880 train_time:28474ms step_avg:38.79ms
step:735/1880 train_time:28535ms step_avg:38.82ms
step:736/1880 train_time:28597ms step_avg:38.85ms
step:737/1880 train_time:28658ms step_avg:38.89ms
step:738/1880 train_time:28719ms step_avg:38.92ms
step:739/1880 train_time:28781ms step_avg:38.95ms
step:740/1880 train_time:28842ms step_avg:38.98ms
step:741/1880 train_time:28904ms step_avg:39.01ms
step:742/1880 train_time:28966ms step_avg:39.04ms
step:743/1880 train_time:29027ms step_avg:39.07ms
step:744/1880 train_time:29089ms step_avg:39.10ms
step:745/1880 train_time:29151ms step_avg:39.13ms
step:746/1880 train_time:29213ms step_avg:39.16ms
step:747/1880 train_time:29275ms step_avg:39.19ms
step:748/1880 train_time:29336ms step_avg:39.22ms
step:749/1880 train_time:29397ms step_avg:39.25ms
step:750/1880 train_time:29459ms step_avg:39.28ms
step:750/1880 val_loss:4.0267 train_time:29523ms step_avg:39.36ms
step:751/1880 train_time:29546ms step_avg:39.34ms
step:752/1880 train_time:29583ms step_avg:39.34ms
step:753/1880 train_time:29648ms step_avg:39.37ms
step:754/1880 train_time:29711ms step_avg:39.41ms
step:755/1880 train_time:29773ms step_avg:39.43ms
step:756/1880 train_time:29834ms step_avg:39.46ms
step:757/1880 train_time:29895ms step_avg:39.49ms
step:758/1880 train_time:29956ms step_avg:39.52ms
step:759/1880 train_time:30017ms step_avg:39.55ms
step:760/1880 train_time:30078ms step_avg:39.58ms
step:761/1880 train_time:30140ms step_avg:39.61ms
step:762/1880 train_time:30201ms step_avg:39.63ms
step:763/1880 train_time:30262ms step_avg:39.66ms
step:764/1880 train_time:30323ms step_avg:39.69ms
step:765/1880 train_time:30384ms step_avg:39.72ms
step:766/1880 train_time:30446ms step_avg:39.75ms
step:767/1880 train_time:30508ms step_avg:39.78ms
step:768/1880 train_time:30570ms step_avg:39.81ms
step:769/1880 train_time:30633ms step_avg:39.84ms
step:770/1880 train_time:30696ms step_avg:39.86ms
step:771/1880 train_time:30758ms step_avg:39.89ms
step:772/1880 train_time:30820ms step_avg:39.92ms
step:773/1880 train_time:30883ms step_avg:39.95ms
step:774/1880 train_time:30944ms step_avg:39.98ms
step:775/1880 train_time:31005ms step_avg:40.01ms
step:776/1880 train_time:31066ms step_avg:40.03ms
step:777/1880 train_time:31128ms step_avg:40.06ms
step:778/1880 train_time:31189ms step_avg:40.09ms
step:779/1880 train_time:31250ms step_avg:40.12ms
step:780/1880 train_time:31311ms step_avg:40.14ms
step:781/1880 train_time:31372ms step_avg:40.17ms
step:782/1880 train_time:31433ms step_avg:40.20ms
step:783/1880 train_time:31494ms step_avg:40.22ms
step:784/1880 train_time:31556ms step_avg:40.25ms
step:785/1880 train_time:31620ms step_avg:40.28ms
step:786/1880 train_time:31683ms step_avg:40.31ms
step:787/1880 train_time:31745ms step_avg:40.34ms
step:788/1880 train_time:31806ms step_avg:40.36ms
step:789/1880 train_time:31869ms step_avg:40.39ms
step:790/1880 train_time:31930ms step_avg:40.42ms
step:791/1880 train_time:31992ms step_avg:40.45ms
step:792/1880 train_time:32053ms step_avg:40.47ms
step:793/1880 train_time:32115ms step_avg:40.50ms
step:794/1880 train_time:32176ms step_avg:40.52ms
step:795/1880 train_time:32237ms step_avg:40.55ms
step:796/1880 train_time:32299ms step_avg:40.58ms
step:797/1880 train_time:32361ms step_avg:40.60ms
step:798/1880 train_time:32422ms step_avg:40.63ms
step:799/1880 train_time:32484ms step_avg:40.66ms
step:800/1880 train_time:32546ms step_avg:40.68ms
step:801/1880 train_time:32608ms step_avg:40.71ms
step:802/1880 train_time:32670ms step_avg:40.74ms
step:803/1880 train_time:32732ms step_avg:40.76ms
step:804/1880 train_time:32793ms step_avg:40.79ms
step:805/1880 train_time:32855ms step_avg:40.81ms
step:806/1880 train_time:32916ms step_avg:40.84ms
step:807/1880 train_time:32979ms step_avg:40.87ms
step:808/1880 train_time:33041ms step_avg:40.89ms
step:809/1880 train_time:33103ms step_avg:40.92ms
step:810/1880 train_time:33164ms step_avg:40.94ms
step:811/1880 train_time:33225ms step_avg:40.97ms
step:812/1880 train_time:33285ms step_avg:40.99ms
step:813/1880 train_time:33346ms step_avg:41.02ms
step:814/1880 train_time:33408ms step_avg:41.04ms
step:815/1880 train_time:33469ms step_avg:41.07ms
step:816/1880 train_time:33530ms step_avg:41.09ms
step:817/1880 train_time:33592ms step_avg:41.12ms
step:818/1880 train_time:33655ms step_avg:41.14ms
step:819/1880 train_time:33717ms step_avg:41.17ms
step:820/1880 train_time:33780ms step_avg:41.20ms
step:821/1880 train_time:33842ms step_avg:41.22ms
step:822/1880 train_time:33904ms step_avg:41.25ms
step:823/1880 train_time:33966ms step_avg:41.27ms
step:824/1880 train_time:34028ms step_avg:41.30ms
step:825/1880 train_time:34089ms step_avg:41.32ms
step:826/1880 train_time:34149ms step_avg:41.34ms
step:827/1880 train_time:34211ms step_avg:41.37ms
step:828/1880 train_time:34273ms step_avg:41.39ms
step:829/1880 train_time:34334ms step_avg:41.42ms
step:830/1880 train_time:34395ms step_avg:41.44ms
step:831/1880 train_time:34457ms step_avg:41.46ms
step:832/1880 train_time:34519ms step_avg:41.49ms
step:833/1880 train_time:34582ms step_avg:41.51ms
step:834/1880 train_time:34644ms step_avg:41.54ms
step:835/1880 train_time:34706ms step_avg:41.56ms
step:836/1880 train_time:34767ms step_avg:41.59ms
step:837/1880 train_time:34829ms step_avg:41.61ms
step:838/1880 train_time:34891ms step_avg:41.64ms
step:839/1880 train_time:34953ms step_avg:41.66ms
step:840/1880 train_time:35014ms step_avg:41.68ms
step:841/1880 train_time:35076ms step_avg:41.71ms
step:842/1880 train_time:35138ms step_avg:41.73ms
step:843/1880 train_time:35200ms step_avg:41.76ms
step:844/1880 train_time:35262ms step_avg:41.78ms
step:845/1880 train_time:35323ms step_avg:41.80ms
step:846/1880 train_time:35385ms step_avg:41.83ms
step:847/1880 train_time:35446ms step_avg:41.85ms
step:848/1880 train_time:35508ms step_avg:41.87ms
step:849/1880 train_time:35569ms step_avg:41.90ms
step:850/1880 train_time:35630ms step_avg:41.92ms
step:851/1880 train_time:35692ms step_avg:41.94ms
step:852/1880 train_time:35754ms step_avg:41.96ms
step:853/1880 train_time:35815ms step_avg:41.99ms
step:854/1880 train_time:35877ms step_avg:42.01ms
step:855/1880 train_time:35939ms step_avg:42.03ms
step:856/1880 train_time:36001ms step_avg:42.06ms
step:857/1880 train_time:36063ms step_avg:42.08ms
step:858/1880 train_time:36124ms step_avg:42.10ms
step:859/1880 train_time:36186ms step_avg:42.13ms
step:860/1880 train_time:36247ms step_avg:42.15ms
step:861/1880 train_time:36309ms step_avg:42.17ms
step:862/1880 train_time:36370ms step_avg:42.19ms
step:863/1880 train_time:36432ms step_avg:42.22ms
step:864/1880 train_time:36494ms step_avg:42.24ms
step:865/1880 train_time:36555ms step_avg:42.26ms
step:866/1880 train_time:36617ms step_avg:42.28ms
step:867/1880 train_time:36680ms step_avg:42.31ms
step:868/1880 train_time:36741ms step_avg:42.33ms
step:869/1880 train_time:36804ms step_avg:42.35ms
step:870/1880 train_time:36866ms step_avg:42.37ms
step:871/1880 train_time:36927ms step_avg:42.40ms
step:872/1880 train_time:36989ms step_avg:42.42ms
step:873/1880 train_time:37051ms step_avg:42.44ms
step:874/1880 train_time:37113ms step_avg:42.46ms
step:875/1880 train_time:37175ms step_avg:42.49ms
step:876/1880 train_time:37237ms step_avg:42.51ms
step:877/1880 train_time:37298ms step_avg:42.53ms
step:878/1880 train_time:37361ms step_avg:42.55ms
step:879/1880 train_time:37422ms step_avg:42.57ms
step:880/1880 train_time:37484ms step_avg:42.60ms
step:881/1880 train_time:37545ms step_avg:42.62ms
step:882/1880 train_time:37606ms step_avg:42.64ms
step:883/1880 train_time:37668ms step_avg:42.66ms
step:884/1880 train_time:37729ms step_avg:42.68ms
step:885/1880 train_time:37790ms step_avg:42.70ms
step:886/1880 train_time:37852ms step_avg:42.72ms
step:887/1880 train_time:37913ms step_avg:42.74ms
step:888/1880 train_time:37975ms step_avg:42.76ms
step:889/1880 train_time:38037ms step_avg:42.79ms
step:890/1880 train_time:38099ms step_avg:42.81ms
step:891/1880 train_time:38161ms step_avg:42.83ms
step:892/1880 train_time:38223ms step_avg:42.85ms
step:893/1880 train_time:38285ms step_avg:42.87ms
step:894/1880 train_time:38346ms step_avg:42.89ms
step:895/1880 train_time:38408ms step_avg:42.91ms
step:896/1880 train_time:38469ms step_avg:42.93ms
step:897/1880 train_time:38531ms step_avg:42.96ms
step:898/1880 train_time:38593ms step_avg:42.98ms
step:899/1880 train_time:38655ms step_avg:43.00ms
step:900/1880 train_time:38716ms step_avg:43.02ms
step:901/1880 train_time:38778ms step_avg:43.04ms
step:902/1880 train_time:38840ms step_avg:43.06ms
step:903/1880 train_time:38901ms step_avg:43.08ms
step:904/1880 train_time:38963ms step_avg:43.10ms
step:905/1880 train_time:39025ms step_avg:43.12ms
step:906/1880 train_time:39086ms step_avg:43.14ms
step:907/1880 train_time:39147ms step_avg:43.16ms
step:908/1880 train_time:39209ms step_avg:43.18ms
step:909/1880 train_time:39271ms step_avg:43.20ms
step:910/1880 train_time:39332ms step_avg:43.22ms
step:911/1880 train_time:39394ms step_avg:43.24ms
step:912/1880 train_time:39456ms step_avg:43.26ms
step:913/1880 train_time:39517ms step_avg:43.28ms
step:914/1880 train_time:39579ms step_avg:43.30ms
step:915/1880 train_time:39641ms step_avg:43.32ms
step:916/1880 train_time:39702ms step_avg:43.34ms
step:917/1880 train_time:39764ms step_avg:43.36ms
step:918/1880 train_time:39826ms step_avg:43.38ms
step:919/1880 train_time:39888ms step_avg:43.40ms
step:920/1880 train_time:39950ms step_avg:43.42ms
step:921/1880 train_time:40011ms step_avg:43.44ms
step:922/1880 train_time:40072ms step_avg:43.46ms
step:923/1880 train_time:40134ms step_avg:43.48ms
step:924/1880 train_time:40197ms step_avg:43.50ms
step:925/1880 train_time:40259ms step_avg:43.52ms
step:926/1880 train_time:40321ms step_avg:43.54ms
step:927/1880 train_time:40383ms step_avg:43.56ms
step:928/1880 train_time:40444ms step_avg:43.58ms
step:929/1880 train_time:40506ms step_avg:43.60ms
step:930/1880 train_time:40567ms step_avg:43.62ms
step:931/1880 train_time:40629ms step_avg:43.64ms
step:932/1880 train_time:40691ms step_avg:43.66ms
step:933/1880 train_time:40753ms step_avg:43.68ms
step:934/1880 train_time:40814ms step_avg:43.70ms
step:935/1880 train_time:40875ms step_avg:43.72ms
step:936/1880 train_time:40937ms step_avg:43.74ms
step:937/1880 train_time:40998ms step_avg:43.75ms
step:938/1880 train_time:41060ms step_avg:43.77ms
step:939/1880 train_time:41122ms step_avg:43.79ms
step:940/1880 train_time:41184ms step_avg:43.81ms
step:941/1880 train_time:41245ms step_avg:43.83ms
step:942/1880 train_time:41307ms step_avg:43.85ms
step:943/1880 train_time:41369ms step_avg:43.87ms
step:944/1880 train_time:41430ms step_avg:43.89ms
step:945/1880 train_time:41492ms step_avg:43.91ms
step:946/1880 train_time:41553ms step_avg:43.93ms
step:947/1880 train_time:41615ms step_avg:43.94ms
step:948/1880 train_time:41677ms step_avg:43.96ms
step:949/1880 train_time:41739ms step_avg:43.98ms
step:950/1880 train_time:41801ms step_avg:44.00ms
step:951/1880 train_time:41863ms step_avg:44.02ms
step:952/1880 train_time:41924ms step_avg:44.04ms
step:953/1880 train_time:41985ms step_avg:44.06ms
step:954/1880 train_time:42047ms step_avg:44.07ms
step:955/1880 train_time:42109ms step_avg:44.09ms
step:956/1880 train_time:42170ms step_avg:44.11ms
step:957/1880 train_time:42232ms step_avg:44.13ms
step:958/1880 train_time:42293ms step_avg:44.15ms
step:959/1880 train_time:42355ms step_avg:44.17ms
step:960/1880 train_time:42416ms step_avg:44.18ms
step:961/1880 train_time:42479ms step_avg:44.20ms
step:962/1880 train_time:42541ms step_avg:44.22ms
step:963/1880 train_time:42603ms step_avg:44.24ms
step:964/1880 train_time:42666ms step_avg:44.26ms
step:965/1880 train_time:42727ms step_avg:44.28ms
step:966/1880 train_time:42789ms step_avg:44.29ms
step:967/1880 train_time:42850ms step_avg:44.31ms
step:968/1880 train_time:42911ms step_avg:44.33ms
step:969/1880 train_time:42972ms step_avg:44.35ms
step:970/1880 train_time:43034ms step_avg:44.36ms
step:971/1880 train_time:43095ms step_avg:44.38ms
step:972/1880 train_time:43158ms step_avg:44.40ms
step:973/1880 train_time:43220ms step_avg:44.42ms
step:974/1880 train_time:43282ms step_avg:44.44ms
step:975/1880 train_time:43343ms step_avg:44.45ms
step:976/1880 train_time:43404ms step_avg:44.47ms
step:977/1880 train_time:43467ms step_avg:44.49ms
step:978/1880 train_time:43528ms step_avg:44.51ms
step:979/1880 train_time:43590ms step_avg:44.53ms
step:980/1880 train_time:43652ms step_avg:44.54ms
step:981/1880 train_time:43713ms step_avg:44.56ms
step:982/1880 train_time:43774ms step_avg:44.58ms
step:983/1880 train_time:43836ms step_avg:44.59ms
step:984/1880 train_time:43898ms step_avg:44.61ms
step:985/1880 train_time:43960ms step_avg:44.63ms
step:986/1880 train_time:44022ms step_avg:44.65ms
step:987/1880 train_time:44083ms step_avg:44.66ms
step:988/1880 train_time:44145ms step_avg:44.68ms
step:989/1880 train_time:44207ms step_avg:44.70ms
step:990/1880 train_time:44268ms step_avg:44.72ms
step:991/1880 train_time:44330ms step_avg:44.73ms
step:992/1880 train_time:44392ms step_avg:44.75ms
step:993/1880 train_time:44453ms step_avg:44.77ms
step:994/1880 train_time:44515ms step_avg:44.78ms
step:995/1880 train_time:44577ms step_avg:44.80ms
step:996/1880 train_time:44638ms step_avg:44.82ms
step:997/1880 train_time:44700ms step_avg:44.83ms
step:998/1880 train_time:44762ms step_avg:44.85ms
step:999/1880 train_time:44824ms step_avg:44.87ms
step:1000/1880 train_time:44885ms step_avg:44.88ms
step:1000/1880 val_loss:3.7748 train_time:44949ms step_avg:44.95ms
step:1001/1880 train_time:44971ms step_avg:44.93ms
step:1002/1880 train_time:45010ms step_avg:44.92ms
step:1003/1880 train_time:45076ms step_avg:44.94ms
step:1004/1880 train_time:45139ms step_avg:44.96ms
step:1005/1880 train_time:45201ms step_avg:44.98ms
step:1006/1880 train_time:45262ms step_avg:44.99ms
step:1007/1880 train_time:45323ms step_avg:45.01ms
step:1008/1880 train_time:45384ms step_avg:45.02ms
step:1009/1880 train_time:45445ms step_avg:45.04ms
step:1010/1880 train_time:45506ms step_avg:45.06ms
step:1011/1880 train_time:45567ms step_avg:45.07ms
step:1012/1880 train_time:45628ms step_avg:45.09ms
step:1013/1880 train_time:45689ms step_avg:45.10ms
step:1014/1880 train_time:45751ms step_avg:45.12ms
step:1015/1880 train_time:45812ms step_avg:45.13ms
step:1016/1880 train_time:45874ms step_avg:45.15ms
step:1017/1880 train_time:45937ms step_avg:45.17ms
step:1018/1880 train_time:46000ms step_avg:45.19ms
step:1019/1880 train_time:46063ms step_avg:45.20ms
step:1020/1880 train_time:46126ms step_avg:45.22ms
step:1021/1880 train_time:46188ms step_avg:45.24ms
step:1022/1880 train_time:46249ms step_avg:45.25ms
step:1023/1880 train_time:46311ms step_avg:45.27ms
step:1024/1880 train_time:46372ms step_avg:45.29ms
step:1025/1880 train_time:46434ms step_avg:45.30ms
step:1026/1880 train_time:46495ms step_avg:45.32ms
step:1027/1880 train_time:46556ms step_avg:45.33ms
step:1028/1880 train_time:46616ms step_avg:45.35ms
step:1029/1880 train_time:46678ms step_avg:45.36ms
step:1030/1880 train_time:46739ms step_avg:45.38ms
step:1031/1880 train_time:46801ms step_avg:45.39ms
step:1032/1880 train_time:46862ms step_avg:45.41ms
step:1033/1880 train_time:46924ms step_avg:45.42ms
step:1034/1880 train_time:46985ms step_avg:45.44ms
step:1035/1880 train_time:47048ms step_avg:45.46ms
step:1036/1880 train_time:47111ms step_avg:45.47ms
step:1037/1880 train_time:47175ms step_avg:45.49ms
step:1038/1880 train_time:47237ms step_avg:45.51ms
step:1039/1880 train_time:47299ms step_avg:45.52ms
step:1040/1880 train_time:47360ms step_avg:45.54ms
step:1041/1880 train_time:47421ms step_avg:45.55ms
step:1042/1880 train_time:47482ms step_avg:45.57ms
step:1043/1880 train_time:47544ms step_avg:45.58ms
step:1044/1880 train_time:47604ms step_avg:45.60ms
step:1045/1880 train_time:47666ms step_avg:45.61ms
step:1046/1880 train_time:47727ms step_avg:45.63ms
step:1047/1880 train_time:47789ms step_avg:45.64ms
step:1048/1880 train_time:47850ms step_avg:45.66ms
step:1049/1880 train_time:47912ms step_avg:45.67ms
step:1050/1880 train_time:47973ms step_avg:45.69ms
step:1051/1880 train_time:48035ms step_avg:45.70ms
step:1052/1880 train_time:48097ms step_avg:45.72ms
step:1053/1880 train_time:48160ms step_avg:45.74ms
step:1054/1880 train_time:48222ms step_avg:45.75ms
step:1055/1880 train_time:48283ms step_avg:45.77ms
step:1056/1880 train_time:48345ms step_avg:45.78ms
step:1057/1880 train_time:48406ms step_avg:45.80ms
step:1058/1880 train_time:48468ms step_avg:45.81ms
step:1059/1880 train_time:48529ms step_avg:45.83ms
step:1060/1880 train_time:48591ms step_avg:45.84ms
step:1061/1880 train_time:48652ms step_avg:45.85ms
step:1062/1880 train_time:48713ms step_avg:45.87ms
step:1063/1880 train_time:48775ms step_avg:45.88ms
step:1064/1880 train_time:48836ms step_avg:45.90ms
step:1065/1880 train_time:48898ms step_avg:45.91ms
step:1066/1880 train_time:48959ms step_avg:45.93ms
step:1067/1880 train_time:49020ms step_avg:45.94ms
step:1068/1880 train_time:49082ms step_avg:45.96ms
step:1069/1880 train_time:49143ms step_avg:45.97ms
step:1070/1880 train_time:49205ms step_avg:45.99ms
step:1071/1880 train_time:49267ms step_avg:46.00ms
step:1072/1880 train_time:49329ms step_avg:46.02ms
step:1073/1880 train_time:49391ms step_avg:46.03ms
step:1074/1880 train_time:49452ms step_avg:46.04ms
step:1075/1880 train_time:49514ms step_avg:46.06ms
step:1076/1880 train_time:49576ms step_avg:46.07ms
step:1077/1880 train_time:49637ms step_avg:46.09ms
step:1078/1880 train_time:49698ms step_avg:46.10ms
step:1079/1880 train_time:49759ms step_avg:46.12ms
step:1080/1880 train_time:49820ms step_avg:46.13ms
step:1081/1880 train_time:49882ms step_avg:46.14ms
step:1082/1880 train_time:49943ms step_avg:46.16ms
step:1083/1880 train_time:50005ms step_avg:46.17ms
step:1084/1880 train_time:50067ms step_avg:46.19ms
step:1085/1880 train_time:50130ms step_avg:46.20ms
step:1086/1880 train_time:50192ms step_avg:46.22ms
step:1087/1880 train_time:50253ms step_avg:46.23ms
step:1088/1880 train_time:50316ms step_avg:46.25ms
step:1089/1880 train_time:50377ms step_avg:46.26ms
step:1090/1880 train_time:50438ms step_avg:46.27ms
step:1091/1880 train_time:50499ms step_avg:46.29ms
step:1092/1880 train_time:50561ms step_avg:46.30ms
step:1093/1880 train_time:50622ms step_avg:46.32ms
step:1094/1880 train_time:50683ms step_avg:46.33ms
step:1095/1880 train_time:50744ms step_avg:46.34ms
step:1096/1880 train_time:50805ms step_avg:46.36ms
step:1097/1880 train_time:50867ms step_avg:46.37ms
step:1098/1880 train_time:50928ms step_avg:46.38ms
step:1099/1880 train_time:50989ms step_avg:46.40ms
step:1100/1880 train_time:51051ms step_avg:46.41ms
step:1101/1880 train_time:51113ms step_avg:46.42ms
step:1102/1880 train_time:51175ms step_avg:46.44ms
step:1103/1880 train_time:51237ms step_avg:46.45ms
step:1104/1880 train_time:51298ms step_avg:46.47ms
step:1105/1880 train_time:51360ms step_avg:46.48ms
step:1106/1880 train_time:51421ms step_avg:46.49ms
step:1107/1880 train_time:51482ms step_avg:46.51ms
step:1108/1880 train_time:51543ms step_avg:46.52ms
step:1109/1880 train_time:51605ms step_avg:46.53ms
step:1110/1880 train_time:51666ms step_avg:46.55ms
step:1111/1880 train_time:51728ms step_avg:46.56ms
step:1112/1880 train_time:51790ms step_avg:46.57ms
step:1113/1880 train_time:51851ms step_avg:46.59ms
step:1114/1880 train_time:51912ms step_avg:46.60ms
step:1115/1880 train_time:51974ms step_avg:46.61ms
step:1116/1880 train_time:52035ms step_avg:46.63ms
step:1117/1880 train_time:52097ms step_avg:46.64ms
step:1118/1880 train_time:52158ms step_avg:46.65ms
step:1119/1880 train_time:52219ms step_avg:46.67ms
step:1120/1880 train_time:52280ms step_avg:46.68ms
step:1121/1880 train_time:52342ms step_avg:46.69ms
step:1122/1880 train_time:52404ms step_avg:46.71ms
step:1123/1880 train_time:52466ms step_avg:46.72ms
step:1124/1880 train_time:52528ms step_avg:46.73ms
step:1125/1880 train_time:52590ms step_avg:46.75ms
step:1126/1880 train_time:52651ms step_avg:46.76ms
step:1127/1880 train_time:52714ms step_avg:46.77ms
step:1128/1880 train_time:52775ms step_avg:46.79ms
step:1129/1880 train_time:52837ms step_avg:46.80ms
step:1130/1880 train_time:52898ms step_avg:46.81ms
step:1131/1880 train_time:52959ms step_avg:46.83ms
step:1132/1880 train_time:53021ms step_avg:46.84ms
step:1133/1880 train_time:53082ms step_avg:46.85ms
step:1134/1880 train_time:53143ms step_avg:46.86ms
step:1135/1880 train_time:53205ms step_avg:46.88ms
step:1136/1880 train_time:53266ms step_avg:46.89ms
step:1137/1880 train_time:53328ms step_avg:46.90ms
step:1138/1880 train_time:53390ms step_avg:46.92ms
step:1139/1880 train_time:53452ms step_avg:46.93ms
step:1140/1880 train_time:53514ms step_avg:46.94ms
step:1141/1880 train_time:53576ms step_avg:46.96ms
step:1142/1880 train_time:53637ms step_avg:46.97ms
step:1143/1880 train_time:53699ms step_avg:46.98ms
step:1144/1880 train_time:53759ms step_avg:46.99ms
step:1145/1880 train_time:53821ms step_avg:47.01ms
step:1146/1880 train_time:53882ms step_avg:47.02ms
step:1147/1880 train_time:53944ms step_avg:47.03ms
step:1148/1880 train_time:54005ms step_avg:47.04ms
step:1149/1880 train_time:54067ms step_avg:47.06ms
step:1150/1880 train_time:54129ms step_avg:47.07ms
step:1151/1880 train_time:54190ms step_avg:47.08ms
step:1152/1880 train_time:54252ms step_avg:47.09ms
step:1153/1880 train_time:54314ms step_avg:47.11ms
step:1154/1880 train_time:54375ms step_avg:47.12ms
step:1155/1880 train_time:54437ms step_avg:47.13ms
step:1156/1880 train_time:54499ms step_avg:47.14ms
step:1157/1880 train_time:54560ms step_avg:47.16ms
step:1158/1880 train_time:54622ms step_avg:47.17ms
step:1159/1880 train_time:54683ms step_avg:47.18ms
step:1160/1880 train_time:54745ms step_avg:47.19ms
step:1161/1880 train_time:54807ms step_avg:47.21ms
step:1162/1880 train_time:54869ms step_avg:47.22ms
step:1163/1880 train_time:54930ms step_avg:47.23ms
step:1164/1880 train_time:54992ms step_avg:47.24ms
step:1165/1880 train_time:55054ms step_avg:47.26ms
step:1166/1880 train_time:55116ms step_avg:47.27ms
step:1167/1880 train_time:55177ms step_avg:47.28ms
step:1168/1880 train_time:55238ms step_avg:47.29ms
step:1169/1880 train_time:55300ms step_avg:47.31ms
step:1170/1880 train_time:55361ms step_avg:47.32ms
step:1171/1880 train_time:55422ms step_avg:47.33ms
step:1172/1880 train_time:55483ms step_avg:47.34ms
step:1173/1880 train_time:55545ms step_avg:47.35ms
step:1174/1880 train_time:55606ms step_avg:47.36ms
step:1175/1880 train_time:55669ms step_avg:47.38ms
step:1176/1880 train_time:55730ms step_avg:47.39ms
step:1177/1880 train_time:55792ms step_avg:47.40ms
step:1178/1880 train_time:55853ms step_avg:47.41ms
step:1179/1880 train_time:55915ms step_avg:47.43ms
step:1180/1880 train_time:55975ms step_avg:47.44ms
step:1181/1880 train_time:56037ms step_avg:47.45ms
step:1182/1880 train_time:56099ms step_avg:47.46ms
step:1183/1880 train_time:56161ms step_avg:47.47ms
step:1184/1880 train_time:56222ms step_avg:47.48ms
step:1185/1880 train_time:56283ms step_avg:47.50ms
step:1186/1880 train_time:56344ms step_avg:47.51ms
step:1187/1880 train_time:56406ms step_avg:47.52ms
step:1188/1880 train_time:56467ms step_avg:47.53ms
step:1189/1880 train_time:56529ms step_avg:47.54ms
step:1190/1880 train_time:56591ms step_avg:47.56ms
step:1191/1880 train_time:56652ms step_avg:47.57ms
step:1192/1880 train_time:56714ms step_avg:47.58ms
step:1193/1880 train_time:56775ms step_avg:47.59ms
step:1194/1880 train_time:56836ms step_avg:47.60ms
step:1195/1880 train_time:56898ms step_avg:47.61ms
step:1196/1880 train_time:56958ms step_avg:47.62ms
step:1197/1880 train_time:57020ms step_avg:47.64ms
step:1198/1880 train_time:57081ms step_avg:47.65ms
step:1199/1880 train_time:57142ms step_avg:47.66ms
step:1200/1880 train_time:57203ms step_avg:47.67ms
step:1201/1880 train_time:57265ms step_avg:47.68ms
step:1202/1880 train_time:57327ms step_avg:47.69ms
step:1203/1880 train_time:57389ms step_avg:47.70ms
step:1204/1880 train_time:57450ms step_avg:47.72ms
step:1205/1880 train_time:57513ms step_avg:47.73ms
step:1206/1880 train_time:57575ms step_avg:47.74ms
step:1207/1880 train_time:57636ms step_avg:47.75ms
step:1208/1880 train_time:57697ms step_avg:47.76ms
step:1209/1880 train_time:57759ms step_avg:47.77ms
step:1210/1880 train_time:57821ms step_avg:47.79ms
step:1211/1880 train_time:57883ms step_avg:47.80ms
step:1212/1880 train_time:57944ms step_avg:47.81ms
step:1213/1880 train_time:58005ms step_avg:47.82ms
step:1214/1880 train_time:58066ms step_avg:47.83ms
step:1215/1880 train_time:58128ms step_avg:47.84ms
step:1216/1880 train_time:58190ms step_avg:47.85ms
step:1217/1880 train_time:58252ms step_avg:47.86ms
step:1218/1880 train_time:58313ms step_avg:47.88ms
step:1219/1880 train_time:58375ms step_avg:47.89ms
step:1220/1880 train_time:58436ms step_avg:47.90ms
step:1221/1880 train_time:58498ms step_avg:47.91ms
step:1222/1880 train_time:58559ms step_avg:47.92ms
step:1223/1880 train_time:58621ms step_avg:47.93ms
step:1224/1880 train_time:58682ms step_avg:47.94ms
step:1225/1880 train_time:58744ms step_avg:47.95ms
step:1226/1880 train_time:58805ms step_avg:47.97ms
step:1227/1880 train_time:58867ms step_avg:47.98ms
step:1228/1880 train_time:58930ms step_avg:47.99ms
step:1229/1880 train_time:59017ms step_avg:48.02ms
step:1230/1880 train_time:59105ms step_avg:48.05ms
step:1231/1880 train_time:59193ms step_avg:48.09ms
step:1232/1880 train_time:59280ms step_avg:48.12ms
step:1233/1880 train_time:59369ms step_avg:48.15ms
step:1234/1880 train_time:59457ms step_avg:48.18ms
step:1235/1880 train_time:59546ms step_avg:48.22ms
step:1236/1880 train_time:59633ms step_avg:48.25ms
step:1237/1880 train_time:59721ms step_avg:48.28ms
step:1238/1880 train_time:59808ms step_avg:48.31ms
step:1239/1880 train_time:59896ms step_avg:48.34ms
step:1240/1880 train_time:59984ms step_avg:48.37ms
step:1241/1880 train_time:60072ms step_avg:48.41ms
step:1242/1880 train_time:60158ms step_avg:48.44ms
step:1243/1880 train_time:60247ms step_avg:48.47ms
step:1244/1880 train_time:60334ms step_avg:48.50ms
step:1245/1880 train_time:60422ms step_avg:48.53ms
step:1246/1880 train_time:60510ms step_avg:48.56ms
step:1247/1880 train_time:60598ms step_avg:48.59ms
step:1248/1880 train_time:60687ms step_avg:48.63ms
step:1249/1880 train_time:60774ms step_avg:48.66ms
step:1250/1880 train_time:60862ms step_avg:48.69ms
step:1250/1880 val_loss:3.5327 train_time:60953ms step_avg:48.76ms
step:1251/1880 train_time:60975ms step_avg:48.74ms
step:1252/1880 train_time:61043ms step_avg:48.76ms
step:1253/1880 train_time:61138ms step_avg:48.79ms
step:1254/1880 train_time:61228ms step_avg:48.83ms
step:1255/1880 train_time:61316ms step_avg:48.86ms
step:1256/1880 train_time:61403ms step_avg:48.89ms
step:1257/1880 train_time:61491ms step_avg:48.92ms
step:1258/1880 train_time:61577ms step_avg:48.95ms
step:1259/1880 train_time:61664ms step_avg:48.98ms
step:1260/1880 train_time:61750ms step_avg:49.01ms
step:1261/1880 train_time:61836ms step_avg:49.04ms
step:1262/1880 train_time:61925ms step_avg:49.07ms
step:1263/1880 train_time:62016ms step_avg:49.10ms
step:1264/1880 train_time:62107ms step_avg:49.14ms
step:1265/1880 train_time:62197ms step_avg:49.17ms
step:1266/1880 train_time:62285ms step_avg:49.20ms
step:1267/1880 train_time:62375ms step_avg:49.23ms
step:1268/1880 train_time:62462ms step_avg:49.26ms
step:1269/1880 train_time:62549ms step_avg:49.29ms
step:1270/1880 train_time:62635ms step_avg:49.32ms
step:1271/1880 train_time:62722ms step_avg:49.35ms
step:1272/1880 train_time:62809ms step_avg:49.38ms
step:1273/1880 train_time:62896ms step_avg:49.41ms
step:1274/1880 train_time:62986ms step_avg:49.44ms
step:1275/1880 train_time:63076ms step_avg:49.47ms
step:1276/1880 train_time:63166ms step_avg:49.50ms
step:1277/1880 train_time:63255ms step_avg:49.53ms
step:1278/1880 train_time:63343ms step_avg:49.56ms
step:1279/1880 train_time:63430ms step_avg:49.59ms
step:1280/1880 train_time:63518ms step_avg:49.62ms
step:1281/1880 train_time:63606ms step_avg:49.65ms
step:1282/1880 train_time:63692ms step_avg:49.68ms
step:1283/1880 train_time:63781ms step_avg:49.71ms
step:1284/1880 train_time:63869ms step_avg:49.74ms
step:1285/1880 train_time:63957ms step_avg:49.77ms
step:1286/1880 train_time:64046ms step_avg:49.80ms
step:1287/1880 train_time:64135ms step_avg:49.83ms
step:1288/1880 train_time:64224ms step_avg:49.86ms
step:1289/1880 train_time:64314ms step_avg:49.89ms
step:1290/1880 train_time:64402ms step_avg:49.92ms
step:1291/1880 train_time:64489ms step_avg:49.95ms
step:1292/1880 train_time:64576ms step_avg:49.98ms
step:1293/1880 train_time:64663ms step_avg:50.01ms
step:1294/1880 train_time:64750ms step_avg:50.04ms
step:1295/1880 train_time:64837ms step_avg:50.07ms
step:1296/1880 train_time:64926ms step_avg:50.10ms
step:1297/1880 train_time:65015ms step_avg:50.13ms
step:1298/1880 train_time:65104ms step_avg:50.16ms
step:1299/1880 train_time:65192ms step_avg:50.19ms
step:1300/1880 train_time:65281ms step_avg:50.22ms
step:1301/1880 train_time:65370ms step_avg:50.25ms
step:1302/1880 train_time:65458ms step_avg:50.27ms
step:1303/1880 train_time:65546ms step_avg:50.30ms
step:1304/1880 train_time:65632ms step_avg:50.33ms
step:1305/1880 train_time:65720ms step_avg:50.36ms
step:1306/1880 train_time:65808ms step_avg:50.39ms
step:1307/1880 train_time:65896ms step_avg:50.42ms
step:1308/1880 train_time:65985ms step_avg:50.45ms
step:1309/1880 train_time:66074ms step_avg:50.48ms
step:1310/1880 train_time:66162ms step_avg:50.51ms
step:1311/1880 train_time:66251ms step_avg:50.53ms
step:1312/1880 train_time:66339ms step_avg:50.56ms
step:1313/1880 train_time:66427ms step_avg:50.59ms
step:1314/1880 train_time:66514ms step_avg:50.62ms
step:1315/1880 train_time:66603ms step_avg:50.65ms
step:1316/1880 train_time:66692ms step_avg:50.68ms
step:1317/1880 train_time:66778ms step_avg:50.70ms
step:1318/1880 train_time:66867ms step_avg:50.73ms
step:1319/1880 train_time:66955ms step_avg:50.76ms
step:1320/1880 train_time:67044ms step_avg:50.79ms
step:1321/1880 train_time:67133ms step_avg:50.82ms
step:1322/1880 train_time:67221ms step_avg:50.85ms
step:1323/1880 train_time:67309ms step_avg:50.88ms
step:1324/1880 train_time:67397ms step_avg:50.90ms
step:1325/1880 train_time:67485ms step_avg:50.93ms
step:1326/1880 train_time:67572ms step_avg:50.96ms
step:1327/1880 train_time:67661ms step_avg:50.99ms
step:1328/1880 train_time:67749ms step_avg:51.02ms
step:1329/1880 train_time:67836ms step_avg:51.04ms
step:1330/1880 train_time:67923ms step_avg:51.07ms
step:1331/1880 train_time:68012ms step_avg:51.10ms
step:1332/1880 train_time:68101ms step_avg:51.13ms
step:1333/1880 train_time:68190ms step_avg:51.16ms
step:1334/1880 train_time:68278ms step_avg:51.18ms
step:1335/1880 train_time:68366ms step_avg:51.21ms
step:1336/1880 train_time:68453ms step_avg:51.24ms
step:1337/1880 train_time:68542ms step_avg:51.27ms
step:1338/1880 train_time:68630ms step_avg:51.29ms
step:1339/1880 train_time:68718ms step_avg:51.32ms
step:1340/1880 train_time:68805ms step_avg:51.35ms
step:1341/1880 train_time:68893ms step_avg:51.37ms
step:1342/1880 train_time:68981ms step_avg:51.40ms
step:1343/1880 train_time:69069ms step_avg:51.43ms
step:1344/1880 train_time:69158ms step_avg:51.46ms
step:1345/1880 train_time:69246ms step_avg:51.48ms
step:1346/1880 train_time:69334ms step_avg:51.51ms
step:1347/1880 train_time:69422ms step_avg:51.54ms
step:1348/1880 train_time:69511ms step_avg:51.57ms
step:1349/1880 train_time:69600ms step_avg:51.59ms
step:1350/1880 train_time:69688ms step_avg:51.62ms
step:1351/1880 train_time:69777ms step_avg:51.65ms
step:1352/1880 train_time:69865ms step_avg:51.68ms
step:1353/1880 train_time:69953ms step_avg:51.70ms
step:1354/1880 train_time:70042ms step_avg:51.73ms
step:1355/1880 train_time:70130ms step_avg:51.76ms
step:1356/1880 train_time:70218ms step_avg:51.78ms
step:1357/1880 train_time:70308ms step_avg:51.81ms
step:1358/1880 train_time:70395ms step_avg:51.84ms
step:1359/1880 train_time:70483ms step_avg:51.86ms
step:1360/1880 train_time:70571ms step_avg:51.89ms
step:1361/1880 train_time:70659ms step_avg:51.92ms
step:1362/1880 train_time:70747ms step_avg:51.94ms
step:1363/1880 train_time:70835ms step_avg:51.97ms
step:1364/1880 train_time:70924ms step_avg:52.00ms
step:1365/1880 train_time:71012ms step_avg:52.02ms
step:1366/1880 train_time:71102ms step_avg:52.05ms
step:1367/1880 train_time:71191ms step_avg:52.08ms
step:1368/1880 train_time:71278ms step_avg:52.10ms
step:1369/1880 train_time:71366ms step_avg:52.13ms
step:1370/1880 train_time:71454ms step_avg:52.16ms
step:1371/1880 train_time:71542ms step_avg:52.18ms
step:1372/1880 train_time:71630ms step_avg:52.21ms
step:1373/1880 train_time:71718ms step_avg:52.23ms
step:1374/1880 train_time:71807ms step_avg:52.26ms
step:1375/1880 train_time:71894ms step_avg:52.29ms
step:1376/1880 train_time:71981ms step_avg:52.31ms
step:1377/1880 train_time:72070ms step_avg:52.34ms
step:1378/1880 train_time:72157ms step_avg:52.36ms
step:1379/1880 train_time:72245ms step_avg:52.39ms
step:1380/1880 train_time:72333ms step_avg:52.41ms
step:1381/1880 train_time:72421ms step_avg:52.44ms
step:1382/1880 train_time:72509ms step_avg:52.47ms
step:1383/1880 train_time:72597ms step_avg:52.49ms
step:1384/1880 train_time:72685ms step_avg:52.52ms
step:1385/1880 train_time:72773ms step_avg:52.54ms
step:1386/1880 train_time:72861ms step_avg:52.57ms
step:1387/1880 train_time:72948ms step_avg:52.59ms
step:1388/1880 train_time:73036ms step_avg:52.62ms
step:1389/1880 train_time:73124ms step_avg:52.65ms
step:1390/1880 train_time:73212ms step_avg:52.67ms
step:1391/1880 train_time:73301ms step_avg:52.70ms
step:1392/1880 train_time:73389ms step_avg:52.72ms
step:1393/1880 train_time:73478ms step_avg:52.75ms
step:1394/1880 train_time:73566ms step_avg:52.77ms
step:1395/1880 train_time:73655ms step_avg:52.80ms
step:1396/1880 train_time:73742ms step_avg:52.82ms
step:1397/1880 train_time:73830ms step_avg:52.85ms
step:1398/1880 train_time:73917ms step_avg:52.87ms
step:1399/1880 train_time:74006ms step_avg:52.90ms
step:1400/1880 train_time:74094ms step_avg:52.92ms
step:1401/1880 train_time:74182ms step_avg:52.95ms
step:1402/1880 train_time:74272ms step_avg:52.98ms
step:1403/1880 train_time:74360ms step_avg:53.00ms
step:1404/1880 train_time:74448ms step_avg:53.03ms
step:1405/1880 train_time:74536ms step_avg:53.05ms
step:1406/1880 train_time:74624ms step_avg:53.08ms
step:1407/1880 train_time:74713ms step_avg:53.10ms
step:1408/1880 train_time:74800ms step_avg:53.13ms
step:1409/1880 train_time:74888ms step_avg:53.15ms
step:1410/1880 train_time:74976ms step_avg:53.17ms
step:1411/1880 train_time:75064ms step_avg:53.20ms
step:1412/1880 train_time:75152ms step_avg:53.22ms
step:1413/1880 train_time:75241ms step_avg:53.25ms
step:1414/1880 train_time:75330ms step_avg:53.27ms
step:1415/1880 train_time:75417ms step_avg:53.30ms
step:1416/1880 train_time:75505ms step_avg:53.32ms
step:1417/1880 train_time:75594ms step_avg:53.35ms
step:1418/1880 train_time:75682ms step_avg:53.37ms
step:1419/1880 train_time:75770ms step_avg:53.40ms
step:1420/1880 train_time:75858ms step_avg:53.42ms
step:1421/1880 train_time:75945ms step_avg:53.44ms
step:1422/1880 train_time:76033ms step_avg:53.47ms
step:1423/1880 train_time:76120ms step_avg:53.49ms
step:1424/1880 train_time:76210ms step_avg:53.52ms
step:1425/1880 train_time:76297ms step_avg:53.54ms
step:1426/1880 train_time:76385ms step_avg:53.57ms
step:1427/1880 train_time:76473ms step_avg:53.59ms
step:1428/1880 train_time:76563ms step_avg:53.62ms
step:1429/1880 train_time:76652ms step_avg:53.64ms
step:1430/1880 train_time:76740ms step_avg:53.66ms
step:1431/1880 train_time:76828ms step_avg:53.69ms
step:1432/1880 train_time:76915ms step_avg:53.71ms
step:1433/1880 train_time:77003ms step_avg:53.74ms
step:1434/1880 train_time:77091ms step_avg:53.76ms
step:1435/1880 train_time:77179ms step_avg:53.78ms
step:1436/1880 train_time:77268ms step_avg:53.81ms
step:1437/1880 train_time:77355ms step_avg:53.83ms
step:1438/1880 train_time:77444ms step_avg:53.86ms
step:1439/1880 train_time:77532ms step_avg:53.88ms
step:1440/1880 train_time:77620ms step_avg:53.90ms
step:1441/1880 train_time:77708ms step_avg:53.93ms
step:1442/1880 train_time:77796ms step_avg:53.95ms
step:1443/1880 train_time:77883ms step_avg:53.97ms
step:1444/1880 train_time:77971ms step_avg:54.00ms
step:1445/1880 train_time:78060ms step_avg:54.02ms
step:1446/1880 train_time:78148ms step_avg:54.04ms
step:1447/1880 train_time:78236ms step_avg:54.07ms
step:1448/1880 train_time:78324ms step_avg:54.09ms
step:1449/1880 train_time:78414ms step_avg:54.12ms
step:1450/1880 train_time:78502ms step_avg:54.14ms
step:1451/1880 train_time:78590ms step_avg:54.16ms
step:1452/1880 train_time:78679ms step_avg:54.19ms
step:1453/1880 train_time:78768ms step_avg:54.21ms
step:1454/1880 train_time:78856ms step_avg:54.23ms
step:1455/1880 train_time:78944ms step_avg:54.26ms
step:1456/1880 train_time:79031ms step_avg:54.28ms
step:1457/1880 train_time:79120ms step_avg:54.30ms
step:1458/1880 train_time:79207ms step_avg:54.33ms
step:1459/1880 train_time:79296ms step_avg:54.35ms
step:1460/1880 train_time:79384ms step_avg:54.37ms
step:1461/1880 train_time:79473ms step_avg:54.40ms
step:1462/1880 train_time:79561ms step_avg:54.42ms
step:1463/1880 train_time:79648ms step_avg:54.44ms
step:1464/1880 train_time:79736ms step_avg:54.46ms
step:1465/1880 train_time:79824ms step_avg:54.49ms
step:1466/1880 train_time:79912ms step_avg:54.51ms
step:1467/1880 train_time:80001ms step_avg:54.53ms
step:1468/1880 train_time:80089ms step_avg:54.56ms
step:1469/1880 train_time:80177ms step_avg:54.58ms
step:1470/1880 train_time:80265ms step_avg:54.60ms
step:1471/1880 train_time:80353ms step_avg:54.62ms
step:1472/1880 train_time:80442ms step_avg:54.65ms
step:1473/1880 train_time:80531ms step_avg:54.67ms
step:1474/1880 train_time:80619ms step_avg:54.69ms
step:1475/1880 train_time:80707ms step_avg:54.72ms
step:1476/1880 train_time:80794ms step_avg:54.74ms
step:1477/1880 train_time:80882ms step_avg:54.76ms
step:1478/1880 train_time:80970ms step_avg:54.78ms
step:1479/1880 train_time:81059ms step_avg:54.81ms
step:1480/1880 train_time:81147ms step_avg:54.83ms
step:1481/1880 train_time:81235ms step_avg:54.85ms
step:1482/1880 train_time:81323ms step_avg:54.87ms
step:1483/1880 train_time:81411ms step_avg:54.90ms
step:1484/1880 train_time:81500ms step_avg:54.92ms
step:1485/1880 train_time:81589ms step_avg:54.94ms
step:1486/1880 train_time:81676ms step_avg:54.96ms
step:1487/1880 train_time:81764ms step_avg:54.99ms
step:1488/1880 train_time:81853ms step_avg:55.01ms
step:1489/1880 train_time:81941ms step_avg:55.03ms
step:1490/1880 train_time:82030ms step_avg:55.05ms
step:1491/1880 train_time:82117ms step_avg:55.08ms
step:1492/1880 train_time:82205ms step_avg:55.10ms
step:1493/1880 train_time:82293ms step_avg:55.12ms
step:1494/1880 train_time:82381ms step_avg:55.14ms
step:1495/1880 train_time:82470ms step_avg:55.16ms
step:1496/1880 train_time:82557ms step_avg:55.19ms
step:1497/1880 train_time:82647ms step_avg:55.21ms
step:1498/1880 train_time:82734ms step_avg:55.23ms
step:1499/1880 train_time:82824ms step_avg:55.25ms
step:1500/1880 train_time:82912ms step_avg:55.27ms
step:1500/1880 val_loss:3.4070 train_time:83002ms step_avg:55.33ms
step:1501/1880 train_time:83022ms step_avg:55.31ms
step:1502/1880 train_time:83093ms step_avg:55.32ms
step:1503/1880 train_time:83184ms step_avg:55.35ms
step:1504/1880 train_time:83271ms step_avg:55.37ms
step:1505/1880 train_time:83358ms step_avg:55.39ms
step:1506/1880 train_time:83446ms step_avg:55.41ms
step:1507/1880 train_time:83533ms step_avg:55.43ms
step:1508/1880 train_time:83620ms step_avg:55.45ms
step:1509/1880 train_time:83708ms step_avg:55.47ms
step:1510/1880 train_time:83795ms step_avg:55.49ms
step:1511/1880 train_time:83883ms step_avg:55.51ms
step:1512/1880 train_time:83972ms step_avg:55.54ms
step:1513/1880 train_time:84063ms step_avg:55.56ms
step:1514/1880 train_time:84153ms step_avg:55.58ms
step:1515/1880 train_time:84242ms step_avg:55.61ms
step:1516/1880 train_time:84330ms step_avg:55.63ms
step:1517/1880 train_time:84418ms step_avg:55.65ms
step:1518/1880 train_time:84504ms step_avg:55.67ms
step:1519/1880 train_time:84591ms step_avg:55.69ms
step:1520/1880 train_time:84679ms step_avg:55.71ms
step:1521/1880 train_time:84766ms step_avg:55.73ms
step:1522/1880 train_time:84854ms step_avg:55.75ms
step:1523/1880 train_time:84943ms step_avg:55.77ms
step:1524/1880 train_time:85031ms step_avg:55.79ms
step:1525/1880 train_time:85120ms step_avg:55.82ms
step:1526/1880 train_time:85209ms step_avg:55.84ms
step:1527/1880 train_time:85297ms step_avg:55.86ms
step:1528/1880 train_time:85386ms step_avg:55.88ms
step:1529/1880 train_time:85473ms step_avg:55.90ms
step:1530/1880 train_time:85561ms step_avg:55.92ms
step:1531/1880 train_time:85649ms step_avg:55.94ms
step:1532/1880 train_time:85736ms step_avg:55.96ms
step:1533/1880 train_time:85824ms step_avg:55.98ms
step:1534/1880 train_time:85912ms step_avg:56.00ms
step:1535/1880 train_time:86000ms step_avg:56.03ms
step:1536/1880 train_time:86090ms step_avg:56.05ms
step:1537/1880 train_time:86179ms step_avg:56.07ms
step:1538/1880 train_time:86268ms step_avg:56.09ms
step:1539/1880 train_time:86356ms step_avg:56.11ms
step:1540/1880 train_time:86444ms step_avg:56.13ms
step:1541/1880 train_time:86532ms step_avg:56.15ms
step:1542/1880 train_time:86621ms step_avg:56.17ms
step:1543/1880 train_time:86709ms step_avg:56.19ms
step:1544/1880 train_time:86795ms step_avg:56.21ms
step:1545/1880 train_time:86883ms step_avg:56.24ms
step:1546/1880 train_time:86972ms step_avg:56.26ms
step:1547/1880 train_time:87061ms step_avg:56.28ms
step:1548/1880 train_time:87149ms step_avg:56.30ms
step:1549/1880 train_time:87237ms step_avg:56.32ms
step:1550/1880 train_time:87326ms step_avg:56.34ms
step:1551/1880 train_time:87413ms step_avg:56.36ms
step:1552/1880 train_time:87500ms step_avg:56.38ms
step:1553/1880 train_time:87589ms step_avg:56.40ms
step:1554/1880 train_time:87678ms step_avg:56.42ms
step:1555/1880 train_time:87766ms step_avg:56.44ms
step:1556/1880 train_time:87854ms step_avg:56.46ms
step:1557/1880 train_time:87942ms step_avg:56.48ms
step:1558/1880 train_time:88029ms step_avg:56.50ms
step:1559/1880 train_time:88118ms step_avg:56.52ms
step:1560/1880 train_time:88207ms step_avg:56.54ms
step:1561/1880 train_time:88295ms step_avg:56.56ms
step:1562/1880 train_time:88383ms step_avg:56.58ms
step:1563/1880 train_time:88471ms step_avg:56.60ms
step:1564/1880 train_time:88558ms step_avg:56.62ms
step:1565/1880 train_time:88647ms step_avg:56.64ms
step:1566/1880 train_time:88733ms step_avg:56.66ms
step:1567/1880 train_time:88821ms step_avg:56.68ms
step:1568/1880 train_time:88910ms step_avg:56.70ms
step:1569/1880 train_time:88998ms step_avg:56.72ms
step:1570/1880 train_time:89086ms step_avg:56.74ms
step:1571/1880 train_time:89174ms step_avg:56.76ms
step:1572/1880 train_time:89262ms step_avg:56.78ms
step:1573/1880 train_time:89350ms step_avg:56.80ms
step:1574/1880 train_time:89438ms step_avg:56.82ms
step:1575/1880 train_time:89527ms step_avg:56.84ms
step:1576/1880 train_time:89615ms step_avg:56.86ms
step:1577/1880 train_time:89702ms step_avg:56.88ms
step:1578/1880 train_time:89790ms step_avg:56.90ms
step:1579/1880 train_time:89878ms step_avg:56.92ms
step:1580/1880 train_time:89966ms step_avg:56.94ms
step:1581/1880 train_time:90053ms step_avg:56.96ms
step:1582/1880 train_time:90141ms step_avg:56.98ms
step:1583/1880 train_time:90230ms step_avg:57.00ms
step:1584/1880 train_time:90318ms step_avg:57.02ms
step:1585/1880 train_time:90406ms step_avg:57.04ms
step:1586/1880 train_time:90493ms step_avg:57.06ms
step:1587/1880 train_time:90582ms step_avg:57.08ms
step:1588/1880 train_time:90670ms step_avg:57.10ms
step:1589/1880 train_time:90757ms step_avg:57.12ms
step:1590/1880 train_time:90846ms step_avg:57.14ms
step:1591/1880 train_time:90933ms step_avg:57.15ms
step:1592/1880 train_time:91022ms step_avg:57.17ms
step:1593/1880 train_time:91110ms step_avg:57.19ms
step:1594/1880 train_time:91199ms step_avg:57.21ms
step:1595/1880 train_time:91287ms step_avg:57.23ms
step:1596/1880 train_time:91374ms step_avg:57.25ms
step:1597/1880 train_time:91463ms step_avg:57.27ms
step:1598/1880 train_time:91549ms step_avg:57.29ms
step:1599/1880 train_time:91637ms step_avg:57.31ms
step:1600/1880 train_time:91726ms step_avg:57.33ms
step:1601/1880 train_time:91813ms step_avg:57.35ms
step:1602/1880 train_time:91902ms step_avg:57.37ms
step:1603/1880 train_time:91990ms step_avg:57.39ms
step:1604/1880 train_time:92078ms step_avg:57.41ms
step:1605/1880 train_time:92167ms step_avg:57.42ms
step:1606/1880 train_time:92254ms step_avg:57.44ms
step:1607/1880 train_time:92341ms step_avg:57.46ms
step:1608/1880 train_time:92431ms step_avg:57.48ms
step:1609/1880 train_time:92519ms step_avg:57.50ms
step:1610/1880 train_time:92607ms step_avg:57.52ms
step:1611/1880 train_time:92695ms step_avg:57.54ms
step:1612/1880 train_time:92783ms step_avg:57.56ms
step:1613/1880 train_time:92872ms step_avg:57.58ms
step:1614/1880 train_time:92959ms step_avg:57.60ms
step:1615/1880 train_time:93047ms step_avg:57.61ms
step:1616/1880 train_time:93135ms step_avg:57.63ms
step:1617/1880 train_time:93223ms step_avg:57.65ms
step:1618/1880 train_time:93310ms step_avg:57.67ms
step:1619/1880 train_time:93398ms step_avg:57.69ms
step:1620/1880 train_time:93487ms step_avg:57.71ms
step:1621/1880 train_time:93574ms step_avg:57.73ms
step:1622/1880 train_time:93662ms step_avg:57.74ms
step:1623/1880 train_time:93752ms step_avg:57.76ms
step:1624/1880 train_time:93840ms step_avg:57.78ms
step:1625/1880 train_time:93929ms step_avg:57.80ms
step:1626/1880 train_time:94017ms step_avg:57.82ms
step:1627/1880 train_time:94105ms step_avg:57.84ms
step:1628/1880 train_time:94192ms step_avg:57.86ms
step:1629/1880 train_time:94280ms step_avg:57.88ms
step:1630/1880 train_time:94369ms step_avg:57.89ms
step:1631/1880 train_time:94457ms step_avg:57.91ms
step:1632/1880 train_time:94545ms step_avg:57.93ms
step:1633/1880 train_time:94633ms step_avg:57.95ms
step:1634/1880 train_time:94722ms step_avg:57.97ms
step:1635/1880 train_time:94811ms step_avg:57.99ms
step:1636/1880 train_time:94900ms step_avg:58.01ms
step:1637/1880 train_time:94990ms step_avg:58.03ms
step:1638/1880 train_time:95077ms step_avg:58.04ms
step:1639/1880 train_time:95165ms step_avg:58.06ms
step:1640/1880 train_time:95252ms step_avg:58.08ms
step:1641/1880 train_time:95339ms step_avg:58.10ms
step:1642/1880 train_time:95428ms step_avg:58.12ms
step:1643/1880 train_time:95516ms step_avg:58.13ms
step:1644/1880 train_time:95603ms step_avg:58.15ms
step:1645/1880 train_time:95692ms step_avg:58.17ms
step:1646/1880 train_time:95781ms step_avg:58.19ms
step:1647/1880 train_time:95870ms step_avg:58.21ms
step:1648/1880 train_time:95957ms step_avg:58.23ms
step:1649/1880 train_time:96046ms step_avg:58.24ms
step:1650/1880 train_time:96134ms step_avg:58.26ms
step:1651/1880 train_time:96221ms step_avg:58.28ms
step:1652/1880 train_time:96309ms step_avg:58.30ms
step:1653/1880 train_time:96396ms step_avg:58.32ms
step:1654/1880 train_time:96485ms step_avg:58.33ms
step:1655/1880 train_time:96573ms step_avg:58.35ms
step:1656/1880 train_time:96661ms step_avg:58.37ms
step:1657/1880 train_time:96750ms step_avg:58.39ms
step:1658/1880 train_time:96838ms step_avg:58.41ms
step:1659/1880 train_time:96926ms step_avg:58.42ms
step:1660/1880 train_time:97014ms step_avg:58.44ms
step:1661/1880 train_time:97101ms step_avg:58.46ms
step:1662/1880 train_time:97190ms step_avg:58.48ms
step:1663/1880 train_time:97277ms step_avg:58.49ms
step:1664/1880 train_time:97365ms step_avg:58.51ms
step:1665/1880 train_time:97453ms step_avg:58.53ms
step:1666/1880 train_time:97541ms step_avg:58.55ms
step:1667/1880 train_time:97632ms step_avg:58.57ms
step:1668/1880 train_time:97720ms step_avg:58.58ms
step:1669/1880 train_time:97809ms step_avg:58.60ms
step:1670/1880 train_time:97896ms step_avg:58.62ms
step:1671/1880 train_time:97985ms step_avg:58.64ms
step:1672/1880 train_time:98072ms step_avg:58.66ms
step:1673/1880 train_time:98160ms step_avg:58.67ms
step:1674/1880 train_time:98248ms step_avg:58.69ms
step:1675/1880 train_time:98336ms step_avg:58.71ms
step:1676/1880 train_time:98425ms step_avg:58.73ms
step:1677/1880 train_time:98512ms step_avg:58.74ms
step:1678/1880 train_time:98600ms step_avg:58.76ms
step:1679/1880 train_time:98689ms step_avg:58.78ms
step:1680/1880 train_time:98778ms step_avg:58.80ms
step:1681/1880 train_time:98866ms step_avg:58.81ms
step:1682/1880 train_time:98954ms step_avg:58.83ms
step:1683/1880 train_time:99043ms step_avg:58.85ms
step:1684/1880 train_time:99131ms step_avg:58.87ms
step:1685/1880 train_time:99219ms step_avg:58.88ms
step:1686/1880 train_time:99306ms step_avg:58.90ms
step:1687/1880 train_time:99394ms step_avg:58.92ms
step:1688/1880 train_time:99482ms step_avg:58.94ms
step:1689/1880 train_time:99572ms step_avg:58.95ms
step:1690/1880 train_time:99661ms step_avg:58.97ms
step:1691/1880 train_time:99750ms step_avg:58.99ms
step:1692/1880 train_time:99838ms step_avg:59.01ms
step:1693/1880 train_time:99925ms step_avg:59.02ms
step:1694/1880 train_time:100013ms step_avg:59.04ms
step:1695/1880 train_time:100102ms step_avg:59.06ms
step:1696/1880 train_time:100190ms step_avg:59.07ms
step:1697/1880 train_time:100278ms step_avg:59.09ms
step:1698/1880 train_time:100367ms step_avg:59.11ms
step:1699/1880 train_time:100454ms step_avg:59.13ms
step:1700/1880 train_time:100541ms step_avg:59.14ms
step:1701/1880 train_time:100630ms step_avg:59.16ms
step:1702/1880 train_time:100719ms step_avg:59.18ms
step:1703/1880 train_time:100808ms step_avg:59.19ms
step:1704/1880 train_time:100895ms step_avg:59.21ms
step:1705/1880 train_time:100984ms step_avg:59.23ms
step:1706/1880 train_time:101072ms step_avg:59.24ms
step:1707/1880 train_time:101160ms step_avg:59.26ms
step:1708/1880 train_time:101248ms step_avg:59.28ms
step:1709/1880 train_time:101336ms step_avg:59.30ms
step:1710/1880 train_time:101423ms step_avg:59.31ms
step:1711/1880 train_time:101511ms step_avg:59.33ms
step:1712/1880 train_time:101599ms step_avg:59.35ms
step:1713/1880 train_time:101689ms step_avg:59.36ms
step:1714/1880 train_time:101777ms step_avg:59.38ms
step:1715/1880 train_time:101865ms step_avg:59.40ms
step:1716/1880 train_time:101952ms step_avg:59.41ms
step:1717/1880 train_time:102040ms step_avg:59.43ms
step:1718/1880 train_time:102129ms step_avg:59.45ms
step:1719/1880 train_time:102217ms step_avg:59.46ms
step:1720/1880 train_time:102305ms step_avg:59.48ms
step:1721/1880 train_time:102393ms step_avg:59.50ms
step:1722/1880 train_time:102480ms step_avg:59.51ms
step:1723/1880 train_time:102568ms step_avg:59.53ms
step:1724/1880 train_time:102656ms step_avg:59.55ms
step:1725/1880 train_time:102744ms step_avg:59.56ms
step:1726/1880 train_time:102832ms step_avg:59.58ms
step:1727/1880 train_time:102921ms step_avg:59.60ms
step:1728/1880 train_time:103010ms step_avg:59.61ms
step:1729/1880 train_time:103098ms step_avg:59.63ms
step:1730/1880 train_time:103187ms step_avg:59.65ms
step:1731/1880 train_time:103274ms step_avg:59.66ms
step:1732/1880 train_time:103363ms step_avg:59.68ms
step:1733/1880 train_time:103450ms step_avg:59.69ms
step:1734/1880 train_time:103538ms step_avg:59.71ms
step:1735/1880 train_time:103627ms step_avg:59.73ms
step:1736/1880 train_time:103714ms step_avg:59.74ms
step:1737/1880 train_time:103803ms step_avg:59.76ms
step:1738/1880 train_time:103891ms step_avg:59.78ms
step:1739/1880 train_time:103979ms step_avg:59.79ms
step:1740/1880 train_time:104068ms step_avg:59.81ms
step:1741/1880 train_time:104157ms step_avg:59.83ms
step:1742/1880 train_time:104245ms step_avg:59.84ms
step:1743/1880 train_time:104333ms step_avg:59.86ms
step:1744/1880 train_time:104421ms step_avg:59.87ms
step:1745/1880 train_time:104509ms step_avg:59.89ms
step:1746/1880 train_time:104597ms step_avg:59.91ms
step:1747/1880 train_time:104686ms step_avg:59.92ms
step:1748/1880 train_time:104773ms step_avg:59.94ms
step:1749/1880 train_time:104863ms step_avg:59.96ms
step:1750/1880 train_time:104951ms step_avg:59.97ms
step:1750/1880 val_loss:3.3123 train_time:105041ms step_avg:60.02ms
step:1751/1880 train_time:105062ms step_avg:60.00ms
step:1752/1880 train_time:105132ms step_avg:60.01ms
step:1753/1880 train_time:105228ms step_avg:60.03ms
step:1754/1880 train_time:105316ms step_avg:60.04ms
step:1755/1880 train_time:105403ms step_avg:60.06ms
step:1756/1880 train_time:105490ms step_avg:60.07ms
step:1757/1880 train_time:105577ms step_avg:60.09ms
step:1758/1880 train_time:105663ms step_avg:60.10ms
step:1759/1880 train_time:105750ms step_avg:60.12ms
step:1760/1880 train_time:105837ms step_avg:60.13ms
step:1761/1880 train_time:105924ms step_avg:60.15ms
step:1762/1880 train_time:106013ms step_avg:60.17ms
step:1763/1880 train_time:106104ms step_avg:60.18ms
step:1764/1880 train_time:106194ms step_avg:60.20ms
step:1765/1880 train_time:106284ms step_avg:60.22ms
step:1766/1880 train_time:106371ms step_avg:60.23ms
step:1767/1880 train_time:106459ms step_avg:60.25ms
step:1768/1880 train_time:106547ms step_avg:60.26ms
step:1769/1880 train_time:106634ms step_avg:60.28ms
step:1770/1880 train_time:106721ms step_avg:60.29ms
step:1771/1880 train_time:106809ms step_avg:60.31ms
step:1772/1880 train_time:106897ms step_avg:60.33ms
step:1773/1880 train_time:106985ms step_avg:60.34ms
step:1774/1880 train_time:107073ms step_avg:60.36ms
step:1775/1880 train_time:107164ms step_avg:60.37ms
step:1776/1880 train_time:107253ms step_avg:60.39ms
step:1777/1880 train_time:107342ms step_avg:60.41ms
step:1778/1880 train_time:107430ms step_avg:60.42ms
step:1779/1880 train_time:107517ms step_avg:60.44ms
step:1780/1880 train_time:107604ms step_avg:60.45ms
step:1781/1880 train_time:107691ms step_avg:60.47ms
step:1782/1880 train_time:107779ms step_avg:60.48ms
step:1783/1880 train_time:107866ms step_avg:60.50ms
step:1784/1880 train_time:107953ms step_avg:60.51ms
step:1785/1880 train_time:108043ms step_avg:60.53ms
step:1786/1880 train_time:108131ms step_avg:60.54ms
step:1787/1880 train_time:108220ms step_avg:60.56ms
step:1788/1880 train_time:108310ms step_avg:60.58ms
step:1789/1880 train_time:108399ms step_avg:60.59ms
step:1790/1880 train_time:108487ms step_avg:60.61ms
step:1791/1880 train_time:108575ms step_avg:60.62ms
step:1792/1880 train_time:108663ms step_avg:60.64ms
step:1793/1880 train_time:108750ms step_avg:60.65ms
step:1794/1880 train_time:108839ms step_avg:60.67ms
step:1795/1880 train_time:108926ms step_avg:60.68ms
step:1796/1880 train_time:109014ms step_avg:60.70ms
step:1797/1880 train_time:109102ms step_avg:60.71ms
step:1798/1880 train_time:109191ms step_avg:60.73ms
step:1799/1880 train_time:109281ms step_avg:60.75ms
step:1800/1880 train_time:109370ms step_avg:60.76ms
step:1801/1880 train_time:109458ms step_avg:60.78ms
step:1802/1880 train_time:109545ms step_avg:60.79ms
step:1803/1880 train_time:109633ms step_avg:60.81ms
step:1804/1880 train_time:109721ms step_avg:60.82ms
step:1805/1880 train_time:109810ms step_avg:60.84ms
step:1806/1880 train_time:109898ms step_avg:60.85ms
step:1807/1880 train_time:109986ms step_avg:60.87ms
step:1808/1880 train_time:110073ms step_avg:60.88ms
step:1809/1880 train_time:110162ms step_avg:60.90ms
step:1810/1880 train_time:110250ms step_avg:60.91ms
step:1811/1880 train_time:110339ms step_avg:60.93ms
step:1812/1880 train_time:110426ms step_avg:60.94ms
step:1813/1880 train_time:110514ms step_avg:60.96ms
step:1814/1880 train_time:110602ms step_avg:60.97ms
step:1815/1880 train_time:110690ms step_avg:60.99ms
step:1816/1880 train_time:110778ms step_avg:61.00ms
step:1817/1880 train_time:110866ms step_avg:61.02ms
step:1818/1880 train_time:110953ms step_avg:61.03ms
step:1819/1880 train_time:111041ms step_avg:61.05ms
step:1820/1880 train_time:111130ms step_avg:61.06ms
step:1821/1880 train_time:111219ms step_avg:61.08ms
step:1822/1880 train_time:111307ms step_avg:61.09ms
step:1823/1880 train_time:111395ms step_avg:61.11ms
step:1824/1880 train_time:111483ms step_avg:61.12ms
step:1825/1880 train_time:111571ms step_avg:61.13ms
step:1826/1880 train_time:111659ms step_avg:61.15ms
step:1827/1880 train_time:111748ms step_avg:61.16ms
step:1828/1880 train_time:111835ms step_avg:61.18ms
step:1829/1880 train_time:111923ms step_avg:61.19ms
step:1830/1880 train_time:112011ms step_avg:61.21ms
step:1831/1880 train_time:112098ms step_avg:61.22ms
step:1832/1880 train_time:112187ms step_avg:61.24ms
step:1833/1880 train_time:112275ms step_avg:61.25ms
step:1834/1880 train_time:112363ms step_avg:61.27ms
step:1835/1880 train_time:112452ms step_avg:61.28ms
step:1836/1880 train_time:112541ms step_avg:61.30ms
step:1837/1880 train_time:112629ms step_avg:61.31ms
step:1838/1880 train_time:112718ms step_avg:61.33ms
step:1839/1880 train_time:112806ms step_avg:61.34ms
step:1840/1880 train_time:112894ms step_avg:61.36ms
step:1841/1880 train_time:112982ms step_avg:61.37ms
step:1842/1880 train_time:113070ms step_avg:61.38ms
step:1843/1880 train_time:113158ms step_avg:61.40ms
step:1844/1880 train_time:113247ms step_avg:61.41ms
step:1845/1880 train_time:113335ms step_avg:61.43ms
step:1846/1880 train_time:113423ms step_avg:61.44ms
step:1847/1880 train_time:113512ms step_avg:61.46ms
step:1848/1880 train_time:113601ms step_avg:61.47ms
step:1849/1880 train_time:113690ms step_avg:61.49ms
step:1850/1880 train_time:113778ms step_avg:61.50ms
step:1851/1880 train_time:113867ms step_avg:61.52ms
step:1852/1880 train_time:113956ms step_avg:61.53ms
step:1853/1880 train_time:114044ms step_avg:61.55ms
step:1854/1880 train_time:114132ms step_avg:61.56ms
step:1855/1880 train_time:114220ms step_avg:61.57ms
step:1856/1880 train_time:114308ms step_avg:61.59ms
step:1857/1880 train_time:114396ms step_avg:61.60ms
step:1858/1880 train_time:114485ms step_avg:61.62ms
step:1859/1880 train_time:114573ms step_avg:61.63ms
step:1860/1880 train_time:114663ms step_avg:61.65ms
step:1861/1880 train_time:114752ms step_avg:61.66ms
step:1862/1880 train_time:114840ms step_avg:61.68ms
step:1863/1880 train_time:114930ms step_avg:61.69ms
step:1864/1880 train_time:115018ms step_avg:61.70ms
step:1865/1880 train_time:115107ms step_avg:61.72ms
step:1866/1880 train_time:115194ms step_avg:61.73ms
step:1867/1880 train_time:115282ms step_avg:61.75ms
step:1868/1880 train_time:115369ms step_avg:61.76ms
step:1869/1880 train_time:115457ms step_avg:61.77ms
step:1870/1880 train_time:115547ms step_avg:61.79ms
step:1871/1880 train_time:115634ms step_avg:61.80ms
step:1872/1880 train_time:115722ms step_avg:61.82ms
step:1873/1880 train_time:115811ms step_avg:61.83ms
step:1874/1880 train_time:115900ms step_avg:61.85ms
step:1875/1880 train_time:115988ms step_avg:61.86ms
step:1876/1880 train_time:116076ms step_avg:61.87ms
step:1877/1880 train_time:116165ms step_avg:61.89ms
step:1878/1880 train_time:116252ms step_avg:61.90ms
step:1879/1880 train_time:116342ms step_avg:61.92ms
step:1880/1880 train_time:116430ms step_avg:61.93ms
step:1880/1880 val_loss:3.2784 train_time:116521ms step_avg:61.98ms
peak memory allocated: 29709 MiB reserved: 43898 MiB

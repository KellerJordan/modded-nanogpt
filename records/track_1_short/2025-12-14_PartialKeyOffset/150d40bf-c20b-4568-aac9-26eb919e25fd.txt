import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 14 20:26:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            128W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   41C    P0            128W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0            122W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     48559      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48560      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48561      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48562      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48563      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48564      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48565      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     48566      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     48560      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     48561      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     48562      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     48563      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     48564      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     48565      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     48566      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:71ms step_avg:70.86ms
step:2/2110 train_time:96ms step_avg:47.94ms
step:3/2110 train_time:122ms step_avg:40.80ms
step:4/2110 train_time:154ms step_avg:38.60ms
step:5/2110 train_time:187ms step_avg:37.49ms
step:6/2110 train_time:379ms step_avg:63.12ms
step:7/2110 train_time:431ms step_avg:61.57ms
step:8/2110 train_time:464ms step_avg:57.94ms
step:9/2110 train_time:497ms step_avg:55.17ms
step:10/2110 train_time:529ms step_avg:52.90ms
step:11/2110 train_time:563ms step_avg:51.15ms
step:12/2110 train_time:595ms step_avg:49.60ms
step:13/2110 train_time:629ms step_avg:48.38ms
step:14/2110 train_time:661ms step_avg:47.24ms
step:15/2110 train_time:695ms step_avg:46.34ms
step:16/2110 train_time:728ms step_avg:45.48ms
step:17/2110 train_time:762ms step_avg:44.80ms
step:18/2110 train_time:794ms step_avg:44.12ms
step:19/2110 train_time:828ms step_avg:43.58ms
step:20/2110 train_time:861ms step_avg:43.03ms
step:21/2110 train_time:894ms step_avg:42.58ms
step:22/2110 train_time:927ms step_avg:42.14ms
step:23/2110 train_time:961ms step_avg:41.77ms
step:24/2110 train_time:993ms step_avg:41.39ms
step:25/2110 train_time:1027ms step_avg:41.08ms
step:26/2110 train_time:1060ms step_avg:40.75ms
step:27/2110 train_time:1093ms step_avg:40.49ms
step:28/2110 train_time:1126ms step_avg:40.21ms
step:29/2110 train_time:1160ms step_avg:39.99ms
step:30/2110 train_time:1192ms step_avg:39.74ms
step:31/2110 train_time:1226ms step_avg:39.54ms
step:32/2110 train_time:1258ms step_avg:39.32ms
step:33/2110 train_time:1293ms step_avg:39.17ms
step:34/2110 train_time:1326ms step_avg:39.01ms
step:35/2110 train_time:1363ms step_avg:38.94ms
step:36/2110 train_time:1396ms step_avg:38.78ms
step:37/2110 train_time:1431ms step_avg:38.68ms
step:38/2110 train_time:1465ms step_avg:38.54ms
step:39/2110 train_time:1498ms step_avg:38.42ms
step:40/2110 train_time:1531ms step_avg:38.29ms
step:41/2110 train_time:1565ms step_avg:38.17ms
step:42/2110 train_time:1598ms step_avg:38.05ms
step:43/2110 train_time:1631ms step_avg:37.94ms
step:44/2110 train_time:1665ms step_avg:37.83ms
step:45/2110 train_time:1698ms step_avg:37.74ms
step:46/2110 train_time:1731ms step_avg:37.63ms
step:47/2110 train_time:1765ms step_avg:37.55ms
step:48/2110 train_time:1797ms step_avg:37.45ms
step:49/2110 train_time:1831ms step_avg:37.37ms
step:50/2110 train_time:1864ms step_avg:37.27ms
step:51/2110 train_time:1898ms step_avg:37.21ms
step:52/2110 train_time:1931ms step_avg:37.13ms
step:53/2110 train_time:1964ms step_avg:37.06ms
step:54/2110 train_time:1997ms step_avg:36.98ms
step:55/2110 train_time:2030ms step_avg:36.91ms
step:56/2110 train_time:2063ms step_avg:36.84ms
step:57/2110 train_time:2097ms step_avg:36.78ms
step:58/2110 train_time:2129ms step_avg:36.71ms
step:59/2110 train_time:2163ms step_avg:36.66ms
step:60/2110 train_time:2196ms step_avg:36.59ms
step:61/2110 train_time:2229ms step_avg:36.55ms
step:62/2110 train_time:2262ms step_avg:36.49ms
step:63/2110 train_time:2297ms step_avg:36.46ms
step:64/2110 train_time:2330ms step_avg:36.40ms
step:65/2110 train_time:2365ms step_avg:36.38ms
step:66/2110 train_time:2397ms step_avg:36.32ms
step:67/2110 train_time:2432ms step_avg:36.30ms
step:68/2110 train_time:2465ms step_avg:36.24ms
step:69/2110 train_time:2499ms step_avg:36.22ms
step:70/2110 train_time:2532ms step_avg:36.17ms
step:71/2110 train_time:2566ms step_avg:36.14ms
step:72/2110 train_time:2599ms step_avg:36.09ms
step:73/2110 train_time:2633ms step_avg:36.06ms
step:74/2110 train_time:2666ms step_avg:36.02ms
step:75/2110 train_time:2700ms step_avg:35.99ms
step:76/2110 train_time:2732ms step_avg:35.95ms
step:77/2110 train_time:2766ms step_avg:35.92ms
step:78/2110 train_time:2799ms step_avg:35.88ms
step:79/2110 train_time:2833ms step_avg:35.86ms
step:80/2110 train_time:2866ms step_avg:35.82ms
step:81/2110 train_time:2899ms step_avg:35.79ms
step:82/2110 train_time:2932ms step_avg:35.75ms
step:83/2110 train_time:2966ms step_avg:35.73ms
step:84/2110 train_time:2998ms step_avg:35.69ms
step:85/2110 train_time:3032ms step_avg:35.67ms
step:86/2110 train_time:3064ms step_avg:35.63ms
step:87/2110 train_time:3098ms step_avg:35.61ms
step:88/2110 train_time:3130ms step_avg:35.57ms
step:89/2110 train_time:3164ms step_avg:35.55ms
step:90/2110 train_time:3197ms step_avg:35.52ms
step:91/2110 train_time:3230ms step_avg:35.50ms
step:92/2110 train_time:3263ms step_avg:35.47ms
step:93/2110 train_time:3297ms step_avg:35.45ms
step:94/2110 train_time:3330ms step_avg:35.42ms
step:95/2110 train_time:3364ms step_avg:35.41ms
step:96/2110 train_time:3396ms step_avg:35.38ms
step:97/2110 train_time:3430ms step_avg:35.36ms
step:98/2110 train_time:3463ms step_avg:35.34ms
step:99/2110 train_time:3497ms step_avg:35.32ms
step:100/2110 train_time:3530ms step_avg:35.30ms
step:101/2110 train_time:3564ms step_avg:35.29ms
step:102/2110 train_time:3597ms step_avg:35.26ms
step:103/2110 train_time:3630ms step_avg:35.25ms
step:104/2110 train_time:3663ms step_avg:35.22ms
step:105/2110 train_time:3697ms step_avg:35.21ms
step:106/2110 train_time:3730ms step_avg:35.19ms
step:107/2110 train_time:3764ms step_avg:35.18ms
step:108/2110 train_time:3796ms step_avg:35.15ms
step:109/2110 train_time:3831ms step_avg:35.14ms
step:110/2110 train_time:3863ms step_avg:35.12ms
step:111/2110 train_time:3897ms step_avg:35.11ms
step:112/2110 train_time:3929ms step_avg:35.08ms
step:113/2110 train_time:3963ms step_avg:35.07ms
step:114/2110 train_time:3996ms step_avg:35.05ms
step:115/2110 train_time:4029ms step_avg:35.04ms
step:116/2110 train_time:4062ms step_avg:35.02ms
step:117/2110 train_time:4095ms step_avg:35.00ms
step:118/2110 train_time:4128ms step_avg:34.98ms
step:119/2110 train_time:4161ms step_avg:34.97ms
step:120/2110 train_time:4194ms step_avg:34.95ms
step:121/2110 train_time:4228ms step_avg:34.94ms
step:122/2110 train_time:4260ms step_avg:34.92ms
step:123/2110 train_time:4294ms step_avg:34.91ms
step:124/2110 train_time:4326ms step_avg:34.89ms
step:125/2110 train_time:4361ms step_avg:34.88ms
step:126/2110 train_time:4393ms step_avg:34.87ms
step:127/2110 train_time:4427ms step_avg:34.86ms
step:128/2110 train_time:4460ms step_avg:34.84ms
step:129/2110 train_time:4494ms step_avg:34.83ms
step:130/2110 train_time:4526ms step_avg:34.82ms
step:131/2110 train_time:4560ms step_avg:34.81ms
step:132/2110 train_time:4593ms step_avg:34.79ms
step:133/2110 train_time:4627ms step_avg:34.79ms
step:134/2110 train_time:4659ms step_avg:34.77ms
step:135/2110 train_time:4693ms step_avg:34.76ms
step:136/2110 train_time:4726ms step_avg:34.75ms
step:137/2110 train_time:4760ms step_avg:34.74ms
step:138/2110 train_time:4792ms step_avg:34.73ms
step:139/2110 train_time:4826ms step_avg:34.72ms
step:140/2110 train_time:4860ms step_avg:34.71ms
step:141/2110 train_time:4893ms step_avg:34.70ms
step:142/2110 train_time:4926ms step_avg:34.69ms
step:143/2110 train_time:4959ms step_avg:34.68ms
step:144/2110 train_time:4992ms step_avg:34.67ms
step:145/2110 train_time:5026ms step_avg:34.66ms
step:146/2110 train_time:5058ms step_avg:34.65ms
step:147/2110 train_time:5092ms step_avg:34.64ms
step:148/2110 train_time:5125ms step_avg:34.63ms
step:149/2110 train_time:5158ms step_avg:34.62ms
step:150/2110 train_time:5191ms step_avg:34.61ms
step:151/2110 train_time:5225ms step_avg:34.60ms
step:152/2110 train_time:5257ms step_avg:34.59ms
step:153/2110 train_time:5291ms step_avg:34.58ms
step:154/2110 train_time:5324ms step_avg:34.57ms
step:155/2110 train_time:5358ms step_avg:34.57ms
step:156/2110 train_time:5390ms step_avg:34.55ms
step:157/2110 train_time:5424ms step_avg:34.55ms
step:158/2110 train_time:5456ms step_avg:34.53ms
step:159/2110 train_time:5490ms step_avg:34.53ms
step:160/2110 train_time:5523ms step_avg:34.52ms
step:161/2110 train_time:5556ms step_avg:34.51ms
step:162/2110 train_time:5589ms step_avg:34.50ms
step:163/2110 train_time:5623ms step_avg:34.49ms
step:164/2110 train_time:5655ms step_avg:34.48ms
step:165/2110 train_time:5689ms step_avg:34.48ms
step:166/2110 train_time:5721ms step_avg:34.46ms
step:167/2110 train_time:5755ms step_avg:34.46ms
step:168/2110 train_time:5787ms step_avg:34.45ms
step:169/2110 train_time:5821ms step_avg:34.45ms
step:170/2110 train_time:5854ms step_avg:34.44ms
step:171/2110 train_time:5887ms step_avg:34.43ms
step:172/2110 train_time:5920ms step_avg:34.42ms
step:173/2110 train_time:5954ms step_avg:34.42ms
step:174/2110 train_time:5986ms step_avg:34.40ms
step:175/2110 train_time:6021ms step_avg:34.41ms
step:176/2110 train_time:6054ms step_avg:34.40ms
step:177/2110 train_time:6087ms step_avg:34.39ms
step:178/2110 train_time:6120ms step_avg:34.38ms
step:179/2110 train_time:6154ms step_avg:34.38ms
step:180/2110 train_time:6186ms step_avg:34.37ms
step:181/2110 train_time:6220ms step_avg:34.36ms
step:182/2110 train_time:6252ms step_avg:34.35ms
step:183/2110 train_time:6286ms step_avg:34.35ms
step:184/2110 train_time:6319ms step_avg:34.34ms
step:185/2110 train_time:6353ms step_avg:34.34ms
step:186/2110 train_time:6385ms step_avg:34.33ms
step:187/2110 train_time:6419ms step_avg:34.33ms
step:188/2110 train_time:6452ms step_avg:34.32ms
step:189/2110 train_time:6486ms step_avg:34.31ms
step:190/2110 train_time:6518ms step_avg:34.31ms
step:191/2110 train_time:6552ms step_avg:34.30ms
step:192/2110 train_time:6585ms step_avg:34.30ms
step:193/2110 train_time:6619ms step_avg:34.30ms
step:194/2110 train_time:6652ms step_avg:34.29ms
step:195/2110 train_time:6686ms step_avg:34.28ms
step:196/2110 train_time:6718ms step_avg:34.27ms
step:197/2110 train_time:6752ms step_avg:34.27ms
step:198/2110 train_time:6784ms step_avg:34.26ms
step:199/2110 train_time:6818ms step_avg:34.26ms
step:200/2110 train_time:6850ms step_avg:34.25ms
step:201/2110 train_time:6884ms step_avg:34.25ms
step:202/2110 train_time:6917ms step_avg:34.24ms
step:203/2110 train_time:6950ms step_avg:34.24ms
step:204/2110 train_time:6983ms step_avg:34.23ms
step:205/2110 train_time:7016ms step_avg:34.23ms
step:206/2110 train_time:7049ms step_avg:34.22ms
step:207/2110 train_time:7083ms step_avg:34.22ms
step:208/2110 train_time:7115ms step_avg:34.21ms
step:209/2110 train_time:7149ms step_avg:34.21ms
step:210/2110 train_time:7181ms step_avg:34.20ms
step:211/2110 train_time:7215ms step_avg:34.20ms
step:212/2110 train_time:7248ms step_avg:34.19ms
step:213/2110 train_time:7282ms step_avg:34.19ms
step:214/2110 train_time:7314ms step_avg:34.18ms
step:215/2110 train_time:7348ms step_avg:34.18ms
step:216/2110 train_time:7380ms step_avg:34.17ms
step:217/2110 train_time:7414ms step_avg:34.17ms
step:218/2110 train_time:7447ms step_avg:34.16ms
step:219/2110 train_time:7481ms step_avg:34.16ms
step:220/2110 train_time:7513ms step_avg:34.15ms
step:221/2110 train_time:7547ms step_avg:34.15ms
step:222/2110 train_time:7580ms step_avg:34.14ms
step:223/2110 train_time:7614ms step_avg:34.14ms
step:224/2110 train_time:7646ms step_avg:34.14ms
step:225/2110 train_time:7680ms step_avg:34.13ms
step:226/2110 train_time:7713ms step_avg:34.13ms
step:227/2110 train_time:7747ms step_avg:34.13ms
step:228/2110 train_time:7780ms step_avg:34.12ms
step:229/2110 train_time:7814ms step_avg:34.12ms
step:230/2110 train_time:7846ms step_avg:34.11ms
step:231/2110 train_time:7880ms step_avg:34.11ms
step:232/2110 train_time:7913ms step_avg:34.11ms
step:233/2110 train_time:7946ms step_avg:34.10ms
step:234/2110 train_time:7979ms step_avg:34.10ms
step:235/2110 train_time:8012ms step_avg:34.09ms
step:236/2110 train_time:8045ms step_avg:34.09ms
step:237/2110 train_time:8078ms step_avg:34.09ms
step:238/2110 train_time:8111ms step_avg:34.08ms
step:239/2110 train_time:8145ms step_avg:34.08ms
step:240/2110 train_time:8177ms step_avg:34.07ms
step:241/2110 train_time:8211ms step_avg:34.07ms
step:242/2110 train_time:8243ms step_avg:34.06ms
step:243/2110 train_time:8277ms step_avg:34.06ms
step:244/2110 train_time:8310ms step_avg:34.06ms
step:245/2110 train_time:8343ms step_avg:34.05ms
step:246/2110 train_time:8375ms step_avg:34.05ms
step:247/2110 train_time:8409ms step_avg:34.05ms
step:248/2110 train_time:8442ms step_avg:34.04ms
step:249/2110 train_time:8475ms step_avg:34.04ms
step:250/2110 train_time:8508ms step_avg:34.03ms
step:250/2110 val_loss:4.2751 train_time:8544ms step_avg:34.18ms
step:251/2110 train_time:8564ms step_avg:34.12ms
step:252/2110 train_time:8584ms step_avg:34.06ms
step:253/2110 train_time:8613ms step_avg:34.04ms
step:254/2110 train_time:8646ms step_avg:34.04ms
step:255/2110 train_time:8681ms step_avg:34.04ms
step:256/2110 train_time:8715ms step_avg:34.04ms
step:257/2110 train_time:8749ms step_avg:34.04ms
step:258/2110 train_time:8781ms step_avg:34.04ms
step:259/2110 train_time:8815ms step_avg:34.03ms
step:260/2110 train_time:8848ms step_avg:34.03ms
step:261/2110 train_time:8882ms step_avg:34.03ms
step:262/2110 train_time:8915ms step_avg:34.03ms
step:263/2110 train_time:8948ms step_avg:34.02ms
step:264/2110 train_time:8980ms step_avg:34.02ms
step:265/2110 train_time:9014ms step_avg:34.01ms
step:266/2110 train_time:9046ms step_avg:34.01ms
step:267/2110 train_time:9080ms step_avg:34.01ms
step:268/2110 train_time:9112ms step_avg:34.00ms
step:269/2110 train_time:9146ms step_avg:34.00ms
step:270/2110 train_time:9178ms step_avg:33.99ms
step:271/2110 train_time:9211ms step_avg:33.99ms
step:272/2110 train_time:9244ms step_avg:33.98ms
step:273/2110 train_time:9277ms step_avg:33.98ms
step:274/2110 train_time:9309ms step_avg:33.98ms
step:275/2110 train_time:9343ms step_avg:33.98ms
step:276/2110 train_time:9376ms step_avg:33.97ms
step:277/2110 train_time:9409ms step_avg:33.97ms
step:278/2110 train_time:9442ms step_avg:33.96ms
step:279/2110 train_time:9475ms step_avg:33.96ms
step:280/2110 train_time:9508ms step_avg:33.96ms
step:281/2110 train_time:9541ms step_avg:33.96ms
step:282/2110 train_time:9574ms step_avg:33.95ms
step:283/2110 train_time:9608ms step_avg:33.95ms
step:284/2110 train_time:9641ms step_avg:33.95ms
step:285/2110 train_time:9675ms step_avg:33.95ms
step:286/2110 train_time:9708ms step_avg:33.94ms
step:287/2110 train_time:9742ms step_avg:33.94ms
step:288/2110 train_time:9775ms step_avg:33.94ms
step:289/2110 train_time:9809ms step_avg:33.94ms
step:290/2110 train_time:9841ms step_avg:33.94ms
step:291/2110 train_time:9876ms step_avg:33.94ms
step:292/2110 train_time:9908ms step_avg:33.93ms
step:293/2110 train_time:9942ms step_avg:33.93ms
step:294/2110 train_time:9974ms step_avg:33.93ms
step:295/2110 train_time:10008ms step_avg:33.92ms
step:296/2110 train_time:10040ms step_avg:33.92ms
step:297/2110 train_time:10073ms step_avg:33.92ms
step:298/2110 train_time:10106ms step_avg:33.91ms
step:299/2110 train_time:10139ms step_avg:33.91ms
step:300/2110 train_time:10172ms step_avg:33.91ms
step:301/2110 train_time:10205ms step_avg:33.90ms
step:302/2110 train_time:10238ms step_avg:33.90ms
step:303/2110 train_time:10271ms step_avg:33.90ms
step:304/2110 train_time:10304ms step_avg:33.89ms
step:305/2110 train_time:10337ms step_avg:33.89ms
step:306/2110 train_time:10369ms step_avg:33.89ms
step:307/2110 train_time:10403ms step_avg:33.89ms
step:308/2110 train_time:10435ms step_avg:33.88ms
step:309/2110 train_time:10469ms step_avg:33.88ms
step:310/2110 train_time:10501ms step_avg:33.88ms
step:311/2110 train_time:10535ms step_avg:33.87ms
step:312/2110 train_time:10567ms step_avg:33.87ms
step:313/2110 train_time:10601ms step_avg:33.87ms
step:314/2110 train_time:10634ms step_avg:33.87ms
step:315/2110 train_time:10668ms step_avg:33.87ms
step:316/2110 train_time:10700ms step_avg:33.86ms
step:317/2110 train_time:10735ms step_avg:33.86ms
step:318/2110 train_time:10768ms step_avg:33.86ms
step:319/2110 train_time:10801ms step_avg:33.86ms
step:320/2110 train_time:10834ms step_avg:33.86ms
step:321/2110 train_time:10868ms step_avg:33.86ms
step:322/2110 train_time:10901ms step_avg:33.85ms
step:323/2110 train_time:10935ms step_avg:33.85ms
step:324/2110 train_time:10967ms step_avg:33.85ms
step:325/2110 train_time:11002ms step_avg:33.85ms
step:326/2110 train_time:11034ms step_avg:33.85ms
step:327/2110 train_time:11068ms step_avg:33.85ms
step:328/2110 train_time:11100ms step_avg:33.84ms
step:329/2110 train_time:11134ms step_avg:33.84ms
step:330/2110 train_time:11166ms step_avg:33.84ms
step:331/2110 train_time:11200ms step_avg:33.84ms
step:332/2110 train_time:11232ms step_avg:33.83ms
step:333/2110 train_time:11266ms step_avg:33.83ms
step:334/2110 train_time:11298ms step_avg:33.83ms
step:335/2110 train_time:11332ms step_avg:33.83ms
step:336/2110 train_time:11364ms step_avg:33.82ms
step:337/2110 train_time:11398ms step_avg:33.82ms
step:338/2110 train_time:11430ms step_avg:33.82ms
step:339/2110 train_time:11464ms step_avg:33.82ms
step:340/2110 train_time:11497ms step_avg:33.81ms
step:341/2110 train_time:11531ms step_avg:33.81ms
step:342/2110 train_time:11563ms step_avg:33.81ms
step:343/2110 train_time:11597ms step_avg:33.81ms
step:344/2110 train_time:11630ms step_avg:33.81ms
step:345/2110 train_time:11663ms step_avg:33.81ms
step:346/2110 train_time:11696ms step_avg:33.80ms
step:347/2110 train_time:11730ms step_avg:33.81ms
step:348/2110 train_time:11763ms step_avg:33.80ms
step:349/2110 train_time:11797ms step_avg:33.80ms
step:350/2110 train_time:11830ms step_avg:33.80ms
step:351/2110 train_time:11864ms step_avg:33.80ms
step:352/2110 train_time:11896ms step_avg:33.80ms
step:353/2110 train_time:11930ms step_avg:33.80ms
step:354/2110 train_time:11963ms step_avg:33.79ms
step:355/2110 train_time:11997ms step_avg:33.79ms
step:356/2110 train_time:12030ms step_avg:33.79ms
step:357/2110 train_time:12063ms step_avg:33.79ms
step:358/2110 train_time:12096ms step_avg:33.79ms
step:359/2110 train_time:12129ms step_avg:33.79ms
step:360/2110 train_time:12161ms step_avg:33.78ms
step:361/2110 train_time:12195ms step_avg:33.78ms
step:362/2110 train_time:12228ms step_avg:33.78ms
step:363/2110 train_time:12261ms step_avg:33.78ms
step:364/2110 train_time:12294ms step_avg:33.77ms
step:365/2110 train_time:12327ms step_avg:33.77ms
step:366/2110 train_time:12360ms step_avg:33.77ms
step:367/2110 train_time:12393ms step_avg:33.77ms
step:368/2110 train_time:12426ms step_avg:33.77ms
step:369/2110 train_time:12459ms step_avg:33.76ms
step:370/2110 train_time:12492ms step_avg:33.76ms
step:371/2110 train_time:12525ms step_avg:33.76ms
step:372/2110 train_time:12558ms step_avg:33.76ms
step:373/2110 train_time:12591ms step_avg:33.76ms
step:374/2110 train_time:12623ms step_avg:33.75ms
step:375/2110 train_time:12657ms step_avg:33.75ms
step:376/2110 train_time:12690ms step_avg:33.75ms
step:377/2110 train_time:12724ms step_avg:33.75ms
step:378/2110 train_time:12756ms step_avg:33.75ms
step:379/2110 train_time:12790ms step_avg:33.75ms
step:380/2110 train_time:12823ms step_avg:33.74ms
step:381/2110 train_time:12856ms step_avg:33.74ms
step:382/2110 train_time:12889ms step_avg:33.74ms
step:383/2110 train_time:12923ms step_avg:33.74ms
step:384/2110 train_time:12956ms step_avg:33.74ms
step:385/2110 train_time:12989ms step_avg:33.74ms
step:386/2110 train_time:13022ms step_avg:33.74ms
step:387/2110 train_time:13055ms step_avg:33.73ms
step:388/2110 train_time:13088ms step_avg:33.73ms
step:389/2110 train_time:13122ms step_avg:33.73ms
step:390/2110 train_time:13154ms step_avg:33.73ms
step:391/2110 train_time:13188ms step_avg:33.73ms
step:392/2110 train_time:13220ms step_avg:33.73ms
step:393/2110 train_time:13254ms step_avg:33.73ms
step:394/2110 train_time:13287ms step_avg:33.72ms
step:395/2110 train_time:13320ms step_avg:33.72ms
step:396/2110 train_time:13353ms step_avg:33.72ms
step:397/2110 train_time:13387ms step_avg:33.72ms
step:398/2110 train_time:13419ms step_avg:33.72ms
step:399/2110 train_time:13453ms step_avg:33.72ms
step:400/2110 train_time:13485ms step_avg:33.71ms
step:401/2110 train_time:13519ms step_avg:33.71ms
step:402/2110 train_time:13551ms step_avg:33.71ms
step:403/2110 train_time:13585ms step_avg:33.71ms
step:404/2110 train_time:13618ms step_avg:33.71ms
step:405/2110 train_time:13651ms step_avg:33.71ms
step:406/2110 train_time:13684ms step_avg:33.70ms
step:407/2110 train_time:13717ms step_avg:33.70ms
step:408/2110 train_time:13750ms step_avg:33.70ms
step:409/2110 train_time:13784ms step_avg:33.70ms
step:410/2110 train_time:13816ms step_avg:33.70ms
step:411/2110 train_time:13850ms step_avg:33.70ms
step:412/2110 train_time:13883ms step_avg:33.70ms
step:413/2110 train_time:13917ms step_avg:33.70ms
step:414/2110 train_time:13949ms step_avg:33.69ms
step:415/2110 train_time:13983ms step_avg:33.69ms
step:416/2110 train_time:14016ms step_avg:33.69ms
step:417/2110 train_time:14049ms step_avg:33.69ms
step:418/2110 train_time:14082ms step_avg:33.69ms
step:419/2110 train_time:14116ms step_avg:33.69ms
step:420/2110 train_time:14148ms step_avg:33.69ms
step:421/2110 train_time:14182ms step_avg:33.69ms
step:422/2110 train_time:14215ms step_avg:33.68ms
step:423/2110 train_time:14248ms step_avg:33.68ms
step:424/2110 train_time:14281ms step_avg:33.68ms
step:425/2110 train_time:14315ms step_avg:33.68ms
step:426/2110 train_time:14348ms step_avg:33.68ms
step:427/2110 train_time:14381ms step_avg:33.68ms
step:428/2110 train_time:14414ms step_avg:33.68ms
step:429/2110 train_time:14447ms step_avg:33.68ms
step:430/2110 train_time:14480ms step_avg:33.67ms
step:431/2110 train_time:14513ms step_avg:33.67ms
step:432/2110 train_time:14546ms step_avg:33.67ms
step:433/2110 train_time:14580ms step_avg:33.67ms
step:434/2110 train_time:14612ms step_avg:33.67ms
step:435/2110 train_time:14646ms step_avg:33.67ms
step:436/2110 train_time:14679ms step_avg:33.67ms
step:437/2110 train_time:14712ms step_avg:33.67ms
step:438/2110 train_time:14745ms step_avg:33.66ms
step:439/2110 train_time:14779ms step_avg:33.67ms
step:440/2110 train_time:14812ms step_avg:33.66ms
step:441/2110 train_time:14846ms step_avg:33.66ms
step:442/2110 train_time:14878ms step_avg:33.66ms
step:443/2110 train_time:14912ms step_avg:33.66ms
step:444/2110 train_time:14945ms step_avg:33.66ms
step:445/2110 train_time:14978ms step_avg:33.66ms
step:446/2110 train_time:15011ms step_avg:33.66ms
step:447/2110 train_time:15045ms step_avg:33.66ms
step:448/2110 train_time:15078ms step_avg:33.66ms
step:449/2110 train_time:15112ms step_avg:33.66ms
step:450/2110 train_time:15144ms step_avg:33.65ms
step:451/2110 train_time:15178ms step_avg:33.65ms
step:452/2110 train_time:15211ms step_avg:33.65ms
step:453/2110 train_time:15244ms step_avg:33.65ms
step:454/2110 train_time:15277ms step_avg:33.65ms
step:455/2110 train_time:15311ms step_avg:33.65ms
step:456/2110 train_time:15344ms step_avg:33.65ms
step:457/2110 train_time:15378ms step_avg:33.65ms
step:458/2110 train_time:15411ms step_avg:33.65ms
step:459/2110 train_time:15444ms step_avg:33.65ms
step:460/2110 train_time:15477ms step_avg:33.65ms
step:461/2110 train_time:15510ms step_avg:33.65ms
step:462/2110 train_time:15543ms step_avg:33.64ms
step:463/2110 train_time:15577ms step_avg:33.64ms
step:464/2110 train_time:15610ms step_avg:33.64ms
step:465/2110 train_time:15643ms step_avg:33.64ms
step:466/2110 train_time:15676ms step_avg:33.64ms
step:467/2110 train_time:15709ms step_avg:33.64ms
step:468/2110 train_time:15742ms step_avg:33.64ms
step:469/2110 train_time:15776ms step_avg:33.64ms
step:470/2110 train_time:15809ms step_avg:33.64ms
step:471/2110 train_time:15842ms step_avg:33.64ms
step:472/2110 train_time:15875ms step_avg:33.63ms
step:473/2110 train_time:15908ms step_avg:33.63ms
step:474/2110 train_time:15941ms step_avg:33.63ms
step:475/2110 train_time:15975ms step_avg:33.63ms
step:476/2110 train_time:16007ms step_avg:33.63ms
step:477/2110 train_time:16041ms step_avg:33.63ms
step:478/2110 train_time:16074ms step_avg:33.63ms
step:479/2110 train_time:16108ms step_avg:33.63ms
step:480/2110 train_time:16140ms step_avg:33.63ms
step:481/2110 train_time:16174ms step_avg:33.63ms
step:482/2110 train_time:16207ms step_avg:33.62ms
step:483/2110 train_time:16240ms step_avg:33.62ms
step:484/2110 train_time:16273ms step_avg:33.62ms
step:485/2110 train_time:16307ms step_avg:33.62ms
step:486/2110 train_time:16339ms step_avg:33.62ms
step:487/2110 train_time:16373ms step_avg:33.62ms
step:488/2110 train_time:16406ms step_avg:33.62ms
step:489/2110 train_time:16440ms step_avg:33.62ms
step:490/2110 train_time:16472ms step_avg:33.62ms
step:491/2110 train_time:16506ms step_avg:33.62ms
step:492/2110 train_time:16539ms step_avg:33.62ms
step:493/2110 train_time:16572ms step_avg:33.61ms
step:494/2110 train_time:16604ms step_avg:33.61ms
step:495/2110 train_time:16638ms step_avg:33.61ms
step:496/2110 train_time:16671ms step_avg:33.61ms
step:497/2110 train_time:16704ms step_avg:33.61ms
step:498/2110 train_time:16737ms step_avg:33.61ms
step:499/2110 train_time:16770ms step_avg:33.61ms
step:500/2110 train_time:16803ms step_avg:33.61ms
step:500/2110 val_loss:4.0042 train_time:16839ms step_avg:33.68ms
step:501/2110 train_time:16859ms step_avg:33.65ms
step:502/2110 train_time:16879ms step_avg:33.62ms
step:503/2110 train_time:16907ms step_avg:33.61ms
step:504/2110 train_time:16940ms step_avg:33.61ms
step:505/2110 train_time:16975ms step_avg:33.61ms
step:506/2110 train_time:17008ms step_avg:33.61ms
step:507/2110 train_time:17042ms step_avg:33.61ms
step:508/2110 train_time:17075ms step_avg:33.61ms
step:509/2110 train_time:17108ms step_avg:33.61ms
step:510/2110 train_time:17141ms step_avg:33.61ms
step:511/2110 train_time:17174ms step_avg:33.61ms
step:512/2110 train_time:17207ms step_avg:33.61ms
step:513/2110 train_time:17241ms step_avg:33.61ms
step:514/2110 train_time:17273ms step_avg:33.61ms
step:515/2110 train_time:17306ms step_avg:33.60ms
step:516/2110 train_time:17339ms step_avg:33.60ms
step:517/2110 train_time:17372ms step_avg:33.60ms
step:518/2110 train_time:17405ms step_avg:33.60ms
step:519/2110 train_time:17438ms step_avg:33.60ms
step:520/2110 train_time:17470ms step_avg:33.60ms
step:521/2110 train_time:17504ms step_avg:33.60ms
step:522/2110 train_time:17536ms step_avg:33.59ms
step:523/2110 train_time:17569ms step_avg:33.59ms
step:524/2110 train_time:17602ms step_avg:33.59ms
step:525/2110 train_time:17635ms step_avg:33.59ms
step:526/2110 train_time:17668ms step_avg:33.59ms
step:527/2110 train_time:17701ms step_avg:33.59ms
step:528/2110 train_time:17734ms step_avg:33.59ms
step:529/2110 train_time:17768ms step_avg:33.59ms
step:530/2110 train_time:17800ms step_avg:33.59ms
step:531/2110 train_time:17834ms step_avg:33.59ms
step:532/2110 train_time:17867ms step_avg:33.58ms
step:533/2110 train_time:17901ms step_avg:33.59ms
step:534/2110 train_time:17934ms step_avg:33.58ms
step:535/2110 train_time:17968ms step_avg:33.59ms
step:536/2110 train_time:18001ms step_avg:33.58ms
step:537/2110 train_time:18035ms step_avg:33.58ms
step:538/2110 train_time:18068ms step_avg:33.58ms
step:539/2110 train_time:18101ms step_avg:33.58ms
step:540/2110 train_time:18134ms step_avg:33.58ms
step:541/2110 train_time:18168ms step_avg:33.58ms
step:542/2110 train_time:18201ms step_avg:33.58ms
step:543/2110 train_time:18234ms step_avg:33.58ms
step:544/2110 train_time:18267ms step_avg:33.58ms
step:545/2110 train_time:18301ms step_avg:33.58ms
step:546/2110 train_time:18333ms step_avg:33.58ms
step:547/2110 train_time:18367ms step_avg:33.58ms
step:548/2110 train_time:18400ms step_avg:33.58ms
step:549/2110 train_time:18433ms step_avg:33.58ms
step:550/2110 train_time:18465ms step_avg:33.57ms
step:551/2110 train_time:18499ms step_avg:33.57ms
step:552/2110 train_time:18531ms step_avg:33.57ms
step:553/2110 train_time:18565ms step_avg:33.57ms
step:554/2110 train_time:18597ms step_avg:33.57ms
step:555/2110 train_time:18630ms step_avg:33.57ms
step:556/2110 train_time:18663ms step_avg:33.57ms
step:557/2110 train_time:18696ms step_avg:33.57ms
step:558/2110 train_time:18729ms step_avg:33.56ms
step:559/2110 train_time:18762ms step_avg:33.56ms
step:560/2110 train_time:18795ms step_avg:33.56ms
step:561/2110 train_time:18828ms step_avg:33.56ms
step:562/2110 train_time:18861ms step_avg:33.56ms
step:563/2110 train_time:18895ms step_avg:33.56ms
step:564/2110 train_time:18928ms step_avg:33.56ms
step:565/2110 train_time:18961ms step_avg:33.56ms
step:566/2110 train_time:18994ms step_avg:33.56ms
step:567/2110 train_time:19027ms step_avg:33.56ms
step:568/2110 train_time:19060ms step_avg:33.56ms
step:569/2110 train_time:19093ms step_avg:33.56ms
step:570/2110 train_time:19126ms step_avg:33.56ms
step:571/2110 train_time:19160ms step_avg:33.56ms
step:572/2110 train_time:19193ms step_avg:33.55ms
step:573/2110 train_time:19227ms step_avg:33.55ms
step:574/2110 train_time:19259ms step_avg:33.55ms
step:575/2110 train_time:19293ms step_avg:33.55ms
step:576/2110 train_time:19326ms step_avg:33.55ms
step:577/2110 train_time:19359ms step_avg:33.55ms
step:578/2110 train_time:19392ms step_avg:33.55ms
step:579/2110 train_time:19426ms step_avg:33.55ms
step:580/2110 train_time:19459ms step_avg:33.55ms
step:581/2110 train_time:19492ms step_avg:33.55ms
step:582/2110 train_time:19525ms step_avg:33.55ms
step:583/2110 train_time:19558ms step_avg:33.55ms
step:584/2110 train_time:19591ms step_avg:33.55ms
step:585/2110 train_time:19624ms step_avg:33.55ms
step:586/2110 train_time:19657ms step_avg:33.54ms
step:587/2110 train_time:19690ms step_avg:33.54ms
step:588/2110 train_time:19723ms step_avg:33.54ms
step:589/2110 train_time:19756ms step_avg:33.54ms
step:590/2110 train_time:19789ms step_avg:33.54ms
step:591/2110 train_time:19822ms step_avg:33.54ms
step:592/2110 train_time:19855ms step_avg:33.54ms
step:593/2110 train_time:19888ms step_avg:33.54ms
step:594/2110 train_time:19921ms step_avg:33.54ms
step:595/2110 train_time:19954ms step_avg:33.54ms
step:596/2110 train_time:19987ms step_avg:33.54ms
step:597/2110 train_time:20021ms step_avg:33.54ms
step:598/2110 train_time:20053ms step_avg:33.53ms
step:599/2110 train_time:20087ms step_avg:33.53ms
step:600/2110 train_time:20119ms step_avg:33.53ms
step:601/2110 train_time:20153ms step_avg:33.53ms
step:602/2110 train_time:20186ms step_avg:33.53ms
step:603/2110 train_time:20220ms step_avg:33.53ms
step:604/2110 train_time:20252ms step_avg:33.53ms
step:605/2110 train_time:20286ms step_avg:33.53ms
step:606/2110 train_time:20319ms step_avg:33.53ms
step:607/2110 train_time:20353ms step_avg:33.53ms
step:608/2110 train_time:20386ms step_avg:33.53ms
step:609/2110 train_time:20419ms step_avg:33.53ms
step:610/2110 train_time:20452ms step_avg:33.53ms
step:611/2110 train_time:20486ms step_avg:33.53ms
step:612/2110 train_time:20518ms step_avg:33.53ms
step:613/2110 train_time:20552ms step_avg:33.53ms
step:614/2110 train_time:20584ms step_avg:33.53ms
step:615/2110 train_time:20618ms step_avg:33.52ms
step:616/2110 train_time:20651ms step_avg:33.52ms
step:617/2110 train_time:20684ms step_avg:33.52ms
step:618/2110 train_time:20717ms step_avg:33.52ms
step:619/2110 train_time:20750ms step_avg:33.52ms
step:620/2110 train_time:20783ms step_avg:33.52ms
step:621/2110 train_time:20816ms step_avg:33.52ms
step:622/2110 train_time:20849ms step_avg:33.52ms
step:623/2110 train_time:20882ms step_avg:33.52ms
step:624/2110 train_time:20915ms step_avg:33.52ms
step:625/2110 train_time:20948ms step_avg:33.52ms
step:626/2110 train_time:20981ms step_avg:33.52ms
step:627/2110 train_time:21014ms step_avg:33.52ms
step:628/2110 train_time:21047ms step_avg:33.51ms
step:629/2110 train_time:21081ms step_avg:33.51ms
step:630/2110 train_time:21114ms step_avg:33.51ms
step:631/2110 train_time:21147ms step_avg:33.51ms
step:632/2110 train_time:21180ms step_avg:33.51ms
step:633/2110 train_time:21213ms step_avg:33.51ms
step:634/2110 train_time:21246ms step_avg:33.51ms
step:635/2110 train_time:21280ms step_avg:33.51ms
step:636/2110 train_time:21313ms step_avg:33.51ms
step:637/2110 train_time:21347ms step_avg:33.51ms
step:638/2110 train_time:21379ms step_avg:33.51ms
step:639/2110 train_time:21413ms step_avg:33.51ms
step:640/2110 train_time:21445ms step_avg:33.51ms
step:641/2110 train_time:21479ms step_avg:33.51ms
step:642/2110 train_time:21512ms step_avg:33.51ms
step:643/2110 train_time:21545ms step_avg:33.51ms
step:644/2110 train_time:21578ms step_avg:33.51ms
step:645/2110 train_time:21611ms step_avg:33.51ms
step:646/2110 train_time:21644ms step_avg:33.50ms
step:647/2110 train_time:21678ms step_avg:33.50ms
step:648/2110 train_time:21710ms step_avg:33.50ms
step:649/2110 train_time:21744ms step_avg:33.50ms
step:650/2110 train_time:21776ms step_avg:33.50ms
step:651/2110 train_time:21810ms step_avg:33.50ms
step:652/2110 train_time:21843ms step_avg:33.50ms
step:653/2110 train_time:21876ms step_avg:33.50ms
step:654/2110 train_time:21909ms step_avg:33.50ms
step:655/2110 train_time:21942ms step_avg:33.50ms
step:656/2110 train_time:21975ms step_avg:33.50ms
step:657/2110 train_time:22008ms step_avg:33.50ms
step:658/2110 train_time:22041ms step_avg:33.50ms
step:659/2110 train_time:22075ms step_avg:33.50ms
step:660/2110 train_time:22107ms step_avg:33.50ms
step:661/2110 train_time:22141ms step_avg:33.50ms
step:662/2110 train_time:22174ms step_avg:33.49ms
step:663/2110 train_time:22207ms step_avg:33.49ms
step:664/2110 train_time:22240ms step_avg:33.49ms
step:665/2110 train_time:22273ms step_avg:33.49ms
step:666/2110 train_time:22306ms step_avg:33.49ms
step:667/2110 train_time:22340ms step_avg:33.49ms
step:668/2110 train_time:22372ms step_avg:33.49ms
step:669/2110 train_time:22406ms step_avg:33.49ms
step:670/2110 train_time:22438ms step_avg:33.49ms
step:671/2110 train_time:22472ms step_avg:33.49ms
step:672/2110 train_time:22505ms step_avg:33.49ms
step:673/2110 train_time:22538ms step_avg:33.49ms
step:674/2110 train_time:22571ms step_avg:33.49ms
step:675/2110 train_time:22605ms step_avg:33.49ms
step:676/2110 train_time:22638ms step_avg:33.49ms
step:677/2110 train_time:22671ms step_avg:33.49ms
step:678/2110 train_time:22704ms step_avg:33.49ms
step:679/2110 train_time:22737ms step_avg:33.49ms
step:680/2110 train_time:22770ms step_avg:33.49ms
step:681/2110 train_time:22804ms step_avg:33.49ms
step:682/2110 train_time:22836ms step_avg:33.48ms
step:683/2110 train_time:22870ms step_avg:33.48ms
step:684/2110 train_time:22902ms step_avg:33.48ms
step:685/2110 train_time:22936ms step_avg:33.48ms
step:686/2110 train_time:22969ms step_avg:33.48ms
step:687/2110 train_time:23002ms step_avg:33.48ms
step:688/2110 train_time:23035ms step_avg:33.48ms
step:689/2110 train_time:23069ms step_avg:33.48ms
step:690/2110 train_time:23101ms step_avg:33.48ms
step:691/2110 train_time:23135ms step_avg:33.48ms
step:692/2110 train_time:23193ms step_avg:33.52ms
step:693/2110 train_time:23255ms step_avg:33.56ms
step:694/2110 train_time:23314ms step_avg:33.59ms
step:695/2110 train_time:23375ms step_avg:33.63ms
step:696/2110 train_time:23434ms step_avg:33.67ms
step:697/2110 train_time:23496ms step_avg:33.71ms
step:698/2110 train_time:23555ms step_avg:33.75ms
step:699/2110 train_time:23616ms step_avg:33.79ms
step:700/2110 train_time:23675ms step_avg:33.82ms
step:701/2110 train_time:23736ms step_avg:33.86ms
step:702/2110 train_time:23796ms step_avg:33.90ms
step:703/2110 train_time:23857ms step_avg:33.94ms
step:704/2110 train_time:23916ms step_avg:33.97ms
step:705/2110 train_time:23977ms step_avg:34.01ms
step:706/2110 train_time:24036ms step_avg:34.05ms
step:707/2110 train_time:24097ms step_avg:34.08ms
step:708/2110 train_time:24155ms step_avg:34.12ms
step:709/2110 train_time:24216ms step_avg:34.16ms
step:710/2110 train_time:24275ms step_avg:34.19ms
step:711/2110 train_time:24337ms step_avg:34.23ms
step:712/2110 train_time:24395ms step_avg:34.26ms
step:713/2110 train_time:24457ms step_avg:34.30ms
step:714/2110 train_time:24515ms step_avg:34.34ms
step:715/2110 train_time:24576ms step_avg:34.37ms
step:716/2110 train_time:24635ms step_avg:34.41ms
step:717/2110 train_time:24696ms step_avg:34.44ms
step:718/2110 train_time:24755ms step_avg:34.48ms
step:719/2110 train_time:24817ms step_avg:34.52ms
step:720/2110 train_time:24876ms step_avg:34.55ms
step:721/2110 train_time:24937ms step_avg:34.59ms
step:722/2110 train_time:24996ms step_avg:34.62ms
step:723/2110 train_time:25058ms step_avg:34.66ms
step:724/2110 train_time:25116ms step_avg:34.69ms
step:725/2110 train_time:25177ms step_avg:34.73ms
step:726/2110 train_time:25235ms step_avg:34.76ms
step:727/2110 train_time:25296ms step_avg:34.80ms
step:728/2110 train_time:25355ms step_avg:34.83ms
step:729/2110 train_time:25417ms step_avg:34.87ms
step:730/2110 train_time:25476ms step_avg:34.90ms
step:731/2110 train_time:25537ms step_avg:34.93ms
step:732/2110 train_time:25596ms step_avg:34.97ms
step:733/2110 train_time:25657ms step_avg:35.00ms
step:734/2110 train_time:25716ms step_avg:35.04ms
step:735/2110 train_time:25777ms step_avg:35.07ms
step:736/2110 train_time:25835ms step_avg:35.10ms
step:737/2110 train_time:25896ms step_avg:35.14ms
step:738/2110 train_time:25956ms step_avg:35.17ms
step:739/2110 train_time:26017ms step_avg:35.21ms
step:740/2110 train_time:26075ms step_avg:35.24ms
step:741/2110 train_time:26136ms step_avg:35.27ms
step:742/2110 train_time:26195ms step_avg:35.30ms
step:743/2110 train_time:26256ms step_avg:35.34ms
step:744/2110 train_time:26315ms step_avg:35.37ms
step:745/2110 train_time:26377ms step_avg:35.41ms
step:746/2110 train_time:26436ms step_avg:35.44ms
step:747/2110 train_time:26497ms step_avg:35.47ms
step:748/2110 train_time:26556ms step_avg:35.50ms
step:749/2110 train_time:26617ms step_avg:35.54ms
step:750/2110 train_time:26676ms step_avg:35.57ms
step:750/2110 val_loss:3.8468 train_time:26738ms step_avg:35.65ms
step:751/2110 train_time:26759ms step_avg:35.63ms
step:752/2110 train_time:26797ms step_avg:35.63ms
step:753/2110 train_time:26861ms step_avg:35.67ms
step:754/2110 train_time:26922ms step_avg:35.71ms
step:755/2110 train_time:26983ms step_avg:35.74ms
step:756/2110 train_time:27043ms step_avg:35.77ms
step:757/2110 train_time:27103ms step_avg:35.80ms
step:758/2110 train_time:27162ms step_avg:35.83ms
step:759/2110 train_time:27223ms step_avg:35.87ms
step:760/2110 train_time:27281ms step_avg:35.90ms
step:761/2110 train_time:27342ms step_avg:35.93ms
step:762/2110 train_time:27402ms step_avg:35.96ms
step:763/2110 train_time:27461ms step_avg:35.99ms
step:764/2110 train_time:27521ms step_avg:36.02ms
step:765/2110 train_time:27583ms step_avg:36.06ms
step:766/2110 train_time:27643ms step_avg:36.09ms
step:767/2110 train_time:27706ms step_avg:36.12ms
step:768/2110 train_time:27766ms step_avg:36.15ms
step:769/2110 train_time:27828ms step_avg:36.19ms
step:770/2110 train_time:27887ms step_avg:36.22ms
step:771/2110 train_time:27948ms step_avg:36.25ms
step:772/2110 train_time:28007ms step_avg:36.28ms
step:773/2110 train_time:28068ms step_avg:36.31ms
step:774/2110 train_time:28126ms step_avg:36.34ms
step:775/2110 train_time:28187ms step_avg:36.37ms
step:776/2110 train_time:28245ms step_avg:36.40ms
step:777/2110 train_time:28305ms step_avg:36.43ms
step:778/2110 train_time:28364ms step_avg:36.46ms
step:779/2110 train_time:28424ms step_avg:36.49ms
step:780/2110 train_time:28483ms step_avg:36.52ms
step:781/2110 train_time:28543ms step_avg:36.55ms
step:782/2110 train_time:28603ms step_avg:36.58ms
step:783/2110 train_time:28664ms step_avg:36.61ms
step:784/2110 train_time:28723ms step_avg:36.64ms
step:785/2110 train_time:28785ms step_avg:36.67ms
step:786/2110 train_time:28845ms step_avg:36.70ms
step:787/2110 train_time:28907ms step_avg:36.73ms
step:788/2110 train_time:28966ms step_avg:36.76ms
step:789/2110 train_time:29027ms step_avg:36.79ms
step:790/2110 train_time:29086ms step_avg:36.82ms
step:791/2110 train_time:29146ms step_avg:36.85ms
step:792/2110 train_time:29205ms step_avg:36.87ms
step:793/2110 train_time:29265ms step_avg:36.90ms
step:794/2110 train_time:29324ms step_avg:36.93ms
step:795/2110 train_time:29384ms step_avg:36.96ms
step:796/2110 train_time:29442ms step_avg:36.99ms
step:797/2110 train_time:29503ms step_avg:37.02ms
step:798/2110 train_time:29563ms step_avg:37.05ms
step:799/2110 train_time:29624ms step_avg:37.08ms
step:800/2110 train_time:29683ms step_avg:37.10ms
step:801/2110 train_time:29745ms step_avg:37.14ms
step:802/2110 train_time:29805ms step_avg:37.16ms
step:803/2110 train_time:29867ms step_avg:37.19ms
step:804/2110 train_time:29926ms step_avg:37.22ms
step:805/2110 train_time:29987ms step_avg:37.25ms
step:806/2110 train_time:30045ms step_avg:37.28ms
step:807/2110 train_time:30106ms step_avg:37.31ms
step:808/2110 train_time:30165ms step_avg:37.33ms
step:809/2110 train_time:30225ms step_avg:37.36ms
step:810/2110 train_time:30284ms step_avg:37.39ms
step:811/2110 train_time:30344ms step_avg:37.42ms
step:812/2110 train_time:30403ms step_avg:37.44ms
step:813/2110 train_time:30464ms step_avg:37.47ms
step:814/2110 train_time:30523ms step_avg:37.50ms
step:815/2110 train_time:30584ms step_avg:37.53ms
step:816/2110 train_time:30643ms step_avg:37.55ms
step:817/2110 train_time:30704ms step_avg:37.58ms
step:818/2110 train_time:30763ms step_avg:37.61ms
step:819/2110 train_time:30826ms step_avg:37.64ms
step:820/2110 train_time:30885ms step_avg:37.66ms
step:821/2110 train_time:30946ms step_avg:37.69ms
step:822/2110 train_time:31006ms step_avg:37.72ms
step:823/2110 train_time:31067ms step_avg:37.75ms
step:824/2110 train_time:31125ms step_avg:37.77ms
step:825/2110 train_time:31186ms step_avg:37.80ms
step:826/2110 train_time:31244ms step_avg:37.83ms
step:827/2110 train_time:31305ms step_avg:37.85ms
step:828/2110 train_time:31364ms step_avg:37.88ms
step:829/2110 train_time:31425ms step_avg:37.91ms
step:830/2110 train_time:31483ms step_avg:37.93ms
step:831/2110 train_time:31544ms step_avg:37.96ms
step:832/2110 train_time:31603ms step_avg:37.98ms
step:833/2110 train_time:31665ms step_avg:38.01ms
step:834/2110 train_time:31724ms step_avg:38.04ms
step:835/2110 train_time:31785ms step_avg:38.07ms
step:836/2110 train_time:31844ms step_avg:38.09ms
step:837/2110 train_time:31906ms step_avg:38.12ms
step:838/2110 train_time:31965ms step_avg:38.14ms
step:839/2110 train_time:32026ms step_avg:38.17ms
step:840/2110 train_time:32085ms step_avg:38.20ms
step:841/2110 train_time:32146ms step_avg:38.22ms
step:842/2110 train_time:32204ms step_avg:38.25ms
step:843/2110 train_time:32265ms step_avg:38.27ms
step:844/2110 train_time:32323ms step_avg:38.30ms
step:845/2110 train_time:32384ms step_avg:38.32ms
step:846/2110 train_time:32443ms step_avg:38.35ms
step:847/2110 train_time:32504ms step_avg:38.37ms
step:848/2110 train_time:32563ms step_avg:38.40ms
step:849/2110 train_time:32624ms step_avg:38.43ms
step:850/2110 train_time:32683ms step_avg:38.45ms
step:851/2110 train_time:32746ms step_avg:38.48ms
step:852/2110 train_time:32805ms step_avg:38.50ms
step:853/2110 train_time:32866ms step_avg:38.53ms
step:854/2110 train_time:32925ms step_avg:38.55ms
step:855/2110 train_time:32986ms step_avg:38.58ms
step:856/2110 train_time:33044ms step_avg:38.60ms
step:857/2110 train_time:33105ms step_avg:38.63ms
step:858/2110 train_time:33164ms step_avg:38.65ms
step:859/2110 train_time:33225ms step_avg:38.68ms
step:860/2110 train_time:33284ms step_avg:38.70ms
step:861/2110 train_time:33344ms step_avg:38.73ms
step:862/2110 train_time:33403ms step_avg:38.75ms
step:863/2110 train_time:33464ms step_avg:38.78ms
step:864/2110 train_time:33523ms step_avg:38.80ms
step:865/2110 train_time:33584ms step_avg:38.83ms
step:866/2110 train_time:33644ms step_avg:38.85ms
step:867/2110 train_time:33705ms step_avg:38.88ms
step:868/2110 train_time:33764ms step_avg:38.90ms
step:869/2110 train_time:33825ms step_avg:38.92ms
step:870/2110 train_time:33884ms step_avg:38.95ms
step:871/2110 train_time:33945ms step_avg:38.97ms
step:872/2110 train_time:34003ms step_avg:38.99ms
step:873/2110 train_time:34064ms step_avg:39.02ms
step:874/2110 train_time:34124ms step_avg:39.04ms
step:875/2110 train_time:34185ms step_avg:39.07ms
step:876/2110 train_time:34244ms step_avg:39.09ms
step:877/2110 train_time:34304ms step_avg:39.12ms
step:878/2110 train_time:34363ms step_avg:39.14ms
step:879/2110 train_time:34424ms step_avg:39.16ms
step:880/2110 train_time:34483ms step_avg:39.18ms
step:881/2110 train_time:34545ms step_avg:39.21ms
step:882/2110 train_time:34604ms step_avg:39.23ms
step:883/2110 train_time:34665ms step_avg:39.26ms
step:884/2110 train_time:34724ms step_avg:39.28ms
step:885/2110 train_time:34785ms step_avg:39.31ms
step:886/2110 train_time:34844ms step_avg:39.33ms
step:887/2110 train_time:34906ms step_avg:39.35ms
step:888/2110 train_time:34964ms step_avg:39.37ms
step:889/2110 train_time:35026ms step_avg:39.40ms
step:890/2110 train_time:35084ms step_avg:39.42ms
step:891/2110 train_time:35145ms step_avg:39.44ms
step:892/2110 train_time:35204ms step_avg:39.47ms
step:893/2110 train_time:35265ms step_avg:39.49ms
step:894/2110 train_time:35324ms step_avg:39.51ms
step:895/2110 train_time:35385ms step_avg:39.54ms
step:896/2110 train_time:35444ms step_avg:39.56ms
step:897/2110 train_time:35504ms step_avg:39.58ms
step:898/2110 train_time:35563ms step_avg:39.60ms
step:899/2110 train_time:35625ms step_avg:39.63ms
step:900/2110 train_time:35683ms step_avg:39.65ms
step:901/2110 train_time:35745ms step_avg:39.67ms
step:902/2110 train_time:35804ms step_avg:39.69ms
step:903/2110 train_time:35865ms step_avg:39.72ms
step:904/2110 train_time:35924ms step_avg:39.74ms
step:905/2110 train_time:35985ms step_avg:39.76ms
step:906/2110 train_time:36044ms step_avg:39.78ms
step:907/2110 train_time:36105ms step_avg:39.81ms
step:908/2110 train_time:36164ms step_avg:39.83ms
step:909/2110 train_time:36225ms step_avg:39.85ms
step:910/2110 train_time:36284ms step_avg:39.87ms
step:911/2110 train_time:36344ms step_avg:39.90ms
step:912/2110 train_time:36404ms step_avg:39.92ms
step:913/2110 train_time:36465ms step_avg:39.94ms
step:914/2110 train_time:36523ms step_avg:39.96ms
step:915/2110 train_time:36584ms step_avg:39.98ms
step:916/2110 train_time:36643ms step_avg:40.00ms
step:917/2110 train_time:36705ms step_avg:40.03ms
step:918/2110 train_time:36764ms step_avg:40.05ms
step:919/2110 train_time:36826ms step_avg:40.07ms
step:920/2110 train_time:36884ms step_avg:40.09ms
step:921/2110 train_time:36946ms step_avg:40.12ms
step:922/2110 train_time:37005ms step_avg:40.14ms
step:923/2110 train_time:37066ms step_avg:40.16ms
step:924/2110 train_time:37125ms step_avg:40.18ms
step:925/2110 train_time:37185ms step_avg:40.20ms
step:926/2110 train_time:37244ms step_avg:40.22ms
step:927/2110 train_time:37305ms step_avg:40.24ms
step:928/2110 train_time:37364ms step_avg:40.26ms
step:929/2110 train_time:37425ms step_avg:40.29ms
step:930/2110 train_time:37484ms step_avg:40.30ms
step:931/2110 train_time:37544ms step_avg:40.33ms
step:932/2110 train_time:37603ms step_avg:40.35ms
step:933/2110 train_time:37664ms step_avg:40.37ms
step:934/2110 train_time:37723ms step_avg:40.39ms
step:935/2110 train_time:37785ms step_avg:40.41ms
step:936/2110 train_time:37844ms step_avg:40.43ms
step:937/2110 train_time:37905ms step_avg:40.45ms
step:938/2110 train_time:37964ms step_avg:40.47ms
step:939/2110 train_time:38025ms step_avg:40.50ms
step:940/2110 train_time:38084ms step_avg:40.52ms
step:941/2110 train_time:38145ms step_avg:40.54ms
step:942/2110 train_time:38204ms step_avg:40.56ms
step:943/2110 train_time:38265ms step_avg:40.58ms
step:944/2110 train_time:38324ms step_avg:40.60ms
step:945/2110 train_time:38384ms step_avg:40.62ms
step:946/2110 train_time:38443ms step_avg:40.64ms
step:947/2110 train_time:38504ms step_avg:40.66ms
step:948/2110 train_time:38563ms step_avg:40.68ms
step:949/2110 train_time:38624ms step_avg:40.70ms
step:950/2110 train_time:38683ms step_avg:40.72ms
step:951/2110 train_time:38744ms step_avg:40.74ms
step:952/2110 train_time:38803ms step_avg:40.76ms
step:953/2110 train_time:38864ms step_avg:40.78ms
step:954/2110 train_time:38923ms step_avg:40.80ms
step:955/2110 train_time:38984ms step_avg:40.82ms
step:956/2110 train_time:39044ms step_avg:40.84ms
step:957/2110 train_time:39106ms step_avg:40.86ms
step:958/2110 train_time:39164ms step_avg:40.88ms
step:959/2110 train_time:39225ms step_avg:40.90ms
step:960/2110 train_time:39283ms step_avg:40.92ms
step:961/2110 train_time:39344ms step_avg:40.94ms
step:962/2110 train_time:39403ms step_avg:40.96ms
step:963/2110 train_time:39464ms step_avg:40.98ms
step:964/2110 train_time:39523ms step_avg:41.00ms
step:965/2110 train_time:39584ms step_avg:41.02ms
step:966/2110 train_time:39643ms step_avg:41.04ms
step:967/2110 train_time:39704ms step_avg:41.06ms
step:968/2110 train_time:39763ms step_avg:41.08ms
step:969/2110 train_time:39825ms step_avg:41.10ms
step:970/2110 train_time:39884ms step_avg:41.12ms
step:971/2110 train_time:39945ms step_avg:41.14ms
step:972/2110 train_time:40004ms step_avg:41.16ms
step:973/2110 train_time:40065ms step_avg:41.18ms
step:974/2110 train_time:40124ms step_avg:41.20ms
step:975/2110 train_time:40185ms step_avg:41.22ms
step:976/2110 train_time:40243ms step_avg:41.23ms
step:977/2110 train_time:40305ms step_avg:41.25ms
step:978/2110 train_time:40364ms step_avg:41.27ms
step:979/2110 train_time:40425ms step_avg:41.29ms
step:980/2110 train_time:40483ms step_avg:41.31ms
step:981/2110 train_time:40544ms step_avg:41.33ms
step:982/2110 train_time:40603ms step_avg:41.35ms
step:983/2110 train_time:40664ms step_avg:41.37ms
step:984/2110 train_time:40722ms step_avg:41.38ms
step:985/2110 train_time:40784ms step_avg:41.40ms
step:986/2110 train_time:40843ms step_avg:41.42ms
step:987/2110 train_time:40905ms step_avg:41.44ms
step:988/2110 train_time:40964ms step_avg:41.46ms
step:989/2110 train_time:41025ms step_avg:41.48ms
step:990/2110 train_time:41084ms step_avg:41.50ms
step:991/2110 train_time:41145ms step_avg:41.52ms
step:992/2110 train_time:41204ms step_avg:41.54ms
step:993/2110 train_time:41264ms step_avg:41.56ms
step:994/2110 train_time:41323ms step_avg:41.57ms
step:995/2110 train_time:41384ms step_avg:41.59ms
step:996/2110 train_time:41443ms step_avg:41.61ms
step:997/2110 train_time:41504ms step_avg:41.63ms
step:998/2110 train_time:41562ms step_avg:41.65ms
step:999/2110 train_time:41623ms step_avg:41.67ms
step:1000/2110 train_time:41683ms step_avg:41.68ms
step:1000/2110 val_loss:3.7012 train_time:41745ms step_avg:41.75ms
step:1001/2110 train_time:41766ms step_avg:41.72ms
step:1002/2110 train_time:41805ms step_avg:41.72ms
step:1003/2110 train_time:41869ms step_avg:41.74ms
step:1004/2110 train_time:41932ms step_avg:41.76ms
step:1005/2110 train_time:41993ms step_avg:41.78ms
step:1006/2110 train_time:42053ms step_avg:41.80ms
step:1007/2110 train_time:42114ms step_avg:41.82ms
step:1008/2110 train_time:42173ms step_avg:41.84ms
step:1009/2110 train_time:42234ms step_avg:41.86ms
step:1010/2110 train_time:42292ms step_avg:41.87ms
step:1011/2110 train_time:42354ms step_avg:41.89ms
step:1012/2110 train_time:42413ms step_avg:41.91ms
step:1013/2110 train_time:42473ms step_avg:41.93ms
step:1014/2110 train_time:42533ms step_avg:41.95ms
step:1015/2110 train_time:42594ms step_avg:41.96ms
step:1016/2110 train_time:42653ms step_avg:41.98ms
step:1017/2110 train_time:42715ms step_avg:42.00ms
step:1018/2110 train_time:42775ms step_avg:42.02ms
step:1019/2110 train_time:42838ms step_avg:42.04ms
step:1020/2110 train_time:42897ms step_avg:42.06ms
step:1021/2110 train_time:42959ms step_avg:42.08ms
step:1022/2110 train_time:43018ms step_avg:42.09ms
step:1023/2110 train_time:43079ms step_avg:42.11ms
step:1024/2110 train_time:43137ms step_avg:42.13ms
step:1025/2110 train_time:43198ms step_avg:42.14ms
step:1026/2110 train_time:43256ms step_avg:42.16ms
step:1027/2110 train_time:43317ms step_avg:42.18ms
step:1028/2110 train_time:43375ms step_avg:42.19ms
step:1029/2110 train_time:43436ms step_avg:42.21ms
step:1030/2110 train_time:43495ms step_avg:42.23ms
step:1031/2110 train_time:43555ms step_avg:42.25ms
step:1032/2110 train_time:43614ms step_avg:42.26ms
step:1033/2110 train_time:43675ms step_avg:42.28ms
step:1034/2110 train_time:43735ms step_avg:42.30ms
step:1035/2110 train_time:43797ms step_avg:42.32ms
step:1036/2110 train_time:43857ms step_avg:42.33ms
step:1037/2110 train_time:43918ms step_avg:42.35ms
step:1038/2110 train_time:43977ms step_avg:42.37ms
step:1039/2110 train_time:44038ms step_avg:42.39ms
step:1040/2110 train_time:44097ms step_avg:42.40ms
step:1041/2110 train_time:44158ms step_avg:42.42ms
step:1042/2110 train_time:44217ms step_avg:42.43ms
step:1043/2110 train_time:44277ms step_avg:42.45ms
step:1044/2110 train_time:44335ms step_avg:42.47ms
step:1045/2110 train_time:44396ms step_avg:42.48ms
step:1046/2110 train_time:44455ms step_avg:42.50ms
step:1047/2110 train_time:44516ms step_avg:42.52ms
step:1048/2110 train_time:44575ms step_avg:42.53ms
step:1049/2110 train_time:44635ms step_avg:42.55ms
step:1050/2110 train_time:44695ms step_avg:42.57ms
step:1051/2110 train_time:44757ms step_avg:42.59ms
step:1052/2110 train_time:44816ms step_avg:42.60ms
step:1053/2110 train_time:44878ms step_avg:42.62ms
step:1054/2110 train_time:44937ms step_avg:42.63ms
step:1055/2110 train_time:44998ms step_avg:42.65ms
step:1056/2110 train_time:45057ms step_avg:42.67ms
step:1057/2110 train_time:45119ms step_avg:42.69ms
step:1058/2110 train_time:45177ms step_avg:42.70ms
step:1059/2110 train_time:45238ms step_avg:42.72ms
step:1060/2110 train_time:45297ms step_avg:42.73ms
step:1061/2110 train_time:45357ms step_avg:42.75ms
step:1062/2110 train_time:45415ms step_avg:42.76ms
step:1063/2110 train_time:45476ms step_avg:42.78ms
step:1064/2110 train_time:45534ms step_avg:42.80ms
step:1065/2110 train_time:45595ms step_avg:42.81ms
step:1066/2110 train_time:45654ms step_avg:42.83ms
step:1067/2110 train_time:45716ms step_avg:42.85ms
step:1068/2110 train_time:45775ms step_avg:42.86ms
step:1069/2110 train_time:45837ms step_avg:42.88ms
step:1070/2110 train_time:45896ms step_avg:42.89ms
step:1071/2110 train_time:45958ms step_avg:42.91ms
step:1072/2110 train_time:46017ms step_avg:42.93ms
step:1073/2110 train_time:46078ms step_avg:42.94ms
step:1074/2110 train_time:46137ms step_avg:42.96ms
step:1075/2110 train_time:46198ms step_avg:42.97ms
step:1076/2110 train_time:46257ms step_avg:42.99ms
step:1077/2110 train_time:46318ms step_avg:43.01ms
step:1078/2110 train_time:46376ms step_avg:43.02ms
step:1079/2110 train_time:46436ms step_avg:43.04ms
step:1080/2110 train_time:46495ms step_avg:43.05ms
step:1081/2110 train_time:46557ms step_avg:43.07ms
step:1082/2110 train_time:46615ms step_avg:43.08ms
step:1083/2110 train_time:46677ms step_avg:43.10ms
step:1084/2110 train_time:46735ms step_avg:43.11ms
step:1085/2110 train_time:46797ms step_avg:43.13ms
step:1086/2110 train_time:46856ms step_avg:43.15ms
step:1087/2110 train_time:46918ms step_avg:43.16ms
step:1088/2110 train_time:46977ms step_avg:43.18ms
step:1089/2110 train_time:47038ms step_avg:43.19ms
step:1090/2110 train_time:47097ms step_avg:43.21ms
step:1091/2110 train_time:47158ms step_avg:43.22ms
step:1092/2110 train_time:47216ms step_avg:43.24ms
step:1093/2110 train_time:47277ms step_avg:43.25ms
step:1094/2110 train_time:47336ms step_avg:43.27ms
step:1095/2110 train_time:47396ms step_avg:43.28ms
step:1096/2110 train_time:47455ms step_avg:43.30ms
step:1097/2110 train_time:47516ms step_avg:43.31ms
step:1098/2110 train_time:47575ms step_avg:43.33ms
step:1099/2110 train_time:47637ms step_avg:43.35ms
step:1100/2110 train_time:47696ms step_avg:43.36ms
step:1101/2110 train_time:47756ms step_avg:43.38ms
step:1102/2110 train_time:47816ms step_avg:43.39ms
step:1103/2110 train_time:47877ms step_avg:43.41ms
step:1104/2110 train_time:47935ms step_avg:43.42ms
step:1105/2110 train_time:47997ms step_avg:43.44ms
step:1106/2110 train_time:48056ms step_avg:43.45ms
step:1107/2110 train_time:48117ms step_avg:43.47ms
step:1108/2110 train_time:48176ms step_avg:43.48ms
step:1109/2110 train_time:48237ms step_avg:43.50ms
step:1110/2110 train_time:48295ms step_avg:43.51ms
step:1111/2110 train_time:48356ms step_avg:43.52ms
step:1112/2110 train_time:48415ms step_avg:43.54ms
step:1113/2110 train_time:48475ms step_avg:43.55ms
step:1114/2110 train_time:48534ms step_avg:43.57ms
step:1115/2110 train_time:48595ms step_avg:43.58ms
step:1116/2110 train_time:48655ms step_avg:43.60ms
step:1117/2110 train_time:48716ms step_avg:43.61ms
step:1118/2110 train_time:48775ms step_avg:43.63ms
step:1119/2110 train_time:48837ms step_avg:43.64ms
step:1120/2110 train_time:48896ms step_avg:43.66ms
step:1121/2110 train_time:48958ms step_avg:43.67ms
step:1122/2110 train_time:49016ms step_avg:43.69ms
step:1123/2110 train_time:49078ms step_avg:43.70ms
step:1124/2110 train_time:49136ms step_avg:43.72ms
step:1125/2110 train_time:49197ms step_avg:43.73ms
step:1126/2110 train_time:49256ms step_avg:43.74ms
step:1127/2110 train_time:49316ms step_avg:43.76ms
step:1128/2110 train_time:49375ms step_avg:43.77ms
step:1129/2110 train_time:49435ms step_avg:43.79ms
step:1130/2110 train_time:49495ms step_avg:43.80ms
step:1131/2110 train_time:49556ms step_avg:43.82ms
step:1132/2110 train_time:49615ms step_avg:43.83ms
step:1133/2110 train_time:49676ms step_avg:43.84ms
step:1134/2110 train_time:49735ms step_avg:43.86ms
step:1135/2110 train_time:49796ms step_avg:43.87ms
step:1136/2110 train_time:49856ms step_avg:43.89ms
step:1137/2110 train_time:49918ms step_avg:43.90ms
step:1138/2110 train_time:49977ms step_avg:43.92ms
step:1139/2110 train_time:50038ms step_avg:43.93ms
step:1140/2110 train_time:50096ms step_avg:43.94ms
step:1141/2110 train_time:50157ms step_avg:43.96ms
step:1142/2110 train_time:50216ms step_avg:43.97ms
step:1143/2110 train_time:50277ms step_avg:43.99ms
step:1144/2110 train_time:50335ms step_avg:44.00ms
step:1145/2110 train_time:50396ms step_avg:44.01ms
step:1146/2110 train_time:50455ms step_avg:44.03ms
step:1147/2110 train_time:50517ms step_avg:44.04ms
step:1148/2110 train_time:50575ms step_avg:44.06ms
step:1149/2110 train_time:50636ms step_avg:44.07ms
step:1150/2110 train_time:50695ms step_avg:44.08ms
step:1151/2110 train_time:50756ms step_avg:44.10ms
step:1152/2110 train_time:50815ms step_avg:44.11ms
step:1153/2110 train_time:50876ms step_avg:44.13ms
step:1154/2110 train_time:50936ms step_avg:44.14ms
step:1155/2110 train_time:50997ms step_avg:44.15ms
step:1156/2110 train_time:51056ms step_avg:44.17ms
step:1157/2110 train_time:51117ms step_avg:44.18ms
step:1158/2110 train_time:51176ms step_avg:44.19ms
step:1159/2110 train_time:51237ms step_avg:44.21ms
step:1160/2110 train_time:51296ms step_avg:44.22ms
step:1161/2110 train_time:51356ms step_avg:44.23ms
step:1162/2110 train_time:51415ms step_avg:44.25ms
step:1163/2110 train_time:51476ms step_avg:44.26ms
step:1164/2110 train_time:51535ms step_avg:44.27ms
step:1165/2110 train_time:51596ms step_avg:44.29ms
step:1166/2110 train_time:51655ms step_avg:44.30ms
step:1167/2110 train_time:51716ms step_avg:44.32ms
step:1168/2110 train_time:51775ms step_avg:44.33ms
step:1169/2110 train_time:51836ms step_avg:44.34ms
step:1170/2110 train_time:51894ms step_avg:44.35ms
step:1171/2110 train_time:51956ms step_avg:44.37ms
step:1172/2110 train_time:52015ms step_avg:44.38ms
step:1173/2110 train_time:52076ms step_avg:44.40ms
step:1174/2110 train_time:52135ms step_avg:44.41ms
step:1175/2110 train_time:52196ms step_avg:44.42ms
step:1176/2110 train_time:52255ms step_avg:44.43ms
step:1177/2110 train_time:52316ms step_avg:44.45ms
step:1178/2110 train_time:52375ms step_avg:44.46ms
step:1179/2110 train_time:52435ms step_avg:44.47ms
step:1180/2110 train_time:52495ms step_avg:44.49ms
step:1181/2110 train_time:52556ms step_avg:44.50ms
step:1182/2110 train_time:52616ms step_avg:44.51ms
step:1183/2110 train_time:52676ms step_avg:44.53ms
step:1184/2110 train_time:52735ms step_avg:44.54ms
step:1185/2110 train_time:52796ms step_avg:44.55ms
step:1186/2110 train_time:52855ms step_avg:44.57ms
step:1187/2110 train_time:52916ms step_avg:44.58ms
step:1188/2110 train_time:52975ms step_avg:44.59ms
step:1189/2110 train_time:53037ms step_avg:44.61ms
step:1190/2110 train_time:53096ms step_avg:44.62ms
step:1191/2110 train_time:53158ms step_avg:44.63ms
step:1192/2110 train_time:53216ms step_avg:44.64ms
step:1193/2110 train_time:53277ms step_avg:44.66ms
step:1194/2110 train_time:53335ms step_avg:44.67ms
step:1195/2110 train_time:53396ms step_avg:44.68ms
step:1196/2110 train_time:53456ms step_avg:44.70ms
step:1197/2110 train_time:53517ms step_avg:44.71ms
step:1198/2110 train_time:53576ms step_avg:44.72ms
step:1199/2110 train_time:53636ms step_avg:44.73ms
step:1200/2110 train_time:53696ms step_avg:44.75ms
step:1201/2110 train_time:53757ms step_avg:44.76ms
step:1202/2110 train_time:53816ms step_avg:44.77ms
step:1203/2110 train_time:53877ms step_avg:44.79ms
step:1204/2110 train_time:53935ms step_avg:44.80ms
step:1205/2110 train_time:53997ms step_avg:44.81ms
step:1206/2110 train_time:54057ms step_avg:44.82ms
step:1207/2110 train_time:54118ms step_avg:44.84ms
step:1208/2110 train_time:54176ms step_avg:44.85ms
step:1209/2110 train_time:54237ms step_avg:44.86ms
step:1210/2110 train_time:54296ms step_avg:44.87ms
step:1211/2110 train_time:54357ms step_avg:44.89ms
step:1212/2110 train_time:54415ms step_avg:44.90ms
step:1213/2110 train_time:54476ms step_avg:44.91ms
step:1214/2110 train_time:54535ms step_avg:44.92ms
step:1215/2110 train_time:54596ms step_avg:44.94ms
step:1216/2110 train_time:54656ms step_avg:44.95ms
step:1217/2110 train_time:54717ms step_avg:44.96ms
step:1218/2110 train_time:54775ms step_avg:44.97ms
step:1219/2110 train_time:54836ms step_avg:44.98ms
step:1220/2110 train_time:54895ms step_avg:45.00ms
step:1221/2110 train_time:54957ms step_avg:45.01ms
step:1222/2110 train_time:55016ms step_avg:45.02ms
step:1223/2110 train_time:55077ms step_avg:45.03ms
step:1224/2110 train_time:55135ms step_avg:45.05ms
step:1225/2110 train_time:55196ms step_avg:45.06ms
step:1226/2110 train_time:55255ms step_avg:45.07ms
step:1227/2110 train_time:55317ms step_avg:45.08ms
step:1228/2110 train_time:55376ms step_avg:45.09ms
step:1229/2110 train_time:55437ms step_avg:45.11ms
step:1230/2110 train_time:55496ms step_avg:45.12ms
step:1231/2110 train_time:55557ms step_avg:45.13ms
step:1232/2110 train_time:55616ms step_avg:45.14ms
step:1233/2110 train_time:55676ms step_avg:45.16ms
step:1234/2110 train_time:55735ms step_avg:45.17ms
step:1235/2110 train_time:55796ms step_avg:45.18ms
step:1236/2110 train_time:55855ms step_avg:45.19ms
step:1237/2110 train_time:55916ms step_avg:45.20ms
step:1238/2110 train_time:55976ms step_avg:45.21ms
step:1239/2110 train_time:56037ms step_avg:45.23ms
step:1240/2110 train_time:56096ms step_avg:45.24ms
step:1241/2110 train_time:56157ms step_avg:45.25ms
step:1242/2110 train_time:56216ms step_avg:45.26ms
step:1243/2110 train_time:56277ms step_avg:45.28ms
step:1244/2110 train_time:56336ms step_avg:45.29ms
step:1245/2110 train_time:56397ms step_avg:45.30ms
step:1246/2110 train_time:56456ms step_avg:45.31ms
step:1247/2110 train_time:56517ms step_avg:45.32ms
step:1248/2110 train_time:56575ms step_avg:45.33ms
step:1249/2110 train_time:56637ms step_avg:45.35ms
step:1250/2110 train_time:56696ms step_avg:45.36ms
step:1250/2110 val_loss:3.5805 train_time:56759ms step_avg:45.41ms
step:1251/2110 train_time:56779ms step_avg:45.39ms
step:1252/2110 train_time:56818ms step_avg:45.38ms
step:1253/2110 train_time:56883ms step_avg:45.40ms
step:1254/2110 train_time:56944ms step_avg:45.41ms
step:1255/2110 train_time:57006ms step_avg:45.42ms
step:1256/2110 train_time:57065ms step_avg:45.43ms
step:1257/2110 train_time:57125ms step_avg:45.45ms
step:1258/2110 train_time:57183ms step_avg:45.46ms
step:1259/2110 train_time:57243ms step_avg:45.47ms
step:1260/2110 train_time:57302ms step_avg:45.48ms
step:1261/2110 train_time:57363ms step_avg:45.49ms
step:1262/2110 train_time:57421ms step_avg:45.50ms
step:1263/2110 train_time:57482ms step_avg:45.51ms
step:1264/2110 train_time:57542ms step_avg:45.52ms
step:1265/2110 train_time:57602ms step_avg:45.54ms
step:1266/2110 train_time:57661ms step_avg:45.55ms
step:1267/2110 train_time:57724ms step_avg:45.56ms
step:1268/2110 train_time:57785ms step_avg:45.57ms
step:1269/2110 train_time:57848ms step_avg:45.59ms
step:1270/2110 train_time:57908ms step_avg:45.60ms
step:1271/2110 train_time:57969ms step_avg:45.61ms
step:1272/2110 train_time:58028ms step_avg:45.62ms
step:1273/2110 train_time:58089ms step_avg:45.63ms
step:1274/2110 train_time:58147ms step_avg:45.64ms
step:1275/2110 train_time:58207ms step_avg:45.65ms
step:1276/2110 train_time:58266ms step_avg:45.66ms
step:1277/2110 train_time:58326ms step_avg:45.67ms
step:1278/2110 train_time:58385ms step_avg:45.68ms
step:1279/2110 train_time:58445ms step_avg:45.70ms
step:1280/2110 train_time:58505ms step_avg:45.71ms
step:1281/2110 train_time:58565ms step_avg:45.72ms
step:1282/2110 train_time:58625ms step_avg:45.73ms
step:1283/2110 train_time:58686ms step_avg:45.74ms
step:1284/2110 train_time:58746ms step_avg:45.75ms
step:1285/2110 train_time:58808ms step_avg:45.77ms
step:1286/2110 train_time:58868ms step_avg:45.78ms
step:1287/2110 train_time:58929ms step_avg:45.79ms
step:1288/2110 train_time:58987ms step_avg:45.80ms
step:1289/2110 train_time:59048ms step_avg:45.81ms
step:1290/2110 train_time:59107ms step_avg:45.82ms
step:1291/2110 train_time:59168ms step_avg:45.83ms
step:1292/2110 train_time:59227ms step_avg:45.84ms
step:1293/2110 train_time:59287ms step_avg:45.85ms
step:1294/2110 train_time:59346ms step_avg:45.86ms
step:1295/2110 train_time:59406ms step_avg:45.87ms
step:1296/2110 train_time:59465ms step_avg:45.88ms
step:1297/2110 train_time:59526ms step_avg:45.90ms
step:1298/2110 train_time:59585ms step_avg:45.91ms
step:1299/2110 train_time:59647ms step_avg:45.92ms
step:1300/2110 train_time:59707ms step_avg:45.93ms
step:1301/2110 train_time:59768ms step_avg:45.94ms
step:1302/2110 train_time:59827ms step_avg:45.95ms
step:1303/2110 train_time:59888ms step_avg:45.96ms
step:1304/2110 train_time:59948ms step_avg:45.97ms
step:1305/2110 train_time:60009ms step_avg:45.98ms
step:1306/2110 train_time:60068ms step_avg:45.99ms
step:1307/2110 train_time:60128ms step_avg:46.00ms
step:1308/2110 train_time:60187ms step_avg:46.01ms
step:1309/2110 train_time:60248ms step_avg:46.03ms
step:1310/2110 train_time:60306ms step_avg:46.04ms
step:1311/2110 train_time:60366ms step_avg:46.05ms
step:1312/2110 train_time:60425ms step_avg:46.06ms
step:1313/2110 train_time:60485ms step_avg:46.07ms
step:1314/2110 train_time:60544ms step_avg:46.08ms
step:1315/2110 train_time:60605ms step_avg:46.09ms
step:1316/2110 train_time:60665ms step_avg:46.10ms
step:1317/2110 train_time:60726ms step_avg:46.11ms
step:1318/2110 train_time:60786ms step_avg:46.12ms
step:1319/2110 train_time:60847ms step_avg:46.13ms
step:1320/2110 train_time:60906ms step_avg:46.14ms
step:1321/2110 train_time:60968ms step_avg:46.15ms
step:1322/2110 train_time:61027ms step_avg:46.16ms
step:1323/2110 train_time:61088ms step_avg:46.17ms
step:1324/2110 train_time:61146ms step_avg:46.18ms
step:1325/2110 train_time:61208ms step_avg:46.19ms
step:1326/2110 train_time:61266ms step_avg:46.20ms
step:1327/2110 train_time:61326ms step_avg:46.21ms
step:1328/2110 train_time:61385ms step_avg:46.22ms
step:1329/2110 train_time:61446ms step_avg:46.23ms
step:1330/2110 train_time:61505ms step_avg:46.24ms
step:1331/2110 train_time:61565ms step_avg:46.25ms
step:1332/2110 train_time:61625ms step_avg:46.26ms
step:1333/2110 train_time:61686ms step_avg:46.28ms
step:1334/2110 train_time:61746ms step_avg:46.29ms
step:1335/2110 train_time:61807ms step_avg:46.30ms
step:1336/2110 train_time:61866ms step_avg:46.31ms
step:1337/2110 train_time:61928ms step_avg:46.32ms
step:1338/2110 train_time:61987ms step_avg:46.33ms
step:1339/2110 train_time:62048ms step_avg:46.34ms
step:1340/2110 train_time:62107ms step_avg:46.35ms
step:1341/2110 train_time:62168ms step_avg:46.36ms
step:1342/2110 train_time:62227ms step_avg:46.37ms
step:1343/2110 train_time:62287ms step_avg:46.38ms
step:1344/2110 train_time:62345ms step_avg:46.39ms
step:1345/2110 train_time:62406ms step_avg:46.40ms
step:1346/2110 train_time:62465ms step_avg:46.41ms
step:1347/2110 train_time:62527ms step_avg:46.42ms
step:1348/2110 train_time:62585ms step_avg:46.43ms
step:1349/2110 train_time:62646ms step_avg:46.44ms
step:1350/2110 train_time:62706ms step_avg:46.45ms
step:1351/2110 train_time:62767ms step_avg:46.46ms
step:1352/2110 train_time:62826ms step_avg:46.47ms
step:1353/2110 train_time:62887ms step_avg:46.48ms
step:1354/2110 train_time:62946ms step_avg:46.49ms
step:1355/2110 train_time:63008ms step_avg:46.50ms
step:1356/2110 train_time:63067ms step_avg:46.51ms
step:1357/2110 train_time:63129ms step_avg:46.52ms
step:1358/2110 train_time:63187ms step_avg:46.53ms
step:1359/2110 train_time:63248ms step_avg:46.54ms
step:1360/2110 train_time:63307ms step_avg:46.55ms
step:1361/2110 train_time:63368ms step_avg:46.56ms
step:1362/2110 train_time:63426ms step_avg:46.57ms
step:1363/2110 train_time:63486ms step_avg:46.58ms
step:1364/2110 train_time:63545ms step_avg:46.59ms
step:1365/2110 train_time:63607ms step_avg:46.60ms
step:1366/2110 train_time:63666ms step_avg:46.61ms
step:1367/2110 train_time:63726ms step_avg:46.62ms
step:1368/2110 train_time:63786ms step_avg:46.63ms
step:1369/2110 train_time:63847ms step_avg:46.64ms
step:1370/2110 train_time:63907ms step_avg:46.65ms
step:1371/2110 train_time:63969ms step_avg:46.66ms
step:1372/2110 train_time:64027ms step_avg:46.67ms
step:1373/2110 train_time:64088ms step_avg:46.68ms
step:1374/2110 train_time:64146ms step_avg:46.69ms
step:1375/2110 train_time:64207ms step_avg:46.70ms
step:1376/2110 train_time:64266ms step_avg:46.71ms
step:1377/2110 train_time:64327ms step_avg:46.72ms
step:1378/2110 train_time:64386ms step_avg:46.72ms
step:1379/2110 train_time:64446ms step_avg:46.73ms
step:1380/2110 train_time:64506ms step_avg:46.74ms
step:1381/2110 train_time:64566ms step_avg:46.75ms
step:1382/2110 train_time:64654ms step_avg:46.78ms
step:1383/2110 train_time:64743ms step_avg:46.81ms
step:1384/2110 train_time:64831ms step_avg:46.84ms
step:1385/2110 train_time:64921ms step_avg:46.87ms
step:1386/2110 train_time:65009ms step_avg:46.90ms
step:1387/2110 train_time:65100ms step_avg:46.94ms
step:1388/2110 train_time:65186ms step_avg:46.96ms
step:1389/2110 train_time:65274ms step_avg:46.99ms
step:1390/2110 train_time:65361ms step_avg:47.02ms
step:1391/2110 train_time:65449ms step_avg:47.05ms
step:1392/2110 train_time:65536ms step_avg:47.08ms
step:1393/2110 train_time:65624ms step_avg:47.11ms
step:1394/2110 train_time:65711ms step_avg:47.14ms
step:1395/2110 train_time:65800ms step_avg:47.17ms
step:1396/2110 train_time:65887ms step_avg:47.20ms
step:1397/2110 train_time:65976ms step_avg:47.23ms
step:1398/2110 train_time:66064ms step_avg:47.26ms
step:1399/2110 train_time:66153ms step_avg:47.29ms
step:1400/2110 train_time:66241ms step_avg:47.31ms
step:1401/2110 train_time:66329ms step_avg:47.34ms
step:1402/2110 train_time:66416ms step_avg:47.37ms
step:1403/2110 train_time:66504ms step_avg:47.40ms
step:1404/2110 train_time:66591ms step_avg:47.43ms
step:1405/2110 train_time:66680ms step_avg:47.46ms
step:1406/2110 train_time:66766ms step_avg:47.49ms
step:1407/2110 train_time:66855ms step_avg:47.52ms
step:1408/2110 train_time:66942ms step_avg:47.54ms
step:1409/2110 train_time:67032ms step_avg:47.57ms
step:1410/2110 train_time:67119ms step_avg:47.60ms
step:1411/2110 train_time:67209ms step_avg:47.63ms
step:1412/2110 train_time:67296ms step_avg:47.66ms
step:1413/2110 train_time:67385ms step_avg:47.69ms
step:1414/2110 train_time:67472ms step_avg:47.72ms
step:1415/2110 train_time:67561ms step_avg:47.75ms
step:1416/2110 train_time:67647ms step_avg:47.77ms
step:1417/2110 train_time:67736ms step_avg:47.80ms
step:1418/2110 train_time:67822ms step_avg:47.83ms
step:1419/2110 train_time:67911ms step_avg:47.86ms
step:1420/2110 train_time:67999ms step_avg:47.89ms
step:1421/2110 train_time:68087ms step_avg:47.91ms
step:1422/2110 train_time:68174ms step_avg:47.94ms
step:1423/2110 train_time:68262ms step_avg:47.97ms
step:1424/2110 train_time:68350ms step_avg:48.00ms
step:1425/2110 train_time:68439ms step_avg:48.03ms
step:1426/2110 train_time:68526ms step_avg:48.05ms
step:1427/2110 train_time:68616ms step_avg:48.08ms
step:1428/2110 train_time:68702ms step_avg:48.11ms
step:1429/2110 train_time:68791ms step_avg:48.14ms
step:1430/2110 train_time:68878ms step_avg:48.17ms
step:1431/2110 train_time:68967ms step_avg:48.20ms
step:1432/2110 train_time:69054ms step_avg:48.22ms
step:1433/2110 train_time:69143ms step_avg:48.25ms
step:1434/2110 train_time:69230ms step_avg:48.28ms
step:1435/2110 train_time:69319ms step_avg:48.31ms
step:1436/2110 train_time:69407ms step_avg:48.33ms
step:1437/2110 train_time:69497ms step_avg:48.36ms
step:1438/2110 train_time:69585ms step_avg:48.39ms
step:1439/2110 train_time:69674ms step_avg:48.42ms
step:1440/2110 train_time:69760ms step_avg:48.44ms
step:1441/2110 train_time:69849ms step_avg:48.47ms
step:1442/2110 train_time:69936ms step_avg:48.50ms
step:1443/2110 train_time:70024ms step_avg:48.53ms
step:1444/2110 train_time:70111ms step_avg:48.55ms
step:1445/2110 train_time:70201ms step_avg:48.58ms
step:1446/2110 train_time:70288ms step_avg:48.61ms
step:1447/2110 train_time:70377ms step_avg:48.64ms
step:1448/2110 train_time:70464ms step_avg:48.66ms
step:1449/2110 train_time:70553ms step_avg:48.69ms
step:1450/2110 train_time:70639ms step_avg:48.72ms
step:1451/2110 train_time:70728ms step_avg:48.74ms
step:1452/2110 train_time:70815ms step_avg:48.77ms
step:1453/2110 train_time:70904ms step_avg:48.80ms
step:1454/2110 train_time:70992ms step_avg:48.83ms
step:1455/2110 train_time:71081ms step_avg:48.85ms
step:1456/2110 train_time:71168ms step_avg:48.88ms
step:1457/2110 train_time:71257ms step_avg:48.91ms
step:1458/2110 train_time:71344ms step_avg:48.93ms
step:1459/2110 train_time:71434ms step_avg:48.96ms
step:1460/2110 train_time:71520ms step_avg:48.99ms
step:1461/2110 train_time:71609ms step_avg:49.01ms
step:1462/2110 train_time:71695ms step_avg:49.04ms
step:1463/2110 train_time:71784ms step_avg:49.07ms
step:1464/2110 train_time:71871ms step_avg:49.09ms
step:1465/2110 train_time:71961ms step_avg:49.12ms
step:1466/2110 train_time:72047ms step_avg:49.15ms
step:1467/2110 train_time:72136ms step_avg:49.17ms
step:1468/2110 train_time:72223ms step_avg:49.20ms
step:1469/2110 train_time:72311ms step_avg:49.22ms
step:1470/2110 train_time:72398ms step_avg:49.25ms
step:1471/2110 train_time:72487ms step_avg:49.28ms
step:1472/2110 train_time:72575ms step_avg:49.30ms
step:1473/2110 train_time:72663ms step_avg:49.33ms
step:1474/2110 train_time:72750ms step_avg:49.36ms
step:1475/2110 train_time:72838ms step_avg:49.38ms
step:1476/2110 train_time:72925ms step_avg:49.41ms
step:1477/2110 train_time:73014ms step_avg:49.43ms
step:1478/2110 train_time:73100ms step_avg:49.46ms
step:1479/2110 train_time:73188ms step_avg:49.49ms
step:1480/2110 train_time:73276ms step_avg:49.51ms
step:1481/2110 train_time:73364ms step_avg:49.54ms
step:1482/2110 train_time:73451ms step_avg:49.56ms
step:1483/2110 train_time:73540ms step_avg:49.59ms
step:1484/2110 train_time:73626ms step_avg:49.61ms
step:1485/2110 train_time:73715ms step_avg:49.64ms
step:1486/2110 train_time:73801ms step_avg:49.66ms
step:1487/2110 train_time:73890ms step_avg:49.69ms
step:1488/2110 train_time:73978ms step_avg:49.72ms
step:1489/2110 train_time:74067ms step_avg:49.74ms
step:1490/2110 train_time:74154ms step_avg:49.77ms
step:1491/2110 train_time:74243ms step_avg:49.79ms
step:1492/2110 train_time:74330ms step_avg:49.82ms
step:1493/2110 train_time:74419ms step_avg:49.84ms
step:1494/2110 train_time:74505ms step_avg:49.87ms
step:1495/2110 train_time:74595ms step_avg:49.90ms
step:1496/2110 train_time:74682ms step_avg:49.92ms
step:1497/2110 train_time:74771ms step_avg:49.95ms
step:1498/2110 train_time:74858ms step_avg:49.97ms
step:1499/2110 train_time:74947ms step_avg:50.00ms
step:1500/2110 train_time:75034ms step_avg:50.02ms
step:1500/2110 val_loss:3.4720 train_time:75124ms step_avg:50.08ms
step:1501/2110 train_time:75144ms step_avg:50.06ms
step:1502/2110 train_time:75213ms step_avg:50.08ms
step:1503/2110 train_time:75305ms step_avg:50.10ms
step:1504/2110 train_time:75392ms step_avg:50.13ms
step:1505/2110 train_time:75481ms step_avg:50.15ms
step:1506/2110 train_time:75567ms step_avg:50.18ms
step:1507/2110 train_time:75656ms step_avg:50.20ms
step:1508/2110 train_time:75742ms step_avg:50.23ms
step:1509/2110 train_time:75829ms step_avg:50.25ms
step:1510/2110 train_time:75916ms step_avg:50.28ms
step:1511/2110 train_time:76004ms step_avg:50.30ms
step:1512/2110 train_time:76091ms step_avg:50.32ms
step:1513/2110 train_time:76182ms step_avg:50.35ms
step:1514/2110 train_time:76272ms step_avg:50.38ms
step:1515/2110 train_time:76363ms step_avg:50.40ms
step:1516/2110 train_time:76449ms step_avg:50.43ms
step:1517/2110 train_time:76538ms step_avg:50.45ms
step:1518/2110 train_time:76624ms step_avg:50.48ms
step:1519/2110 train_time:76713ms step_avg:50.50ms
step:1520/2110 train_time:76799ms step_avg:50.53ms
step:1521/2110 train_time:76888ms step_avg:50.55ms
step:1522/2110 train_time:76974ms step_avg:50.57ms
step:1523/2110 train_time:77063ms step_avg:50.60ms
step:1524/2110 train_time:77151ms step_avg:50.62ms
step:1525/2110 train_time:77242ms step_avg:50.65ms
step:1526/2110 train_time:77330ms step_avg:50.68ms
step:1527/2110 train_time:77419ms step_avg:50.70ms
step:1528/2110 train_time:77506ms step_avg:50.72ms
step:1529/2110 train_time:77596ms step_avg:50.75ms
step:1530/2110 train_time:77683ms step_avg:50.77ms
step:1531/2110 train_time:77771ms step_avg:50.80ms
step:1532/2110 train_time:77857ms step_avg:50.82ms
step:1533/2110 train_time:77945ms step_avg:50.84ms
step:1534/2110 train_time:78033ms step_avg:50.87ms
step:1535/2110 train_time:78122ms step_avg:50.89ms
step:1536/2110 train_time:78210ms step_avg:50.92ms
step:1537/2110 train_time:78302ms step_avg:50.94ms
step:1538/2110 train_time:78388ms step_avg:50.97ms
step:1539/2110 train_time:78477ms step_avg:50.99ms
step:1540/2110 train_time:78564ms step_avg:51.02ms
step:1541/2110 train_time:78652ms step_avg:51.04ms
step:1542/2110 train_time:78739ms step_avg:51.06ms
step:1543/2110 train_time:78826ms step_avg:51.09ms
step:1544/2110 train_time:78913ms step_avg:51.11ms
step:1545/2110 train_time:79002ms step_avg:51.13ms
step:1546/2110 train_time:79088ms step_avg:51.16ms
step:1547/2110 train_time:79178ms step_avg:51.18ms
step:1548/2110 train_time:79266ms step_avg:51.21ms
step:1549/2110 train_time:79355ms step_avg:51.23ms
step:1550/2110 train_time:79443ms step_avg:51.25ms
step:1551/2110 train_time:79532ms step_avg:51.28ms
step:1552/2110 train_time:79619ms step_avg:51.30ms
step:1553/2110 train_time:79707ms step_avg:51.32ms
step:1554/2110 train_time:79794ms step_avg:51.35ms
step:1555/2110 train_time:79883ms step_avg:51.37ms
step:1556/2110 train_time:79969ms step_avg:51.39ms
step:1557/2110 train_time:80058ms step_avg:51.42ms
step:1558/2110 train_time:80145ms step_avg:51.44ms
step:1559/2110 train_time:80233ms step_avg:51.46ms
step:1560/2110 train_time:80321ms step_avg:51.49ms
step:1561/2110 train_time:80410ms step_avg:51.51ms
step:1562/2110 train_time:80498ms step_avg:51.53ms
step:1563/2110 train_time:80587ms step_avg:51.56ms
step:1564/2110 train_time:80673ms step_avg:51.58ms
step:1565/2110 train_time:80762ms step_avg:51.60ms
step:1566/2110 train_time:80848ms step_avg:51.63ms
step:1567/2110 train_time:80936ms step_avg:51.65ms
step:1568/2110 train_time:81023ms step_avg:51.67ms
step:1569/2110 train_time:81111ms step_avg:51.70ms
step:1570/2110 train_time:81199ms step_avg:51.72ms
step:1571/2110 train_time:81288ms step_avg:51.74ms
step:1572/2110 train_time:81376ms step_avg:51.77ms
step:1573/2110 train_time:81465ms step_avg:51.79ms
step:1574/2110 train_time:81552ms step_avg:51.81ms
step:1575/2110 train_time:81641ms step_avg:51.84ms
step:1576/2110 train_time:81728ms step_avg:51.86ms
step:1577/2110 train_time:81817ms step_avg:51.88ms
step:1578/2110 train_time:81904ms step_avg:51.90ms
step:1579/2110 train_time:81994ms step_avg:51.93ms
step:1580/2110 train_time:82081ms step_avg:51.95ms
step:1581/2110 train_time:82168ms step_avg:51.97ms
step:1582/2110 train_time:82256ms step_avg:51.99ms
step:1583/2110 train_time:82345ms step_avg:52.02ms
step:1584/2110 train_time:82432ms step_avg:52.04ms
step:1585/2110 train_time:82522ms step_avg:52.06ms
step:1586/2110 train_time:82610ms step_avg:52.09ms
step:1587/2110 train_time:82699ms step_avg:52.11ms
step:1588/2110 train_time:82786ms step_avg:52.13ms
step:1589/2110 train_time:82875ms step_avg:52.16ms
step:1590/2110 train_time:82962ms step_avg:52.18ms
step:1591/2110 train_time:83050ms step_avg:52.20ms
step:1592/2110 train_time:83138ms step_avg:52.22ms
step:1593/2110 train_time:83227ms step_avg:52.25ms
step:1594/2110 train_time:83314ms step_avg:52.27ms
step:1595/2110 train_time:83403ms step_avg:52.29ms
step:1596/2110 train_time:83490ms step_avg:52.31ms
step:1597/2110 train_time:83578ms step_avg:52.33ms
step:1598/2110 train_time:83665ms step_avg:52.36ms
step:1599/2110 train_time:83753ms step_avg:52.38ms
step:1600/2110 train_time:83841ms step_avg:52.40ms
step:1601/2110 train_time:83930ms step_avg:52.42ms
step:1602/2110 train_time:84017ms step_avg:52.45ms
step:1603/2110 train_time:84107ms step_avg:52.47ms
step:1604/2110 train_time:84193ms step_avg:52.49ms
step:1605/2110 train_time:84283ms step_avg:52.51ms
step:1606/2110 train_time:84369ms step_avg:52.53ms
step:1607/2110 train_time:84460ms step_avg:52.56ms
step:1608/2110 train_time:84546ms step_avg:52.58ms
step:1609/2110 train_time:84634ms step_avg:52.60ms
step:1610/2110 train_time:84721ms step_avg:52.62ms
step:1611/2110 train_time:84810ms step_avg:52.64ms
step:1612/2110 train_time:84897ms step_avg:52.67ms
step:1613/2110 train_time:84986ms step_avg:52.69ms
step:1614/2110 train_time:85073ms step_avg:52.71ms
step:1615/2110 train_time:85161ms step_avg:52.73ms
step:1616/2110 train_time:85248ms step_avg:52.75ms
step:1617/2110 train_time:85336ms step_avg:52.77ms
step:1618/2110 train_time:85424ms step_avg:52.80ms
step:1619/2110 train_time:85513ms step_avg:52.82ms
step:1620/2110 train_time:85600ms step_avg:52.84ms
step:1621/2110 train_time:85689ms step_avg:52.86ms
step:1622/2110 train_time:85776ms step_avg:52.88ms
step:1623/2110 train_time:85865ms step_avg:52.90ms
step:1624/2110 train_time:85951ms step_avg:52.93ms
step:1625/2110 train_time:86040ms step_avg:52.95ms
step:1626/2110 train_time:86127ms step_avg:52.97ms
step:1627/2110 train_time:86215ms step_avg:52.99ms
step:1628/2110 train_time:86303ms step_avg:53.01ms
step:1629/2110 train_time:86392ms step_avg:53.03ms
step:1630/2110 train_time:86479ms step_avg:53.05ms
step:1631/2110 train_time:86568ms step_avg:53.08ms
step:1632/2110 train_time:86655ms step_avg:53.10ms
step:1633/2110 train_time:86744ms step_avg:53.12ms
step:1634/2110 train_time:86832ms step_avg:53.14ms
step:1635/2110 train_time:86922ms step_avg:53.16ms
step:1636/2110 train_time:87008ms step_avg:53.18ms
step:1637/2110 train_time:87097ms step_avg:53.21ms
step:1638/2110 train_time:87184ms step_avg:53.23ms
step:1639/2110 train_time:87272ms step_avg:53.25ms
step:1640/2110 train_time:87359ms step_avg:53.27ms
step:1641/2110 train_time:87448ms step_avg:53.29ms
step:1642/2110 train_time:87536ms step_avg:53.31ms
step:1643/2110 train_time:87627ms step_avg:53.33ms
step:1644/2110 train_time:87713ms step_avg:53.35ms
step:1645/2110 train_time:87802ms step_avg:53.38ms
step:1646/2110 train_time:87890ms step_avg:53.40ms
step:1647/2110 train_time:87979ms step_avg:53.42ms
step:1648/2110 train_time:88066ms step_avg:53.44ms
step:1649/2110 train_time:88155ms step_avg:53.46ms
step:1650/2110 train_time:88242ms step_avg:53.48ms
step:1651/2110 train_time:88331ms step_avg:53.50ms
step:1652/2110 train_time:88417ms step_avg:53.52ms
step:1653/2110 train_time:88508ms step_avg:53.54ms
step:1654/2110 train_time:88596ms step_avg:53.56ms
step:1655/2110 train_time:88686ms step_avg:53.59ms
step:1656/2110 train_time:88773ms step_avg:53.61ms
step:1657/2110 train_time:88862ms step_avg:53.63ms
step:1658/2110 train_time:88950ms step_avg:53.65ms
step:1659/2110 train_time:89039ms step_avg:53.67ms
step:1660/2110 train_time:89126ms step_avg:53.69ms
step:1661/2110 train_time:89214ms step_avg:53.71ms
step:1662/2110 train_time:89302ms step_avg:53.73ms
step:1663/2110 train_time:89390ms step_avg:53.75ms
step:1664/2110 train_time:89477ms step_avg:53.77ms
step:1665/2110 train_time:89566ms step_avg:53.79ms
step:1666/2110 train_time:89653ms step_avg:53.81ms
step:1667/2110 train_time:89742ms step_avg:53.83ms
step:1668/2110 train_time:89829ms step_avg:53.85ms
step:1669/2110 train_time:89918ms step_avg:53.88ms
step:1670/2110 train_time:90005ms step_avg:53.90ms
step:1671/2110 train_time:90094ms step_avg:53.92ms
step:1672/2110 train_time:90181ms step_avg:53.94ms
step:1673/2110 train_time:90270ms step_avg:53.96ms
step:1674/2110 train_time:90357ms step_avg:53.98ms
step:1675/2110 train_time:90447ms step_avg:54.00ms
step:1676/2110 train_time:90533ms step_avg:54.02ms
step:1677/2110 train_time:90623ms step_avg:54.04ms
step:1678/2110 train_time:90710ms step_avg:54.06ms
step:1679/2110 train_time:90799ms step_avg:54.08ms
step:1680/2110 train_time:90886ms step_avg:54.10ms
step:1681/2110 train_time:90975ms step_avg:54.12ms
step:1682/2110 train_time:91062ms step_avg:54.14ms
step:1683/2110 train_time:91150ms step_avg:54.16ms
step:1684/2110 train_time:91237ms step_avg:54.18ms
step:1685/2110 train_time:91327ms step_avg:54.20ms
step:1686/2110 train_time:91414ms step_avg:54.22ms
step:1687/2110 train_time:91503ms step_avg:54.24ms
step:1688/2110 train_time:91590ms step_avg:54.26ms
step:1689/2110 train_time:91679ms step_avg:54.28ms
step:1690/2110 train_time:91766ms step_avg:54.30ms
step:1691/2110 train_time:91854ms step_avg:54.32ms
step:1692/2110 train_time:91941ms step_avg:54.34ms
step:1693/2110 train_time:92030ms step_avg:54.36ms
step:1694/2110 train_time:92117ms step_avg:54.38ms
step:1695/2110 train_time:92206ms step_avg:54.40ms
step:1696/2110 train_time:92293ms step_avg:54.42ms
step:1697/2110 train_time:92382ms step_avg:54.44ms
step:1698/2110 train_time:92469ms step_avg:54.46ms
step:1699/2110 train_time:92558ms step_avg:54.48ms
step:1700/2110 train_time:92645ms step_avg:54.50ms
step:1701/2110 train_time:92734ms step_avg:54.52ms
step:1702/2110 train_time:92822ms step_avg:54.54ms
step:1703/2110 train_time:92909ms step_avg:54.56ms
step:1704/2110 train_time:92996ms step_avg:54.58ms
step:1705/2110 train_time:93086ms step_avg:54.60ms
step:1706/2110 train_time:93174ms step_avg:54.62ms
step:1707/2110 train_time:93262ms step_avg:54.64ms
step:1708/2110 train_time:93349ms step_avg:54.65ms
step:1709/2110 train_time:93438ms step_avg:54.67ms
step:1710/2110 train_time:93526ms step_avg:54.69ms
step:1711/2110 train_time:93614ms step_avg:54.71ms
step:1712/2110 train_time:93701ms step_avg:54.73ms
step:1713/2110 train_time:93790ms step_avg:54.75ms
step:1714/2110 train_time:93876ms step_avg:54.77ms
step:1715/2110 train_time:93966ms step_avg:54.79ms
step:1716/2110 train_time:94053ms step_avg:54.81ms
step:1717/2110 train_time:94142ms step_avg:54.83ms
step:1718/2110 train_time:94229ms step_avg:54.85ms
step:1719/2110 train_time:94317ms step_avg:54.87ms
step:1720/2110 train_time:94404ms step_avg:54.89ms
step:1721/2110 train_time:94493ms step_avg:54.91ms
step:1722/2110 train_time:94580ms step_avg:54.92ms
step:1723/2110 train_time:94669ms step_avg:54.94ms
step:1724/2110 train_time:94756ms step_avg:54.96ms
step:1725/2110 train_time:94846ms step_avg:54.98ms
step:1726/2110 train_time:94934ms step_avg:55.00ms
step:1727/2110 train_time:95023ms step_avg:55.02ms
step:1728/2110 train_time:95109ms step_avg:55.04ms
step:1729/2110 train_time:95199ms step_avg:55.06ms
step:1730/2110 train_time:95287ms step_avg:55.08ms
step:1731/2110 train_time:95375ms step_avg:55.10ms
step:1732/2110 train_time:95462ms step_avg:55.12ms
step:1733/2110 train_time:95551ms step_avg:55.14ms
step:1734/2110 train_time:95638ms step_avg:55.15ms
step:1735/2110 train_time:95728ms step_avg:55.17ms
step:1736/2110 train_time:95815ms step_avg:55.19ms
step:1737/2110 train_time:95905ms step_avg:55.21ms
step:1738/2110 train_time:95993ms step_avg:55.23ms
step:1739/2110 train_time:96083ms step_avg:55.25ms
step:1740/2110 train_time:96169ms step_avg:55.27ms
step:1741/2110 train_time:96258ms step_avg:55.29ms
step:1742/2110 train_time:96346ms step_avg:55.31ms
step:1743/2110 train_time:96436ms step_avg:55.33ms
step:1744/2110 train_time:96523ms step_avg:55.35ms
step:1745/2110 train_time:96612ms step_avg:55.37ms
step:1746/2110 train_time:96699ms step_avg:55.38ms
step:1747/2110 train_time:96789ms step_avg:55.40ms
step:1748/2110 train_time:96877ms step_avg:55.42ms
step:1749/2110 train_time:96966ms step_avg:55.44ms
step:1750/2110 train_time:97053ms step_avg:55.46ms
step:1750/2110 val_loss:3.3759 train_time:97145ms step_avg:55.51ms
step:1751/2110 train_time:97165ms step_avg:55.49ms
step:1752/2110 train_time:97236ms step_avg:55.50ms
step:1753/2110 train_time:97329ms step_avg:55.52ms
step:1754/2110 train_time:97416ms step_avg:55.54ms
step:1755/2110 train_time:97504ms step_avg:55.56ms
step:1756/2110 train_time:97589ms step_avg:55.57ms
step:1757/2110 train_time:97677ms step_avg:55.59ms
step:1758/2110 train_time:97763ms step_avg:55.61ms
step:1759/2110 train_time:97852ms step_avg:55.63ms
step:1760/2110 train_time:97940ms step_avg:55.65ms
step:1761/2110 train_time:98028ms step_avg:55.67ms
step:1762/2110 train_time:98117ms step_avg:55.68ms
step:1763/2110 train_time:98209ms step_avg:55.71ms
step:1764/2110 train_time:98299ms step_avg:55.72ms
step:1765/2110 train_time:98389ms step_avg:55.74ms
step:1766/2110 train_time:98475ms step_avg:55.76ms
step:1767/2110 train_time:98563ms step_avg:55.78ms
step:1768/2110 train_time:98650ms step_avg:55.80ms
step:1769/2110 train_time:98737ms step_avg:55.82ms
step:1770/2110 train_time:98824ms step_avg:55.83ms
step:1771/2110 train_time:98912ms step_avg:55.85ms
step:1772/2110 train_time:99000ms step_avg:55.87ms
step:1773/2110 train_time:99089ms step_avg:55.89ms
step:1774/2110 train_time:99177ms step_avg:55.91ms
step:1775/2110 train_time:99269ms step_avg:55.93ms
step:1776/2110 train_time:99358ms step_avg:55.94ms
step:1777/2110 train_time:99447ms step_avg:55.96ms
step:1778/2110 train_time:99535ms step_avg:55.98ms
step:1779/2110 train_time:99624ms step_avg:56.00ms
step:1780/2110 train_time:99709ms step_avg:56.02ms
step:1781/2110 train_time:99798ms step_avg:56.03ms
step:1782/2110 train_time:99884ms step_avg:56.05ms
step:1783/2110 train_time:99973ms step_avg:56.07ms
step:1784/2110 train_time:100061ms step_avg:56.09ms
step:1785/2110 train_time:100150ms step_avg:56.11ms
step:1786/2110 train_time:100238ms step_avg:56.12ms
step:1787/2110 train_time:100329ms step_avg:56.14ms
step:1788/2110 train_time:100416ms step_avg:56.16ms
step:1789/2110 train_time:100504ms step_avg:56.18ms
step:1790/2110 train_time:100590ms step_avg:56.20ms
step:1791/2110 train_time:100679ms step_avg:56.21ms
step:1792/2110 train_time:100764ms step_avg:56.23ms
step:1793/2110 train_time:100853ms step_avg:56.25ms
step:1794/2110 train_time:100940ms step_avg:56.27ms
step:1795/2110 train_time:101029ms step_avg:56.28ms
step:1796/2110 train_time:101117ms step_avg:56.30ms
step:1797/2110 train_time:101207ms step_avg:56.32ms
step:1798/2110 train_time:101294ms step_avg:56.34ms
step:1799/2110 train_time:101383ms step_avg:56.36ms
step:1800/2110 train_time:101469ms step_avg:56.37ms
step:1801/2110 train_time:101557ms step_avg:56.39ms
step:1802/2110 train_time:101644ms step_avg:56.41ms
step:1803/2110 train_time:101732ms step_avg:56.42ms
step:1804/2110 train_time:101819ms step_avg:56.44ms
step:1805/2110 train_time:101908ms step_avg:56.46ms
step:1806/2110 train_time:101994ms step_avg:56.48ms
step:1807/2110 train_time:102084ms step_avg:56.49ms
step:1808/2110 train_time:102171ms step_avg:56.51ms
step:1809/2110 train_time:102261ms step_avg:56.53ms
step:1810/2110 train_time:102348ms step_avg:56.55ms
step:1811/2110 train_time:102438ms step_avg:56.56ms
step:1812/2110 train_time:102526ms step_avg:56.58ms
step:1813/2110 train_time:102613ms step_avg:56.60ms
step:1814/2110 train_time:102700ms step_avg:56.62ms
step:1815/2110 train_time:102788ms step_avg:56.63ms
step:1816/2110 train_time:102875ms step_avg:56.65ms
step:1817/2110 train_time:102964ms step_avg:56.67ms
step:1818/2110 train_time:103051ms step_avg:56.68ms
step:1819/2110 train_time:103141ms step_avg:56.70ms
step:1820/2110 train_time:103228ms step_avg:56.72ms
step:1821/2110 train_time:103317ms step_avg:56.74ms
step:1822/2110 train_time:103405ms step_avg:56.75ms
step:1823/2110 train_time:103493ms step_avg:56.77ms
step:1824/2110 train_time:103581ms step_avg:56.79ms
step:1825/2110 train_time:103669ms step_avg:56.80ms
step:1826/2110 train_time:103755ms step_avg:56.82ms
step:1827/2110 train_time:103844ms step_avg:56.84ms
step:1828/2110 train_time:103931ms step_avg:56.86ms
step:1829/2110 train_time:104020ms step_avg:56.87ms
step:1830/2110 train_time:104108ms step_avg:56.89ms
step:1831/2110 train_time:104196ms step_avg:56.91ms
step:1832/2110 train_time:104284ms step_avg:56.92ms
step:1833/2110 train_time:104371ms step_avg:56.94ms
step:1834/2110 train_time:104459ms step_avg:56.96ms
step:1835/2110 train_time:104548ms step_avg:56.97ms
step:1836/2110 train_time:104635ms step_avg:56.99ms
step:1837/2110 train_time:104724ms step_avg:57.01ms
step:1838/2110 train_time:104811ms step_avg:57.02ms
step:1839/2110 train_time:104900ms step_avg:57.04ms
step:1840/2110 train_time:104987ms step_avg:57.06ms
step:1841/2110 train_time:105077ms step_avg:57.08ms
step:1842/2110 train_time:105164ms step_avg:57.09ms
step:1843/2110 train_time:105253ms step_avg:57.11ms
step:1844/2110 train_time:105340ms step_avg:57.13ms
step:1845/2110 train_time:105429ms step_avg:57.14ms
step:1846/2110 train_time:105516ms step_avg:57.16ms
step:1847/2110 train_time:105604ms step_avg:57.18ms
step:1848/2110 train_time:105690ms step_avg:57.19ms
step:1849/2110 train_time:105779ms step_avg:57.21ms
step:1850/2110 train_time:105866ms step_avg:57.22ms
step:1851/2110 train_time:105954ms step_avg:57.24ms
step:1852/2110 train_time:106042ms step_avg:57.26ms
step:1853/2110 train_time:106131ms step_avg:57.28ms
step:1854/2110 train_time:106218ms step_avg:57.29ms
step:1855/2110 train_time:106307ms step_avg:57.31ms
step:1856/2110 train_time:106394ms step_avg:57.32ms
step:1857/2110 train_time:106484ms step_avg:57.34ms
step:1858/2110 train_time:106571ms step_avg:57.36ms
step:1859/2110 train_time:106659ms step_avg:57.37ms
step:1860/2110 train_time:106746ms step_avg:57.39ms
step:1861/2110 train_time:106834ms step_avg:57.41ms
step:1862/2110 train_time:106921ms step_avg:57.42ms
step:1863/2110 train_time:107010ms step_avg:57.44ms
step:1864/2110 train_time:107097ms step_avg:57.46ms
step:1865/2110 train_time:107186ms step_avg:57.47ms
step:1866/2110 train_time:107273ms step_avg:57.49ms
step:1867/2110 train_time:107363ms step_avg:57.51ms
step:1868/2110 train_time:107450ms step_avg:57.52ms
step:1869/2110 train_time:107538ms step_avg:57.54ms
step:1870/2110 train_time:107626ms step_avg:57.55ms
step:1871/2110 train_time:107714ms step_avg:57.57ms
step:1872/2110 train_time:107801ms step_avg:57.59ms
step:1873/2110 train_time:107889ms step_avg:57.60ms
step:1874/2110 train_time:107977ms step_avg:57.62ms
step:1875/2110 train_time:108067ms step_avg:57.64ms
step:1876/2110 train_time:108154ms step_avg:57.65ms
step:1877/2110 train_time:108244ms step_avg:57.67ms
step:1878/2110 train_time:108330ms step_avg:57.68ms
step:1879/2110 train_time:108419ms step_avg:57.70ms
step:1880/2110 train_time:108506ms step_avg:57.72ms
step:1881/2110 train_time:108595ms step_avg:57.73ms
step:1882/2110 train_time:108682ms step_avg:57.75ms
step:1883/2110 train_time:108770ms step_avg:57.76ms
step:1884/2110 train_time:108857ms step_avg:57.78ms
step:1885/2110 train_time:108947ms step_avg:57.80ms
step:1886/2110 train_time:109033ms step_avg:57.81ms
step:1887/2110 train_time:109123ms step_avg:57.83ms
step:1888/2110 train_time:109210ms step_avg:57.84ms
step:1889/2110 train_time:109299ms step_avg:57.86ms
step:1890/2110 train_time:109386ms step_avg:57.88ms
step:1891/2110 train_time:109475ms step_avg:57.89ms
step:1892/2110 train_time:109561ms step_avg:57.91ms
step:1893/2110 train_time:109650ms step_avg:57.92ms
step:1894/2110 train_time:109737ms step_avg:57.94ms
step:1895/2110 train_time:109826ms step_avg:57.96ms
step:1896/2110 train_time:109912ms step_avg:57.97ms
step:1897/2110 train_time:110001ms step_avg:57.99ms
step:1898/2110 train_time:110087ms step_avg:58.00ms
step:1899/2110 train_time:110176ms step_avg:58.02ms
step:1900/2110 train_time:110264ms step_avg:58.03ms
step:1901/2110 train_time:110353ms step_avg:58.05ms
step:1902/2110 train_time:110440ms step_avg:58.07ms
step:1903/2110 train_time:110529ms step_avg:58.08ms
step:1904/2110 train_time:110615ms step_avg:58.10ms
step:1905/2110 train_time:110704ms step_avg:58.11ms
step:1906/2110 train_time:110791ms step_avg:58.13ms
step:1907/2110 train_time:110880ms step_avg:58.14ms
step:1908/2110 train_time:110969ms step_avg:58.16ms
step:1909/2110 train_time:111058ms step_avg:58.18ms
step:1910/2110 train_time:111145ms step_avg:58.19ms
step:1911/2110 train_time:111233ms step_avg:58.21ms
step:1912/2110 train_time:111320ms step_avg:58.22ms
step:1913/2110 train_time:111409ms step_avg:58.24ms
step:1914/2110 train_time:111497ms step_avg:58.25ms
step:1915/2110 train_time:111586ms step_avg:58.27ms
step:1916/2110 train_time:111672ms step_avg:58.28ms
step:1917/2110 train_time:111762ms step_avg:58.30ms
step:1918/2110 train_time:111848ms step_avg:58.32ms
step:1919/2110 train_time:111937ms step_avg:58.33ms
step:1920/2110 train_time:112025ms step_avg:58.35ms
step:1921/2110 train_time:112113ms step_avg:58.36ms
step:1922/2110 train_time:112201ms step_avg:58.38ms
step:1923/2110 train_time:112289ms step_avg:58.39ms
step:1924/2110 train_time:112376ms step_avg:58.41ms
step:1925/2110 train_time:112465ms step_avg:58.42ms
step:1926/2110 train_time:112551ms step_avg:58.44ms
step:1927/2110 train_time:112640ms step_avg:58.45ms
step:1928/2110 train_time:112728ms step_avg:58.47ms
step:1929/2110 train_time:112816ms step_avg:58.48ms
step:1930/2110 train_time:112903ms step_avg:58.50ms
step:1931/2110 train_time:112992ms step_avg:58.51ms
step:1932/2110 train_time:113079ms step_avg:58.53ms
step:1933/2110 train_time:113169ms step_avg:58.55ms
step:1934/2110 train_time:113256ms step_avg:58.56ms
step:1935/2110 train_time:113345ms step_avg:58.58ms
step:1936/2110 train_time:113431ms step_avg:58.59ms
step:1937/2110 train_time:113521ms step_avg:58.61ms
step:1938/2110 train_time:113608ms step_avg:58.62ms
step:1939/2110 train_time:113696ms step_avg:58.64ms
step:1940/2110 train_time:113783ms step_avg:58.65ms
step:1941/2110 train_time:113871ms step_avg:58.67ms
step:1942/2110 train_time:113959ms step_avg:58.68ms
step:1943/2110 train_time:114048ms step_avg:58.70ms
step:1944/2110 train_time:114135ms step_avg:58.71ms
step:1945/2110 train_time:114224ms step_avg:58.73ms
step:1946/2110 train_time:114311ms step_avg:58.74ms
step:1947/2110 train_time:114399ms step_avg:58.76ms
step:1948/2110 train_time:114486ms step_avg:58.77ms
step:1949/2110 train_time:114575ms step_avg:58.79ms
step:1950/2110 train_time:114662ms step_avg:58.80ms
step:1951/2110 train_time:114750ms step_avg:58.82ms
step:1952/2110 train_time:114838ms step_avg:58.83ms
step:1953/2110 train_time:114929ms step_avg:58.85ms
step:1954/2110 train_time:115016ms step_avg:58.86ms
step:1955/2110 train_time:115106ms step_avg:58.88ms
step:1956/2110 train_time:115192ms step_avg:58.89ms
step:1957/2110 train_time:115281ms step_avg:58.91ms
step:1958/2110 train_time:115367ms step_avg:58.92ms
step:1959/2110 train_time:115456ms step_avg:58.94ms
step:1960/2110 train_time:115543ms step_avg:58.95ms
step:1961/2110 train_time:115631ms step_avg:58.97ms
step:1962/2110 train_time:115719ms step_avg:58.98ms
step:1963/2110 train_time:115808ms step_avg:59.00ms
step:1964/2110 train_time:115896ms step_avg:59.01ms
step:1965/2110 train_time:115984ms step_avg:59.03ms
step:1966/2110 train_time:116071ms step_avg:59.04ms
step:1967/2110 train_time:116161ms step_avg:59.05ms
step:1968/2110 train_time:116248ms step_avg:59.07ms
step:1969/2110 train_time:116337ms step_avg:59.08ms
step:1970/2110 train_time:116425ms step_avg:59.10ms
step:1971/2110 train_time:116514ms step_avg:59.11ms
step:1972/2110 train_time:116601ms step_avg:59.13ms
step:1973/2110 train_time:116689ms step_avg:59.14ms
step:1974/2110 train_time:116776ms step_avg:59.16ms
step:1975/2110 train_time:116866ms step_avg:59.17ms
step:1976/2110 train_time:116954ms step_avg:59.19ms
step:1977/2110 train_time:117043ms step_avg:59.20ms
step:1978/2110 train_time:117130ms step_avg:59.22ms
step:1979/2110 train_time:117220ms step_avg:59.23ms
step:1980/2110 train_time:117307ms step_avg:59.25ms
step:1981/2110 train_time:117397ms step_avg:59.26ms
step:1982/2110 train_time:117484ms step_avg:59.28ms
step:1983/2110 train_time:117572ms step_avg:59.29ms
step:1984/2110 train_time:117660ms step_avg:59.30ms
step:1985/2110 train_time:117748ms step_avg:59.32ms
step:1986/2110 train_time:117836ms step_avg:59.33ms
step:1987/2110 train_time:117925ms step_avg:59.35ms
step:1988/2110 train_time:118012ms step_avg:59.36ms
step:1989/2110 train_time:118101ms step_avg:59.38ms
step:1990/2110 train_time:118189ms step_avg:59.39ms
step:1991/2110 train_time:118276ms step_avg:59.41ms
step:1992/2110 train_time:118364ms step_avg:59.42ms
step:1993/2110 train_time:118453ms step_avg:59.43ms
step:1994/2110 train_time:118541ms step_avg:59.45ms
step:1995/2110 train_time:118630ms step_avg:59.46ms
step:1996/2110 train_time:118716ms step_avg:59.48ms
step:1997/2110 train_time:118806ms step_avg:59.49ms
step:1998/2110 train_time:118892ms step_avg:59.51ms
step:1999/2110 train_time:118981ms step_avg:59.52ms
step:2000/2110 train_time:119069ms step_avg:59.53ms
step:2000/2110 val_loss:3.3019 train_time:119159ms step_avg:59.58ms
step:2001/2110 train_time:119180ms step_avg:59.56ms
step:2002/2110 train_time:119248ms step_avg:59.56ms
step:2003/2110 train_time:119343ms step_avg:59.58ms
step:2004/2110 train_time:119430ms step_avg:59.60ms
step:2005/2110 train_time:119518ms step_avg:59.61ms
step:2006/2110 train_time:119604ms step_avg:59.62ms
step:2007/2110 train_time:119691ms step_avg:59.64ms
step:2008/2110 train_time:119778ms step_avg:59.65ms
step:2009/2110 train_time:119866ms step_avg:59.66ms
step:2010/2110 train_time:119952ms step_avg:59.68ms
step:2011/2110 train_time:120041ms step_avg:59.69ms
step:2012/2110 train_time:120130ms step_avg:59.71ms
step:2013/2110 train_time:120219ms step_avg:59.72ms
step:2014/2110 train_time:120308ms step_avg:59.74ms
step:2015/2110 train_time:120398ms step_avg:59.75ms
step:2016/2110 train_time:120485ms step_avg:59.76ms
step:2017/2110 train_time:120575ms step_avg:59.78ms
step:2018/2110 train_time:120662ms step_avg:59.79ms
step:2019/2110 train_time:120750ms step_avg:59.81ms
step:2020/2110 train_time:120836ms step_avg:59.82ms
step:2021/2110 train_time:120923ms step_avg:59.83ms
step:2022/2110 train_time:121010ms step_avg:59.85ms
step:2023/2110 train_time:121099ms step_avg:59.86ms
step:2024/2110 train_time:121188ms step_avg:59.88ms
step:2025/2110 train_time:121279ms step_avg:59.89ms
step:2026/2110 train_time:121368ms step_avg:59.91ms
step:2027/2110 train_time:121457ms step_avg:59.92ms
step:2028/2110 train_time:121546ms step_avg:59.93ms
step:2029/2110 train_time:121634ms step_avg:59.95ms
step:2030/2110 train_time:121721ms step_avg:59.96ms
step:2031/2110 train_time:121810ms step_avg:59.98ms
step:2032/2110 train_time:121896ms step_avg:59.99ms
step:2033/2110 train_time:121984ms step_avg:60.00ms
step:2034/2110 train_time:122071ms step_avg:60.02ms
step:2035/2110 train_time:122160ms step_avg:60.03ms
step:2036/2110 train_time:122248ms step_avg:60.04ms
step:2037/2110 train_time:122338ms step_avg:60.06ms
step:2038/2110 train_time:122426ms step_avg:60.07ms
step:2039/2110 train_time:122516ms step_avg:60.09ms
step:2040/2110 train_time:122602ms step_avg:60.10ms
step:2041/2110 train_time:122692ms step_avg:60.11ms
step:2042/2110 train_time:122779ms step_avg:60.13ms
step:2043/2110 train_time:122868ms step_avg:60.14ms
step:2044/2110 train_time:122954ms step_avg:60.15ms
step:2045/2110 train_time:123042ms step_avg:60.17ms
step:2046/2110 train_time:123129ms step_avg:60.18ms
step:2047/2110 train_time:123218ms step_avg:60.19ms
step:2048/2110 train_time:123305ms step_avg:60.21ms
step:2049/2110 train_time:123395ms step_avg:60.22ms
step:2050/2110 train_time:123483ms step_avg:60.24ms
step:2051/2110 train_time:123572ms step_avg:60.25ms
step:2052/2110 train_time:123660ms step_avg:60.26ms
step:2053/2110 train_time:123750ms step_avg:60.28ms
step:2054/2110 train_time:123836ms step_avg:60.29ms
step:2055/2110 train_time:123924ms step_avg:60.30ms
step:2056/2110 train_time:124011ms step_avg:60.32ms
step:2057/2110 train_time:124100ms step_avg:60.33ms
step:2058/2110 train_time:124187ms step_avg:60.34ms
step:2059/2110 train_time:124277ms step_avg:60.36ms
step:2060/2110 train_time:124365ms step_avg:60.37ms
step:2061/2110 train_time:124454ms step_avg:60.39ms
step:2062/2110 train_time:124541ms step_avg:60.40ms
step:2063/2110 train_time:124631ms step_avg:60.41ms
step:2064/2110 train_time:124718ms step_avg:60.43ms
step:2065/2110 train_time:124807ms step_avg:60.44ms
step:2066/2110 train_time:124893ms step_avg:60.45ms
step:2067/2110 train_time:124983ms step_avg:60.47ms
step:2068/2110 train_time:125071ms step_avg:60.48ms
step:2069/2110 train_time:125160ms step_avg:60.49ms
step:2070/2110 train_time:125248ms step_avg:60.51ms
step:2071/2110 train_time:125336ms step_avg:60.52ms
step:2072/2110 train_time:125424ms step_avg:60.53ms
step:2073/2110 train_time:125513ms step_avg:60.55ms
step:2074/2110 train_time:125600ms step_avg:60.56ms
step:2075/2110 train_time:125689ms step_avg:60.57ms
step:2076/2110 train_time:125776ms step_avg:60.59ms
step:2077/2110 train_time:125865ms step_avg:60.60ms
step:2078/2110 train_time:125953ms step_avg:60.61ms
step:2079/2110 train_time:126041ms step_avg:60.63ms
step:2080/2110 train_time:126128ms step_avg:60.64ms
step:2081/2110 train_time:126217ms step_avg:60.65ms
step:2082/2110 train_time:126305ms step_avg:60.67ms
step:2083/2110 train_time:126394ms step_avg:60.68ms
step:2084/2110 train_time:126481ms step_avg:60.69ms
step:2085/2110 train_time:126570ms step_avg:60.71ms
step:2086/2110 train_time:126657ms step_avg:60.72ms
step:2087/2110 train_time:126747ms step_avg:60.73ms
step:2088/2110 train_time:126834ms step_avg:60.74ms
step:2089/2110 train_time:126923ms step_avg:60.76ms
step:2090/2110 train_time:127011ms step_avg:60.77ms
step:2091/2110 train_time:127100ms step_avg:60.78ms
step:2092/2110 train_time:127188ms step_avg:60.80ms
step:2093/2110 train_time:127277ms step_avg:60.81ms
step:2094/2110 train_time:127364ms step_avg:60.82ms
step:2095/2110 train_time:127453ms step_avg:60.84ms
step:2096/2110 train_time:127541ms step_avg:60.85ms
step:2097/2110 train_time:127630ms step_avg:60.86ms
step:2098/2110 train_time:127717ms step_avg:60.88ms
step:2099/2110 train_time:127806ms step_avg:60.89ms
step:2100/2110 train_time:127893ms step_avg:60.90ms
step:2101/2110 train_time:127982ms step_avg:60.91ms
step:2102/2110 train_time:128070ms step_avg:60.93ms
step:2103/2110 train_time:128159ms step_avg:60.94ms
step:2104/2110 train_time:128247ms step_avg:60.95ms
step:2105/2110 train_time:128336ms step_avg:60.97ms
step:2106/2110 train_time:128424ms step_avg:60.98ms
step:2107/2110 train_time:128514ms step_avg:60.99ms
step:2108/2110 train_time:128601ms step_avg:61.01ms
step:2109/2110 train_time:128691ms step_avg:61.02ms
step:2110/2110 train_time:128778ms step_avg:61.03ms
step:2110/2110 val_loss:3.2769 train_time:128869ms step_avg:61.08ms
peak memory allocated: 30054 MiB reserved: 44936 MiB

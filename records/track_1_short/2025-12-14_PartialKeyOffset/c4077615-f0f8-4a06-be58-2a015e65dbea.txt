import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 14 19:53:01 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    5858MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0            123W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   24C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   23C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     35440      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     35441      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     35442      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     35443      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     35444      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     35445      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     35446      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     35447      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     35441      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     35442      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     35443      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     35444      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     35445      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     35446      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     35447      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:80ms step_avg:80.16ms
step:2/2110 train_time:104ms step_avg:52.10ms
step:3/2110 train_time:125ms step_avg:41.83ms
step:4/2110 train_time:158ms step_avg:39.38ms
step:5/2110 train_time:190ms step_avg:38.09ms
step:6/2110 train_time:405ms step_avg:67.49ms
step:7/2110 train_time:428ms step_avg:61.17ms
step:8/2110 train_time:461ms step_avg:57.59ms
step:9/2110 train_time:494ms step_avg:54.88ms
step:10/2110 train_time:526ms step_avg:52.62ms
step:11/2110 train_time:560ms step_avg:50.89ms
step:12/2110 train_time:593ms step_avg:49.40ms
step:13/2110 train_time:626ms step_avg:48.17ms
step:14/2110 train_time:659ms step_avg:47.07ms
step:15/2110 train_time:692ms step_avg:46.15ms
step:16/2110 train_time:725ms step_avg:45.30ms
step:17/2110 train_time:759ms step_avg:44.63ms
step:18/2110 train_time:791ms step_avg:43.96ms
step:19/2110 train_time:825ms step_avg:43.42ms
step:20/2110 train_time:857ms step_avg:42.87ms
step:21/2110 train_time:891ms step_avg:42.43ms
step:22/2110 train_time:924ms step_avg:41.99ms
step:23/2110 train_time:957ms step_avg:41.62ms
step:24/2110 train_time:990ms step_avg:41.25ms
step:25/2110 train_time:1024ms step_avg:40.95ms
step:26/2110 train_time:1056ms step_avg:40.63ms
step:27/2110 train_time:1090ms step_avg:40.36ms
step:28/2110 train_time:1122ms step_avg:40.09ms
step:29/2110 train_time:1156ms step_avg:39.86ms
step:30/2110 train_time:1189ms step_avg:39.62ms
step:31/2110 train_time:1222ms step_avg:39.43ms
step:32/2110 train_time:1255ms step_avg:39.23ms
step:33/2110 train_time:1289ms step_avg:39.06ms
step:34/2110 train_time:1322ms step_avg:38.88ms
step:35/2110 train_time:1356ms step_avg:38.75ms
step:36/2110 train_time:1390ms step_avg:38.60ms
step:37/2110 train_time:1425ms step_avg:38.51ms
step:38/2110 train_time:1458ms step_avg:38.36ms
step:39/2110 train_time:1492ms step_avg:38.26ms
step:40/2110 train_time:1525ms step_avg:38.12ms
step:41/2110 train_time:1560ms step_avg:38.04ms
step:42/2110 train_time:1592ms step_avg:37.91ms
step:43/2110 train_time:1626ms step_avg:37.82ms
step:44/2110 train_time:1659ms step_avg:37.71ms
step:45/2110 train_time:1693ms step_avg:37.62ms
step:46/2110 train_time:1726ms step_avg:37.52ms
step:47/2110 train_time:1760ms step_avg:37.44ms
step:48/2110 train_time:1792ms step_avg:37.34ms
step:49/2110 train_time:1826ms step_avg:37.26ms
step:50/2110 train_time:1858ms step_avg:37.17ms
step:51/2110 train_time:1892ms step_avg:37.10ms
step:52/2110 train_time:1924ms step_avg:37.01ms
step:53/2110 train_time:1958ms step_avg:36.95ms
step:54/2110 train_time:1991ms step_avg:36.87ms
step:55/2110 train_time:2025ms step_avg:36.82ms
step:56/2110 train_time:2057ms step_avg:36.74ms
step:57/2110 train_time:2091ms step_avg:36.68ms
step:58/2110 train_time:2124ms step_avg:36.61ms
step:59/2110 train_time:2157ms step_avg:36.56ms
step:60/2110 train_time:2189ms step_avg:36.49ms
step:61/2110 train_time:2224ms step_avg:36.45ms
step:62/2110 train_time:2256ms step_avg:36.39ms
step:63/2110 train_time:2290ms step_avg:36.35ms
step:64/2110 train_time:2323ms step_avg:36.29ms
step:65/2110 train_time:2357ms step_avg:36.27ms
step:66/2110 train_time:2390ms step_avg:36.21ms
step:67/2110 train_time:2424ms step_avg:36.18ms
step:68/2110 train_time:2457ms step_avg:36.13ms
step:69/2110 train_time:2491ms step_avg:36.10ms
step:70/2110 train_time:2523ms step_avg:36.05ms
step:71/2110 train_time:2557ms step_avg:36.02ms
step:72/2110 train_time:2590ms step_avg:35.97ms
step:73/2110 train_time:2624ms step_avg:35.95ms
step:74/2110 train_time:2657ms step_avg:35.90ms
step:75/2110 train_time:2691ms step_avg:35.88ms
step:76/2110 train_time:2724ms step_avg:35.84ms
step:77/2110 train_time:2757ms step_avg:35.81ms
step:78/2110 train_time:2790ms step_avg:35.77ms
step:79/2110 train_time:2824ms step_avg:35.75ms
step:80/2110 train_time:2857ms step_avg:35.71ms
step:81/2110 train_time:2891ms step_avg:35.69ms
step:82/2110 train_time:2923ms step_avg:35.65ms
step:83/2110 train_time:2957ms step_avg:35.62ms
step:84/2110 train_time:2989ms step_avg:35.59ms
step:85/2110 train_time:3023ms step_avg:35.57ms
step:86/2110 train_time:3056ms step_avg:35.54ms
step:87/2110 train_time:3090ms step_avg:35.52ms
step:88/2110 train_time:3123ms step_avg:35.49ms
step:89/2110 train_time:3156ms step_avg:35.46ms
step:90/2110 train_time:3189ms step_avg:35.43ms
step:91/2110 train_time:3223ms step_avg:35.41ms
step:92/2110 train_time:3255ms step_avg:35.38ms
step:93/2110 train_time:3289ms step_avg:35.37ms
step:94/2110 train_time:3322ms step_avg:35.34ms
step:95/2110 train_time:3355ms step_avg:35.32ms
step:96/2110 train_time:3388ms step_avg:35.29ms
step:97/2110 train_time:3422ms step_avg:35.28ms
step:98/2110 train_time:3455ms step_avg:35.25ms
step:99/2110 train_time:3489ms step_avg:35.24ms
step:100/2110 train_time:3521ms step_avg:35.21ms
step:101/2110 train_time:3555ms step_avg:35.20ms
step:102/2110 train_time:3587ms step_avg:35.17ms
step:103/2110 train_time:3622ms step_avg:35.16ms
step:104/2110 train_time:3655ms step_avg:35.14ms
step:105/2110 train_time:3688ms step_avg:35.13ms
step:106/2110 train_time:3721ms step_avg:35.10ms
step:107/2110 train_time:3754ms step_avg:35.09ms
step:108/2110 train_time:3787ms step_avg:35.06ms
step:109/2110 train_time:3821ms step_avg:35.05ms
step:110/2110 train_time:3853ms step_avg:35.03ms
step:111/2110 train_time:3887ms step_avg:35.02ms
step:112/2110 train_time:3920ms step_avg:35.00ms
step:113/2110 train_time:3954ms step_avg:34.99ms
step:114/2110 train_time:3986ms step_avg:34.97ms
step:115/2110 train_time:4020ms step_avg:34.96ms
step:116/2110 train_time:4052ms step_avg:34.93ms
step:117/2110 train_time:4086ms step_avg:34.92ms
step:118/2110 train_time:4118ms step_avg:34.90ms
step:119/2110 train_time:4152ms step_avg:34.89ms
step:120/2110 train_time:4184ms step_avg:34.87ms
step:121/2110 train_time:4218ms step_avg:34.86ms
step:122/2110 train_time:4250ms step_avg:34.84ms
step:123/2110 train_time:4284ms step_avg:34.83ms
step:124/2110 train_time:4317ms step_avg:34.81ms
step:125/2110 train_time:4350ms step_avg:34.80ms
step:126/2110 train_time:4383ms step_avg:34.78ms
step:127/2110 train_time:4416ms step_avg:34.77ms
step:128/2110 train_time:4449ms step_avg:34.76ms
step:129/2110 train_time:4483ms step_avg:34.75ms
step:130/2110 train_time:4515ms step_avg:34.73ms
step:131/2110 train_time:4549ms step_avg:34.72ms
step:132/2110 train_time:4581ms step_avg:34.71ms
step:133/2110 train_time:4615ms step_avg:34.70ms
step:134/2110 train_time:4647ms step_avg:34.68ms
step:135/2110 train_time:4681ms step_avg:34.68ms
step:136/2110 train_time:4714ms step_avg:34.66ms
step:137/2110 train_time:4748ms step_avg:34.66ms
step:138/2110 train_time:4781ms step_avg:34.64ms
step:139/2110 train_time:4814ms step_avg:34.64ms
step:140/2110 train_time:4847ms step_avg:34.62ms
step:141/2110 train_time:4881ms step_avg:34.62ms
step:142/2110 train_time:4913ms step_avg:34.60ms
step:143/2110 train_time:4947ms step_avg:34.59ms
step:144/2110 train_time:4979ms step_avg:34.58ms
step:145/2110 train_time:5013ms step_avg:34.57ms
step:146/2110 train_time:5045ms step_avg:34.55ms
step:147/2110 train_time:5078ms step_avg:34.55ms
step:148/2110 train_time:5111ms step_avg:34.53ms
step:149/2110 train_time:5145ms step_avg:34.53ms
step:150/2110 train_time:5178ms step_avg:34.52ms
step:151/2110 train_time:5211ms step_avg:34.51ms
step:152/2110 train_time:5243ms step_avg:34.50ms
step:153/2110 train_time:5277ms step_avg:34.49ms
step:154/2110 train_time:5309ms step_avg:34.48ms
step:155/2110 train_time:5344ms step_avg:34.47ms
step:156/2110 train_time:5376ms step_avg:34.46ms
step:157/2110 train_time:5409ms step_avg:34.46ms
step:158/2110 train_time:5442ms step_avg:34.44ms
step:159/2110 train_time:5475ms step_avg:34.44ms
step:160/2110 train_time:5508ms step_avg:34.43ms
step:161/2110 train_time:5542ms step_avg:34.42ms
step:162/2110 train_time:5575ms step_avg:34.41ms
step:163/2110 train_time:5608ms step_avg:34.40ms
step:164/2110 train_time:5641ms step_avg:34.39ms
step:165/2110 train_time:5674ms step_avg:34.39ms
step:166/2110 train_time:5706ms step_avg:34.38ms
step:167/2110 train_time:5740ms step_avg:34.37ms
step:168/2110 train_time:5773ms step_avg:34.36ms
step:169/2110 train_time:5806ms step_avg:34.36ms
step:170/2110 train_time:5839ms step_avg:34.35ms
step:171/2110 train_time:5872ms step_avg:34.34ms
step:172/2110 train_time:5905ms step_avg:34.33ms
step:173/2110 train_time:5938ms step_avg:34.32ms
step:174/2110 train_time:5971ms step_avg:34.31ms
step:175/2110 train_time:6005ms step_avg:34.31ms
step:176/2110 train_time:6037ms step_avg:34.30ms
step:177/2110 train_time:6071ms step_avg:34.30ms
step:178/2110 train_time:6103ms step_avg:34.29ms
step:179/2110 train_time:6137ms step_avg:34.28ms
step:180/2110 train_time:6170ms step_avg:34.28ms
step:181/2110 train_time:6203ms step_avg:34.27ms
step:182/2110 train_time:6236ms step_avg:34.26ms
step:183/2110 train_time:6269ms step_avg:34.26ms
step:184/2110 train_time:6302ms step_avg:34.25ms
step:185/2110 train_time:6335ms step_avg:34.24ms
step:186/2110 train_time:6367ms step_avg:34.23ms
step:187/2110 train_time:6401ms step_avg:34.23ms
step:188/2110 train_time:6434ms step_avg:34.22ms
step:189/2110 train_time:6468ms step_avg:34.22ms
step:190/2110 train_time:6501ms step_avg:34.21ms
step:191/2110 train_time:6534ms step_avg:34.21ms
step:192/2110 train_time:6567ms step_avg:34.20ms
step:193/2110 train_time:6601ms step_avg:34.20ms
step:194/2110 train_time:6633ms step_avg:34.19ms
step:195/2110 train_time:6667ms step_avg:34.19ms
step:196/2110 train_time:6699ms step_avg:34.18ms
step:197/2110 train_time:6732ms step_avg:34.17ms
step:198/2110 train_time:6765ms step_avg:34.17ms
step:199/2110 train_time:6798ms step_avg:34.16ms
step:200/2110 train_time:6831ms step_avg:34.15ms
step:201/2110 train_time:6864ms step_avg:34.15ms
step:202/2110 train_time:6897ms step_avg:34.14ms
step:203/2110 train_time:6931ms step_avg:34.14ms
step:204/2110 train_time:6963ms step_avg:34.13ms
step:205/2110 train_time:6997ms step_avg:34.13ms
step:206/2110 train_time:7029ms step_avg:34.12ms
step:207/2110 train_time:7063ms step_avg:34.12ms
step:208/2110 train_time:7095ms step_avg:34.11ms
step:209/2110 train_time:7129ms step_avg:34.11ms
step:210/2110 train_time:7162ms step_avg:34.10ms
step:211/2110 train_time:7196ms step_avg:34.10ms
step:212/2110 train_time:7228ms step_avg:34.10ms
step:213/2110 train_time:7262ms step_avg:34.09ms
step:214/2110 train_time:7294ms step_avg:34.08ms
step:215/2110 train_time:7328ms step_avg:34.08ms
step:216/2110 train_time:7361ms step_avg:34.08ms
step:217/2110 train_time:7394ms step_avg:34.07ms
step:218/2110 train_time:7427ms step_avg:34.07ms
step:219/2110 train_time:7461ms step_avg:34.07ms
step:220/2110 train_time:7493ms step_avg:34.06ms
step:221/2110 train_time:7527ms step_avg:34.06ms
step:222/2110 train_time:7559ms step_avg:34.05ms
step:223/2110 train_time:7593ms step_avg:34.05ms
step:224/2110 train_time:7626ms step_avg:34.05ms
step:225/2110 train_time:7660ms step_avg:34.04ms
step:226/2110 train_time:7692ms step_avg:34.04ms
step:227/2110 train_time:7726ms step_avg:34.03ms
step:228/2110 train_time:7758ms step_avg:34.03ms
step:229/2110 train_time:7791ms step_avg:34.02ms
step:230/2110 train_time:7824ms step_avg:34.02ms
step:231/2110 train_time:7858ms step_avg:34.02ms
step:232/2110 train_time:7890ms step_avg:34.01ms
step:233/2110 train_time:7924ms step_avg:34.01ms
step:234/2110 train_time:7957ms step_avg:34.00ms
step:235/2110 train_time:7990ms step_avg:34.00ms
step:236/2110 train_time:8023ms step_avg:33.99ms
step:237/2110 train_time:8056ms step_avg:33.99ms
step:238/2110 train_time:8088ms step_avg:33.98ms
step:239/2110 train_time:8122ms step_avg:33.98ms
step:240/2110 train_time:8155ms step_avg:33.98ms
step:241/2110 train_time:8188ms step_avg:33.98ms
step:242/2110 train_time:8221ms step_avg:33.97ms
step:243/2110 train_time:8255ms step_avg:33.97ms
step:244/2110 train_time:8287ms step_avg:33.96ms
step:245/2110 train_time:8321ms step_avg:33.96ms
step:246/2110 train_time:8354ms step_avg:33.96ms
step:247/2110 train_time:8387ms step_avg:33.96ms
step:248/2110 train_time:8420ms step_avg:33.95ms
step:249/2110 train_time:8453ms step_avg:33.95ms
step:250/2110 train_time:8486ms step_avg:33.94ms
step:250/2110 val_loss:4.2650 train_time:8522ms step_avg:34.09ms
step:251/2110 train_time:8542ms step_avg:34.03ms
step:252/2110 train_time:8561ms step_avg:33.97ms
step:253/2110 train_time:8591ms step_avg:33.96ms
step:254/2110 train_time:8624ms step_avg:33.95ms
step:255/2110 train_time:8660ms step_avg:33.96ms
step:256/2110 train_time:8694ms step_avg:33.96ms
step:257/2110 train_time:8729ms step_avg:33.96ms
step:258/2110 train_time:8761ms step_avg:33.96ms
step:259/2110 train_time:8795ms step_avg:33.96ms
step:260/2110 train_time:8828ms step_avg:33.95ms
step:261/2110 train_time:8862ms step_avg:33.95ms
step:262/2110 train_time:8894ms step_avg:33.95ms
step:263/2110 train_time:8928ms step_avg:33.95ms
step:264/2110 train_time:8960ms step_avg:33.94ms
step:265/2110 train_time:8993ms step_avg:33.94ms
step:266/2110 train_time:9026ms step_avg:33.93ms
step:267/2110 train_time:9059ms step_avg:33.93ms
step:268/2110 train_time:9091ms step_avg:33.92ms
step:269/2110 train_time:9125ms step_avg:33.92ms
step:270/2110 train_time:9157ms step_avg:33.92ms
step:271/2110 train_time:9190ms step_avg:33.91ms
step:272/2110 train_time:9223ms step_avg:33.91ms
step:273/2110 train_time:9256ms step_avg:33.91ms
step:274/2110 train_time:9289ms step_avg:33.90ms
step:275/2110 train_time:9322ms step_avg:33.90ms
step:276/2110 train_time:9355ms step_avg:33.89ms
step:277/2110 train_time:9388ms step_avg:33.89ms
step:278/2110 train_time:9420ms step_avg:33.89ms
step:279/2110 train_time:9454ms step_avg:33.89ms
step:280/2110 train_time:9487ms step_avg:33.88ms
step:281/2110 train_time:9520ms step_avg:33.88ms
step:282/2110 train_time:9553ms step_avg:33.88ms
step:283/2110 train_time:9587ms step_avg:33.88ms
step:284/2110 train_time:9620ms step_avg:33.87ms
step:285/2110 train_time:9654ms step_avg:33.87ms
step:286/2110 train_time:9687ms step_avg:33.87ms
step:287/2110 train_time:9721ms step_avg:33.87ms
step:288/2110 train_time:9753ms step_avg:33.87ms
step:289/2110 train_time:9787ms step_avg:33.87ms
step:290/2110 train_time:9820ms step_avg:33.86ms
step:291/2110 train_time:9854ms step_avg:33.86ms
step:292/2110 train_time:9886ms step_avg:33.86ms
step:293/2110 train_time:9920ms step_avg:33.86ms
step:294/2110 train_time:9952ms step_avg:33.85ms
step:295/2110 train_time:9986ms step_avg:33.85ms
step:296/2110 train_time:10018ms step_avg:33.84ms
step:297/2110 train_time:10051ms step_avg:33.84ms
step:298/2110 train_time:10084ms step_avg:33.84ms
step:299/2110 train_time:10117ms step_avg:33.84ms
step:300/2110 train_time:10149ms step_avg:33.83ms
step:301/2110 train_time:10183ms step_avg:33.83ms
step:302/2110 train_time:10215ms step_avg:33.83ms
step:303/2110 train_time:10249ms step_avg:33.82ms
step:304/2110 train_time:10281ms step_avg:33.82ms
step:305/2110 train_time:10315ms step_avg:33.82ms
step:306/2110 train_time:10347ms step_avg:33.81ms
step:307/2110 train_time:10381ms step_avg:33.81ms
step:308/2110 train_time:10413ms step_avg:33.81ms
step:309/2110 train_time:10447ms step_avg:33.81ms
step:310/2110 train_time:10479ms step_avg:33.80ms
step:311/2110 train_time:10513ms step_avg:33.80ms
step:312/2110 train_time:10545ms step_avg:33.80ms
step:313/2110 train_time:10579ms step_avg:33.80ms
step:314/2110 train_time:10612ms step_avg:33.80ms
step:315/2110 train_time:10645ms step_avg:33.79ms
step:316/2110 train_time:10678ms step_avg:33.79ms
step:317/2110 train_time:10712ms step_avg:33.79ms
step:318/2110 train_time:10745ms step_avg:33.79ms
step:319/2110 train_time:10779ms step_avg:33.79ms
step:320/2110 train_time:10811ms step_avg:33.78ms
step:321/2110 train_time:10845ms step_avg:33.79ms
step:322/2110 train_time:10878ms step_avg:33.78ms
step:323/2110 train_time:10912ms step_avg:33.78ms
step:324/2110 train_time:10944ms step_avg:33.78ms
step:325/2110 train_time:10978ms step_avg:33.78ms
step:326/2110 train_time:11010ms step_avg:33.77ms
step:327/2110 train_time:11045ms step_avg:33.78ms
step:328/2110 train_time:11077ms step_avg:33.77ms
step:329/2110 train_time:11111ms step_avg:33.77ms
step:330/2110 train_time:11143ms step_avg:33.77ms
step:331/2110 train_time:11177ms step_avg:33.77ms
step:332/2110 train_time:11209ms step_avg:33.76ms
step:333/2110 train_time:11243ms step_avg:33.76ms
step:334/2110 train_time:11276ms step_avg:33.76ms
step:335/2110 train_time:11309ms step_avg:33.76ms
step:336/2110 train_time:11341ms step_avg:33.75ms
step:337/2110 train_time:11375ms step_avg:33.75ms
step:338/2110 train_time:11407ms step_avg:33.75ms
step:339/2110 train_time:11441ms step_avg:33.75ms
step:340/2110 train_time:11473ms step_avg:33.74ms
step:341/2110 train_time:11507ms step_avg:33.74ms
step:342/2110 train_time:11540ms step_avg:33.74ms
step:343/2110 train_time:11573ms step_avg:33.74ms
step:344/2110 train_time:11605ms step_avg:33.74ms
step:345/2110 train_time:11639ms step_avg:33.74ms
step:346/2110 train_time:11671ms step_avg:33.73ms
step:347/2110 train_time:11705ms step_avg:33.73ms
step:348/2110 train_time:11738ms step_avg:33.73ms
step:349/2110 train_time:11772ms step_avg:33.73ms
step:350/2110 train_time:11804ms step_avg:33.73ms
step:351/2110 train_time:11838ms step_avg:33.73ms
step:352/2110 train_time:11871ms step_avg:33.72ms
step:353/2110 train_time:11904ms step_avg:33.72ms
step:354/2110 train_time:11937ms step_avg:33.72ms
step:355/2110 train_time:11971ms step_avg:33.72ms
step:356/2110 train_time:12003ms step_avg:33.72ms
step:357/2110 train_time:12038ms step_avg:33.72ms
step:358/2110 train_time:12070ms step_avg:33.72ms
step:359/2110 train_time:12104ms step_avg:33.72ms
step:360/2110 train_time:12137ms step_avg:33.71ms
step:361/2110 train_time:12171ms step_avg:33.71ms
step:362/2110 train_time:12203ms step_avg:33.71ms
step:363/2110 train_time:12237ms step_avg:33.71ms
step:364/2110 train_time:12269ms step_avg:33.71ms
step:365/2110 train_time:12303ms step_avg:33.71ms
step:366/2110 train_time:12335ms step_avg:33.70ms
step:367/2110 train_time:12369ms step_avg:33.70ms
step:368/2110 train_time:12401ms step_avg:33.70ms
step:369/2110 train_time:12435ms step_avg:33.70ms
step:370/2110 train_time:12467ms step_avg:33.70ms
step:371/2110 train_time:12501ms step_avg:33.70ms
step:372/2110 train_time:12534ms step_avg:33.69ms
step:373/2110 train_time:12568ms step_avg:33.69ms
step:374/2110 train_time:12600ms step_avg:33.69ms
step:375/2110 train_time:12634ms step_avg:33.69ms
step:376/2110 train_time:12666ms step_avg:33.69ms
step:377/2110 train_time:12700ms step_avg:33.69ms
step:378/2110 train_time:12732ms step_avg:33.68ms
step:379/2110 train_time:12766ms step_avg:33.68ms
step:380/2110 train_time:12798ms step_avg:33.68ms
step:381/2110 train_time:12832ms step_avg:33.68ms
step:382/2110 train_time:12865ms step_avg:33.68ms
step:383/2110 train_time:12899ms step_avg:33.68ms
step:384/2110 train_time:12931ms step_avg:33.67ms
step:385/2110 train_time:12965ms step_avg:33.68ms
step:386/2110 train_time:12998ms step_avg:33.67ms
step:387/2110 train_time:13032ms step_avg:33.67ms
step:388/2110 train_time:13064ms step_avg:33.67ms
step:389/2110 train_time:13098ms step_avg:33.67ms
step:390/2110 train_time:13130ms step_avg:33.67ms
step:391/2110 train_time:13164ms step_avg:33.67ms
step:392/2110 train_time:13197ms step_avg:33.67ms
step:393/2110 train_time:13230ms step_avg:33.67ms
step:394/2110 train_time:13263ms step_avg:33.66ms
step:395/2110 train_time:13296ms step_avg:33.66ms
step:396/2110 train_time:13329ms step_avg:33.66ms
step:397/2110 train_time:13362ms step_avg:33.66ms
step:398/2110 train_time:13394ms step_avg:33.65ms
step:399/2110 train_time:13428ms step_avg:33.65ms
step:400/2110 train_time:13461ms step_avg:33.65ms
step:401/2110 train_time:13494ms step_avg:33.65ms
step:402/2110 train_time:13526ms step_avg:33.65ms
step:403/2110 train_time:13560ms step_avg:33.65ms
step:404/2110 train_time:13593ms step_avg:33.65ms
step:405/2110 train_time:13627ms step_avg:33.65ms
step:406/2110 train_time:13659ms step_avg:33.64ms
step:407/2110 train_time:13693ms step_avg:33.64ms
step:408/2110 train_time:13725ms step_avg:33.64ms
step:409/2110 train_time:13759ms step_avg:33.64ms
step:410/2110 train_time:13792ms step_avg:33.64ms
step:411/2110 train_time:13825ms step_avg:33.64ms
step:412/2110 train_time:13858ms step_avg:33.64ms
step:413/2110 train_time:13891ms step_avg:33.64ms
step:414/2110 train_time:13924ms step_avg:33.63ms
step:415/2110 train_time:13958ms step_avg:33.63ms
step:416/2110 train_time:13990ms step_avg:33.63ms
step:417/2110 train_time:14024ms step_avg:33.63ms
step:418/2110 train_time:14056ms step_avg:33.63ms
step:419/2110 train_time:14090ms step_avg:33.63ms
step:420/2110 train_time:14123ms step_avg:33.63ms
step:421/2110 train_time:14156ms step_avg:33.63ms
step:422/2110 train_time:14189ms step_avg:33.62ms
step:423/2110 train_time:14222ms step_avg:33.62ms
step:424/2110 train_time:14255ms step_avg:33.62ms
step:425/2110 train_time:14289ms step_avg:33.62ms
step:426/2110 train_time:14321ms step_avg:33.62ms
step:427/2110 train_time:14355ms step_avg:33.62ms
step:428/2110 train_time:14387ms step_avg:33.61ms
step:429/2110 train_time:14421ms step_avg:33.62ms
step:430/2110 train_time:14453ms step_avg:33.61ms
step:431/2110 train_time:14487ms step_avg:33.61ms
step:432/2110 train_time:14519ms step_avg:33.61ms
step:433/2110 train_time:14553ms step_avg:33.61ms
step:434/2110 train_time:14585ms step_avg:33.61ms
step:435/2110 train_time:14619ms step_avg:33.61ms
step:436/2110 train_time:14651ms step_avg:33.60ms
step:437/2110 train_time:14686ms step_avg:33.61ms
step:438/2110 train_time:14718ms step_avg:33.60ms
step:439/2110 train_time:14752ms step_avg:33.60ms
step:440/2110 train_time:14785ms step_avg:33.60ms
step:441/2110 train_time:14818ms step_avg:33.60ms
step:442/2110 train_time:14851ms step_avg:33.60ms
step:443/2110 train_time:14885ms step_avg:33.60ms
step:444/2110 train_time:14917ms step_avg:33.60ms
step:445/2110 train_time:14951ms step_avg:33.60ms
step:446/2110 train_time:14984ms step_avg:33.60ms
step:447/2110 train_time:15017ms step_avg:33.60ms
step:448/2110 train_time:15049ms step_avg:33.59ms
step:449/2110 train_time:15083ms step_avg:33.59ms
step:450/2110 train_time:15115ms step_avg:33.59ms
step:451/2110 train_time:15149ms step_avg:33.59ms
step:452/2110 train_time:15182ms step_avg:33.59ms
step:453/2110 train_time:15216ms step_avg:33.59ms
step:454/2110 train_time:15248ms step_avg:33.59ms
step:455/2110 train_time:15282ms step_avg:33.59ms
step:456/2110 train_time:15314ms step_avg:33.58ms
step:457/2110 train_time:15348ms step_avg:33.58ms
step:458/2110 train_time:15380ms step_avg:33.58ms
step:459/2110 train_time:15414ms step_avg:33.58ms
step:460/2110 train_time:15446ms step_avg:33.58ms
step:461/2110 train_time:15480ms step_avg:33.58ms
step:462/2110 train_time:15513ms step_avg:33.58ms
step:463/2110 train_time:15547ms step_avg:33.58ms
step:464/2110 train_time:15579ms step_avg:33.58ms
step:465/2110 train_time:15613ms step_avg:33.58ms
step:466/2110 train_time:15646ms step_avg:33.57ms
step:467/2110 train_time:15679ms step_avg:33.57ms
step:468/2110 train_time:15712ms step_avg:33.57ms
step:469/2110 train_time:15746ms step_avg:33.57ms
step:470/2110 train_time:15779ms step_avg:33.57ms
step:471/2110 train_time:15812ms step_avg:33.57ms
step:472/2110 train_time:15845ms step_avg:33.57ms
step:473/2110 train_time:15879ms step_avg:33.57ms
step:474/2110 train_time:15911ms step_avg:33.57ms
step:475/2110 train_time:15945ms step_avg:33.57ms
step:476/2110 train_time:15977ms step_avg:33.57ms
step:477/2110 train_time:16011ms step_avg:33.57ms
step:478/2110 train_time:16044ms step_avg:33.56ms
step:479/2110 train_time:16077ms step_avg:33.56ms
step:480/2110 train_time:16110ms step_avg:33.56ms
step:481/2110 train_time:16143ms step_avg:33.56ms
step:482/2110 train_time:16176ms step_avg:33.56ms
step:483/2110 train_time:16209ms step_avg:33.56ms
step:484/2110 train_time:16242ms step_avg:33.56ms
step:485/2110 train_time:16276ms step_avg:33.56ms
step:486/2110 train_time:16308ms step_avg:33.56ms
step:487/2110 train_time:16342ms step_avg:33.56ms
step:488/2110 train_time:16374ms step_avg:33.55ms
step:489/2110 train_time:16408ms step_avg:33.55ms
step:490/2110 train_time:16441ms step_avg:33.55ms
step:491/2110 train_time:16474ms step_avg:33.55ms
step:492/2110 train_time:16507ms step_avg:33.55ms
step:493/2110 train_time:16541ms step_avg:33.55ms
step:494/2110 train_time:16573ms step_avg:33.55ms
step:495/2110 train_time:16607ms step_avg:33.55ms
step:496/2110 train_time:16639ms step_avg:33.55ms
step:497/2110 train_time:16673ms step_avg:33.55ms
step:498/2110 train_time:16706ms step_avg:33.55ms
step:499/2110 train_time:16739ms step_avg:33.55ms
step:500/2110 train_time:16772ms step_avg:33.54ms
step:500/2110 val_loss:4.0042 train_time:16808ms step_avg:33.62ms
step:501/2110 train_time:16828ms step_avg:33.59ms
step:502/2110 train_time:16848ms step_avg:33.56ms
step:503/2110 train_time:16876ms step_avg:33.55ms
step:504/2110 train_time:16909ms step_avg:33.55ms
step:505/2110 train_time:16945ms step_avg:33.55ms
step:506/2110 train_time:16978ms step_avg:33.55ms
step:507/2110 train_time:17013ms step_avg:33.56ms
step:508/2110 train_time:17045ms step_avg:33.55ms
step:509/2110 train_time:17079ms step_avg:33.55ms
step:510/2110 train_time:17112ms step_avg:33.55ms
step:511/2110 train_time:17145ms step_avg:33.55ms
step:512/2110 train_time:17178ms step_avg:33.55ms
step:513/2110 train_time:17211ms step_avg:33.55ms
step:514/2110 train_time:17244ms step_avg:33.55ms
step:515/2110 train_time:17277ms step_avg:33.55ms
step:516/2110 train_time:17310ms step_avg:33.55ms
step:517/2110 train_time:17343ms step_avg:33.55ms
step:518/2110 train_time:17375ms step_avg:33.54ms
step:519/2110 train_time:17408ms step_avg:33.54ms
step:520/2110 train_time:17440ms step_avg:33.54ms
step:521/2110 train_time:17474ms step_avg:33.54ms
step:522/2110 train_time:17506ms step_avg:33.54ms
step:523/2110 train_time:17540ms step_avg:33.54ms
step:524/2110 train_time:17572ms step_avg:33.53ms
step:525/2110 train_time:17606ms step_avg:33.53ms
step:526/2110 train_time:17638ms step_avg:33.53ms
step:527/2110 train_time:17671ms step_avg:33.53ms
step:528/2110 train_time:17704ms step_avg:33.53ms
step:529/2110 train_time:17738ms step_avg:33.53ms
step:530/2110 train_time:17770ms step_avg:33.53ms
step:531/2110 train_time:17804ms step_avg:33.53ms
step:532/2110 train_time:17837ms step_avg:33.53ms
step:533/2110 train_time:17872ms step_avg:33.53ms
step:534/2110 train_time:17905ms step_avg:33.53ms
step:535/2110 train_time:17939ms step_avg:33.53ms
step:536/2110 train_time:17971ms step_avg:33.53ms
step:537/2110 train_time:18005ms step_avg:33.53ms
step:538/2110 train_time:18038ms step_avg:33.53ms
step:539/2110 train_time:18072ms step_avg:33.53ms
step:540/2110 train_time:18105ms step_avg:33.53ms
step:541/2110 train_time:18138ms step_avg:33.53ms
step:542/2110 train_time:18171ms step_avg:33.53ms
step:543/2110 train_time:18205ms step_avg:33.53ms
step:544/2110 train_time:18237ms step_avg:33.52ms
step:545/2110 train_time:18271ms step_avg:33.52ms
step:546/2110 train_time:18303ms step_avg:33.52ms
step:547/2110 train_time:18337ms step_avg:33.52ms
step:548/2110 train_time:18370ms step_avg:33.52ms
step:549/2110 train_time:18403ms step_avg:33.52ms
step:550/2110 train_time:18436ms step_avg:33.52ms
step:551/2110 train_time:18469ms step_avg:33.52ms
step:552/2110 train_time:18501ms step_avg:33.52ms
step:553/2110 train_time:18535ms step_avg:33.52ms
step:554/2110 train_time:18568ms step_avg:33.52ms
step:555/2110 train_time:18601ms step_avg:33.52ms
step:556/2110 train_time:18634ms step_avg:33.51ms
step:557/2110 train_time:18667ms step_avg:33.51ms
step:558/2110 train_time:18700ms step_avg:33.51ms
step:559/2110 train_time:18733ms step_avg:33.51ms
step:560/2110 train_time:18766ms step_avg:33.51ms
step:561/2110 train_time:18799ms step_avg:33.51ms
step:562/2110 train_time:18832ms step_avg:33.51ms
step:563/2110 train_time:18866ms step_avg:33.51ms
step:564/2110 train_time:18898ms step_avg:33.51ms
step:565/2110 train_time:18932ms step_avg:33.51ms
step:566/2110 train_time:18965ms step_avg:33.51ms
step:567/2110 train_time:18999ms step_avg:33.51ms
step:568/2110 train_time:19032ms step_avg:33.51ms
step:569/2110 train_time:19066ms step_avg:33.51ms
step:570/2110 train_time:19098ms step_avg:33.51ms
step:571/2110 train_time:19132ms step_avg:33.51ms
step:572/2110 train_time:19165ms step_avg:33.50ms
step:573/2110 train_time:19198ms step_avg:33.50ms
step:574/2110 train_time:19231ms step_avg:33.50ms
step:575/2110 train_time:19264ms step_avg:33.50ms
step:576/2110 train_time:19297ms step_avg:33.50ms
step:577/2110 train_time:19331ms step_avg:33.50ms
step:578/2110 train_time:19363ms step_avg:33.50ms
step:579/2110 train_time:19397ms step_avg:33.50ms
step:580/2110 train_time:19429ms step_avg:33.50ms
step:581/2110 train_time:19463ms step_avg:33.50ms
step:582/2110 train_time:19495ms step_avg:33.50ms
step:583/2110 train_time:19528ms step_avg:33.50ms
step:584/2110 train_time:19561ms step_avg:33.49ms
step:585/2110 train_time:19594ms step_avg:33.49ms
step:586/2110 train_time:19627ms step_avg:33.49ms
step:587/2110 train_time:19661ms step_avg:33.49ms
step:588/2110 train_time:19693ms step_avg:33.49ms
step:589/2110 train_time:19727ms step_avg:33.49ms
step:590/2110 train_time:19759ms step_avg:33.49ms
step:591/2110 train_time:19793ms step_avg:33.49ms
step:592/2110 train_time:19826ms step_avg:33.49ms
step:593/2110 train_time:19860ms step_avg:33.49ms
step:594/2110 train_time:19892ms step_avg:33.49ms
step:595/2110 train_time:19926ms step_avg:33.49ms
step:596/2110 train_time:19959ms step_avg:33.49ms
step:597/2110 train_time:19993ms step_avg:33.49ms
step:598/2110 train_time:20026ms step_avg:33.49ms
step:599/2110 train_time:20060ms step_avg:33.49ms
step:600/2110 train_time:20092ms step_avg:33.49ms
step:601/2110 train_time:20126ms step_avg:33.49ms
step:602/2110 train_time:20158ms step_avg:33.49ms
step:603/2110 train_time:20192ms step_avg:33.49ms
step:604/2110 train_time:20225ms step_avg:33.48ms
step:605/2110 train_time:20258ms step_avg:33.48ms
step:606/2110 train_time:20291ms step_avg:33.48ms
step:607/2110 train_time:20324ms step_avg:33.48ms
step:608/2110 train_time:20357ms step_avg:33.48ms
step:609/2110 train_time:20390ms step_avg:33.48ms
step:610/2110 train_time:20422ms step_avg:33.48ms
step:611/2110 train_time:20456ms step_avg:33.48ms
step:612/2110 train_time:20489ms step_avg:33.48ms
step:613/2110 train_time:20522ms step_avg:33.48ms
step:614/2110 train_time:20555ms step_avg:33.48ms
step:615/2110 train_time:20589ms step_avg:33.48ms
step:616/2110 train_time:20621ms step_avg:33.48ms
step:617/2110 train_time:20655ms step_avg:33.48ms
step:618/2110 train_time:20687ms step_avg:33.47ms
step:619/2110 train_time:20721ms step_avg:33.48ms
step:620/2110 train_time:20754ms step_avg:33.47ms
step:621/2110 train_time:20787ms step_avg:33.47ms
step:622/2110 train_time:20820ms step_avg:33.47ms
step:623/2110 train_time:20854ms step_avg:33.47ms
step:624/2110 train_time:20886ms step_avg:33.47ms
step:625/2110 train_time:20920ms step_avg:33.47ms
step:626/2110 train_time:20952ms step_avg:33.47ms
step:627/2110 train_time:20986ms step_avg:33.47ms
step:628/2110 train_time:21019ms step_avg:33.47ms
step:629/2110 train_time:21052ms step_avg:33.47ms
step:630/2110 train_time:21085ms step_avg:33.47ms
step:631/2110 train_time:21119ms step_avg:33.47ms
step:632/2110 train_time:21151ms step_avg:33.47ms
step:633/2110 train_time:21185ms step_avg:33.47ms
step:634/2110 train_time:21218ms step_avg:33.47ms
step:635/2110 train_time:21252ms step_avg:33.47ms
step:636/2110 train_time:21284ms step_avg:33.47ms
step:637/2110 train_time:21318ms step_avg:33.47ms
step:638/2110 train_time:21350ms step_avg:33.46ms
step:639/2110 train_time:21384ms step_avg:33.46ms
step:640/2110 train_time:21416ms step_avg:33.46ms
step:641/2110 train_time:21450ms step_avg:33.46ms
step:642/2110 train_time:21483ms step_avg:33.46ms
step:643/2110 train_time:21516ms step_avg:33.46ms
step:644/2110 train_time:21549ms step_avg:33.46ms
step:645/2110 train_time:21583ms step_avg:33.46ms
step:646/2110 train_time:21615ms step_avg:33.46ms
step:647/2110 train_time:21649ms step_avg:33.46ms
step:648/2110 train_time:21681ms step_avg:33.46ms
step:649/2110 train_time:21715ms step_avg:33.46ms
step:650/2110 train_time:21748ms step_avg:33.46ms
step:651/2110 train_time:21782ms step_avg:33.46ms
step:652/2110 train_time:21814ms step_avg:33.46ms
step:653/2110 train_time:21848ms step_avg:33.46ms
step:654/2110 train_time:21880ms step_avg:33.46ms
step:655/2110 train_time:21914ms step_avg:33.46ms
step:656/2110 train_time:21946ms step_avg:33.45ms
step:657/2110 train_time:21980ms step_avg:33.46ms
step:658/2110 train_time:22013ms step_avg:33.45ms
step:659/2110 train_time:22047ms step_avg:33.45ms
step:660/2110 train_time:22079ms step_avg:33.45ms
step:661/2110 train_time:22113ms step_avg:33.45ms
step:662/2110 train_time:22146ms step_avg:33.45ms
step:663/2110 train_time:22180ms step_avg:33.45ms
step:664/2110 train_time:22212ms step_avg:33.45ms
step:665/2110 train_time:22246ms step_avg:33.45ms
step:666/2110 train_time:22278ms step_avg:33.45ms
step:667/2110 train_time:22312ms step_avg:33.45ms
step:668/2110 train_time:22345ms step_avg:33.45ms
step:669/2110 train_time:22379ms step_avg:33.45ms
step:670/2110 train_time:22411ms step_avg:33.45ms
step:671/2110 train_time:22445ms step_avg:33.45ms
step:672/2110 train_time:22478ms step_avg:33.45ms
step:673/2110 train_time:22511ms step_avg:33.45ms
step:674/2110 train_time:22544ms step_avg:33.45ms
step:675/2110 train_time:22578ms step_avg:33.45ms
step:676/2110 train_time:22611ms step_avg:33.45ms
step:677/2110 train_time:22644ms step_avg:33.45ms
step:678/2110 train_time:22677ms step_avg:33.45ms
step:679/2110 train_time:22710ms step_avg:33.45ms
step:680/2110 train_time:22743ms step_avg:33.44ms
step:681/2110 train_time:22776ms step_avg:33.45ms
step:682/2110 train_time:22809ms step_avg:33.44ms
step:683/2110 train_time:22843ms step_avg:33.44ms
step:684/2110 train_time:22875ms step_avg:33.44ms
step:685/2110 train_time:22909ms step_avg:33.44ms
step:686/2110 train_time:22942ms step_avg:33.44ms
step:687/2110 train_time:22976ms step_avg:33.44ms
step:688/2110 train_time:23008ms step_avg:33.44ms
step:689/2110 train_time:23042ms step_avg:33.44ms
step:690/2110 train_time:23075ms step_avg:33.44ms
step:691/2110 train_time:23109ms step_avg:33.44ms
step:692/2110 train_time:23168ms step_avg:33.48ms
step:693/2110 train_time:23230ms step_avg:33.52ms
step:694/2110 train_time:23289ms step_avg:33.56ms
step:695/2110 train_time:23351ms step_avg:33.60ms
step:696/2110 train_time:23410ms step_avg:33.63ms
step:697/2110 train_time:23471ms step_avg:33.67ms
step:698/2110 train_time:23530ms step_avg:33.71ms
step:699/2110 train_time:23591ms step_avg:33.75ms
step:700/2110 train_time:23650ms step_avg:33.79ms
step:701/2110 train_time:23712ms step_avg:33.83ms
step:702/2110 train_time:23770ms step_avg:33.86ms
step:703/2110 train_time:23831ms step_avg:33.90ms
step:704/2110 train_time:23891ms step_avg:33.94ms
step:705/2110 train_time:23951ms step_avg:33.97ms
step:706/2110 train_time:24011ms step_avg:34.01ms
step:707/2110 train_time:24072ms step_avg:34.05ms
step:708/2110 train_time:24131ms step_avg:34.08ms
step:709/2110 train_time:24193ms step_avg:34.12ms
step:710/2110 train_time:24252ms step_avg:34.16ms
step:711/2110 train_time:24313ms step_avg:34.20ms
step:712/2110 train_time:24372ms step_avg:34.23ms
step:713/2110 train_time:24433ms step_avg:34.27ms
step:714/2110 train_time:24492ms step_avg:34.30ms
step:715/2110 train_time:24553ms step_avg:34.34ms
step:716/2110 train_time:24612ms step_avg:34.37ms
step:717/2110 train_time:24673ms step_avg:34.41ms
step:718/2110 train_time:24732ms step_avg:34.45ms
step:719/2110 train_time:24794ms step_avg:34.48ms
step:720/2110 train_time:24853ms step_avg:34.52ms
step:721/2110 train_time:24914ms step_avg:34.55ms
step:722/2110 train_time:24974ms step_avg:34.59ms
step:723/2110 train_time:25035ms step_avg:34.63ms
step:724/2110 train_time:25095ms step_avg:34.66ms
step:725/2110 train_time:25155ms step_avg:34.70ms
step:726/2110 train_time:25214ms step_avg:34.73ms
step:727/2110 train_time:25275ms step_avg:34.77ms
step:728/2110 train_time:25334ms step_avg:34.80ms
step:729/2110 train_time:25395ms step_avg:34.84ms
step:730/2110 train_time:25454ms step_avg:34.87ms
step:731/2110 train_time:25515ms step_avg:34.90ms
step:732/2110 train_time:25574ms step_avg:34.94ms
step:733/2110 train_time:25635ms step_avg:34.97ms
step:734/2110 train_time:25694ms step_avg:35.01ms
step:735/2110 train_time:25754ms step_avg:35.04ms
step:736/2110 train_time:25813ms step_avg:35.07ms
step:737/2110 train_time:25874ms step_avg:35.11ms
step:738/2110 train_time:25934ms step_avg:35.14ms
step:739/2110 train_time:25995ms step_avg:35.18ms
step:740/2110 train_time:26054ms step_avg:35.21ms
step:741/2110 train_time:26115ms step_avg:35.24ms
step:742/2110 train_time:26175ms step_avg:35.28ms
step:743/2110 train_time:26235ms step_avg:35.31ms
step:744/2110 train_time:26294ms step_avg:35.34ms
step:745/2110 train_time:26355ms step_avg:35.38ms
step:746/2110 train_time:26414ms step_avg:35.41ms
step:747/2110 train_time:26475ms step_avg:35.44ms
step:748/2110 train_time:26534ms step_avg:35.47ms
step:749/2110 train_time:26596ms step_avg:35.51ms
step:750/2110 train_time:26655ms step_avg:35.54ms
step:750/2110 val_loss:3.8512 train_time:26717ms step_avg:35.62ms
step:751/2110 train_time:26738ms step_avg:35.60ms
step:752/2110 train_time:26777ms step_avg:35.61ms
step:753/2110 train_time:26842ms step_avg:35.65ms
step:754/2110 train_time:26902ms step_avg:35.68ms
step:755/2110 train_time:26964ms step_avg:35.71ms
step:756/2110 train_time:27024ms step_avg:35.75ms
step:757/2110 train_time:27084ms step_avg:35.78ms
step:758/2110 train_time:27143ms step_avg:35.81ms
step:759/2110 train_time:27203ms step_avg:35.84ms
step:760/2110 train_time:27261ms step_avg:35.87ms
step:761/2110 train_time:27321ms step_avg:35.90ms
step:762/2110 train_time:27379ms step_avg:35.93ms
step:763/2110 train_time:27440ms step_avg:35.96ms
step:764/2110 train_time:27499ms step_avg:35.99ms
step:765/2110 train_time:27560ms step_avg:36.03ms
step:766/2110 train_time:27619ms step_avg:36.06ms
step:767/2110 train_time:27681ms step_avg:36.09ms
step:768/2110 train_time:27742ms step_avg:36.12ms
step:769/2110 train_time:27803ms step_avg:36.16ms
step:770/2110 train_time:27864ms step_avg:36.19ms
step:771/2110 train_time:27925ms step_avg:36.22ms
step:772/2110 train_time:27984ms step_avg:36.25ms
step:773/2110 train_time:28045ms step_avg:36.28ms
step:774/2110 train_time:28105ms step_avg:36.31ms
step:775/2110 train_time:28165ms step_avg:36.34ms
step:776/2110 train_time:28224ms step_avg:36.37ms
step:777/2110 train_time:28284ms step_avg:36.40ms
step:778/2110 train_time:28342ms step_avg:36.43ms
step:779/2110 train_time:28403ms step_avg:36.46ms
step:780/2110 train_time:28462ms step_avg:36.49ms
step:781/2110 train_time:28523ms step_avg:36.52ms
step:782/2110 train_time:28581ms step_avg:36.55ms
step:783/2110 train_time:28643ms step_avg:36.58ms
step:784/2110 train_time:28704ms step_avg:36.61ms
step:785/2110 train_time:28765ms step_avg:36.64ms
step:786/2110 train_time:28824ms step_avg:36.67ms
step:787/2110 train_time:28886ms step_avg:36.70ms
step:788/2110 train_time:28945ms step_avg:36.73ms
step:789/2110 train_time:29006ms step_avg:36.76ms
step:790/2110 train_time:29065ms step_avg:36.79ms
step:791/2110 train_time:29125ms step_avg:36.82ms
step:792/2110 train_time:29185ms step_avg:36.85ms
step:793/2110 train_time:29245ms step_avg:36.88ms
step:794/2110 train_time:29303ms step_avg:36.91ms
step:795/2110 train_time:29364ms step_avg:36.94ms
step:796/2110 train_time:29423ms step_avg:36.96ms
step:797/2110 train_time:29484ms step_avg:36.99ms
step:798/2110 train_time:29543ms step_avg:37.02ms
step:799/2110 train_time:29604ms step_avg:37.05ms
step:800/2110 train_time:29664ms step_avg:37.08ms
step:801/2110 train_time:29726ms step_avg:37.11ms
step:802/2110 train_time:29785ms step_avg:37.14ms
step:803/2110 train_time:29846ms step_avg:37.17ms
step:804/2110 train_time:29906ms step_avg:37.20ms
step:805/2110 train_time:29967ms step_avg:37.23ms
step:806/2110 train_time:30026ms step_avg:37.25ms
step:807/2110 train_time:30087ms step_avg:37.28ms
step:808/2110 train_time:30146ms step_avg:37.31ms
step:809/2110 train_time:30208ms step_avg:37.34ms
step:810/2110 train_time:30266ms step_avg:37.37ms
step:811/2110 train_time:30328ms step_avg:37.40ms
step:812/2110 train_time:30387ms step_avg:37.42ms
step:813/2110 train_time:30448ms step_avg:37.45ms
step:814/2110 train_time:30507ms step_avg:37.48ms
step:815/2110 train_time:30568ms step_avg:37.51ms
step:816/2110 train_time:30627ms step_avg:37.53ms
step:817/2110 train_time:30688ms step_avg:37.56ms
step:818/2110 train_time:30747ms step_avg:37.59ms
step:819/2110 train_time:30808ms step_avg:37.62ms
step:820/2110 train_time:30867ms step_avg:37.64ms
step:821/2110 train_time:30928ms step_avg:37.67ms
step:822/2110 train_time:30986ms step_avg:37.70ms
step:823/2110 train_time:31047ms step_avg:37.72ms
step:824/2110 train_time:31106ms step_avg:37.75ms
step:825/2110 train_time:31167ms step_avg:37.78ms
step:826/2110 train_time:31226ms step_avg:37.80ms
step:827/2110 train_time:31286ms step_avg:37.83ms
step:828/2110 train_time:31345ms step_avg:37.86ms
step:829/2110 train_time:31407ms step_avg:37.89ms
step:830/2110 train_time:31466ms step_avg:37.91ms
step:831/2110 train_time:31526ms step_avg:37.94ms
step:832/2110 train_time:31585ms step_avg:37.96ms
step:833/2110 train_time:31646ms step_avg:37.99ms
step:834/2110 train_time:31706ms step_avg:38.02ms
step:835/2110 train_time:31767ms step_avg:38.04ms
step:836/2110 train_time:31826ms step_avg:38.07ms
step:837/2110 train_time:31887ms step_avg:38.10ms
step:838/2110 train_time:31946ms step_avg:38.12ms
step:839/2110 train_time:32008ms step_avg:38.15ms
step:840/2110 train_time:32067ms step_avg:38.18ms
step:841/2110 train_time:32127ms step_avg:38.20ms
step:842/2110 train_time:32186ms step_avg:38.23ms
step:843/2110 train_time:32247ms step_avg:38.25ms
step:844/2110 train_time:32306ms step_avg:38.28ms
step:845/2110 train_time:32366ms step_avg:38.30ms
step:846/2110 train_time:32425ms step_avg:38.33ms
step:847/2110 train_time:32486ms step_avg:38.35ms
step:848/2110 train_time:32545ms step_avg:38.38ms
step:849/2110 train_time:32607ms step_avg:38.41ms
step:850/2110 train_time:32666ms step_avg:38.43ms
step:851/2110 train_time:32727ms step_avg:38.46ms
step:852/2110 train_time:32787ms step_avg:38.48ms
step:853/2110 train_time:32848ms step_avg:38.51ms
step:854/2110 train_time:32908ms step_avg:38.53ms
step:855/2110 train_time:32969ms step_avg:38.56ms
step:856/2110 train_time:33028ms step_avg:38.58ms
step:857/2110 train_time:33089ms step_avg:38.61ms
step:858/2110 train_time:33148ms step_avg:38.63ms
step:859/2110 train_time:33209ms step_avg:38.66ms
step:860/2110 train_time:33268ms step_avg:38.68ms
step:861/2110 train_time:33329ms step_avg:38.71ms
step:862/2110 train_time:33388ms step_avg:38.73ms
step:863/2110 train_time:33449ms step_avg:38.76ms
step:864/2110 train_time:33508ms step_avg:38.78ms
step:865/2110 train_time:33569ms step_avg:38.81ms
step:866/2110 train_time:33628ms step_avg:38.83ms
step:867/2110 train_time:33690ms step_avg:38.86ms
step:868/2110 train_time:33749ms step_avg:38.88ms
step:869/2110 train_time:33810ms step_avg:38.91ms
step:870/2110 train_time:33869ms step_avg:38.93ms
step:871/2110 train_time:33930ms step_avg:38.96ms
step:872/2110 train_time:33989ms step_avg:38.98ms
step:873/2110 train_time:34050ms step_avg:39.00ms
step:874/2110 train_time:34109ms step_avg:39.03ms
step:875/2110 train_time:34170ms step_avg:39.05ms
step:876/2110 train_time:34229ms step_avg:39.07ms
step:877/2110 train_time:34291ms step_avg:39.10ms
step:878/2110 train_time:34350ms step_avg:39.12ms
step:879/2110 train_time:34411ms step_avg:39.15ms
step:880/2110 train_time:34471ms step_avg:39.17ms
step:881/2110 train_time:34532ms step_avg:39.20ms
step:882/2110 train_time:34591ms step_avg:39.22ms
step:883/2110 train_time:34652ms step_avg:39.24ms
step:884/2110 train_time:34711ms step_avg:39.27ms
step:885/2110 train_time:34773ms step_avg:39.29ms
step:886/2110 train_time:34832ms step_avg:39.31ms
step:887/2110 train_time:34893ms step_avg:39.34ms
step:888/2110 train_time:34952ms step_avg:39.36ms
step:889/2110 train_time:35013ms step_avg:39.39ms
step:890/2110 train_time:35073ms step_avg:39.41ms
step:891/2110 train_time:35134ms step_avg:39.43ms
step:892/2110 train_time:35193ms step_avg:39.45ms
step:893/2110 train_time:35255ms step_avg:39.48ms
step:894/2110 train_time:35313ms step_avg:39.50ms
step:895/2110 train_time:35373ms step_avg:39.52ms
step:896/2110 train_time:35433ms step_avg:39.55ms
step:897/2110 train_time:35495ms step_avg:39.57ms
step:898/2110 train_time:35554ms step_avg:39.59ms
step:899/2110 train_time:35616ms step_avg:39.62ms
step:900/2110 train_time:35676ms step_avg:39.64ms
step:901/2110 train_time:35738ms step_avg:39.66ms
step:902/2110 train_time:35797ms step_avg:39.69ms
step:903/2110 train_time:35859ms step_avg:39.71ms
step:904/2110 train_time:35918ms step_avg:39.73ms
step:905/2110 train_time:35979ms step_avg:39.76ms
step:906/2110 train_time:36038ms step_avg:39.78ms
step:907/2110 train_time:36099ms step_avg:39.80ms
step:908/2110 train_time:36158ms step_avg:39.82ms
step:909/2110 train_time:36219ms step_avg:39.85ms
step:910/2110 train_time:36279ms step_avg:39.87ms
step:911/2110 train_time:36340ms step_avg:39.89ms
step:912/2110 train_time:36399ms step_avg:39.91ms
step:913/2110 train_time:36460ms step_avg:39.93ms
step:914/2110 train_time:36520ms step_avg:39.96ms
step:915/2110 train_time:36581ms step_avg:39.98ms
step:916/2110 train_time:36640ms step_avg:40.00ms
step:917/2110 train_time:36701ms step_avg:40.02ms
step:918/2110 train_time:36760ms step_avg:40.04ms
step:919/2110 train_time:36821ms step_avg:40.07ms
step:920/2110 train_time:36880ms step_avg:40.09ms
step:921/2110 train_time:36940ms step_avg:40.11ms
step:922/2110 train_time:37000ms step_avg:40.13ms
step:923/2110 train_time:37060ms step_avg:40.15ms
step:924/2110 train_time:37119ms step_avg:40.17ms
step:925/2110 train_time:37180ms step_avg:40.19ms
step:926/2110 train_time:37239ms step_avg:40.21ms
step:927/2110 train_time:37299ms step_avg:40.24ms
step:928/2110 train_time:37359ms step_avg:40.26ms
step:929/2110 train_time:37420ms step_avg:40.28ms
step:930/2110 train_time:37480ms step_avg:40.30ms
step:931/2110 train_time:37541ms step_avg:40.32ms
step:932/2110 train_time:37600ms step_avg:40.34ms
step:933/2110 train_time:37661ms step_avg:40.37ms
step:934/2110 train_time:37721ms step_avg:40.39ms
step:935/2110 train_time:37781ms step_avg:40.41ms
step:936/2110 train_time:37840ms step_avg:40.43ms
step:937/2110 train_time:37901ms step_avg:40.45ms
step:938/2110 train_time:37960ms step_avg:40.47ms
step:939/2110 train_time:38020ms step_avg:40.49ms
step:940/2110 train_time:38079ms step_avg:40.51ms
step:941/2110 train_time:38140ms step_avg:40.53ms
step:942/2110 train_time:38200ms step_avg:40.55ms
step:943/2110 train_time:38260ms step_avg:40.57ms
step:944/2110 train_time:38320ms step_avg:40.59ms
step:945/2110 train_time:38380ms step_avg:40.61ms
step:946/2110 train_time:38440ms step_avg:40.63ms
step:947/2110 train_time:38501ms step_avg:40.66ms
step:948/2110 train_time:38560ms step_avg:40.67ms
step:949/2110 train_time:38621ms step_avg:40.70ms
step:950/2110 train_time:38680ms step_avg:40.72ms
step:951/2110 train_time:38741ms step_avg:40.74ms
step:952/2110 train_time:38800ms step_avg:40.76ms
step:953/2110 train_time:38861ms step_avg:40.78ms
step:954/2110 train_time:38921ms step_avg:40.80ms
step:955/2110 train_time:38981ms step_avg:40.82ms
step:956/2110 train_time:39041ms step_avg:40.84ms
step:957/2110 train_time:39101ms step_avg:40.86ms
step:958/2110 train_time:39161ms step_avg:40.88ms
step:959/2110 train_time:39221ms step_avg:40.90ms
step:960/2110 train_time:39280ms step_avg:40.92ms
step:961/2110 train_time:39341ms step_avg:40.94ms
step:962/2110 train_time:39401ms step_avg:40.96ms
step:963/2110 train_time:39461ms step_avg:40.98ms
step:964/2110 train_time:39520ms step_avg:41.00ms
step:965/2110 train_time:39581ms step_avg:41.02ms
step:966/2110 train_time:39640ms step_avg:41.04ms
step:967/2110 train_time:39700ms step_avg:41.06ms
step:968/2110 train_time:39760ms step_avg:41.07ms
step:969/2110 train_time:39821ms step_avg:41.10ms
step:970/2110 train_time:39880ms step_avg:41.11ms
step:971/2110 train_time:39941ms step_avg:41.13ms
step:972/2110 train_time:40000ms step_avg:41.15ms
step:973/2110 train_time:40061ms step_avg:41.17ms
step:974/2110 train_time:40119ms step_avg:41.19ms
step:975/2110 train_time:40181ms step_avg:41.21ms
step:976/2110 train_time:40240ms step_avg:41.23ms
step:977/2110 train_time:40301ms step_avg:41.25ms
step:978/2110 train_time:40360ms step_avg:41.27ms
step:979/2110 train_time:40421ms step_avg:41.29ms
step:980/2110 train_time:40480ms step_avg:41.31ms
step:981/2110 train_time:40541ms step_avg:41.33ms
step:982/2110 train_time:40601ms step_avg:41.34ms
step:983/2110 train_time:40661ms step_avg:41.36ms
step:984/2110 train_time:40721ms step_avg:41.38ms
step:985/2110 train_time:40782ms step_avg:41.40ms
step:986/2110 train_time:40841ms step_avg:41.42ms
step:987/2110 train_time:40902ms step_avg:41.44ms
step:988/2110 train_time:40961ms step_avg:41.46ms
step:989/2110 train_time:41022ms step_avg:41.48ms
step:990/2110 train_time:41081ms step_avg:41.50ms
step:991/2110 train_time:41142ms step_avg:41.52ms
step:992/2110 train_time:41201ms step_avg:41.53ms
step:993/2110 train_time:41262ms step_avg:41.55ms
step:994/2110 train_time:41321ms step_avg:41.57ms
step:995/2110 train_time:41382ms step_avg:41.59ms
step:996/2110 train_time:41441ms step_avg:41.61ms
step:997/2110 train_time:41502ms step_avg:41.63ms
step:998/2110 train_time:41563ms step_avg:41.65ms
step:999/2110 train_time:41623ms step_avg:41.66ms
step:1000/2110 train_time:41682ms step_avg:41.68ms
step:1000/2110 val_loss:3.6976 train_time:41745ms step_avg:41.74ms
step:1001/2110 train_time:41765ms step_avg:41.72ms
step:1002/2110 train_time:41804ms step_avg:41.72ms
step:1003/2110 train_time:41867ms step_avg:41.74ms
step:1004/2110 train_time:41930ms step_avg:41.76ms
step:1005/2110 train_time:41990ms step_avg:41.78ms
step:1006/2110 train_time:42051ms step_avg:41.80ms
step:1007/2110 train_time:42111ms step_avg:41.82ms
step:1008/2110 train_time:42170ms step_avg:41.84ms
step:1009/2110 train_time:42230ms step_avg:41.85ms
step:1010/2110 train_time:42289ms step_avg:41.87ms
step:1011/2110 train_time:42350ms step_avg:41.89ms
step:1012/2110 train_time:42409ms step_avg:41.91ms
step:1013/2110 train_time:42469ms step_avg:41.92ms
step:1014/2110 train_time:42529ms step_avg:41.94ms
step:1015/2110 train_time:42591ms step_avg:41.96ms
step:1016/2110 train_time:42650ms step_avg:41.98ms
step:1017/2110 train_time:42713ms step_avg:42.00ms
step:1018/2110 train_time:42775ms step_avg:42.02ms
step:1019/2110 train_time:42839ms step_avg:42.04ms
step:1020/2110 train_time:42898ms step_avg:42.06ms
step:1021/2110 train_time:42959ms step_avg:42.08ms
step:1022/2110 train_time:43019ms step_avg:42.09ms
step:1023/2110 train_time:43080ms step_avg:42.11ms
step:1024/2110 train_time:43139ms step_avg:42.13ms
step:1025/2110 train_time:43200ms step_avg:42.15ms
step:1026/2110 train_time:43260ms step_avg:42.16ms
step:1027/2110 train_time:43321ms step_avg:42.18ms
step:1028/2110 train_time:43380ms step_avg:42.20ms
step:1029/2110 train_time:43440ms step_avg:42.22ms
step:1030/2110 train_time:43499ms step_avg:42.23ms
step:1031/2110 train_time:43559ms step_avg:42.25ms
step:1032/2110 train_time:43618ms step_avg:42.27ms
step:1033/2110 train_time:43680ms step_avg:42.28ms
step:1034/2110 train_time:43740ms step_avg:42.30ms
step:1035/2110 train_time:43802ms step_avg:42.32ms
step:1036/2110 train_time:43861ms step_avg:42.34ms
step:1037/2110 train_time:43922ms step_avg:42.36ms
step:1038/2110 train_time:43982ms step_avg:42.37ms
step:1039/2110 train_time:44042ms step_avg:42.39ms
step:1040/2110 train_time:44102ms step_avg:42.41ms
step:1041/2110 train_time:44162ms step_avg:42.42ms
step:1042/2110 train_time:44221ms step_avg:42.44ms
step:1043/2110 train_time:44282ms step_avg:42.46ms
step:1044/2110 train_time:44342ms step_avg:42.47ms
step:1045/2110 train_time:44403ms step_avg:42.49ms
step:1046/2110 train_time:44463ms step_avg:42.51ms
step:1047/2110 train_time:44524ms step_avg:42.52ms
step:1048/2110 train_time:44583ms step_avg:42.54ms
step:1049/2110 train_time:44645ms step_avg:42.56ms
step:1050/2110 train_time:44704ms step_avg:42.58ms
step:1051/2110 train_time:44765ms step_avg:42.59ms
step:1052/2110 train_time:44824ms step_avg:42.61ms
step:1053/2110 train_time:44885ms step_avg:42.63ms
step:1054/2110 train_time:44945ms step_avg:42.64ms
step:1055/2110 train_time:45006ms step_avg:42.66ms
step:1056/2110 train_time:45065ms step_avg:42.68ms
step:1057/2110 train_time:45127ms step_avg:42.69ms
step:1058/2110 train_time:45186ms step_avg:42.71ms
step:1059/2110 train_time:45248ms step_avg:42.73ms
step:1060/2110 train_time:45307ms step_avg:42.74ms
step:1061/2110 train_time:45367ms step_avg:42.76ms
step:1062/2110 train_time:45427ms step_avg:42.77ms
step:1063/2110 train_time:45487ms step_avg:42.79ms
step:1064/2110 train_time:45546ms step_avg:42.81ms
step:1065/2110 train_time:45607ms step_avg:42.82ms
step:1066/2110 train_time:45666ms step_avg:42.84ms
step:1067/2110 train_time:45727ms step_avg:42.86ms
step:1068/2110 train_time:45787ms step_avg:42.87ms
step:1069/2110 train_time:45848ms step_avg:42.89ms
step:1070/2110 train_time:45907ms step_avg:42.90ms
step:1071/2110 train_time:45969ms step_avg:42.92ms
step:1072/2110 train_time:46028ms step_avg:42.94ms
step:1073/2110 train_time:46090ms step_avg:42.95ms
step:1074/2110 train_time:46149ms step_avg:42.97ms
step:1075/2110 train_time:46210ms step_avg:42.99ms
step:1076/2110 train_time:46271ms step_avg:43.00ms
step:1077/2110 train_time:46333ms step_avg:43.02ms
step:1078/2110 train_time:46392ms step_avg:43.03ms
step:1079/2110 train_time:46453ms step_avg:43.05ms
step:1080/2110 train_time:46512ms step_avg:43.07ms
step:1081/2110 train_time:46574ms step_avg:43.08ms
step:1082/2110 train_time:46632ms step_avg:43.10ms
step:1083/2110 train_time:46693ms step_avg:43.11ms
step:1084/2110 train_time:46753ms step_avg:43.13ms
step:1085/2110 train_time:46814ms step_avg:43.15ms
step:1086/2110 train_time:46874ms step_avg:43.16ms
step:1087/2110 train_time:46936ms step_avg:43.18ms
step:1088/2110 train_time:46995ms step_avg:43.19ms
step:1089/2110 train_time:47057ms step_avg:43.21ms
step:1090/2110 train_time:47116ms step_avg:43.23ms
step:1091/2110 train_time:47177ms step_avg:43.24ms
step:1092/2110 train_time:47236ms step_avg:43.26ms
step:1093/2110 train_time:47297ms step_avg:43.27ms
step:1094/2110 train_time:47357ms step_avg:43.29ms
step:1095/2110 train_time:47417ms step_avg:43.30ms
step:1096/2110 train_time:47476ms step_avg:43.32ms
step:1097/2110 train_time:47537ms step_avg:43.33ms
step:1098/2110 train_time:47597ms step_avg:43.35ms
step:1099/2110 train_time:47658ms step_avg:43.36ms
step:1100/2110 train_time:47717ms step_avg:43.38ms
step:1101/2110 train_time:47778ms step_avg:43.39ms
step:1102/2110 train_time:47838ms step_avg:43.41ms
step:1103/2110 train_time:47899ms step_avg:43.43ms
step:1104/2110 train_time:47958ms step_avg:43.44ms
step:1105/2110 train_time:48019ms step_avg:43.46ms
step:1106/2110 train_time:48078ms step_avg:43.47ms
step:1107/2110 train_time:48139ms step_avg:43.49ms
step:1108/2110 train_time:48198ms step_avg:43.50ms
step:1109/2110 train_time:48259ms step_avg:43.52ms
step:1110/2110 train_time:48317ms step_avg:43.53ms
step:1111/2110 train_time:48378ms step_avg:43.54ms
step:1112/2110 train_time:48438ms step_avg:43.56ms
step:1113/2110 train_time:48499ms step_avg:43.57ms
step:1114/2110 train_time:48558ms step_avg:43.59ms
step:1115/2110 train_time:48619ms step_avg:43.60ms
step:1116/2110 train_time:48678ms step_avg:43.62ms
step:1117/2110 train_time:48738ms step_avg:43.63ms
step:1118/2110 train_time:48798ms step_avg:43.65ms
step:1119/2110 train_time:48860ms step_avg:43.66ms
step:1120/2110 train_time:48919ms step_avg:43.68ms
step:1121/2110 train_time:48980ms step_avg:43.69ms
step:1122/2110 train_time:49039ms step_avg:43.71ms
step:1123/2110 train_time:49099ms step_avg:43.72ms
step:1124/2110 train_time:49159ms step_avg:43.74ms
step:1125/2110 train_time:49219ms step_avg:43.75ms
step:1126/2110 train_time:49278ms step_avg:43.76ms
step:1127/2110 train_time:49339ms step_avg:43.78ms
step:1128/2110 train_time:49399ms step_avg:43.79ms
step:1129/2110 train_time:49460ms step_avg:43.81ms
step:1130/2110 train_time:49519ms step_avg:43.82ms
step:1131/2110 train_time:49581ms step_avg:43.84ms
step:1132/2110 train_time:49641ms step_avg:43.85ms
step:1133/2110 train_time:49701ms step_avg:43.87ms
step:1134/2110 train_time:49760ms step_avg:43.88ms
step:1135/2110 train_time:49822ms step_avg:43.90ms
step:1136/2110 train_time:49882ms step_avg:43.91ms
step:1137/2110 train_time:49944ms step_avg:43.93ms
step:1138/2110 train_time:50003ms step_avg:43.94ms
step:1139/2110 train_time:50065ms step_avg:43.95ms
step:1140/2110 train_time:50123ms step_avg:43.97ms
step:1141/2110 train_time:50184ms step_avg:43.98ms
step:1142/2110 train_time:50243ms step_avg:44.00ms
step:1143/2110 train_time:50304ms step_avg:44.01ms
step:1144/2110 train_time:50363ms step_avg:44.02ms
step:1145/2110 train_time:50424ms step_avg:44.04ms
step:1146/2110 train_time:50483ms step_avg:44.05ms
step:1147/2110 train_time:50544ms step_avg:44.07ms
step:1148/2110 train_time:50603ms step_avg:44.08ms
step:1149/2110 train_time:50664ms step_avg:44.09ms
step:1150/2110 train_time:50723ms step_avg:44.11ms
step:1151/2110 train_time:50785ms step_avg:44.12ms
step:1152/2110 train_time:50845ms step_avg:44.14ms
step:1153/2110 train_time:50906ms step_avg:44.15ms
step:1154/2110 train_time:50965ms step_avg:44.16ms
step:1155/2110 train_time:51026ms step_avg:44.18ms
step:1156/2110 train_time:51085ms step_avg:44.19ms
step:1157/2110 train_time:51146ms step_avg:44.21ms
step:1158/2110 train_time:51205ms step_avg:44.22ms
step:1159/2110 train_time:51266ms step_avg:44.23ms
step:1160/2110 train_time:51325ms step_avg:44.25ms
step:1161/2110 train_time:51386ms step_avg:44.26ms
step:1162/2110 train_time:51446ms step_avg:44.27ms
step:1163/2110 train_time:51508ms step_avg:44.29ms
step:1164/2110 train_time:51567ms step_avg:44.30ms
step:1165/2110 train_time:51629ms step_avg:44.32ms
step:1166/2110 train_time:51688ms step_avg:44.33ms
step:1167/2110 train_time:51750ms step_avg:44.34ms
step:1168/2110 train_time:51809ms step_avg:44.36ms
step:1169/2110 train_time:51871ms step_avg:44.37ms
step:1170/2110 train_time:51930ms step_avg:44.38ms
step:1171/2110 train_time:51991ms step_avg:44.40ms
step:1172/2110 train_time:52051ms step_avg:44.41ms
step:1173/2110 train_time:52112ms step_avg:44.43ms
step:1174/2110 train_time:52171ms step_avg:44.44ms
step:1175/2110 train_time:52233ms step_avg:44.45ms
step:1176/2110 train_time:52292ms step_avg:44.47ms
step:1177/2110 train_time:52355ms step_avg:44.48ms
step:1178/2110 train_time:52414ms step_avg:44.49ms
step:1179/2110 train_time:52475ms step_avg:44.51ms
step:1180/2110 train_time:52535ms step_avg:44.52ms
step:1181/2110 train_time:52596ms step_avg:44.54ms
step:1182/2110 train_time:52655ms step_avg:44.55ms
step:1183/2110 train_time:52717ms step_avg:44.56ms
step:1184/2110 train_time:52776ms step_avg:44.57ms
step:1185/2110 train_time:52837ms step_avg:44.59ms
step:1186/2110 train_time:52897ms step_avg:44.60ms
step:1187/2110 train_time:52959ms step_avg:44.62ms
step:1188/2110 train_time:53019ms step_avg:44.63ms
step:1189/2110 train_time:53080ms step_avg:44.64ms
step:1190/2110 train_time:53139ms step_avg:44.65ms
step:1191/2110 train_time:53200ms step_avg:44.67ms
step:1192/2110 train_time:53261ms step_avg:44.68ms
step:1193/2110 train_time:53322ms step_avg:44.70ms
step:1194/2110 train_time:53381ms step_avg:44.71ms
step:1195/2110 train_time:53442ms step_avg:44.72ms
step:1196/2110 train_time:53501ms step_avg:44.73ms
step:1197/2110 train_time:53562ms step_avg:44.75ms
step:1198/2110 train_time:53621ms step_avg:44.76ms
step:1199/2110 train_time:53682ms step_avg:44.77ms
step:1200/2110 train_time:53741ms step_avg:44.78ms
step:1201/2110 train_time:53802ms step_avg:44.80ms
step:1202/2110 train_time:53861ms step_avg:44.81ms
step:1203/2110 train_time:53922ms step_avg:44.82ms
step:1204/2110 train_time:53981ms step_avg:44.83ms
step:1205/2110 train_time:54042ms step_avg:44.85ms
step:1206/2110 train_time:54101ms step_avg:44.86ms
step:1207/2110 train_time:54162ms step_avg:44.87ms
step:1208/2110 train_time:54222ms step_avg:44.89ms
step:1209/2110 train_time:54283ms step_avg:44.90ms
step:1210/2110 train_time:54342ms step_avg:44.91ms
step:1211/2110 train_time:54403ms step_avg:44.92ms
step:1212/2110 train_time:54462ms step_avg:44.94ms
step:1213/2110 train_time:54523ms step_avg:44.95ms
step:1214/2110 train_time:54582ms step_avg:44.96ms
step:1215/2110 train_time:54643ms step_avg:44.97ms
step:1216/2110 train_time:54702ms step_avg:44.99ms
step:1217/2110 train_time:54762ms step_avg:45.00ms
step:1218/2110 train_time:54822ms step_avg:45.01ms
step:1219/2110 train_time:54883ms step_avg:45.02ms
step:1220/2110 train_time:54943ms step_avg:45.04ms
step:1221/2110 train_time:55004ms step_avg:45.05ms
step:1222/2110 train_time:55064ms step_avg:45.06ms
step:1223/2110 train_time:55124ms step_avg:45.07ms
step:1224/2110 train_time:55184ms step_avg:45.08ms
step:1225/2110 train_time:55245ms step_avg:45.10ms
step:1226/2110 train_time:55304ms step_avg:45.11ms
step:1227/2110 train_time:55366ms step_avg:45.12ms
step:1228/2110 train_time:55425ms step_avg:45.13ms
step:1229/2110 train_time:55486ms step_avg:45.15ms
step:1230/2110 train_time:55545ms step_avg:45.16ms
step:1231/2110 train_time:55606ms step_avg:45.17ms
step:1232/2110 train_time:55665ms step_avg:45.18ms
step:1233/2110 train_time:55727ms step_avg:45.20ms
step:1234/2110 train_time:55787ms step_avg:45.21ms
step:1235/2110 train_time:55849ms step_avg:45.22ms
step:1236/2110 train_time:55908ms step_avg:45.23ms
step:1237/2110 train_time:55969ms step_avg:45.25ms
step:1238/2110 train_time:56028ms step_avg:45.26ms
step:1239/2110 train_time:56089ms step_avg:45.27ms
step:1240/2110 train_time:56148ms step_avg:45.28ms
step:1241/2110 train_time:56210ms step_avg:45.29ms
step:1242/2110 train_time:56269ms step_avg:45.30ms
step:1243/2110 train_time:56330ms step_avg:45.32ms
step:1244/2110 train_time:56390ms step_avg:45.33ms
step:1245/2110 train_time:56452ms step_avg:45.34ms
step:1246/2110 train_time:56511ms step_avg:45.35ms
step:1247/2110 train_time:56573ms step_avg:45.37ms
step:1248/2110 train_time:56632ms step_avg:45.38ms
step:1249/2110 train_time:56694ms step_avg:45.39ms
step:1250/2110 train_time:56753ms step_avg:45.40ms
step:1250/2110 val_loss:3.5831 train_time:56817ms step_avg:45.45ms
step:1251/2110 train_time:56839ms step_avg:45.43ms
step:1252/2110 train_time:56878ms step_avg:45.43ms
step:1253/2110 train_time:56942ms step_avg:45.44ms
step:1254/2110 train_time:57005ms step_avg:45.46ms
step:1255/2110 train_time:57066ms step_avg:45.47ms
step:1256/2110 train_time:57125ms step_avg:45.48ms
step:1257/2110 train_time:57186ms step_avg:45.49ms
step:1258/2110 train_time:57244ms step_avg:45.50ms
step:1259/2110 train_time:57305ms step_avg:45.52ms
step:1260/2110 train_time:57363ms step_avg:45.53ms
step:1261/2110 train_time:57424ms step_avg:45.54ms
step:1262/2110 train_time:57482ms step_avg:45.55ms
step:1263/2110 train_time:57542ms step_avg:45.56ms
step:1264/2110 train_time:57601ms step_avg:45.57ms
step:1265/2110 train_time:57661ms step_avg:45.58ms
step:1266/2110 train_time:57720ms step_avg:45.59ms
step:1267/2110 train_time:57782ms step_avg:45.61ms
step:1268/2110 train_time:57843ms step_avg:45.62ms
step:1269/2110 train_time:57906ms step_avg:45.63ms
step:1270/2110 train_time:57966ms step_avg:45.64ms
step:1271/2110 train_time:58027ms step_avg:45.65ms
step:1272/2110 train_time:58086ms step_avg:45.67ms
step:1273/2110 train_time:58148ms step_avg:45.68ms
step:1274/2110 train_time:58207ms step_avg:45.69ms
step:1275/2110 train_time:58267ms step_avg:45.70ms
step:1276/2110 train_time:58326ms step_avg:45.71ms
step:1277/2110 train_time:58386ms step_avg:45.72ms
step:1278/2110 train_time:58445ms step_avg:45.73ms
step:1279/2110 train_time:58505ms step_avg:45.74ms
step:1280/2110 train_time:58564ms step_avg:45.75ms
step:1281/2110 train_time:58624ms step_avg:45.76ms
step:1282/2110 train_time:58684ms step_avg:45.78ms
step:1283/2110 train_time:58745ms step_avg:45.79ms
step:1284/2110 train_time:58806ms step_avg:45.80ms
step:1285/2110 train_time:58867ms step_avg:45.81ms
step:1286/2110 train_time:58928ms step_avg:45.82ms
step:1287/2110 train_time:58989ms step_avg:45.83ms
step:1288/2110 train_time:59049ms step_avg:45.85ms
step:1289/2110 train_time:59110ms step_avg:45.86ms
step:1290/2110 train_time:59169ms step_avg:45.87ms
step:1291/2110 train_time:59230ms step_avg:45.88ms
step:1292/2110 train_time:59290ms step_avg:45.89ms
step:1293/2110 train_time:59349ms step_avg:45.90ms
step:1294/2110 train_time:59408ms step_avg:45.91ms
step:1295/2110 train_time:59468ms step_avg:45.92ms
step:1296/2110 train_time:59526ms step_avg:45.93ms
step:1297/2110 train_time:59587ms step_avg:45.94ms
step:1298/2110 train_time:59647ms step_avg:45.95ms
step:1299/2110 train_time:59708ms step_avg:45.96ms
step:1300/2110 train_time:59767ms step_avg:45.97ms
step:1301/2110 train_time:59828ms step_avg:45.99ms
step:1302/2110 train_time:59888ms step_avg:46.00ms
step:1303/2110 train_time:59949ms step_avg:46.01ms
step:1304/2110 train_time:60010ms step_avg:46.02ms
step:1305/2110 train_time:60070ms step_avg:46.03ms
step:1306/2110 train_time:60129ms step_avg:46.04ms
step:1307/2110 train_time:60191ms step_avg:46.05ms
step:1308/2110 train_time:60250ms step_avg:46.06ms
step:1309/2110 train_time:60310ms step_avg:46.07ms
step:1310/2110 train_time:60369ms step_avg:46.08ms
step:1311/2110 train_time:60429ms step_avg:46.09ms
step:1312/2110 train_time:60488ms step_avg:46.10ms
step:1313/2110 train_time:60548ms step_avg:46.11ms
step:1314/2110 train_time:60608ms step_avg:46.12ms
step:1315/2110 train_time:60669ms step_avg:46.14ms
step:1316/2110 train_time:60729ms step_avg:46.15ms
step:1317/2110 train_time:60791ms step_avg:46.16ms
step:1318/2110 train_time:60850ms step_avg:46.17ms
step:1319/2110 train_time:60911ms step_avg:46.18ms
step:1320/2110 train_time:60971ms step_avg:46.19ms
step:1321/2110 train_time:61032ms step_avg:46.20ms
step:1322/2110 train_time:61092ms step_avg:46.21ms
step:1323/2110 train_time:61152ms step_avg:46.22ms
step:1324/2110 train_time:61211ms step_avg:46.23ms
step:1325/2110 train_time:61271ms step_avg:46.24ms
step:1326/2110 train_time:61330ms step_avg:46.25ms
step:1327/2110 train_time:61391ms step_avg:46.26ms
step:1328/2110 train_time:61450ms step_avg:46.27ms
step:1329/2110 train_time:61510ms step_avg:46.28ms
step:1330/2110 train_time:61569ms step_avg:46.29ms
step:1331/2110 train_time:61631ms step_avg:46.30ms
step:1332/2110 train_time:61691ms step_avg:46.31ms
step:1333/2110 train_time:61752ms step_avg:46.33ms
step:1334/2110 train_time:61811ms step_avg:46.34ms
step:1335/2110 train_time:61872ms step_avg:46.35ms
step:1336/2110 train_time:61932ms step_avg:46.36ms
step:1337/2110 train_time:61993ms step_avg:46.37ms
step:1338/2110 train_time:62052ms step_avg:46.38ms
step:1339/2110 train_time:62113ms step_avg:46.39ms
step:1340/2110 train_time:62172ms step_avg:46.40ms
step:1341/2110 train_time:62232ms step_avg:46.41ms
step:1342/2110 train_time:62291ms step_avg:46.42ms
step:1343/2110 train_time:62353ms step_avg:46.43ms
step:1344/2110 train_time:62412ms step_avg:46.44ms
step:1345/2110 train_time:62474ms step_avg:46.45ms
step:1346/2110 train_time:62533ms step_avg:46.46ms
step:1347/2110 train_time:62595ms step_avg:46.47ms
step:1348/2110 train_time:62654ms step_avg:46.48ms
step:1349/2110 train_time:62715ms step_avg:46.49ms
step:1350/2110 train_time:62775ms step_avg:46.50ms
step:1351/2110 train_time:62836ms step_avg:46.51ms
step:1352/2110 train_time:62896ms step_avg:46.52ms
step:1353/2110 train_time:62957ms step_avg:46.53ms
step:1354/2110 train_time:63016ms step_avg:46.54ms
step:1355/2110 train_time:63077ms step_avg:46.55ms
step:1356/2110 train_time:63136ms step_avg:46.56ms
step:1357/2110 train_time:63197ms step_avg:46.57ms
step:1358/2110 train_time:63256ms step_avg:46.58ms
step:1359/2110 train_time:63318ms step_avg:46.59ms
step:1360/2110 train_time:63378ms step_avg:46.60ms
step:1361/2110 train_time:63439ms step_avg:46.61ms
step:1362/2110 train_time:63499ms step_avg:46.62ms
step:1363/2110 train_time:63561ms step_avg:46.63ms
step:1364/2110 train_time:63620ms step_avg:46.64ms
step:1365/2110 train_time:63681ms step_avg:46.65ms
step:1366/2110 train_time:63740ms step_avg:46.66ms
step:1367/2110 train_time:63801ms step_avg:46.67ms
step:1368/2110 train_time:63860ms step_avg:46.68ms
step:1369/2110 train_time:63921ms step_avg:46.69ms
step:1370/2110 train_time:63981ms step_avg:46.70ms
step:1371/2110 train_time:64041ms step_avg:46.71ms
step:1372/2110 train_time:64101ms step_avg:46.72ms
step:1373/2110 train_time:64163ms step_avg:46.73ms
step:1374/2110 train_time:64223ms step_avg:46.74ms
step:1375/2110 train_time:64283ms step_avg:46.75ms
step:1376/2110 train_time:64342ms step_avg:46.76ms
step:1377/2110 train_time:64403ms step_avg:46.77ms
step:1378/2110 train_time:64463ms step_avg:46.78ms
step:1379/2110 train_time:64524ms step_avg:46.79ms
step:1380/2110 train_time:64584ms step_avg:46.80ms
step:1381/2110 train_time:64645ms step_avg:46.81ms
step:1382/2110 train_time:64735ms step_avg:46.84ms
step:1383/2110 train_time:64823ms step_avg:46.87ms
step:1384/2110 train_time:64911ms step_avg:46.90ms
step:1385/2110 train_time:64999ms step_avg:46.93ms
step:1386/2110 train_time:65086ms step_avg:46.96ms
step:1387/2110 train_time:65175ms step_avg:46.99ms
step:1388/2110 train_time:65263ms step_avg:47.02ms
step:1389/2110 train_time:65350ms step_avg:47.05ms
step:1390/2110 train_time:65437ms step_avg:47.08ms
step:1391/2110 train_time:65526ms step_avg:47.11ms
step:1392/2110 train_time:65613ms step_avg:47.14ms
step:1393/2110 train_time:65701ms step_avg:47.17ms
step:1394/2110 train_time:65788ms step_avg:47.19ms
step:1395/2110 train_time:65877ms step_avg:47.22ms
step:1396/2110 train_time:65964ms step_avg:47.25ms
step:1397/2110 train_time:66054ms step_avg:47.28ms
step:1398/2110 train_time:66141ms step_avg:47.31ms
step:1399/2110 train_time:66230ms step_avg:47.34ms
step:1400/2110 train_time:66317ms step_avg:47.37ms
step:1401/2110 train_time:66406ms step_avg:47.40ms
step:1402/2110 train_time:66493ms step_avg:47.43ms
step:1403/2110 train_time:66582ms step_avg:47.46ms
step:1404/2110 train_time:66670ms step_avg:47.49ms
step:1405/2110 train_time:66759ms step_avg:47.52ms
step:1406/2110 train_time:66846ms step_avg:47.54ms
step:1407/2110 train_time:66935ms step_avg:47.57ms
step:1408/2110 train_time:67021ms step_avg:47.60ms
step:1409/2110 train_time:67110ms step_avg:47.63ms
step:1410/2110 train_time:67197ms step_avg:47.66ms
step:1411/2110 train_time:67285ms step_avg:47.69ms
step:1412/2110 train_time:67373ms step_avg:47.71ms
step:1413/2110 train_time:67461ms step_avg:47.74ms
step:1414/2110 train_time:67548ms step_avg:47.77ms
step:1415/2110 train_time:67637ms step_avg:47.80ms
step:1416/2110 train_time:67724ms step_avg:47.83ms
step:1417/2110 train_time:67813ms step_avg:47.86ms
step:1418/2110 train_time:67900ms step_avg:47.88ms
step:1419/2110 train_time:67988ms step_avg:47.91ms
step:1420/2110 train_time:68075ms step_avg:47.94ms
step:1421/2110 train_time:68164ms step_avg:47.97ms
step:1422/2110 train_time:68251ms step_avg:48.00ms
step:1423/2110 train_time:68340ms step_avg:48.03ms
step:1424/2110 train_time:68427ms step_avg:48.05ms
step:1425/2110 train_time:68516ms step_avg:48.08ms
step:1426/2110 train_time:68602ms step_avg:48.11ms
step:1427/2110 train_time:68692ms step_avg:48.14ms
step:1428/2110 train_time:68778ms step_avg:48.16ms
step:1429/2110 train_time:68867ms step_avg:48.19ms
step:1430/2110 train_time:68954ms step_avg:48.22ms
step:1431/2110 train_time:69043ms step_avg:48.25ms
step:1432/2110 train_time:69130ms step_avg:48.28ms
step:1433/2110 train_time:69219ms step_avg:48.30ms
step:1434/2110 train_time:69307ms step_avg:48.33ms
step:1435/2110 train_time:69396ms step_avg:48.36ms
step:1436/2110 train_time:69482ms step_avg:48.39ms
step:1437/2110 train_time:69571ms step_avg:48.41ms
step:1438/2110 train_time:69658ms step_avg:48.44ms
step:1439/2110 train_time:69747ms step_avg:48.47ms
step:1440/2110 train_time:69834ms step_avg:48.50ms
step:1441/2110 train_time:69924ms step_avg:48.52ms
step:1442/2110 train_time:70010ms step_avg:48.55ms
step:1443/2110 train_time:70099ms step_avg:48.58ms
step:1444/2110 train_time:70187ms step_avg:48.61ms
step:1445/2110 train_time:70276ms step_avg:48.63ms
step:1446/2110 train_time:70362ms step_avg:48.66ms
step:1447/2110 train_time:70452ms step_avg:48.69ms
step:1448/2110 train_time:70539ms step_avg:48.71ms
step:1449/2110 train_time:70627ms step_avg:48.74ms
step:1450/2110 train_time:70714ms step_avg:48.77ms
step:1451/2110 train_time:70802ms step_avg:48.80ms
step:1452/2110 train_time:70889ms step_avg:48.82ms
step:1453/2110 train_time:70978ms step_avg:48.85ms
step:1454/2110 train_time:71065ms step_avg:48.88ms
step:1455/2110 train_time:71155ms step_avg:48.90ms
step:1456/2110 train_time:71242ms step_avg:48.93ms
step:1457/2110 train_time:71331ms step_avg:48.96ms
step:1458/2110 train_time:71418ms step_avg:48.98ms
step:1459/2110 train_time:71507ms step_avg:49.01ms
step:1460/2110 train_time:71593ms step_avg:49.04ms
step:1461/2110 train_time:71682ms step_avg:49.06ms
step:1462/2110 train_time:71769ms step_avg:49.09ms
step:1463/2110 train_time:71858ms step_avg:49.12ms
step:1464/2110 train_time:71946ms step_avg:49.14ms
step:1465/2110 train_time:72036ms step_avg:49.17ms
step:1466/2110 train_time:72122ms step_avg:49.20ms
step:1467/2110 train_time:72212ms step_avg:49.22ms
step:1468/2110 train_time:72299ms step_avg:49.25ms
step:1469/2110 train_time:72388ms step_avg:49.28ms
step:1470/2110 train_time:72475ms step_avg:49.30ms
step:1471/2110 train_time:72564ms step_avg:49.33ms
step:1472/2110 train_time:72651ms step_avg:49.36ms
step:1473/2110 train_time:72740ms step_avg:49.38ms
step:1474/2110 train_time:72827ms step_avg:49.41ms
step:1475/2110 train_time:72916ms step_avg:49.43ms
step:1476/2110 train_time:73003ms step_avg:49.46ms
step:1477/2110 train_time:73093ms step_avg:49.49ms
step:1478/2110 train_time:73180ms step_avg:49.51ms
step:1479/2110 train_time:73268ms step_avg:49.54ms
step:1480/2110 train_time:73355ms step_avg:49.56ms
step:1481/2110 train_time:73444ms step_avg:49.59ms
step:1482/2110 train_time:73532ms step_avg:49.62ms
step:1483/2110 train_time:73620ms step_avg:49.64ms
step:1484/2110 train_time:73707ms step_avg:49.67ms
step:1485/2110 train_time:73795ms step_avg:49.69ms
step:1486/2110 train_time:73882ms step_avg:49.72ms
step:1487/2110 train_time:73970ms step_avg:49.74ms
step:1488/2110 train_time:74057ms step_avg:49.77ms
step:1489/2110 train_time:74147ms step_avg:49.80ms
step:1490/2110 train_time:74234ms step_avg:49.82ms
step:1491/2110 train_time:74323ms step_avg:49.85ms
step:1492/2110 train_time:74410ms step_avg:49.87ms
step:1493/2110 train_time:74499ms step_avg:49.90ms
step:1494/2110 train_time:74586ms step_avg:49.92ms
step:1495/2110 train_time:74674ms step_avg:49.95ms
step:1496/2110 train_time:74760ms step_avg:49.97ms
step:1497/2110 train_time:74849ms step_avg:50.00ms
step:1498/2110 train_time:74936ms step_avg:50.02ms
step:1499/2110 train_time:75025ms step_avg:50.05ms
step:1500/2110 train_time:75113ms step_avg:50.08ms
step:1500/2110 val_loss:3.4724 train_time:75203ms step_avg:50.14ms
step:1501/2110 train_time:75225ms step_avg:50.12ms
step:1502/2110 train_time:75293ms step_avg:50.13ms
step:1503/2110 train_time:75388ms step_avg:50.16ms
step:1504/2110 train_time:75475ms step_avg:50.18ms
step:1505/2110 train_time:75564ms step_avg:50.21ms
step:1506/2110 train_time:75650ms step_avg:50.23ms
step:1507/2110 train_time:75737ms step_avg:50.26ms
step:1508/2110 train_time:75823ms step_avg:50.28ms
step:1509/2110 train_time:75911ms step_avg:50.31ms
step:1510/2110 train_time:75997ms step_avg:50.33ms
step:1511/2110 train_time:76085ms step_avg:50.35ms
step:1512/2110 train_time:76173ms step_avg:50.38ms
step:1513/2110 train_time:76265ms step_avg:50.41ms
step:1514/2110 train_time:76354ms step_avg:50.43ms
step:1515/2110 train_time:76444ms step_avg:50.46ms
step:1516/2110 train_time:76531ms step_avg:50.48ms
step:1517/2110 train_time:76619ms step_avg:50.51ms
step:1518/2110 train_time:76706ms step_avg:50.53ms
step:1519/2110 train_time:76794ms step_avg:50.56ms
step:1520/2110 train_time:76880ms step_avg:50.58ms
step:1521/2110 train_time:76968ms step_avg:50.60ms
step:1522/2110 train_time:77055ms step_avg:50.63ms
step:1523/2110 train_time:77144ms step_avg:50.65ms
step:1524/2110 train_time:77232ms step_avg:50.68ms
step:1525/2110 train_time:77322ms step_avg:50.70ms
step:1526/2110 train_time:77410ms step_avg:50.73ms
step:1527/2110 train_time:77499ms step_avg:50.75ms
step:1528/2110 train_time:77586ms step_avg:50.78ms
step:1529/2110 train_time:77675ms step_avg:50.80ms
step:1530/2110 train_time:77762ms step_avg:50.82ms
step:1531/2110 train_time:77849ms step_avg:50.85ms
step:1532/2110 train_time:77935ms step_avg:50.87ms
step:1533/2110 train_time:78023ms step_avg:50.90ms
step:1534/2110 train_time:78110ms step_avg:50.92ms
step:1535/2110 train_time:78199ms step_avg:50.94ms
step:1536/2110 train_time:78287ms step_avg:50.97ms
step:1537/2110 train_time:78377ms step_avg:50.99ms
step:1538/2110 train_time:78465ms step_avg:51.02ms
step:1539/2110 train_time:78555ms step_avg:51.04ms
step:1540/2110 train_time:78643ms step_avg:51.07ms
step:1541/2110 train_time:78732ms step_avg:51.09ms
step:1542/2110 train_time:78818ms step_avg:51.11ms
step:1543/2110 train_time:78906ms step_avg:51.14ms
step:1544/2110 train_time:78992ms step_avg:51.16ms
step:1545/2110 train_time:79080ms step_avg:51.18ms
step:1546/2110 train_time:79168ms step_avg:51.21ms
step:1547/2110 train_time:79258ms step_avg:51.23ms
step:1548/2110 train_time:79344ms step_avg:51.26ms
step:1549/2110 train_time:79434ms step_avg:51.28ms
step:1550/2110 train_time:79521ms step_avg:51.30ms
step:1551/2110 train_time:79612ms step_avg:51.33ms
step:1552/2110 train_time:79698ms step_avg:51.35ms
step:1553/2110 train_time:79787ms step_avg:51.38ms
step:1554/2110 train_time:79873ms step_avg:51.40ms
step:1555/2110 train_time:79962ms step_avg:51.42ms
step:1556/2110 train_time:80049ms step_avg:51.45ms
step:1557/2110 train_time:80137ms step_avg:51.47ms
step:1558/2110 train_time:80224ms step_avg:51.49ms
step:1559/2110 train_time:80314ms step_avg:51.52ms
step:1560/2110 train_time:80402ms step_avg:51.54ms
step:1561/2110 train_time:80493ms step_avg:51.56ms
step:1562/2110 train_time:80580ms step_avg:51.59ms
step:1563/2110 train_time:80669ms step_avg:51.61ms
step:1564/2110 train_time:80755ms step_avg:51.63ms
step:1565/2110 train_time:80844ms step_avg:51.66ms
step:1566/2110 train_time:80930ms step_avg:51.68ms
step:1567/2110 train_time:81018ms step_avg:51.70ms
step:1568/2110 train_time:81105ms step_avg:51.73ms
step:1569/2110 train_time:81194ms step_avg:51.75ms
step:1570/2110 train_time:81282ms step_avg:51.77ms
step:1571/2110 train_time:81372ms step_avg:51.80ms
step:1572/2110 train_time:81460ms step_avg:51.82ms
step:1573/2110 train_time:81550ms step_avg:51.84ms
step:1574/2110 train_time:81636ms step_avg:51.87ms
step:1575/2110 train_time:81725ms step_avg:51.89ms
step:1576/2110 train_time:81811ms step_avg:51.91ms
step:1577/2110 train_time:81899ms step_avg:51.93ms
step:1578/2110 train_time:81987ms step_avg:51.96ms
step:1579/2110 train_time:82076ms step_avg:51.98ms
step:1580/2110 train_time:82164ms step_avg:52.00ms
step:1581/2110 train_time:82253ms step_avg:52.03ms
step:1582/2110 train_time:82340ms step_avg:52.05ms
step:1583/2110 train_time:82429ms step_avg:52.07ms
step:1584/2110 train_time:82516ms step_avg:52.09ms
step:1585/2110 train_time:82605ms step_avg:52.12ms
step:1586/2110 train_time:82693ms step_avg:52.14ms
step:1587/2110 train_time:82781ms step_avg:52.16ms
step:1588/2110 train_time:82868ms step_avg:52.18ms
step:1589/2110 train_time:82956ms step_avg:52.21ms
step:1590/2110 train_time:83043ms step_avg:52.23ms
step:1591/2110 train_time:83133ms step_avg:52.25ms
step:1592/2110 train_time:83220ms step_avg:52.27ms
step:1593/2110 train_time:83310ms step_avg:52.30ms
step:1594/2110 train_time:83397ms step_avg:52.32ms
step:1595/2110 train_time:83486ms step_avg:52.34ms
step:1596/2110 train_time:83573ms step_avg:52.36ms
step:1597/2110 train_time:83661ms step_avg:52.39ms
step:1598/2110 train_time:83748ms step_avg:52.41ms
step:1599/2110 train_time:83837ms step_avg:52.43ms
step:1600/2110 train_time:83924ms step_avg:52.45ms
step:1601/2110 train_time:84013ms step_avg:52.48ms
step:1602/2110 train_time:84101ms step_avg:52.50ms
step:1603/2110 train_time:84188ms step_avg:52.52ms
step:1604/2110 train_time:84275ms step_avg:52.54ms
step:1605/2110 train_time:84364ms step_avg:52.56ms
step:1606/2110 train_time:84451ms step_avg:52.58ms
step:1607/2110 train_time:84540ms step_avg:52.61ms
step:1608/2110 train_time:84627ms step_avg:52.63ms
step:1609/2110 train_time:84716ms step_avg:52.65ms
step:1610/2110 train_time:84803ms step_avg:52.67ms
step:1611/2110 train_time:84892ms step_avg:52.70ms
step:1612/2110 train_time:84979ms step_avg:52.72ms
step:1613/2110 train_time:85068ms step_avg:52.74ms
step:1614/2110 train_time:85156ms step_avg:52.76ms
step:1615/2110 train_time:85244ms step_avg:52.78ms
step:1616/2110 train_time:85332ms step_avg:52.80ms
step:1617/2110 train_time:85420ms step_avg:52.83ms
step:1618/2110 train_time:85507ms step_avg:52.85ms
step:1619/2110 train_time:85596ms step_avg:52.87ms
step:1620/2110 train_time:85684ms step_avg:52.89ms
step:1621/2110 train_time:85773ms step_avg:52.91ms
step:1622/2110 train_time:85860ms step_avg:52.93ms
step:1623/2110 train_time:85948ms step_avg:52.96ms
step:1624/2110 train_time:86036ms step_avg:52.98ms
step:1625/2110 train_time:86124ms step_avg:53.00ms
step:1626/2110 train_time:86211ms step_avg:53.02ms
step:1627/2110 train_time:86300ms step_avg:53.04ms
step:1628/2110 train_time:86388ms step_avg:53.06ms
step:1629/2110 train_time:86477ms step_avg:53.09ms
step:1630/2110 train_time:86564ms step_avg:53.11ms
step:1631/2110 train_time:86653ms step_avg:53.13ms
step:1632/2110 train_time:86740ms step_avg:53.15ms
step:1633/2110 train_time:86829ms step_avg:53.17ms
step:1634/2110 train_time:86916ms step_avg:53.19ms
step:1635/2110 train_time:87005ms step_avg:53.21ms
step:1636/2110 train_time:87092ms step_avg:53.23ms
step:1637/2110 train_time:87180ms step_avg:53.26ms
step:1638/2110 train_time:87267ms step_avg:53.28ms
step:1639/2110 train_time:87356ms step_avg:53.30ms
step:1640/2110 train_time:87443ms step_avg:53.32ms
step:1641/2110 train_time:87533ms step_avg:53.34ms
step:1642/2110 train_time:87620ms step_avg:53.36ms
step:1643/2110 train_time:87708ms step_avg:53.38ms
step:1644/2110 train_time:87795ms step_avg:53.40ms
step:1645/2110 train_time:87884ms step_avg:53.42ms
step:1646/2110 train_time:87971ms step_avg:53.45ms
step:1647/2110 train_time:88060ms step_avg:53.47ms
step:1648/2110 train_time:88147ms step_avg:53.49ms
step:1649/2110 train_time:88237ms step_avg:53.51ms
step:1650/2110 train_time:88324ms step_avg:53.53ms
step:1651/2110 train_time:88415ms step_avg:53.55ms
step:1652/2110 train_time:88502ms step_avg:53.57ms
step:1653/2110 train_time:88592ms step_avg:53.59ms
step:1654/2110 train_time:88679ms step_avg:53.61ms
step:1655/2110 train_time:88768ms step_avg:53.64ms
step:1656/2110 train_time:88854ms step_avg:53.66ms
step:1657/2110 train_time:88944ms step_avg:53.68ms
step:1658/2110 train_time:89031ms step_avg:53.70ms
step:1659/2110 train_time:89120ms step_avg:53.72ms
step:1660/2110 train_time:89207ms step_avg:53.74ms
step:1661/2110 train_time:89297ms step_avg:53.76ms
step:1662/2110 train_time:89384ms step_avg:53.78ms
step:1663/2110 train_time:89474ms step_avg:53.80ms
step:1664/2110 train_time:89560ms step_avg:53.82ms
step:1665/2110 train_time:89649ms step_avg:53.84ms
step:1666/2110 train_time:89736ms step_avg:53.86ms
step:1667/2110 train_time:89824ms step_avg:53.88ms
step:1668/2110 train_time:89912ms step_avg:53.90ms
step:1669/2110 train_time:90000ms step_avg:53.92ms
step:1670/2110 train_time:90087ms step_avg:53.94ms
step:1671/2110 train_time:90176ms step_avg:53.97ms
step:1672/2110 train_time:90264ms step_avg:53.99ms
step:1673/2110 train_time:90352ms step_avg:54.01ms
step:1674/2110 train_time:90439ms step_avg:54.03ms
step:1675/2110 train_time:90529ms step_avg:54.05ms
step:1676/2110 train_time:90616ms step_avg:54.07ms
step:1677/2110 train_time:90705ms step_avg:54.09ms
step:1678/2110 train_time:90793ms step_avg:54.11ms
step:1679/2110 train_time:90882ms step_avg:54.13ms
step:1680/2110 train_time:90970ms step_avg:54.15ms
step:1681/2110 train_time:91058ms step_avg:54.17ms
step:1682/2110 train_time:91145ms step_avg:54.19ms
step:1683/2110 train_time:91234ms step_avg:54.21ms
step:1684/2110 train_time:91321ms step_avg:54.23ms
step:1685/2110 train_time:91410ms step_avg:54.25ms
step:1686/2110 train_time:91497ms step_avg:54.27ms
step:1687/2110 train_time:91586ms step_avg:54.29ms
step:1688/2110 train_time:91673ms step_avg:54.31ms
step:1689/2110 train_time:91761ms step_avg:54.33ms
step:1690/2110 train_time:91849ms step_avg:54.35ms
step:1691/2110 train_time:91938ms step_avg:54.37ms
step:1692/2110 train_time:92025ms step_avg:54.39ms
step:1693/2110 train_time:92115ms step_avg:54.41ms
step:1694/2110 train_time:92202ms step_avg:54.43ms
step:1695/2110 train_time:92291ms step_avg:54.45ms
step:1696/2110 train_time:92378ms step_avg:54.47ms
step:1697/2110 train_time:92466ms step_avg:54.49ms
step:1698/2110 train_time:92553ms step_avg:54.51ms
step:1699/2110 train_time:92642ms step_avg:54.53ms
step:1700/2110 train_time:92730ms step_avg:54.55ms
step:1701/2110 train_time:92819ms step_avg:54.57ms
step:1702/2110 train_time:92907ms step_avg:54.59ms
step:1703/2110 train_time:92995ms step_avg:54.61ms
step:1704/2110 train_time:93083ms step_avg:54.63ms
step:1705/2110 train_time:93172ms step_avg:54.65ms
step:1706/2110 train_time:93260ms step_avg:54.67ms
step:1707/2110 train_time:93349ms step_avg:54.69ms
step:1708/2110 train_time:93437ms step_avg:54.71ms
step:1709/2110 train_time:93525ms step_avg:54.73ms
step:1710/2110 train_time:93613ms step_avg:54.74ms
step:1711/2110 train_time:93702ms step_avg:54.76ms
step:1712/2110 train_time:93789ms step_avg:54.78ms
step:1713/2110 train_time:93878ms step_avg:54.80ms
step:1714/2110 train_time:93965ms step_avg:54.82ms
step:1715/2110 train_time:94053ms step_avg:54.84ms
step:1716/2110 train_time:94140ms step_avg:54.86ms
step:1717/2110 train_time:94229ms step_avg:54.88ms
step:1718/2110 train_time:94315ms step_avg:54.90ms
step:1719/2110 train_time:94404ms step_avg:54.92ms
step:1720/2110 train_time:94492ms step_avg:54.94ms
step:1721/2110 train_time:94580ms step_avg:54.96ms
step:1722/2110 train_time:94668ms step_avg:54.98ms
step:1723/2110 train_time:94757ms step_avg:55.00ms
step:1724/2110 train_time:94843ms step_avg:55.01ms
step:1725/2110 train_time:94933ms step_avg:55.03ms
step:1726/2110 train_time:95020ms step_avg:55.05ms
step:1727/2110 train_time:95110ms step_avg:55.07ms
step:1728/2110 train_time:95196ms step_avg:55.09ms
step:1729/2110 train_time:95285ms step_avg:55.11ms
step:1730/2110 train_time:95372ms step_avg:55.13ms
step:1731/2110 train_time:95462ms step_avg:55.15ms
step:1732/2110 train_time:95549ms step_avg:55.17ms
step:1733/2110 train_time:95638ms step_avg:55.19ms
step:1734/2110 train_time:95725ms step_avg:55.20ms
step:1735/2110 train_time:95814ms step_avg:55.22ms
step:1736/2110 train_time:95901ms step_avg:55.24ms
step:1737/2110 train_time:95990ms step_avg:55.26ms
step:1738/2110 train_time:96076ms step_avg:55.28ms
step:1739/2110 train_time:96166ms step_avg:55.30ms
step:1740/2110 train_time:96253ms step_avg:55.32ms
step:1741/2110 train_time:96342ms step_avg:55.34ms
step:1742/2110 train_time:96429ms step_avg:55.36ms
step:1743/2110 train_time:96518ms step_avg:55.37ms
step:1744/2110 train_time:96605ms step_avg:55.39ms
step:1745/2110 train_time:96695ms step_avg:55.41ms
step:1746/2110 train_time:96782ms step_avg:55.43ms
step:1747/2110 train_time:96872ms step_avg:55.45ms
step:1748/2110 train_time:96958ms step_avg:55.47ms
step:1749/2110 train_time:97047ms step_avg:55.49ms
step:1750/2110 train_time:97134ms step_avg:55.51ms
step:1750/2110 val_loss:3.3770 train_time:97224ms step_avg:55.56ms
step:1751/2110 train_time:97245ms step_avg:55.54ms
step:1752/2110 train_time:97315ms step_avg:55.55ms
step:1753/2110 train_time:97408ms step_avg:55.57ms
step:1754/2110 train_time:97494ms step_avg:55.58ms
step:1755/2110 train_time:97582ms step_avg:55.60ms
step:1756/2110 train_time:97668ms step_avg:55.62ms
step:1757/2110 train_time:97755ms step_avg:55.64ms
step:1758/2110 train_time:97842ms step_avg:55.66ms
step:1759/2110 train_time:97930ms step_avg:55.67ms
step:1760/2110 train_time:98017ms step_avg:55.69ms
step:1761/2110 train_time:98105ms step_avg:55.71ms
step:1762/2110 train_time:98193ms step_avg:55.73ms
step:1763/2110 train_time:98284ms step_avg:55.75ms
step:1764/2110 train_time:98375ms step_avg:55.77ms
step:1765/2110 train_time:98464ms step_avg:55.79ms
step:1766/2110 train_time:98551ms step_avg:55.80ms
step:1767/2110 train_time:98638ms step_avg:55.82ms
step:1768/2110 train_time:98724ms step_avg:55.84ms
step:1769/2110 train_time:98813ms step_avg:55.86ms
step:1770/2110 train_time:98899ms step_avg:55.88ms
step:1771/2110 train_time:98986ms step_avg:55.89ms
step:1772/2110 train_time:99073ms step_avg:55.91ms
step:1773/2110 train_time:99162ms step_avg:55.93ms
step:1774/2110 train_time:99251ms step_avg:55.95ms
step:1775/2110 train_time:99341ms step_avg:55.97ms
step:1776/2110 train_time:99431ms step_avg:55.99ms
step:1777/2110 train_time:99520ms step_avg:56.00ms
step:1778/2110 train_time:99608ms step_avg:56.02ms
step:1779/2110 train_time:99696ms step_avg:56.04ms
step:1780/2110 train_time:99781ms step_avg:56.06ms
step:1781/2110 train_time:99871ms step_avg:56.08ms
step:1782/2110 train_time:99957ms step_avg:56.09ms
step:1783/2110 train_time:100045ms step_avg:56.11ms
step:1784/2110 train_time:100132ms step_avg:56.13ms
step:1785/2110 train_time:100221ms step_avg:56.15ms
step:1786/2110 train_time:100309ms step_avg:56.16ms
step:1787/2110 train_time:100398ms step_avg:56.18ms
step:1788/2110 train_time:100486ms step_avg:56.20ms
step:1789/2110 train_time:100574ms step_avg:56.22ms
step:1790/2110 train_time:100661ms step_avg:56.24ms
step:1791/2110 train_time:100749ms step_avg:56.25ms
step:1792/2110 train_time:100835ms step_avg:56.27ms
step:1793/2110 train_time:100924ms step_avg:56.29ms
step:1794/2110 train_time:101011ms step_avg:56.31ms
step:1795/2110 train_time:101100ms step_avg:56.32ms
step:1796/2110 train_time:101187ms step_avg:56.34ms
step:1797/2110 train_time:101276ms step_avg:56.36ms
step:1798/2110 train_time:101363ms step_avg:56.38ms
step:1799/2110 train_time:101453ms step_avg:56.39ms
step:1800/2110 train_time:101541ms step_avg:56.41ms
step:1801/2110 train_time:101630ms step_avg:56.43ms
step:1802/2110 train_time:101717ms step_avg:56.45ms
step:1803/2110 train_time:101806ms step_avg:56.46ms
step:1804/2110 train_time:101893ms step_avg:56.48ms
step:1805/2110 train_time:101981ms step_avg:56.50ms
step:1806/2110 train_time:102067ms step_avg:56.52ms
step:1807/2110 train_time:102157ms step_avg:56.53ms
step:1808/2110 train_time:102245ms step_avg:56.55ms
step:1809/2110 train_time:102335ms step_avg:56.57ms
step:1810/2110 train_time:102422ms step_avg:56.59ms
step:1811/2110 train_time:102513ms step_avg:56.61ms
step:1812/2110 train_time:102599ms step_avg:56.62ms
step:1813/2110 train_time:102689ms step_avg:56.64ms
step:1814/2110 train_time:102775ms step_avg:56.66ms
step:1815/2110 train_time:102863ms step_avg:56.67ms
step:1816/2110 train_time:102950ms step_avg:56.69ms
step:1817/2110 train_time:103038ms step_avg:56.71ms
step:1818/2110 train_time:103125ms step_avg:56.72ms
step:1819/2110 train_time:103215ms step_avg:56.74ms
step:1820/2110 train_time:103303ms step_avg:56.76ms
step:1821/2110 train_time:103393ms step_avg:56.78ms
step:1822/2110 train_time:103480ms step_avg:56.79ms
step:1823/2110 train_time:103570ms step_avg:56.81ms
step:1824/2110 train_time:103656ms step_avg:56.83ms
step:1825/2110 train_time:103744ms step_avg:56.85ms
step:1826/2110 train_time:103832ms step_avg:56.86ms
step:1827/2110 train_time:103921ms step_avg:56.88ms
step:1828/2110 train_time:104008ms step_avg:56.90ms
step:1829/2110 train_time:104096ms step_avg:56.91ms
step:1830/2110 train_time:104183ms step_avg:56.93ms
step:1831/2110 train_time:104272ms step_avg:56.95ms
step:1832/2110 train_time:104360ms step_avg:56.96ms
step:1833/2110 train_time:104448ms step_avg:56.98ms
step:1834/2110 train_time:104536ms step_avg:57.00ms
step:1835/2110 train_time:104625ms step_avg:57.02ms
step:1836/2110 train_time:104712ms step_avg:57.03ms
step:1837/2110 train_time:104800ms step_avg:57.05ms
step:1838/2110 train_time:104886ms step_avg:57.07ms
step:1839/2110 train_time:104975ms step_avg:57.08ms
step:1840/2110 train_time:105062ms step_avg:57.10ms
step:1841/2110 train_time:105151ms step_avg:57.12ms
step:1842/2110 train_time:105238ms step_avg:57.13ms
step:1843/2110 train_time:105327ms step_avg:57.15ms
step:1844/2110 train_time:105414ms step_avg:57.17ms
step:1845/2110 train_time:105503ms step_avg:57.18ms
step:1846/2110 train_time:105591ms step_avg:57.20ms
step:1847/2110 train_time:105679ms step_avg:57.22ms
step:1848/2110 train_time:105767ms step_avg:57.23ms
step:1849/2110 train_time:105855ms step_avg:57.25ms
step:1850/2110 train_time:105942ms step_avg:57.27ms
step:1851/2110 train_time:106032ms step_avg:57.28ms
step:1852/2110 train_time:106118ms step_avg:57.30ms
step:1853/2110 train_time:106207ms step_avg:57.32ms
step:1854/2110 train_time:106294ms step_avg:57.33ms
step:1855/2110 train_time:106383ms step_avg:57.35ms
step:1856/2110 train_time:106470ms step_avg:57.37ms
step:1857/2110 train_time:106559ms step_avg:57.38ms
step:1858/2110 train_time:106646ms step_avg:57.40ms
step:1859/2110 train_time:106734ms step_avg:57.42ms
step:1860/2110 train_time:106822ms step_avg:57.43ms
step:1861/2110 train_time:106912ms step_avg:57.45ms
step:1862/2110 train_time:107000ms step_avg:57.46ms
step:1863/2110 train_time:107088ms step_avg:57.48ms
step:1864/2110 train_time:107175ms step_avg:57.50ms
step:1865/2110 train_time:107263ms step_avg:57.51ms
step:1866/2110 train_time:107351ms step_avg:57.53ms
step:1867/2110 train_time:107439ms step_avg:57.55ms
step:1868/2110 train_time:107526ms step_avg:57.56ms
step:1869/2110 train_time:107615ms step_avg:57.58ms
step:1870/2110 train_time:107702ms step_avg:57.59ms
step:1871/2110 train_time:107791ms step_avg:57.61ms
step:1872/2110 train_time:107878ms step_avg:57.63ms
step:1873/2110 train_time:107968ms step_avg:57.64ms
step:1874/2110 train_time:108054ms step_avg:57.66ms
step:1875/2110 train_time:108143ms step_avg:57.68ms
step:1876/2110 train_time:108231ms step_avg:57.69ms
step:1877/2110 train_time:108320ms step_avg:57.71ms
step:1878/2110 train_time:108407ms step_avg:57.72ms
step:1879/2110 train_time:108497ms step_avg:57.74ms
step:1880/2110 train_time:108583ms step_avg:57.76ms
step:1881/2110 train_time:108673ms step_avg:57.77ms
step:1882/2110 train_time:108759ms step_avg:57.79ms
step:1883/2110 train_time:108848ms step_avg:57.81ms
step:1884/2110 train_time:108935ms step_avg:57.82ms
step:1885/2110 train_time:109024ms step_avg:57.84ms
step:1886/2110 train_time:109111ms step_avg:57.85ms
step:1887/2110 train_time:109199ms step_avg:57.87ms
step:1888/2110 train_time:109285ms step_avg:57.88ms
step:1889/2110 train_time:109374ms step_avg:57.90ms
step:1890/2110 train_time:109461ms step_avg:57.92ms
step:1891/2110 train_time:109550ms step_avg:57.93ms
step:1892/2110 train_time:109637ms step_avg:57.95ms
step:1893/2110 train_time:109726ms step_avg:57.96ms
step:1894/2110 train_time:109813ms step_avg:57.98ms
step:1895/2110 train_time:109902ms step_avg:58.00ms
step:1896/2110 train_time:109989ms step_avg:58.01ms
step:1897/2110 train_time:110078ms step_avg:58.03ms
step:1898/2110 train_time:110165ms step_avg:58.04ms
step:1899/2110 train_time:110253ms step_avg:58.06ms
step:1900/2110 train_time:110340ms step_avg:58.07ms
step:1901/2110 train_time:110429ms step_avg:58.09ms
step:1902/2110 train_time:110516ms step_avg:58.11ms
step:1903/2110 train_time:110605ms step_avg:58.12ms
step:1904/2110 train_time:110692ms step_avg:58.14ms
step:1905/2110 train_time:110780ms step_avg:58.15ms
step:1906/2110 train_time:110867ms step_avg:58.17ms
step:1907/2110 train_time:110957ms step_avg:58.18ms
step:1908/2110 train_time:111046ms step_avg:58.20ms
step:1909/2110 train_time:111136ms step_avg:58.22ms
step:1910/2110 train_time:111223ms step_avg:58.23ms
step:1911/2110 train_time:111312ms step_avg:58.25ms
step:1912/2110 train_time:111399ms step_avg:58.26ms
step:1913/2110 train_time:111488ms step_avg:58.28ms
step:1914/2110 train_time:111575ms step_avg:58.29ms
step:1915/2110 train_time:111664ms step_avg:58.31ms
step:1916/2110 train_time:111751ms step_avg:58.33ms
step:1917/2110 train_time:111840ms step_avg:58.34ms
step:1918/2110 train_time:111926ms step_avg:58.36ms
step:1919/2110 train_time:112016ms step_avg:58.37ms
step:1920/2110 train_time:112104ms step_avg:58.39ms
step:1921/2110 train_time:112193ms step_avg:58.40ms
step:1922/2110 train_time:112280ms step_avg:58.42ms
step:1923/2110 train_time:112370ms step_avg:58.43ms
step:1924/2110 train_time:112457ms step_avg:58.45ms
step:1925/2110 train_time:112546ms step_avg:58.47ms
step:1926/2110 train_time:112633ms step_avg:58.48ms
step:1927/2110 train_time:112722ms step_avg:58.50ms
step:1928/2110 train_time:112808ms step_avg:58.51ms
step:1929/2110 train_time:112897ms step_avg:58.53ms
step:1930/2110 train_time:112984ms step_avg:58.54ms
step:1931/2110 train_time:113072ms step_avg:58.56ms
step:1932/2110 train_time:113160ms step_avg:58.57ms
step:1933/2110 train_time:113249ms step_avg:58.59ms
step:1934/2110 train_time:113336ms step_avg:58.60ms
step:1935/2110 train_time:113425ms step_avg:58.62ms
step:1936/2110 train_time:113513ms step_avg:58.63ms
step:1937/2110 train_time:113601ms step_avg:58.65ms
step:1938/2110 train_time:113687ms step_avg:58.66ms
step:1939/2110 train_time:113776ms step_avg:58.68ms
step:1940/2110 train_time:113863ms step_avg:58.69ms
step:1941/2110 train_time:113952ms step_avg:58.71ms
step:1942/2110 train_time:114038ms step_avg:58.72ms
step:1943/2110 train_time:114128ms step_avg:58.74ms
step:1944/2110 train_time:114215ms step_avg:58.75ms
step:1945/2110 train_time:114304ms step_avg:58.77ms
step:1946/2110 train_time:114391ms step_avg:58.78ms
step:1947/2110 train_time:114480ms step_avg:58.80ms
step:1948/2110 train_time:114567ms step_avg:58.81ms
step:1949/2110 train_time:114656ms step_avg:58.83ms
step:1950/2110 train_time:114743ms step_avg:58.84ms
step:1951/2110 train_time:114833ms step_avg:58.86ms
step:1952/2110 train_time:114919ms step_avg:58.87ms
step:1953/2110 train_time:115009ms step_avg:58.89ms
step:1954/2110 train_time:115096ms step_avg:58.90ms
step:1955/2110 train_time:115185ms step_avg:58.92ms
step:1956/2110 train_time:115272ms step_avg:58.93ms
step:1957/2110 train_time:115360ms step_avg:58.95ms
step:1958/2110 train_time:115449ms step_avg:58.96ms
step:1959/2110 train_time:115537ms step_avg:58.98ms
step:1960/2110 train_time:115625ms step_avg:58.99ms
step:1961/2110 train_time:115713ms step_avg:59.01ms
step:1962/2110 train_time:115801ms step_avg:59.02ms
step:1963/2110 train_time:115889ms step_avg:59.04ms
step:1964/2110 train_time:115976ms step_avg:59.05ms
step:1965/2110 train_time:116065ms step_avg:59.07ms
step:1966/2110 train_time:116153ms step_avg:59.08ms
step:1967/2110 train_time:116241ms step_avg:59.10ms
step:1968/2110 train_time:116328ms step_avg:59.11ms
step:1969/2110 train_time:116418ms step_avg:59.13ms
step:1970/2110 train_time:116505ms step_avg:59.14ms
step:1971/2110 train_time:116595ms step_avg:59.16ms
step:1972/2110 train_time:116682ms step_avg:59.17ms
step:1973/2110 train_time:116770ms step_avg:59.18ms
step:1974/2110 train_time:116856ms step_avg:59.20ms
step:1975/2110 train_time:116945ms step_avg:59.21ms
step:1976/2110 train_time:117033ms step_avg:59.23ms
step:1977/2110 train_time:117121ms step_avg:59.24ms
step:1978/2110 train_time:117209ms step_avg:59.26ms
step:1979/2110 train_time:117298ms step_avg:59.27ms
step:1980/2110 train_time:117384ms step_avg:59.29ms
step:1981/2110 train_time:117474ms step_avg:59.30ms
step:1982/2110 train_time:117561ms step_avg:59.31ms
step:1983/2110 train_time:117650ms step_avg:59.33ms
step:1984/2110 train_time:117737ms step_avg:59.34ms
step:1985/2110 train_time:117826ms step_avg:59.36ms
step:1986/2110 train_time:117913ms step_avg:59.37ms
step:1987/2110 train_time:118002ms step_avg:59.39ms
step:1988/2110 train_time:118089ms step_avg:59.40ms
step:1989/2110 train_time:118178ms step_avg:59.42ms
step:1990/2110 train_time:118265ms step_avg:59.43ms
step:1991/2110 train_time:118354ms step_avg:59.44ms
step:1992/2110 train_time:118442ms step_avg:59.46ms
step:1993/2110 train_time:118531ms step_avg:59.47ms
step:1994/2110 train_time:118618ms step_avg:59.49ms
step:1995/2110 train_time:118707ms step_avg:59.50ms
step:1996/2110 train_time:118793ms step_avg:59.52ms
step:1997/2110 train_time:118883ms step_avg:59.53ms
step:1998/2110 train_time:118971ms step_avg:59.54ms
step:1999/2110 train_time:119059ms step_avg:59.56ms
step:2000/2110 train_time:119146ms step_avg:59.57ms
step:2000/2110 val_loss:3.3028 train_time:119237ms step_avg:59.62ms
step:2001/2110 train_time:119259ms step_avg:59.60ms
step:2002/2110 train_time:119325ms step_avg:59.60ms
step:2003/2110 train_time:119417ms step_avg:59.62ms
step:2004/2110 train_time:119505ms step_avg:59.63ms
step:2005/2110 train_time:119594ms step_avg:59.65ms
step:2006/2110 train_time:119680ms step_avg:59.66ms
step:2007/2110 train_time:119768ms step_avg:59.68ms
step:2008/2110 train_time:119854ms step_avg:59.69ms
step:2009/2110 train_time:119942ms step_avg:59.70ms
step:2010/2110 train_time:120029ms step_avg:59.72ms
step:2011/2110 train_time:120117ms step_avg:59.73ms
step:2012/2110 train_time:120205ms step_avg:59.74ms
step:2013/2110 train_time:120296ms step_avg:59.76ms
step:2014/2110 train_time:120385ms step_avg:59.77ms
step:2015/2110 train_time:120475ms step_avg:59.79ms
step:2016/2110 train_time:120563ms step_avg:59.80ms
step:2017/2110 train_time:120652ms step_avg:59.82ms
step:2018/2110 train_time:120738ms step_avg:59.83ms
step:2019/2110 train_time:120826ms step_avg:59.84ms
step:2020/2110 train_time:120912ms step_avg:59.86ms
step:2021/2110 train_time:121000ms step_avg:59.87ms
step:2022/2110 train_time:121086ms step_avg:59.88ms
step:2023/2110 train_time:121177ms step_avg:59.90ms
step:2024/2110 train_time:121265ms step_avg:59.91ms
step:2025/2110 train_time:121355ms step_avg:59.93ms
step:2026/2110 train_time:121444ms step_avg:59.94ms
step:2027/2110 train_time:121534ms step_avg:59.96ms
step:2028/2110 train_time:121621ms step_avg:59.97ms
step:2029/2110 train_time:121709ms step_avg:59.98ms
step:2030/2110 train_time:121795ms step_avg:60.00ms
step:2031/2110 train_time:121883ms step_avg:60.01ms
step:2032/2110 train_time:121969ms step_avg:60.02ms
step:2033/2110 train_time:122058ms step_avg:60.04ms
step:2034/2110 train_time:122145ms step_avg:60.05ms
step:2035/2110 train_time:122235ms step_avg:60.07ms
step:2036/2110 train_time:122323ms step_avg:60.08ms
step:2037/2110 train_time:122413ms step_avg:60.09ms
step:2038/2110 train_time:122501ms step_avg:60.11ms
step:2039/2110 train_time:122591ms step_avg:60.12ms
step:2040/2110 train_time:122677ms step_avg:60.14ms
step:2041/2110 train_time:122766ms step_avg:60.15ms
step:2042/2110 train_time:122852ms step_avg:60.16ms
step:2043/2110 train_time:122940ms step_avg:60.18ms
step:2044/2110 train_time:123027ms step_avg:60.19ms
step:2045/2110 train_time:123116ms step_avg:60.20ms
step:2046/2110 train_time:123204ms step_avg:60.22ms
step:2047/2110 train_time:123293ms step_avg:60.23ms
step:2048/2110 train_time:123382ms step_avg:60.25ms
step:2049/2110 train_time:123471ms step_avg:60.26ms
step:2050/2110 train_time:123560ms step_avg:60.27ms
step:2051/2110 train_time:123648ms step_avg:60.29ms
step:2052/2110 train_time:123736ms step_avg:60.30ms
step:2053/2110 train_time:123825ms step_avg:60.31ms
step:2054/2110 train_time:123912ms step_avg:60.33ms
step:2055/2110 train_time:124000ms step_avg:60.34ms
step:2056/2110 train_time:124086ms step_avg:60.35ms
step:2057/2110 train_time:124176ms step_avg:60.37ms
step:2058/2110 train_time:124263ms step_avg:60.38ms
step:2059/2110 train_time:124353ms step_avg:60.39ms
step:2060/2110 train_time:124441ms step_avg:60.41ms
step:2061/2110 train_time:124530ms step_avg:60.42ms
step:2062/2110 train_time:124617ms step_avg:60.44ms
step:2063/2110 train_time:124706ms step_avg:60.45ms
step:2064/2110 train_time:124792ms step_avg:60.46ms
step:2065/2110 train_time:124882ms step_avg:60.48ms
step:2066/2110 train_time:124969ms step_avg:60.49ms
step:2067/2110 train_time:125058ms step_avg:60.50ms
step:2068/2110 train_time:125145ms step_avg:60.52ms
step:2069/2110 train_time:125234ms step_avg:60.53ms
step:2070/2110 train_time:125322ms step_avg:60.54ms
step:2071/2110 train_time:125412ms step_avg:60.56ms
step:2072/2110 train_time:125499ms step_avg:60.57ms
step:2073/2110 train_time:125588ms step_avg:60.58ms
step:2074/2110 train_time:125675ms step_avg:60.60ms
step:2075/2110 train_time:125765ms step_avg:60.61ms
step:2076/2110 train_time:125852ms step_avg:60.62ms
step:2077/2110 train_time:125942ms step_avg:60.64ms
step:2078/2110 train_time:126029ms step_avg:60.65ms
step:2079/2110 train_time:126119ms step_avg:60.66ms
step:2080/2110 train_time:126206ms step_avg:60.68ms
step:2081/2110 train_time:126295ms step_avg:60.69ms
step:2082/2110 train_time:126383ms step_avg:60.70ms
step:2083/2110 train_time:126472ms step_avg:60.72ms
step:2084/2110 train_time:126559ms step_avg:60.73ms
step:2085/2110 train_time:126648ms step_avg:60.74ms
step:2086/2110 train_time:126736ms step_avg:60.76ms
step:2087/2110 train_time:126826ms step_avg:60.77ms
step:2088/2110 train_time:126913ms step_avg:60.78ms
step:2089/2110 train_time:127003ms step_avg:60.80ms
step:2090/2110 train_time:127091ms step_avg:60.81ms
step:2091/2110 train_time:127181ms step_avg:60.82ms
step:2092/2110 train_time:127269ms step_avg:60.84ms
step:2093/2110 train_time:127357ms step_avg:60.85ms
step:2094/2110 train_time:127444ms step_avg:60.86ms
step:2095/2110 train_time:127535ms step_avg:60.88ms
step:2096/2110 train_time:127622ms step_avg:60.89ms
step:2097/2110 train_time:127711ms step_avg:60.90ms
step:2098/2110 train_time:127798ms step_avg:60.91ms
step:2099/2110 train_time:127887ms step_avg:60.93ms
step:2100/2110 train_time:127975ms step_avg:60.94ms
step:2101/2110 train_time:128064ms step_avg:60.95ms
step:2102/2110 train_time:128151ms step_avg:60.97ms
step:2103/2110 train_time:128242ms step_avg:60.98ms
step:2104/2110 train_time:128329ms step_avg:60.99ms
step:2105/2110 train_time:128419ms step_avg:61.01ms
step:2106/2110 train_time:128506ms step_avg:61.02ms
step:2107/2110 train_time:128594ms step_avg:61.03ms
step:2108/2110 train_time:128682ms step_avg:61.04ms
step:2109/2110 train_time:128772ms step_avg:61.06ms
step:2110/2110 train_time:128858ms step_avg:61.07ms
step:2110/2110 val_loss:3.2783 train_time:128950ms step_avg:61.11ms
peak memory allocated: 29816 MiB reserved: 44496 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 14 20:34:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0            128W /  700W |    5858MiB /  81559MiB |      1%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0            127W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     51638      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     51639      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     51640      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     51641      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     51642      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     51643      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     51644      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     51645      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     51639      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     51640      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     51641      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     51642      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     51643      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     51644      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     51645      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.04ms
step:1/2110 train_time:83ms step_avg:82.59ms
step:2/2110 train_time:107ms step_avg:53.63ms
step:3/2110 train_time:129ms step_avg:42.92ms
step:4/2110 train_time:154ms step_avg:38.49ms
step:5/2110 train_time:187ms step_avg:37.34ms
step:6/2110 train_time:413ms step_avg:68.81ms
step:7/2110 train_time:432ms step_avg:61.65ms
step:8/2110 train_time:458ms step_avg:57.19ms
step:9/2110 train_time:491ms step_avg:54.53ms
step:10/2110 train_time:523ms step_avg:52.33ms
step:11/2110 train_time:557ms step_avg:50.61ms
step:12/2110 train_time:589ms step_avg:49.12ms
step:13/2110 train_time:623ms step_avg:47.94ms
step:14/2110 train_time:656ms step_avg:46.84ms
step:15/2110 train_time:689ms step_avg:45.95ms
step:16/2110 train_time:722ms step_avg:45.12ms
step:17/2110 train_time:756ms step_avg:44.45ms
step:18/2110 train_time:788ms step_avg:43.79ms
step:19/2110 train_time:822ms step_avg:43.28ms
step:20/2110 train_time:855ms step_avg:42.75ms
step:21/2110 train_time:889ms step_avg:42.32ms
step:22/2110 train_time:921ms step_avg:41.88ms
step:23/2110 train_time:955ms step_avg:41.53ms
step:24/2110 train_time:988ms step_avg:41.16ms
step:25/2110 train_time:1022ms step_avg:40.87ms
step:26/2110 train_time:1054ms step_avg:40.55ms
step:27/2110 train_time:1088ms step_avg:40.31ms
step:28/2110 train_time:1121ms step_avg:40.03ms
step:29/2110 train_time:1155ms step_avg:39.81ms
step:30/2110 train_time:1187ms step_avg:39.57ms
step:31/2110 train_time:1221ms step_avg:39.38ms
step:32/2110 train_time:1254ms step_avg:39.18ms
step:33/2110 train_time:1288ms step_avg:39.02ms
step:34/2110 train_time:1321ms step_avg:38.86ms
step:35/2110 train_time:1358ms step_avg:38.80ms
step:36/2110 train_time:1391ms step_avg:38.65ms
step:37/2110 train_time:1427ms step_avg:38.57ms
step:38/2110 train_time:1460ms step_avg:38.42ms
step:39/2110 train_time:1495ms step_avg:38.32ms
step:40/2110 train_time:1528ms step_avg:38.19ms
step:41/2110 train_time:1563ms step_avg:38.11ms
step:42/2110 train_time:1595ms step_avg:37.98ms
step:43/2110 train_time:1629ms step_avg:37.89ms
step:44/2110 train_time:1662ms step_avg:37.77ms
step:45/2110 train_time:1696ms step_avg:37.69ms
step:46/2110 train_time:1729ms step_avg:37.58ms
step:47/2110 train_time:1763ms step_avg:37.50ms
step:48/2110 train_time:1795ms step_avg:37.40ms
step:49/2110 train_time:1829ms step_avg:37.33ms
step:50/2110 train_time:1862ms step_avg:37.23ms
step:51/2110 train_time:1895ms step_avg:37.16ms
step:52/2110 train_time:1928ms step_avg:37.08ms
step:53/2110 train_time:1962ms step_avg:37.01ms
step:54/2110 train_time:1994ms step_avg:36.93ms
step:55/2110 train_time:2028ms step_avg:36.87ms
step:56/2110 train_time:2061ms step_avg:36.80ms
step:57/2110 train_time:2095ms step_avg:36.75ms
step:58/2110 train_time:2128ms step_avg:36.69ms
step:59/2110 train_time:2161ms step_avg:36.63ms
step:60/2110 train_time:2194ms step_avg:36.57ms
step:61/2110 train_time:2227ms step_avg:36.52ms
step:62/2110 train_time:2260ms step_avg:36.45ms
step:63/2110 train_time:2294ms step_avg:36.42ms
step:64/2110 train_time:2328ms step_avg:36.37ms
step:65/2110 train_time:2362ms step_avg:36.34ms
step:66/2110 train_time:2395ms step_avg:36.29ms
step:67/2110 train_time:2429ms step_avg:36.25ms
step:68/2110 train_time:2462ms step_avg:36.20ms
step:69/2110 train_time:2496ms step_avg:36.17ms
step:70/2110 train_time:2529ms step_avg:36.12ms
step:71/2110 train_time:2563ms step_avg:36.09ms
step:72/2110 train_time:2596ms step_avg:36.05ms
step:73/2110 train_time:2630ms step_avg:36.02ms
step:74/2110 train_time:2662ms step_avg:35.98ms
step:75/2110 train_time:2697ms step_avg:35.96ms
step:76/2110 train_time:2730ms step_avg:35.92ms
step:77/2110 train_time:2763ms step_avg:35.89ms
step:78/2110 train_time:2796ms step_avg:35.85ms
step:79/2110 train_time:2830ms step_avg:35.82ms
step:80/2110 train_time:2862ms step_avg:35.78ms
step:81/2110 train_time:2897ms step_avg:35.77ms
step:82/2110 train_time:2930ms step_avg:35.73ms
step:83/2110 train_time:2964ms step_avg:35.71ms
step:84/2110 train_time:2996ms step_avg:35.67ms
step:85/2110 train_time:3030ms step_avg:35.65ms
step:86/2110 train_time:3063ms step_avg:35.61ms
step:87/2110 train_time:3096ms step_avg:35.59ms
step:88/2110 train_time:3129ms step_avg:35.55ms
step:89/2110 train_time:3162ms step_avg:35.53ms
step:90/2110 train_time:3195ms step_avg:35.50ms
step:91/2110 train_time:3229ms step_avg:35.48ms
step:92/2110 train_time:3261ms step_avg:35.45ms
step:93/2110 train_time:3295ms step_avg:35.43ms
step:94/2110 train_time:3328ms step_avg:35.40ms
step:95/2110 train_time:3362ms step_avg:35.39ms
step:96/2110 train_time:3395ms step_avg:35.36ms
step:97/2110 train_time:3428ms step_avg:35.34ms
step:98/2110 train_time:3461ms step_avg:35.32ms
step:99/2110 train_time:3495ms step_avg:35.30ms
step:100/2110 train_time:3528ms step_avg:35.28ms
step:101/2110 train_time:3562ms step_avg:35.27ms
step:102/2110 train_time:3595ms step_avg:35.24ms
step:103/2110 train_time:3628ms step_avg:35.23ms
step:104/2110 train_time:3661ms step_avg:35.20ms
step:105/2110 train_time:3695ms step_avg:35.19ms
step:106/2110 train_time:3727ms step_avg:35.16ms
step:107/2110 train_time:3762ms step_avg:35.15ms
step:108/2110 train_time:3794ms step_avg:35.13ms
step:109/2110 train_time:3828ms step_avg:35.12ms
step:110/2110 train_time:3860ms step_avg:35.09ms
step:111/2110 train_time:3894ms step_avg:35.08ms
step:112/2110 train_time:3927ms step_avg:35.06ms
step:113/2110 train_time:3960ms step_avg:35.05ms
step:114/2110 train_time:3993ms step_avg:35.03ms
step:115/2110 train_time:4026ms step_avg:35.01ms
step:116/2110 train_time:4059ms step_avg:34.99ms
step:117/2110 train_time:4093ms step_avg:34.98ms
step:118/2110 train_time:4125ms step_avg:34.96ms
step:119/2110 train_time:4159ms step_avg:34.95ms
step:120/2110 train_time:4191ms step_avg:34.93ms
step:121/2110 train_time:4225ms step_avg:34.92ms
step:122/2110 train_time:4258ms step_avg:34.90ms
step:123/2110 train_time:4291ms step_avg:34.89ms
step:124/2110 train_time:4324ms step_avg:34.87ms
step:125/2110 train_time:4358ms step_avg:34.86ms
step:126/2110 train_time:4391ms step_avg:34.85ms
step:127/2110 train_time:4424ms step_avg:34.84ms
step:128/2110 train_time:4457ms step_avg:34.82ms
step:129/2110 train_time:4491ms step_avg:34.81ms
step:130/2110 train_time:4524ms step_avg:34.80ms
step:131/2110 train_time:4558ms step_avg:34.79ms
step:132/2110 train_time:4590ms step_avg:34.78ms
step:133/2110 train_time:4624ms step_avg:34.77ms
step:134/2110 train_time:4657ms step_avg:34.76ms
step:135/2110 train_time:4691ms step_avg:34.75ms
step:136/2110 train_time:4724ms step_avg:34.73ms
step:137/2110 train_time:4758ms step_avg:34.73ms
step:138/2110 train_time:4790ms step_avg:34.71ms
step:139/2110 train_time:4824ms step_avg:34.71ms
step:140/2110 train_time:4857ms step_avg:34.69ms
step:141/2110 train_time:4890ms step_avg:34.68ms
step:142/2110 train_time:4923ms step_avg:34.67ms
step:143/2110 train_time:4957ms step_avg:34.66ms
step:144/2110 train_time:4989ms step_avg:34.65ms
step:145/2110 train_time:5023ms step_avg:34.64ms
step:146/2110 train_time:5056ms step_avg:34.63ms
step:147/2110 train_time:5089ms step_avg:34.62ms
step:148/2110 train_time:5122ms step_avg:34.61ms
step:149/2110 train_time:5155ms step_avg:34.60ms
step:150/2110 train_time:5188ms step_avg:34.59ms
step:151/2110 train_time:5222ms step_avg:34.58ms
step:152/2110 train_time:5254ms step_avg:34.57ms
step:153/2110 train_time:5288ms step_avg:34.56ms
step:154/2110 train_time:5320ms step_avg:34.55ms
step:155/2110 train_time:5354ms step_avg:34.54ms
step:156/2110 train_time:5386ms step_avg:34.53ms
step:157/2110 train_time:5420ms step_avg:34.52ms
step:158/2110 train_time:5453ms step_avg:34.51ms
step:159/2110 train_time:5486ms step_avg:34.51ms
step:160/2110 train_time:5519ms step_avg:34.49ms
step:161/2110 train_time:5553ms step_avg:34.49ms
step:162/2110 train_time:5586ms step_avg:34.48ms
step:163/2110 train_time:5619ms step_avg:34.47ms
step:164/2110 train_time:5652ms step_avg:34.46ms
step:165/2110 train_time:5686ms step_avg:34.46ms
step:166/2110 train_time:5718ms step_avg:34.45ms
step:167/2110 train_time:5753ms step_avg:34.45ms
step:168/2110 train_time:5785ms step_avg:34.44ms
step:169/2110 train_time:5819ms step_avg:34.43ms
step:170/2110 train_time:5852ms step_avg:34.42ms
step:171/2110 train_time:5885ms step_avg:34.42ms
step:172/2110 train_time:5918ms step_avg:34.41ms
step:173/2110 train_time:5951ms step_avg:34.40ms
step:174/2110 train_time:5984ms step_avg:34.39ms
step:175/2110 train_time:6017ms step_avg:34.38ms
step:176/2110 train_time:6050ms step_avg:34.38ms
step:177/2110 train_time:6083ms step_avg:34.37ms
step:178/2110 train_time:6116ms step_avg:34.36ms
step:179/2110 train_time:6150ms step_avg:34.36ms
step:180/2110 train_time:6183ms step_avg:34.35ms
step:181/2110 train_time:6216ms step_avg:34.34ms
step:182/2110 train_time:6248ms step_avg:34.33ms
step:183/2110 train_time:6282ms step_avg:34.33ms
step:184/2110 train_time:6315ms step_avg:34.32ms
step:185/2110 train_time:6348ms step_avg:34.31ms
step:186/2110 train_time:6381ms step_avg:34.30ms
step:187/2110 train_time:6415ms step_avg:34.30ms
step:188/2110 train_time:6447ms step_avg:34.29ms
step:189/2110 train_time:6481ms step_avg:34.29ms
step:190/2110 train_time:6513ms step_avg:34.28ms
step:191/2110 train_time:6548ms step_avg:34.28ms
step:192/2110 train_time:6580ms step_avg:34.27ms
step:193/2110 train_time:6614ms step_avg:34.27ms
step:194/2110 train_time:6647ms step_avg:34.26ms
step:195/2110 train_time:6682ms step_avg:34.26ms
step:196/2110 train_time:6714ms step_avg:34.26ms
step:197/2110 train_time:6748ms step_avg:34.25ms
step:198/2110 train_time:6780ms step_avg:34.24ms
step:199/2110 train_time:6814ms step_avg:34.24ms
step:200/2110 train_time:6846ms step_avg:34.23ms
step:201/2110 train_time:6881ms step_avg:34.23ms
step:202/2110 train_time:6913ms step_avg:34.22ms
step:203/2110 train_time:6947ms step_avg:34.22ms
step:204/2110 train_time:6980ms step_avg:34.21ms
step:205/2110 train_time:7013ms step_avg:34.21ms
step:206/2110 train_time:7046ms step_avg:34.20ms
step:207/2110 train_time:7079ms step_avg:34.20ms
step:208/2110 train_time:7112ms step_avg:34.19ms
step:209/2110 train_time:7146ms step_avg:34.19ms
step:210/2110 train_time:7179ms step_avg:34.18ms
step:211/2110 train_time:7212ms step_avg:34.18ms
step:212/2110 train_time:7244ms step_avg:34.17ms
step:213/2110 train_time:7278ms step_avg:34.17ms
step:214/2110 train_time:7311ms step_avg:34.16ms
step:215/2110 train_time:7345ms step_avg:34.16ms
step:216/2110 train_time:7377ms step_avg:34.15ms
step:217/2110 train_time:7411ms step_avg:34.15ms
step:218/2110 train_time:7444ms step_avg:34.15ms
step:219/2110 train_time:7477ms step_avg:34.14ms
step:220/2110 train_time:7510ms step_avg:34.14ms
step:221/2110 train_time:7544ms step_avg:34.13ms
step:222/2110 train_time:7576ms step_avg:34.13ms
step:223/2110 train_time:7610ms step_avg:34.12ms
step:224/2110 train_time:7642ms step_avg:34.12ms
step:225/2110 train_time:7676ms step_avg:34.11ms
step:226/2110 train_time:7709ms step_avg:34.11ms
step:227/2110 train_time:7742ms step_avg:34.11ms
step:228/2110 train_time:7775ms step_avg:34.10ms
step:229/2110 train_time:7808ms step_avg:34.10ms
step:230/2110 train_time:7841ms step_avg:34.09ms
step:231/2110 train_time:7874ms step_avg:34.09ms
step:232/2110 train_time:7907ms step_avg:34.08ms
step:233/2110 train_time:7941ms step_avg:34.08ms
step:234/2110 train_time:7973ms step_avg:34.07ms
step:235/2110 train_time:8007ms step_avg:34.07ms
step:236/2110 train_time:8040ms step_avg:34.07ms
step:237/2110 train_time:8073ms step_avg:34.06ms
step:238/2110 train_time:8106ms step_avg:34.06ms
step:239/2110 train_time:8140ms step_avg:34.06ms
step:240/2110 train_time:8173ms step_avg:34.05ms
step:241/2110 train_time:8206ms step_avg:34.05ms
step:242/2110 train_time:8239ms step_avg:34.04ms
step:243/2110 train_time:8272ms step_avg:34.04ms
step:244/2110 train_time:8305ms step_avg:34.04ms
step:245/2110 train_time:8338ms step_avg:34.03ms
step:246/2110 train_time:8371ms step_avg:34.03ms
step:247/2110 train_time:8405ms step_avg:34.03ms
step:248/2110 train_time:8437ms step_avg:34.02ms
step:249/2110 train_time:8471ms step_avg:34.02ms
step:250/2110 train_time:8504ms step_avg:34.01ms
step:250/2110 val_loss:4.2644 train_time:8539ms step_avg:34.16ms
step:251/2110 train_time:8559ms step_avg:34.10ms
step:252/2110 train_time:8579ms step_avg:34.04ms
step:253/2110 train_time:8609ms step_avg:34.03ms
step:254/2110 train_time:8641ms step_avg:34.02ms
step:255/2110 train_time:8677ms step_avg:34.03ms
step:256/2110 train_time:8710ms step_avg:34.02ms
step:257/2110 train_time:8745ms step_avg:34.03ms
step:258/2110 train_time:8778ms step_avg:34.02ms
step:259/2110 train_time:8811ms step_avg:34.02ms
step:260/2110 train_time:8844ms step_avg:34.01ms
step:261/2110 train_time:8878ms step_avg:34.01ms
step:262/2110 train_time:8910ms step_avg:34.01ms
step:263/2110 train_time:8944ms step_avg:34.01ms
step:264/2110 train_time:8976ms step_avg:34.00ms
step:265/2110 train_time:9010ms step_avg:34.00ms
step:266/2110 train_time:9042ms step_avg:33.99ms
step:267/2110 train_time:9075ms step_avg:33.99ms
step:268/2110 train_time:9108ms step_avg:33.99ms
step:269/2110 train_time:9142ms step_avg:33.98ms
step:270/2110 train_time:9174ms step_avg:33.98ms
step:271/2110 train_time:9207ms step_avg:33.98ms
step:272/2110 train_time:9240ms step_avg:33.97ms
step:273/2110 train_time:9273ms step_avg:33.97ms
step:274/2110 train_time:9305ms step_avg:33.96ms
step:275/2110 train_time:9339ms step_avg:33.96ms
step:276/2110 train_time:9372ms step_avg:33.96ms
step:277/2110 train_time:9405ms step_avg:33.95ms
step:278/2110 train_time:9438ms step_avg:33.95ms
step:279/2110 train_time:9471ms step_avg:33.95ms
step:280/2110 train_time:9504ms step_avg:33.94ms
step:281/2110 train_time:9538ms step_avg:33.94ms
step:282/2110 train_time:9570ms step_avg:33.94ms
step:283/2110 train_time:9605ms step_avg:33.94ms
step:284/2110 train_time:9638ms step_avg:33.94ms
step:285/2110 train_time:9672ms step_avg:33.94ms
step:286/2110 train_time:9705ms step_avg:33.93ms
step:287/2110 train_time:9739ms step_avg:33.93ms
step:288/2110 train_time:9771ms step_avg:33.93ms
step:289/2110 train_time:9805ms step_avg:33.93ms
step:290/2110 train_time:9838ms step_avg:33.92ms
step:291/2110 train_time:9871ms step_avg:33.92ms
step:292/2110 train_time:9903ms step_avg:33.92ms
step:293/2110 train_time:9937ms step_avg:33.92ms
step:294/2110 train_time:9970ms step_avg:33.91ms
step:295/2110 train_time:10004ms step_avg:33.91ms
step:296/2110 train_time:10036ms step_avg:33.91ms
step:297/2110 train_time:10070ms step_avg:33.90ms
step:298/2110 train_time:10102ms step_avg:33.90ms
step:299/2110 train_time:10135ms step_avg:33.90ms
step:300/2110 train_time:10168ms step_avg:33.89ms
step:301/2110 train_time:10201ms step_avg:33.89ms
step:302/2110 train_time:10234ms step_avg:33.89ms
step:303/2110 train_time:10267ms step_avg:33.88ms
step:304/2110 train_time:10299ms step_avg:33.88ms
step:305/2110 train_time:10333ms step_avg:33.88ms
step:306/2110 train_time:10365ms step_avg:33.87ms
step:307/2110 train_time:10399ms step_avg:33.87ms
step:308/2110 train_time:10432ms step_avg:33.87ms
step:309/2110 train_time:10465ms step_avg:33.87ms
step:310/2110 train_time:10497ms step_avg:33.86ms
step:311/2110 train_time:10531ms step_avg:33.86ms
step:312/2110 train_time:10564ms step_avg:33.86ms
step:313/2110 train_time:10597ms step_avg:33.86ms
step:314/2110 train_time:10630ms step_avg:33.85ms
step:315/2110 train_time:10664ms step_avg:33.85ms
step:316/2110 train_time:10697ms step_avg:33.85ms
step:317/2110 train_time:10731ms step_avg:33.85ms
step:318/2110 train_time:10763ms step_avg:33.85ms
step:319/2110 train_time:10797ms step_avg:33.85ms
step:320/2110 train_time:10829ms step_avg:33.84ms
step:321/2110 train_time:10863ms step_avg:33.84ms
step:322/2110 train_time:10896ms step_avg:33.84ms
step:323/2110 train_time:10929ms step_avg:33.84ms
step:324/2110 train_time:10962ms step_avg:33.83ms
step:325/2110 train_time:10996ms step_avg:33.83ms
step:326/2110 train_time:11028ms step_avg:33.83ms
step:327/2110 train_time:11062ms step_avg:33.83ms
step:328/2110 train_time:11094ms step_avg:33.82ms
step:329/2110 train_time:11128ms step_avg:33.82ms
step:330/2110 train_time:11161ms step_avg:33.82ms
step:331/2110 train_time:11194ms step_avg:33.82ms
step:332/2110 train_time:11227ms step_avg:33.82ms
step:333/2110 train_time:11261ms step_avg:33.82ms
step:334/2110 train_time:11293ms step_avg:33.81ms
step:335/2110 train_time:11327ms step_avg:33.81ms
step:336/2110 train_time:11359ms step_avg:33.81ms
step:337/2110 train_time:11393ms step_avg:33.81ms
step:338/2110 train_time:11425ms step_avg:33.80ms
step:339/2110 train_time:11459ms step_avg:33.80ms
step:340/2110 train_time:11492ms step_avg:33.80ms
step:341/2110 train_time:11525ms step_avg:33.80ms
step:342/2110 train_time:11558ms step_avg:33.79ms
step:343/2110 train_time:11591ms step_avg:33.79ms
step:344/2110 train_time:11624ms step_avg:33.79ms
step:345/2110 train_time:11658ms step_avg:33.79ms
step:346/2110 train_time:11690ms step_avg:33.79ms
step:347/2110 train_time:11724ms step_avg:33.79ms
step:348/2110 train_time:11757ms step_avg:33.78ms
step:349/2110 train_time:11790ms step_avg:33.78ms
step:350/2110 train_time:11823ms step_avg:33.78ms
step:351/2110 train_time:11856ms step_avg:33.78ms
step:352/2110 train_time:11889ms step_avg:33.77ms
step:353/2110 train_time:11923ms step_avg:33.78ms
step:354/2110 train_time:11956ms step_avg:33.77ms
step:355/2110 train_time:11989ms step_avg:33.77ms
step:356/2110 train_time:12021ms step_avg:33.77ms
step:357/2110 train_time:12055ms step_avg:33.77ms
step:358/2110 train_time:12088ms step_avg:33.77ms
step:359/2110 train_time:12121ms step_avg:33.76ms
step:360/2110 train_time:12154ms step_avg:33.76ms
step:361/2110 train_time:12187ms step_avg:33.76ms
step:362/2110 train_time:12219ms step_avg:33.76ms
step:363/2110 train_time:12253ms step_avg:33.75ms
step:364/2110 train_time:12286ms step_avg:33.75ms
step:365/2110 train_time:12319ms step_avg:33.75ms
step:366/2110 train_time:12352ms step_avg:33.75ms
step:367/2110 train_time:12385ms step_avg:33.75ms
step:368/2110 train_time:12418ms step_avg:33.74ms
step:369/2110 train_time:12451ms step_avg:33.74ms
step:370/2110 train_time:12484ms step_avg:33.74ms
step:371/2110 train_time:12518ms step_avg:33.74ms
step:372/2110 train_time:12551ms step_avg:33.74ms
step:373/2110 train_time:12584ms step_avg:33.74ms
step:374/2110 train_time:12617ms step_avg:33.74ms
step:375/2110 train_time:12650ms step_avg:33.73ms
step:376/2110 train_time:12683ms step_avg:33.73ms
step:377/2110 train_time:12717ms step_avg:33.73ms
step:378/2110 train_time:12750ms step_avg:33.73ms
step:379/2110 train_time:12783ms step_avg:33.73ms
step:380/2110 train_time:12816ms step_avg:33.73ms
step:381/2110 train_time:12850ms step_avg:33.73ms
step:382/2110 train_time:12882ms step_avg:33.72ms
step:383/2110 train_time:12916ms step_avg:33.72ms
step:384/2110 train_time:12949ms step_avg:33.72ms
step:385/2110 train_time:12982ms step_avg:33.72ms
step:386/2110 train_time:13015ms step_avg:33.72ms
step:387/2110 train_time:13049ms step_avg:33.72ms
step:388/2110 train_time:13081ms step_avg:33.71ms
step:389/2110 train_time:13115ms step_avg:33.71ms
step:390/2110 train_time:13148ms step_avg:33.71ms
step:391/2110 train_time:13181ms step_avg:33.71ms
step:392/2110 train_time:13214ms step_avg:33.71ms
step:393/2110 train_time:13247ms step_avg:33.71ms
step:394/2110 train_time:13280ms step_avg:33.70ms
step:395/2110 train_time:13313ms step_avg:33.70ms
step:396/2110 train_time:13346ms step_avg:33.70ms
step:397/2110 train_time:13379ms step_avg:33.70ms
step:398/2110 train_time:13412ms step_avg:33.70ms
step:399/2110 train_time:13445ms step_avg:33.70ms
step:400/2110 train_time:13478ms step_avg:33.69ms
step:401/2110 train_time:13511ms step_avg:33.69ms
step:402/2110 train_time:13544ms step_avg:33.69ms
step:403/2110 train_time:13577ms step_avg:33.69ms
step:404/2110 train_time:13610ms step_avg:33.69ms
step:405/2110 train_time:13644ms step_avg:33.69ms
step:406/2110 train_time:13677ms step_avg:33.69ms
step:407/2110 train_time:13710ms step_avg:33.69ms
step:408/2110 train_time:13742ms step_avg:33.68ms
step:409/2110 train_time:13776ms step_avg:33.68ms
step:410/2110 train_time:13808ms step_avg:33.68ms
step:411/2110 train_time:13842ms step_avg:33.68ms
step:412/2110 train_time:13874ms step_avg:33.68ms
step:413/2110 train_time:13908ms step_avg:33.68ms
step:414/2110 train_time:13941ms step_avg:33.67ms
step:415/2110 train_time:13975ms step_avg:33.67ms
step:416/2110 train_time:14007ms step_avg:33.67ms
step:417/2110 train_time:14041ms step_avg:33.67ms
step:418/2110 train_time:14074ms step_avg:33.67ms
step:419/2110 train_time:14107ms step_avg:33.67ms
step:420/2110 train_time:14140ms step_avg:33.67ms
step:421/2110 train_time:14174ms step_avg:33.67ms
step:422/2110 train_time:14206ms step_avg:33.66ms
step:423/2110 train_time:14240ms step_avg:33.66ms
step:424/2110 train_time:14272ms step_avg:33.66ms
step:425/2110 train_time:14306ms step_avg:33.66ms
step:426/2110 train_time:14338ms step_avg:33.66ms
step:427/2110 train_time:14371ms step_avg:33.66ms
step:428/2110 train_time:14404ms step_avg:33.65ms
step:429/2110 train_time:14438ms step_avg:33.65ms
step:430/2110 train_time:14470ms step_avg:33.65ms
step:431/2110 train_time:14504ms step_avg:33.65ms
step:432/2110 train_time:14536ms step_avg:33.65ms
step:433/2110 train_time:14570ms step_avg:33.65ms
step:434/2110 train_time:14602ms step_avg:33.65ms
step:435/2110 train_time:14636ms step_avg:33.65ms
step:436/2110 train_time:14669ms step_avg:33.64ms
step:437/2110 train_time:14702ms step_avg:33.64ms
step:438/2110 train_time:14735ms step_avg:33.64ms
step:439/2110 train_time:14769ms step_avg:33.64ms
step:440/2110 train_time:14801ms step_avg:33.64ms
step:441/2110 train_time:14835ms step_avg:33.64ms
step:442/2110 train_time:14867ms step_avg:33.64ms
step:443/2110 train_time:14901ms step_avg:33.64ms
step:444/2110 train_time:14934ms step_avg:33.64ms
step:445/2110 train_time:14968ms step_avg:33.64ms
step:446/2110 train_time:15000ms step_avg:33.63ms
step:447/2110 train_time:15034ms step_avg:33.63ms
step:448/2110 train_time:15067ms step_avg:33.63ms
step:449/2110 train_time:15100ms step_avg:33.63ms
step:450/2110 train_time:15133ms step_avg:33.63ms
step:451/2110 train_time:15166ms step_avg:33.63ms
step:452/2110 train_time:15198ms step_avg:33.62ms
step:453/2110 train_time:15232ms step_avg:33.62ms
step:454/2110 train_time:15265ms step_avg:33.62ms
step:455/2110 train_time:15298ms step_avg:33.62ms
step:456/2110 train_time:15331ms step_avg:33.62ms
step:457/2110 train_time:15365ms step_avg:33.62ms
step:458/2110 train_time:15397ms step_avg:33.62ms
step:459/2110 train_time:15431ms step_avg:33.62ms
step:460/2110 train_time:15463ms step_avg:33.62ms
step:461/2110 train_time:15497ms step_avg:33.62ms
step:462/2110 train_time:15530ms step_avg:33.61ms
step:463/2110 train_time:15563ms step_avg:33.61ms
step:464/2110 train_time:15596ms step_avg:33.61ms
step:465/2110 train_time:15630ms step_avg:33.61ms
step:466/2110 train_time:15662ms step_avg:33.61ms
step:467/2110 train_time:15696ms step_avg:33.61ms
step:468/2110 train_time:15729ms step_avg:33.61ms
step:469/2110 train_time:15762ms step_avg:33.61ms
step:470/2110 train_time:15795ms step_avg:33.61ms
step:471/2110 train_time:15828ms step_avg:33.61ms
step:472/2110 train_time:15861ms step_avg:33.60ms
step:473/2110 train_time:15895ms step_avg:33.60ms
step:474/2110 train_time:15927ms step_avg:33.60ms
step:475/2110 train_time:15961ms step_avg:33.60ms
step:476/2110 train_time:15993ms step_avg:33.60ms
step:477/2110 train_time:16027ms step_avg:33.60ms
step:478/2110 train_time:16060ms step_avg:33.60ms
step:479/2110 train_time:16093ms step_avg:33.60ms
step:480/2110 train_time:16126ms step_avg:33.60ms
step:481/2110 train_time:16159ms step_avg:33.59ms
step:482/2110 train_time:16192ms step_avg:33.59ms
step:483/2110 train_time:16225ms step_avg:33.59ms
step:484/2110 train_time:16258ms step_avg:33.59ms
step:485/2110 train_time:16291ms step_avg:33.59ms
step:486/2110 train_time:16324ms step_avg:33.59ms
step:487/2110 train_time:16357ms step_avg:33.59ms
step:488/2110 train_time:16390ms step_avg:33.59ms
step:489/2110 train_time:16424ms step_avg:33.59ms
step:490/2110 train_time:16456ms step_avg:33.58ms
step:491/2110 train_time:16490ms step_avg:33.58ms
step:492/2110 train_time:16523ms step_avg:33.58ms
step:493/2110 train_time:16556ms step_avg:33.58ms
step:494/2110 train_time:16588ms step_avg:33.58ms
step:495/2110 train_time:16622ms step_avg:33.58ms
step:496/2110 train_time:16655ms step_avg:33.58ms
step:497/2110 train_time:16689ms step_avg:33.58ms
step:498/2110 train_time:16721ms step_avg:33.58ms
step:499/2110 train_time:16754ms step_avg:33.58ms
step:500/2110 train_time:16787ms step_avg:33.57ms
step:500/2110 val_loss:4.0065 train_time:16823ms step_avg:33.65ms
step:501/2110 train_time:16843ms step_avg:33.62ms
step:502/2110 train_time:16863ms step_avg:33.59ms
step:503/2110 train_time:16889ms step_avg:33.58ms
step:504/2110 train_time:16922ms step_avg:33.58ms
step:505/2110 train_time:16959ms step_avg:33.58ms
step:506/2110 train_time:16992ms step_avg:33.58ms
step:507/2110 train_time:17026ms step_avg:33.58ms
step:508/2110 train_time:17059ms step_avg:33.58ms
step:509/2110 train_time:17092ms step_avg:33.58ms
step:510/2110 train_time:17125ms step_avg:33.58ms
step:511/2110 train_time:17159ms step_avg:33.58ms
step:512/2110 train_time:17191ms step_avg:33.58ms
step:513/2110 train_time:17224ms step_avg:33.58ms
step:514/2110 train_time:17257ms step_avg:33.57ms
step:515/2110 train_time:17290ms step_avg:33.57ms
step:516/2110 train_time:17322ms step_avg:33.57ms
step:517/2110 train_time:17356ms step_avg:33.57ms
step:518/2110 train_time:17388ms step_avg:33.57ms
step:519/2110 train_time:17421ms step_avg:33.57ms
step:520/2110 train_time:17454ms step_avg:33.56ms
step:521/2110 train_time:17487ms step_avg:33.56ms
step:522/2110 train_time:17519ms step_avg:33.56ms
step:523/2110 train_time:17552ms step_avg:33.56ms
step:524/2110 train_time:17585ms step_avg:33.56ms
step:525/2110 train_time:17618ms step_avg:33.56ms
step:526/2110 train_time:17651ms step_avg:33.56ms
step:527/2110 train_time:17684ms step_avg:33.56ms
step:528/2110 train_time:17717ms step_avg:33.55ms
step:529/2110 train_time:17750ms step_avg:33.55ms
step:530/2110 train_time:17783ms step_avg:33.55ms
step:531/2110 train_time:17817ms step_avg:33.55ms
step:532/2110 train_time:17849ms step_avg:33.55ms
step:533/2110 train_time:17883ms step_avg:33.55ms
step:534/2110 train_time:17917ms step_avg:33.55ms
step:535/2110 train_time:17951ms step_avg:33.55ms
step:536/2110 train_time:17983ms step_avg:33.55ms
step:537/2110 train_time:18017ms step_avg:33.55ms
step:538/2110 train_time:18050ms step_avg:33.55ms
step:539/2110 train_time:18084ms step_avg:33.55ms
step:540/2110 train_time:18116ms step_avg:33.55ms
step:541/2110 train_time:18150ms step_avg:33.55ms
step:542/2110 train_time:18183ms step_avg:33.55ms
step:543/2110 train_time:18217ms step_avg:33.55ms
step:544/2110 train_time:18249ms step_avg:33.55ms
step:545/2110 train_time:18283ms step_avg:33.55ms
step:546/2110 train_time:18315ms step_avg:33.54ms
step:547/2110 train_time:18349ms step_avg:33.54ms
step:548/2110 train_time:18381ms step_avg:33.54ms
step:549/2110 train_time:18415ms step_avg:33.54ms
step:550/2110 train_time:18447ms step_avg:33.54ms
step:551/2110 train_time:18481ms step_avg:33.54ms
step:552/2110 train_time:18513ms step_avg:33.54ms
step:553/2110 train_time:18546ms step_avg:33.54ms
step:554/2110 train_time:18579ms step_avg:33.54ms
step:555/2110 train_time:18612ms step_avg:33.54ms
step:556/2110 train_time:18645ms step_avg:33.53ms
step:557/2110 train_time:18678ms step_avg:33.53ms
step:558/2110 train_time:18710ms step_avg:33.53ms
step:559/2110 train_time:18744ms step_avg:33.53ms
step:560/2110 train_time:18777ms step_avg:33.53ms
step:561/2110 train_time:18810ms step_avg:33.53ms
step:562/2110 train_time:18843ms step_avg:33.53ms
step:563/2110 train_time:18876ms step_avg:33.53ms
step:564/2110 train_time:18909ms step_avg:33.53ms
step:565/2110 train_time:18943ms step_avg:33.53ms
step:566/2110 train_time:18976ms step_avg:33.53ms
step:567/2110 train_time:19010ms step_avg:33.53ms
step:568/2110 train_time:19042ms step_avg:33.53ms
step:569/2110 train_time:19076ms step_avg:33.53ms
step:570/2110 train_time:19109ms step_avg:33.52ms
step:571/2110 train_time:19143ms step_avg:33.52ms
step:572/2110 train_time:19175ms step_avg:33.52ms
step:573/2110 train_time:19209ms step_avg:33.52ms
step:574/2110 train_time:19241ms step_avg:33.52ms
step:575/2110 train_time:19275ms step_avg:33.52ms
step:576/2110 train_time:19308ms step_avg:33.52ms
step:577/2110 train_time:19341ms step_avg:33.52ms
step:578/2110 train_time:19374ms step_avg:33.52ms
step:579/2110 train_time:19407ms step_avg:33.52ms
step:580/2110 train_time:19439ms step_avg:33.52ms
step:581/2110 train_time:19473ms step_avg:33.52ms
step:582/2110 train_time:19506ms step_avg:33.51ms
step:583/2110 train_time:19539ms step_avg:33.52ms
step:584/2110 train_time:19572ms step_avg:33.51ms
step:585/2110 train_time:19605ms step_avg:33.51ms
step:586/2110 train_time:19638ms step_avg:33.51ms
step:587/2110 train_time:19671ms step_avg:33.51ms
step:588/2110 train_time:19704ms step_avg:33.51ms
step:589/2110 train_time:19737ms step_avg:33.51ms
step:590/2110 train_time:19769ms step_avg:33.51ms
step:591/2110 train_time:19803ms step_avg:33.51ms
step:592/2110 train_time:19836ms step_avg:33.51ms
step:593/2110 train_time:19869ms step_avg:33.51ms
step:594/2110 train_time:19901ms step_avg:33.50ms
step:595/2110 train_time:19935ms step_avg:33.50ms
step:596/2110 train_time:19968ms step_avg:33.50ms
step:597/2110 train_time:20001ms step_avg:33.50ms
step:598/2110 train_time:20035ms step_avg:33.50ms
step:599/2110 train_time:20068ms step_avg:33.50ms
step:600/2110 train_time:20101ms step_avg:33.50ms
step:601/2110 train_time:20134ms step_avg:33.50ms
step:602/2110 train_time:20167ms step_avg:33.50ms
step:603/2110 train_time:20201ms step_avg:33.50ms
step:604/2110 train_time:20234ms step_avg:33.50ms
step:605/2110 train_time:20267ms step_avg:33.50ms
step:606/2110 train_time:20300ms step_avg:33.50ms
step:607/2110 train_time:20334ms step_avg:33.50ms
step:608/2110 train_time:20367ms step_avg:33.50ms
step:609/2110 train_time:20400ms step_avg:33.50ms
step:610/2110 train_time:20433ms step_avg:33.50ms
step:611/2110 train_time:20466ms step_avg:33.50ms
step:612/2110 train_time:20498ms step_avg:33.49ms
step:613/2110 train_time:20532ms step_avg:33.49ms
step:614/2110 train_time:20564ms step_avg:33.49ms
step:615/2110 train_time:20598ms step_avg:33.49ms
step:616/2110 train_time:20630ms step_avg:33.49ms
step:617/2110 train_time:20664ms step_avg:33.49ms
step:618/2110 train_time:20696ms step_avg:33.49ms
step:619/2110 train_time:20730ms step_avg:33.49ms
step:620/2110 train_time:20762ms step_avg:33.49ms
step:621/2110 train_time:20795ms step_avg:33.49ms
step:622/2110 train_time:20828ms step_avg:33.49ms
step:623/2110 train_time:20861ms step_avg:33.49ms
step:624/2110 train_time:20894ms step_avg:33.48ms
step:625/2110 train_time:20927ms step_avg:33.48ms
step:626/2110 train_time:20960ms step_avg:33.48ms
step:627/2110 train_time:20994ms step_avg:33.48ms
step:628/2110 train_time:21027ms step_avg:33.48ms
step:629/2110 train_time:21060ms step_avg:33.48ms
step:630/2110 train_time:21093ms step_avg:33.48ms
step:631/2110 train_time:21127ms step_avg:33.48ms
step:632/2110 train_time:21160ms step_avg:33.48ms
step:633/2110 train_time:21193ms step_avg:33.48ms
step:634/2110 train_time:21226ms step_avg:33.48ms
step:635/2110 train_time:21260ms step_avg:33.48ms
step:636/2110 train_time:21292ms step_avg:33.48ms
step:637/2110 train_time:21326ms step_avg:33.48ms
step:638/2110 train_time:21359ms step_avg:33.48ms
step:639/2110 train_time:21392ms step_avg:33.48ms
step:640/2110 train_time:21425ms step_avg:33.48ms
step:641/2110 train_time:21459ms step_avg:33.48ms
step:642/2110 train_time:21491ms step_avg:33.48ms
step:643/2110 train_time:21525ms step_avg:33.48ms
step:644/2110 train_time:21558ms step_avg:33.48ms
step:645/2110 train_time:21591ms step_avg:33.48ms
step:646/2110 train_time:21624ms step_avg:33.47ms
step:647/2110 train_time:21658ms step_avg:33.47ms
step:648/2110 train_time:21691ms step_avg:33.47ms
step:649/2110 train_time:21724ms step_avg:33.47ms
step:650/2110 train_time:21757ms step_avg:33.47ms
step:651/2110 train_time:21790ms step_avg:33.47ms
step:652/2110 train_time:21823ms step_avg:33.47ms
step:653/2110 train_time:21856ms step_avg:33.47ms
step:654/2110 train_time:21889ms step_avg:33.47ms
step:655/2110 train_time:21922ms step_avg:33.47ms
step:656/2110 train_time:21955ms step_avg:33.47ms
step:657/2110 train_time:21989ms step_avg:33.47ms
step:658/2110 train_time:22021ms step_avg:33.47ms
step:659/2110 train_time:22055ms step_avg:33.47ms
step:660/2110 train_time:22088ms step_avg:33.47ms
step:661/2110 train_time:22122ms step_avg:33.47ms
step:662/2110 train_time:22154ms step_avg:33.47ms
step:663/2110 train_time:22188ms step_avg:33.47ms
step:664/2110 train_time:22220ms step_avg:33.46ms
step:665/2110 train_time:22254ms step_avg:33.46ms
step:666/2110 train_time:22286ms step_avg:33.46ms
step:667/2110 train_time:22320ms step_avg:33.46ms
step:668/2110 train_time:22352ms step_avg:33.46ms
step:669/2110 train_time:22386ms step_avg:33.46ms
step:670/2110 train_time:22418ms step_avg:33.46ms
step:671/2110 train_time:22452ms step_avg:33.46ms
step:672/2110 train_time:22485ms step_avg:33.46ms
step:673/2110 train_time:22518ms step_avg:33.46ms
step:674/2110 train_time:22551ms step_avg:33.46ms
step:675/2110 train_time:22584ms step_avg:33.46ms
step:676/2110 train_time:22617ms step_avg:33.46ms
step:677/2110 train_time:22651ms step_avg:33.46ms
step:678/2110 train_time:22683ms step_avg:33.46ms
step:679/2110 train_time:22717ms step_avg:33.46ms
step:680/2110 train_time:22749ms step_avg:33.46ms
step:681/2110 train_time:22783ms step_avg:33.46ms
step:682/2110 train_time:22816ms step_avg:33.45ms
step:683/2110 train_time:22849ms step_avg:33.45ms
step:684/2110 train_time:22883ms step_avg:33.45ms
step:685/2110 train_time:22916ms step_avg:33.45ms
step:686/2110 train_time:22948ms step_avg:33.45ms
step:687/2110 train_time:22981ms step_avg:33.45ms
step:688/2110 train_time:23014ms step_avg:33.45ms
step:689/2110 train_time:23048ms step_avg:33.45ms
step:690/2110 train_time:23081ms step_avg:33.45ms
step:691/2110 train_time:23115ms step_avg:33.45ms
step:692/2110 train_time:23173ms step_avg:33.49ms
step:693/2110 train_time:23234ms step_avg:33.53ms
step:694/2110 train_time:23292ms step_avg:33.56ms
step:695/2110 train_time:23354ms step_avg:33.60ms
step:696/2110 train_time:23413ms step_avg:33.64ms
step:697/2110 train_time:23474ms step_avg:33.68ms
step:698/2110 train_time:23534ms step_avg:33.72ms
step:699/2110 train_time:23595ms step_avg:33.75ms
step:700/2110 train_time:23654ms step_avg:33.79ms
step:701/2110 train_time:23716ms step_avg:33.83ms
step:702/2110 train_time:23775ms step_avg:33.87ms
step:703/2110 train_time:23836ms step_avg:33.91ms
step:704/2110 train_time:23896ms step_avg:33.94ms
step:705/2110 train_time:23958ms step_avg:33.98ms
step:706/2110 train_time:24017ms step_avg:34.02ms
step:707/2110 train_time:24078ms step_avg:34.06ms
step:708/2110 train_time:24137ms step_avg:34.09ms
step:709/2110 train_time:24199ms step_avg:34.13ms
step:710/2110 train_time:24257ms step_avg:34.17ms
step:711/2110 train_time:24319ms step_avg:34.20ms
step:712/2110 train_time:24378ms step_avg:34.24ms
step:713/2110 train_time:24439ms step_avg:34.28ms
step:714/2110 train_time:24498ms step_avg:34.31ms
step:715/2110 train_time:24559ms step_avg:34.35ms
step:716/2110 train_time:24619ms step_avg:34.38ms
step:717/2110 train_time:24681ms step_avg:34.42ms
step:718/2110 train_time:24740ms step_avg:34.46ms
step:719/2110 train_time:24801ms step_avg:34.49ms
step:720/2110 train_time:24860ms step_avg:34.53ms
step:721/2110 train_time:24922ms step_avg:34.57ms
step:722/2110 train_time:24982ms step_avg:34.60ms
step:723/2110 train_time:25043ms step_avg:34.64ms
step:724/2110 train_time:25102ms step_avg:34.67ms
step:725/2110 train_time:25163ms step_avg:34.71ms
step:726/2110 train_time:25222ms step_avg:34.74ms
step:727/2110 train_time:25284ms step_avg:34.78ms
step:728/2110 train_time:25343ms step_avg:34.81ms
step:729/2110 train_time:25405ms step_avg:34.85ms
step:730/2110 train_time:25465ms step_avg:34.88ms
step:731/2110 train_time:25526ms step_avg:34.92ms
step:732/2110 train_time:25586ms step_avg:34.95ms
step:733/2110 train_time:25647ms step_avg:34.99ms
step:734/2110 train_time:25707ms step_avg:35.02ms
step:735/2110 train_time:25768ms step_avg:35.06ms
step:736/2110 train_time:25827ms step_avg:35.09ms
step:737/2110 train_time:25888ms step_avg:35.13ms
step:738/2110 train_time:25948ms step_avg:35.16ms
step:739/2110 train_time:26008ms step_avg:35.19ms
step:740/2110 train_time:26067ms step_avg:35.23ms
step:741/2110 train_time:26128ms step_avg:35.26ms
step:742/2110 train_time:26187ms step_avg:35.29ms
step:743/2110 train_time:26248ms step_avg:35.33ms
step:744/2110 train_time:26307ms step_avg:35.36ms
step:745/2110 train_time:26368ms step_avg:35.39ms
step:746/2110 train_time:26427ms step_avg:35.43ms
step:747/2110 train_time:26488ms step_avg:35.46ms
step:748/2110 train_time:26548ms step_avg:35.49ms
step:749/2110 train_time:26609ms step_avg:35.53ms
step:750/2110 train_time:26668ms step_avg:35.56ms
step:750/2110 val_loss:3.8437 train_time:26731ms step_avg:35.64ms
step:751/2110 train_time:26753ms step_avg:35.62ms
step:752/2110 train_time:26789ms step_avg:35.62ms
step:753/2110 train_time:26852ms step_avg:35.66ms
step:754/2110 train_time:26913ms step_avg:35.69ms
step:755/2110 train_time:26974ms step_avg:35.73ms
step:756/2110 train_time:27033ms step_avg:35.76ms
step:757/2110 train_time:27094ms step_avg:35.79ms
step:758/2110 train_time:27152ms step_avg:35.82ms
step:759/2110 train_time:27213ms step_avg:35.85ms
step:760/2110 train_time:27271ms step_avg:35.88ms
step:761/2110 train_time:27332ms step_avg:35.92ms
step:762/2110 train_time:27391ms step_avg:35.95ms
step:763/2110 train_time:27451ms step_avg:35.98ms
step:764/2110 train_time:27510ms step_avg:36.01ms
step:765/2110 train_time:27571ms step_avg:36.04ms
step:766/2110 train_time:27630ms step_avg:36.07ms
step:767/2110 train_time:27693ms step_avg:36.11ms
step:768/2110 train_time:27753ms step_avg:36.14ms
step:769/2110 train_time:27816ms step_avg:36.17ms
step:770/2110 train_time:27877ms step_avg:36.20ms
step:771/2110 train_time:27939ms step_avg:36.24ms
step:772/2110 train_time:27998ms step_avg:36.27ms
step:773/2110 train_time:28060ms step_avg:36.30ms
step:774/2110 train_time:28119ms step_avg:36.33ms
step:775/2110 train_time:28179ms step_avg:36.36ms
step:776/2110 train_time:28239ms step_avg:36.39ms
step:777/2110 train_time:28300ms step_avg:36.42ms
step:778/2110 train_time:28359ms step_avg:36.45ms
step:779/2110 train_time:28419ms step_avg:36.48ms
step:780/2110 train_time:28478ms step_avg:36.51ms
step:781/2110 train_time:28539ms step_avg:36.54ms
step:782/2110 train_time:28598ms step_avg:36.57ms
step:783/2110 train_time:28660ms step_avg:36.60ms
step:784/2110 train_time:28720ms step_avg:36.63ms
step:785/2110 train_time:28781ms step_avg:36.66ms
step:786/2110 train_time:28840ms step_avg:36.69ms
step:787/2110 train_time:28902ms step_avg:36.72ms
step:788/2110 train_time:28961ms step_avg:36.75ms
step:789/2110 train_time:29023ms step_avg:36.78ms
step:790/2110 train_time:29082ms step_avg:36.81ms
step:791/2110 train_time:29144ms step_avg:36.84ms
step:792/2110 train_time:29203ms step_avg:36.87ms
step:793/2110 train_time:29264ms step_avg:36.90ms
step:794/2110 train_time:29323ms step_avg:36.93ms
step:795/2110 train_time:29383ms step_avg:36.96ms
step:796/2110 train_time:29443ms step_avg:36.99ms
step:797/2110 train_time:29503ms step_avg:37.02ms
step:798/2110 train_time:29562ms step_avg:37.05ms
step:799/2110 train_time:29623ms step_avg:37.08ms
step:800/2110 train_time:29683ms step_avg:37.10ms
step:801/2110 train_time:29745ms step_avg:37.13ms
step:802/2110 train_time:29805ms step_avg:37.16ms
step:803/2110 train_time:29865ms step_avg:37.19ms
step:804/2110 train_time:29924ms step_avg:37.22ms
step:805/2110 train_time:29985ms step_avg:37.25ms
step:806/2110 train_time:30045ms step_avg:37.28ms
step:807/2110 train_time:30105ms step_avg:37.31ms
step:808/2110 train_time:30164ms step_avg:37.33ms
step:809/2110 train_time:30224ms step_avg:37.36ms
step:810/2110 train_time:30284ms step_avg:37.39ms
step:811/2110 train_time:30345ms step_avg:37.42ms
step:812/2110 train_time:30404ms step_avg:37.44ms
step:813/2110 train_time:30465ms step_avg:37.47ms
step:814/2110 train_time:30524ms step_avg:37.50ms
step:815/2110 train_time:30585ms step_avg:37.53ms
step:816/2110 train_time:30645ms step_avg:37.56ms
step:817/2110 train_time:30706ms step_avg:37.58ms
step:818/2110 train_time:30766ms step_avg:37.61ms
step:819/2110 train_time:30827ms step_avg:37.64ms
step:820/2110 train_time:30886ms step_avg:37.67ms
step:821/2110 train_time:30947ms step_avg:37.69ms
step:822/2110 train_time:31005ms step_avg:37.72ms
step:823/2110 train_time:31066ms step_avg:37.75ms
step:824/2110 train_time:31125ms step_avg:37.77ms
step:825/2110 train_time:31185ms step_avg:37.80ms
step:826/2110 train_time:31245ms step_avg:37.83ms
step:827/2110 train_time:31306ms step_avg:37.85ms
step:828/2110 train_time:31364ms step_avg:37.88ms
step:829/2110 train_time:31425ms step_avg:37.91ms
step:830/2110 train_time:31485ms step_avg:37.93ms
step:831/2110 train_time:31546ms step_avg:37.96ms
step:832/2110 train_time:31605ms step_avg:37.99ms
step:833/2110 train_time:31666ms step_avg:38.01ms
step:834/2110 train_time:31725ms step_avg:38.04ms
step:835/2110 train_time:31786ms step_avg:38.07ms
step:836/2110 train_time:31846ms step_avg:38.09ms
step:837/2110 train_time:31906ms step_avg:38.12ms
step:838/2110 train_time:31965ms step_avg:38.14ms
step:839/2110 train_time:32026ms step_avg:38.17ms
step:840/2110 train_time:32085ms step_avg:38.20ms
step:841/2110 train_time:32146ms step_avg:38.22ms
step:842/2110 train_time:32206ms step_avg:38.25ms
step:843/2110 train_time:32266ms step_avg:38.28ms
step:844/2110 train_time:32325ms step_avg:38.30ms
step:845/2110 train_time:32385ms step_avg:38.33ms
step:846/2110 train_time:32445ms step_avg:38.35ms
step:847/2110 train_time:32507ms step_avg:38.38ms
step:848/2110 train_time:32565ms step_avg:38.40ms
step:849/2110 train_time:32626ms step_avg:38.43ms
step:850/2110 train_time:32685ms step_avg:38.45ms
step:851/2110 train_time:32747ms step_avg:38.48ms
step:852/2110 train_time:32805ms step_avg:38.50ms
step:853/2110 train_time:32866ms step_avg:38.53ms
step:854/2110 train_time:32926ms step_avg:38.56ms
step:855/2110 train_time:32986ms step_avg:38.58ms
step:856/2110 train_time:33046ms step_avg:38.60ms
step:857/2110 train_time:33106ms step_avg:38.63ms
step:858/2110 train_time:33165ms step_avg:38.65ms
step:859/2110 train_time:33226ms step_avg:38.68ms
step:860/2110 train_time:33286ms step_avg:38.70ms
step:861/2110 train_time:33347ms step_avg:38.73ms
step:862/2110 train_time:33406ms step_avg:38.75ms
step:863/2110 train_time:33467ms step_avg:38.78ms
step:864/2110 train_time:33526ms step_avg:38.80ms
step:865/2110 train_time:33587ms step_avg:38.83ms
step:866/2110 train_time:33647ms step_avg:38.85ms
step:867/2110 train_time:33707ms step_avg:38.88ms
step:868/2110 train_time:33766ms step_avg:38.90ms
step:869/2110 train_time:33827ms step_avg:38.93ms
step:870/2110 train_time:33886ms step_avg:38.95ms
step:871/2110 train_time:33947ms step_avg:38.97ms
step:872/2110 train_time:34006ms step_avg:39.00ms
step:873/2110 train_time:34067ms step_avg:39.02ms
step:874/2110 train_time:34126ms step_avg:39.05ms
step:875/2110 train_time:34187ms step_avg:39.07ms
step:876/2110 train_time:34246ms step_avg:39.09ms
step:877/2110 train_time:34306ms step_avg:39.12ms
step:878/2110 train_time:34365ms step_avg:39.14ms
step:879/2110 train_time:34426ms step_avg:39.16ms
step:880/2110 train_time:34486ms step_avg:39.19ms
step:881/2110 train_time:34547ms step_avg:39.21ms
step:882/2110 train_time:34606ms step_avg:39.24ms
step:883/2110 train_time:34667ms step_avg:39.26ms
step:884/2110 train_time:34725ms step_avg:39.28ms
step:885/2110 train_time:34787ms step_avg:39.31ms
step:886/2110 train_time:34846ms step_avg:39.33ms
step:887/2110 train_time:34907ms step_avg:39.35ms
step:888/2110 train_time:34966ms step_avg:39.38ms
step:889/2110 train_time:35027ms step_avg:39.40ms
step:890/2110 train_time:35085ms step_avg:39.42ms
step:891/2110 train_time:35147ms step_avg:39.45ms
step:892/2110 train_time:35206ms step_avg:39.47ms
step:893/2110 train_time:35266ms step_avg:39.49ms
step:894/2110 train_time:35325ms step_avg:39.51ms
step:895/2110 train_time:35386ms step_avg:39.54ms
step:896/2110 train_time:35446ms step_avg:39.56ms
step:897/2110 train_time:35507ms step_avg:39.58ms
step:898/2110 train_time:35566ms step_avg:39.61ms
step:899/2110 train_time:35627ms step_avg:39.63ms
step:900/2110 train_time:35686ms step_avg:39.65ms
step:901/2110 train_time:35747ms step_avg:39.67ms
step:902/2110 train_time:35806ms step_avg:39.70ms
step:903/2110 train_time:35866ms step_avg:39.72ms
step:904/2110 train_time:35926ms step_avg:39.74ms
step:905/2110 train_time:35986ms step_avg:39.76ms
step:906/2110 train_time:36045ms step_avg:39.79ms
step:907/2110 train_time:36106ms step_avg:39.81ms
step:908/2110 train_time:36166ms step_avg:39.83ms
step:909/2110 train_time:36226ms step_avg:39.85ms
step:910/2110 train_time:36285ms step_avg:39.87ms
step:911/2110 train_time:36346ms step_avg:39.90ms
step:912/2110 train_time:36405ms step_avg:39.92ms
step:913/2110 train_time:36466ms step_avg:39.94ms
step:914/2110 train_time:36525ms step_avg:39.96ms
step:915/2110 train_time:36586ms step_avg:39.98ms
step:916/2110 train_time:36646ms step_avg:40.01ms
step:917/2110 train_time:36706ms step_avg:40.03ms
step:918/2110 train_time:36765ms step_avg:40.05ms
step:919/2110 train_time:36826ms step_avg:40.07ms
step:920/2110 train_time:36885ms step_avg:40.09ms
step:921/2110 train_time:36947ms step_avg:40.12ms
step:922/2110 train_time:37006ms step_avg:40.14ms
step:923/2110 train_time:37067ms step_avg:40.16ms
step:924/2110 train_time:37127ms step_avg:40.18ms
step:925/2110 train_time:37187ms step_avg:40.20ms
step:926/2110 train_time:37246ms step_avg:40.22ms
step:927/2110 train_time:37306ms step_avg:40.24ms
step:928/2110 train_time:37365ms step_avg:40.26ms
step:929/2110 train_time:37426ms step_avg:40.29ms
step:930/2110 train_time:37486ms step_avg:40.31ms
step:931/2110 train_time:37547ms step_avg:40.33ms
step:932/2110 train_time:37607ms step_avg:40.35ms
step:933/2110 train_time:37667ms step_avg:40.37ms
step:934/2110 train_time:37727ms step_avg:40.39ms
step:935/2110 train_time:37787ms step_avg:40.41ms
step:936/2110 train_time:37846ms step_avg:40.43ms
step:937/2110 train_time:37907ms step_avg:40.46ms
step:938/2110 train_time:37966ms step_avg:40.48ms
step:939/2110 train_time:38027ms step_avg:40.50ms
step:940/2110 train_time:38086ms step_avg:40.52ms
step:941/2110 train_time:38147ms step_avg:40.54ms
step:942/2110 train_time:38206ms step_avg:40.56ms
step:943/2110 train_time:38267ms step_avg:40.58ms
step:944/2110 train_time:38326ms step_avg:40.60ms
step:945/2110 train_time:38386ms step_avg:40.62ms
step:946/2110 train_time:38446ms step_avg:40.64ms
step:947/2110 train_time:38507ms step_avg:40.66ms
step:948/2110 train_time:38567ms step_avg:40.68ms
step:949/2110 train_time:38628ms step_avg:40.70ms
step:950/2110 train_time:38687ms step_avg:40.72ms
step:951/2110 train_time:38748ms step_avg:40.74ms
step:952/2110 train_time:38807ms step_avg:40.76ms
step:953/2110 train_time:38868ms step_avg:40.79ms
step:954/2110 train_time:38927ms step_avg:40.80ms
step:955/2110 train_time:38988ms step_avg:40.82ms
step:956/2110 train_time:39047ms step_avg:40.84ms
step:957/2110 train_time:39108ms step_avg:40.86ms
step:958/2110 train_time:39167ms step_avg:40.88ms
step:959/2110 train_time:39227ms step_avg:40.90ms
step:960/2110 train_time:39286ms step_avg:40.92ms
step:961/2110 train_time:39347ms step_avg:40.94ms
step:962/2110 train_time:39406ms step_avg:40.96ms
step:963/2110 train_time:39468ms step_avg:40.98ms
step:964/2110 train_time:39527ms step_avg:41.00ms
step:965/2110 train_time:39588ms step_avg:41.02ms
step:966/2110 train_time:39647ms step_avg:41.04ms
step:967/2110 train_time:39708ms step_avg:41.06ms
step:968/2110 train_time:39767ms step_avg:41.08ms
step:969/2110 train_time:39828ms step_avg:41.10ms
step:970/2110 train_time:39887ms step_avg:41.12ms
step:971/2110 train_time:39948ms step_avg:41.14ms
step:972/2110 train_time:40007ms step_avg:41.16ms
step:973/2110 train_time:40068ms step_avg:41.18ms
step:974/2110 train_time:40127ms step_avg:41.20ms
step:975/2110 train_time:40188ms step_avg:41.22ms
step:976/2110 train_time:40247ms step_avg:41.24ms
step:977/2110 train_time:40308ms step_avg:41.26ms
step:978/2110 train_time:40367ms step_avg:41.27ms
step:979/2110 train_time:40428ms step_avg:41.30ms
step:980/2110 train_time:40487ms step_avg:41.31ms
step:981/2110 train_time:40549ms step_avg:41.33ms
step:982/2110 train_time:40607ms step_avg:41.35ms
step:983/2110 train_time:40668ms step_avg:41.37ms
step:984/2110 train_time:40727ms step_avg:41.39ms
step:985/2110 train_time:40788ms step_avg:41.41ms
step:986/2110 train_time:40846ms step_avg:41.43ms
step:987/2110 train_time:40907ms step_avg:41.45ms
step:988/2110 train_time:40966ms step_avg:41.46ms
step:989/2110 train_time:41026ms step_avg:41.48ms
step:990/2110 train_time:41086ms step_avg:41.50ms
step:991/2110 train_time:41146ms step_avg:41.52ms
step:992/2110 train_time:41205ms step_avg:41.54ms
step:993/2110 train_time:41266ms step_avg:41.56ms
step:994/2110 train_time:41324ms step_avg:41.57ms
step:995/2110 train_time:41386ms step_avg:41.59ms
step:996/2110 train_time:41446ms step_avg:41.61ms
step:997/2110 train_time:41506ms step_avg:41.63ms
step:998/2110 train_time:41566ms step_avg:41.65ms
step:999/2110 train_time:41626ms step_avg:41.67ms
step:1000/2110 train_time:41685ms step_avg:41.69ms
step:1000/2110 val_loss:3.6963 train_time:41748ms step_avg:41.75ms
step:1001/2110 train_time:41771ms step_avg:41.73ms
step:1002/2110 train_time:41807ms step_avg:41.72ms
step:1003/2110 train_time:41869ms step_avg:41.74ms
step:1004/2110 train_time:41931ms step_avg:41.76ms
step:1005/2110 train_time:41993ms step_avg:41.78ms
step:1006/2110 train_time:42052ms step_avg:41.80ms
step:1007/2110 train_time:42112ms step_avg:41.82ms
step:1008/2110 train_time:42170ms step_avg:41.84ms
step:1009/2110 train_time:42231ms step_avg:41.85ms
step:1010/2110 train_time:42289ms step_avg:41.87ms
step:1011/2110 train_time:42350ms step_avg:41.89ms
step:1012/2110 train_time:42409ms step_avg:41.91ms
step:1013/2110 train_time:42470ms step_avg:41.92ms
step:1014/2110 train_time:42529ms step_avg:41.94ms
step:1015/2110 train_time:42589ms step_avg:41.96ms
step:1016/2110 train_time:42648ms step_avg:41.98ms
step:1017/2110 train_time:42710ms step_avg:42.00ms
step:1018/2110 train_time:42770ms step_avg:42.01ms
step:1019/2110 train_time:42833ms step_avg:42.03ms
step:1020/2110 train_time:42893ms step_avg:42.05ms
step:1021/2110 train_time:42955ms step_avg:42.07ms
step:1022/2110 train_time:43014ms step_avg:42.09ms
step:1023/2110 train_time:43074ms step_avg:42.11ms
step:1024/2110 train_time:43134ms step_avg:42.12ms
step:1025/2110 train_time:43196ms step_avg:42.14ms
step:1026/2110 train_time:43253ms step_avg:42.16ms
step:1027/2110 train_time:43314ms step_avg:42.18ms
step:1028/2110 train_time:43372ms step_avg:42.19ms
step:1029/2110 train_time:43434ms step_avg:42.21ms
step:1030/2110 train_time:43493ms step_avg:42.23ms
step:1031/2110 train_time:43554ms step_avg:42.24ms
step:1032/2110 train_time:43614ms step_avg:42.26ms
step:1033/2110 train_time:43676ms step_avg:42.28ms
step:1034/2110 train_time:43735ms step_avg:42.30ms
step:1035/2110 train_time:43798ms step_avg:42.32ms
step:1036/2110 train_time:43858ms step_avg:42.33ms
step:1037/2110 train_time:43920ms step_avg:42.35ms
step:1038/2110 train_time:43979ms step_avg:42.37ms
step:1039/2110 train_time:44040ms step_avg:42.39ms
step:1040/2110 train_time:44100ms step_avg:42.40ms
step:1041/2110 train_time:44161ms step_avg:42.42ms
step:1042/2110 train_time:44219ms step_avg:42.44ms
step:1043/2110 train_time:44280ms step_avg:42.45ms
step:1044/2110 train_time:44340ms step_avg:42.47ms
step:1045/2110 train_time:44401ms step_avg:42.49ms
step:1046/2110 train_time:44460ms step_avg:42.50ms
step:1047/2110 train_time:44521ms step_avg:42.52ms
step:1048/2110 train_time:44580ms step_avg:42.54ms
step:1049/2110 train_time:44641ms step_avg:42.56ms
step:1050/2110 train_time:44701ms step_avg:42.57ms
step:1051/2110 train_time:44762ms step_avg:42.59ms
step:1052/2110 train_time:44822ms step_avg:42.61ms
step:1053/2110 train_time:44883ms step_avg:42.62ms
step:1054/2110 train_time:44942ms step_avg:42.64ms
step:1055/2110 train_time:45005ms step_avg:42.66ms
step:1056/2110 train_time:45064ms step_avg:42.67ms
step:1057/2110 train_time:45125ms step_avg:42.69ms
step:1058/2110 train_time:45184ms step_avg:42.71ms
step:1059/2110 train_time:45245ms step_avg:42.72ms
step:1060/2110 train_time:45304ms step_avg:42.74ms
step:1061/2110 train_time:45365ms step_avg:42.76ms
step:1062/2110 train_time:45425ms step_avg:42.77ms
step:1063/2110 train_time:45486ms step_avg:42.79ms
step:1064/2110 train_time:45545ms step_avg:42.81ms
step:1065/2110 train_time:45606ms step_avg:42.82ms
step:1066/2110 train_time:45665ms step_avg:42.84ms
step:1067/2110 train_time:45726ms step_avg:42.85ms
step:1068/2110 train_time:45786ms step_avg:42.87ms
step:1069/2110 train_time:45847ms step_avg:42.89ms
step:1070/2110 train_time:45906ms step_avg:42.90ms
step:1071/2110 train_time:45967ms step_avg:42.92ms
step:1072/2110 train_time:46025ms step_avg:42.93ms
step:1073/2110 train_time:46087ms step_avg:42.95ms
step:1074/2110 train_time:46145ms step_avg:42.97ms
step:1075/2110 train_time:46206ms step_avg:42.98ms
step:1076/2110 train_time:46265ms step_avg:43.00ms
step:1077/2110 train_time:46327ms step_avg:43.01ms
step:1078/2110 train_time:46385ms step_avg:43.03ms
step:1079/2110 train_time:46446ms step_avg:43.05ms
step:1080/2110 train_time:46505ms step_avg:43.06ms
step:1081/2110 train_time:46566ms step_avg:43.08ms
step:1082/2110 train_time:46624ms step_avg:43.09ms
step:1083/2110 train_time:46686ms step_avg:43.11ms
step:1084/2110 train_time:46745ms step_avg:43.12ms
step:1085/2110 train_time:46806ms step_avg:43.14ms
step:1086/2110 train_time:46865ms step_avg:43.15ms
step:1087/2110 train_time:46926ms step_avg:43.17ms
step:1088/2110 train_time:46985ms step_avg:43.19ms
step:1089/2110 train_time:47047ms step_avg:43.20ms
step:1090/2110 train_time:47105ms step_avg:43.22ms
step:1091/2110 train_time:47166ms step_avg:43.23ms
step:1092/2110 train_time:47226ms step_avg:43.25ms
step:1093/2110 train_time:47287ms step_avg:43.26ms
step:1094/2110 train_time:47345ms step_avg:43.28ms
step:1095/2110 train_time:47406ms step_avg:43.29ms
step:1096/2110 train_time:47465ms step_avg:43.31ms
step:1097/2110 train_time:47526ms step_avg:43.32ms
step:1098/2110 train_time:47586ms step_avg:43.34ms
step:1099/2110 train_time:47646ms step_avg:43.35ms
step:1100/2110 train_time:47705ms step_avg:43.37ms
step:1101/2110 train_time:47766ms step_avg:43.38ms
step:1102/2110 train_time:47825ms step_avg:43.40ms
step:1103/2110 train_time:47886ms step_avg:43.41ms
step:1104/2110 train_time:47945ms step_avg:43.43ms
step:1105/2110 train_time:48006ms step_avg:43.44ms
step:1106/2110 train_time:48065ms step_avg:43.46ms
step:1107/2110 train_time:48127ms step_avg:43.47ms
step:1108/2110 train_time:48186ms step_avg:43.49ms
step:1109/2110 train_time:48247ms step_avg:43.50ms
step:1110/2110 train_time:48306ms step_avg:43.52ms
step:1111/2110 train_time:48367ms step_avg:43.53ms
step:1112/2110 train_time:48425ms step_avg:43.55ms
step:1113/2110 train_time:48487ms step_avg:43.56ms
step:1114/2110 train_time:48546ms step_avg:43.58ms
step:1115/2110 train_time:48607ms step_avg:43.59ms
step:1116/2110 train_time:48666ms step_avg:43.61ms
step:1117/2110 train_time:48727ms step_avg:43.62ms
step:1118/2110 train_time:48787ms step_avg:43.64ms
step:1119/2110 train_time:48847ms step_avg:43.65ms
step:1120/2110 train_time:48906ms step_avg:43.67ms
step:1121/2110 train_time:48967ms step_avg:43.68ms
step:1122/2110 train_time:49027ms step_avg:43.70ms
step:1123/2110 train_time:49087ms step_avg:43.71ms
step:1124/2110 train_time:49146ms step_avg:43.72ms
step:1125/2110 train_time:49208ms step_avg:43.74ms
step:1126/2110 train_time:49266ms step_avg:43.75ms
step:1127/2110 train_time:49327ms step_avg:43.77ms
step:1128/2110 train_time:49386ms step_avg:43.78ms
step:1129/2110 train_time:49447ms step_avg:43.80ms
step:1130/2110 train_time:49506ms step_avg:43.81ms
step:1131/2110 train_time:49566ms step_avg:43.83ms
step:1132/2110 train_time:49625ms step_avg:43.84ms
step:1133/2110 train_time:49687ms step_avg:43.85ms
step:1134/2110 train_time:49746ms step_avg:43.87ms
step:1135/2110 train_time:49807ms step_avg:43.88ms
step:1136/2110 train_time:49866ms step_avg:43.90ms
step:1137/2110 train_time:49927ms step_avg:43.91ms
step:1138/2110 train_time:49987ms step_avg:43.92ms
step:1139/2110 train_time:50047ms step_avg:43.94ms
step:1140/2110 train_time:50107ms step_avg:43.95ms
step:1141/2110 train_time:50167ms step_avg:43.97ms
step:1142/2110 train_time:50226ms step_avg:43.98ms
step:1143/2110 train_time:50287ms step_avg:44.00ms
step:1144/2110 train_time:50346ms step_avg:44.01ms
step:1145/2110 train_time:50407ms step_avg:44.02ms
step:1146/2110 train_time:50466ms step_avg:44.04ms
step:1147/2110 train_time:50527ms step_avg:44.05ms
step:1148/2110 train_time:50586ms step_avg:44.06ms
step:1149/2110 train_time:50648ms step_avg:44.08ms
step:1150/2110 train_time:50707ms step_avg:44.09ms
step:1151/2110 train_time:50768ms step_avg:44.11ms
step:1152/2110 train_time:50826ms step_avg:44.12ms
step:1153/2110 train_time:50888ms step_avg:44.14ms
step:1154/2110 train_time:50947ms step_avg:44.15ms
step:1155/2110 train_time:51008ms step_avg:44.16ms
step:1156/2110 train_time:51066ms step_avg:44.17ms
step:1157/2110 train_time:51128ms step_avg:44.19ms
step:1158/2110 train_time:51187ms step_avg:44.20ms
step:1159/2110 train_time:51247ms step_avg:44.22ms
step:1160/2110 train_time:51306ms step_avg:44.23ms
step:1161/2110 train_time:51367ms step_avg:44.24ms
step:1162/2110 train_time:51425ms step_avg:44.26ms
step:1163/2110 train_time:51486ms step_avg:44.27ms
step:1164/2110 train_time:51545ms step_avg:44.28ms
step:1165/2110 train_time:51606ms step_avg:44.30ms
step:1166/2110 train_time:51666ms step_avg:44.31ms
step:1167/2110 train_time:51726ms step_avg:44.32ms
step:1168/2110 train_time:51785ms step_avg:44.34ms
step:1169/2110 train_time:51846ms step_avg:44.35ms
step:1170/2110 train_time:51905ms step_avg:44.36ms
step:1171/2110 train_time:51966ms step_avg:44.38ms
step:1172/2110 train_time:52025ms step_avg:44.39ms
step:1173/2110 train_time:52086ms step_avg:44.40ms
step:1174/2110 train_time:52145ms step_avg:44.42ms
step:1175/2110 train_time:52207ms step_avg:44.43ms
step:1176/2110 train_time:52265ms step_avg:44.44ms
step:1177/2110 train_time:52326ms step_avg:44.46ms
step:1178/2110 train_time:52386ms step_avg:44.47ms
step:1179/2110 train_time:52447ms step_avg:44.48ms
step:1180/2110 train_time:52506ms step_avg:44.50ms
step:1181/2110 train_time:52567ms step_avg:44.51ms
step:1182/2110 train_time:52626ms step_avg:44.52ms
step:1183/2110 train_time:52687ms step_avg:44.54ms
step:1184/2110 train_time:52746ms step_avg:44.55ms
step:1185/2110 train_time:52807ms step_avg:44.56ms
step:1186/2110 train_time:52866ms step_avg:44.58ms
step:1187/2110 train_time:52927ms step_avg:44.59ms
step:1188/2110 train_time:52987ms step_avg:44.60ms
step:1189/2110 train_time:53047ms step_avg:44.61ms
step:1190/2110 train_time:53106ms step_avg:44.63ms
step:1191/2110 train_time:53166ms step_avg:44.64ms
step:1192/2110 train_time:53226ms step_avg:44.65ms
step:1193/2110 train_time:53287ms step_avg:44.67ms
step:1194/2110 train_time:53345ms step_avg:44.68ms
step:1195/2110 train_time:53406ms step_avg:44.69ms
step:1196/2110 train_time:53465ms step_avg:44.70ms
step:1197/2110 train_time:53526ms step_avg:44.72ms
step:1198/2110 train_time:53585ms step_avg:44.73ms
step:1199/2110 train_time:53647ms step_avg:44.74ms
step:1200/2110 train_time:53706ms step_avg:44.75ms
step:1201/2110 train_time:53766ms step_avg:44.77ms
step:1202/2110 train_time:53826ms step_avg:44.78ms
step:1203/2110 train_time:53887ms step_avg:44.79ms
step:1204/2110 train_time:53945ms step_avg:44.81ms
step:1205/2110 train_time:54006ms step_avg:44.82ms
step:1206/2110 train_time:54065ms step_avg:44.83ms
step:1207/2110 train_time:54126ms step_avg:44.84ms
step:1208/2110 train_time:54186ms step_avg:44.86ms
step:1209/2110 train_time:54247ms step_avg:44.87ms
step:1210/2110 train_time:54306ms step_avg:44.88ms
step:1211/2110 train_time:54366ms step_avg:44.89ms
step:1212/2110 train_time:54426ms step_avg:44.91ms
step:1213/2110 train_time:54488ms step_avg:44.92ms
step:1214/2110 train_time:54546ms step_avg:44.93ms
step:1215/2110 train_time:54606ms step_avg:44.94ms
step:1216/2110 train_time:54665ms step_avg:44.95ms
step:1217/2110 train_time:54727ms step_avg:44.97ms
step:1218/2110 train_time:54786ms step_avg:44.98ms
step:1219/2110 train_time:54846ms step_avg:44.99ms
step:1220/2110 train_time:54905ms step_avg:45.00ms
step:1221/2110 train_time:54966ms step_avg:45.02ms
step:1222/2110 train_time:55025ms step_avg:45.03ms
step:1223/2110 train_time:55086ms step_avg:45.04ms
step:1224/2110 train_time:55145ms step_avg:45.05ms
step:1225/2110 train_time:55206ms step_avg:45.07ms
step:1226/2110 train_time:55265ms step_avg:45.08ms
step:1227/2110 train_time:55326ms step_avg:45.09ms
step:1228/2110 train_time:55385ms step_avg:45.10ms
step:1229/2110 train_time:55446ms step_avg:45.11ms
step:1230/2110 train_time:55506ms step_avg:45.13ms
step:1231/2110 train_time:55566ms step_avg:45.14ms
step:1232/2110 train_time:55625ms step_avg:45.15ms
step:1233/2110 train_time:55686ms step_avg:45.16ms
step:1234/2110 train_time:55745ms step_avg:45.17ms
step:1235/2110 train_time:55806ms step_avg:45.19ms
step:1236/2110 train_time:55865ms step_avg:45.20ms
step:1237/2110 train_time:55925ms step_avg:45.21ms
step:1238/2110 train_time:55985ms step_avg:45.22ms
step:1239/2110 train_time:56046ms step_avg:45.23ms
step:1240/2110 train_time:56105ms step_avg:45.25ms
step:1241/2110 train_time:56166ms step_avg:45.26ms
step:1242/2110 train_time:56225ms step_avg:45.27ms
step:1243/2110 train_time:56287ms step_avg:45.28ms
step:1244/2110 train_time:56346ms step_avg:45.29ms
step:1245/2110 train_time:56407ms step_avg:45.31ms
step:1246/2110 train_time:56466ms step_avg:45.32ms
step:1247/2110 train_time:56527ms step_avg:45.33ms
step:1248/2110 train_time:56586ms step_avg:45.34ms
step:1249/2110 train_time:56647ms step_avg:45.35ms
step:1250/2110 train_time:56706ms step_avg:45.36ms
step:1250/2110 val_loss:3.5811 train_time:56769ms step_avg:45.42ms
step:1251/2110 train_time:56790ms step_avg:45.40ms
step:1252/2110 train_time:56833ms step_avg:45.39ms
step:1253/2110 train_time:56895ms step_avg:45.41ms
step:1254/2110 train_time:56955ms step_avg:45.42ms
step:1255/2110 train_time:57015ms step_avg:45.43ms
step:1256/2110 train_time:57074ms step_avg:45.44ms
step:1257/2110 train_time:57135ms step_avg:45.45ms
step:1258/2110 train_time:57194ms step_avg:45.46ms
step:1259/2110 train_time:57256ms step_avg:45.48ms
step:1260/2110 train_time:57314ms step_avg:45.49ms
step:1261/2110 train_time:57374ms step_avg:45.50ms
step:1262/2110 train_time:57433ms step_avg:45.51ms
step:1263/2110 train_time:57493ms step_avg:45.52ms
step:1264/2110 train_time:57552ms step_avg:45.53ms
step:1265/2110 train_time:57612ms step_avg:45.54ms
step:1266/2110 train_time:57671ms step_avg:45.55ms
step:1267/2110 train_time:57733ms step_avg:45.57ms
step:1268/2110 train_time:57793ms step_avg:45.58ms
step:1269/2110 train_time:57856ms step_avg:45.59ms
step:1270/2110 train_time:57915ms step_avg:45.60ms
step:1271/2110 train_time:57978ms step_avg:45.62ms
step:1272/2110 train_time:58037ms step_avg:45.63ms
step:1273/2110 train_time:58098ms step_avg:45.64ms
step:1274/2110 train_time:58157ms step_avg:45.65ms
step:1275/2110 train_time:58218ms step_avg:45.66ms
step:1276/2110 train_time:58276ms step_avg:45.67ms
step:1277/2110 train_time:58337ms step_avg:45.68ms
step:1278/2110 train_time:58396ms step_avg:45.69ms
step:1279/2110 train_time:58457ms step_avg:45.71ms
step:1280/2110 train_time:58515ms step_avg:45.72ms
step:1281/2110 train_time:58576ms step_avg:45.73ms
step:1282/2110 train_time:58636ms step_avg:45.74ms
step:1283/2110 train_time:58697ms step_avg:45.75ms
step:1284/2110 train_time:58757ms step_avg:45.76ms
step:1285/2110 train_time:58819ms step_avg:45.77ms
step:1286/2110 train_time:58878ms step_avg:45.78ms
step:1287/2110 train_time:58940ms step_avg:45.80ms
step:1288/2110 train_time:58999ms step_avg:45.81ms
step:1289/2110 train_time:59061ms step_avg:45.82ms
step:1290/2110 train_time:59120ms step_avg:45.83ms
step:1291/2110 train_time:59182ms step_avg:45.84ms
step:1292/2110 train_time:59242ms step_avg:45.85ms
step:1293/2110 train_time:59304ms step_avg:45.87ms
step:1294/2110 train_time:59363ms step_avg:45.88ms
step:1295/2110 train_time:59424ms step_avg:45.89ms
step:1296/2110 train_time:59483ms step_avg:45.90ms
step:1297/2110 train_time:59544ms step_avg:45.91ms
step:1298/2110 train_time:59603ms step_avg:45.92ms
step:1299/2110 train_time:59664ms step_avg:45.93ms
step:1300/2110 train_time:59724ms step_avg:45.94ms
step:1301/2110 train_time:59785ms step_avg:45.95ms
step:1302/2110 train_time:59845ms step_avg:45.96ms
step:1303/2110 train_time:59906ms step_avg:45.98ms
step:1304/2110 train_time:59966ms step_avg:45.99ms
step:1305/2110 train_time:60027ms step_avg:46.00ms
step:1306/2110 train_time:60086ms step_avg:46.01ms
step:1307/2110 train_time:60148ms step_avg:46.02ms
step:1308/2110 train_time:60206ms step_avg:46.03ms
step:1309/2110 train_time:60268ms step_avg:46.04ms
step:1310/2110 train_time:60327ms step_avg:46.05ms
step:1311/2110 train_time:60388ms step_avg:46.06ms
step:1312/2110 train_time:60447ms step_avg:46.07ms
step:1313/2110 train_time:60508ms step_avg:46.08ms
step:1314/2110 train_time:60569ms step_avg:46.09ms
step:1315/2110 train_time:60629ms step_avg:46.11ms
step:1316/2110 train_time:60688ms step_avg:46.12ms
step:1317/2110 train_time:60749ms step_avg:46.13ms
step:1318/2110 train_time:60808ms step_avg:46.14ms
step:1319/2110 train_time:60868ms step_avg:46.15ms
step:1320/2110 train_time:60928ms step_avg:46.16ms
step:1321/2110 train_time:60989ms step_avg:46.17ms
step:1322/2110 train_time:61048ms step_avg:46.18ms
step:1323/2110 train_time:61108ms step_avg:46.19ms
step:1324/2110 train_time:61167ms step_avg:46.20ms
step:1325/2110 train_time:61229ms step_avg:46.21ms
step:1326/2110 train_time:61287ms step_avg:46.22ms
step:1327/2110 train_time:61349ms step_avg:46.23ms
step:1328/2110 train_time:61407ms step_avg:46.24ms
step:1329/2110 train_time:61469ms step_avg:46.25ms
step:1330/2110 train_time:61528ms step_avg:46.26ms
step:1331/2110 train_time:61589ms step_avg:46.27ms
step:1332/2110 train_time:61649ms step_avg:46.28ms
step:1333/2110 train_time:61709ms step_avg:46.29ms
step:1334/2110 train_time:61768ms step_avg:46.30ms
step:1335/2110 train_time:61830ms step_avg:46.31ms
step:1336/2110 train_time:61889ms step_avg:46.32ms
step:1337/2110 train_time:61949ms step_avg:46.33ms
step:1338/2110 train_time:62008ms step_avg:46.34ms
step:1339/2110 train_time:62069ms step_avg:46.35ms
step:1340/2110 train_time:62128ms step_avg:46.36ms
step:1341/2110 train_time:62190ms step_avg:46.38ms
step:1342/2110 train_time:62248ms step_avg:46.38ms
step:1343/2110 train_time:62309ms step_avg:46.40ms
step:1344/2110 train_time:62368ms step_avg:46.40ms
step:1345/2110 train_time:62430ms step_avg:46.42ms
step:1346/2110 train_time:62489ms step_avg:46.43ms
step:1347/2110 train_time:62549ms step_avg:46.44ms
step:1348/2110 train_time:62609ms step_avg:46.45ms
step:1349/2110 train_time:62669ms step_avg:46.46ms
step:1350/2110 train_time:62729ms step_avg:46.47ms
step:1351/2110 train_time:62789ms step_avg:46.48ms
step:1352/2110 train_time:62848ms step_avg:46.49ms
step:1353/2110 train_time:62909ms step_avg:46.50ms
step:1354/2110 train_time:62968ms step_avg:46.51ms
step:1355/2110 train_time:63030ms step_avg:46.52ms
step:1356/2110 train_time:63089ms step_avg:46.53ms
step:1357/2110 train_time:63150ms step_avg:46.54ms
step:1358/2110 train_time:63209ms step_avg:46.55ms
step:1359/2110 train_time:63270ms step_avg:46.56ms
step:1360/2110 train_time:63329ms step_avg:46.57ms
step:1361/2110 train_time:63390ms step_avg:46.58ms
step:1362/2110 train_time:63449ms step_avg:46.58ms
step:1363/2110 train_time:63509ms step_avg:46.60ms
step:1364/2110 train_time:63569ms step_avg:46.60ms
step:1365/2110 train_time:63630ms step_avg:46.62ms
step:1366/2110 train_time:63689ms step_avg:46.62ms
step:1367/2110 train_time:63749ms step_avg:46.63ms
step:1368/2110 train_time:63809ms step_avg:46.64ms
step:1369/2110 train_time:63869ms step_avg:46.65ms
step:1370/2110 train_time:63929ms step_avg:46.66ms
step:1371/2110 train_time:63989ms step_avg:46.67ms
step:1372/2110 train_time:64049ms step_avg:46.68ms
step:1373/2110 train_time:64109ms step_avg:46.69ms
step:1374/2110 train_time:64168ms step_avg:46.70ms
step:1375/2110 train_time:64229ms step_avg:46.71ms
step:1376/2110 train_time:64289ms step_avg:46.72ms
step:1377/2110 train_time:64350ms step_avg:46.73ms
step:1378/2110 train_time:64409ms step_avg:46.74ms
step:1379/2110 train_time:64470ms step_avg:46.75ms
step:1380/2110 train_time:64529ms step_avg:46.76ms
step:1381/2110 train_time:64590ms step_avg:46.77ms
step:1382/2110 train_time:64678ms step_avg:46.80ms
step:1383/2110 train_time:64768ms step_avg:46.83ms
step:1384/2110 train_time:64855ms step_avg:46.86ms
step:1385/2110 train_time:64942ms step_avg:46.89ms
step:1386/2110 train_time:65029ms step_avg:46.92ms
step:1387/2110 train_time:65118ms step_avg:46.95ms
step:1388/2110 train_time:65205ms step_avg:46.98ms
step:1389/2110 train_time:65295ms step_avg:47.01ms
step:1390/2110 train_time:65381ms step_avg:47.04ms
step:1391/2110 train_time:65470ms step_avg:47.07ms
step:1392/2110 train_time:65558ms step_avg:47.10ms
step:1393/2110 train_time:65646ms step_avg:47.13ms
step:1394/2110 train_time:65733ms step_avg:47.15ms
step:1395/2110 train_time:65822ms step_avg:47.18ms
step:1396/2110 train_time:65909ms step_avg:47.21ms
step:1397/2110 train_time:65998ms step_avg:47.24ms
step:1398/2110 train_time:66084ms step_avg:47.27ms
step:1399/2110 train_time:66174ms step_avg:47.30ms
step:1400/2110 train_time:66261ms step_avg:47.33ms
step:1401/2110 train_time:66350ms step_avg:47.36ms
step:1402/2110 train_time:66437ms step_avg:47.39ms
step:1403/2110 train_time:66525ms step_avg:47.42ms
step:1404/2110 train_time:66613ms step_avg:47.44ms
step:1405/2110 train_time:66702ms step_avg:47.47ms
step:1406/2110 train_time:66789ms step_avg:47.50ms
step:1407/2110 train_time:66878ms step_avg:47.53ms
step:1408/2110 train_time:66965ms step_avg:47.56ms
step:1409/2110 train_time:67054ms step_avg:47.59ms
step:1410/2110 train_time:67141ms step_avg:47.62ms
step:1411/2110 train_time:67230ms step_avg:47.65ms
step:1412/2110 train_time:67317ms step_avg:47.67ms
step:1413/2110 train_time:67405ms step_avg:47.70ms
step:1414/2110 train_time:67493ms step_avg:47.73ms
step:1415/2110 train_time:67583ms step_avg:47.76ms
step:1416/2110 train_time:67669ms step_avg:47.79ms
step:1417/2110 train_time:67758ms step_avg:47.82ms
step:1418/2110 train_time:67846ms step_avg:47.85ms
step:1419/2110 train_time:67935ms step_avg:47.88ms
step:1420/2110 train_time:68022ms step_avg:47.90ms
step:1421/2110 train_time:68111ms step_avg:47.93ms
step:1422/2110 train_time:68199ms step_avg:47.96ms
step:1423/2110 train_time:68287ms step_avg:47.99ms
step:1424/2110 train_time:68374ms step_avg:48.02ms
step:1425/2110 train_time:68463ms step_avg:48.04ms
step:1426/2110 train_time:68549ms step_avg:48.07ms
step:1427/2110 train_time:68639ms step_avg:48.10ms
step:1428/2110 train_time:68725ms step_avg:48.13ms
step:1429/2110 train_time:68814ms step_avg:48.16ms
step:1430/2110 train_time:68901ms step_avg:48.18ms
step:1431/2110 train_time:68989ms step_avg:48.21ms
step:1432/2110 train_time:69076ms step_avg:48.24ms
step:1433/2110 train_time:69165ms step_avg:48.27ms
step:1434/2110 train_time:69251ms step_avg:48.29ms
step:1435/2110 train_time:69341ms step_avg:48.32ms
step:1436/2110 train_time:69427ms step_avg:48.35ms
step:1437/2110 train_time:69516ms step_avg:48.38ms
step:1438/2110 train_time:69603ms step_avg:48.40ms
step:1439/2110 train_time:69692ms step_avg:48.43ms
step:1440/2110 train_time:69779ms step_avg:48.46ms
step:1441/2110 train_time:69868ms step_avg:48.49ms
step:1442/2110 train_time:69955ms step_avg:48.51ms
step:1443/2110 train_time:70044ms step_avg:48.54ms
step:1444/2110 train_time:70131ms step_avg:48.57ms
step:1445/2110 train_time:70220ms step_avg:48.60ms
step:1446/2110 train_time:70307ms step_avg:48.62ms
step:1447/2110 train_time:70396ms step_avg:48.65ms
step:1448/2110 train_time:70483ms step_avg:48.68ms
step:1449/2110 train_time:70571ms step_avg:48.70ms
step:1450/2110 train_time:70658ms step_avg:48.73ms
step:1451/2110 train_time:70747ms step_avg:48.76ms
step:1452/2110 train_time:70834ms step_avg:48.78ms
step:1453/2110 train_time:70922ms step_avg:48.81ms
step:1454/2110 train_time:71010ms step_avg:48.84ms
step:1455/2110 train_time:71100ms step_avg:48.87ms
step:1456/2110 train_time:71186ms step_avg:48.89ms
step:1457/2110 train_time:71276ms step_avg:48.92ms
step:1458/2110 train_time:71362ms step_avg:48.95ms
step:1459/2110 train_time:71451ms step_avg:48.97ms
step:1460/2110 train_time:71538ms step_avg:49.00ms
step:1461/2110 train_time:71626ms step_avg:49.03ms
step:1462/2110 train_time:71713ms step_avg:49.05ms
step:1463/2110 train_time:71802ms step_avg:49.08ms
step:1464/2110 train_time:71889ms step_avg:49.10ms
step:1465/2110 train_time:71978ms step_avg:49.13ms
step:1466/2110 train_time:72066ms step_avg:49.16ms
step:1467/2110 train_time:72155ms step_avg:49.19ms
step:1468/2110 train_time:72242ms step_avg:49.21ms
step:1469/2110 train_time:72331ms step_avg:49.24ms
step:1470/2110 train_time:72418ms step_avg:49.26ms
step:1471/2110 train_time:72506ms step_avg:49.29ms
step:1472/2110 train_time:72594ms step_avg:49.32ms
step:1473/2110 train_time:72683ms step_avg:49.34ms
step:1474/2110 train_time:72770ms step_avg:49.37ms
step:1475/2110 train_time:72859ms step_avg:49.40ms
step:1476/2110 train_time:72946ms step_avg:49.42ms
step:1477/2110 train_time:73036ms step_avg:49.45ms
step:1478/2110 train_time:73122ms step_avg:49.47ms
step:1479/2110 train_time:73210ms step_avg:49.50ms
step:1480/2110 train_time:73298ms step_avg:49.53ms
step:1481/2110 train_time:73386ms step_avg:49.55ms
step:1482/2110 train_time:73472ms step_avg:49.58ms
step:1483/2110 train_time:73562ms step_avg:49.60ms
step:1484/2110 train_time:73648ms step_avg:49.63ms
step:1485/2110 train_time:73737ms step_avg:49.65ms
step:1486/2110 train_time:73824ms step_avg:49.68ms
step:1487/2110 train_time:73913ms step_avg:49.71ms
step:1488/2110 train_time:74000ms step_avg:49.73ms
step:1489/2110 train_time:74090ms step_avg:49.76ms
step:1490/2110 train_time:74177ms step_avg:49.78ms
step:1491/2110 train_time:74267ms step_avg:49.81ms
step:1492/2110 train_time:74354ms step_avg:49.84ms
step:1493/2110 train_time:74443ms step_avg:49.86ms
step:1494/2110 train_time:74530ms step_avg:49.89ms
step:1495/2110 train_time:74619ms step_avg:49.91ms
step:1496/2110 train_time:74706ms step_avg:49.94ms
step:1497/2110 train_time:74795ms step_avg:49.96ms
step:1498/2110 train_time:74882ms step_avg:49.99ms
step:1499/2110 train_time:74971ms step_avg:50.01ms
step:1500/2110 train_time:75059ms step_avg:50.04ms
step:1500/2110 val_loss:3.4737 train_time:75147ms step_avg:50.10ms
step:1501/2110 train_time:75170ms step_avg:50.08ms
step:1502/2110 train_time:75235ms step_avg:50.09ms
step:1503/2110 train_time:75329ms step_avg:50.12ms
step:1504/2110 train_time:75415ms step_avg:50.14ms
step:1505/2110 train_time:75504ms step_avg:50.17ms
step:1506/2110 train_time:75591ms step_avg:50.19ms
step:1507/2110 train_time:75678ms step_avg:50.22ms
step:1508/2110 train_time:75765ms step_avg:50.24ms
step:1509/2110 train_time:75854ms step_avg:50.27ms
step:1510/2110 train_time:75941ms step_avg:50.29ms
step:1511/2110 train_time:76030ms step_avg:50.32ms
step:1512/2110 train_time:76119ms step_avg:50.34ms
step:1513/2110 train_time:76211ms step_avg:50.37ms
step:1514/2110 train_time:76300ms step_avg:50.40ms
step:1515/2110 train_time:76390ms step_avg:50.42ms
step:1516/2110 train_time:76477ms step_avg:50.45ms
step:1517/2110 train_time:76565ms step_avg:50.47ms
step:1518/2110 train_time:76651ms step_avg:50.49ms
step:1519/2110 train_time:76739ms step_avg:50.52ms
step:1520/2110 train_time:76826ms step_avg:50.54ms
step:1521/2110 train_time:76914ms step_avg:50.57ms
step:1522/2110 train_time:77002ms step_avg:50.59ms
step:1523/2110 train_time:77092ms step_avg:50.62ms
step:1524/2110 train_time:77179ms step_avg:50.64ms
step:1525/2110 train_time:77270ms step_avg:50.67ms
step:1526/2110 train_time:77358ms step_avg:50.69ms
step:1527/2110 train_time:77448ms step_avg:50.72ms
step:1528/2110 train_time:77536ms step_avg:50.74ms
step:1529/2110 train_time:77624ms step_avg:50.77ms
step:1530/2110 train_time:77710ms step_avg:50.79ms
step:1531/2110 train_time:77798ms step_avg:50.82ms
step:1532/2110 train_time:77885ms step_avg:50.84ms
step:1533/2110 train_time:77974ms step_avg:50.86ms
step:1534/2110 train_time:78062ms step_avg:50.89ms
step:1535/2110 train_time:78151ms step_avg:50.91ms
step:1536/2110 train_time:78239ms step_avg:50.94ms
step:1537/2110 train_time:78329ms step_avg:50.96ms
step:1538/2110 train_time:78416ms step_avg:50.99ms
step:1539/2110 train_time:78506ms step_avg:51.01ms
step:1540/2110 train_time:78592ms step_avg:51.03ms
step:1541/2110 train_time:78681ms step_avg:51.06ms
step:1542/2110 train_time:78767ms step_avg:51.08ms
step:1543/2110 train_time:78855ms step_avg:51.11ms
step:1544/2110 train_time:78942ms step_avg:51.13ms
step:1545/2110 train_time:79031ms step_avg:51.15ms
step:1546/2110 train_time:79120ms step_avg:51.18ms
step:1547/2110 train_time:79210ms step_avg:51.20ms
step:1548/2110 train_time:79298ms step_avg:51.23ms
step:1549/2110 train_time:79388ms step_avg:51.25ms
step:1550/2110 train_time:79475ms step_avg:51.27ms
step:1551/2110 train_time:79564ms step_avg:51.30ms
step:1552/2110 train_time:79650ms step_avg:51.32ms
step:1553/2110 train_time:79738ms step_avg:51.34ms
step:1554/2110 train_time:79824ms step_avg:51.37ms
step:1555/2110 train_time:79912ms step_avg:51.39ms
step:1556/2110 train_time:80000ms step_avg:51.41ms
step:1557/2110 train_time:80091ms step_avg:51.44ms
step:1558/2110 train_time:80178ms step_avg:51.46ms
step:1559/2110 train_time:80267ms step_avg:51.49ms
step:1560/2110 train_time:80355ms step_avg:51.51ms
step:1561/2110 train_time:80443ms step_avg:51.53ms
step:1562/2110 train_time:80530ms step_avg:51.56ms
step:1563/2110 train_time:80619ms step_avg:51.58ms
step:1564/2110 train_time:80706ms step_avg:51.60ms
step:1565/2110 train_time:80795ms step_avg:51.63ms
step:1566/2110 train_time:80881ms step_avg:51.65ms
step:1567/2110 train_time:80969ms step_avg:51.67ms
step:1568/2110 train_time:81057ms step_avg:51.69ms
step:1569/2110 train_time:81146ms step_avg:51.72ms
step:1570/2110 train_time:81233ms step_avg:51.74ms
step:1571/2110 train_time:81322ms step_avg:51.76ms
step:1572/2110 train_time:81410ms step_avg:51.79ms
step:1573/2110 train_time:81499ms step_avg:51.81ms
step:1574/2110 train_time:81586ms step_avg:51.83ms
step:1575/2110 train_time:81675ms step_avg:51.86ms
step:1576/2110 train_time:81762ms step_avg:51.88ms
step:1577/2110 train_time:81851ms step_avg:51.90ms
step:1578/2110 train_time:81938ms step_avg:51.93ms
step:1579/2110 train_time:82028ms step_avg:51.95ms
step:1580/2110 train_time:82115ms step_avg:51.97ms
step:1581/2110 train_time:82205ms step_avg:52.00ms
step:1582/2110 train_time:82292ms step_avg:52.02ms
step:1583/2110 train_time:82381ms step_avg:52.04ms
step:1584/2110 train_time:82468ms step_avg:52.06ms
step:1585/2110 train_time:82556ms step_avg:52.09ms
step:1586/2110 train_time:82642ms step_avg:52.11ms
step:1587/2110 train_time:82732ms step_avg:52.13ms
step:1588/2110 train_time:82818ms step_avg:52.15ms
step:1589/2110 train_time:82907ms step_avg:52.18ms
step:1590/2110 train_time:82994ms step_avg:52.20ms
step:1591/2110 train_time:83084ms step_avg:52.22ms
step:1592/2110 train_time:83171ms step_avg:52.24ms
step:1593/2110 train_time:83260ms step_avg:52.27ms
step:1594/2110 train_time:83347ms step_avg:52.29ms
step:1595/2110 train_time:83436ms step_avg:52.31ms
step:1596/2110 train_time:83523ms step_avg:52.33ms
step:1597/2110 train_time:83612ms step_avg:52.36ms
step:1598/2110 train_time:83699ms step_avg:52.38ms
step:1599/2110 train_time:83788ms step_avg:52.40ms
step:1600/2110 train_time:83875ms step_avg:52.42ms
step:1601/2110 train_time:83963ms step_avg:52.44ms
step:1602/2110 train_time:84051ms step_avg:52.47ms
step:1603/2110 train_time:84140ms step_avg:52.49ms
step:1604/2110 train_time:84228ms step_avg:52.51ms
step:1605/2110 train_time:84316ms step_avg:52.53ms
step:1606/2110 train_time:84404ms step_avg:52.56ms
step:1607/2110 train_time:84494ms step_avg:52.58ms
step:1608/2110 train_time:84581ms step_avg:52.60ms
step:1609/2110 train_time:84669ms step_avg:52.62ms
step:1610/2110 train_time:84756ms step_avg:52.64ms
step:1611/2110 train_time:84844ms step_avg:52.67ms
step:1612/2110 train_time:84931ms step_avg:52.69ms
step:1613/2110 train_time:85019ms step_avg:52.71ms
step:1614/2110 train_time:85107ms step_avg:52.73ms
step:1615/2110 train_time:85196ms step_avg:52.75ms
step:1616/2110 train_time:85283ms step_avg:52.77ms
step:1617/2110 train_time:85373ms step_avg:52.80ms
step:1618/2110 train_time:85460ms step_avg:52.82ms
step:1619/2110 train_time:85550ms step_avg:52.84ms
step:1620/2110 train_time:85636ms step_avg:52.86ms
step:1621/2110 train_time:85725ms step_avg:52.88ms
step:1622/2110 train_time:85812ms step_avg:52.91ms
step:1623/2110 train_time:85901ms step_avg:52.93ms
step:1624/2110 train_time:85987ms step_avg:52.95ms
step:1625/2110 train_time:86076ms step_avg:52.97ms
step:1626/2110 train_time:86163ms step_avg:52.99ms
step:1627/2110 train_time:86252ms step_avg:53.01ms
step:1628/2110 train_time:86340ms step_avg:53.03ms
step:1629/2110 train_time:86430ms step_avg:53.06ms
step:1630/2110 train_time:86517ms step_avg:53.08ms
step:1631/2110 train_time:86606ms step_avg:53.10ms
step:1632/2110 train_time:86693ms step_avg:53.12ms
step:1633/2110 train_time:86782ms step_avg:53.14ms
step:1634/2110 train_time:86869ms step_avg:53.16ms
step:1635/2110 train_time:86957ms step_avg:53.18ms
step:1636/2110 train_time:87044ms step_avg:53.21ms
step:1637/2110 train_time:87134ms step_avg:53.23ms
step:1638/2110 train_time:87221ms step_avg:53.25ms
step:1639/2110 train_time:87310ms step_avg:53.27ms
step:1640/2110 train_time:87397ms step_avg:53.29ms
step:1641/2110 train_time:87485ms step_avg:53.31ms
step:1642/2110 train_time:87573ms step_avg:53.33ms
step:1643/2110 train_time:87662ms step_avg:53.35ms
step:1644/2110 train_time:87750ms step_avg:53.38ms
step:1645/2110 train_time:87838ms step_avg:53.40ms
step:1646/2110 train_time:87925ms step_avg:53.42ms
step:1647/2110 train_time:88015ms step_avg:53.44ms
step:1648/2110 train_time:88102ms step_avg:53.46ms
step:1649/2110 train_time:88192ms step_avg:53.48ms
step:1650/2110 train_time:88279ms step_avg:53.50ms
step:1651/2110 train_time:88368ms step_avg:53.52ms
step:1652/2110 train_time:88455ms step_avg:53.54ms
step:1653/2110 train_time:88544ms step_avg:53.57ms
step:1654/2110 train_time:88632ms step_avg:53.59ms
step:1655/2110 train_time:88721ms step_avg:53.61ms
step:1656/2110 train_time:88807ms step_avg:53.63ms
step:1657/2110 train_time:88896ms step_avg:53.65ms
step:1658/2110 train_time:88983ms step_avg:53.67ms
step:1659/2110 train_time:89072ms step_avg:53.69ms
step:1660/2110 train_time:89160ms step_avg:53.71ms
step:1661/2110 train_time:89249ms step_avg:53.73ms
step:1662/2110 train_time:89336ms step_avg:53.75ms
step:1663/2110 train_time:89424ms step_avg:53.77ms
step:1664/2110 train_time:89511ms step_avg:53.79ms
step:1665/2110 train_time:89601ms step_avg:53.81ms
step:1666/2110 train_time:89688ms step_avg:53.83ms
step:1667/2110 train_time:89776ms step_avg:53.85ms
step:1668/2110 train_time:89863ms step_avg:53.87ms
step:1669/2110 train_time:89952ms step_avg:53.90ms
step:1670/2110 train_time:90039ms step_avg:53.92ms
step:1671/2110 train_time:90129ms step_avg:53.94ms
step:1672/2110 train_time:90216ms step_avg:53.96ms
step:1673/2110 train_time:90305ms step_avg:53.98ms
step:1674/2110 train_time:90392ms step_avg:54.00ms
step:1675/2110 train_time:90481ms step_avg:54.02ms
step:1676/2110 train_time:90569ms step_avg:54.04ms
step:1677/2110 train_time:90657ms step_avg:54.06ms
step:1678/2110 train_time:90745ms step_avg:54.08ms
step:1679/2110 train_time:90834ms step_avg:54.10ms
step:1680/2110 train_time:90921ms step_avg:54.12ms
step:1681/2110 train_time:91011ms step_avg:54.14ms
step:1682/2110 train_time:91099ms step_avg:54.16ms
step:1683/2110 train_time:91188ms step_avg:54.18ms
step:1684/2110 train_time:91274ms step_avg:54.20ms
step:1685/2110 train_time:91364ms step_avg:54.22ms
step:1686/2110 train_time:91450ms step_avg:54.24ms
step:1687/2110 train_time:91539ms step_avg:54.26ms
step:1688/2110 train_time:91626ms step_avg:54.28ms
step:1689/2110 train_time:91714ms step_avg:54.30ms
step:1690/2110 train_time:91801ms step_avg:54.32ms
step:1691/2110 train_time:91890ms step_avg:54.34ms
step:1692/2110 train_time:91977ms step_avg:54.36ms
step:1693/2110 train_time:92067ms step_avg:54.38ms
step:1694/2110 train_time:92153ms step_avg:54.40ms
step:1695/2110 train_time:92241ms step_avg:54.42ms
step:1696/2110 train_time:92329ms step_avg:54.44ms
step:1697/2110 train_time:92418ms step_avg:54.46ms
step:1698/2110 train_time:92505ms step_avg:54.48ms
step:1699/2110 train_time:92593ms step_avg:54.50ms
step:1700/2110 train_time:92681ms step_avg:54.52ms
step:1701/2110 train_time:92770ms step_avg:54.54ms
step:1702/2110 train_time:92857ms step_avg:54.56ms
step:1703/2110 train_time:92945ms step_avg:54.58ms
step:1704/2110 train_time:93031ms step_avg:54.60ms
step:1705/2110 train_time:93120ms step_avg:54.62ms
step:1706/2110 train_time:93209ms step_avg:54.64ms
step:1707/2110 train_time:93298ms step_avg:54.66ms
step:1708/2110 train_time:93386ms step_avg:54.68ms
step:1709/2110 train_time:93475ms step_avg:54.70ms
step:1710/2110 train_time:93561ms step_avg:54.71ms
step:1711/2110 train_time:93650ms step_avg:54.73ms
step:1712/2110 train_time:93737ms step_avg:54.75ms
step:1713/2110 train_time:93827ms step_avg:54.77ms
step:1714/2110 train_time:93913ms step_avg:54.79ms
step:1715/2110 train_time:94002ms step_avg:54.81ms
step:1716/2110 train_time:94089ms step_avg:54.83ms
step:1717/2110 train_time:94178ms step_avg:54.85ms
step:1718/2110 train_time:94265ms step_avg:54.87ms
step:1719/2110 train_time:94355ms step_avg:54.89ms
step:1720/2110 train_time:94442ms step_avg:54.91ms
step:1721/2110 train_time:94531ms step_avg:54.93ms
step:1722/2110 train_time:94618ms step_avg:54.95ms
step:1723/2110 train_time:94707ms step_avg:54.97ms
step:1724/2110 train_time:94794ms step_avg:54.99ms
step:1725/2110 train_time:94883ms step_avg:55.00ms
step:1726/2110 train_time:94970ms step_avg:55.02ms
step:1727/2110 train_time:95058ms step_avg:55.04ms
step:1728/2110 train_time:95145ms step_avg:55.06ms
step:1729/2110 train_time:95235ms step_avg:55.08ms
step:1730/2110 train_time:95322ms step_avg:55.10ms
step:1731/2110 train_time:95411ms step_avg:55.12ms
step:1732/2110 train_time:95498ms step_avg:55.14ms
step:1733/2110 train_time:95587ms step_avg:55.16ms
step:1734/2110 train_time:95674ms step_avg:55.18ms
step:1735/2110 train_time:95763ms step_avg:55.19ms
step:1736/2110 train_time:95850ms step_avg:55.21ms
step:1737/2110 train_time:95939ms step_avg:55.23ms
step:1738/2110 train_time:96026ms step_avg:55.25ms
step:1739/2110 train_time:96116ms step_avg:55.27ms
step:1740/2110 train_time:96203ms step_avg:55.29ms
step:1741/2110 train_time:96293ms step_avg:55.31ms
step:1742/2110 train_time:96380ms step_avg:55.33ms
step:1743/2110 train_time:96470ms step_avg:55.35ms
step:1744/2110 train_time:96557ms step_avg:55.37ms
step:1745/2110 train_time:96646ms step_avg:55.38ms
step:1746/2110 train_time:96734ms step_avg:55.40ms
step:1747/2110 train_time:96822ms step_avg:55.42ms
step:1748/2110 train_time:96909ms step_avg:55.44ms
step:1749/2110 train_time:96998ms step_avg:55.46ms
step:1750/2110 train_time:97085ms step_avg:55.48ms
step:1750/2110 val_loss:3.3775 train_time:97176ms step_avg:55.53ms
step:1751/2110 train_time:97196ms step_avg:55.51ms
step:1752/2110 train_time:97266ms step_avg:55.52ms
step:1753/2110 train_time:97361ms step_avg:55.54ms
step:1754/2110 train_time:97449ms step_avg:55.56ms
step:1755/2110 train_time:97537ms step_avg:55.58ms
step:1756/2110 train_time:97623ms step_avg:55.59ms
step:1757/2110 train_time:97711ms step_avg:55.61ms
step:1758/2110 train_time:97796ms step_avg:55.63ms
step:1759/2110 train_time:97884ms step_avg:55.65ms
step:1760/2110 train_time:97969ms step_avg:55.66ms
step:1761/2110 train_time:98056ms step_avg:55.68ms
step:1762/2110 train_time:98144ms step_avg:55.70ms
step:1763/2110 train_time:98235ms step_avg:55.72ms
step:1764/2110 train_time:98326ms step_avg:55.74ms
step:1765/2110 train_time:98416ms step_avg:55.76ms
step:1766/2110 train_time:98503ms step_avg:55.78ms
step:1767/2110 train_time:98592ms step_avg:55.80ms
step:1768/2110 train_time:98678ms step_avg:55.81ms
step:1769/2110 train_time:98766ms step_avg:55.83ms
step:1770/2110 train_time:98852ms step_avg:55.85ms
step:1771/2110 train_time:98940ms step_avg:55.87ms
step:1772/2110 train_time:99027ms step_avg:55.88ms
step:1773/2110 train_time:99116ms step_avg:55.90ms
step:1774/2110 train_time:99205ms step_avg:55.92ms
step:1775/2110 train_time:99296ms step_avg:55.94ms
step:1776/2110 train_time:99385ms step_avg:55.96ms
step:1777/2110 train_time:99474ms step_avg:55.98ms
step:1778/2110 train_time:99561ms step_avg:56.00ms
step:1779/2110 train_time:99650ms step_avg:56.01ms
step:1780/2110 train_time:99736ms step_avg:56.03ms
step:1781/2110 train_time:99825ms step_avg:56.05ms
step:1782/2110 train_time:99910ms step_avg:56.07ms
step:1783/2110 train_time:99998ms step_avg:56.08ms
step:1784/2110 train_time:100085ms step_avg:56.10ms
step:1785/2110 train_time:100174ms step_avg:56.12ms
step:1786/2110 train_time:100262ms step_avg:56.14ms
step:1787/2110 train_time:100353ms step_avg:56.16ms
step:1788/2110 train_time:100440ms step_avg:56.17ms
step:1789/2110 train_time:100529ms step_avg:56.19ms
step:1790/2110 train_time:100616ms step_avg:56.21ms
step:1791/2110 train_time:100705ms step_avg:56.23ms
step:1792/2110 train_time:100791ms step_avg:56.25ms
step:1793/2110 train_time:100880ms step_avg:56.26ms
step:1794/2110 train_time:100968ms step_avg:56.28ms
step:1795/2110 train_time:101056ms step_avg:56.30ms
step:1796/2110 train_time:101143ms step_avg:56.32ms
step:1797/2110 train_time:101233ms step_avg:56.33ms
step:1798/2110 train_time:101321ms step_avg:56.35ms
step:1799/2110 train_time:101410ms step_avg:56.37ms
step:1800/2110 train_time:101496ms step_avg:56.39ms
step:1801/2110 train_time:101586ms step_avg:56.41ms
step:1802/2110 train_time:101673ms step_avg:56.42ms
step:1803/2110 train_time:101761ms step_avg:56.44ms
step:1804/2110 train_time:101848ms step_avg:56.46ms
step:1805/2110 train_time:101936ms step_avg:56.47ms
step:1806/2110 train_time:102022ms step_avg:56.49ms
step:1807/2110 train_time:102111ms step_avg:56.51ms
step:1808/2110 train_time:102198ms step_avg:56.53ms
step:1809/2110 train_time:102288ms step_avg:56.54ms
step:1810/2110 train_time:102376ms step_avg:56.56ms
step:1811/2110 train_time:102466ms step_avg:56.58ms
step:1812/2110 train_time:102553ms step_avg:56.60ms
step:1813/2110 train_time:102642ms step_avg:56.61ms
step:1814/2110 train_time:102729ms step_avg:56.63ms
step:1815/2110 train_time:102817ms step_avg:56.65ms
step:1816/2110 train_time:102904ms step_avg:56.66ms
step:1817/2110 train_time:102992ms step_avg:56.68ms
step:1818/2110 train_time:103079ms step_avg:56.70ms
step:1819/2110 train_time:103168ms step_avg:56.72ms
step:1820/2110 train_time:103255ms step_avg:56.73ms
step:1821/2110 train_time:103345ms step_avg:56.75ms
step:1822/2110 train_time:103432ms step_avg:56.77ms
step:1823/2110 train_time:103520ms step_avg:56.79ms
step:1824/2110 train_time:103608ms step_avg:56.80ms
step:1825/2110 train_time:103696ms step_avg:56.82ms
step:1826/2110 train_time:103782ms step_avg:56.84ms
step:1827/2110 train_time:103872ms step_avg:56.85ms
step:1828/2110 train_time:103958ms step_avg:56.87ms
step:1829/2110 train_time:104047ms step_avg:56.89ms
step:1830/2110 train_time:104134ms step_avg:56.90ms
step:1831/2110 train_time:104223ms step_avg:56.92ms
step:1832/2110 train_time:104310ms step_avg:56.94ms
step:1833/2110 train_time:104399ms step_avg:56.96ms
step:1834/2110 train_time:104486ms step_avg:56.97ms
step:1835/2110 train_time:104575ms step_avg:56.99ms
step:1836/2110 train_time:104662ms step_avg:57.01ms
step:1837/2110 train_time:104751ms step_avg:57.02ms
step:1838/2110 train_time:104837ms step_avg:57.04ms
step:1839/2110 train_time:104926ms step_avg:57.06ms
step:1840/2110 train_time:105013ms step_avg:57.07ms
step:1841/2110 train_time:105101ms step_avg:57.09ms
step:1842/2110 train_time:105189ms step_avg:57.11ms
step:1843/2110 train_time:105278ms step_avg:57.12ms
step:1844/2110 train_time:105365ms step_avg:57.14ms
step:1845/2110 train_time:105455ms step_avg:57.16ms
step:1846/2110 train_time:105543ms step_avg:57.17ms
step:1847/2110 train_time:105632ms step_avg:57.19ms
step:1848/2110 train_time:105719ms step_avg:57.21ms
step:1849/2110 train_time:105807ms step_avg:57.22ms
step:1850/2110 train_time:105894ms step_avg:57.24ms
step:1851/2110 train_time:105983ms step_avg:57.26ms
step:1852/2110 train_time:106070ms step_avg:57.27ms
step:1853/2110 train_time:106159ms step_avg:57.29ms
step:1854/2110 train_time:106246ms step_avg:57.31ms
step:1855/2110 train_time:106336ms step_avg:57.32ms
step:1856/2110 train_time:106423ms step_avg:57.34ms
step:1857/2110 train_time:106513ms step_avg:57.36ms
step:1858/2110 train_time:106600ms step_avg:57.37ms
step:1859/2110 train_time:106689ms step_avg:57.39ms
step:1860/2110 train_time:106775ms step_avg:57.41ms
step:1861/2110 train_time:106864ms step_avg:57.42ms
step:1862/2110 train_time:106951ms step_avg:57.44ms
step:1863/2110 train_time:107039ms step_avg:57.46ms
step:1864/2110 train_time:107126ms step_avg:57.47ms
step:1865/2110 train_time:107215ms step_avg:57.49ms
step:1866/2110 train_time:107303ms step_avg:57.50ms
step:1867/2110 train_time:107392ms step_avg:57.52ms
step:1868/2110 train_time:107479ms step_avg:57.54ms
step:1869/2110 train_time:107567ms step_avg:57.55ms
step:1870/2110 train_time:107654ms step_avg:57.57ms
step:1871/2110 train_time:107743ms step_avg:57.59ms
step:1872/2110 train_time:107830ms step_avg:57.60ms
step:1873/2110 train_time:107919ms step_avg:57.62ms
step:1874/2110 train_time:108006ms step_avg:57.63ms
step:1875/2110 train_time:108095ms step_avg:57.65ms
step:1876/2110 train_time:108182ms step_avg:57.67ms
step:1877/2110 train_time:108272ms step_avg:57.68ms
step:1878/2110 train_time:108358ms step_avg:57.70ms
step:1879/2110 train_time:108447ms step_avg:57.72ms
step:1880/2110 train_time:108534ms step_avg:57.73ms
step:1881/2110 train_time:108623ms step_avg:57.75ms
step:1882/2110 train_time:108710ms step_avg:57.76ms
step:1883/2110 train_time:108798ms step_avg:57.78ms
step:1884/2110 train_time:108886ms step_avg:57.79ms
step:1885/2110 train_time:108974ms step_avg:57.81ms
step:1886/2110 train_time:109062ms step_avg:57.83ms
step:1887/2110 train_time:109151ms step_avg:57.84ms
step:1888/2110 train_time:109238ms step_avg:57.86ms
step:1889/2110 train_time:109326ms step_avg:57.88ms
step:1890/2110 train_time:109413ms step_avg:57.89ms
step:1891/2110 train_time:109501ms step_avg:57.91ms
step:1892/2110 train_time:109588ms step_avg:57.92ms
step:1893/2110 train_time:109676ms step_avg:57.94ms
step:1894/2110 train_time:109764ms step_avg:57.95ms
step:1895/2110 train_time:109853ms step_avg:57.97ms
step:1896/2110 train_time:109940ms step_avg:57.99ms
step:1897/2110 train_time:110029ms step_avg:58.00ms
step:1898/2110 train_time:110117ms step_avg:58.02ms
step:1899/2110 train_time:110206ms step_avg:58.03ms
step:1900/2110 train_time:110292ms step_avg:58.05ms
step:1901/2110 train_time:110381ms step_avg:58.06ms
step:1902/2110 train_time:110467ms step_avg:58.08ms
step:1903/2110 train_time:110556ms step_avg:58.10ms
step:1904/2110 train_time:110643ms step_avg:58.11ms
step:1905/2110 train_time:110732ms step_avg:58.13ms
step:1906/2110 train_time:110819ms step_avg:58.14ms
step:1907/2110 train_time:110908ms step_avg:58.16ms
step:1908/2110 train_time:110995ms step_avg:58.17ms
step:1909/2110 train_time:111084ms step_avg:58.19ms
step:1910/2110 train_time:111171ms step_avg:58.20ms
step:1911/2110 train_time:111259ms step_avg:58.22ms
step:1912/2110 train_time:111347ms step_avg:58.24ms
step:1913/2110 train_time:111435ms step_avg:58.25ms
step:1914/2110 train_time:111523ms step_avg:58.27ms
step:1915/2110 train_time:111612ms step_avg:58.28ms
step:1916/2110 train_time:111699ms step_avg:58.30ms
step:1917/2110 train_time:111787ms step_avg:58.31ms
step:1918/2110 train_time:111875ms step_avg:58.33ms
step:1919/2110 train_time:111963ms step_avg:58.34ms
step:1920/2110 train_time:112051ms step_avg:58.36ms
step:1921/2110 train_time:112140ms step_avg:58.38ms
step:1922/2110 train_time:112227ms step_avg:58.39ms
step:1923/2110 train_time:112316ms step_avg:58.41ms
step:1924/2110 train_time:112403ms step_avg:58.42ms
step:1925/2110 train_time:112492ms step_avg:58.44ms
step:1926/2110 train_time:112579ms step_avg:58.45ms
step:1927/2110 train_time:112667ms step_avg:58.47ms
step:1928/2110 train_time:112754ms step_avg:58.48ms
step:1929/2110 train_time:112843ms step_avg:58.50ms
step:1930/2110 train_time:112931ms step_avg:58.51ms
step:1931/2110 train_time:113019ms step_avg:58.53ms
step:1932/2110 train_time:113106ms step_avg:58.54ms
step:1933/2110 train_time:113195ms step_avg:58.56ms
step:1934/2110 train_time:113282ms step_avg:58.57ms
step:1935/2110 train_time:113371ms step_avg:58.59ms
step:1936/2110 train_time:113458ms step_avg:58.60ms
step:1937/2110 train_time:113547ms step_avg:58.62ms
step:1938/2110 train_time:113635ms step_avg:58.64ms
step:1939/2110 train_time:113723ms step_avg:58.65ms
step:1940/2110 train_time:113810ms step_avg:58.67ms
step:1941/2110 train_time:113898ms step_avg:58.68ms
step:1942/2110 train_time:113986ms step_avg:58.70ms
step:1943/2110 train_time:114076ms step_avg:58.71ms
step:1944/2110 train_time:114163ms step_avg:58.73ms
step:1945/2110 train_time:114252ms step_avg:58.74ms
step:1946/2110 train_time:114338ms step_avg:58.76ms
step:1947/2110 train_time:114427ms step_avg:58.77ms
step:1948/2110 train_time:114514ms step_avg:58.79ms
step:1949/2110 train_time:114602ms step_avg:58.80ms
step:1950/2110 train_time:114689ms step_avg:58.81ms
step:1951/2110 train_time:114777ms step_avg:58.83ms
step:1952/2110 train_time:114864ms step_avg:58.84ms
step:1953/2110 train_time:114953ms step_avg:58.86ms
step:1954/2110 train_time:115040ms step_avg:58.87ms
step:1955/2110 train_time:115130ms step_avg:58.89ms
step:1956/2110 train_time:115217ms step_avg:58.90ms
step:1957/2110 train_time:115307ms step_avg:58.92ms
step:1958/2110 train_time:115393ms step_avg:58.93ms
step:1959/2110 train_time:115482ms step_avg:58.95ms
step:1960/2110 train_time:115569ms step_avg:58.96ms
step:1961/2110 train_time:115657ms step_avg:58.98ms
step:1962/2110 train_time:115744ms step_avg:58.99ms
step:1963/2110 train_time:115834ms step_avg:59.01ms
step:1964/2110 train_time:115921ms step_avg:59.02ms
step:1965/2110 train_time:116010ms step_avg:59.04ms
step:1966/2110 train_time:116097ms step_avg:59.05ms
step:1967/2110 train_time:116188ms step_avg:59.07ms
step:1968/2110 train_time:116274ms step_avg:59.08ms
step:1969/2110 train_time:116363ms step_avg:59.10ms
step:1970/2110 train_time:116450ms step_avg:59.11ms
step:1971/2110 train_time:116539ms step_avg:59.13ms
step:1972/2110 train_time:116627ms step_avg:59.14ms
step:1973/2110 train_time:116716ms step_avg:59.16ms
step:1974/2110 train_time:116803ms step_avg:59.17ms
step:1975/2110 train_time:116892ms step_avg:59.19ms
step:1976/2110 train_time:116978ms step_avg:59.20ms
step:1977/2110 train_time:117067ms step_avg:59.21ms
step:1978/2110 train_time:117155ms step_avg:59.23ms
step:1979/2110 train_time:117245ms step_avg:59.24ms
step:1980/2110 train_time:117332ms step_avg:59.26ms
step:1981/2110 train_time:117421ms step_avg:59.27ms
step:1982/2110 train_time:117508ms step_avg:59.29ms
step:1983/2110 train_time:117597ms step_avg:59.30ms
step:1984/2110 train_time:117684ms step_avg:59.32ms
step:1985/2110 train_time:117773ms step_avg:59.33ms
step:1986/2110 train_time:117860ms step_avg:59.35ms
step:1987/2110 train_time:117949ms step_avg:59.36ms
step:1988/2110 train_time:118036ms step_avg:59.37ms
step:1989/2110 train_time:118126ms step_avg:59.39ms
step:1990/2110 train_time:118213ms step_avg:59.40ms
step:1991/2110 train_time:118301ms step_avg:59.42ms
step:1992/2110 train_time:118389ms step_avg:59.43ms
step:1993/2110 train_time:118479ms step_avg:59.45ms
step:1994/2110 train_time:118565ms step_avg:59.46ms
step:1995/2110 train_time:118654ms step_avg:59.48ms
step:1996/2110 train_time:118741ms step_avg:59.49ms
step:1997/2110 train_time:118830ms step_avg:59.50ms
step:1998/2110 train_time:118916ms step_avg:59.52ms
step:1999/2110 train_time:119005ms step_avg:59.53ms
step:2000/2110 train_time:119092ms step_avg:59.55ms
step:2000/2110 val_loss:3.3033 train_time:119182ms step_avg:59.59ms
step:2001/2110 train_time:119203ms step_avg:59.57ms
step:2002/2110 train_time:119274ms step_avg:59.58ms
step:2003/2110 train_time:119365ms step_avg:59.59ms
step:2004/2110 train_time:119452ms step_avg:59.61ms
step:2005/2110 train_time:119540ms step_avg:59.62ms
step:2006/2110 train_time:119626ms step_avg:59.63ms
step:2007/2110 train_time:119714ms step_avg:59.65ms
step:2008/2110 train_time:119801ms step_avg:59.66ms
step:2009/2110 train_time:119889ms step_avg:59.68ms
step:2010/2110 train_time:119976ms step_avg:59.69ms
step:2011/2110 train_time:120065ms step_avg:59.70ms
step:2012/2110 train_time:120153ms step_avg:59.72ms
step:2013/2110 train_time:120244ms step_avg:59.73ms
step:2014/2110 train_time:120332ms step_avg:59.75ms
step:2015/2110 train_time:120422ms step_avg:59.76ms
step:2016/2110 train_time:120509ms step_avg:59.78ms
step:2017/2110 train_time:120598ms step_avg:59.79ms
step:2018/2110 train_time:120684ms step_avg:59.80ms
step:2019/2110 train_time:120772ms step_avg:59.82ms
step:2020/2110 train_time:120858ms step_avg:59.83ms
step:2021/2110 train_time:120946ms step_avg:59.84ms
step:2022/2110 train_time:121033ms step_avg:59.86ms
step:2023/2110 train_time:121122ms step_avg:59.87ms
step:2024/2110 train_time:121211ms step_avg:59.89ms
step:2025/2110 train_time:121301ms step_avg:59.90ms
step:2026/2110 train_time:121390ms step_avg:59.92ms
step:2027/2110 train_time:121479ms step_avg:59.93ms
step:2028/2110 train_time:121566ms step_avg:59.94ms
step:2029/2110 train_time:121655ms step_avg:59.96ms
step:2030/2110 train_time:121741ms step_avg:59.97ms
step:2031/2110 train_time:121829ms step_avg:59.98ms
step:2032/2110 train_time:121915ms step_avg:60.00ms
step:2033/2110 train_time:122004ms step_avg:60.01ms
step:2034/2110 train_time:122091ms step_avg:60.02ms
step:2035/2110 train_time:122180ms step_avg:60.04ms
step:2036/2110 train_time:122268ms step_avg:60.05ms
step:2037/2110 train_time:122359ms step_avg:60.07ms
step:2038/2110 train_time:122446ms step_avg:60.08ms
step:2039/2110 train_time:122535ms step_avg:60.10ms
step:2040/2110 train_time:122622ms step_avg:60.11ms
step:2041/2110 train_time:122711ms step_avg:60.12ms
step:2042/2110 train_time:122798ms step_avg:60.14ms
step:2043/2110 train_time:122886ms step_avg:60.15ms
step:2044/2110 train_time:122973ms step_avg:60.16ms
step:2045/2110 train_time:123062ms step_avg:60.18ms
step:2046/2110 train_time:123150ms step_avg:60.19ms
step:2047/2110 train_time:123240ms step_avg:60.21ms
step:2048/2110 train_time:123328ms step_avg:60.22ms
step:2049/2110 train_time:123416ms step_avg:60.23ms
step:2050/2110 train_time:123504ms step_avg:60.25ms
step:2051/2110 train_time:123593ms step_avg:60.26ms
step:2052/2110 train_time:123680ms step_avg:60.27ms
step:2053/2110 train_time:123769ms step_avg:60.29ms
step:2054/2110 train_time:123856ms step_avg:60.30ms
step:2055/2110 train_time:123945ms step_avg:60.31ms
step:2056/2110 train_time:124032ms step_avg:60.33ms
step:2057/2110 train_time:124122ms step_avg:60.34ms
step:2058/2110 train_time:124209ms step_avg:60.35ms
step:2059/2110 train_time:124298ms step_avg:60.37ms
step:2060/2110 train_time:124386ms step_avg:60.38ms
step:2061/2110 train_time:124474ms step_avg:60.40ms
step:2062/2110 train_time:124562ms step_avg:60.41ms
step:2063/2110 train_time:124652ms step_avg:60.42ms
step:2064/2110 train_time:124739ms step_avg:60.44ms
step:2065/2110 train_time:124829ms step_avg:60.45ms
step:2066/2110 train_time:124915ms step_avg:60.46ms
step:2067/2110 train_time:125004ms step_avg:60.48ms
step:2068/2110 train_time:125092ms step_avg:60.49ms
step:2069/2110 train_time:125181ms step_avg:60.50ms
step:2070/2110 train_time:125269ms step_avg:60.52ms
step:2071/2110 train_time:125358ms step_avg:60.53ms
step:2072/2110 train_time:125445ms step_avg:60.54ms
step:2073/2110 train_time:125534ms step_avg:60.56ms
step:2074/2110 train_time:125623ms step_avg:60.57ms
step:2075/2110 train_time:125713ms step_avg:60.58ms
step:2076/2110 train_time:125799ms step_avg:60.60ms
step:2077/2110 train_time:125889ms step_avg:60.61ms
step:2078/2110 train_time:125976ms step_avg:60.62ms
step:2079/2110 train_time:126065ms step_avg:60.64ms
step:2080/2110 train_time:126152ms step_avg:60.65ms
step:2081/2110 train_time:126242ms step_avg:60.66ms
step:2082/2110 train_time:126329ms step_avg:60.68ms
step:2083/2110 train_time:126418ms step_avg:60.69ms
step:2084/2110 train_time:126505ms step_avg:60.70ms
step:2085/2110 train_time:126594ms step_avg:60.72ms
step:2086/2110 train_time:126682ms step_avg:60.73ms
step:2087/2110 train_time:126772ms step_avg:60.74ms
step:2088/2110 train_time:126859ms step_avg:60.76ms
step:2089/2110 train_time:126949ms step_avg:60.77ms
step:2090/2110 train_time:127036ms step_avg:60.78ms
step:2091/2110 train_time:127125ms step_avg:60.80ms
step:2092/2110 train_time:127212ms step_avg:60.81ms
step:2093/2110 train_time:127301ms step_avg:60.82ms
step:2094/2110 train_time:127388ms step_avg:60.83ms
step:2095/2110 train_time:127477ms step_avg:60.85ms
step:2096/2110 train_time:127566ms step_avg:60.86ms
step:2097/2110 train_time:127655ms step_avg:60.87ms
step:2098/2110 train_time:127743ms step_avg:60.89ms
step:2099/2110 train_time:127831ms step_avg:60.90ms
step:2100/2110 train_time:127918ms step_avg:60.91ms
step:2101/2110 train_time:128007ms step_avg:60.93ms
step:2102/2110 train_time:128094ms step_avg:60.94ms
step:2103/2110 train_time:128184ms step_avg:60.95ms
step:2104/2110 train_time:128271ms step_avg:60.97ms
step:2105/2110 train_time:128360ms step_avg:60.98ms
step:2106/2110 train_time:128447ms step_avg:60.99ms
step:2107/2110 train_time:128536ms step_avg:61.00ms
step:2108/2110 train_time:128625ms step_avg:61.02ms
step:2109/2110 train_time:128714ms step_avg:61.03ms
step:2110/2110 train_time:128802ms step_avg:61.04ms
step:2110/2110 val_loss:3.2788 train_time:128892ms step_avg:61.09ms
peak memory allocated: 29816 MiB reserved: 44756 MiB

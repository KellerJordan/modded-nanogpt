import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 14 20:08:42 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            129W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            126W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     40854      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40855      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40856      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40857      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40858      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40859      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40860      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     40861      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     40855      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     40856      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     40857      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     40858      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     40859      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     40860      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     40861      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:80ms step_avg:80.30ms
step:2/2110 train_time:103ms step_avg:51.54ms
step:3/2110 train_time:123ms step_avg:41.02ms
step:4/2110 train_time:154ms step_avg:38.62ms
step:5/2110 train_time:188ms step_avg:37.55ms
step:6/2110 train_time:405ms step_avg:67.42ms
step:7/2110 train_time:428ms step_avg:61.09ms
step:8/2110 train_time:460ms step_avg:57.49ms
step:9/2110 train_time:493ms step_avg:54.81ms
step:10/2110 train_time:526ms step_avg:52.56ms
step:11/2110 train_time:560ms step_avg:50.90ms
step:12/2110 train_time:592ms step_avg:49.37ms
step:13/2110 train_time:626ms step_avg:48.17ms
step:14/2110 train_time:659ms step_avg:47.06ms
step:15/2110 train_time:693ms step_avg:46.19ms
step:16/2110 train_time:725ms step_avg:45.32ms
step:17/2110 train_time:759ms step_avg:44.65ms
step:18/2110 train_time:792ms step_avg:44.00ms
step:19/2110 train_time:825ms step_avg:43.45ms
step:20/2110 train_time:858ms step_avg:42.91ms
step:21/2110 train_time:892ms step_avg:42.48ms
step:22/2110 train_time:925ms step_avg:42.04ms
step:23/2110 train_time:958ms step_avg:41.67ms
step:24/2110 train_time:991ms step_avg:41.29ms
step:25/2110 train_time:1025ms step_avg:41.01ms
step:26/2110 train_time:1058ms step_avg:40.69ms
step:27/2110 train_time:1092ms step_avg:40.43ms
step:28/2110 train_time:1124ms step_avg:40.16ms
step:29/2110 train_time:1158ms step_avg:39.93ms
step:30/2110 train_time:1191ms step_avg:39.69ms
step:31/2110 train_time:1224ms step_avg:39.49ms
step:32/2110 train_time:1257ms step_avg:39.27ms
step:33/2110 train_time:1291ms step_avg:39.12ms
step:34/2110 train_time:1325ms step_avg:38.96ms
step:35/2110 train_time:1360ms step_avg:38.85ms
step:36/2110 train_time:1393ms step_avg:38.70ms
step:37/2110 train_time:1428ms step_avg:38.60ms
step:38/2110 train_time:1461ms step_avg:38.45ms
step:39/2110 train_time:1495ms step_avg:38.33ms
step:40/2110 train_time:1528ms step_avg:38.20ms
step:41/2110 train_time:1562ms step_avg:38.09ms
step:42/2110 train_time:1595ms step_avg:37.97ms
step:43/2110 train_time:1628ms step_avg:37.87ms
step:44/2110 train_time:1661ms step_avg:37.75ms
step:45/2110 train_time:1695ms step_avg:37.66ms
step:46/2110 train_time:1728ms step_avg:37.56ms
step:47/2110 train_time:1761ms step_avg:37.47ms
step:48/2110 train_time:1794ms step_avg:37.37ms
step:49/2110 train_time:1828ms step_avg:37.30ms
step:50/2110 train_time:1860ms step_avg:37.21ms
step:51/2110 train_time:1894ms step_avg:37.14ms
step:52/2110 train_time:1927ms step_avg:37.05ms
step:53/2110 train_time:1960ms step_avg:36.99ms
step:54/2110 train_time:1993ms step_avg:36.90ms
step:55/2110 train_time:2027ms step_avg:36.85ms
step:56/2110 train_time:2059ms step_avg:36.77ms
step:57/2110 train_time:2093ms step_avg:36.72ms
step:58/2110 train_time:2126ms step_avg:36.65ms
step:59/2110 train_time:2159ms step_avg:36.60ms
step:60/2110 train_time:2192ms step_avg:36.53ms
step:61/2110 train_time:2226ms step_avg:36.48ms
step:62/2110 train_time:2258ms step_avg:36.42ms
step:63/2110 train_time:2292ms step_avg:36.39ms
step:64/2110 train_time:2325ms step_avg:36.33ms
step:65/2110 train_time:2359ms step_avg:36.30ms
step:66/2110 train_time:2392ms step_avg:36.25ms
step:67/2110 train_time:2427ms step_avg:36.22ms
step:68/2110 train_time:2459ms step_avg:36.17ms
step:69/2110 train_time:2493ms step_avg:36.13ms
step:70/2110 train_time:2526ms step_avg:36.09ms
step:71/2110 train_time:2560ms step_avg:36.06ms
step:72/2110 train_time:2593ms step_avg:36.01ms
step:73/2110 train_time:2627ms step_avg:35.98ms
step:74/2110 train_time:2659ms step_avg:35.94ms
step:75/2110 train_time:2694ms step_avg:35.92ms
step:76/2110 train_time:2726ms step_avg:35.87ms
step:77/2110 train_time:2760ms step_avg:35.84ms
step:78/2110 train_time:2793ms step_avg:35.81ms
step:79/2110 train_time:2827ms step_avg:35.78ms
step:80/2110 train_time:2859ms step_avg:35.74ms
step:81/2110 train_time:2893ms step_avg:35.71ms
step:82/2110 train_time:2926ms step_avg:35.68ms
step:83/2110 train_time:2959ms step_avg:35.65ms
step:84/2110 train_time:2992ms step_avg:35.61ms
step:85/2110 train_time:3025ms step_avg:35.59ms
step:86/2110 train_time:3057ms step_avg:35.55ms
step:87/2110 train_time:3092ms step_avg:35.54ms
step:88/2110 train_time:3124ms step_avg:35.50ms
step:89/2110 train_time:3158ms step_avg:35.48ms
step:90/2110 train_time:3191ms step_avg:35.45ms
step:91/2110 train_time:3224ms step_avg:35.43ms
step:92/2110 train_time:3256ms step_avg:35.40ms
step:93/2110 train_time:3291ms step_avg:35.39ms
step:94/2110 train_time:3324ms step_avg:35.36ms
step:95/2110 train_time:3357ms step_avg:35.34ms
step:96/2110 train_time:3390ms step_avg:35.31ms
step:97/2110 train_time:3424ms step_avg:35.30ms
step:98/2110 train_time:3457ms step_avg:35.27ms
step:99/2110 train_time:3491ms step_avg:35.26ms
step:100/2110 train_time:3523ms step_avg:35.23ms
step:101/2110 train_time:3557ms step_avg:35.22ms
step:102/2110 train_time:3590ms step_avg:35.19ms
step:103/2110 train_time:3624ms step_avg:35.18ms
step:104/2110 train_time:3656ms step_avg:35.15ms
step:105/2110 train_time:3690ms step_avg:35.15ms
step:106/2110 train_time:3723ms step_avg:35.12ms
step:107/2110 train_time:3757ms step_avg:35.11ms
step:108/2110 train_time:3790ms step_avg:35.09ms
step:109/2110 train_time:3823ms step_avg:35.08ms
step:110/2110 train_time:3856ms step_avg:35.06ms
step:111/2110 train_time:3890ms step_avg:35.04ms
step:112/2110 train_time:3922ms step_avg:35.02ms
step:113/2110 train_time:3956ms step_avg:35.01ms
step:114/2110 train_time:3989ms step_avg:34.99ms
step:115/2110 train_time:4022ms step_avg:34.97ms
step:116/2110 train_time:4055ms step_avg:34.95ms
step:117/2110 train_time:4089ms step_avg:34.95ms
step:118/2110 train_time:4121ms step_avg:34.92ms
step:119/2110 train_time:4155ms step_avg:34.92ms
step:120/2110 train_time:4188ms step_avg:34.90ms
step:121/2110 train_time:4222ms step_avg:34.89ms
step:122/2110 train_time:4254ms step_avg:34.87ms
step:123/2110 train_time:4289ms step_avg:34.87ms
step:124/2110 train_time:4321ms step_avg:34.85ms
step:125/2110 train_time:4355ms step_avg:34.84ms
step:126/2110 train_time:4388ms step_avg:34.82ms
step:127/2110 train_time:4422ms step_avg:34.82ms
step:128/2110 train_time:4455ms step_avg:34.80ms
step:129/2110 train_time:4489ms step_avg:34.79ms
step:130/2110 train_time:4521ms step_avg:34.78ms
step:131/2110 train_time:4555ms step_avg:34.77ms
step:132/2110 train_time:4587ms step_avg:34.75ms
step:133/2110 train_time:4621ms step_avg:34.75ms
step:134/2110 train_time:4654ms step_avg:34.73ms
step:135/2110 train_time:4687ms step_avg:34.72ms
step:136/2110 train_time:4720ms step_avg:34.71ms
step:137/2110 train_time:4754ms step_avg:34.70ms
step:138/2110 train_time:4787ms step_avg:34.69ms
step:139/2110 train_time:4820ms step_avg:34.68ms
step:140/2110 train_time:4853ms step_avg:34.66ms
step:141/2110 train_time:4887ms step_avg:34.66ms
step:142/2110 train_time:4919ms step_avg:34.64ms
step:143/2110 train_time:4953ms step_avg:34.64ms
step:144/2110 train_time:4986ms step_avg:34.62ms
step:145/2110 train_time:5019ms step_avg:34.62ms
step:146/2110 train_time:5052ms step_avg:34.60ms
step:147/2110 train_time:5085ms step_avg:34.59ms
step:148/2110 train_time:5118ms step_avg:34.58ms
step:149/2110 train_time:5152ms step_avg:34.57ms
step:150/2110 train_time:5184ms step_avg:34.56ms
step:151/2110 train_time:5218ms step_avg:34.55ms
step:152/2110 train_time:5250ms step_avg:34.54ms
step:153/2110 train_time:5284ms step_avg:34.54ms
step:154/2110 train_time:5317ms step_avg:34.52ms
step:155/2110 train_time:5351ms step_avg:34.52ms
step:156/2110 train_time:5383ms step_avg:34.51ms
step:157/2110 train_time:5417ms step_avg:34.50ms
step:158/2110 train_time:5450ms step_avg:34.49ms
step:159/2110 train_time:5484ms step_avg:34.49ms
step:160/2110 train_time:5516ms step_avg:34.48ms
step:161/2110 train_time:5550ms step_avg:34.47ms
step:162/2110 train_time:5582ms step_avg:34.46ms
step:163/2110 train_time:5616ms step_avg:34.46ms
step:164/2110 train_time:5649ms step_avg:34.44ms
step:165/2110 train_time:5683ms step_avg:34.44ms
step:166/2110 train_time:5715ms step_avg:34.43ms
step:167/2110 train_time:5749ms step_avg:34.43ms
step:168/2110 train_time:5782ms step_avg:34.41ms
step:169/2110 train_time:5815ms step_avg:34.41ms
step:170/2110 train_time:5848ms step_avg:34.40ms
step:171/2110 train_time:5882ms step_avg:34.40ms
step:172/2110 train_time:5914ms step_avg:34.38ms
step:173/2110 train_time:5948ms step_avg:34.38ms
step:174/2110 train_time:5980ms step_avg:34.37ms
step:175/2110 train_time:6014ms step_avg:34.37ms
step:176/2110 train_time:6047ms step_avg:34.36ms
step:177/2110 train_time:6080ms step_avg:34.35ms
step:178/2110 train_time:6113ms step_avg:34.34ms
step:179/2110 train_time:6146ms step_avg:34.34ms
step:180/2110 train_time:6179ms step_avg:34.33ms
step:181/2110 train_time:6212ms step_avg:34.32ms
step:182/2110 train_time:6245ms step_avg:34.31ms
step:183/2110 train_time:6278ms step_avg:34.31ms
step:184/2110 train_time:6311ms step_avg:34.30ms
step:185/2110 train_time:6344ms step_avg:34.29ms
step:186/2110 train_time:6377ms step_avg:34.28ms
step:187/2110 train_time:6410ms step_avg:34.28ms
step:188/2110 train_time:6443ms step_avg:34.27ms
step:189/2110 train_time:6476ms step_avg:34.27ms
step:190/2110 train_time:6509ms step_avg:34.26ms
step:191/2110 train_time:6543ms step_avg:34.25ms
step:192/2110 train_time:6575ms step_avg:34.24ms
step:193/2110 train_time:6609ms step_avg:34.24ms
step:194/2110 train_time:6642ms step_avg:34.24ms
step:195/2110 train_time:6675ms step_avg:34.23ms
step:196/2110 train_time:6708ms step_avg:34.22ms
step:197/2110 train_time:6742ms step_avg:34.22ms
step:198/2110 train_time:6774ms step_avg:34.21ms
step:199/2110 train_time:6808ms step_avg:34.21ms
step:200/2110 train_time:6840ms step_avg:34.20ms
step:201/2110 train_time:6874ms step_avg:34.20ms
step:202/2110 train_time:6907ms step_avg:34.19ms
step:203/2110 train_time:6941ms step_avg:34.19ms
step:204/2110 train_time:6973ms step_avg:34.18ms
step:205/2110 train_time:7007ms step_avg:34.18ms
step:206/2110 train_time:7039ms step_avg:34.17ms
step:207/2110 train_time:7074ms step_avg:34.17ms
step:208/2110 train_time:7106ms step_avg:34.16ms
step:209/2110 train_time:7140ms step_avg:34.16ms
step:210/2110 train_time:7173ms step_avg:34.16ms
step:211/2110 train_time:7206ms step_avg:34.15ms
step:212/2110 train_time:7239ms step_avg:34.15ms
step:213/2110 train_time:7273ms step_avg:34.14ms
step:214/2110 train_time:7305ms step_avg:34.14ms
step:215/2110 train_time:7339ms step_avg:34.13ms
step:216/2110 train_time:7371ms step_avg:34.13ms
step:217/2110 train_time:7405ms step_avg:34.12ms
step:218/2110 train_time:7437ms step_avg:34.12ms
step:219/2110 train_time:7471ms step_avg:34.11ms
step:220/2110 train_time:7504ms step_avg:34.11ms
step:221/2110 train_time:7537ms step_avg:34.11ms
step:222/2110 train_time:7570ms step_avg:34.10ms
step:223/2110 train_time:7603ms step_avg:34.10ms
step:224/2110 train_time:7636ms step_avg:34.09ms
step:225/2110 train_time:7670ms step_avg:34.09ms
step:226/2110 train_time:7702ms step_avg:34.08ms
step:227/2110 train_time:7736ms step_avg:34.08ms
step:228/2110 train_time:7768ms step_avg:34.07ms
step:229/2110 train_time:7802ms step_avg:34.07ms
step:230/2110 train_time:7834ms step_avg:34.06ms
step:231/2110 train_time:7868ms step_avg:34.06ms
step:232/2110 train_time:7900ms step_avg:34.05ms
step:233/2110 train_time:7934ms step_avg:34.05ms
step:234/2110 train_time:7966ms step_avg:34.04ms
step:235/2110 train_time:8000ms step_avg:34.04ms
step:236/2110 train_time:8033ms step_avg:34.04ms
step:237/2110 train_time:8066ms step_avg:34.03ms
step:238/2110 train_time:8099ms step_avg:34.03ms
step:239/2110 train_time:8132ms step_avg:34.03ms
step:240/2110 train_time:8165ms step_avg:34.02ms
step:241/2110 train_time:8199ms step_avg:34.02ms
step:242/2110 train_time:8231ms step_avg:34.01ms
step:243/2110 train_time:8265ms step_avg:34.01ms
step:244/2110 train_time:8297ms step_avg:34.00ms
step:245/2110 train_time:8331ms step_avg:34.00ms
step:246/2110 train_time:8363ms step_avg:34.00ms
step:247/2110 train_time:8397ms step_avg:34.00ms
step:248/2110 train_time:8429ms step_avg:33.99ms
step:249/2110 train_time:8463ms step_avg:33.99ms
step:250/2110 train_time:8496ms step_avg:33.98ms
step:250/2110 val_loss:4.2685 train_time:8532ms step_avg:34.13ms
step:251/2110 train_time:8552ms step_avg:34.07ms
step:252/2110 train_time:8572ms step_avg:34.01ms
step:253/2110 train_time:8601ms step_avg:34.00ms
step:254/2110 train_time:8634ms step_avg:33.99ms
step:255/2110 train_time:8670ms step_avg:34.00ms
step:256/2110 train_time:8703ms step_avg:34.00ms
step:257/2110 train_time:8738ms step_avg:34.00ms
step:258/2110 train_time:8770ms step_avg:33.99ms
step:259/2110 train_time:8804ms step_avg:33.99ms
step:260/2110 train_time:8837ms step_avg:33.99ms
step:261/2110 train_time:8871ms step_avg:33.99ms
step:262/2110 train_time:8903ms step_avg:33.98ms
step:263/2110 train_time:8936ms step_avg:33.98ms
step:264/2110 train_time:8969ms step_avg:33.97ms
step:265/2110 train_time:9002ms step_avg:33.97ms
step:266/2110 train_time:9034ms step_avg:33.96ms
step:267/2110 train_time:9068ms step_avg:33.96ms
step:268/2110 train_time:9100ms step_avg:33.96ms
step:269/2110 train_time:9134ms step_avg:33.96ms
step:270/2110 train_time:9167ms step_avg:33.95ms
step:271/2110 train_time:9200ms step_avg:33.95ms
step:272/2110 train_time:9232ms step_avg:33.94ms
step:273/2110 train_time:9266ms step_avg:33.94ms
step:274/2110 train_time:9298ms step_avg:33.94ms
step:275/2110 train_time:9332ms step_avg:33.93ms
step:276/2110 train_time:9364ms step_avg:33.93ms
step:277/2110 train_time:9398ms step_avg:33.93ms
step:278/2110 train_time:9430ms step_avg:33.92ms
step:279/2110 train_time:9464ms step_avg:33.92ms
step:280/2110 train_time:9496ms step_avg:33.92ms
step:281/2110 train_time:9530ms step_avg:33.92ms
step:282/2110 train_time:9563ms step_avg:33.91ms
step:283/2110 train_time:9597ms step_avg:33.91ms
step:284/2110 train_time:9630ms step_avg:33.91ms
step:285/2110 train_time:9664ms step_avg:33.91ms
step:286/2110 train_time:9696ms step_avg:33.90ms
step:287/2110 train_time:9730ms step_avg:33.90ms
step:288/2110 train_time:9763ms step_avg:33.90ms
step:289/2110 train_time:9797ms step_avg:33.90ms
step:290/2110 train_time:9829ms step_avg:33.89ms
step:291/2110 train_time:9863ms step_avg:33.89ms
step:292/2110 train_time:9896ms step_avg:33.89ms
step:293/2110 train_time:9929ms step_avg:33.89ms
step:294/2110 train_time:9962ms step_avg:33.88ms
step:295/2110 train_time:9995ms step_avg:33.88ms
step:296/2110 train_time:10028ms step_avg:33.88ms
step:297/2110 train_time:10061ms step_avg:33.88ms
step:298/2110 train_time:10094ms step_avg:33.87ms
step:299/2110 train_time:10127ms step_avg:33.87ms
step:300/2110 train_time:10159ms step_avg:33.86ms
step:301/2110 train_time:10193ms step_avg:33.86ms
step:302/2110 train_time:10225ms step_avg:33.86ms
step:303/2110 train_time:10259ms step_avg:33.86ms
step:304/2110 train_time:10291ms step_avg:33.85ms
step:305/2110 train_time:10324ms step_avg:33.85ms
step:306/2110 train_time:10357ms step_avg:33.85ms
step:307/2110 train_time:10391ms step_avg:33.85ms
step:308/2110 train_time:10423ms step_avg:33.84ms
step:309/2110 train_time:10456ms step_avg:33.84ms
step:310/2110 train_time:10489ms step_avg:33.83ms
step:311/2110 train_time:10522ms step_avg:33.83ms
step:312/2110 train_time:10555ms step_avg:33.83ms
step:313/2110 train_time:10589ms step_avg:33.83ms
step:314/2110 train_time:10621ms step_avg:33.83ms
step:315/2110 train_time:10655ms step_avg:33.83ms
step:316/2110 train_time:10687ms step_avg:33.82ms
step:317/2110 train_time:10721ms step_avg:33.82ms
step:318/2110 train_time:10754ms step_avg:33.82ms
step:319/2110 train_time:10788ms step_avg:33.82ms
step:320/2110 train_time:10820ms step_avg:33.81ms
step:321/2110 train_time:10855ms step_avg:33.82ms
step:322/2110 train_time:10887ms step_avg:33.81ms
step:323/2110 train_time:10921ms step_avg:33.81ms
step:324/2110 train_time:10954ms step_avg:33.81ms
step:325/2110 train_time:10987ms step_avg:33.81ms
step:326/2110 train_time:11020ms step_avg:33.80ms
step:327/2110 train_time:11053ms step_avg:33.80ms
step:328/2110 train_time:11086ms step_avg:33.80ms
step:329/2110 train_time:11119ms step_avg:33.80ms
step:330/2110 train_time:11152ms step_avg:33.79ms
step:331/2110 train_time:11186ms step_avg:33.79ms
step:332/2110 train_time:11218ms step_avg:33.79ms
step:333/2110 train_time:11252ms step_avg:33.79ms
step:334/2110 train_time:11284ms step_avg:33.79ms
step:335/2110 train_time:11318ms step_avg:33.78ms
step:336/2110 train_time:11350ms step_avg:33.78ms
step:337/2110 train_time:11384ms step_avg:33.78ms
step:338/2110 train_time:11416ms step_avg:33.78ms
step:339/2110 train_time:11450ms step_avg:33.78ms
step:340/2110 train_time:11483ms step_avg:33.77ms
step:341/2110 train_time:11516ms step_avg:33.77ms
step:342/2110 train_time:11549ms step_avg:33.77ms
step:343/2110 train_time:11582ms step_avg:33.77ms
step:344/2110 train_time:11615ms step_avg:33.76ms
step:345/2110 train_time:11649ms step_avg:33.76ms
step:346/2110 train_time:11681ms step_avg:33.76ms
step:347/2110 train_time:11715ms step_avg:33.76ms
step:348/2110 train_time:11748ms step_avg:33.76ms
step:349/2110 train_time:11781ms step_avg:33.76ms
step:350/2110 train_time:11814ms step_avg:33.75ms
step:351/2110 train_time:11848ms step_avg:33.75ms
step:352/2110 train_time:11880ms step_avg:33.75ms
step:353/2110 train_time:11914ms step_avg:33.75ms
step:354/2110 train_time:11947ms step_avg:33.75ms
step:355/2110 train_time:11980ms step_avg:33.75ms
step:356/2110 train_time:12013ms step_avg:33.74ms
step:357/2110 train_time:12046ms step_avg:33.74ms
step:358/2110 train_time:12079ms step_avg:33.74ms
step:359/2110 train_time:12113ms step_avg:33.74ms
step:360/2110 train_time:12146ms step_avg:33.74ms
step:361/2110 train_time:12179ms step_avg:33.74ms
step:362/2110 train_time:12212ms step_avg:33.73ms
step:363/2110 train_time:12245ms step_avg:33.73ms
step:364/2110 train_time:12278ms step_avg:33.73ms
step:365/2110 train_time:12311ms step_avg:33.73ms
step:366/2110 train_time:12344ms step_avg:33.73ms
step:367/2110 train_time:12377ms step_avg:33.73ms
step:368/2110 train_time:12410ms step_avg:33.72ms
step:369/2110 train_time:12444ms step_avg:33.72ms
step:370/2110 train_time:12476ms step_avg:33.72ms
step:371/2110 train_time:12510ms step_avg:33.72ms
step:372/2110 train_time:12542ms step_avg:33.72ms
step:373/2110 train_time:12576ms step_avg:33.72ms
step:374/2110 train_time:12609ms step_avg:33.71ms
step:375/2110 train_time:12642ms step_avg:33.71ms
step:376/2110 train_time:12675ms step_avg:33.71ms
step:377/2110 train_time:12708ms step_avg:33.71ms
step:378/2110 train_time:12741ms step_avg:33.71ms
step:379/2110 train_time:12775ms step_avg:33.71ms
step:380/2110 train_time:12807ms step_avg:33.70ms
step:381/2110 train_time:12841ms step_avg:33.70ms
step:382/2110 train_time:12873ms step_avg:33.70ms
step:383/2110 train_time:12907ms step_avg:33.70ms
step:384/2110 train_time:12940ms step_avg:33.70ms
step:385/2110 train_time:12973ms step_avg:33.70ms
step:386/2110 train_time:13006ms step_avg:33.69ms
step:387/2110 train_time:13039ms step_avg:33.69ms
step:388/2110 train_time:13072ms step_avg:33.69ms
step:389/2110 train_time:13106ms step_avg:33.69ms
step:390/2110 train_time:13138ms step_avg:33.69ms
step:391/2110 train_time:13172ms step_avg:33.69ms
step:392/2110 train_time:13204ms step_avg:33.68ms
step:393/2110 train_time:13237ms step_avg:33.68ms
step:394/2110 train_time:13270ms step_avg:33.68ms
step:395/2110 train_time:13304ms step_avg:33.68ms
step:396/2110 train_time:13336ms step_avg:33.68ms
step:397/2110 train_time:13370ms step_avg:33.68ms
step:398/2110 train_time:13402ms step_avg:33.67ms
step:399/2110 train_time:13436ms step_avg:33.67ms
step:400/2110 train_time:13468ms step_avg:33.67ms
step:401/2110 train_time:13502ms step_avg:33.67ms
step:402/2110 train_time:13534ms step_avg:33.67ms
step:403/2110 train_time:13568ms step_avg:33.67ms
step:404/2110 train_time:13601ms step_avg:33.66ms
step:405/2110 train_time:13634ms step_avg:33.66ms
step:406/2110 train_time:13667ms step_avg:33.66ms
step:407/2110 train_time:13700ms step_avg:33.66ms
step:408/2110 train_time:13733ms step_avg:33.66ms
step:409/2110 train_time:13767ms step_avg:33.66ms
step:410/2110 train_time:13799ms step_avg:33.66ms
step:411/2110 train_time:13833ms step_avg:33.66ms
step:412/2110 train_time:13866ms step_avg:33.65ms
step:413/2110 train_time:13899ms step_avg:33.65ms
step:414/2110 train_time:13932ms step_avg:33.65ms
step:415/2110 train_time:13966ms step_avg:33.65ms
step:416/2110 train_time:13998ms step_avg:33.65ms
step:417/2110 train_time:14032ms step_avg:33.65ms
step:418/2110 train_time:14064ms step_avg:33.65ms
step:419/2110 train_time:14098ms step_avg:33.65ms
step:420/2110 train_time:14130ms step_avg:33.64ms
step:421/2110 train_time:14164ms step_avg:33.64ms
step:422/2110 train_time:14196ms step_avg:33.64ms
step:423/2110 train_time:14230ms step_avg:33.64ms
step:424/2110 train_time:14262ms step_avg:33.64ms
step:425/2110 train_time:14295ms step_avg:33.64ms
step:426/2110 train_time:14328ms step_avg:33.63ms
step:427/2110 train_time:14361ms step_avg:33.63ms
step:428/2110 train_time:14394ms step_avg:33.63ms
step:429/2110 train_time:14428ms step_avg:33.63ms
step:430/2110 train_time:14460ms step_avg:33.63ms
step:431/2110 train_time:14494ms step_avg:33.63ms
step:432/2110 train_time:14527ms step_avg:33.63ms
step:433/2110 train_time:14560ms step_avg:33.63ms
step:434/2110 train_time:14593ms step_avg:33.62ms
step:435/2110 train_time:14626ms step_avg:33.62ms
step:436/2110 train_time:14659ms step_avg:33.62ms
step:437/2110 train_time:14692ms step_avg:33.62ms
step:438/2110 train_time:14725ms step_avg:33.62ms
step:439/2110 train_time:14758ms step_avg:33.62ms
step:440/2110 train_time:14791ms step_avg:33.62ms
step:441/2110 train_time:14824ms step_avg:33.62ms
step:442/2110 train_time:14857ms step_avg:33.61ms
step:443/2110 train_time:14891ms step_avg:33.61ms
step:444/2110 train_time:14924ms step_avg:33.61ms
step:445/2110 train_time:14957ms step_avg:33.61ms
step:446/2110 train_time:14989ms step_avg:33.61ms
step:447/2110 train_time:15023ms step_avg:33.61ms
step:448/2110 train_time:15055ms step_avg:33.61ms
step:449/2110 train_time:15089ms step_avg:33.61ms
step:450/2110 train_time:15122ms step_avg:33.60ms
step:451/2110 train_time:15155ms step_avg:33.60ms
step:452/2110 train_time:15189ms step_avg:33.60ms
step:453/2110 train_time:15222ms step_avg:33.60ms
step:454/2110 train_time:15255ms step_avg:33.60ms
step:455/2110 train_time:15288ms step_avg:33.60ms
step:456/2110 train_time:15321ms step_avg:33.60ms
step:457/2110 train_time:15355ms step_avg:33.60ms
step:458/2110 train_time:15387ms step_avg:33.60ms
step:459/2110 train_time:15421ms step_avg:33.60ms
step:460/2110 train_time:15453ms step_avg:33.59ms
step:461/2110 train_time:15487ms step_avg:33.59ms
step:462/2110 train_time:15519ms step_avg:33.59ms
step:463/2110 train_time:15553ms step_avg:33.59ms
step:464/2110 train_time:15585ms step_avg:33.59ms
step:465/2110 train_time:15619ms step_avg:33.59ms
step:466/2110 train_time:15651ms step_avg:33.59ms
step:467/2110 train_time:15685ms step_avg:33.59ms
step:468/2110 train_time:15717ms step_avg:33.58ms
step:469/2110 train_time:15751ms step_avg:33.58ms
step:470/2110 train_time:15783ms step_avg:33.58ms
step:471/2110 train_time:15817ms step_avg:33.58ms
step:472/2110 train_time:15850ms step_avg:33.58ms
step:473/2110 train_time:15883ms step_avg:33.58ms
step:474/2110 train_time:15916ms step_avg:33.58ms
step:475/2110 train_time:15950ms step_avg:33.58ms
step:476/2110 train_time:15983ms step_avg:33.58ms
step:477/2110 train_time:16016ms step_avg:33.58ms
step:478/2110 train_time:16048ms step_avg:33.57ms
step:479/2110 train_time:16082ms step_avg:33.57ms
step:480/2110 train_time:16115ms step_avg:33.57ms
step:481/2110 train_time:16148ms step_avg:33.57ms
step:482/2110 train_time:16181ms step_avg:33.57ms
step:483/2110 train_time:16215ms step_avg:33.57ms
step:484/2110 train_time:16247ms step_avg:33.57ms
step:485/2110 train_time:16281ms step_avg:33.57ms
step:486/2110 train_time:16313ms step_avg:33.57ms
step:487/2110 train_time:16347ms step_avg:33.57ms
step:488/2110 train_time:16380ms step_avg:33.56ms
step:489/2110 train_time:16413ms step_avg:33.56ms
step:490/2110 train_time:16446ms step_avg:33.56ms
step:491/2110 train_time:16479ms step_avg:33.56ms
step:492/2110 train_time:16511ms step_avg:33.56ms
step:493/2110 train_time:16545ms step_avg:33.56ms
step:494/2110 train_time:16577ms step_avg:33.56ms
step:495/2110 train_time:16611ms step_avg:33.56ms
step:496/2110 train_time:16643ms step_avg:33.56ms
step:497/2110 train_time:16677ms step_avg:33.55ms
step:498/2110 train_time:16709ms step_avg:33.55ms
step:499/2110 train_time:16743ms step_avg:33.55ms
step:500/2110 train_time:16776ms step_avg:33.55ms
step:500/2110 val_loss:4.0023 train_time:16812ms step_avg:33.62ms
step:501/2110 train_time:16831ms step_avg:33.59ms
step:502/2110 train_time:16850ms step_avg:33.57ms
step:503/2110 train_time:16880ms step_avg:33.56ms
step:504/2110 train_time:16913ms step_avg:33.56ms
step:505/2110 train_time:16948ms step_avg:33.56ms
step:506/2110 train_time:16981ms step_avg:33.56ms
step:507/2110 train_time:17016ms step_avg:33.56ms
step:508/2110 train_time:17048ms step_avg:33.56ms
step:509/2110 train_time:17082ms step_avg:33.56ms
step:510/2110 train_time:17115ms step_avg:33.56ms
step:511/2110 train_time:17148ms step_avg:33.56ms
step:512/2110 train_time:17181ms step_avg:33.56ms
step:513/2110 train_time:17214ms step_avg:33.56ms
step:514/2110 train_time:17246ms step_avg:33.55ms
step:515/2110 train_time:17280ms step_avg:33.55ms
step:516/2110 train_time:17312ms step_avg:33.55ms
step:517/2110 train_time:17346ms step_avg:33.55ms
step:518/2110 train_time:17378ms step_avg:33.55ms
step:519/2110 train_time:17411ms step_avg:33.55ms
step:520/2110 train_time:17444ms step_avg:33.55ms
step:521/2110 train_time:17477ms step_avg:33.55ms
step:522/2110 train_time:17509ms step_avg:33.54ms
step:523/2110 train_time:17543ms step_avg:33.54ms
step:524/2110 train_time:17575ms step_avg:33.54ms
step:525/2110 train_time:17609ms step_avg:33.54ms
step:526/2110 train_time:17641ms step_avg:33.54ms
step:527/2110 train_time:17674ms step_avg:33.54ms
step:528/2110 train_time:17707ms step_avg:33.54ms
step:529/2110 train_time:17741ms step_avg:33.54ms
step:530/2110 train_time:17773ms step_avg:33.53ms
step:531/2110 train_time:17807ms step_avg:33.53ms
step:532/2110 train_time:17841ms step_avg:33.53ms
step:533/2110 train_time:17875ms step_avg:33.54ms
step:534/2110 train_time:17908ms step_avg:33.53ms
step:535/2110 train_time:17942ms step_avg:33.54ms
step:536/2110 train_time:17975ms step_avg:33.54ms
step:537/2110 train_time:18009ms step_avg:33.54ms
step:538/2110 train_time:18042ms step_avg:33.53ms
step:539/2110 train_time:18076ms step_avg:33.54ms
step:540/2110 train_time:18108ms step_avg:33.53ms
step:541/2110 train_time:18142ms step_avg:33.53ms
step:542/2110 train_time:18175ms step_avg:33.53ms
step:543/2110 train_time:18208ms step_avg:33.53ms
step:544/2110 train_time:18241ms step_avg:33.53ms
step:545/2110 train_time:18274ms step_avg:33.53ms
step:546/2110 train_time:18307ms step_avg:33.53ms
step:547/2110 train_time:18340ms step_avg:33.53ms
step:548/2110 train_time:18373ms step_avg:33.53ms
step:549/2110 train_time:18406ms step_avg:33.53ms
step:550/2110 train_time:18439ms step_avg:33.52ms
step:551/2110 train_time:18472ms step_avg:33.52ms
step:552/2110 train_time:18504ms step_avg:33.52ms
step:553/2110 train_time:18538ms step_avg:33.52ms
step:554/2110 train_time:18571ms step_avg:33.52ms
step:555/2110 train_time:18604ms step_avg:33.52ms
step:556/2110 train_time:18637ms step_avg:33.52ms
step:557/2110 train_time:18670ms step_avg:33.52ms
step:558/2110 train_time:18703ms step_avg:33.52ms
step:559/2110 train_time:18736ms step_avg:33.52ms
step:560/2110 train_time:18769ms step_avg:33.52ms
step:561/2110 train_time:18803ms step_avg:33.52ms
step:562/2110 train_time:18836ms step_avg:33.52ms
step:563/2110 train_time:18869ms step_avg:33.52ms
step:564/2110 train_time:18902ms step_avg:33.51ms
step:565/2110 train_time:18936ms step_avg:33.51ms
step:566/2110 train_time:18968ms step_avg:33.51ms
step:567/2110 train_time:19002ms step_avg:33.51ms
step:568/2110 train_time:19035ms step_avg:33.51ms
step:569/2110 train_time:19069ms step_avg:33.51ms
step:570/2110 train_time:19102ms step_avg:33.51ms
step:571/2110 train_time:19135ms step_avg:33.51ms
step:572/2110 train_time:19168ms step_avg:33.51ms
step:573/2110 train_time:19201ms step_avg:33.51ms
step:574/2110 train_time:19234ms step_avg:33.51ms
step:575/2110 train_time:19268ms step_avg:33.51ms
step:576/2110 train_time:19300ms step_avg:33.51ms
step:577/2110 train_time:19334ms step_avg:33.51ms
step:578/2110 train_time:19366ms step_avg:33.51ms
step:579/2110 train_time:19400ms step_avg:33.51ms
step:580/2110 train_time:19433ms step_avg:33.50ms
step:581/2110 train_time:19466ms step_avg:33.50ms
step:582/2110 train_time:19498ms step_avg:33.50ms
step:583/2110 train_time:19532ms step_avg:33.50ms
step:584/2110 train_time:19564ms step_avg:33.50ms
step:585/2110 train_time:19598ms step_avg:33.50ms
step:586/2110 train_time:19631ms step_avg:33.50ms
step:587/2110 train_time:19665ms step_avg:33.50ms
step:588/2110 train_time:19697ms step_avg:33.50ms
step:589/2110 train_time:19731ms step_avg:33.50ms
step:590/2110 train_time:19763ms step_avg:33.50ms
step:591/2110 train_time:19797ms step_avg:33.50ms
step:592/2110 train_time:19829ms step_avg:33.50ms
step:593/2110 train_time:19863ms step_avg:33.50ms
step:594/2110 train_time:19896ms step_avg:33.49ms
step:595/2110 train_time:19929ms step_avg:33.49ms
step:596/2110 train_time:19962ms step_avg:33.49ms
step:597/2110 train_time:19996ms step_avg:33.49ms
step:598/2110 train_time:20028ms step_avg:33.49ms
step:599/2110 train_time:20062ms step_avg:33.49ms
step:600/2110 train_time:20095ms step_avg:33.49ms
step:601/2110 train_time:20128ms step_avg:33.49ms
step:602/2110 train_time:20161ms step_avg:33.49ms
step:603/2110 train_time:20194ms step_avg:33.49ms
step:604/2110 train_time:20227ms step_avg:33.49ms
step:605/2110 train_time:20261ms step_avg:33.49ms
step:606/2110 train_time:20293ms step_avg:33.49ms
step:607/2110 train_time:20327ms step_avg:33.49ms
step:608/2110 train_time:20360ms step_avg:33.49ms
step:609/2110 train_time:20393ms step_avg:33.49ms
step:610/2110 train_time:20426ms step_avg:33.48ms
step:611/2110 train_time:20459ms step_avg:33.48ms
step:612/2110 train_time:20491ms step_avg:33.48ms
step:613/2110 train_time:20525ms step_avg:33.48ms
step:614/2110 train_time:20558ms step_avg:33.48ms
step:615/2110 train_time:20592ms step_avg:33.48ms
step:616/2110 train_time:20624ms step_avg:33.48ms
step:617/2110 train_time:20658ms step_avg:33.48ms
step:618/2110 train_time:20690ms step_avg:33.48ms
step:619/2110 train_time:20724ms step_avg:33.48ms
step:620/2110 train_time:20757ms step_avg:33.48ms
step:621/2110 train_time:20790ms step_avg:33.48ms
step:622/2110 train_time:20823ms step_avg:33.48ms
step:623/2110 train_time:20857ms step_avg:33.48ms
step:624/2110 train_time:20889ms step_avg:33.48ms
step:625/2110 train_time:20923ms step_avg:33.48ms
step:626/2110 train_time:20956ms step_avg:33.48ms
step:627/2110 train_time:20989ms step_avg:33.48ms
step:628/2110 train_time:21022ms step_avg:33.47ms
step:629/2110 train_time:21055ms step_avg:33.47ms
step:630/2110 train_time:21088ms step_avg:33.47ms
step:631/2110 train_time:21122ms step_avg:33.47ms
step:632/2110 train_time:21154ms step_avg:33.47ms
step:633/2110 train_time:21188ms step_avg:33.47ms
step:634/2110 train_time:21220ms step_avg:33.47ms
step:635/2110 train_time:21254ms step_avg:33.47ms
step:636/2110 train_time:21286ms step_avg:33.47ms
step:637/2110 train_time:21320ms step_avg:33.47ms
step:638/2110 train_time:21352ms step_avg:33.47ms
step:639/2110 train_time:21385ms step_avg:33.47ms
step:640/2110 train_time:21418ms step_avg:33.47ms
step:641/2110 train_time:21451ms step_avg:33.47ms
step:642/2110 train_time:21484ms step_avg:33.46ms
step:643/2110 train_time:21517ms step_avg:33.46ms
step:644/2110 train_time:21550ms step_avg:33.46ms
step:645/2110 train_time:21583ms step_avg:33.46ms
step:646/2110 train_time:21616ms step_avg:33.46ms
step:647/2110 train_time:21649ms step_avg:33.46ms
step:648/2110 train_time:21682ms step_avg:33.46ms
step:649/2110 train_time:21715ms step_avg:33.46ms
step:650/2110 train_time:21748ms step_avg:33.46ms
step:651/2110 train_time:21782ms step_avg:33.46ms
step:652/2110 train_time:21814ms step_avg:33.46ms
step:653/2110 train_time:21848ms step_avg:33.46ms
step:654/2110 train_time:21881ms step_avg:33.46ms
step:655/2110 train_time:21915ms step_avg:33.46ms
step:656/2110 train_time:21947ms step_avg:33.46ms
step:657/2110 train_time:21981ms step_avg:33.46ms
step:658/2110 train_time:22014ms step_avg:33.46ms
step:659/2110 train_time:22048ms step_avg:33.46ms
step:660/2110 train_time:22080ms step_avg:33.45ms
step:661/2110 train_time:22114ms step_avg:33.46ms
step:662/2110 train_time:22146ms step_avg:33.45ms
step:663/2110 train_time:22180ms step_avg:33.45ms
step:664/2110 train_time:22213ms step_avg:33.45ms
step:665/2110 train_time:22246ms step_avg:33.45ms
step:666/2110 train_time:22279ms step_avg:33.45ms
step:667/2110 train_time:22312ms step_avg:33.45ms
step:668/2110 train_time:22345ms step_avg:33.45ms
step:669/2110 train_time:22378ms step_avg:33.45ms
step:670/2110 train_time:22411ms step_avg:33.45ms
step:671/2110 train_time:22444ms step_avg:33.45ms
step:672/2110 train_time:22477ms step_avg:33.45ms
step:673/2110 train_time:22511ms step_avg:33.45ms
step:674/2110 train_time:22544ms step_avg:33.45ms
step:675/2110 train_time:22577ms step_avg:33.45ms
step:676/2110 train_time:22609ms step_avg:33.45ms
step:677/2110 train_time:22644ms step_avg:33.45ms
step:678/2110 train_time:22676ms step_avg:33.45ms
step:679/2110 train_time:22710ms step_avg:33.45ms
step:680/2110 train_time:22743ms step_avg:33.45ms
step:681/2110 train_time:22776ms step_avg:33.44ms
step:682/2110 train_time:22808ms step_avg:33.44ms
step:683/2110 train_time:22842ms step_avg:33.44ms
step:684/2110 train_time:22875ms step_avg:33.44ms
step:685/2110 train_time:22908ms step_avg:33.44ms
step:686/2110 train_time:22941ms step_avg:33.44ms
step:687/2110 train_time:22974ms step_avg:33.44ms
step:688/2110 train_time:23007ms step_avg:33.44ms
step:689/2110 train_time:23041ms step_avg:33.44ms
step:690/2110 train_time:23073ms step_avg:33.44ms
step:691/2110 train_time:23108ms step_avg:33.44ms
step:692/2110 train_time:23166ms step_avg:33.48ms
step:693/2110 train_time:23227ms step_avg:33.52ms
step:694/2110 train_time:23287ms step_avg:33.55ms
step:695/2110 train_time:23348ms step_avg:33.59ms
step:696/2110 train_time:23408ms step_avg:33.63ms
step:697/2110 train_time:23470ms step_avg:33.67ms
step:698/2110 train_time:23529ms step_avg:33.71ms
step:699/2110 train_time:23590ms step_avg:33.75ms
step:700/2110 train_time:23650ms step_avg:33.79ms
step:701/2110 train_time:23712ms step_avg:33.83ms
step:702/2110 train_time:23772ms step_avg:33.86ms
step:703/2110 train_time:23833ms step_avg:33.90ms
step:704/2110 train_time:23892ms step_avg:33.94ms
step:705/2110 train_time:23954ms step_avg:33.98ms
step:706/2110 train_time:24014ms step_avg:34.01ms
step:707/2110 train_time:24075ms step_avg:34.05ms
step:708/2110 train_time:24134ms step_avg:34.09ms
step:709/2110 train_time:24195ms step_avg:34.13ms
step:710/2110 train_time:24254ms step_avg:34.16ms
step:711/2110 train_time:24315ms step_avg:34.20ms
step:712/2110 train_time:24374ms step_avg:34.23ms
step:713/2110 train_time:24435ms step_avg:34.27ms
step:714/2110 train_time:24494ms step_avg:34.31ms
step:715/2110 train_time:24556ms step_avg:34.34ms
step:716/2110 train_time:24615ms step_avg:34.38ms
step:717/2110 train_time:24676ms step_avg:34.42ms
step:718/2110 train_time:24735ms step_avg:34.45ms
step:719/2110 train_time:24797ms step_avg:34.49ms
step:720/2110 train_time:24855ms step_avg:34.52ms
step:721/2110 train_time:24917ms step_avg:34.56ms
step:722/2110 train_time:24975ms step_avg:34.59ms
step:723/2110 train_time:25036ms step_avg:34.63ms
step:724/2110 train_time:25095ms step_avg:34.66ms
step:725/2110 train_time:25155ms step_avg:34.70ms
step:726/2110 train_time:25214ms step_avg:34.73ms
step:727/2110 train_time:25275ms step_avg:34.77ms
step:728/2110 train_time:25334ms step_avg:34.80ms
step:729/2110 train_time:25395ms step_avg:34.84ms
step:730/2110 train_time:25455ms step_avg:34.87ms
step:731/2110 train_time:25516ms step_avg:34.90ms
step:732/2110 train_time:25574ms step_avg:34.94ms
step:733/2110 train_time:25635ms step_avg:34.97ms
step:734/2110 train_time:25694ms step_avg:35.01ms
step:735/2110 train_time:25756ms step_avg:35.04ms
step:736/2110 train_time:25815ms step_avg:35.07ms
step:737/2110 train_time:25876ms step_avg:35.11ms
step:738/2110 train_time:25935ms step_avg:35.14ms
step:739/2110 train_time:25996ms step_avg:35.18ms
step:740/2110 train_time:26054ms step_avg:35.21ms
step:741/2110 train_time:26116ms step_avg:35.24ms
step:742/2110 train_time:26175ms step_avg:35.28ms
step:743/2110 train_time:26236ms step_avg:35.31ms
step:744/2110 train_time:26294ms step_avg:35.34ms
step:745/2110 train_time:26355ms step_avg:35.38ms
step:746/2110 train_time:26414ms step_avg:35.41ms
step:747/2110 train_time:26475ms step_avg:35.44ms
step:748/2110 train_time:26535ms step_avg:35.47ms
step:749/2110 train_time:26596ms step_avg:35.51ms
step:750/2110 train_time:26655ms step_avg:35.54ms
step:750/2110 val_loss:3.8436 train_time:26718ms step_avg:35.62ms
step:751/2110 train_time:26738ms step_avg:35.60ms
step:752/2110 train_time:26781ms step_avg:35.61ms
step:753/2110 train_time:26843ms step_avg:35.65ms
step:754/2110 train_time:26905ms step_avg:35.68ms
step:755/2110 train_time:26966ms step_avg:35.72ms
step:756/2110 train_time:27025ms step_avg:35.75ms
step:757/2110 train_time:27085ms step_avg:35.78ms
step:758/2110 train_time:27144ms step_avg:35.81ms
step:759/2110 train_time:27204ms step_avg:35.84ms
step:760/2110 train_time:27262ms step_avg:35.87ms
step:761/2110 train_time:27322ms step_avg:35.90ms
step:762/2110 train_time:27381ms step_avg:35.93ms
step:763/2110 train_time:27441ms step_avg:35.96ms
step:764/2110 train_time:27499ms step_avg:35.99ms
step:765/2110 train_time:27558ms step_avg:36.02ms
step:766/2110 train_time:27617ms step_avg:36.05ms
step:767/2110 train_time:27680ms step_avg:36.09ms
step:768/2110 train_time:27741ms step_avg:36.12ms
step:769/2110 train_time:27803ms step_avg:36.16ms
step:770/2110 train_time:27864ms step_avg:36.19ms
step:771/2110 train_time:27925ms step_avg:36.22ms
step:772/2110 train_time:27985ms step_avg:36.25ms
step:773/2110 train_time:28046ms step_avg:36.28ms
step:774/2110 train_time:28105ms step_avg:36.31ms
step:775/2110 train_time:28165ms step_avg:36.34ms
step:776/2110 train_time:28224ms step_avg:36.37ms
step:777/2110 train_time:28284ms step_avg:36.40ms
step:778/2110 train_time:28343ms step_avg:36.43ms
step:779/2110 train_time:28403ms step_avg:36.46ms
step:780/2110 train_time:28462ms step_avg:36.49ms
step:781/2110 train_time:28522ms step_avg:36.52ms
step:782/2110 train_time:28582ms step_avg:36.55ms
step:783/2110 train_time:28643ms step_avg:36.58ms
step:784/2110 train_time:28702ms step_avg:36.61ms
step:785/2110 train_time:28764ms step_avg:36.64ms
step:786/2110 train_time:28824ms step_avg:36.67ms
step:787/2110 train_time:28886ms step_avg:36.70ms
step:788/2110 train_time:28946ms step_avg:36.73ms
step:789/2110 train_time:29006ms step_avg:36.76ms
step:790/2110 train_time:29065ms step_avg:36.79ms
step:791/2110 train_time:29125ms step_avg:36.82ms
step:792/2110 train_time:29184ms step_avg:36.85ms
step:793/2110 train_time:29244ms step_avg:36.88ms
step:794/2110 train_time:29303ms step_avg:36.91ms
step:795/2110 train_time:29363ms step_avg:36.93ms
step:796/2110 train_time:29422ms step_avg:36.96ms
step:797/2110 train_time:29482ms step_avg:36.99ms
step:798/2110 train_time:29541ms step_avg:37.02ms
step:799/2110 train_time:29602ms step_avg:37.05ms
step:800/2110 train_time:29661ms step_avg:37.08ms
step:801/2110 train_time:29723ms step_avg:37.11ms
step:802/2110 train_time:29782ms step_avg:37.14ms
step:803/2110 train_time:29845ms step_avg:37.17ms
step:804/2110 train_time:29904ms step_avg:37.19ms
step:805/2110 train_time:29966ms step_avg:37.22ms
step:806/2110 train_time:30025ms step_avg:37.25ms
step:807/2110 train_time:30086ms step_avg:37.28ms
step:808/2110 train_time:30145ms step_avg:37.31ms
step:809/2110 train_time:30206ms step_avg:37.34ms
step:810/2110 train_time:30265ms step_avg:37.36ms
step:811/2110 train_time:30324ms step_avg:37.39ms
step:812/2110 train_time:30383ms step_avg:37.42ms
step:813/2110 train_time:30443ms step_avg:37.45ms
step:814/2110 train_time:30502ms step_avg:37.47ms
step:815/2110 train_time:30563ms step_avg:37.50ms
step:816/2110 train_time:30623ms step_avg:37.53ms
step:817/2110 train_time:30684ms step_avg:37.56ms
step:818/2110 train_time:30743ms step_avg:37.58ms
step:819/2110 train_time:30804ms step_avg:37.61ms
step:820/2110 train_time:30865ms step_avg:37.64ms
step:821/2110 train_time:30926ms step_avg:37.67ms
step:822/2110 train_time:30985ms step_avg:37.69ms
step:823/2110 train_time:31046ms step_avg:37.72ms
step:824/2110 train_time:31104ms step_avg:37.75ms
step:825/2110 train_time:31165ms step_avg:37.78ms
step:826/2110 train_time:31224ms step_avg:37.80ms
step:827/2110 train_time:31284ms step_avg:37.83ms
step:828/2110 train_time:31343ms step_avg:37.85ms
step:829/2110 train_time:31403ms step_avg:37.88ms
step:830/2110 train_time:31462ms step_avg:37.91ms
step:831/2110 train_time:31523ms step_avg:37.93ms
step:832/2110 train_time:31582ms step_avg:37.96ms
step:833/2110 train_time:31643ms step_avg:37.99ms
step:834/2110 train_time:31702ms step_avg:38.01ms
step:835/2110 train_time:31763ms step_avg:38.04ms
step:836/2110 train_time:31822ms step_avg:38.06ms
step:837/2110 train_time:31883ms step_avg:38.09ms
step:838/2110 train_time:31942ms step_avg:38.12ms
step:839/2110 train_time:32003ms step_avg:38.14ms
step:840/2110 train_time:32063ms step_avg:38.17ms
step:841/2110 train_time:32124ms step_avg:38.20ms
step:842/2110 train_time:32183ms step_avg:38.22ms
step:843/2110 train_time:32244ms step_avg:38.25ms
step:844/2110 train_time:32302ms step_avg:38.27ms
step:845/2110 train_time:32362ms step_avg:38.30ms
step:846/2110 train_time:32421ms step_avg:38.32ms
step:847/2110 train_time:32482ms step_avg:38.35ms
step:848/2110 train_time:32541ms step_avg:38.37ms
step:849/2110 train_time:32601ms step_avg:38.40ms
step:850/2110 train_time:32660ms step_avg:38.42ms
step:851/2110 train_time:32721ms step_avg:38.45ms
step:852/2110 train_time:32780ms step_avg:38.47ms
step:853/2110 train_time:32841ms step_avg:38.50ms
step:854/2110 train_time:32900ms step_avg:38.53ms
step:855/2110 train_time:32962ms step_avg:38.55ms
step:856/2110 train_time:33021ms step_avg:38.58ms
step:857/2110 train_time:33082ms step_avg:38.60ms
step:858/2110 train_time:33141ms step_avg:38.63ms
step:859/2110 train_time:33202ms step_avg:38.65ms
step:860/2110 train_time:33261ms step_avg:38.68ms
step:861/2110 train_time:33322ms step_avg:38.70ms
step:862/2110 train_time:33381ms step_avg:38.72ms
step:863/2110 train_time:33442ms step_avg:38.75ms
step:864/2110 train_time:33501ms step_avg:38.77ms
step:865/2110 train_time:33561ms step_avg:38.80ms
step:866/2110 train_time:33620ms step_avg:38.82ms
step:867/2110 train_time:33681ms step_avg:38.85ms
step:868/2110 train_time:33740ms step_avg:38.87ms
step:869/2110 train_time:33802ms step_avg:38.90ms
step:870/2110 train_time:33860ms step_avg:38.92ms
step:871/2110 train_time:33921ms step_avg:38.94ms
step:872/2110 train_time:33980ms step_avg:38.97ms
step:873/2110 train_time:34042ms step_avg:38.99ms
step:874/2110 train_time:34100ms step_avg:39.02ms
step:875/2110 train_time:34161ms step_avg:39.04ms
step:876/2110 train_time:34220ms step_avg:39.06ms
step:877/2110 train_time:34281ms step_avg:39.09ms
step:878/2110 train_time:34340ms step_avg:39.11ms
step:879/2110 train_time:34401ms step_avg:39.14ms
step:880/2110 train_time:34459ms step_avg:39.16ms
step:881/2110 train_time:34520ms step_avg:39.18ms
step:882/2110 train_time:34578ms step_avg:39.20ms
step:883/2110 train_time:34639ms step_avg:39.23ms
step:884/2110 train_time:34698ms step_avg:39.25ms
step:885/2110 train_time:34759ms step_avg:39.28ms
step:886/2110 train_time:34817ms step_avg:39.30ms
step:887/2110 train_time:34878ms step_avg:39.32ms
step:888/2110 train_time:34936ms step_avg:39.34ms
step:889/2110 train_time:34997ms step_avg:39.37ms
step:890/2110 train_time:35056ms step_avg:39.39ms
step:891/2110 train_time:35117ms step_avg:39.41ms
step:892/2110 train_time:35176ms step_avg:39.43ms
step:893/2110 train_time:35237ms step_avg:39.46ms
step:894/2110 train_time:35296ms step_avg:39.48ms
step:895/2110 train_time:35357ms step_avg:39.50ms
step:896/2110 train_time:35416ms step_avg:39.53ms
step:897/2110 train_time:35477ms step_avg:39.55ms
step:898/2110 train_time:35535ms step_avg:39.57ms
step:899/2110 train_time:35596ms step_avg:39.60ms
step:900/2110 train_time:35655ms step_avg:39.62ms
step:901/2110 train_time:35717ms step_avg:39.64ms
step:902/2110 train_time:35775ms step_avg:39.66ms
step:903/2110 train_time:35836ms step_avg:39.69ms
step:904/2110 train_time:35895ms step_avg:39.71ms
step:905/2110 train_time:35956ms step_avg:39.73ms
step:906/2110 train_time:36015ms step_avg:39.75ms
step:907/2110 train_time:36076ms step_avg:39.78ms
step:908/2110 train_time:36135ms step_avg:39.80ms
step:909/2110 train_time:36197ms step_avg:39.82ms
step:910/2110 train_time:36255ms step_avg:39.84ms
step:911/2110 train_time:36316ms step_avg:39.86ms
step:912/2110 train_time:36375ms step_avg:39.88ms
step:913/2110 train_time:36435ms step_avg:39.91ms
step:914/2110 train_time:36495ms step_avg:39.93ms
step:915/2110 train_time:36556ms step_avg:39.95ms
step:916/2110 train_time:36615ms step_avg:39.97ms
step:917/2110 train_time:36676ms step_avg:40.00ms
step:918/2110 train_time:36735ms step_avg:40.02ms
step:919/2110 train_time:36796ms step_avg:40.04ms
step:920/2110 train_time:36855ms step_avg:40.06ms
step:921/2110 train_time:36916ms step_avg:40.08ms
step:922/2110 train_time:36975ms step_avg:40.10ms
step:923/2110 train_time:37036ms step_avg:40.13ms
step:924/2110 train_time:37095ms step_avg:40.15ms
step:925/2110 train_time:37157ms step_avg:40.17ms
step:926/2110 train_time:37216ms step_avg:40.19ms
step:927/2110 train_time:37277ms step_avg:40.21ms
step:928/2110 train_time:37336ms step_avg:40.23ms
step:929/2110 train_time:37397ms step_avg:40.26ms
step:930/2110 train_time:37456ms step_avg:40.27ms
step:931/2110 train_time:37517ms step_avg:40.30ms
step:932/2110 train_time:37576ms step_avg:40.32ms
step:933/2110 train_time:37636ms step_avg:40.34ms
step:934/2110 train_time:37695ms step_avg:40.36ms
step:935/2110 train_time:37756ms step_avg:40.38ms
step:936/2110 train_time:37814ms step_avg:40.40ms
step:937/2110 train_time:37876ms step_avg:40.42ms
step:938/2110 train_time:37935ms step_avg:40.44ms
step:939/2110 train_time:37996ms step_avg:40.46ms
step:940/2110 train_time:38055ms step_avg:40.48ms
step:941/2110 train_time:38116ms step_avg:40.51ms
step:942/2110 train_time:38175ms step_avg:40.53ms
step:943/2110 train_time:38236ms step_avg:40.55ms
step:944/2110 train_time:38295ms step_avg:40.57ms
step:945/2110 train_time:38356ms step_avg:40.59ms
step:946/2110 train_time:38416ms step_avg:40.61ms
step:947/2110 train_time:38477ms step_avg:40.63ms
step:948/2110 train_time:38536ms step_avg:40.65ms
step:949/2110 train_time:38597ms step_avg:40.67ms
step:950/2110 train_time:38656ms step_avg:40.69ms
step:951/2110 train_time:38717ms step_avg:40.71ms
step:952/2110 train_time:38776ms step_avg:40.73ms
step:953/2110 train_time:38836ms step_avg:40.75ms
step:954/2110 train_time:38895ms step_avg:40.77ms
step:955/2110 train_time:38956ms step_avg:40.79ms
step:956/2110 train_time:39015ms step_avg:40.81ms
step:957/2110 train_time:39076ms step_avg:40.83ms
step:958/2110 train_time:39135ms step_avg:40.85ms
step:959/2110 train_time:39197ms step_avg:40.87ms
step:960/2110 train_time:39256ms step_avg:40.89ms
step:961/2110 train_time:39317ms step_avg:40.91ms
step:962/2110 train_time:39376ms step_avg:40.93ms
step:963/2110 train_time:39436ms step_avg:40.95ms
step:964/2110 train_time:39496ms step_avg:40.97ms
step:965/2110 train_time:39556ms step_avg:40.99ms
step:966/2110 train_time:39615ms step_avg:41.01ms
step:967/2110 train_time:39677ms step_avg:41.03ms
step:968/2110 train_time:39736ms step_avg:41.05ms
step:969/2110 train_time:39797ms step_avg:41.07ms
step:970/2110 train_time:39855ms step_avg:41.09ms
step:971/2110 train_time:39916ms step_avg:41.11ms
step:972/2110 train_time:39975ms step_avg:41.13ms
step:973/2110 train_time:40036ms step_avg:41.15ms
step:974/2110 train_time:40095ms step_avg:41.17ms
step:975/2110 train_time:40156ms step_avg:41.19ms
step:976/2110 train_time:40215ms step_avg:41.20ms
step:977/2110 train_time:40276ms step_avg:41.22ms
step:978/2110 train_time:40335ms step_avg:41.24ms
step:979/2110 train_time:40397ms step_avg:41.26ms
step:980/2110 train_time:40456ms step_avg:41.28ms
step:981/2110 train_time:40517ms step_avg:41.30ms
step:982/2110 train_time:40575ms step_avg:41.32ms
step:983/2110 train_time:40636ms step_avg:41.34ms
step:984/2110 train_time:40695ms step_avg:41.36ms
step:985/2110 train_time:40756ms step_avg:41.38ms
step:986/2110 train_time:40815ms step_avg:41.39ms
step:987/2110 train_time:40876ms step_avg:41.41ms
step:988/2110 train_time:40935ms step_avg:41.43ms
step:989/2110 train_time:40996ms step_avg:41.45ms
step:990/2110 train_time:41054ms step_avg:41.47ms
step:991/2110 train_time:41115ms step_avg:41.49ms
step:992/2110 train_time:41174ms step_avg:41.51ms
step:993/2110 train_time:41236ms step_avg:41.53ms
step:994/2110 train_time:41295ms step_avg:41.54ms
step:995/2110 train_time:41356ms step_avg:41.56ms
step:996/2110 train_time:41415ms step_avg:41.58ms
step:997/2110 train_time:41476ms step_avg:41.60ms
step:998/2110 train_time:41535ms step_avg:41.62ms
step:999/2110 train_time:41596ms step_avg:41.64ms
step:1000/2110 train_time:41655ms step_avg:41.66ms
step:1000/2110 val_loss:3.6934 train_time:41719ms step_avg:41.72ms
step:1001/2110 train_time:41739ms step_avg:41.70ms
step:1002/2110 train_time:41778ms step_avg:41.69ms
step:1003/2110 train_time:41842ms step_avg:41.72ms
step:1004/2110 train_time:41902ms step_avg:41.74ms
step:1005/2110 train_time:41964ms step_avg:41.75ms
step:1006/2110 train_time:42024ms step_avg:41.77ms
step:1007/2110 train_time:42085ms step_avg:41.79ms
step:1008/2110 train_time:42143ms step_avg:41.81ms
step:1009/2110 train_time:42204ms step_avg:41.83ms
step:1010/2110 train_time:42262ms step_avg:41.84ms
step:1011/2110 train_time:42322ms step_avg:41.86ms
step:1012/2110 train_time:42380ms step_avg:41.88ms
step:1013/2110 train_time:42439ms step_avg:41.89ms
step:1014/2110 train_time:42499ms step_avg:41.91ms
step:1015/2110 train_time:42559ms step_avg:41.93ms
step:1016/2110 train_time:42617ms step_avg:41.95ms
step:1017/2110 train_time:42679ms step_avg:41.97ms
step:1018/2110 train_time:42739ms step_avg:41.98ms
step:1019/2110 train_time:42801ms step_avg:42.00ms
step:1020/2110 train_time:42861ms step_avg:42.02ms
step:1021/2110 train_time:42922ms step_avg:42.04ms
step:1022/2110 train_time:42982ms step_avg:42.06ms
step:1023/2110 train_time:43043ms step_avg:42.08ms
step:1024/2110 train_time:43102ms step_avg:42.09ms
step:1025/2110 train_time:43163ms step_avg:42.11ms
step:1026/2110 train_time:43221ms step_avg:42.13ms
step:1027/2110 train_time:43282ms step_avg:42.14ms
step:1028/2110 train_time:43340ms step_avg:42.16ms
step:1029/2110 train_time:43399ms step_avg:42.18ms
step:1030/2110 train_time:43457ms step_avg:42.19ms
step:1031/2110 train_time:43518ms step_avg:42.21ms
step:1032/2110 train_time:43576ms step_avg:42.22ms
step:1033/2110 train_time:43637ms step_avg:42.24ms
step:1034/2110 train_time:43696ms step_avg:42.26ms
step:1035/2110 train_time:43757ms step_avg:42.28ms
step:1036/2110 train_time:43818ms step_avg:42.29ms
step:1037/2110 train_time:43880ms step_avg:42.31ms
step:1038/2110 train_time:43939ms step_avg:42.33ms
step:1039/2110 train_time:44001ms step_avg:42.35ms
step:1040/2110 train_time:44059ms step_avg:42.36ms
step:1041/2110 train_time:44121ms step_avg:42.38ms
step:1042/2110 train_time:44179ms step_avg:42.40ms
step:1043/2110 train_time:44240ms step_avg:42.42ms
step:1044/2110 train_time:44299ms step_avg:42.43ms
step:1045/2110 train_time:44359ms step_avg:42.45ms
step:1046/2110 train_time:44417ms step_avg:42.46ms
step:1047/2110 train_time:44478ms step_avg:42.48ms
step:1048/2110 train_time:44537ms step_avg:42.50ms
step:1049/2110 train_time:44598ms step_avg:42.51ms
step:1050/2110 train_time:44657ms step_avg:42.53ms
step:1051/2110 train_time:44718ms step_avg:42.55ms
step:1052/2110 train_time:44777ms step_avg:42.56ms
step:1053/2110 train_time:44838ms step_avg:42.58ms
step:1054/2110 train_time:44898ms step_avg:42.60ms
step:1055/2110 train_time:44960ms step_avg:42.62ms
step:1056/2110 train_time:45019ms step_avg:42.63ms
step:1057/2110 train_time:45081ms step_avg:42.65ms
step:1058/2110 train_time:45140ms step_avg:42.67ms
step:1059/2110 train_time:45201ms step_avg:42.68ms
step:1060/2110 train_time:45259ms step_avg:42.70ms
step:1061/2110 train_time:45320ms step_avg:42.71ms
step:1062/2110 train_time:45378ms step_avg:42.73ms
step:1063/2110 train_time:45439ms step_avg:42.75ms
step:1064/2110 train_time:45497ms step_avg:42.76ms
step:1065/2110 train_time:45558ms step_avg:42.78ms
step:1066/2110 train_time:45617ms step_avg:42.79ms
step:1067/2110 train_time:45677ms step_avg:42.81ms
step:1068/2110 train_time:45737ms step_avg:42.82ms
step:1069/2110 train_time:45798ms step_avg:42.84ms
step:1070/2110 train_time:45856ms step_avg:42.86ms
step:1071/2110 train_time:45919ms step_avg:42.87ms
step:1072/2110 train_time:45978ms step_avg:42.89ms
step:1073/2110 train_time:46040ms step_avg:42.91ms
step:1074/2110 train_time:46099ms step_avg:42.92ms
step:1075/2110 train_time:46160ms step_avg:42.94ms
step:1076/2110 train_time:46219ms step_avg:42.95ms
step:1077/2110 train_time:46280ms step_avg:42.97ms
step:1078/2110 train_time:46338ms step_avg:42.99ms
step:1079/2110 train_time:46399ms step_avg:43.00ms
step:1080/2110 train_time:46457ms step_avg:43.02ms
step:1081/2110 train_time:46518ms step_avg:43.03ms
step:1082/2110 train_time:46576ms step_avg:43.05ms
step:1083/2110 train_time:46638ms step_avg:43.06ms
step:1084/2110 train_time:46698ms step_avg:43.08ms
step:1085/2110 train_time:46758ms step_avg:43.10ms
step:1086/2110 train_time:46817ms step_avg:43.11ms
step:1087/2110 train_time:46879ms step_avg:43.13ms
step:1088/2110 train_time:46938ms step_avg:43.14ms
step:1089/2110 train_time:47000ms step_avg:43.16ms
step:1090/2110 train_time:47058ms step_avg:43.17ms
step:1091/2110 train_time:47120ms step_avg:43.19ms
step:1092/2110 train_time:47178ms step_avg:43.20ms
step:1093/2110 train_time:47239ms step_avg:43.22ms
step:1094/2110 train_time:47298ms step_avg:43.23ms
step:1095/2110 train_time:47359ms step_avg:43.25ms
step:1096/2110 train_time:47418ms step_avg:43.26ms
step:1097/2110 train_time:47479ms step_avg:43.28ms
step:1098/2110 train_time:47537ms step_avg:43.29ms
step:1099/2110 train_time:47599ms step_avg:43.31ms
step:1100/2110 train_time:47657ms step_avg:43.32ms
step:1101/2110 train_time:47718ms step_avg:43.34ms
step:1102/2110 train_time:47777ms step_avg:43.36ms
step:1103/2110 train_time:47839ms step_avg:43.37ms
step:1104/2110 train_time:47898ms step_avg:43.39ms
step:1105/2110 train_time:47960ms step_avg:43.40ms
step:1106/2110 train_time:48019ms step_avg:43.42ms
step:1107/2110 train_time:48080ms step_avg:43.43ms
step:1108/2110 train_time:48139ms step_avg:43.45ms
step:1109/2110 train_time:48200ms step_avg:43.46ms
step:1110/2110 train_time:48258ms step_avg:43.48ms
step:1111/2110 train_time:48319ms step_avg:43.49ms
step:1112/2110 train_time:48378ms step_avg:43.51ms
step:1113/2110 train_time:48439ms step_avg:43.52ms
step:1114/2110 train_time:48497ms step_avg:43.53ms
step:1115/2110 train_time:48558ms step_avg:43.55ms
step:1116/2110 train_time:48617ms step_avg:43.56ms
step:1117/2110 train_time:48678ms step_avg:43.58ms
step:1118/2110 train_time:48736ms step_avg:43.59ms
step:1119/2110 train_time:48798ms step_avg:43.61ms
step:1120/2110 train_time:48857ms step_avg:43.62ms
step:1121/2110 train_time:48919ms step_avg:43.64ms
step:1122/2110 train_time:48977ms step_avg:43.65ms
step:1123/2110 train_time:49040ms step_avg:43.67ms
step:1124/2110 train_time:49098ms step_avg:43.68ms
step:1125/2110 train_time:49159ms step_avg:43.70ms
step:1126/2110 train_time:49219ms step_avg:43.71ms
step:1127/2110 train_time:49279ms step_avg:43.73ms
step:1128/2110 train_time:49337ms step_avg:43.74ms
step:1129/2110 train_time:49399ms step_avg:43.75ms
step:1130/2110 train_time:49457ms step_avg:43.77ms
step:1131/2110 train_time:49518ms step_avg:43.78ms
step:1132/2110 train_time:49577ms step_avg:43.80ms
step:1133/2110 train_time:49638ms step_avg:43.81ms
step:1134/2110 train_time:49697ms step_avg:43.82ms
step:1135/2110 train_time:49758ms step_avg:43.84ms
step:1136/2110 train_time:49818ms step_avg:43.85ms
step:1137/2110 train_time:49879ms step_avg:43.87ms
step:1138/2110 train_time:49938ms step_avg:43.88ms
step:1139/2110 train_time:49999ms step_avg:43.90ms
step:1140/2110 train_time:50058ms step_avg:43.91ms
step:1141/2110 train_time:50119ms step_avg:43.93ms
step:1142/2110 train_time:50178ms step_avg:43.94ms
step:1143/2110 train_time:50239ms step_avg:43.95ms
step:1144/2110 train_time:50297ms step_avg:43.97ms
step:1145/2110 train_time:50359ms step_avg:43.98ms
step:1146/2110 train_time:50418ms step_avg:44.00ms
step:1147/2110 train_time:50479ms step_avg:44.01ms
step:1148/2110 train_time:50538ms step_avg:44.02ms
step:1149/2110 train_time:50599ms step_avg:44.04ms
step:1150/2110 train_time:50658ms step_avg:44.05ms
step:1151/2110 train_time:50719ms step_avg:44.06ms
step:1152/2110 train_time:50777ms step_avg:44.08ms
step:1153/2110 train_time:50839ms step_avg:44.09ms
step:1154/2110 train_time:50898ms step_avg:44.11ms
step:1155/2110 train_time:50959ms step_avg:44.12ms
step:1156/2110 train_time:51018ms step_avg:44.13ms
step:1157/2110 train_time:51080ms step_avg:44.15ms
step:1158/2110 train_time:51138ms step_avg:44.16ms
step:1159/2110 train_time:51199ms step_avg:44.18ms
step:1160/2110 train_time:51258ms step_avg:44.19ms
step:1161/2110 train_time:51319ms step_avg:44.20ms
step:1162/2110 train_time:51378ms step_avg:44.21ms
step:1163/2110 train_time:51439ms step_avg:44.23ms
step:1164/2110 train_time:51497ms step_avg:44.24ms
step:1165/2110 train_time:51559ms step_avg:44.26ms
step:1166/2110 train_time:51618ms step_avg:44.27ms
step:1167/2110 train_time:51679ms step_avg:44.28ms
step:1168/2110 train_time:51738ms step_avg:44.30ms
step:1169/2110 train_time:51799ms step_avg:44.31ms
step:1170/2110 train_time:51858ms step_avg:44.32ms
step:1171/2110 train_time:51919ms step_avg:44.34ms
step:1172/2110 train_time:51978ms step_avg:44.35ms
step:1173/2110 train_time:52039ms step_avg:44.36ms
step:1174/2110 train_time:52097ms step_avg:44.38ms
step:1175/2110 train_time:52159ms step_avg:44.39ms
step:1176/2110 train_time:52218ms step_avg:44.40ms
step:1177/2110 train_time:52279ms step_avg:44.42ms
step:1178/2110 train_time:52338ms step_avg:44.43ms
step:1179/2110 train_time:52399ms step_avg:44.44ms
step:1180/2110 train_time:52458ms step_avg:44.46ms
step:1181/2110 train_time:52519ms step_avg:44.47ms
step:1182/2110 train_time:52577ms step_avg:44.48ms
step:1183/2110 train_time:52638ms step_avg:44.50ms
step:1184/2110 train_time:52697ms step_avg:44.51ms
step:1185/2110 train_time:52758ms step_avg:44.52ms
step:1186/2110 train_time:52817ms step_avg:44.53ms
step:1187/2110 train_time:52878ms step_avg:44.55ms
step:1188/2110 train_time:52937ms step_avg:44.56ms
step:1189/2110 train_time:52998ms step_avg:44.57ms
step:1190/2110 train_time:53057ms step_avg:44.59ms
step:1191/2110 train_time:53119ms step_avg:44.60ms
step:1192/2110 train_time:53178ms step_avg:44.61ms
step:1193/2110 train_time:53239ms step_avg:44.63ms
step:1194/2110 train_time:53298ms step_avg:44.64ms
step:1195/2110 train_time:53358ms step_avg:44.65ms
step:1196/2110 train_time:53418ms step_avg:44.66ms
step:1197/2110 train_time:53479ms step_avg:44.68ms
step:1198/2110 train_time:53537ms step_avg:44.69ms
step:1199/2110 train_time:53598ms step_avg:44.70ms
step:1200/2110 train_time:53658ms step_avg:44.71ms
step:1201/2110 train_time:53719ms step_avg:44.73ms
step:1202/2110 train_time:53778ms step_avg:44.74ms
step:1203/2110 train_time:53839ms step_avg:44.75ms
step:1204/2110 train_time:53897ms step_avg:44.76ms
step:1205/2110 train_time:53958ms step_avg:44.78ms
step:1206/2110 train_time:54017ms step_avg:44.79ms
step:1207/2110 train_time:54079ms step_avg:44.80ms
step:1208/2110 train_time:54138ms step_avg:44.82ms
step:1209/2110 train_time:54199ms step_avg:44.83ms
step:1210/2110 train_time:54258ms step_avg:44.84ms
step:1211/2110 train_time:54319ms step_avg:44.85ms
step:1212/2110 train_time:54378ms step_avg:44.87ms
step:1213/2110 train_time:54439ms step_avg:44.88ms
step:1214/2110 train_time:54498ms step_avg:44.89ms
step:1215/2110 train_time:54559ms step_avg:44.90ms
step:1216/2110 train_time:54618ms step_avg:44.92ms
step:1217/2110 train_time:54679ms step_avg:44.93ms
step:1218/2110 train_time:54738ms step_avg:44.94ms
step:1219/2110 train_time:54799ms step_avg:44.95ms
step:1220/2110 train_time:54858ms step_avg:44.97ms
step:1221/2110 train_time:54919ms step_avg:44.98ms
step:1222/2110 train_time:54978ms step_avg:44.99ms
step:1223/2110 train_time:55039ms step_avg:45.00ms
step:1224/2110 train_time:55098ms step_avg:45.01ms
step:1225/2110 train_time:55159ms step_avg:45.03ms
step:1226/2110 train_time:55218ms step_avg:45.04ms
step:1227/2110 train_time:55279ms step_avg:45.05ms
step:1228/2110 train_time:55338ms step_avg:45.06ms
step:1229/2110 train_time:55399ms step_avg:45.08ms
step:1230/2110 train_time:55458ms step_avg:45.09ms
step:1231/2110 train_time:55518ms step_avg:45.10ms
step:1232/2110 train_time:55577ms step_avg:45.11ms
step:1233/2110 train_time:55639ms step_avg:45.12ms
step:1234/2110 train_time:55698ms step_avg:45.14ms
step:1235/2110 train_time:55759ms step_avg:45.15ms
step:1236/2110 train_time:55818ms step_avg:45.16ms
step:1237/2110 train_time:55879ms step_avg:45.17ms
step:1238/2110 train_time:55937ms step_avg:45.18ms
step:1239/2110 train_time:55999ms step_avg:45.20ms
step:1240/2110 train_time:56058ms step_avg:45.21ms
step:1241/2110 train_time:56119ms step_avg:45.22ms
step:1242/2110 train_time:56177ms step_avg:45.23ms
step:1243/2110 train_time:56239ms step_avg:45.24ms
step:1244/2110 train_time:56298ms step_avg:45.26ms
step:1245/2110 train_time:56358ms step_avg:45.27ms
step:1246/2110 train_time:56417ms step_avg:45.28ms
step:1247/2110 train_time:56478ms step_avg:45.29ms
step:1248/2110 train_time:56538ms step_avg:45.30ms
step:1249/2110 train_time:56599ms step_avg:45.32ms
step:1250/2110 train_time:56658ms step_avg:45.33ms
step:1250/2110 val_loss:3.5787 train_time:56721ms step_avg:45.38ms
step:1251/2110 train_time:56741ms step_avg:45.36ms
step:1252/2110 train_time:56781ms step_avg:45.35ms
step:1253/2110 train_time:56845ms step_avg:45.37ms
step:1254/2110 train_time:56907ms step_avg:45.38ms
step:1255/2110 train_time:56969ms step_avg:45.39ms
step:1256/2110 train_time:57027ms step_avg:45.40ms
step:1257/2110 train_time:57087ms step_avg:45.42ms
step:1258/2110 train_time:57146ms step_avg:45.43ms
step:1259/2110 train_time:57206ms step_avg:45.44ms
step:1260/2110 train_time:57264ms step_avg:45.45ms
step:1261/2110 train_time:57324ms step_avg:45.46ms
step:1262/2110 train_time:57382ms step_avg:45.47ms
step:1263/2110 train_time:57442ms step_avg:45.48ms
step:1264/2110 train_time:57500ms step_avg:45.49ms
step:1265/2110 train_time:57560ms step_avg:45.50ms
step:1266/2110 train_time:57618ms step_avg:45.51ms
step:1267/2110 train_time:57681ms step_avg:45.53ms
step:1268/2110 train_time:57740ms step_avg:45.54ms
step:1269/2110 train_time:57803ms step_avg:45.55ms
step:1270/2110 train_time:57863ms step_avg:45.56ms
step:1271/2110 train_time:57925ms step_avg:45.57ms
step:1272/2110 train_time:57984ms step_avg:45.58ms
step:1273/2110 train_time:58044ms step_avg:45.60ms
step:1274/2110 train_time:58103ms step_avg:45.61ms
step:1275/2110 train_time:58164ms step_avg:45.62ms
step:1276/2110 train_time:58222ms step_avg:45.63ms
step:1277/2110 train_time:58282ms step_avg:45.64ms
step:1278/2110 train_time:58341ms step_avg:45.65ms
step:1279/2110 train_time:58401ms step_avg:45.66ms
step:1280/2110 train_time:58459ms step_avg:45.67ms
step:1281/2110 train_time:58519ms step_avg:45.68ms
step:1282/2110 train_time:58578ms step_avg:45.69ms
step:1283/2110 train_time:58640ms step_avg:45.71ms
step:1284/2110 train_time:58700ms step_avg:45.72ms
step:1285/2110 train_time:58763ms step_avg:45.73ms
step:1286/2110 train_time:58822ms step_avg:45.74ms
step:1287/2110 train_time:58884ms step_avg:45.75ms
step:1288/2110 train_time:58942ms step_avg:45.76ms
step:1289/2110 train_time:59003ms step_avg:45.77ms
step:1290/2110 train_time:59062ms step_avg:45.78ms
step:1291/2110 train_time:59123ms step_avg:45.80ms
step:1292/2110 train_time:59182ms step_avg:45.81ms
step:1293/2110 train_time:59243ms step_avg:45.82ms
step:1294/2110 train_time:59301ms step_avg:45.83ms
step:1295/2110 train_time:59361ms step_avg:45.84ms
step:1296/2110 train_time:59419ms step_avg:45.85ms
step:1297/2110 train_time:59480ms step_avg:45.86ms
step:1298/2110 train_time:59539ms step_avg:45.87ms
step:1299/2110 train_time:59599ms step_avg:45.88ms
step:1300/2110 train_time:59658ms step_avg:45.89ms
step:1301/2110 train_time:59720ms step_avg:45.90ms
step:1302/2110 train_time:59779ms step_avg:45.91ms
step:1303/2110 train_time:59841ms step_avg:45.93ms
step:1304/2110 train_time:59900ms step_avg:45.94ms
step:1305/2110 train_time:59962ms step_avg:45.95ms
step:1306/2110 train_time:60021ms step_avg:45.96ms
step:1307/2110 train_time:60083ms step_avg:45.97ms
step:1308/2110 train_time:60142ms step_avg:45.98ms
step:1309/2110 train_time:60202ms step_avg:45.99ms
step:1310/2110 train_time:60261ms step_avg:46.00ms
step:1311/2110 train_time:60322ms step_avg:46.01ms
step:1312/2110 train_time:60380ms step_avg:46.02ms
step:1313/2110 train_time:60440ms step_avg:46.03ms
step:1314/2110 train_time:60499ms step_avg:46.04ms
step:1315/2110 train_time:60560ms step_avg:46.05ms
step:1316/2110 train_time:60619ms step_avg:46.06ms
step:1317/2110 train_time:60681ms step_avg:46.08ms
step:1318/2110 train_time:60740ms step_avg:46.08ms
step:1319/2110 train_time:60801ms step_avg:46.10ms
step:1320/2110 train_time:60860ms step_avg:46.11ms
step:1321/2110 train_time:60922ms step_avg:46.12ms
step:1322/2110 train_time:60981ms step_avg:46.13ms
step:1323/2110 train_time:61042ms step_avg:46.14ms
step:1324/2110 train_time:61101ms step_avg:46.15ms
step:1325/2110 train_time:61162ms step_avg:46.16ms
step:1326/2110 train_time:61220ms step_avg:46.17ms
step:1327/2110 train_time:61282ms step_avg:46.18ms
step:1328/2110 train_time:61340ms step_avg:46.19ms
step:1329/2110 train_time:61401ms step_avg:46.20ms
step:1330/2110 train_time:61459ms step_avg:46.21ms
step:1331/2110 train_time:61520ms step_avg:46.22ms
step:1332/2110 train_time:61579ms step_avg:46.23ms
step:1333/2110 train_time:61639ms step_avg:46.24ms
step:1334/2110 train_time:61698ms step_avg:46.25ms
step:1335/2110 train_time:61760ms step_avg:46.26ms
step:1336/2110 train_time:61819ms step_avg:46.27ms
step:1337/2110 train_time:61881ms step_avg:46.28ms
step:1338/2110 train_time:61940ms step_avg:46.29ms
step:1339/2110 train_time:62001ms step_avg:46.30ms
step:1340/2110 train_time:62060ms step_avg:46.31ms
step:1341/2110 train_time:62121ms step_avg:46.32ms
step:1342/2110 train_time:62180ms step_avg:46.33ms
step:1343/2110 train_time:62241ms step_avg:46.34ms
step:1344/2110 train_time:62299ms step_avg:46.35ms
step:1345/2110 train_time:62360ms step_avg:46.36ms
step:1346/2110 train_time:62419ms step_avg:46.37ms
step:1347/2110 train_time:62479ms step_avg:46.38ms
step:1348/2110 train_time:62537ms step_avg:46.39ms
step:1349/2110 train_time:62598ms step_avg:46.40ms
step:1350/2110 train_time:62658ms step_avg:46.41ms
step:1351/2110 train_time:62719ms step_avg:46.42ms
step:1352/2110 train_time:62779ms step_avg:46.43ms
step:1353/2110 train_time:62840ms step_avg:46.45ms
step:1354/2110 train_time:62900ms step_avg:46.46ms
step:1355/2110 train_time:62961ms step_avg:46.47ms
step:1356/2110 train_time:63020ms step_avg:46.48ms
step:1357/2110 train_time:63082ms step_avg:46.49ms
step:1358/2110 train_time:63140ms step_avg:46.50ms
step:1359/2110 train_time:63202ms step_avg:46.51ms
step:1360/2110 train_time:63261ms step_avg:46.52ms
step:1361/2110 train_time:63322ms step_avg:46.53ms
step:1362/2110 train_time:63380ms step_avg:46.53ms
step:1363/2110 train_time:63441ms step_avg:46.54ms
step:1364/2110 train_time:63499ms step_avg:46.55ms
step:1365/2110 train_time:63561ms step_avg:46.56ms
step:1366/2110 train_time:63619ms step_avg:46.57ms
step:1367/2110 train_time:63680ms step_avg:46.58ms
step:1368/2110 train_time:63739ms step_avg:46.59ms
step:1369/2110 train_time:63800ms step_avg:46.60ms
step:1370/2110 train_time:63860ms step_avg:46.61ms
step:1371/2110 train_time:63921ms step_avg:46.62ms
step:1372/2110 train_time:63981ms step_avg:46.63ms
step:1373/2110 train_time:64042ms step_avg:46.64ms
step:1374/2110 train_time:64101ms step_avg:46.65ms
step:1375/2110 train_time:64163ms step_avg:46.66ms
step:1376/2110 train_time:64221ms step_avg:46.67ms
step:1377/2110 train_time:64283ms step_avg:46.68ms
step:1378/2110 train_time:64341ms step_avg:46.69ms
step:1379/2110 train_time:64401ms step_avg:46.70ms
step:1380/2110 train_time:64460ms step_avg:46.71ms
step:1381/2110 train_time:64521ms step_avg:46.72ms
step:1382/2110 train_time:64608ms step_avg:46.75ms
step:1383/2110 train_time:64697ms step_avg:46.78ms
step:1384/2110 train_time:64784ms step_avg:46.81ms
step:1385/2110 train_time:64873ms step_avg:46.84ms
step:1386/2110 train_time:64961ms step_avg:46.87ms
step:1387/2110 train_time:65051ms step_avg:46.90ms
step:1388/2110 train_time:65138ms step_avg:46.93ms
step:1389/2110 train_time:65227ms step_avg:46.96ms
step:1390/2110 train_time:65314ms step_avg:46.99ms
step:1391/2110 train_time:65402ms step_avg:47.02ms
step:1392/2110 train_time:65488ms step_avg:47.05ms
step:1393/2110 train_time:65576ms step_avg:47.08ms
step:1394/2110 train_time:65663ms step_avg:47.10ms
step:1395/2110 train_time:65753ms step_avg:47.13ms
step:1396/2110 train_time:65840ms step_avg:47.16ms
step:1397/2110 train_time:65929ms step_avg:47.19ms
step:1398/2110 train_time:66016ms step_avg:47.22ms
step:1399/2110 train_time:66105ms step_avg:47.25ms
step:1400/2110 train_time:66193ms step_avg:47.28ms
step:1401/2110 train_time:66281ms step_avg:47.31ms
step:1402/2110 train_time:66367ms step_avg:47.34ms
step:1403/2110 train_time:66455ms step_avg:47.37ms
step:1404/2110 train_time:66542ms step_avg:47.39ms
step:1405/2110 train_time:66631ms step_avg:47.42ms
step:1406/2110 train_time:66717ms step_avg:47.45ms
step:1407/2110 train_time:66807ms step_avg:47.48ms
step:1408/2110 train_time:66893ms step_avg:47.51ms
step:1409/2110 train_time:66982ms step_avg:47.54ms
step:1410/2110 train_time:67070ms step_avg:47.57ms
step:1411/2110 train_time:67158ms step_avg:47.60ms
step:1412/2110 train_time:67245ms step_avg:47.62ms
step:1413/2110 train_time:67334ms step_avg:47.65ms
step:1414/2110 train_time:67420ms step_avg:47.68ms
step:1415/2110 train_time:67509ms step_avg:47.71ms
step:1416/2110 train_time:67596ms step_avg:47.74ms
step:1417/2110 train_time:67684ms step_avg:47.77ms
step:1418/2110 train_time:67771ms step_avg:47.79ms
step:1419/2110 train_time:67860ms step_avg:47.82ms
step:1420/2110 train_time:67947ms step_avg:47.85ms
step:1421/2110 train_time:68037ms step_avg:47.88ms
step:1422/2110 train_time:68124ms step_avg:47.91ms
step:1423/2110 train_time:68214ms step_avg:47.94ms
step:1424/2110 train_time:68300ms step_avg:47.96ms
step:1425/2110 train_time:68388ms step_avg:47.99ms
step:1426/2110 train_time:68476ms step_avg:48.02ms
step:1427/2110 train_time:68564ms step_avg:48.05ms
step:1428/2110 train_time:68651ms step_avg:48.08ms
step:1429/2110 train_time:68740ms step_avg:48.10ms
step:1430/2110 train_time:68827ms step_avg:48.13ms
step:1431/2110 train_time:68916ms step_avg:48.16ms
step:1432/2110 train_time:69003ms step_avg:48.19ms
step:1433/2110 train_time:69093ms step_avg:48.22ms
step:1434/2110 train_time:69180ms step_avg:48.24ms
step:1435/2110 train_time:69269ms step_avg:48.27ms
step:1436/2110 train_time:69356ms step_avg:48.30ms
step:1437/2110 train_time:69444ms step_avg:48.33ms
step:1438/2110 train_time:69531ms step_avg:48.35ms
step:1439/2110 train_time:69619ms step_avg:48.38ms
step:1440/2110 train_time:69706ms step_avg:48.41ms
step:1441/2110 train_time:69796ms step_avg:48.44ms
step:1442/2110 train_time:69883ms step_avg:48.46ms
step:1443/2110 train_time:69973ms step_avg:48.49ms
step:1444/2110 train_time:70060ms step_avg:48.52ms
step:1445/2110 train_time:70149ms step_avg:48.55ms
step:1446/2110 train_time:70235ms step_avg:48.57ms
step:1447/2110 train_time:70324ms step_avg:48.60ms
step:1448/2110 train_time:70411ms step_avg:48.63ms
step:1449/2110 train_time:70499ms step_avg:48.65ms
step:1450/2110 train_time:70587ms step_avg:48.68ms
step:1451/2110 train_time:70674ms step_avg:48.71ms
step:1452/2110 train_time:70761ms step_avg:48.73ms
step:1453/2110 train_time:70850ms step_avg:48.76ms
step:1454/2110 train_time:70937ms step_avg:48.79ms
step:1455/2110 train_time:71026ms step_avg:48.82ms
step:1456/2110 train_time:71113ms step_avg:48.84ms
step:1457/2110 train_time:71202ms step_avg:48.87ms
step:1458/2110 train_time:71289ms step_avg:48.89ms
step:1459/2110 train_time:71377ms step_avg:48.92ms
step:1460/2110 train_time:71464ms step_avg:48.95ms
step:1461/2110 train_time:71553ms step_avg:48.98ms
step:1462/2110 train_time:71640ms step_avg:49.00ms
step:1463/2110 train_time:71730ms step_avg:49.03ms
step:1464/2110 train_time:71817ms step_avg:49.06ms
step:1465/2110 train_time:71906ms step_avg:49.08ms
step:1466/2110 train_time:71993ms step_avg:49.11ms
step:1467/2110 train_time:72081ms step_avg:49.14ms
step:1468/2110 train_time:72169ms step_avg:49.16ms
step:1469/2110 train_time:72258ms step_avg:49.19ms
step:1470/2110 train_time:72345ms step_avg:49.21ms
step:1471/2110 train_time:72433ms step_avg:49.24ms
step:1472/2110 train_time:72520ms step_avg:49.27ms
step:1473/2110 train_time:72608ms step_avg:49.29ms
step:1474/2110 train_time:72696ms step_avg:49.32ms
step:1475/2110 train_time:72784ms step_avg:49.35ms
step:1476/2110 train_time:72871ms step_avg:49.37ms
step:1477/2110 train_time:72960ms step_avg:49.40ms
step:1478/2110 train_time:73047ms step_avg:49.42ms
step:1479/2110 train_time:73136ms step_avg:49.45ms
step:1480/2110 train_time:73223ms step_avg:49.47ms
step:1481/2110 train_time:73313ms step_avg:49.50ms
step:1482/2110 train_time:73400ms step_avg:49.53ms
step:1483/2110 train_time:73489ms step_avg:49.55ms
step:1484/2110 train_time:73575ms step_avg:49.58ms
step:1485/2110 train_time:73664ms step_avg:49.61ms
step:1486/2110 train_time:73752ms step_avg:49.63ms
step:1487/2110 train_time:73840ms step_avg:49.66ms
step:1488/2110 train_time:73928ms step_avg:49.68ms
step:1489/2110 train_time:74018ms step_avg:49.71ms
step:1490/2110 train_time:74105ms step_avg:49.73ms
step:1491/2110 train_time:74194ms step_avg:49.76ms
step:1492/2110 train_time:74281ms step_avg:49.79ms
step:1493/2110 train_time:74370ms step_avg:49.81ms
step:1494/2110 train_time:74457ms step_avg:49.84ms
step:1495/2110 train_time:74547ms step_avg:49.86ms
step:1496/2110 train_time:74634ms step_avg:49.89ms
step:1497/2110 train_time:74723ms step_avg:49.92ms
step:1498/2110 train_time:74810ms step_avg:49.94ms
step:1499/2110 train_time:74899ms step_avg:49.97ms
step:1500/2110 train_time:74986ms step_avg:49.99ms
step:1500/2110 val_loss:3.4708 train_time:75076ms step_avg:50.05ms
step:1501/2110 train_time:75096ms step_avg:50.03ms
step:1502/2110 train_time:75167ms step_avg:50.04ms
step:1503/2110 train_time:75261ms step_avg:50.07ms
step:1504/2110 train_time:75348ms step_avg:50.10ms
step:1505/2110 train_time:75436ms step_avg:50.12ms
step:1506/2110 train_time:75523ms step_avg:50.15ms
step:1507/2110 train_time:75610ms step_avg:50.17ms
step:1508/2110 train_time:75695ms step_avg:50.20ms
step:1509/2110 train_time:75784ms step_avg:50.22ms
step:1510/2110 train_time:75869ms step_avg:50.24ms
step:1511/2110 train_time:75957ms step_avg:50.27ms
step:1512/2110 train_time:76047ms step_avg:50.30ms
step:1513/2110 train_time:76139ms step_avg:50.32ms
step:1514/2110 train_time:76228ms step_avg:50.35ms
step:1515/2110 train_time:76320ms step_avg:50.38ms
step:1516/2110 train_time:76407ms step_avg:50.40ms
step:1517/2110 train_time:76495ms step_avg:50.43ms
step:1518/2110 train_time:76582ms step_avg:50.45ms
step:1519/2110 train_time:76670ms step_avg:50.47ms
step:1520/2110 train_time:76756ms step_avg:50.50ms
step:1521/2110 train_time:76845ms step_avg:50.52ms
step:1522/2110 train_time:76932ms step_avg:50.55ms
step:1523/2110 train_time:77022ms step_avg:50.57ms
step:1524/2110 train_time:77110ms step_avg:50.60ms
step:1525/2110 train_time:77199ms step_avg:50.62ms
step:1526/2110 train_time:77288ms step_avg:50.65ms
step:1527/2110 train_time:77377ms step_avg:50.67ms
step:1528/2110 train_time:77464ms step_avg:50.70ms
step:1529/2110 train_time:77552ms step_avg:50.72ms
step:1530/2110 train_time:77638ms step_avg:50.74ms
step:1531/2110 train_time:77727ms step_avg:50.77ms
step:1532/2110 train_time:77814ms step_avg:50.79ms
step:1533/2110 train_time:77902ms step_avg:50.82ms
step:1534/2110 train_time:77989ms step_avg:50.84ms
step:1535/2110 train_time:78078ms step_avg:50.87ms
step:1536/2110 train_time:78166ms step_avg:50.89ms
step:1537/2110 train_time:78257ms step_avg:50.92ms
step:1538/2110 train_time:78345ms step_avg:50.94ms
step:1539/2110 train_time:78435ms step_avg:50.97ms
step:1540/2110 train_time:78522ms step_avg:50.99ms
step:1541/2110 train_time:78610ms step_avg:51.01ms
step:1542/2110 train_time:78697ms step_avg:51.04ms
step:1543/2110 train_time:78785ms step_avg:51.06ms
step:1544/2110 train_time:78871ms step_avg:51.08ms
step:1545/2110 train_time:78960ms step_avg:51.11ms
step:1546/2110 train_time:79047ms step_avg:51.13ms
step:1547/2110 train_time:79137ms step_avg:51.15ms
step:1548/2110 train_time:79225ms step_avg:51.18ms
step:1549/2110 train_time:79315ms step_avg:51.20ms
step:1550/2110 train_time:79402ms step_avg:51.23ms
step:1551/2110 train_time:79490ms step_avg:51.25ms
step:1552/2110 train_time:79578ms step_avg:51.27ms
step:1553/2110 train_time:79667ms step_avg:51.30ms
step:1554/2110 train_time:79754ms step_avg:51.32ms
step:1555/2110 train_time:79843ms step_avg:51.35ms
step:1556/2110 train_time:79929ms step_avg:51.37ms
step:1557/2110 train_time:80018ms step_avg:51.39ms
step:1558/2110 train_time:80105ms step_avg:51.42ms
step:1559/2110 train_time:80195ms step_avg:51.44ms
step:1560/2110 train_time:80282ms step_avg:51.46ms
step:1561/2110 train_time:80371ms step_avg:51.49ms
step:1562/2110 train_time:80458ms step_avg:51.51ms
step:1563/2110 train_time:80548ms step_avg:51.53ms
step:1564/2110 train_time:80637ms step_avg:51.56ms
step:1565/2110 train_time:80726ms step_avg:51.58ms
step:1566/2110 train_time:80812ms step_avg:51.60ms
step:1567/2110 train_time:80901ms step_avg:51.63ms
step:1568/2110 train_time:80988ms step_avg:51.65ms
step:1569/2110 train_time:81076ms step_avg:51.67ms
step:1570/2110 train_time:81164ms step_avg:51.70ms
step:1571/2110 train_time:81254ms step_avg:51.72ms
step:1572/2110 train_time:81342ms step_avg:51.74ms
step:1573/2110 train_time:81430ms step_avg:51.77ms
step:1574/2110 train_time:81518ms step_avg:51.79ms
step:1575/2110 train_time:81607ms step_avg:51.81ms
step:1576/2110 train_time:81694ms step_avg:51.84ms
step:1577/2110 train_time:81784ms step_avg:51.86ms
step:1578/2110 train_time:81870ms step_avg:51.88ms
step:1579/2110 train_time:81960ms step_avg:51.91ms
step:1580/2110 train_time:82048ms step_avg:51.93ms
step:1581/2110 train_time:82136ms step_avg:51.95ms
step:1582/2110 train_time:82224ms step_avg:51.97ms
step:1583/2110 train_time:82313ms step_avg:52.00ms
step:1584/2110 train_time:82401ms step_avg:52.02ms
step:1585/2110 train_time:82489ms step_avg:52.04ms
step:1586/2110 train_time:82576ms step_avg:52.07ms
step:1587/2110 train_time:82666ms step_avg:52.09ms
step:1588/2110 train_time:82752ms step_avg:52.11ms
step:1589/2110 train_time:82841ms step_avg:52.13ms
step:1590/2110 train_time:82928ms step_avg:52.16ms
step:1591/2110 train_time:83017ms step_avg:52.18ms
step:1592/2110 train_time:83104ms step_avg:52.20ms
step:1593/2110 train_time:83194ms step_avg:52.22ms
step:1594/2110 train_time:83281ms step_avg:52.25ms
step:1595/2110 train_time:83370ms step_avg:52.27ms
step:1596/2110 train_time:83457ms step_avg:52.29ms
step:1597/2110 train_time:83546ms step_avg:52.31ms
step:1598/2110 train_time:83632ms step_avg:52.34ms
step:1599/2110 train_time:83721ms step_avg:52.36ms
step:1600/2110 train_time:83808ms step_avg:52.38ms
step:1601/2110 train_time:83896ms step_avg:52.40ms
step:1602/2110 train_time:83983ms step_avg:52.42ms
step:1603/2110 train_time:84071ms step_avg:52.45ms
step:1604/2110 train_time:84159ms step_avg:52.47ms
step:1605/2110 train_time:84248ms step_avg:52.49ms
step:1606/2110 train_time:84335ms step_avg:52.51ms
step:1607/2110 train_time:84424ms step_avg:52.54ms
step:1608/2110 train_time:84512ms step_avg:52.56ms
step:1609/2110 train_time:84600ms step_avg:52.58ms
step:1610/2110 train_time:84687ms step_avg:52.60ms
step:1611/2110 train_time:84776ms step_avg:52.62ms
step:1612/2110 train_time:84864ms step_avg:52.65ms
step:1613/2110 train_time:84952ms step_avg:52.67ms
step:1614/2110 train_time:85039ms step_avg:52.69ms
step:1615/2110 train_time:85129ms step_avg:52.71ms
step:1616/2110 train_time:85217ms step_avg:52.73ms
step:1617/2110 train_time:85305ms step_avg:52.76ms
step:1618/2110 train_time:85392ms step_avg:52.78ms
step:1619/2110 train_time:85480ms step_avg:52.80ms
step:1620/2110 train_time:85567ms step_avg:52.82ms
step:1621/2110 train_time:85657ms step_avg:52.84ms
step:1622/2110 train_time:85744ms step_avg:52.86ms
step:1623/2110 train_time:85832ms step_avg:52.88ms
step:1624/2110 train_time:85919ms step_avg:52.91ms
step:1625/2110 train_time:86008ms step_avg:52.93ms
step:1626/2110 train_time:86095ms step_avg:52.95ms
step:1627/2110 train_time:86185ms step_avg:52.97ms
step:1628/2110 train_time:86272ms step_avg:52.99ms
step:1629/2110 train_time:86361ms step_avg:53.01ms
step:1630/2110 train_time:86449ms step_avg:53.04ms
step:1631/2110 train_time:86539ms step_avg:53.06ms
step:1632/2110 train_time:86626ms step_avg:53.08ms
step:1633/2110 train_time:86716ms step_avg:53.10ms
step:1634/2110 train_time:86802ms step_avg:53.12ms
step:1635/2110 train_time:86891ms step_avg:53.14ms
step:1636/2110 train_time:86978ms step_avg:53.17ms
step:1637/2110 train_time:87067ms step_avg:53.19ms
step:1638/2110 train_time:87155ms step_avg:53.21ms
step:1639/2110 train_time:87244ms step_avg:53.23ms
step:1640/2110 train_time:87332ms step_avg:53.25ms
step:1641/2110 train_time:87421ms step_avg:53.27ms
step:1642/2110 train_time:87507ms step_avg:53.29ms
step:1643/2110 train_time:87596ms step_avg:53.31ms
step:1644/2110 train_time:87683ms step_avg:53.34ms
step:1645/2110 train_time:87772ms step_avg:53.36ms
step:1646/2110 train_time:87859ms step_avg:53.38ms
step:1647/2110 train_time:87948ms step_avg:53.40ms
step:1648/2110 train_time:88036ms step_avg:53.42ms
step:1649/2110 train_time:88125ms step_avg:53.44ms
step:1650/2110 train_time:88212ms step_avg:53.46ms
step:1651/2110 train_time:88301ms step_avg:53.48ms
step:1652/2110 train_time:88388ms step_avg:53.50ms
step:1653/2110 train_time:88476ms step_avg:53.52ms
step:1654/2110 train_time:88565ms step_avg:53.55ms
step:1655/2110 train_time:88654ms step_avg:53.57ms
step:1656/2110 train_time:88740ms step_avg:53.59ms
step:1657/2110 train_time:88829ms step_avg:53.61ms
step:1658/2110 train_time:88917ms step_avg:53.63ms
step:1659/2110 train_time:89007ms step_avg:53.65ms
step:1660/2110 train_time:89094ms step_avg:53.67ms
step:1661/2110 train_time:89183ms step_avg:53.69ms
step:1662/2110 train_time:89270ms step_avg:53.71ms
step:1663/2110 train_time:89359ms step_avg:53.73ms
step:1664/2110 train_time:89446ms step_avg:53.75ms
step:1665/2110 train_time:89535ms step_avg:53.77ms
step:1666/2110 train_time:89622ms step_avg:53.79ms
step:1667/2110 train_time:89711ms step_avg:53.82ms
step:1668/2110 train_time:89798ms step_avg:53.84ms
step:1669/2110 train_time:89887ms step_avg:53.86ms
step:1670/2110 train_time:89974ms step_avg:53.88ms
step:1671/2110 train_time:90063ms step_avg:53.90ms
step:1672/2110 train_time:90150ms step_avg:53.92ms
step:1673/2110 train_time:90239ms step_avg:53.94ms
step:1674/2110 train_time:90326ms step_avg:53.96ms
step:1675/2110 train_time:90415ms step_avg:53.98ms
step:1676/2110 train_time:90503ms step_avg:54.00ms
step:1677/2110 train_time:90591ms step_avg:54.02ms
step:1678/2110 train_time:90679ms step_avg:54.04ms
step:1679/2110 train_time:90768ms step_avg:54.06ms
step:1680/2110 train_time:90855ms step_avg:54.08ms
step:1681/2110 train_time:90944ms step_avg:54.10ms
step:1682/2110 train_time:91032ms step_avg:54.12ms
step:1683/2110 train_time:91121ms step_avg:54.14ms
step:1684/2110 train_time:91207ms step_avg:54.16ms
step:1685/2110 train_time:91296ms step_avg:54.18ms
step:1686/2110 train_time:91384ms step_avg:54.20ms
step:1687/2110 train_time:91472ms step_avg:54.22ms
step:1688/2110 train_time:91559ms step_avg:54.24ms
step:1689/2110 train_time:91650ms step_avg:54.26ms
step:1690/2110 train_time:91738ms step_avg:54.28ms
step:1691/2110 train_time:91828ms step_avg:54.30ms
step:1692/2110 train_time:91915ms step_avg:54.32ms
step:1693/2110 train_time:92004ms step_avg:54.34ms
step:1694/2110 train_time:92091ms step_avg:54.36ms
step:1695/2110 train_time:92180ms step_avg:54.38ms
step:1696/2110 train_time:92266ms step_avg:54.40ms
step:1697/2110 train_time:92353ms step_avg:54.42ms
step:1698/2110 train_time:92441ms step_avg:54.44ms
step:1699/2110 train_time:92529ms step_avg:54.46ms
step:1700/2110 train_time:92617ms step_avg:54.48ms
step:1701/2110 train_time:92706ms step_avg:54.50ms
step:1702/2110 train_time:92793ms step_avg:54.52ms
step:1703/2110 train_time:92883ms step_avg:54.54ms
step:1704/2110 train_time:92969ms step_avg:54.56ms
step:1705/2110 train_time:93059ms step_avg:54.58ms
step:1706/2110 train_time:93147ms step_avg:54.60ms
step:1707/2110 train_time:93235ms step_avg:54.62ms
step:1708/2110 train_time:93323ms step_avg:54.64ms
step:1709/2110 train_time:93411ms step_avg:54.66ms
step:1710/2110 train_time:93499ms step_avg:54.68ms
step:1711/2110 train_time:93588ms step_avg:54.70ms
step:1712/2110 train_time:93675ms step_avg:54.72ms
step:1713/2110 train_time:93764ms step_avg:54.74ms
step:1714/2110 train_time:93851ms step_avg:54.76ms
step:1715/2110 train_time:93940ms step_avg:54.78ms
step:1716/2110 train_time:94028ms step_avg:54.79ms
step:1717/2110 train_time:94116ms step_avg:54.81ms
step:1718/2110 train_time:94204ms step_avg:54.83ms
step:1719/2110 train_time:94293ms step_avg:54.85ms
step:1720/2110 train_time:94380ms step_avg:54.87ms
step:1721/2110 train_time:94470ms step_avg:54.89ms
step:1722/2110 train_time:94558ms step_avg:54.91ms
step:1723/2110 train_time:94647ms step_avg:54.93ms
step:1724/2110 train_time:94735ms step_avg:54.95ms
step:1725/2110 train_time:94823ms step_avg:54.97ms
step:1726/2110 train_time:94910ms step_avg:54.99ms
step:1727/2110 train_time:95000ms step_avg:55.01ms
step:1728/2110 train_time:95087ms step_avg:55.03ms
step:1729/2110 train_time:95175ms step_avg:55.05ms
step:1730/2110 train_time:95263ms step_avg:55.07ms
step:1731/2110 train_time:95351ms step_avg:55.08ms
step:1732/2110 train_time:95439ms step_avg:55.10ms
step:1733/2110 train_time:95528ms step_avg:55.12ms
step:1734/2110 train_time:95615ms step_avg:55.14ms
step:1735/2110 train_time:95705ms step_avg:55.16ms
step:1736/2110 train_time:95791ms step_avg:55.18ms
step:1737/2110 train_time:95881ms step_avg:55.20ms
step:1738/2110 train_time:95968ms step_avg:55.22ms
step:1739/2110 train_time:96057ms step_avg:55.24ms
step:1740/2110 train_time:96145ms step_avg:55.26ms
step:1741/2110 train_time:96234ms step_avg:55.27ms
step:1742/2110 train_time:96321ms step_avg:55.29ms
step:1743/2110 train_time:96410ms step_avg:55.31ms
step:1744/2110 train_time:96497ms step_avg:55.33ms
step:1745/2110 train_time:96587ms step_avg:55.35ms
step:1746/2110 train_time:96674ms step_avg:55.37ms
step:1747/2110 train_time:96763ms step_avg:55.39ms
step:1748/2110 train_time:96850ms step_avg:55.41ms
step:1749/2110 train_time:96939ms step_avg:55.43ms
step:1750/2110 train_time:97027ms step_avg:55.44ms
step:1750/2110 val_loss:3.3755 train_time:97118ms step_avg:55.50ms
step:1751/2110 train_time:97138ms step_avg:55.48ms
step:1752/2110 train_time:97206ms step_avg:55.48ms
step:1753/2110 train_time:97301ms step_avg:55.51ms
step:1754/2110 train_time:97389ms step_avg:55.52ms
step:1755/2110 train_time:97477ms step_avg:55.54ms
step:1756/2110 train_time:97562ms step_avg:55.56ms
step:1757/2110 train_time:97650ms step_avg:55.58ms
step:1758/2110 train_time:97736ms step_avg:55.59ms
step:1759/2110 train_time:97823ms step_avg:55.61ms
step:1760/2110 train_time:97911ms step_avg:55.63ms
step:1761/2110 train_time:97999ms step_avg:55.65ms
step:1762/2110 train_time:98086ms step_avg:55.67ms
step:1763/2110 train_time:98176ms step_avg:55.69ms
step:1764/2110 train_time:98265ms step_avg:55.71ms
step:1765/2110 train_time:98356ms step_avg:55.73ms
step:1766/2110 train_time:98442ms step_avg:55.74ms
step:1767/2110 train_time:98530ms step_avg:55.76ms
step:1768/2110 train_time:98616ms step_avg:55.78ms
step:1769/2110 train_time:98704ms step_avg:55.80ms
step:1770/2110 train_time:98790ms step_avg:55.81ms
step:1771/2110 train_time:98878ms step_avg:55.83ms
step:1772/2110 train_time:98965ms step_avg:55.85ms
step:1773/2110 train_time:99055ms step_avg:55.87ms
step:1774/2110 train_time:99143ms step_avg:55.89ms
step:1775/2110 train_time:99233ms step_avg:55.91ms
step:1776/2110 train_time:99321ms step_avg:55.92ms
step:1777/2110 train_time:99410ms step_avg:55.94ms
step:1778/2110 train_time:99498ms step_avg:55.96ms
step:1779/2110 train_time:99586ms step_avg:55.98ms
step:1780/2110 train_time:99673ms step_avg:56.00ms
step:1781/2110 train_time:99762ms step_avg:56.01ms
step:1782/2110 train_time:99848ms step_avg:56.03ms
step:1783/2110 train_time:99937ms step_avg:56.05ms
step:1784/2110 train_time:100023ms step_avg:56.07ms
step:1785/2110 train_time:100112ms step_avg:56.08ms
step:1786/2110 train_time:100201ms step_avg:56.10ms
step:1787/2110 train_time:100290ms step_avg:56.12ms
step:1788/2110 train_time:100378ms step_avg:56.14ms
step:1789/2110 train_time:100466ms step_avg:56.16ms
step:1790/2110 train_time:100553ms step_avg:56.17ms
step:1791/2110 train_time:100642ms step_avg:56.19ms
step:1792/2110 train_time:100728ms step_avg:56.21ms
step:1793/2110 train_time:100816ms step_avg:56.23ms
step:1794/2110 train_time:100902ms step_avg:56.24ms
step:1795/2110 train_time:100991ms step_avg:56.26ms
step:1796/2110 train_time:101079ms step_avg:56.28ms
step:1797/2110 train_time:101168ms step_avg:56.30ms
step:1798/2110 train_time:101256ms step_avg:56.32ms
step:1799/2110 train_time:101345ms step_avg:56.33ms
step:1800/2110 train_time:101433ms step_avg:56.35ms
step:1801/2110 train_time:101522ms step_avg:56.37ms
step:1802/2110 train_time:101609ms step_avg:56.39ms
step:1803/2110 train_time:101699ms step_avg:56.41ms
step:1804/2110 train_time:101785ms step_avg:56.42ms
step:1805/2110 train_time:101873ms step_avg:56.44ms
step:1806/2110 train_time:101959ms step_avg:56.46ms
step:1807/2110 train_time:102048ms step_avg:56.47ms
step:1808/2110 train_time:102136ms step_avg:56.49ms
step:1809/2110 train_time:102225ms step_avg:56.51ms
step:1810/2110 train_time:102313ms step_avg:56.53ms
step:1811/2110 train_time:102402ms step_avg:56.54ms
step:1812/2110 train_time:102489ms step_avg:56.56ms
step:1813/2110 train_time:102579ms step_avg:56.58ms
step:1814/2110 train_time:102665ms step_avg:56.60ms
step:1815/2110 train_time:102755ms step_avg:56.61ms
step:1816/2110 train_time:102841ms step_avg:56.63ms
step:1817/2110 train_time:102929ms step_avg:56.65ms
step:1818/2110 train_time:103016ms step_avg:56.66ms
step:1819/2110 train_time:103105ms step_avg:56.68ms
step:1820/2110 train_time:103193ms step_avg:56.70ms
step:1821/2110 train_time:103282ms step_avg:56.72ms
step:1822/2110 train_time:103370ms step_avg:56.73ms
step:1823/2110 train_time:103458ms step_avg:56.75ms
step:1824/2110 train_time:103545ms step_avg:56.77ms
step:1825/2110 train_time:103633ms step_avg:56.79ms
step:1826/2110 train_time:103719ms step_avg:56.80ms
step:1827/2110 train_time:103808ms step_avg:56.82ms
step:1828/2110 train_time:103894ms step_avg:56.84ms
step:1829/2110 train_time:103984ms step_avg:56.85ms
step:1830/2110 train_time:104071ms step_avg:56.87ms
step:1831/2110 train_time:104160ms step_avg:56.89ms
step:1832/2110 train_time:104247ms step_avg:56.90ms
step:1833/2110 train_time:104336ms step_avg:56.92ms
step:1834/2110 train_time:104423ms step_avg:56.94ms
step:1835/2110 train_time:104511ms step_avg:56.95ms
step:1836/2110 train_time:104599ms step_avg:56.97ms
step:1837/2110 train_time:104688ms step_avg:56.99ms
step:1838/2110 train_time:104775ms step_avg:57.00ms
step:1839/2110 train_time:104864ms step_avg:57.02ms
step:1840/2110 train_time:104952ms step_avg:57.04ms
step:1841/2110 train_time:105042ms step_avg:57.06ms
step:1842/2110 train_time:105128ms step_avg:57.07ms
step:1843/2110 train_time:105216ms step_avg:57.09ms
step:1844/2110 train_time:105303ms step_avg:57.11ms
step:1845/2110 train_time:105392ms step_avg:57.12ms
step:1846/2110 train_time:105480ms step_avg:57.14ms
step:1847/2110 train_time:105569ms step_avg:57.16ms
step:1848/2110 train_time:105656ms step_avg:57.17ms
step:1849/2110 train_time:105744ms step_avg:57.19ms
step:1850/2110 train_time:105831ms step_avg:57.21ms
step:1851/2110 train_time:105920ms step_avg:57.22ms
step:1852/2110 train_time:106006ms step_avg:57.24ms
step:1853/2110 train_time:106095ms step_avg:57.26ms
step:1854/2110 train_time:106182ms step_avg:57.27ms
step:1855/2110 train_time:106271ms step_avg:57.29ms
step:1856/2110 train_time:106358ms step_avg:57.30ms
step:1857/2110 train_time:106446ms step_avg:57.32ms
step:1858/2110 train_time:106533ms step_avg:57.34ms
step:1859/2110 train_time:106623ms step_avg:57.35ms
step:1860/2110 train_time:106710ms step_avg:57.37ms
step:1861/2110 train_time:106799ms step_avg:57.39ms
step:1862/2110 train_time:106886ms step_avg:57.40ms
step:1863/2110 train_time:106974ms step_avg:57.42ms
step:1864/2110 train_time:107061ms step_avg:57.44ms
step:1865/2110 train_time:107150ms step_avg:57.45ms
step:1866/2110 train_time:107237ms step_avg:57.47ms
step:1867/2110 train_time:107326ms step_avg:57.49ms
step:1868/2110 train_time:107412ms step_avg:57.50ms
step:1869/2110 train_time:107502ms step_avg:57.52ms
step:1870/2110 train_time:107589ms step_avg:57.53ms
step:1871/2110 train_time:107678ms step_avg:57.55ms
step:1872/2110 train_time:107766ms step_avg:57.57ms
step:1873/2110 train_time:107854ms step_avg:57.58ms
step:1874/2110 train_time:107941ms step_avg:57.60ms
step:1875/2110 train_time:108030ms step_avg:57.62ms
step:1876/2110 train_time:108117ms step_avg:57.63ms
step:1877/2110 train_time:108205ms step_avg:57.65ms
step:1878/2110 train_time:108291ms step_avg:57.66ms
step:1879/2110 train_time:108380ms step_avg:57.68ms
step:1880/2110 train_time:108467ms step_avg:57.70ms
step:1881/2110 train_time:108556ms step_avg:57.71ms
step:1882/2110 train_time:108642ms step_avg:57.73ms
step:1883/2110 train_time:108731ms step_avg:57.74ms
step:1884/2110 train_time:108819ms step_avg:57.76ms
step:1885/2110 train_time:108909ms step_avg:57.78ms
step:1886/2110 train_time:108996ms step_avg:57.79ms
step:1887/2110 train_time:109084ms step_avg:57.81ms
step:1888/2110 train_time:109172ms step_avg:57.82ms
step:1889/2110 train_time:109262ms step_avg:57.84ms
step:1890/2110 train_time:109348ms step_avg:57.86ms
step:1891/2110 train_time:109437ms step_avg:57.87ms
step:1892/2110 train_time:109523ms step_avg:57.89ms
step:1893/2110 train_time:109612ms step_avg:57.90ms
step:1894/2110 train_time:109699ms step_avg:57.92ms
step:1895/2110 train_time:109788ms step_avg:57.94ms
step:1896/2110 train_time:109875ms step_avg:57.95ms
step:1897/2110 train_time:109963ms step_avg:57.97ms
step:1898/2110 train_time:110050ms step_avg:57.98ms
step:1899/2110 train_time:110138ms step_avg:58.00ms
step:1900/2110 train_time:110226ms step_avg:58.01ms
step:1901/2110 train_time:110314ms step_avg:58.03ms
step:1902/2110 train_time:110401ms step_avg:58.04ms
step:1903/2110 train_time:110489ms step_avg:58.06ms
step:1904/2110 train_time:110576ms step_avg:58.08ms
step:1905/2110 train_time:110665ms step_avg:58.09ms
step:1906/2110 train_time:110752ms step_avg:58.11ms
step:1907/2110 train_time:110842ms step_avg:58.12ms
step:1908/2110 train_time:110929ms step_avg:58.14ms
step:1909/2110 train_time:111018ms step_avg:58.16ms
step:1910/2110 train_time:111105ms step_avg:58.17ms
step:1911/2110 train_time:111193ms step_avg:58.19ms
step:1912/2110 train_time:111280ms step_avg:58.20ms
step:1913/2110 train_time:111368ms step_avg:58.22ms
step:1914/2110 train_time:111456ms step_avg:58.23ms
step:1915/2110 train_time:111543ms step_avg:58.25ms
step:1916/2110 train_time:111630ms step_avg:58.26ms
step:1917/2110 train_time:111719ms step_avg:58.28ms
step:1918/2110 train_time:111806ms step_avg:58.29ms
step:1919/2110 train_time:111895ms step_avg:58.31ms
step:1920/2110 train_time:111982ms step_avg:58.32ms
step:1921/2110 train_time:112071ms step_avg:58.34ms
step:1922/2110 train_time:112159ms step_avg:58.36ms
step:1923/2110 train_time:112247ms step_avg:58.37ms
step:1924/2110 train_time:112334ms step_avg:58.39ms
step:1925/2110 train_time:112423ms step_avg:58.40ms
step:1926/2110 train_time:112509ms step_avg:58.42ms
step:1927/2110 train_time:112598ms step_avg:58.43ms
step:1928/2110 train_time:112685ms step_avg:58.45ms
step:1929/2110 train_time:112774ms step_avg:58.46ms
step:1930/2110 train_time:112862ms step_avg:58.48ms
step:1931/2110 train_time:112950ms step_avg:58.49ms
step:1932/2110 train_time:113037ms step_avg:58.51ms
step:1933/2110 train_time:113126ms step_avg:58.52ms
step:1934/2110 train_time:113213ms step_avg:58.54ms
step:1935/2110 train_time:113302ms step_avg:58.55ms
step:1936/2110 train_time:113389ms step_avg:58.57ms
step:1937/2110 train_time:113478ms step_avg:58.58ms
step:1938/2110 train_time:113565ms step_avg:58.60ms
step:1939/2110 train_time:113652ms step_avg:58.61ms
step:1940/2110 train_time:113740ms step_avg:58.63ms
step:1941/2110 train_time:113829ms step_avg:58.64ms
step:1942/2110 train_time:113915ms step_avg:58.66ms
step:1943/2110 train_time:114004ms step_avg:58.67ms
step:1944/2110 train_time:114091ms step_avg:58.69ms
step:1945/2110 train_time:114181ms step_avg:58.70ms
step:1946/2110 train_time:114267ms step_avg:58.72ms
step:1947/2110 train_time:114356ms step_avg:58.73ms
step:1948/2110 train_time:114442ms step_avg:58.75ms
step:1949/2110 train_time:114531ms step_avg:58.76ms
step:1950/2110 train_time:114619ms step_avg:58.78ms
step:1951/2110 train_time:114707ms step_avg:58.79ms
step:1952/2110 train_time:114794ms step_avg:58.81ms
step:1953/2110 train_time:114884ms step_avg:58.82ms
step:1954/2110 train_time:114971ms step_avg:58.84ms
step:1955/2110 train_time:115061ms step_avg:58.85ms
step:1956/2110 train_time:115147ms step_avg:58.87ms
step:1957/2110 train_time:115236ms step_avg:58.88ms
step:1958/2110 train_time:115322ms step_avg:58.90ms
step:1959/2110 train_time:115411ms step_avg:58.91ms
step:1960/2110 train_time:115498ms step_avg:58.93ms
step:1961/2110 train_time:115587ms step_avg:58.94ms
step:1962/2110 train_time:115674ms step_avg:58.96ms
step:1963/2110 train_time:115763ms step_avg:58.97ms
step:1964/2110 train_time:115851ms step_avg:58.99ms
step:1965/2110 train_time:115941ms step_avg:59.00ms
step:1966/2110 train_time:116028ms step_avg:59.02ms
step:1967/2110 train_time:116116ms step_avg:59.03ms
step:1968/2110 train_time:116202ms step_avg:59.05ms
step:1969/2110 train_time:116291ms step_avg:59.06ms
step:1970/2110 train_time:116379ms step_avg:59.08ms
step:1971/2110 train_time:116468ms step_avg:59.09ms
step:1972/2110 train_time:116555ms step_avg:59.11ms
step:1973/2110 train_time:116644ms step_avg:59.12ms
step:1974/2110 train_time:116731ms step_avg:59.13ms
step:1975/2110 train_time:116820ms step_avg:59.15ms
step:1976/2110 train_time:116907ms step_avg:59.16ms
step:1977/2110 train_time:116996ms step_avg:59.18ms
step:1978/2110 train_time:117083ms step_avg:59.19ms
step:1979/2110 train_time:117172ms step_avg:59.21ms
step:1980/2110 train_time:117259ms step_avg:59.22ms
step:1981/2110 train_time:117349ms step_avg:59.24ms
step:1982/2110 train_time:117436ms step_avg:59.25ms
step:1983/2110 train_time:117525ms step_avg:59.27ms
step:1984/2110 train_time:117612ms step_avg:59.28ms
step:1985/2110 train_time:117701ms step_avg:59.30ms
step:1986/2110 train_time:117788ms step_avg:59.31ms
step:1987/2110 train_time:117877ms step_avg:59.32ms
step:1988/2110 train_time:117964ms step_avg:59.34ms
step:1989/2110 train_time:118053ms step_avg:59.35ms
step:1990/2110 train_time:118140ms step_avg:59.37ms
step:1991/2110 train_time:118228ms step_avg:59.38ms
step:1992/2110 train_time:118316ms step_avg:59.40ms
step:1993/2110 train_time:118405ms step_avg:59.41ms
step:1994/2110 train_time:118491ms step_avg:59.42ms
step:1995/2110 train_time:118582ms step_avg:59.44ms
step:1996/2110 train_time:118668ms step_avg:59.45ms
step:1997/2110 train_time:118758ms step_avg:59.47ms
step:1998/2110 train_time:118845ms step_avg:59.48ms
step:1999/2110 train_time:118933ms step_avg:59.50ms
step:2000/2110 train_time:119020ms step_avg:59.51ms
step:2000/2110 val_loss:3.3006 train_time:119110ms step_avg:59.56ms
step:2001/2110 train_time:119131ms step_avg:59.54ms
step:2002/2110 train_time:119202ms step_avg:59.54ms
step:2003/2110 train_time:119298ms step_avg:59.56ms
step:2004/2110 train_time:119386ms step_avg:59.57ms
step:2005/2110 train_time:119475ms step_avg:59.59ms
step:2006/2110 train_time:119560ms step_avg:59.60ms
step:2007/2110 train_time:119648ms step_avg:59.62ms
step:2008/2110 train_time:119735ms step_avg:59.63ms
step:2009/2110 train_time:119822ms step_avg:59.64ms
step:2010/2110 train_time:119909ms step_avg:59.66ms
step:2011/2110 train_time:119996ms step_avg:59.67ms
step:2012/2110 train_time:120085ms step_avg:59.68ms
step:2013/2110 train_time:120176ms step_avg:59.70ms
step:2014/2110 train_time:120265ms step_avg:59.71ms
step:2015/2110 train_time:120356ms step_avg:59.73ms
step:2016/2110 train_time:120444ms step_avg:59.74ms
step:2017/2110 train_time:120532ms step_avg:59.76ms
step:2018/2110 train_time:120618ms step_avg:59.77ms
step:2019/2110 train_time:120706ms step_avg:59.79ms
step:2020/2110 train_time:120793ms step_avg:59.80ms
step:2021/2110 train_time:120880ms step_avg:59.81ms
step:2022/2110 train_time:120966ms step_avg:59.83ms
step:2023/2110 train_time:121056ms step_avg:59.84ms
step:2024/2110 train_time:121145ms step_avg:59.85ms
step:2025/2110 train_time:121235ms step_avg:59.87ms
step:2026/2110 train_time:121323ms step_avg:59.88ms
step:2027/2110 train_time:121413ms step_avg:59.90ms
step:2028/2110 train_time:121499ms step_avg:59.91ms
step:2029/2110 train_time:121587ms step_avg:59.92ms
step:2030/2110 train_time:121674ms step_avg:59.94ms
step:2031/2110 train_time:121761ms step_avg:59.95ms
step:2032/2110 train_time:121847ms step_avg:59.96ms
step:2033/2110 train_time:121936ms step_avg:59.98ms
step:2034/2110 train_time:122023ms step_avg:59.99ms
step:2035/2110 train_time:122112ms step_avg:60.01ms
step:2036/2110 train_time:122200ms step_avg:60.02ms
step:2037/2110 train_time:122291ms step_avg:60.03ms
step:2038/2110 train_time:122379ms step_avg:60.05ms
step:2039/2110 train_time:122467ms step_avg:60.06ms
step:2040/2110 train_time:122554ms step_avg:60.08ms
step:2041/2110 train_time:122642ms step_avg:60.09ms
step:2042/2110 train_time:122729ms step_avg:60.10ms
step:2043/2110 train_time:122817ms step_avg:60.12ms
step:2044/2110 train_time:122903ms step_avg:60.13ms
step:2045/2110 train_time:122992ms step_avg:60.14ms
step:2046/2110 train_time:123079ms step_avg:60.16ms
step:2047/2110 train_time:123168ms step_avg:60.17ms
step:2048/2110 train_time:123256ms step_avg:60.18ms
step:2049/2110 train_time:123346ms step_avg:60.20ms
step:2050/2110 train_time:123434ms step_avg:60.21ms
step:2051/2110 train_time:123523ms step_avg:60.23ms
step:2052/2110 train_time:123611ms step_avg:60.24ms
step:2053/2110 train_time:123699ms step_avg:60.25ms
step:2054/2110 train_time:123786ms step_avg:60.27ms
step:2055/2110 train_time:123874ms step_avg:60.28ms
step:2056/2110 train_time:123961ms step_avg:60.29ms
step:2057/2110 train_time:124050ms step_avg:60.31ms
step:2058/2110 train_time:124137ms step_avg:60.32ms
step:2059/2110 train_time:124227ms step_avg:60.33ms
step:2060/2110 train_time:124316ms step_avg:60.35ms
step:2061/2110 train_time:124405ms step_avg:60.36ms
step:2062/2110 train_time:124493ms step_avg:60.37ms
step:2063/2110 train_time:124582ms step_avg:60.39ms
step:2064/2110 train_time:124669ms step_avg:60.40ms
step:2065/2110 train_time:124758ms step_avg:60.42ms
step:2066/2110 train_time:124845ms step_avg:60.43ms
step:2067/2110 train_time:124933ms step_avg:60.44ms
step:2068/2110 train_time:125020ms step_avg:60.45ms
step:2069/2110 train_time:125109ms step_avg:60.47ms
step:2070/2110 train_time:125196ms step_avg:60.48ms
step:2071/2110 train_time:125285ms step_avg:60.49ms
step:2072/2110 train_time:125373ms step_avg:60.51ms
step:2073/2110 train_time:125462ms step_avg:60.52ms
step:2074/2110 train_time:125550ms step_avg:60.54ms
step:2075/2110 train_time:125640ms step_avg:60.55ms
step:2076/2110 train_time:125727ms step_avg:60.56ms
step:2077/2110 train_time:125817ms step_avg:60.58ms
step:2078/2110 train_time:125904ms step_avg:60.59ms
step:2079/2110 train_time:125993ms step_avg:60.60ms
step:2080/2110 train_time:126081ms step_avg:60.62ms
step:2081/2110 train_time:126169ms step_avg:60.63ms
step:2082/2110 train_time:126256ms step_avg:60.64ms
step:2083/2110 train_time:126345ms step_avg:60.66ms
step:2084/2110 train_time:126433ms step_avg:60.67ms
step:2085/2110 train_time:126521ms step_avg:60.68ms
step:2086/2110 train_time:126608ms step_avg:60.69ms
step:2087/2110 train_time:126698ms step_avg:60.71ms
step:2088/2110 train_time:126786ms step_avg:60.72ms
step:2089/2110 train_time:126875ms step_avg:60.73ms
step:2090/2110 train_time:126963ms step_avg:60.75ms
step:2091/2110 train_time:127052ms step_avg:60.76ms
step:2092/2110 train_time:127140ms step_avg:60.77ms
step:2093/2110 train_time:127229ms step_avg:60.79ms
step:2094/2110 train_time:127316ms step_avg:60.80ms
step:2095/2110 train_time:127406ms step_avg:60.81ms
step:2096/2110 train_time:127494ms step_avg:60.83ms
step:2097/2110 train_time:127582ms step_avg:60.84ms
step:2098/2110 train_time:127669ms step_avg:60.85ms
step:2099/2110 train_time:127759ms step_avg:60.87ms
step:2100/2110 train_time:127847ms step_avg:60.88ms
step:2101/2110 train_time:127938ms step_avg:60.89ms
step:2102/2110 train_time:128026ms step_avg:60.91ms
step:2103/2110 train_time:128115ms step_avg:60.92ms
step:2104/2110 train_time:128202ms step_avg:60.93ms
step:2105/2110 train_time:128291ms step_avg:60.95ms
step:2106/2110 train_time:128378ms step_avg:60.96ms
step:2107/2110 train_time:128467ms step_avg:60.97ms
step:2108/2110 train_time:128554ms step_avg:60.98ms
step:2109/2110 train_time:128643ms step_avg:61.00ms
step:2110/2110 train_time:128730ms step_avg:61.01ms
step:2110/2110 val_loss:3.2762 train_time:128822ms step_avg:61.05ms
peak memory allocated: 29862 MiB reserved: 44836 MiB

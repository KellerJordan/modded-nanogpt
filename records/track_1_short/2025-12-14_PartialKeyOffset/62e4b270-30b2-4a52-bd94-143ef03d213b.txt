import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64


# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)
  
    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm
    
    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        
        # Referencing X twice causes pytorch to make a defensive copy, 
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split 
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X  
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
            
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, v, wd_tensor, lr_tensor):
    """Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors."""
    mask = (v * p) >= 0
    wd_factor = wd_tensor.to(p.dtype)
    lr_factor = lr_tensor.to(p.dtype)
    p.copy_(p - (p * mask * wd_factor * lr_factor) - (v * lr_factor))
    

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))

import torch
from torch import Tensor
import torch.distributed as dist
from collections import defaultdict

# -----------------------------------------------------------------------------
# NorMuon optimizer


class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])
            
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            # The below shape-based heuristic assumes that matrices have their input along the 
            # row dimension and their output along the columns. Gates are an exception.
            is_gate = ref_param.label in ['smear_gate', 'attn_gate']
            
            if "second_momentum_buffer" not in group:                
                if is_gate:
                    group["second_momentum_buffer"] = torch.zeros_like(updated_grads[..., :, :1])
                else:
                    group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                        if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                    )
            second_momentum_buffer = group["second_momentum_buffer"]
   
            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick 
            red_dim = -1 if (is_gate or param_shape[-2] >= param_shape[-1]) else -2
            
            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)
       
            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx],
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        # lm_head is ready earliest.
        label_order = ['lm_head', 'scalars', 'value_embed', 'embed']
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for label in label_order:
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in self.param_groups:
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float
    key_shift: bool

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas, key_shift = attn_args.ve, attn_args.sa_lambdas, attn_args.key_shift
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if key_shift:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim//4:self.head_dim//2] = k[:, :-1, :, self.head_dim//4:self.head_dim//2]
            k[:, 1:, :, self.head_dim//4+self.head_dim//2:] = k[:, :-1, :, self.head_dim//4+self.head_dim//2:]
        if ve is not None:
            v = v + ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim        
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2. 

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        # label all modules for explicit optimizer grouping
        self.embed.weight.label = 'embed'
        
        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.label = 'lm_head'
        # Add learnable skip connection weights for decoder layers
        pad = (-num_layers * 4 - 3) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    0 * torch.ones(num_layers),  # x0 lambdas
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> σ(-1.5) ≈ 0.18
                    torch.ones(pad),
                ]
            )
        )
        
        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.scalars[1 * self.num_layers: 2 * self.num_layers]
        sa_lambdas = self.scalars[2 * self.num_layers: 4 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[4 * self.num_layers]
        backout_lambda = self.scalars[4 * self.num_layers+1]
        skip_lambda = self.scalars[4 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_shift = [b==long_bm for b in bm_sizes] # apply key shift to long windows
        
        # start forward pass
        x = self.embed(input_seq)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers
        
        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        for i in range(self.num_layers):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale,
                key_shift = key_shift[i]
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_lambda)  # in (0, 1)
                x = x + gate * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args) 
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2070  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    embed_params + scalar_params + head_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.51  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.train()
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        if step % 2 == 1:
            optimizers[0].should_sync = True
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        if step % 2 == 0:
            optimizers[1].step()
            optimizers[1].zero_grad(set_to_none=True)
        else:
            for opt in optimizers:
                opt.step()
            model.zero_grad(set_to_none=True)
            optimizers[0].should_sync = False

model.zero_grad(set_to_none=True)
optimizers[0].should_sync = False
model.eval()

# warm up validation too
val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
val_loss = 0
with torch.no_grad():
    for step in range(val_steps):
        inputs, targets, cum_seqlens = next(val_loader)
        ws_idx = step % len(ws_schedule)
        if ws_idx==0:
            model.yarn.reset()
            ws_long = ws_schedule[0]
        else:
            new_ws_long = ws_schedule[ws_idx]
            model.yarn.apply(ws_long, new_ws_long)
            ws_long = new_ws_long
        val_loss += model(inputs, targets, cum_seqlens, ws_long // 2, ws_long)

del val_loader, val_loss
model.train()
model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

import gc
gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()
====================================================================================================
Running Python 3.10.12 (main, Nov  4 2025, 08:48:33) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Sun Dec 14 20:12:21 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   40C    P0            128W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            120W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   40C    P0            129W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0            125W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            121W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     42395      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42396      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42397      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42398      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42399      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42400      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42401      C   /usr/bin/python                                 0MiB |
|    0   N/A  N/A     42402      C   /usr/bin/python                                 0MiB |
|    1   N/A  N/A     42396      C   /usr/bin/python                                 0MiB |
|    2   N/A  N/A     42397      C   /usr/bin/python                                 0MiB |
|    3   N/A  N/A     42398      C   /usr/bin/python                                 0MiB |
|    4   N/A  N/A     42399      C   /usr/bin/python                                 0MiB |
|    5   N/A  N/A     42400      C   /usr/bin/python                                 0MiB |
|    6   N/A  N/A     42401      C   /usr/bin/python                                 0MiB |
|    7   N/A  N/A     42402      C   /usr/bin/python                                 0MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2110 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2110 train_time:79ms step_avg:79.03ms
step:2/2110 train_time:103ms step_avg:51.32ms
step:3/2110 train_time:123ms step_avg:41.12ms
step:4/2110 train_time:151ms step_avg:37.79ms
step:5/2110 train_time:184ms step_avg:36.80ms
step:6/2110 train_time:402ms step_avg:67.06ms
step:7/2110 train_time:421ms step_avg:60.10ms
step:8/2110 train_time:453ms step_avg:56.65ms
step:9/2110 train_time:487ms step_avg:54.06ms
step:10/2110 train_time:519ms step_avg:51.90ms
step:11/2110 train_time:553ms step_avg:50.25ms
step:12/2110 train_time:586ms step_avg:48.80ms
step:13/2110 train_time:619ms step_avg:47.62ms
step:14/2110 train_time:652ms step_avg:46.56ms
step:15/2110 train_time:686ms step_avg:45.71ms
step:16/2110 train_time:718ms step_avg:44.90ms
step:17/2110 train_time:752ms step_avg:44.24ms
step:18/2110 train_time:785ms step_avg:43.60ms
step:19/2110 train_time:819ms step_avg:43.09ms
step:20/2110 train_time:851ms step_avg:42.57ms
step:21/2110 train_time:885ms step_avg:42.14ms
step:22/2110 train_time:918ms step_avg:41.72ms
step:23/2110 train_time:952ms step_avg:41.38ms
step:24/2110 train_time:984ms step_avg:41.01ms
step:25/2110 train_time:1018ms step_avg:40.72ms
step:26/2110 train_time:1051ms step_avg:40.41ms
step:27/2110 train_time:1084ms step_avg:40.16ms
step:28/2110 train_time:1117ms step_avg:39.90ms
step:29/2110 train_time:1151ms step_avg:39.69ms
step:30/2110 train_time:1184ms step_avg:39.45ms
step:31/2110 train_time:1217ms step_avg:39.26ms
step:32/2110 train_time:1250ms step_avg:39.05ms
step:33/2110 train_time:1284ms step_avg:38.90ms
step:34/2110 train_time:1318ms step_avg:38.76ms
step:35/2110 train_time:1352ms step_avg:38.64ms
step:36/2110 train_time:1386ms step_avg:38.51ms
step:37/2110 train_time:1421ms step_avg:38.41ms
step:38/2110 train_time:1455ms step_avg:38.28ms
step:39/2110 train_time:1490ms step_avg:38.21ms
step:40/2110 train_time:1523ms step_avg:38.07ms
step:41/2110 train_time:1557ms step_avg:37.98ms
step:42/2110 train_time:1591ms step_avg:37.87ms
step:43/2110 train_time:1624ms step_avg:37.78ms
step:44/2110 train_time:1657ms step_avg:37.66ms
step:45/2110 train_time:1692ms step_avg:37.59ms
step:46/2110 train_time:1724ms step_avg:37.49ms
step:47/2110 train_time:1758ms step_avg:37.41ms
step:48/2110 train_time:1791ms step_avg:37.31ms
step:49/2110 train_time:1825ms step_avg:37.24ms
step:50/2110 train_time:1857ms step_avg:37.15ms
step:51/2110 train_time:1892ms step_avg:37.09ms
step:52/2110 train_time:1924ms step_avg:37.00ms
step:53/2110 train_time:1958ms step_avg:36.94ms
step:54/2110 train_time:1991ms step_avg:36.86ms
step:55/2110 train_time:2025ms step_avg:36.81ms
step:56/2110 train_time:2057ms step_avg:36.74ms
step:57/2110 train_time:2091ms step_avg:36.68ms
step:58/2110 train_time:2123ms step_avg:36.61ms
step:59/2110 train_time:2157ms step_avg:36.56ms
step:60/2110 train_time:2190ms step_avg:36.51ms
step:61/2110 train_time:2224ms step_avg:36.46ms
step:62/2110 train_time:2257ms step_avg:36.40ms
step:63/2110 train_time:2291ms step_avg:36.36ms
step:64/2110 train_time:2324ms step_avg:36.31ms
step:65/2110 train_time:2358ms step_avg:36.28ms
step:66/2110 train_time:2391ms step_avg:36.23ms
step:67/2110 train_time:2426ms step_avg:36.20ms
step:68/2110 train_time:2458ms step_avg:36.15ms
step:69/2110 train_time:2493ms step_avg:36.12ms
step:70/2110 train_time:2525ms step_avg:36.08ms
step:71/2110 train_time:2559ms step_avg:36.04ms
step:72/2110 train_time:2592ms step_avg:36.00ms
step:73/2110 train_time:2626ms step_avg:35.98ms
step:74/2110 train_time:2659ms step_avg:35.93ms
step:75/2110 train_time:2693ms step_avg:35.91ms
step:76/2110 train_time:2726ms step_avg:35.86ms
step:77/2110 train_time:2760ms step_avg:35.84ms
step:78/2110 train_time:2793ms step_avg:35.81ms
step:79/2110 train_time:2826ms step_avg:35.78ms
step:80/2110 train_time:2859ms step_avg:35.74ms
step:81/2110 train_time:2893ms step_avg:35.71ms
step:82/2110 train_time:2925ms step_avg:35.67ms
step:83/2110 train_time:2959ms step_avg:35.65ms
step:84/2110 train_time:2992ms step_avg:35.61ms
step:85/2110 train_time:3025ms step_avg:35.59ms
step:86/2110 train_time:3058ms step_avg:35.55ms
step:87/2110 train_time:3091ms step_avg:35.53ms
step:88/2110 train_time:3124ms step_avg:35.50ms
step:89/2110 train_time:3158ms step_avg:35.48ms
step:90/2110 train_time:3190ms step_avg:35.45ms
step:91/2110 train_time:3224ms step_avg:35.43ms
step:92/2110 train_time:3256ms step_avg:35.40ms
step:93/2110 train_time:3291ms step_avg:35.39ms
step:94/2110 train_time:3324ms step_avg:35.36ms
step:95/2110 train_time:3358ms step_avg:35.34ms
step:96/2110 train_time:3390ms step_avg:35.32ms
step:97/2110 train_time:3425ms step_avg:35.31ms
step:98/2110 train_time:3458ms step_avg:35.28ms
step:99/2110 train_time:3492ms step_avg:35.27ms
step:100/2110 train_time:3525ms step_avg:35.25ms
step:101/2110 train_time:3559ms step_avg:35.24ms
step:102/2110 train_time:3592ms step_avg:35.22ms
step:103/2110 train_time:3626ms step_avg:35.20ms
step:104/2110 train_time:3659ms step_avg:35.18ms
step:105/2110 train_time:3692ms step_avg:35.17ms
step:106/2110 train_time:3725ms step_avg:35.14ms
step:107/2110 train_time:3759ms step_avg:35.13ms
step:108/2110 train_time:3792ms step_avg:35.11ms
step:109/2110 train_time:3825ms step_avg:35.10ms
step:110/2110 train_time:3858ms step_avg:35.07ms
step:111/2110 train_time:3892ms step_avg:35.06ms
step:112/2110 train_time:3925ms step_avg:35.04ms
step:113/2110 train_time:3958ms step_avg:35.03ms
step:114/2110 train_time:3991ms step_avg:35.01ms
step:115/2110 train_time:4024ms step_avg:35.00ms
step:116/2110 train_time:4057ms step_avg:34.98ms
step:117/2110 train_time:4091ms step_avg:34.96ms
step:118/2110 train_time:4123ms step_avg:34.94ms
step:119/2110 train_time:4157ms step_avg:34.93ms
step:120/2110 train_time:4190ms step_avg:34.91ms
step:121/2110 train_time:4223ms step_avg:34.90ms
step:122/2110 train_time:4256ms step_avg:34.88ms
step:123/2110 train_time:4290ms step_avg:34.88ms
step:124/2110 train_time:4322ms step_avg:34.86ms
step:125/2110 train_time:4356ms step_avg:34.85ms
step:126/2110 train_time:4389ms step_avg:34.83ms
step:127/2110 train_time:4423ms step_avg:34.82ms
step:128/2110 train_time:4456ms step_avg:34.81ms
step:129/2110 train_time:4490ms step_avg:34.81ms
step:130/2110 train_time:4523ms step_avg:34.79ms
step:131/2110 train_time:4557ms step_avg:34.79ms
step:132/2110 train_time:4590ms step_avg:34.77ms
step:133/2110 train_time:4624ms step_avg:34.76ms
step:134/2110 train_time:4656ms step_avg:34.75ms
step:135/2110 train_time:4691ms step_avg:34.74ms
step:136/2110 train_time:4723ms step_avg:34.73ms
step:137/2110 train_time:4757ms step_avg:34.72ms
step:138/2110 train_time:4790ms step_avg:34.71ms
step:139/2110 train_time:4824ms step_avg:34.71ms
step:140/2110 train_time:4857ms step_avg:34.69ms
step:141/2110 train_time:4891ms step_avg:34.69ms
step:142/2110 train_time:4924ms step_avg:34.67ms
step:143/2110 train_time:4957ms step_avg:34.67ms
step:144/2110 train_time:4990ms step_avg:34.65ms
step:145/2110 train_time:5023ms step_avg:34.64ms
step:146/2110 train_time:5056ms step_avg:34.63ms
step:147/2110 train_time:5090ms step_avg:34.63ms
step:148/2110 train_time:5123ms step_avg:34.61ms
step:149/2110 train_time:5156ms step_avg:34.61ms
step:150/2110 train_time:5189ms step_avg:34.60ms
step:151/2110 train_time:5223ms step_avg:34.59ms
step:152/2110 train_time:5255ms step_avg:34.57ms
step:153/2110 train_time:5289ms step_avg:34.57ms
step:154/2110 train_time:5322ms step_avg:34.56ms
step:155/2110 train_time:5356ms step_avg:34.55ms
step:156/2110 train_time:5389ms step_avg:34.54ms
step:157/2110 train_time:5422ms step_avg:34.54ms
step:158/2110 train_time:5455ms step_avg:34.52ms
step:159/2110 train_time:5489ms step_avg:34.52ms
step:160/2110 train_time:5522ms step_avg:34.51ms
step:161/2110 train_time:5556ms step_avg:34.51ms
step:162/2110 train_time:5589ms step_avg:34.50ms
step:163/2110 train_time:5622ms step_avg:34.49ms
step:164/2110 train_time:5655ms step_avg:34.48ms
step:165/2110 train_time:5690ms step_avg:34.48ms
step:166/2110 train_time:5722ms step_avg:34.47ms
step:167/2110 train_time:5757ms step_avg:34.47ms
step:168/2110 train_time:5789ms step_avg:34.46ms
step:169/2110 train_time:5823ms step_avg:34.46ms
step:170/2110 train_time:5856ms step_avg:34.45ms
step:171/2110 train_time:5890ms step_avg:34.44ms
step:172/2110 train_time:5922ms step_avg:34.43ms
step:173/2110 train_time:5956ms step_avg:34.43ms
step:174/2110 train_time:5989ms step_avg:34.42ms
step:175/2110 train_time:6023ms step_avg:34.42ms
step:176/2110 train_time:6055ms step_avg:34.40ms
step:177/2110 train_time:6089ms step_avg:34.40ms
step:178/2110 train_time:6122ms step_avg:34.39ms
step:179/2110 train_time:6156ms step_avg:34.39ms
step:180/2110 train_time:6188ms step_avg:34.38ms
step:181/2110 train_time:6222ms step_avg:34.38ms
step:182/2110 train_time:6255ms step_avg:34.37ms
step:183/2110 train_time:6289ms step_avg:34.36ms
step:184/2110 train_time:6321ms step_avg:34.35ms
step:185/2110 train_time:6355ms step_avg:34.35ms
step:186/2110 train_time:6387ms step_avg:34.34ms
step:187/2110 train_time:6421ms step_avg:34.34ms
step:188/2110 train_time:6453ms step_avg:34.33ms
step:189/2110 train_time:6487ms step_avg:34.32ms
step:190/2110 train_time:6520ms step_avg:34.32ms
step:191/2110 train_time:6554ms step_avg:34.31ms
step:192/2110 train_time:6586ms step_avg:34.30ms
step:193/2110 train_time:6620ms step_avg:34.30ms
step:194/2110 train_time:6653ms step_avg:34.29ms
step:195/2110 train_time:6687ms step_avg:34.29ms
step:196/2110 train_time:6719ms step_avg:34.28ms
step:197/2110 train_time:6754ms step_avg:34.28ms
step:198/2110 train_time:6786ms step_avg:34.27ms
step:199/2110 train_time:6820ms step_avg:34.27ms
step:200/2110 train_time:6853ms step_avg:34.26ms
step:201/2110 train_time:6886ms step_avg:34.26ms
step:202/2110 train_time:6919ms step_avg:34.25ms
step:203/2110 train_time:6953ms step_avg:34.25ms
step:204/2110 train_time:6985ms step_avg:34.24ms
step:205/2110 train_time:7019ms step_avg:34.24ms
step:206/2110 train_time:7052ms step_avg:34.23ms
step:207/2110 train_time:7085ms step_avg:34.23ms
step:208/2110 train_time:7118ms step_avg:34.22ms
step:209/2110 train_time:7152ms step_avg:34.22ms
step:210/2110 train_time:7184ms step_avg:34.21ms
step:211/2110 train_time:7218ms step_avg:34.21ms
step:212/2110 train_time:7251ms step_avg:34.20ms
step:213/2110 train_time:7285ms step_avg:34.20ms
step:214/2110 train_time:7317ms step_avg:34.19ms
step:215/2110 train_time:7351ms step_avg:34.19ms
step:216/2110 train_time:7384ms step_avg:34.18ms
step:217/2110 train_time:7418ms step_avg:34.18ms
step:218/2110 train_time:7451ms step_avg:34.18ms
step:219/2110 train_time:7485ms step_avg:34.18ms
step:220/2110 train_time:7517ms step_avg:34.17ms
step:221/2110 train_time:7551ms step_avg:34.17ms
step:222/2110 train_time:7584ms step_avg:34.16ms
step:223/2110 train_time:7618ms step_avg:34.16ms
step:224/2110 train_time:7651ms step_avg:34.15ms
step:225/2110 train_time:7685ms step_avg:34.15ms
step:226/2110 train_time:7717ms step_avg:34.15ms
step:227/2110 train_time:7751ms step_avg:34.14ms
step:228/2110 train_time:7783ms step_avg:34.14ms
step:229/2110 train_time:7817ms step_avg:34.13ms
step:230/2110 train_time:7850ms step_avg:34.13ms
step:231/2110 train_time:7883ms step_avg:34.13ms
step:232/2110 train_time:7916ms step_avg:34.12ms
step:233/2110 train_time:7950ms step_avg:34.12ms
step:234/2110 train_time:7982ms step_avg:34.11ms
step:235/2110 train_time:8016ms step_avg:34.11ms
step:236/2110 train_time:8049ms step_avg:34.11ms
step:237/2110 train_time:8082ms step_avg:34.10ms
step:238/2110 train_time:8115ms step_avg:34.10ms
step:239/2110 train_time:8149ms step_avg:34.10ms
step:240/2110 train_time:8181ms step_avg:34.09ms
step:241/2110 train_time:8216ms step_avg:34.09ms
step:242/2110 train_time:8248ms step_avg:34.08ms
step:243/2110 train_time:8282ms step_avg:34.08ms
step:244/2110 train_time:8315ms step_avg:34.08ms
step:245/2110 train_time:8349ms step_avg:34.08ms
step:246/2110 train_time:8382ms step_avg:34.07ms
step:247/2110 train_time:8416ms step_avg:34.07ms
step:248/2110 train_time:8449ms step_avg:34.07ms
step:249/2110 train_time:8483ms step_avg:34.07ms
step:250/2110 train_time:8515ms step_avg:34.06ms
step:250/2110 val_loss:4.2632 train_time:8552ms step_avg:34.21ms
step:251/2110 train_time:8571ms step_avg:34.15ms
step:252/2110 train_time:8591ms step_avg:34.09ms
step:253/2110 train_time:8619ms step_avg:34.07ms
step:254/2110 train_time:8652ms step_avg:34.06ms
step:255/2110 train_time:8688ms step_avg:34.07ms
step:256/2110 train_time:8721ms step_avg:34.07ms
step:257/2110 train_time:8756ms step_avg:34.07ms
step:258/2110 train_time:8789ms step_avg:34.06ms
step:259/2110 train_time:8822ms step_avg:34.06ms
step:260/2110 train_time:8855ms step_avg:34.06ms
step:261/2110 train_time:8888ms step_avg:34.06ms
step:262/2110 train_time:8921ms step_avg:34.05ms
step:263/2110 train_time:8954ms step_avg:34.05ms
step:264/2110 train_time:8987ms step_avg:34.04ms
step:265/2110 train_time:9020ms step_avg:34.04ms
step:266/2110 train_time:9053ms step_avg:34.03ms
step:267/2110 train_time:9086ms step_avg:34.03ms
step:268/2110 train_time:9119ms step_avg:34.02ms
step:269/2110 train_time:9152ms step_avg:34.02ms
step:270/2110 train_time:9184ms step_avg:34.02ms
step:271/2110 train_time:9218ms step_avg:34.01ms
step:272/2110 train_time:9250ms step_avg:34.01ms
step:273/2110 train_time:9283ms step_avg:34.01ms
step:274/2110 train_time:9316ms step_avg:34.00ms
step:275/2110 train_time:9349ms step_avg:34.00ms
step:276/2110 train_time:9382ms step_avg:33.99ms
step:277/2110 train_time:9415ms step_avg:33.99ms
step:278/2110 train_time:9448ms step_avg:33.98ms
step:279/2110 train_time:9482ms step_avg:33.99ms
step:280/2110 train_time:9515ms step_avg:33.98ms
step:281/2110 train_time:9548ms step_avg:33.98ms
step:282/2110 train_time:9581ms step_avg:33.98ms
step:283/2110 train_time:9616ms step_avg:33.98ms
step:284/2110 train_time:9649ms step_avg:33.98ms
step:285/2110 train_time:9683ms step_avg:33.97ms
step:286/2110 train_time:9716ms step_avg:33.97ms
step:287/2110 train_time:9750ms step_avg:33.97ms
step:288/2110 train_time:9783ms step_avg:33.97ms
step:289/2110 train_time:9817ms step_avg:33.97ms
step:290/2110 train_time:9850ms step_avg:33.96ms
step:291/2110 train_time:9883ms step_avg:33.96ms
step:292/2110 train_time:9915ms step_avg:33.96ms
step:293/2110 train_time:9949ms step_avg:33.96ms
step:294/2110 train_time:9982ms step_avg:33.95ms
step:295/2110 train_time:10016ms step_avg:33.95ms
step:296/2110 train_time:10048ms step_avg:33.95ms
step:297/2110 train_time:10082ms step_avg:33.95ms
step:298/2110 train_time:10114ms step_avg:33.94ms
step:299/2110 train_time:10148ms step_avg:33.94ms
step:300/2110 train_time:10181ms step_avg:33.94ms
step:301/2110 train_time:10214ms step_avg:33.93ms
step:302/2110 train_time:10247ms step_avg:33.93ms
step:303/2110 train_time:10280ms step_avg:33.93ms
step:304/2110 train_time:10312ms step_avg:33.92ms
step:305/2110 train_time:10346ms step_avg:33.92ms
step:306/2110 train_time:10378ms step_avg:33.92ms
step:307/2110 train_time:10412ms step_avg:33.91ms
step:308/2110 train_time:10444ms step_avg:33.91ms
step:309/2110 train_time:10478ms step_avg:33.91ms
step:310/2110 train_time:10511ms step_avg:33.91ms
step:311/2110 train_time:10544ms step_avg:33.90ms
step:312/2110 train_time:10577ms step_avg:33.90ms
step:313/2110 train_time:10611ms step_avg:33.90ms
step:314/2110 train_time:10644ms step_avg:33.90ms
step:315/2110 train_time:10678ms step_avg:33.90ms
step:316/2110 train_time:10710ms step_avg:33.89ms
step:317/2110 train_time:10744ms step_avg:33.89ms
step:318/2110 train_time:10777ms step_avg:33.89ms
step:319/2110 train_time:10812ms step_avg:33.89ms
step:320/2110 train_time:10844ms step_avg:33.89ms
step:321/2110 train_time:10879ms step_avg:33.89ms
step:322/2110 train_time:10911ms step_avg:33.89ms
step:323/2110 train_time:10945ms step_avg:33.89ms
step:324/2110 train_time:10978ms step_avg:33.88ms
step:325/2110 train_time:11012ms step_avg:33.88ms
step:326/2110 train_time:11045ms step_avg:33.88ms
step:327/2110 train_time:11079ms step_avg:33.88ms
step:328/2110 train_time:11111ms step_avg:33.87ms
step:329/2110 train_time:11145ms step_avg:33.88ms
step:330/2110 train_time:11178ms step_avg:33.87ms
step:331/2110 train_time:11211ms step_avg:33.87ms
step:332/2110 train_time:11243ms step_avg:33.87ms
step:333/2110 train_time:11277ms step_avg:33.87ms
step:334/2110 train_time:11310ms step_avg:33.86ms
step:335/2110 train_time:11344ms step_avg:33.86ms
step:336/2110 train_time:11376ms step_avg:33.86ms
step:337/2110 train_time:11410ms step_avg:33.86ms
step:338/2110 train_time:11442ms step_avg:33.85ms
step:339/2110 train_time:11476ms step_avg:33.85ms
step:340/2110 train_time:11508ms step_avg:33.85ms
step:341/2110 train_time:11542ms step_avg:33.85ms
step:342/2110 train_time:11575ms step_avg:33.85ms
step:343/2110 train_time:11609ms step_avg:33.84ms
step:344/2110 train_time:11642ms step_avg:33.84ms
step:345/2110 train_time:11675ms step_avg:33.84ms
step:346/2110 train_time:11708ms step_avg:33.84ms
step:347/2110 train_time:11742ms step_avg:33.84ms
step:348/2110 train_time:11775ms step_avg:33.84ms
step:349/2110 train_time:11808ms step_avg:33.83ms
step:350/2110 train_time:11841ms step_avg:33.83ms
step:351/2110 train_time:11875ms step_avg:33.83ms
step:352/2110 train_time:11908ms step_avg:33.83ms
step:353/2110 train_time:11941ms step_avg:33.83ms
step:354/2110 train_time:11974ms step_avg:33.83ms
step:355/2110 train_time:12008ms step_avg:33.82ms
step:356/2110 train_time:12040ms step_avg:33.82ms
step:357/2110 train_time:12074ms step_avg:33.82ms
step:358/2110 train_time:12107ms step_avg:33.82ms
step:359/2110 train_time:12140ms step_avg:33.82ms
step:360/2110 train_time:12173ms step_avg:33.81ms
step:361/2110 train_time:12206ms step_avg:33.81ms
step:362/2110 train_time:12239ms step_avg:33.81ms
step:363/2110 train_time:12273ms step_avg:33.81ms
step:364/2110 train_time:12305ms step_avg:33.81ms
step:365/2110 train_time:12339ms step_avg:33.81ms
step:366/2110 train_time:12371ms step_avg:33.80ms
step:367/2110 train_time:12405ms step_avg:33.80ms
step:368/2110 train_time:12438ms step_avg:33.80ms
step:369/2110 train_time:12472ms step_avg:33.80ms
step:370/2110 train_time:12505ms step_avg:33.80ms
step:371/2110 train_time:12538ms step_avg:33.80ms
step:372/2110 train_time:12571ms step_avg:33.79ms
step:373/2110 train_time:12605ms step_avg:33.79ms
step:374/2110 train_time:12638ms step_avg:33.79ms
step:375/2110 train_time:12671ms step_avg:33.79ms
step:376/2110 train_time:12704ms step_avg:33.79ms
step:377/2110 train_time:12737ms step_avg:33.79ms
step:378/2110 train_time:12770ms step_avg:33.78ms
step:379/2110 train_time:12804ms step_avg:33.78ms
step:380/2110 train_time:12836ms step_avg:33.78ms
step:381/2110 train_time:12871ms step_avg:33.78ms
step:382/2110 train_time:12903ms step_avg:33.78ms
step:383/2110 train_time:12937ms step_avg:33.78ms
step:384/2110 train_time:12969ms step_avg:33.77ms
step:385/2110 train_time:13003ms step_avg:33.77ms
step:386/2110 train_time:13036ms step_avg:33.77ms
step:387/2110 train_time:13070ms step_avg:33.77ms
step:388/2110 train_time:13102ms step_avg:33.77ms
step:389/2110 train_time:13136ms step_avg:33.77ms
step:390/2110 train_time:13169ms step_avg:33.77ms
step:391/2110 train_time:13203ms step_avg:33.77ms
step:392/2110 train_time:13235ms step_avg:33.76ms
step:393/2110 train_time:13269ms step_avg:33.76ms
step:394/2110 train_time:13302ms step_avg:33.76ms
step:395/2110 train_time:13336ms step_avg:33.76ms
step:396/2110 train_time:13368ms step_avg:33.76ms
step:397/2110 train_time:13402ms step_avg:33.76ms
step:398/2110 train_time:13434ms step_avg:33.75ms
step:399/2110 train_time:13468ms step_avg:33.75ms
step:400/2110 train_time:13501ms step_avg:33.75ms
step:401/2110 train_time:13535ms step_avg:33.75ms
step:402/2110 train_time:13567ms step_avg:33.75ms
step:403/2110 train_time:13601ms step_avg:33.75ms
step:404/2110 train_time:13634ms step_avg:33.75ms
step:405/2110 train_time:13668ms step_avg:33.75ms
step:406/2110 train_time:13700ms step_avg:33.74ms
step:407/2110 train_time:13734ms step_avg:33.75ms
step:408/2110 train_time:13767ms step_avg:33.74ms
step:409/2110 train_time:13800ms step_avg:33.74ms
step:410/2110 train_time:13833ms step_avg:33.74ms
step:411/2110 train_time:13867ms step_avg:33.74ms
step:412/2110 train_time:13900ms step_avg:33.74ms
step:413/2110 train_time:13933ms step_avg:33.74ms
step:414/2110 train_time:13966ms step_avg:33.73ms
step:415/2110 train_time:14000ms step_avg:33.73ms
step:416/2110 train_time:14033ms step_avg:33.73ms
step:417/2110 train_time:14066ms step_avg:33.73ms
step:418/2110 train_time:14099ms step_avg:33.73ms
step:419/2110 train_time:14133ms step_avg:33.73ms
step:420/2110 train_time:14166ms step_avg:33.73ms
step:421/2110 train_time:14200ms step_avg:33.73ms
step:422/2110 train_time:14233ms step_avg:33.73ms
step:423/2110 train_time:14267ms step_avg:33.73ms
step:424/2110 train_time:14299ms step_avg:33.73ms
step:425/2110 train_time:14334ms step_avg:33.73ms
step:426/2110 train_time:14366ms step_avg:33.72ms
step:427/2110 train_time:14399ms step_avg:33.72ms
step:428/2110 train_time:14432ms step_avg:33.72ms
step:429/2110 train_time:14466ms step_avg:33.72ms
step:430/2110 train_time:14499ms step_avg:33.72ms
step:431/2110 train_time:14532ms step_avg:33.72ms
step:432/2110 train_time:14564ms step_avg:33.71ms
step:433/2110 train_time:14598ms step_avg:33.71ms
step:434/2110 train_time:14631ms step_avg:33.71ms
step:435/2110 train_time:14664ms step_avg:33.71ms
step:436/2110 train_time:14697ms step_avg:33.71ms
step:437/2110 train_time:14731ms step_avg:33.71ms
step:438/2110 train_time:14763ms step_avg:33.71ms
step:439/2110 train_time:14798ms step_avg:33.71ms
step:440/2110 train_time:14831ms step_avg:33.71ms
step:441/2110 train_time:14864ms step_avg:33.71ms
step:442/2110 train_time:14897ms step_avg:33.70ms
step:443/2110 train_time:14930ms step_avg:33.70ms
step:444/2110 train_time:14963ms step_avg:33.70ms
step:445/2110 train_time:14997ms step_avg:33.70ms
step:446/2110 train_time:15030ms step_avg:33.70ms
step:447/2110 train_time:15064ms step_avg:33.70ms
step:448/2110 train_time:15096ms step_avg:33.70ms
step:449/2110 train_time:15131ms step_avg:33.70ms
step:450/2110 train_time:15164ms step_avg:33.70ms
step:451/2110 train_time:15198ms step_avg:33.70ms
step:452/2110 train_time:15231ms step_avg:33.70ms
step:453/2110 train_time:15265ms step_avg:33.70ms
step:454/2110 train_time:15297ms step_avg:33.69ms
step:455/2110 train_time:15331ms step_avg:33.70ms
step:456/2110 train_time:15364ms step_avg:33.69ms
step:457/2110 train_time:15398ms step_avg:33.69ms
step:458/2110 train_time:15431ms step_avg:33.69ms
step:459/2110 train_time:15464ms step_avg:33.69ms
step:460/2110 train_time:15497ms step_avg:33.69ms
step:461/2110 train_time:15531ms step_avg:33.69ms
step:462/2110 train_time:15563ms step_avg:33.69ms
step:463/2110 train_time:15597ms step_avg:33.69ms
step:464/2110 train_time:15630ms step_avg:33.69ms
step:465/2110 train_time:15663ms step_avg:33.68ms
step:466/2110 train_time:15696ms step_avg:33.68ms
step:467/2110 train_time:15730ms step_avg:33.68ms
step:468/2110 train_time:15762ms step_avg:33.68ms
step:469/2110 train_time:15797ms step_avg:33.68ms
step:470/2110 train_time:15829ms step_avg:33.68ms
step:471/2110 train_time:15863ms step_avg:33.68ms
step:472/2110 train_time:15895ms step_avg:33.68ms
step:473/2110 train_time:15929ms step_avg:33.68ms
step:474/2110 train_time:15962ms step_avg:33.67ms
step:475/2110 train_time:15996ms step_avg:33.68ms
step:476/2110 train_time:16028ms step_avg:33.67ms
step:477/2110 train_time:16062ms step_avg:33.67ms
step:478/2110 train_time:16095ms step_avg:33.67ms
step:479/2110 train_time:16128ms step_avg:33.67ms
step:480/2110 train_time:16161ms step_avg:33.67ms
step:481/2110 train_time:16194ms step_avg:33.67ms
step:482/2110 train_time:16227ms step_avg:33.67ms
step:483/2110 train_time:16260ms step_avg:33.67ms
step:484/2110 train_time:16293ms step_avg:33.66ms
step:485/2110 train_time:16327ms step_avg:33.66ms
step:486/2110 train_time:16360ms step_avg:33.66ms
step:487/2110 train_time:16394ms step_avg:33.66ms
step:488/2110 train_time:16427ms step_avg:33.66ms
step:489/2110 train_time:16461ms step_avg:33.66ms
step:490/2110 train_time:16493ms step_avg:33.66ms
step:491/2110 train_time:16527ms step_avg:33.66ms
step:492/2110 train_time:16560ms step_avg:33.66ms
step:493/2110 train_time:16593ms step_avg:33.66ms
step:494/2110 train_time:16626ms step_avg:33.66ms
step:495/2110 train_time:16660ms step_avg:33.66ms
step:496/2110 train_time:16692ms step_avg:33.65ms
step:497/2110 train_time:16726ms step_avg:33.65ms
step:498/2110 train_time:16759ms step_avg:33.65ms
step:499/2110 train_time:16792ms step_avg:33.65ms
step:500/2110 train_time:16825ms step_avg:33.65ms
step:500/2110 val_loss:3.9991 train_time:16861ms step_avg:33.72ms
step:501/2110 train_time:16881ms step_avg:33.69ms
step:502/2110 train_time:16901ms step_avg:33.67ms
step:503/2110 train_time:16929ms step_avg:33.66ms
step:504/2110 train_time:16962ms step_avg:33.65ms
step:505/2110 train_time:16997ms step_avg:33.66ms
step:506/2110 train_time:17031ms step_avg:33.66ms
step:507/2110 train_time:17065ms step_avg:33.66ms
step:508/2110 train_time:17099ms step_avg:33.66ms
step:509/2110 train_time:17131ms step_avg:33.66ms
step:510/2110 train_time:17164ms step_avg:33.66ms
step:511/2110 train_time:17197ms step_avg:33.65ms
step:512/2110 train_time:17230ms step_avg:33.65ms
step:513/2110 train_time:17263ms step_avg:33.65ms
step:514/2110 train_time:17296ms step_avg:33.65ms
step:515/2110 train_time:17329ms step_avg:33.65ms
step:516/2110 train_time:17362ms step_avg:33.65ms
step:517/2110 train_time:17395ms step_avg:33.65ms
step:518/2110 train_time:17428ms step_avg:33.64ms
step:519/2110 train_time:17461ms step_avg:33.64ms
step:520/2110 train_time:17494ms step_avg:33.64ms
step:521/2110 train_time:17527ms step_avg:33.64ms
step:522/2110 train_time:17559ms step_avg:33.64ms
step:523/2110 train_time:17593ms step_avg:33.64ms
step:524/2110 train_time:17625ms step_avg:33.64ms
step:525/2110 train_time:17659ms step_avg:33.64ms
step:526/2110 train_time:17692ms step_avg:33.63ms
step:527/2110 train_time:17725ms step_avg:33.63ms
step:528/2110 train_time:17757ms step_avg:33.63ms
step:529/2110 train_time:17792ms step_avg:33.63ms
step:530/2110 train_time:17824ms step_avg:33.63ms
step:531/2110 train_time:17859ms step_avg:33.63ms
step:532/2110 train_time:17892ms step_avg:33.63ms
step:533/2110 train_time:17926ms step_avg:33.63ms
step:534/2110 train_time:17960ms step_avg:33.63ms
step:535/2110 train_time:17994ms step_avg:33.63ms
step:536/2110 train_time:18027ms step_avg:33.63ms
step:537/2110 train_time:18060ms step_avg:33.63ms
step:538/2110 train_time:18093ms step_avg:33.63ms
step:539/2110 train_time:18127ms step_avg:33.63ms
step:540/2110 train_time:18160ms step_avg:33.63ms
step:541/2110 train_time:18193ms step_avg:33.63ms
step:542/2110 train_time:18226ms step_avg:33.63ms
step:543/2110 train_time:18260ms step_avg:33.63ms
step:544/2110 train_time:18292ms step_avg:33.63ms
step:545/2110 train_time:18326ms step_avg:33.63ms
step:546/2110 train_time:18359ms step_avg:33.62ms
step:547/2110 train_time:18392ms step_avg:33.62ms
step:548/2110 train_time:18424ms step_avg:33.62ms
step:549/2110 train_time:18458ms step_avg:33.62ms
step:550/2110 train_time:18491ms step_avg:33.62ms
step:551/2110 train_time:18524ms step_avg:33.62ms
step:552/2110 train_time:18557ms step_avg:33.62ms
step:553/2110 train_time:18590ms step_avg:33.62ms
step:554/2110 train_time:18623ms step_avg:33.62ms
step:555/2110 train_time:18656ms step_avg:33.61ms
step:556/2110 train_time:18689ms step_avg:33.61ms
step:557/2110 train_time:18722ms step_avg:33.61ms
step:558/2110 train_time:18755ms step_avg:33.61ms
step:559/2110 train_time:18789ms step_avg:33.61ms
step:560/2110 train_time:18822ms step_avg:33.61ms
step:561/2110 train_time:18856ms step_avg:33.61ms
step:562/2110 train_time:18888ms step_avg:33.61ms
step:563/2110 train_time:18922ms step_avg:33.61ms
step:564/2110 train_time:18955ms step_avg:33.61ms
step:565/2110 train_time:18989ms step_avg:33.61ms
step:566/2110 train_time:19022ms step_avg:33.61ms
step:567/2110 train_time:19056ms step_avg:33.61ms
step:568/2110 train_time:19088ms step_avg:33.61ms
step:569/2110 train_time:19122ms step_avg:33.61ms
step:570/2110 train_time:19155ms step_avg:33.60ms
step:571/2110 train_time:19188ms step_avg:33.60ms
step:572/2110 train_time:19221ms step_avg:33.60ms
step:573/2110 train_time:19255ms step_avg:33.60ms
step:574/2110 train_time:19288ms step_avg:33.60ms
step:575/2110 train_time:19321ms step_avg:33.60ms
step:576/2110 train_time:19354ms step_avg:33.60ms
step:577/2110 train_time:19387ms step_avg:33.60ms
step:578/2110 train_time:19420ms step_avg:33.60ms
step:579/2110 train_time:19454ms step_avg:33.60ms
step:580/2110 train_time:19486ms step_avg:33.60ms
step:581/2110 train_time:19520ms step_avg:33.60ms
step:582/2110 train_time:19552ms step_avg:33.60ms
step:583/2110 train_time:19586ms step_avg:33.60ms
step:584/2110 train_time:19619ms step_avg:33.59ms
step:585/2110 train_time:19652ms step_avg:33.59ms
step:586/2110 train_time:19685ms step_avg:33.59ms
step:587/2110 train_time:19718ms step_avg:33.59ms
step:588/2110 train_time:19751ms step_avg:33.59ms
step:589/2110 train_time:19785ms step_avg:33.59ms
step:590/2110 train_time:19817ms step_avg:33.59ms
step:591/2110 train_time:19852ms step_avg:33.59ms
step:592/2110 train_time:19884ms step_avg:33.59ms
step:593/2110 train_time:19918ms step_avg:33.59ms
step:594/2110 train_time:19951ms step_avg:33.59ms
step:595/2110 train_time:19985ms step_avg:33.59ms
step:596/2110 train_time:20018ms step_avg:33.59ms
step:597/2110 train_time:20052ms step_avg:33.59ms
step:598/2110 train_time:20084ms step_avg:33.59ms
step:599/2110 train_time:20118ms step_avg:33.59ms
step:600/2110 train_time:20151ms step_avg:33.58ms
step:601/2110 train_time:20185ms step_avg:33.58ms
step:602/2110 train_time:20217ms step_avg:33.58ms
step:603/2110 train_time:20251ms step_avg:33.58ms
step:604/2110 train_time:20283ms step_avg:33.58ms
step:605/2110 train_time:20317ms step_avg:33.58ms
step:606/2110 train_time:20350ms step_avg:33.58ms
step:607/2110 train_time:20383ms step_avg:33.58ms
step:608/2110 train_time:20416ms step_avg:33.58ms
step:609/2110 train_time:20450ms step_avg:33.58ms
step:610/2110 train_time:20482ms step_avg:33.58ms
step:611/2110 train_time:20516ms step_avg:33.58ms
step:612/2110 train_time:20549ms step_avg:33.58ms
step:613/2110 train_time:20583ms step_avg:33.58ms
step:614/2110 train_time:20616ms step_avg:33.58ms
step:615/2110 train_time:20649ms step_avg:33.58ms
step:616/2110 train_time:20681ms step_avg:33.57ms
step:617/2110 train_time:20715ms step_avg:33.57ms
step:618/2110 train_time:20748ms step_avg:33.57ms
step:619/2110 train_time:20782ms step_avg:33.57ms
step:620/2110 train_time:20814ms step_avg:33.57ms
step:621/2110 train_time:20849ms step_avg:33.57ms
step:622/2110 train_time:20881ms step_avg:33.57ms
step:623/2110 train_time:20916ms step_avg:33.57ms
step:624/2110 train_time:20949ms step_avg:33.57ms
step:625/2110 train_time:20982ms step_avg:33.57ms
step:626/2110 train_time:21015ms step_avg:33.57ms
step:627/2110 train_time:21049ms step_avg:33.57ms
step:628/2110 train_time:21081ms step_avg:33.57ms
step:629/2110 train_time:21115ms step_avg:33.57ms
step:630/2110 train_time:21148ms step_avg:33.57ms
step:631/2110 train_time:21181ms step_avg:33.57ms
step:632/2110 train_time:21214ms step_avg:33.57ms
step:633/2110 train_time:21248ms step_avg:33.57ms
step:634/2110 train_time:21280ms step_avg:33.57ms
step:635/2110 train_time:21314ms step_avg:33.57ms
step:636/2110 train_time:21347ms step_avg:33.56ms
step:637/2110 train_time:21380ms step_avg:33.56ms
step:638/2110 train_time:21413ms step_avg:33.56ms
step:639/2110 train_time:21447ms step_avg:33.56ms
step:640/2110 train_time:21479ms step_avg:33.56ms
step:641/2110 train_time:21514ms step_avg:33.56ms
step:642/2110 train_time:21547ms step_avg:33.56ms
step:643/2110 train_time:21580ms step_avg:33.56ms
step:644/2110 train_time:21613ms step_avg:33.56ms
step:645/2110 train_time:21646ms step_avg:33.56ms
step:646/2110 train_time:21679ms step_avg:33.56ms
step:647/2110 train_time:21713ms step_avg:33.56ms
step:648/2110 train_time:21746ms step_avg:33.56ms
step:649/2110 train_time:21779ms step_avg:33.56ms
step:650/2110 train_time:21812ms step_avg:33.56ms
step:651/2110 train_time:21846ms step_avg:33.56ms
step:652/2110 train_time:21879ms step_avg:33.56ms
step:653/2110 train_time:21912ms step_avg:33.56ms
step:654/2110 train_time:21945ms step_avg:33.55ms
step:655/2110 train_time:21978ms step_avg:33.55ms
step:656/2110 train_time:22011ms step_avg:33.55ms
step:657/2110 train_time:22046ms step_avg:33.56ms
step:658/2110 train_time:22079ms step_avg:33.55ms
step:659/2110 train_time:22112ms step_avg:33.55ms
step:660/2110 train_time:22145ms step_avg:33.55ms
step:661/2110 train_time:22179ms step_avg:33.55ms
step:662/2110 train_time:22211ms step_avg:33.55ms
step:663/2110 train_time:22245ms step_avg:33.55ms
step:664/2110 train_time:22278ms step_avg:33.55ms
step:665/2110 train_time:22311ms step_avg:33.55ms
step:666/2110 train_time:22344ms step_avg:33.55ms
step:667/2110 train_time:22378ms step_avg:33.55ms
step:668/2110 train_time:22411ms step_avg:33.55ms
step:669/2110 train_time:22444ms step_avg:33.55ms
step:670/2110 train_time:22477ms step_avg:33.55ms
step:671/2110 train_time:22511ms step_avg:33.55ms
step:672/2110 train_time:22544ms step_avg:33.55ms
step:673/2110 train_time:22577ms step_avg:33.55ms
step:674/2110 train_time:22610ms step_avg:33.55ms
step:675/2110 train_time:22643ms step_avg:33.55ms
step:676/2110 train_time:22676ms step_avg:33.54ms
step:677/2110 train_time:22710ms step_avg:33.55ms
step:678/2110 train_time:22743ms step_avg:33.54ms
step:679/2110 train_time:22776ms step_avg:33.54ms
step:680/2110 train_time:22809ms step_avg:33.54ms
step:681/2110 train_time:22843ms step_avg:33.54ms
step:682/2110 train_time:22875ms step_avg:33.54ms
step:683/2110 train_time:22909ms step_avg:33.54ms
step:684/2110 train_time:22941ms step_avg:33.54ms
step:685/2110 train_time:22976ms step_avg:33.54ms
step:686/2110 train_time:23008ms step_avg:33.54ms
step:687/2110 train_time:23042ms step_avg:33.54ms
step:688/2110 train_time:23075ms step_avg:33.54ms
step:689/2110 train_time:23108ms step_avg:33.54ms
step:690/2110 train_time:23141ms step_avg:33.54ms
step:691/2110 train_time:23176ms step_avg:33.54ms
step:692/2110 train_time:23234ms step_avg:33.57ms
step:693/2110 train_time:23295ms step_avg:33.62ms
step:694/2110 train_time:23355ms step_avg:33.65ms
step:695/2110 train_time:23416ms step_avg:33.69ms
step:696/2110 train_time:23476ms step_avg:33.73ms
step:697/2110 train_time:23537ms step_avg:33.77ms
step:698/2110 train_time:23597ms step_avg:33.81ms
step:699/2110 train_time:23657ms step_avg:33.84ms
step:700/2110 train_time:23717ms step_avg:33.88ms
step:701/2110 train_time:23778ms step_avg:33.92ms
step:702/2110 train_time:23838ms step_avg:33.96ms
step:703/2110 train_time:23899ms step_avg:34.00ms
step:704/2110 train_time:23959ms step_avg:34.03ms
step:705/2110 train_time:24022ms step_avg:34.07ms
step:706/2110 train_time:24082ms step_avg:34.11ms
step:707/2110 train_time:24144ms step_avg:34.15ms
step:708/2110 train_time:24203ms step_avg:34.19ms
step:709/2110 train_time:24264ms step_avg:34.22ms
step:710/2110 train_time:24323ms step_avg:34.26ms
step:711/2110 train_time:24385ms step_avg:34.30ms
step:712/2110 train_time:24444ms step_avg:34.33ms
step:713/2110 train_time:24506ms step_avg:34.37ms
step:714/2110 train_time:24565ms step_avg:34.41ms
step:715/2110 train_time:24627ms step_avg:34.44ms
step:716/2110 train_time:24686ms step_avg:34.48ms
step:717/2110 train_time:24746ms step_avg:34.51ms
step:718/2110 train_time:24806ms step_avg:34.55ms
step:719/2110 train_time:24867ms step_avg:34.59ms
step:720/2110 train_time:24926ms step_avg:34.62ms
step:721/2110 train_time:24988ms step_avg:34.66ms
step:722/2110 train_time:25047ms step_avg:34.69ms
step:723/2110 train_time:25108ms step_avg:34.73ms
step:724/2110 train_time:25167ms step_avg:34.76ms
step:725/2110 train_time:25228ms step_avg:34.80ms
step:726/2110 train_time:25287ms step_avg:34.83ms
step:727/2110 train_time:25348ms step_avg:34.87ms
step:728/2110 train_time:25407ms step_avg:34.90ms
step:729/2110 train_time:25468ms step_avg:34.93ms
step:730/2110 train_time:25526ms step_avg:34.97ms
step:731/2110 train_time:25588ms step_avg:35.00ms
step:732/2110 train_time:25647ms step_avg:35.04ms
step:733/2110 train_time:25708ms step_avg:35.07ms
step:734/2110 train_time:25767ms step_avg:35.10ms
step:735/2110 train_time:25828ms step_avg:35.14ms
step:736/2110 train_time:25887ms step_avg:35.17ms
step:737/2110 train_time:25947ms step_avg:35.21ms
step:738/2110 train_time:26006ms step_avg:35.24ms
step:739/2110 train_time:26068ms step_avg:35.27ms
step:740/2110 train_time:26127ms step_avg:35.31ms
step:741/2110 train_time:26188ms step_avg:35.34ms
step:742/2110 train_time:26248ms step_avg:35.37ms
step:743/2110 train_time:26309ms step_avg:35.41ms
step:744/2110 train_time:26368ms step_avg:35.44ms
step:745/2110 train_time:26428ms step_avg:35.47ms
step:746/2110 train_time:26487ms step_avg:35.51ms
step:747/2110 train_time:26548ms step_avg:35.54ms
step:748/2110 train_time:26606ms step_avg:35.57ms
step:749/2110 train_time:26667ms step_avg:35.60ms
step:750/2110 train_time:26726ms step_avg:35.63ms
step:750/2110 val_loss:3.8437 train_time:26789ms step_avg:35.72ms
step:751/2110 train_time:26809ms step_avg:35.70ms
step:752/2110 train_time:26849ms step_avg:35.70ms
step:753/2110 train_time:26913ms step_avg:35.74ms
step:754/2110 train_time:26974ms step_avg:35.77ms
step:755/2110 train_time:27035ms step_avg:35.81ms
step:756/2110 train_time:27094ms step_avg:35.84ms
step:757/2110 train_time:27154ms step_avg:35.87ms
step:758/2110 train_time:27213ms step_avg:35.90ms
step:759/2110 train_time:27274ms step_avg:35.93ms
step:760/2110 train_time:27333ms step_avg:35.96ms
step:761/2110 train_time:27395ms step_avg:36.00ms
step:762/2110 train_time:27454ms step_avg:36.03ms
step:763/2110 train_time:27514ms step_avg:36.06ms
step:764/2110 train_time:27574ms step_avg:36.09ms
step:765/2110 train_time:27635ms step_avg:36.12ms
step:766/2110 train_time:27694ms step_avg:36.15ms
step:767/2110 train_time:27757ms step_avg:36.19ms
step:768/2110 train_time:27817ms step_avg:36.22ms
step:769/2110 train_time:27879ms step_avg:36.25ms
step:770/2110 train_time:27939ms step_avg:36.28ms
step:771/2110 train_time:27999ms step_avg:36.32ms
step:772/2110 train_time:28058ms step_avg:36.35ms
step:773/2110 train_time:28119ms step_avg:36.38ms
step:774/2110 train_time:28177ms step_avg:36.40ms
step:775/2110 train_time:28238ms step_avg:36.44ms
step:776/2110 train_time:28296ms step_avg:36.46ms
step:777/2110 train_time:28357ms step_avg:36.50ms
step:778/2110 train_time:28415ms step_avg:36.52ms
step:779/2110 train_time:28476ms step_avg:36.55ms
step:780/2110 train_time:28536ms step_avg:36.58ms
step:781/2110 train_time:28597ms step_avg:36.62ms
step:782/2110 train_time:28656ms step_avg:36.64ms
step:783/2110 train_time:28718ms step_avg:36.68ms
step:784/2110 train_time:28777ms step_avg:36.71ms
step:785/2110 train_time:28839ms step_avg:36.74ms
step:786/2110 train_time:28898ms step_avg:36.77ms
step:787/2110 train_time:28959ms step_avg:36.80ms
step:788/2110 train_time:29018ms step_avg:36.82ms
step:789/2110 train_time:29078ms step_avg:36.85ms
step:790/2110 train_time:29137ms step_avg:36.88ms
step:791/2110 train_time:29198ms step_avg:36.91ms
step:792/2110 train_time:29257ms step_avg:36.94ms
step:793/2110 train_time:29317ms step_avg:36.97ms
step:794/2110 train_time:29376ms step_avg:37.00ms
step:795/2110 train_time:29436ms step_avg:37.03ms
step:796/2110 train_time:29495ms step_avg:37.05ms
step:797/2110 train_time:29557ms step_avg:37.08ms
step:798/2110 train_time:29616ms step_avg:37.11ms
step:799/2110 train_time:29677ms step_avg:37.14ms
step:800/2110 train_time:29736ms step_avg:37.17ms
step:801/2110 train_time:29798ms step_avg:37.20ms
step:802/2110 train_time:29857ms step_avg:37.23ms
step:803/2110 train_time:29919ms step_avg:37.26ms
step:804/2110 train_time:29977ms step_avg:37.29ms
step:805/2110 train_time:30039ms step_avg:37.32ms
step:806/2110 train_time:30097ms step_avg:37.34ms
step:807/2110 train_time:30158ms step_avg:37.37ms
step:808/2110 train_time:30216ms step_avg:37.40ms
step:809/2110 train_time:30277ms step_avg:37.43ms
step:810/2110 train_time:30336ms step_avg:37.45ms
step:811/2110 train_time:30396ms step_avg:37.48ms
step:812/2110 train_time:30455ms step_avg:37.51ms
step:813/2110 train_time:30516ms step_avg:37.54ms
step:814/2110 train_time:30575ms step_avg:37.56ms
step:815/2110 train_time:30637ms step_avg:37.59ms
step:816/2110 train_time:30696ms step_avg:37.62ms
step:817/2110 train_time:30758ms step_avg:37.65ms
step:818/2110 train_time:30816ms step_avg:37.67ms
step:819/2110 train_time:30878ms step_avg:37.70ms
step:820/2110 train_time:30938ms step_avg:37.73ms
step:821/2110 train_time:30999ms step_avg:37.76ms
step:822/2110 train_time:31058ms step_avg:37.78ms
step:823/2110 train_time:31119ms step_avg:37.81ms
step:824/2110 train_time:31177ms step_avg:37.84ms
step:825/2110 train_time:31238ms step_avg:37.86ms
step:826/2110 train_time:31296ms step_avg:37.89ms
step:827/2110 train_time:31357ms step_avg:37.92ms
step:828/2110 train_time:31415ms step_avg:37.94ms
step:829/2110 train_time:31476ms step_avg:37.97ms
step:830/2110 train_time:31535ms step_avg:37.99ms
step:831/2110 train_time:31596ms step_avg:38.02ms
step:832/2110 train_time:31655ms step_avg:38.05ms
step:833/2110 train_time:31717ms step_avg:38.08ms
step:834/2110 train_time:31775ms step_avg:38.10ms
step:835/2110 train_time:31837ms step_avg:38.13ms
step:836/2110 train_time:31896ms step_avg:38.15ms
step:837/2110 train_time:31958ms step_avg:38.18ms
step:838/2110 train_time:32017ms step_avg:38.21ms
step:839/2110 train_time:32078ms step_avg:38.23ms
step:840/2110 train_time:32137ms step_avg:38.26ms
step:841/2110 train_time:32197ms step_avg:38.28ms
step:842/2110 train_time:32256ms step_avg:38.31ms
step:843/2110 train_time:32317ms step_avg:38.34ms
step:844/2110 train_time:32375ms step_avg:38.36ms
step:845/2110 train_time:32436ms step_avg:38.39ms
step:846/2110 train_time:32495ms step_avg:38.41ms
step:847/2110 train_time:32556ms step_avg:38.44ms
step:848/2110 train_time:32615ms step_avg:38.46ms
step:849/2110 train_time:32676ms step_avg:38.49ms
step:850/2110 train_time:32735ms step_avg:38.51ms
step:851/2110 train_time:32797ms step_avg:38.54ms
step:852/2110 train_time:32856ms step_avg:38.56ms
step:853/2110 train_time:32917ms step_avg:38.59ms
step:854/2110 train_time:32976ms step_avg:38.61ms
step:855/2110 train_time:33037ms step_avg:38.64ms
step:856/2110 train_time:33096ms step_avg:38.66ms
step:857/2110 train_time:33157ms step_avg:38.69ms
step:858/2110 train_time:33216ms step_avg:38.71ms
step:859/2110 train_time:33277ms step_avg:38.74ms
step:860/2110 train_time:33336ms step_avg:38.76ms
step:861/2110 train_time:33397ms step_avg:38.79ms
step:862/2110 train_time:33456ms step_avg:38.81ms
step:863/2110 train_time:33517ms step_avg:38.84ms
step:864/2110 train_time:33575ms step_avg:38.86ms
step:865/2110 train_time:33636ms step_avg:38.89ms
step:866/2110 train_time:33696ms step_avg:38.91ms
step:867/2110 train_time:33757ms step_avg:38.94ms
step:868/2110 train_time:33816ms step_avg:38.96ms
step:869/2110 train_time:33877ms step_avg:38.98ms
step:870/2110 train_time:33936ms step_avg:39.01ms
step:871/2110 train_time:33997ms step_avg:39.03ms
step:872/2110 train_time:34056ms step_avg:39.06ms
step:873/2110 train_time:34118ms step_avg:39.08ms
step:874/2110 train_time:34176ms step_avg:39.10ms
step:875/2110 train_time:34237ms step_avg:39.13ms
step:876/2110 train_time:34296ms step_avg:39.15ms
step:877/2110 train_time:34357ms step_avg:39.18ms
step:878/2110 train_time:34415ms step_avg:39.20ms
step:879/2110 train_time:34476ms step_avg:39.22ms
step:880/2110 train_time:34535ms step_avg:39.24ms
step:881/2110 train_time:34596ms step_avg:39.27ms
step:882/2110 train_time:34655ms step_avg:39.29ms
step:883/2110 train_time:34716ms step_avg:39.32ms
step:884/2110 train_time:34775ms step_avg:39.34ms
step:885/2110 train_time:34837ms step_avg:39.36ms
step:886/2110 train_time:34896ms step_avg:39.39ms
step:887/2110 train_time:34957ms step_avg:39.41ms
step:888/2110 train_time:35016ms step_avg:39.43ms
step:889/2110 train_time:35078ms step_avg:39.46ms
step:890/2110 train_time:35137ms step_avg:39.48ms
step:891/2110 train_time:35198ms step_avg:39.50ms
step:892/2110 train_time:35256ms step_avg:39.52ms
step:893/2110 train_time:35317ms step_avg:39.55ms
step:894/2110 train_time:35376ms step_avg:39.57ms
step:895/2110 train_time:35437ms step_avg:39.59ms
step:896/2110 train_time:35495ms step_avg:39.62ms
step:897/2110 train_time:35556ms step_avg:39.64ms
step:898/2110 train_time:35615ms step_avg:39.66ms
step:899/2110 train_time:35676ms step_avg:39.68ms
step:900/2110 train_time:35736ms step_avg:39.71ms
step:901/2110 train_time:35797ms step_avg:39.73ms
step:902/2110 train_time:35856ms step_avg:39.75ms
step:903/2110 train_time:35917ms step_avg:39.78ms
step:904/2110 train_time:35976ms step_avg:39.80ms
step:905/2110 train_time:36038ms step_avg:39.82ms
step:906/2110 train_time:36097ms step_avg:39.84ms
step:907/2110 train_time:36157ms step_avg:39.86ms
step:908/2110 train_time:36216ms step_avg:39.89ms
step:909/2110 train_time:36277ms step_avg:39.91ms
step:910/2110 train_time:36336ms step_avg:39.93ms
step:911/2110 train_time:36397ms step_avg:39.95ms
step:912/2110 train_time:36455ms step_avg:39.97ms
step:913/2110 train_time:36516ms step_avg:40.00ms
step:914/2110 train_time:36575ms step_avg:40.02ms
step:915/2110 train_time:36636ms step_avg:40.04ms
step:916/2110 train_time:36695ms step_avg:40.06ms
step:917/2110 train_time:36756ms step_avg:40.08ms
step:918/2110 train_time:36815ms step_avg:40.10ms
step:919/2110 train_time:36877ms step_avg:40.13ms
step:920/2110 train_time:36936ms step_avg:40.15ms
step:921/2110 train_time:36997ms step_avg:40.17ms
step:922/2110 train_time:37056ms step_avg:40.19ms
step:923/2110 train_time:37118ms step_avg:40.21ms
step:924/2110 train_time:37176ms step_avg:40.23ms
step:925/2110 train_time:37237ms step_avg:40.26ms
step:926/2110 train_time:37296ms step_avg:40.28ms
step:927/2110 train_time:37357ms step_avg:40.30ms
step:928/2110 train_time:37415ms step_avg:40.32ms
step:929/2110 train_time:37476ms step_avg:40.34ms
step:930/2110 train_time:37535ms step_avg:40.36ms
step:931/2110 train_time:37596ms step_avg:40.38ms
step:932/2110 train_time:37655ms step_avg:40.40ms
step:933/2110 train_time:37716ms step_avg:40.42ms
step:934/2110 train_time:37775ms step_avg:40.44ms
step:935/2110 train_time:37837ms step_avg:40.47ms
step:936/2110 train_time:37895ms step_avg:40.49ms
step:937/2110 train_time:37956ms step_avg:40.51ms
step:938/2110 train_time:38015ms step_avg:40.53ms
step:939/2110 train_time:38076ms step_avg:40.55ms
step:940/2110 train_time:38136ms step_avg:40.57ms
step:941/2110 train_time:38197ms step_avg:40.59ms
step:942/2110 train_time:38256ms step_avg:40.61ms
step:943/2110 train_time:38317ms step_avg:40.63ms
step:944/2110 train_time:38376ms step_avg:40.65ms
step:945/2110 train_time:38437ms step_avg:40.67ms
step:946/2110 train_time:38496ms step_avg:40.69ms
step:947/2110 train_time:38556ms step_avg:40.71ms
step:948/2110 train_time:38615ms step_avg:40.73ms
step:949/2110 train_time:38676ms step_avg:40.75ms
step:950/2110 train_time:38736ms step_avg:40.77ms
step:951/2110 train_time:38796ms step_avg:40.80ms
step:952/2110 train_time:38855ms step_avg:40.81ms
step:953/2110 train_time:38917ms step_avg:40.84ms
step:954/2110 train_time:38975ms step_avg:40.85ms
step:955/2110 train_time:39037ms step_avg:40.88ms
step:956/2110 train_time:39096ms step_avg:40.90ms
step:957/2110 train_time:39157ms step_avg:40.92ms
step:958/2110 train_time:39216ms step_avg:40.94ms
step:959/2110 train_time:39277ms step_avg:40.96ms
step:960/2110 train_time:39336ms step_avg:40.98ms
step:961/2110 train_time:39397ms step_avg:41.00ms
step:962/2110 train_time:39456ms step_avg:41.01ms
step:963/2110 train_time:39516ms step_avg:41.03ms
step:964/2110 train_time:39575ms step_avg:41.05ms
step:965/2110 train_time:39637ms step_avg:41.07ms
step:966/2110 train_time:39696ms step_avg:41.09ms
step:967/2110 train_time:39757ms step_avg:41.11ms
step:968/2110 train_time:39816ms step_avg:41.13ms
step:969/2110 train_time:39877ms step_avg:41.15ms
step:970/2110 train_time:39936ms step_avg:41.17ms
step:971/2110 train_time:39997ms step_avg:41.19ms
step:972/2110 train_time:40056ms step_avg:41.21ms
step:973/2110 train_time:40117ms step_avg:41.23ms
step:974/2110 train_time:40176ms step_avg:41.25ms
step:975/2110 train_time:40238ms step_avg:41.27ms
step:976/2110 train_time:40296ms step_avg:41.29ms
step:977/2110 train_time:40357ms step_avg:41.31ms
step:978/2110 train_time:40416ms step_avg:41.33ms
step:979/2110 train_time:40477ms step_avg:41.35ms
step:980/2110 train_time:40536ms step_avg:41.36ms
step:981/2110 train_time:40597ms step_avg:41.38ms
step:982/2110 train_time:40656ms step_avg:41.40ms
step:983/2110 train_time:40717ms step_avg:41.42ms
step:984/2110 train_time:40776ms step_avg:41.44ms
step:985/2110 train_time:40838ms step_avg:41.46ms
step:986/2110 train_time:40896ms step_avg:41.48ms
step:987/2110 train_time:40958ms step_avg:41.50ms
step:988/2110 train_time:41017ms step_avg:41.51ms
step:989/2110 train_time:41078ms step_avg:41.53ms
step:990/2110 train_time:41137ms step_avg:41.55ms
step:991/2110 train_time:41198ms step_avg:41.57ms
step:992/2110 train_time:41256ms step_avg:41.59ms
step:993/2110 train_time:41317ms step_avg:41.61ms
step:994/2110 train_time:41376ms step_avg:41.63ms
step:995/2110 train_time:41437ms step_avg:41.65ms
step:996/2110 train_time:41496ms step_avg:41.66ms
step:997/2110 train_time:41557ms step_avg:41.68ms
step:998/2110 train_time:41616ms step_avg:41.70ms
step:999/2110 train_time:41677ms step_avg:41.72ms
step:1000/2110 train_time:41737ms step_avg:41.74ms
step:1000/2110 val_loss:3.6970 train_time:41800ms step_avg:41.80ms
step:1001/2110 train_time:41819ms step_avg:41.78ms
step:1002/2110 train_time:41859ms step_avg:41.78ms
step:1003/2110 train_time:41922ms step_avg:41.80ms
step:1004/2110 train_time:41985ms step_avg:41.82ms
step:1005/2110 train_time:42046ms step_avg:41.84ms
step:1006/2110 train_time:42104ms step_avg:41.85ms
step:1007/2110 train_time:42164ms step_avg:41.87ms
step:1008/2110 train_time:42223ms step_avg:41.89ms
step:1009/2110 train_time:42283ms step_avg:41.91ms
step:1010/2110 train_time:42341ms step_avg:41.92ms
step:1011/2110 train_time:42402ms step_avg:41.94ms
step:1012/2110 train_time:42460ms step_avg:41.96ms
step:1013/2110 train_time:42520ms step_avg:41.97ms
step:1014/2110 train_time:42579ms step_avg:41.99ms
step:1015/2110 train_time:42640ms step_avg:42.01ms
step:1016/2110 train_time:42700ms step_avg:42.03ms
step:1017/2110 train_time:42762ms step_avg:42.05ms
step:1018/2110 train_time:42821ms step_avg:42.06ms
step:1019/2110 train_time:42884ms step_avg:42.08ms
step:1020/2110 train_time:42944ms step_avg:42.10ms
step:1021/2110 train_time:43005ms step_avg:42.12ms
step:1022/2110 train_time:43064ms step_avg:42.14ms
step:1023/2110 train_time:43125ms step_avg:42.16ms
step:1024/2110 train_time:43184ms step_avg:42.17ms
step:1025/2110 train_time:43244ms step_avg:42.19ms
step:1026/2110 train_time:43303ms step_avg:42.21ms
step:1027/2110 train_time:43363ms step_avg:42.22ms
step:1028/2110 train_time:43421ms step_avg:42.24ms
step:1029/2110 train_time:43482ms step_avg:42.26ms
step:1030/2110 train_time:43540ms step_avg:42.27ms
step:1031/2110 train_time:43601ms step_avg:42.29ms
step:1032/2110 train_time:43660ms step_avg:42.31ms
step:1033/2110 train_time:43721ms step_avg:42.32ms
step:1034/2110 train_time:43780ms step_avg:42.34ms
step:1035/2110 train_time:43842ms step_avg:42.36ms
step:1036/2110 train_time:43902ms step_avg:42.38ms
step:1037/2110 train_time:43964ms step_avg:42.39ms
step:1038/2110 train_time:44023ms step_avg:42.41ms
step:1039/2110 train_time:44085ms step_avg:42.43ms
step:1040/2110 train_time:44143ms step_avg:42.45ms
step:1041/2110 train_time:44204ms step_avg:42.46ms
step:1042/2110 train_time:44263ms step_avg:42.48ms
step:1043/2110 train_time:44323ms step_avg:42.50ms
step:1044/2110 train_time:44382ms step_avg:42.51ms
step:1045/2110 train_time:44442ms step_avg:42.53ms
step:1046/2110 train_time:44501ms step_avg:42.54ms
step:1047/2110 train_time:44561ms step_avg:42.56ms
step:1048/2110 train_time:44620ms step_avg:42.58ms
step:1049/2110 train_time:44682ms step_avg:42.59ms
step:1050/2110 train_time:44741ms step_avg:42.61ms
step:1051/2110 train_time:44803ms step_avg:42.63ms
step:1052/2110 train_time:44862ms step_avg:42.64ms
step:1053/2110 train_time:44923ms step_avg:42.66ms
step:1054/2110 train_time:44983ms step_avg:42.68ms
step:1055/2110 train_time:45044ms step_avg:42.70ms
step:1056/2110 train_time:45103ms step_avg:42.71ms
step:1057/2110 train_time:45164ms step_avg:42.73ms
step:1058/2110 train_time:45222ms step_avg:42.74ms
step:1059/2110 train_time:45284ms step_avg:42.76ms
step:1060/2110 train_time:45342ms step_avg:42.78ms
step:1061/2110 train_time:45403ms step_avg:42.79ms
step:1062/2110 train_time:45461ms step_avg:42.81ms
step:1063/2110 train_time:45523ms step_avg:42.82ms
step:1064/2110 train_time:45581ms step_avg:42.84ms
step:1065/2110 train_time:45642ms step_avg:42.86ms
step:1066/2110 train_time:45701ms step_avg:42.87ms
step:1067/2110 train_time:45762ms step_avg:42.89ms
step:1068/2110 train_time:45820ms step_avg:42.90ms
step:1069/2110 train_time:45882ms step_avg:42.92ms
step:1070/2110 train_time:45941ms step_avg:42.94ms
step:1071/2110 train_time:46003ms step_avg:42.95ms
step:1072/2110 train_time:46062ms step_avg:42.97ms
step:1073/2110 train_time:46123ms step_avg:42.99ms
step:1074/2110 train_time:46182ms step_avg:43.00ms
step:1075/2110 train_time:46243ms step_avg:43.02ms
step:1076/2110 train_time:46302ms step_avg:43.03ms
step:1077/2110 train_time:46362ms step_avg:43.05ms
step:1078/2110 train_time:46421ms step_avg:43.06ms
step:1079/2110 train_time:46482ms step_avg:43.08ms
step:1080/2110 train_time:46540ms step_avg:43.09ms
step:1081/2110 train_time:46601ms step_avg:43.11ms
step:1082/2110 train_time:46659ms step_avg:43.12ms
step:1083/2110 train_time:46721ms step_avg:43.14ms
step:1084/2110 train_time:46782ms step_avg:43.16ms
step:1085/2110 train_time:46843ms step_avg:43.17ms
step:1086/2110 train_time:46901ms step_avg:43.19ms
step:1087/2110 train_time:46962ms step_avg:43.20ms
step:1088/2110 train_time:47022ms step_avg:43.22ms
step:1089/2110 train_time:47083ms step_avg:43.24ms
step:1090/2110 train_time:47142ms step_avg:43.25ms
step:1091/2110 train_time:47203ms step_avg:43.27ms
step:1092/2110 train_time:47262ms step_avg:43.28ms
step:1093/2110 train_time:47323ms step_avg:43.30ms
step:1094/2110 train_time:47382ms step_avg:43.31ms
step:1095/2110 train_time:47442ms step_avg:43.33ms
step:1096/2110 train_time:47501ms step_avg:43.34ms
step:1097/2110 train_time:47562ms step_avg:43.36ms
step:1098/2110 train_time:47621ms step_avg:43.37ms
step:1099/2110 train_time:47683ms step_avg:43.39ms
step:1100/2110 train_time:47741ms step_avg:43.40ms
step:1101/2110 train_time:47803ms step_avg:43.42ms
step:1102/2110 train_time:47862ms step_avg:43.43ms
step:1103/2110 train_time:47923ms step_avg:43.45ms
step:1104/2110 train_time:47982ms step_avg:43.46ms
step:1105/2110 train_time:48044ms step_avg:43.48ms
step:1106/2110 train_time:48103ms step_avg:43.49ms
step:1107/2110 train_time:48164ms step_avg:43.51ms
step:1108/2110 train_time:48223ms step_avg:43.52ms
step:1109/2110 train_time:48284ms step_avg:43.54ms
step:1110/2110 train_time:48342ms step_avg:43.55ms
step:1111/2110 train_time:48403ms step_avg:43.57ms
step:1112/2110 train_time:48462ms step_avg:43.58ms
step:1113/2110 train_time:48523ms step_avg:43.60ms
step:1114/2110 train_time:48581ms step_avg:43.61ms
step:1115/2110 train_time:48642ms step_avg:43.62ms
step:1116/2110 train_time:48700ms step_avg:43.64ms
step:1117/2110 train_time:48762ms step_avg:43.65ms
step:1118/2110 train_time:48821ms step_avg:43.67ms
step:1119/2110 train_time:48883ms step_avg:43.68ms
step:1120/2110 train_time:48942ms step_avg:43.70ms
step:1121/2110 train_time:49003ms step_avg:43.71ms
step:1122/2110 train_time:49062ms step_avg:43.73ms
step:1123/2110 train_time:49124ms step_avg:43.74ms
step:1124/2110 train_time:49183ms step_avg:43.76ms
step:1125/2110 train_time:49244ms step_avg:43.77ms
step:1126/2110 train_time:49302ms step_avg:43.79ms
step:1127/2110 train_time:49363ms step_avg:43.80ms
step:1128/2110 train_time:49421ms step_avg:43.81ms
step:1129/2110 train_time:49482ms step_avg:43.83ms
step:1130/2110 train_time:49541ms step_avg:43.84ms
step:1131/2110 train_time:49601ms step_avg:43.86ms
step:1132/2110 train_time:49660ms step_avg:43.87ms
step:1133/2110 train_time:49722ms step_avg:43.89ms
step:1134/2110 train_time:49781ms step_avg:43.90ms
step:1135/2110 train_time:49843ms step_avg:43.91ms
step:1136/2110 train_time:49902ms step_avg:43.93ms
step:1137/2110 train_time:49963ms step_avg:43.94ms
step:1138/2110 train_time:50022ms step_avg:43.96ms
step:1139/2110 train_time:50084ms step_avg:43.97ms
step:1140/2110 train_time:50143ms step_avg:43.99ms
step:1141/2110 train_time:50204ms step_avg:44.00ms
step:1142/2110 train_time:50263ms step_avg:44.01ms
step:1143/2110 train_time:50323ms step_avg:44.03ms
step:1144/2110 train_time:50383ms step_avg:44.04ms
step:1145/2110 train_time:50443ms step_avg:44.06ms
step:1146/2110 train_time:50502ms step_avg:44.07ms
step:1147/2110 train_time:50563ms step_avg:44.08ms
step:1148/2110 train_time:50621ms step_avg:44.09ms
step:1149/2110 train_time:50682ms step_avg:44.11ms
step:1150/2110 train_time:50741ms step_avg:44.12ms
step:1151/2110 train_time:50803ms step_avg:44.14ms
step:1152/2110 train_time:50861ms step_avg:44.15ms
step:1153/2110 train_time:50922ms step_avg:44.17ms
step:1154/2110 train_time:50982ms step_avg:44.18ms
step:1155/2110 train_time:51043ms step_avg:44.19ms
step:1156/2110 train_time:51103ms step_avg:44.21ms
step:1157/2110 train_time:51163ms step_avg:44.22ms
step:1158/2110 train_time:51222ms step_avg:44.23ms
step:1159/2110 train_time:51283ms step_avg:44.25ms
step:1160/2110 train_time:51342ms step_avg:44.26ms
step:1161/2110 train_time:51402ms step_avg:44.27ms
step:1162/2110 train_time:51461ms step_avg:44.29ms
step:1163/2110 train_time:51523ms step_avg:44.30ms
step:1164/2110 train_time:51581ms step_avg:44.31ms
step:1165/2110 train_time:51642ms step_avg:44.33ms
step:1166/2110 train_time:51701ms step_avg:44.34ms
step:1167/2110 train_time:51762ms step_avg:44.35ms
step:1168/2110 train_time:51821ms step_avg:44.37ms
step:1169/2110 train_time:51882ms step_avg:44.38ms
step:1170/2110 train_time:51941ms step_avg:44.39ms
step:1171/2110 train_time:52003ms step_avg:44.41ms
step:1172/2110 train_time:52062ms step_avg:44.42ms
step:1173/2110 train_time:52122ms step_avg:44.44ms
step:1174/2110 train_time:52181ms step_avg:44.45ms
step:1175/2110 train_time:52242ms step_avg:44.46ms
step:1176/2110 train_time:52301ms step_avg:44.47ms
step:1177/2110 train_time:52362ms step_avg:44.49ms
step:1178/2110 train_time:52421ms step_avg:44.50ms
step:1179/2110 train_time:52483ms step_avg:44.51ms
step:1180/2110 train_time:52541ms step_avg:44.53ms
step:1181/2110 train_time:52603ms step_avg:44.54ms
step:1182/2110 train_time:52661ms step_avg:44.55ms
step:1183/2110 train_time:52722ms step_avg:44.57ms
step:1184/2110 train_time:52781ms step_avg:44.58ms
step:1185/2110 train_time:52842ms step_avg:44.59ms
step:1186/2110 train_time:52901ms step_avg:44.60ms
step:1187/2110 train_time:52962ms step_avg:44.62ms
step:1188/2110 train_time:53021ms step_avg:44.63ms
step:1189/2110 train_time:53083ms step_avg:44.65ms
step:1190/2110 train_time:53142ms step_avg:44.66ms
step:1191/2110 train_time:53203ms step_avg:44.67ms
step:1192/2110 train_time:53262ms step_avg:44.68ms
step:1193/2110 train_time:53323ms step_avg:44.70ms
step:1194/2110 train_time:53382ms step_avg:44.71ms
step:1195/2110 train_time:53443ms step_avg:44.72ms
step:1196/2110 train_time:53502ms step_avg:44.73ms
step:1197/2110 train_time:53563ms step_avg:44.75ms
step:1198/2110 train_time:53622ms step_avg:44.76ms
step:1199/2110 train_time:53683ms step_avg:44.77ms
step:1200/2110 train_time:53742ms step_avg:44.78ms
step:1201/2110 train_time:53803ms step_avg:44.80ms
step:1202/2110 train_time:53862ms step_avg:44.81ms
step:1203/2110 train_time:53923ms step_avg:44.82ms
step:1204/2110 train_time:53982ms step_avg:44.84ms
step:1205/2110 train_time:54043ms step_avg:44.85ms
step:1206/2110 train_time:54102ms step_avg:44.86ms
step:1207/2110 train_time:54163ms step_avg:44.87ms
step:1208/2110 train_time:54221ms step_avg:44.89ms
step:1209/2110 train_time:54283ms step_avg:44.90ms
step:1210/2110 train_time:54341ms step_avg:44.91ms
step:1211/2110 train_time:54403ms step_avg:44.92ms
step:1212/2110 train_time:54462ms step_avg:44.94ms
step:1213/2110 train_time:54523ms step_avg:44.95ms
step:1214/2110 train_time:54582ms step_avg:44.96ms
step:1215/2110 train_time:54643ms step_avg:44.97ms
step:1216/2110 train_time:54702ms step_avg:44.99ms
step:1217/2110 train_time:54763ms step_avg:45.00ms
step:1218/2110 train_time:54822ms step_avg:45.01ms
step:1219/2110 train_time:54884ms step_avg:45.02ms
step:1220/2110 train_time:54942ms step_avg:45.03ms
step:1221/2110 train_time:55003ms step_avg:45.05ms
step:1222/2110 train_time:55061ms step_avg:45.06ms
step:1223/2110 train_time:55123ms step_avg:45.07ms
step:1224/2110 train_time:55182ms step_avg:45.08ms
step:1225/2110 train_time:55243ms step_avg:45.10ms
step:1226/2110 train_time:55301ms step_avg:45.11ms
step:1227/2110 train_time:55363ms step_avg:45.12ms
step:1228/2110 train_time:55421ms step_avg:45.13ms
step:1229/2110 train_time:55483ms step_avg:45.14ms
step:1230/2110 train_time:55541ms step_avg:45.16ms
step:1231/2110 train_time:55602ms step_avg:45.17ms
step:1232/2110 train_time:55661ms step_avg:45.18ms
step:1233/2110 train_time:55721ms step_avg:45.19ms
step:1234/2110 train_time:55781ms step_avg:45.20ms
step:1235/2110 train_time:55842ms step_avg:45.22ms
step:1236/2110 train_time:55901ms step_avg:45.23ms
step:1237/2110 train_time:55962ms step_avg:45.24ms
step:1238/2110 train_time:56021ms step_avg:45.25ms
step:1239/2110 train_time:56082ms step_avg:45.26ms
step:1240/2110 train_time:56141ms step_avg:45.28ms
step:1241/2110 train_time:56202ms step_avg:45.29ms
step:1242/2110 train_time:56261ms step_avg:45.30ms
step:1243/2110 train_time:56321ms step_avg:45.31ms
step:1244/2110 train_time:56380ms step_avg:45.32ms
step:1245/2110 train_time:56442ms step_avg:45.33ms
step:1246/2110 train_time:56500ms step_avg:45.35ms
step:1247/2110 train_time:56561ms step_avg:45.36ms
step:1248/2110 train_time:56621ms step_avg:45.37ms
step:1249/2110 train_time:56682ms step_avg:45.38ms
step:1250/2110 train_time:56741ms step_avg:45.39ms
step:1250/2110 val_loss:3.5831 train_time:56804ms step_avg:45.44ms
step:1251/2110 train_time:56824ms step_avg:45.42ms
step:1252/2110 train_time:56863ms step_avg:45.42ms
step:1253/2110 train_time:56926ms step_avg:45.43ms
step:1254/2110 train_time:56987ms step_avg:45.44ms
step:1255/2110 train_time:57049ms step_avg:45.46ms
step:1256/2110 train_time:57108ms step_avg:45.47ms
step:1257/2110 train_time:57168ms step_avg:45.48ms
step:1258/2110 train_time:57227ms step_avg:45.49ms
step:1259/2110 train_time:57288ms step_avg:45.50ms
step:1260/2110 train_time:57347ms step_avg:45.51ms
step:1261/2110 train_time:57409ms step_avg:45.53ms
step:1262/2110 train_time:57469ms step_avg:45.54ms
step:1263/2110 train_time:57529ms step_avg:45.55ms
step:1264/2110 train_time:57588ms step_avg:45.56ms
step:1265/2110 train_time:57649ms step_avg:45.57ms
step:1266/2110 train_time:57708ms step_avg:45.58ms
step:1267/2110 train_time:57771ms step_avg:45.60ms
step:1268/2110 train_time:57831ms step_avg:45.61ms
step:1269/2110 train_time:57893ms step_avg:45.62ms
step:1270/2110 train_time:57953ms step_avg:45.63ms
step:1271/2110 train_time:58013ms step_avg:45.64ms
step:1272/2110 train_time:58072ms step_avg:45.65ms
step:1273/2110 train_time:58132ms step_avg:45.67ms
step:1274/2110 train_time:58191ms step_avg:45.68ms
step:1275/2110 train_time:58251ms step_avg:45.69ms
step:1276/2110 train_time:58309ms step_avg:45.70ms
step:1277/2110 train_time:58370ms step_avg:45.71ms
step:1278/2110 train_time:58430ms step_avg:45.72ms
step:1279/2110 train_time:58490ms step_avg:45.73ms
step:1280/2110 train_time:58549ms step_avg:45.74ms
step:1281/2110 train_time:58609ms step_avg:45.75ms
step:1282/2110 train_time:58669ms step_avg:45.76ms
step:1283/2110 train_time:58731ms step_avg:45.78ms
step:1284/2110 train_time:58790ms step_avg:45.79ms
step:1285/2110 train_time:58852ms step_avg:45.80ms
step:1286/2110 train_time:58911ms step_avg:45.81ms
step:1287/2110 train_time:58973ms step_avg:45.82ms
step:1288/2110 train_time:59033ms step_avg:45.83ms
step:1289/2110 train_time:59093ms step_avg:45.84ms
step:1290/2110 train_time:59152ms step_avg:45.85ms
step:1291/2110 train_time:59213ms step_avg:45.87ms
step:1292/2110 train_time:59272ms step_avg:45.88ms
step:1293/2110 train_time:59332ms step_avg:45.89ms
step:1294/2110 train_time:59390ms step_avg:45.90ms
step:1295/2110 train_time:59451ms step_avg:45.91ms
step:1296/2110 train_time:59509ms step_avg:45.92ms
step:1297/2110 train_time:59570ms step_avg:45.93ms
step:1298/2110 train_time:59629ms step_avg:45.94ms
step:1299/2110 train_time:59690ms step_avg:45.95ms
step:1300/2110 train_time:59749ms step_avg:45.96ms
step:1301/2110 train_time:59811ms step_avg:45.97ms
step:1302/2110 train_time:59870ms step_avg:45.98ms
step:1303/2110 train_time:59933ms step_avg:46.00ms
step:1304/2110 train_time:59991ms step_avg:46.01ms
step:1305/2110 train_time:60053ms step_avg:46.02ms
step:1306/2110 train_time:60111ms step_avg:46.03ms
step:1307/2110 train_time:60172ms step_avg:46.04ms
step:1308/2110 train_time:60231ms step_avg:46.05ms
step:1309/2110 train_time:60292ms step_avg:46.06ms
step:1310/2110 train_time:60351ms step_avg:46.07ms
step:1311/2110 train_time:60411ms step_avg:46.08ms
step:1312/2110 train_time:60470ms step_avg:46.09ms
step:1313/2110 train_time:60530ms step_avg:46.10ms
step:1314/2110 train_time:60588ms step_avg:46.11ms
step:1315/2110 train_time:60650ms step_avg:46.12ms
step:1316/2110 train_time:60708ms step_avg:46.13ms
step:1317/2110 train_time:60770ms step_avg:46.14ms
step:1318/2110 train_time:60829ms step_avg:46.15ms
step:1319/2110 train_time:60891ms step_avg:46.16ms
step:1320/2110 train_time:60950ms step_avg:46.17ms
step:1321/2110 train_time:61012ms step_avg:46.19ms
step:1322/2110 train_time:61071ms step_avg:46.20ms
step:1323/2110 train_time:61133ms step_avg:46.21ms
step:1324/2110 train_time:61191ms step_avg:46.22ms
step:1325/2110 train_time:61252ms step_avg:46.23ms
step:1326/2110 train_time:61311ms step_avg:46.24ms
step:1327/2110 train_time:61372ms step_avg:46.25ms
step:1328/2110 train_time:61431ms step_avg:46.26ms
step:1329/2110 train_time:61491ms step_avg:46.27ms
step:1330/2110 train_time:61549ms step_avg:46.28ms
step:1331/2110 train_time:61610ms step_avg:46.29ms
step:1332/2110 train_time:61669ms step_avg:46.30ms
step:1333/2110 train_time:61731ms step_avg:46.31ms
step:1334/2110 train_time:61790ms step_avg:46.32ms
step:1335/2110 train_time:61852ms step_avg:46.33ms
step:1336/2110 train_time:61911ms step_avg:46.34ms
step:1337/2110 train_time:61972ms step_avg:46.35ms
step:1338/2110 train_time:62031ms step_avg:46.36ms
step:1339/2110 train_time:62092ms step_avg:46.37ms
step:1340/2110 train_time:62151ms step_avg:46.38ms
step:1341/2110 train_time:62212ms step_avg:46.39ms
step:1342/2110 train_time:62271ms step_avg:46.40ms
step:1343/2110 train_time:62333ms step_avg:46.41ms
step:1344/2110 train_time:62391ms step_avg:46.42ms
step:1345/2110 train_time:62451ms step_avg:46.43ms
step:1346/2110 train_time:62510ms step_avg:46.44ms
step:1347/2110 train_time:62570ms step_avg:46.45ms
step:1348/2110 train_time:62629ms step_avg:46.46ms
step:1349/2110 train_time:62690ms step_avg:46.47ms
step:1350/2110 train_time:62749ms step_avg:46.48ms
step:1351/2110 train_time:62811ms step_avg:46.49ms
step:1352/2110 train_time:62869ms step_avg:46.50ms
step:1353/2110 train_time:62931ms step_avg:46.51ms
step:1354/2110 train_time:62989ms step_avg:46.52ms
step:1355/2110 train_time:63051ms step_avg:46.53ms
step:1356/2110 train_time:63110ms step_avg:46.54ms
step:1357/2110 train_time:63171ms step_avg:46.55ms
step:1358/2110 train_time:63231ms step_avg:46.56ms
step:1359/2110 train_time:63292ms step_avg:46.57ms
step:1360/2110 train_time:63350ms step_avg:46.58ms
step:1361/2110 train_time:63411ms step_avg:46.59ms
step:1362/2110 train_time:63470ms step_avg:46.60ms
step:1363/2110 train_time:63531ms step_avg:46.61ms
step:1364/2110 train_time:63589ms step_avg:46.62ms
step:1365/2110 train_time:63650ms step_avg:46.63ms
step:1366/2110 train_time:63709ms step_avg:46.64ms
step:1367/2110 train_time:63771ms step_avg:46.65ms
step:1368/2110 train_time:63830ms step_avg:46.66ms
step:1369/2110 train_time:63891ms step_avg:46.67ms
step:1370/2110 train_time:63951ms step_avg:46.68ms
step:1371/2110 train_time:64012ms step_avg:46.69ms
step:1372/2110 train_time:64071ms step_avg:46.70ms
step:1373/2110 train_time:64133ms step_avg:46.71ms
step:1374/2110 train_time:64192ms step_avg:46.72ms
step:1375/2110 train_time:64253ms step_avg:46.73ms
step:1376/2110 train_time:64312ms step_avg:46.74ms
step:1377/2110 train_time:64373ms step_avg:46.75ms
step:1378/2110 train_time:64431ms step_avg:46.76ms
step:1379/2110 train_time:64492ms step_avg:46.77ms
step:1380/2110 train_time:64551ms step_avg:46.78ms
step:1381/2110 train_time:64612ms step_avg:46.79ms
step:1382/2110 train_time:64699ms step_avg:46.82ms
step:1383/2110 train_time:64788ms step_avg:46.85ms
step:1384/2110 train_time:64875ms step_avg:46.87ms
step:1385/2110 train_time:64965ms step_avg:46.91ms
step:1386/2110 train_time:65053ms step_avg:46.94ms
step:1387/2110 train_time:65142ms step_avg:46.97ms
step:1388/2110 train_time:65230ms step_avg:47.00ms
step:1389/2110 train_time:65319ms step_avg:47.03ms
step:1390/2110 train_time:65405ms step_avg:47.05ms
step:1391/2110 train_time:65495ms step_avg:47.08ms
step:1392/2110 train_time:65581ms step_avg:47.11ms
step:1393/2110 train_time:65669ms step_avg:47.14ms
step:1394/2110 train_time:65756ms step_avg:47.17ms
step:1395/2110 train_time:65844ms step_avg:47.20ms
step:1396/2110 train_time:65932ms step_avg:47.23ms
step:1397/2110 train_time:66021ms step_avg:47.26ms
step:1398/2110 train_time:66108ms step_avg:47.29ms
step:1399/2110 train_time:66197ms step_avg:47.32ms
step:1400/2110 train_time:66284ms step_avg:47.35ms
step:1401/2110 train_time:66373ms step_avg:47.38ms
step:1402/2110 train_time:66461ms step_avg:47.40ms
step:1403/2110 train_time:66550ms step_avg:47.43ms
step:1404/2110 train_time:66637ms step_avg:47.46ms
step:1405/2110 train_time:66725ms step_avg:47.49ms
step:1406/2110 train_time:66812ms step_avg:47.52ms
step:1407/2110 train_time:66901ms step_avg:47.55ms
step:1408/2110 train_time:66987ms step_avg:47.58ms
step:1409/2110 train_time:67077ms step_avg:47.61ms
step:1410/2110 train_time:67164ms step_avg:47.63ms
step:1411/2110 train_time:67253ms step_avg:47.66ms
step:1412/2110 train_time:67340ms step_avg:47.69ms
step:1413/2110 train_time:67429ms step_avg:47.72ms
step:1414/2110 train_time:67516ms step_avg:47.75ms
step:1415/2110 train_time:67606ms step_avg:47.78ms
step:1416/2110 train_time:67693ms step_avg:47.81ms
step:1417/2110 train_time:67782ms step_avg:47.84ms
step:1418/2110 train_time:67870ms step_avg:47.86ms
step:1419/2110 train_time:67958ms step_avg:47.89ms
step:1420/2110 train_time:68045ms step_avg:47.92ms
step:1421/2110 train_time:68135ms step_avg:47.95ms
step:1422/2110 train_time:68222ms step_avg:47.98ms
step:1423/2110 train_time:68311ms step_avg:48.00ms
step:1424/2110 train_time:68398ms step_avg:48.03ms
step:1425/2110 train_time:68486ms step_avg:48.06ms
step:1426/2110 train_time:68573ms step_avg:48.09ms
step:1427/2110 train_time:68663ms step_avg:48.12ms
step:1428/2110 train_time:68751ms step_avg:48.14ms
step:1429/2110 train_time:68841ms step_avg:48.17ms
step:1430/2110 train_time:68927ms step_avg:48.20ms
step:1431/2110 train_time:69016ms step_avg:48.23ms
step:1432/2110 train_time:69104ms step_avg:48.26ms
step:1433/2110 train_time:69192ms step_avg:48.28ms
step:1434/2110 train_time:69280ms step_avg:48.31ms
step:1435/2110 train_time:69369ms step_avg:48.34ms
step:1436/2110 train_time:69456ms step_avg:48.37ms
step:1437/2110 train_time:69545ms step_avg:48.40ms
step:1438/2110 train_time:69632ms step_avg:48.42ms
step:1439/2110 train_time:69722ms step_avg:48.45ms
step:1440/2110 train_time:69809ms step_avg:48.48ms
step:1441/2110 train_time:69898ms step_avg:48.51ms
step:1442/2110 train_time:69985ms step_avg:48.53ms
step:1443/2110 train_time:70074ms step_avg:48.56ms
step:1444/2110 train_time:70161ms step_avg:48.59ms
step:1445/2110 train_time:70251ms step_avg:48.62ms
step:1446/2110 train_time:70338ms step_avg:48.64ms
step:1447/2110 train_time:70427ms step_avg:48.67ms
step:1448/2110 train_time:70514ms step_avg:48.70ms
step:1449/2110 train_time:70604ms step_avg:48.73ms
step:1450/2110 train_time:70691ms step_avg:48.75ms
step:1451/2110 train_time:70781ms step_avg:48.78ms
step:1452/2110 train_time:70867ms step_avg:48.81ms
step:1453/2110 train_time:70956ms step_avg:48.83ms
step:1454/2110 train_time:71043ms step_avg:48.86ms
step:1455/2110 train_time:71132ms step_avg:48.89ms
step:1456/2110 train_time:71219ms step_avg:48.91ms
step:1457/2110 train_time:71308ms step_avg:48.94ms
step:1458/2110 train_time:71395ms step_avg:48.97ms
step:1459/2110 train_time:71485ms step_avg:49.00ms
step:1460/2110 train_time:71572ms step_avg:49.02ms
step:1461/2110 train_time:71662ms step_avg:49.05ms
step:1462/2110 train_time:71748ms step_avg:49.08ms
step:1463/2110 train_time:71837ms step_avg:49.10ms
step:1464/2110 train_time:71924ms step_avg:49.13ms
step:1465/2110 train_time:72012ms step_avg:49.16ms
step:1466/2110 train_time:72100ms step_avg:49.18ms
step:1467/2110 train_time:72189ms step_avg:49.21ms
step:1468/2110 train_time:72276ms step_avg:49.23ms
step:1469/2110 train_time:72365ms step_avg:49.26ms
step:1470/2110 train_time:72452ms step_avg:49.29ms
step:1471/2110 train_time:72541ms step_avg:49.31ms
step:1472/2110 train_time:72628ms step_avg:49.34ms
step:1473/2110 train_time:72717ms step_avg:49.37ms
step:1474/2110 train_time:72804ms step_avg:49.39ms
step:1475/2110 train_time:72893ms step_avg:49.42ms
step:1476/2110 train_time:72980ms step_avg:49.44ms
step:1477/2110 train_time:73069ms step_avg:49.47ms
step:1478/2110 train_time:73156ms step_avg:49.50ms
step:1479/2110 train_time:73244ms step_avg:49.52ms
step:1480/2110 train_time:73331ms step_avg:49.55ms
step:1481/2110 train_time:73420ms step_avg:49.57ms
step:1482/2110 train_time:73507ms step_avg:49.60ms
step:1483/2110 train_time:73595ms step_avg:49.63ms
step:1484/2110 train_time:73682ms step_avg:49.65ms
step:1485/2110 train_time:73771ms step_avg:49.68ms
step:1486/2110 train_time:73859ms step_avg:49.70ms
step:1487/2110 train_time:73947ms step_avg:49.73ms
step:1488/2110 train_time:74034ms step_avg:49.75ms
step:1489/2110 train_time:74124ms step_avg:49.78ms
step:1490/2110 train_time:74211ms step_avg:49.81ms
step:1491/2110 train_time:74300ms step_avg:49.83ms
step:1492/2110 train_time:74387ms step_avg:49.86ms
step:1493/2110 train_time:74475ms step_avg:49.88ms
step:1494/2110 train_time:74563ms step_avg:49.91ms
step:1495/2110 train_time:74651ms step_avg:49.93ms
step:1496/2110 train_time:74739ms step_avg:49.96ms
step:1497/2110 train_time:74827ms step_avg:49.98ms
step:1498/2110 train_time:74914ms step_avg:50.01ms
step:1499/2110 train_time:75005ms step_avg:50.04ms
step:1500/2110 train_time:75092ms step_avg:50.06ms
step:1500/2110 val_loss:3.4738 train_time:75183ms step_avg:50.12ms
step:1501/2110 train_time:75203ms step_avg:50.10ms
step:1502/2110 train_time:75271ms step_avg:50.11ms
step:1503/2110 train_time:75363ms step_avg:50.14ms
step:1504/2110 train_time:75451ms step_avg:50.17ms
step:1505/2110 train_time:75541ms step_avg:50.19ms
step:1506/2110 train_time:75626ms step_avg:50.22ms
step:1507/2110 train_time:75715ms step_avg:50.24ms
step:1508/2110 train_time:75801ms step_avg:50.27ms
step:1509/2110 train_time:75889ms step_avg:50.29ms
step:1510/2110 train_time:75976ms step_avg:50.32ms
step:1511/2110 train_time:76064ms step_avg:50.34ms
step:1512/2110 train_time:76152ms step_avg:50.37ms
step:1513/2110 train_time:76243ms step_avg:50.39ms
step:1514/2110 train_time:76331ms step_avg:50.42ms
step:1515/2110 train_time:76421ms step_avg:50.44ms
step:1516/2110 train_time:76508ms step_avg:50.47ms
step:1517/2110 train_time:76598ms step_avg:50.49ms
step:1518/2110 train_time:76684ms step_avg:50.52ms
step:1519/2110 train_time:76771ms step_avg:50.54ms
step:1520/2110 train_time:76858ms step_avg:50.56ms
step:1521/2110 train_time:76947ms step_avg:50.59ms
step:1522/2110 train_time:77034ms step_avg:50.61ms
step:1523/2110 train_time:77124ms step_avg:50.64ms
step:1524/2110 train_time:77212ms step_avg:50.66ms
step:1525/2110 train_time:77302ms step_avg:50.69ms
step:1526/2110 train_time:77389ms step_avg:50.71ms
step:1527/2110 train_time:77479ms step_avg:50.74ms
step:1528/2110 train_time:77565ms step_avg:50.76ms
step:1529/2110 train_time:77654ms step_avg:50.79ms
step:1530/2110 train_time:77741ms step_avg:50.81ms
step:1531/2110 train_time:77829ms step_avg:50.84ms
step:1532/2110 train_time:77916ms step_avg:50.86ms
step:1533/2110 train_time:78004ms step_avg:50.88ms
step:1534/2110 train_time:78092ms step_avg:50.91ms
step:1535/2110 train_time:78180ms step_avg:50.93ms
step:1536/2110 train_time:78267ms step_avg:50.96ms
step:1537/2110 train_time:78357ms step_avg:50.98ms
step:1538/2110 train_time:78445ms step_avg:51.00ms
step:1539/2110 train_time:78534ms step_avg:51.03ms
step:1540/2110 train_time:78622ms step_avg:51.05ms
step:1541/2110 train_time:78710ms step_avg:51.08ms
step:1542/2110 train_time:78797ms step_avg:51.10ms
step:1543/2110 train_time:78885ms step_avg:51.12ms
step:1544/2110 train_time:78971ms step_avg:51.15ms
step:1545/2110 train_time:79061ms step_avg:51.17ms
step:1546/2110 train_time:79147ms step_avg:51.19ms
step:1547/2110 train_time:79237ms step_avg:51.22ms
step:1548/2110 train_time:79325ms step_avg:51.24ms
step:1549/2110 train_time:79414ms step_avg:51.27ms
step:1550/2110 train_time:79502ms step_avg:51.29ms
step:1551/2110 train_time:79591ms step_avg:51.32ms
step:1552/2110 train_time:79679ms step_avg:51.34ms
step:1553/2110 train_time:79766ms step_avg:51.36ms
step:1554/2110 train_time:79853ms step_avg:51.39ms
step:1555/2110 train_time:79942ms step_avg:51.41ms
step:1556/2110 train_time:80029ms step_avg:51.43ms
step:1557/2110 train_time:80117ms step_avg:51.46ms
step:1558/2110 train_time:80204ms step_avg:51.48ms
step:1559/2110 train_time:80293ms step_avg:51.50ms
step:1560/2110 train_time:80381ms step_avg:51.53ms
step:1561/2110 train_time:80470ms step_avg:51.55ms
step:1562/2110 train_time:80557ms step_avg:51.57ms
step:1563/2110 train_time:80646ms step_avg:51.60ms
step:1564/2110 train_time:80733ms step_avg:51.62ms
step:1565/2110 train_time:80823ms step_avg:51.64ms
step:1566/2110 train_time:80909ms step_avg:51.67ms
step:1567/2110 train_time:80998ms step_avg:51.69ms
step:1568/2110 train_time:81084ms step_avg:51.71ms
step:1569/2110 train_time:81173ms step_avg:51.74ms
step:1570/2110 train_time:81261ms step_avg:51.76ms
step:1571/2110 train_time:81350ms step_avg:51.78ms
step:1572/2110 train_time:81437ms step_avg:51.80ms
step:1573/2110 train_time:81527ms step_avg:51.83ms
step:1574/2110 train_time:81613ms step_avg:51.85ms
step:1575/2110 train_time:81702ms step_avg:51.87ms
step:1576/2110 train_time:81789ms step_avg:51.90ms
step:1577/2110 train_time:81878ms step_avg:51.92ms
step:1578/2110 train_time:81964ms step_avg:51.94ms
step:1579/2110 train_time:82054ms step_avg:51.97ms
step:1580/2110 train_time:82142ms step_avg:51.99ms
step:1581/2110 train_time:82231ms step_avg:52.01ms
step:1582/2110 train_time:82318ms step_avg:52.03ms
step:1583/2110 train_time:82407ms step_avg:52.06ms
step:1584/2110 train_time:82495ms step_avg:52.08ms
step:1585/2110 train_time:82584ms step_avg:52.10ms
step:1586/2110 train_time:82671ms step_avg:52.13ms
step:1587/2110 train_time:82760ms step_avg:52.15ms
step:1588/2110 train_time:82847ms step_avg:52.17ms
step:1589/2110 train_time:82935ms step_avg:52.19ms
step:1590/2110 train_time:83022ms step_avg:52.22ms
step:1591/2110 train_time:83111ms step_avg:52.24ms
step:1592/2110 train_time:83199ms step_avg:52.26ms
step:1593/2110 train_time:83287ms step_avg:52.28ms
step:1594/2110 train_time:83374ms step_avg:52.31ms
step:1595/2110 train_time:83464ms step_avg:52.33ms
step:1596/2110 train_time:83551ms step_avg:52.35ms
step:1597/2110 train_time:83641ms step_avg:52.37ms
step:1598/2110 train_time:83727ms step_avg:52.39ms
step:1599/2110 train_time:83815ms step_avg:52.42ms
step:1600/2110 train_time:83903ms step_avg:52.44ms
step:1601/2110 train_time:83991ms step_avg:52.46ms
step:1602/2110 train_time:84079ms step_avg:52.48ms
step:1603/2110 train_time:84168ms step_avg:52.51ms
step:1604/2110 train_time:84255ms step_avg:52.53ms
step:1605/2110 train_time:84344ms step_avg:52.55ms
step:1606/2110 train_time:84431ms step_avg:52.57ms
step:1607/2110 train_time:84521ms step_avg:52.60ms
step:1608/2110 train_time:84607ms step_avg:52.62ms
step:1609/2110 train_time:84696ms step_avg:52.64ms
step:1610/2110 train_time:84782ms step_avg:52.66ms
step:1611/2110 train_time:84871ms step_avg:52.68ms
step:1612/2110 train_time:84958ms step_avg:52.70ms
step:1613/2110 train_time:85046ms step_avg:52.73ms
step:1614/2110 train_time:85134ms step_avg:52.75ms
step:1615/2110 train_time:85223ms step_avg:52.77ms
step:1616/2110 train_time:85310ms step_avg:52.79ms
step:1617/2110 train_time:85399ms step_avg:52.81ms
step:1618/2110 train_time:85486ms step_avg:52.83ms
step:1619/2110 train_time:85574ms step_avg:52.86ms
step:1620/2110 train_time:85662ms step_avg:52.88ms
step:1621/2110 train_time:85751ms step_avg:52.90ms
step:1622/2110 train_time:85839ms step_avg:52.92ms
step:1623/2110 train_time:85927ms step_avg:52.94ms
step:1624/2110 train_time:86014ms step_avg:52.96ms
step:1625/2110 train_time:86103ms step_avg:52.99ms
step:1626/2110 train_time:86190ms step_avg:53.01ms
step:1627/2110 train_time:86279ms step_avg:53.03ms
step:1628/2110 train_time:86366ms step_avg:53.05ms
step:1629/2110 train_time:86455ms step_avg:53.07ms
step:1630/2110 train_time:86542ms step_avg:53.09ms
step:1631/2110 train_time:86631ms step_avg:53.12ms
step:1632/2110 train_time:86719ms step_avg:53.14ms
step:1633/2110 train_time:86808ms step_avg:53.16ms
step:1634/2110 train_time:86895ms step_avg:53.18ms
step:1635/2110 train_time:86984ms step_avg:53.20ms
step:1636/2110 train_time:87071ms step_avg:53.22ms
step:1637/2110 train_time:87160ms step_avg:53.24ms
step:1638/2110 train_time:87246ms step_avg:53.26ms
step:1639/2110 train_time:87335ms step_avg:53.29ms
step:1640/2110 train_time:87422ms step_avg:53.31ms
step:1641/2110 train_time:87511ms step_avg:53.33ms
step:1642/2110 train_time:87598ms step_avg:53.35ms
step:1643/2110 train_time:87687ms step_avg:53.37ms
step:1644/2110 train_time:87774ms step_avg:53.39ms
step:1645/2110 train_time:87864ms step_avg:53.41ms
step:1646/2110 train_time:87950ms step_avg:53.43ms
step:1647/2110 train_time:88040ms step_avg:53.45ms
step:1648/2110 train_time:88126ms step_avg:53.47ms
step:1649/2110 train_time:88216ms step_avg:53.50ms
step:1650/2110 train_time:88304ms step_avg:53.52ms
step:1651/2110 train_time:88393ms step_avg:53.54ms
step:1652/2110 train_time:88479ms step_avg:53.56ms
step:1653/2110 train_time:88568ms step_avg:53.58ms
step:1654/2110 train_time:88655ms step_avg:53.60ms
step:1655/2110 train_time:88744ms step_avg:53.62ms
step:1656/2110 train_time:88831ms step_avg:53.64ms
step:1657/2110 train_time:88920ms step_avg:53.66ms
step:1658/2110 train_time:89006ms step_avg:53.68ms
step:1659/2110 train_time:89096ms step_avg:53.70ms
step:1660/2110 train_time:89183ms step_avg:53.72ms
step:1661/2110 train_time:89272ms step_avg:53.75ms
step:1662/2110 train_time:89359ms step_avg:53.77ms
step:1663/2110 train_time:89448ms step_avg:53.79ms
step:1664/2110 train_time:89535ms step_avg:53.81ms
step:1665/2110 train_time:89625ms step_avg:53.83ms
step:1666/2110 train_time:89712ms step_avg:53.85ms
step:1667/2110 train_time:89802ms step_avg:53.87ms
step:1668/2110 train_time:89889ms step_avg:53.89ms
step:1669/2110 train_time:89978ms step_avg:53.91ms
step:1670/2110 train_time:90064ms step_avg:53.93ms
step:1671/2110 train_time:90153ms step_avg:53.95ms
step:1672/2110 train_time:90241ms step_avg:53.97ms
step:1673/2110 train_time:90330ms step_avg:53.99ms
step:1674/2110 train_time:90416ms step_avg:54.01ms
step:1675/2110 train_time:90506ms step_avg:54.03ms
step:1676/2110 train_time:90593ms step_avg:54.05ms
step:1677/2110 train_time:90682ms step_avg:54.07ms
step:1678/2110 train_time:90768ms step_avg:54.09ms
step:1679/2110 train_time:90857ms step_avg:54.11ms
step:1680/2110 train_time:90945ms step_avg:54.13ms
step:1681/2110 train_time:91034ms step_avg:54.15ms
step:1682/2110 train_time:91122ms step_avg:54.17ms
step:1683/2110 train_time:91212ms step_avg:54.20ms
step:1684/2110 train_time:91299ms step_avg:54.22ms
step:1685/2110 train_time:91387ms step_avg:54.24ms
step:1686/2110 train_time:91474ms step_avg:54.26ms
step:1687/2110 train_time:91564ms step_avg:54.28ms
step:1688/2110 train_time:91653ms step_avg:54.30ms
step:1689/2110 train_time:91742ms step_avg:54.32ms
step:1690/2110 train_time:91829ms step_avg:54.34ms
step:1691/2110 train_time:91918ms step_avg:54.36ms
step:1692/2110 train_time:92005ms step_avg:54.38ms
step:1693/2110 train_time:92093ms step_avg:54.40ms
step:1694/2110 train_time:92180ms step_avg:54.42ms
step:1695/2110 train_time:92269ms step_avg:54.44ms
step:1696/2110 train_time:92355ms step_avg:54.45ms
step:1697/2110 train_time:92444ms step_avg:54.48ms
step:1698/2110 train_time:92532ms step_avg:54.49ms
step:1699/2110 train_time:92621ms step_avg:54.51ms
step:1700/2110 train_time:92708ms step_avg:54.53ms
step:1701/2110 train_time:92797ms step_avg:54.55ms
step:1702/2110 train_time:92884ms step_avg:54.57ms
step:1703/2110 train_time:92973ms step_avg:54.59ms
step:1704/2110 train_time:93060ms step_avg:54.61ms
step:1705/2110 train_time:93149ms step_avg:54.63ms
step:1706/2110 train_time:93236ms step_avg:54.65ms
step:1707/2110 train_time:93324ms step_avg:54.67ms
step:1708/2110 train_time:93412ms step_avg:54.69ms
step:1709/2110 train_time:93501ms step_avg:54.71ms
step:1710/2110 train_time:93588ms step_avg:54.73ms
step:1711/2110 train_time:93677ms step_avg:54.75ms
step:1712/2110 train_time:93763ms step_avg:54.77ms
step:1713/2110 train_time:93852ms step_avg:54.79ms
step:1714/2110 train_time:93939ms step_avg:54.81ms
step:1715/2110 train_time:94028ms step_avg:54.83ms
step:1716/2110 train_time:94115ms step_avg:54.85ms
step:1717/2110 train_time:94204ms step_avg:54.87ms
step:1718/2110 train_time:94291ms step_avg:54.88ms
step:1719/2110 train_time:94380ms step_avg:54.90ms
step:1720/2110 train_time:94466ms step_avg:54.92ms
step:1721/2110 train_time:94555ms step_avg:54.94ms
step:1722/2110 train_time:94643ms step_avg:54.96ms
step:1723/2110 train_time:94732ms step_avg:54.98ms
step:1724/2110 train_time:94820ms step_avg:55.00ms
step:1725/2110 train_time:94909ms step_avg:55.02ms
step:1726/2110 train_time:94997ms step_avg:55.04ms
step:1727/2110 train_time:95085ms step_avg:55.06ms
step:1728/2110 train_time:95172ms step_avg:55.08ms
step:1729/2110 train_time:95261ms step_avg:55.10ms
step:1730/2110 train_time:95348ms step_avg:55.11ms
step:1731/2110 train_time:95438ms step_avg:55.13ms
step:1732/2110 train_time:95524ms step_avg:55.15ms
step:1733/2110 train_time:95614ms step_avg:55.17ms
step:1734/2110 train_time:95702ms step_avg:55.19ms
step:1735/2110 train_time:95792ms step_avg:55.21ms
step:1736/2110 train_time:95879ms step_avg:55.23ms
step:1737/2110 train_time:95968ms step_avg:55.25ms
step:1738/2110 train_time:96055ms step_avg:55.27ms
step:1739/2110 train_time:96145ms step_avg:55.29ms
step:1740/2110 train_time:96233ms step_avg:55.31ms
step:1741/2110 train_time:96322ms step_avg:55.33ms
step:1742/2110 train_time:96410ms step_avg:55.34ms
step:1743/2110 train_time:96498ms step_avg:55.36ms
step:1744/2110 train_time:96586ms step_avg:55.38ms
step:1745/2110 train_time:96674ms step_avg:55.40ms
step:1746/2110 train_time:96761ms step_avg:55.42ms
step:1747/2110 train_time:96851ms step_avg:55.44ms
step:1748/2110 train_time:96939ms step_avg:55.46ms
step:1749/2110 train_time:97027ms step_avg:55.48ms
step:1750/2110 train_time:97115ms step_avg:55.49ms
step:1750/2110 val_loss:3.3783 train_time:97206ms step_avg:55.55ms
step:1751/2110 train_time:97226ms step_avg:55.53ms
step:1752/2110 train_time:97297ms step_avg:55.53ms
step:1753/2110 train_time:97390ms step_avg:55.56ms
step:1754/2110 train_time:97478ms step_avg:55.57ms
step:1755/2110 train_time:97567ms step_avg:55.59ms
step:1756/2110 train_time:97653ms step_avg:55.61ms
step:1757/2110 train_time:97741ms step_avg:55.63ms
step:1758/2110 train_time:97827ms step_avg:55.65ms
step:1759/2110 train_time:97915ms step_avg:55.67ms
step:1760/2110 train_time:98003ms step_avg:55.68ms
step:1761/2110 train_time:98090ms step_avg:55.70ms
step:1762/2110 train_time:98178ms step_avg:55.72ms
step:1763/2110 train_time:98269ms step_avg:55.74ms
step:1764/2110 train_time:98357ms step_avg:55.76ms
step:1765/2110 train_time:98448ms step_avg:55.78ms
step:1766/2110 train_time:98536ms step_avg:55.80ms
step:1767/2110 train_time:98624ms step_avg:55.81ms
step:1768/2110 train_time:98710ms step_avg:55.83ms
step:1769/2110 train_time:98798ms step_avg:55.85ms
step:1770/2110 train_time:98884ms step_avg:55.87ms
step:1771/2110 train_time:98972ms step_avg:55.88ms
step:1772/2110 train_time:99059ms step_avg:55.90ms
step:1773/2110 train_time:99149ms step_avg:55.92ms
step:1774/2110 train_time:99237ms step_avg:55.94ms
step:1775/2110 train_time:99328ms step_avg:55.96ms
step:1776/2110 train_time:99416ms step_avg:55.98ms
step:1777/2110 train_time:99506ms step_avg:56.00ms
step:1778/2110 train_time:99592ms step_avg:56.01ms
step:1779/2110 train_time:99680ms step_avg:56.03ms
step:1780/2110 train_time:99766ms step_avg:56.05ms
step:1781/2110 train_time:99855ms step_avg:56.07ms
step:1782/2110 train_time:99942ms step_avg:56.08ms
step:1783/2110 train_time:100030ms step_avg:56.10ms
step:1784/2110 train_time:100118ms step_avg:56.12ms
step:1785/2110 train_time:100207ms step_avg:56.14ms
step:1786/2110 train_time:100295ms step_avg:56.16ms
step:1787/2110 train_time:100385ms step_avg:56.18ms
step:1788/2110 train_time:100473ms step_avg:56.19ms
step:1789/2110 train_time:100563ms step_avg:56.21ms
step:1790/2110 train_time:100649ms step_avg:56.23ms
step:1791/2110 train_time:100737ms step_avg:56.25ms
step:1792/2110 train_time:100824ms step_avg:56.26ms
step:1793/2110 train_time:100913ms step_avg:56.28ms
step:1794/2110 train_time:100999ms step_avg:56.30ms
step:1795/2110 train_time:101089ms step_avg:56.32ms
step:1796/2110 train_time:101176ms step_avg:56.33ms
step:1797/2110 train_time:101266ms step_avg:56.35ms
step:1798/2110 train_time:101353ms step_avg:56.37ms
step:1799/2110 train_time:101443ms step_avg:56.39ms
step:1800/2110 train_time:101530ms step_avg:56.41ms
step:1801/2110 train_time:101618ms step_avg:56.42ms
step:1802/2110 train_time:101705ms step_avg:56.44ms
step:1803/2110 train_time:101793ms step_avg:56.46ms
step:1804/2110 train_time:101880ms step_avg:56.47ms
step:1805/2110 train_time:101968ms step_avg:56.49ms
step:1806/2110 train_time:102055ms step_avg:56.51ms
step:1807/2110 train_time:102145ms step_avg:56.53ms
step:1808/2110 train_time:102232ms step_avg:56.54ms
step:1809/2110 train_time:102321ms step_avg:56.56ms
step:1810/2110 train_time:102409ms step_avg:56.58ms
step:1811/2110 train_time:102498ms step_avg:56.60ms
step:1812/2110 train_time:102585ms step_avg:56.61ms
step:1813/2110 train_time:102674ms step_avg:56.63ms
step:1814/2110 train_time:102762ms step_avg:56.65ms
step:1815/2110 train_time:102850ms step_avg:56.67ms
step:1816/2110 train_time:102937ms step_avg:56.68ms
step:1817/2110 train_time:103025ms step_avg:56.70ms
step:1818/2110 train_time:103112ms step_avg:56.72ms
step:1819/2110 train_time:103201ms step_avg:56.74ms
step:1820/2110 train_time:103288ms step_avg:56.75ms
step:1821/2110 train_time:103378ms step_avg:56.77ms
step:1822/2110 train_time:103467ms step_avg:56.79ms
step:1823/2110 train_time:103556ms step_avg:56.81ms
step:1824/2110 train_time:103644ms step_avg:56.82ms
step:1825/2110 train_time:103733ms step_avg:56.84ms
step:1826/2110 train_time:103819ms step_avg:56.86ms
step:1827/2110 train_time:103908ms step_avg:56.87ms
step:1828/2110 train_time:103994ms step_avg:56.89ms
step:1829/2110 train_time:104084ms step_avg:56.91ms
step:1830/2110 train_time:104172ms step_avg:56.92ms
step:1831/2110 train_time:104261ms step_avg:56.94ms
step:1832/2110 train_time:104348ms step_avg:56.96ms
step:1833/2110 train_time:104436ms step_avg:56.98ms
step:1834/2110 train_time:104523ms step_avg:56.99ms
step:1835/2110 train_time:104612ms step_avg:57.01ms
step:1836/2110 train_time:104699ms step_avg:57.03ms
step:1837/2110 train_time:104788ms step_avg:57.04ms
step:1838/2110 train_time:104875ms step_avg:57.06ms
step:1839/2110 train_time:104964ms step_avg:57.08ms
step:1840/2110 train_time:105051ms step_avg:57.09ms
step:1841/2110 train_time:105140ms step_avg:57.11ms
step:1842/2110 train_time:105226ms step_avg:57.13ms
step:1843/2110 train_time:105315ms step_avg:57.14ms
step:1844/2110 train_time:105402ms step_avg:57.16ms
step:1845/2110 train_time:105490ms step_avg:57.18ms
step:1846/2110 train_time:105577ms step_avg:57.19ms
step:1847/2110 train_time:105667ms step_avg:57.21ms
step:1848/2110 train_time:105754ms step_avg:57.23ms
step:1849/2110 train_time:105842ms step_avg:57.24ms
step:1850/2110 train_time:105929ms step_avg:57.26ms
step:1851/2110 train_time:106017ms step_avg:57.28ms
step:1852/2110 train_time:106104ms step_avg:57.29ms
step:1853/2110 train_time:106192ms step_avg:57.31ms
step:1854/2110 train_time:106281ms step_avg:57.33ms
step:1855/2110 train_time:106369ms step_avg:57.34ms
step:1856/2110 train_time:106457ms step_avg:57.36ms
step:1857/2110 train_time:106546ms step_avg:57.38ms
step:1858/2110 train_time:106633ms step_avg:57.39ms
step:1859/2110 train_time:106722ms step_avg:57.41ms
step:1860/2110 train_time:106809ms step_avg:57.42ms
step:1861/2110 train_time:106899ms step_avg:57.44ms
step:1862/2110 train_time:106985ms step_avg:57.46ms
step:1863/2110 train_time:107074ms step_avg:57.47ms
step:1864/2110 train_time:107161ms step_avg:57.49ms
step:1865/2110 train_time:107250ms step_avg:57.51ms
step:1866/2110 train_time:107338ms step_avg:57.52ms
step:1867/2110 train_time:107427ms step_avg:57.54ms
step:1868/2110 train_time:107514ms step_avg:57.56ms
step:1869/2110 train_time:107604ms step_avg:57.57ms
step:1870/2110 train_time:107690ms step_avg:57.59ms
step:1871/2110 train_time:107779ms step_avg:57.61ms
step:1872/2110 train_time:107866ms step_avg:57.62ms
step:1873/2110 train_time:107955ms step_avg:57.64ms
step:1874/2110 train_time:108044ms step_avg:57.65ms
step:1875/2110 train_time:108133ms step_avg:57.67ms
step:1876/2110 train_time:108220ms step_avg:57.69ms
step:1877/2110 train_time:108309ms step_avg:57.70ms
step:1878/2110 train_time:108397ms step_avg:57.72ms
step:1879/2110 train_time:108486ms step_avg:57.74ms
step:1880/2110 train_time:108574ms step_avg:57.75ms
step:1881/2110 train_time:108663ms step_avg:57.77ms
step:1882/2110 train_time:108750ms step_avg:57.78ms
step:1883/2110 train_time:108838ms step_avg:57.80ms
step:1884/2110 train_time:108926ms step_avg:57.82ms
step:1885/2110 train_time:109015ms step_avg:57.83ms
step:1886/2110 train_time:109101ms step_avg:57.85ms
step:1887/2110 train_time:109190ms step_avg:57.86ms
step:1888/2110 train_time:109278ms step_avg:57.88ms
step:1889/2110 train_time:109366ms step_avg:57.90ms
step:1890/2110 train_time:109454ms step_avg:57.91ms
step:1891/2110 train_time:109543ms step_avg:57.93ms
step:1892/2110 train_time:109630ms step_avg:57.94ms
step:1893/2110 train_time:109719ms step_avg:57.96ms
step:1894/2110 train_time:109806ms step_avg:57.98ms
step:1895/2110 train_time:109894ms step_avg:57.99ms
step:1896/2110 train_time:109982ms step_avg:58.01ms
step:1897/2110 train_time:110071ms step_avg:58.02ms
step:1898/2110 train_time:110158ms step_avg:58.04ms
step:1899/2110 train_time:110247ms step_avg:58.06ms
step:1900/2110 train_time:110334ms step_avg:58.07ms
step:1901/2110 train_time:110423ms step_avg:58.09ms
step:1902/2110 train_time:110510ms step_avg:58.10ms
step:1903/2110 train_time:110598ms step_avg:58.12ms
step:1904/2110 train_time:110686ms step_avg:58.13ms
step:1905/2110 train_time:110774ms step_avg:58.15ms
step:1906/2110 train_time:110861ms step_avg:58.16ms
step:1907/2110 train_time:110951ms step_avg:58.18ms
step:1908/2110 train_time:111037ms step_avg:58.20ms
step:1909/2110 train_time:111126ms step_avg:58.21ms
step:1910/2110 train_time:111213ms step_avg:58.23ms
step:1911/2110 train_time:111301ms step_avg:58.24ms
step:1912/2110 train_time:111388ms step_avg:58.26ms
step:1913/2110 train_time:111477ms step_avg:58.27ms
step:1914/2110 train_time:111565ms step_avg:58.29ms
step:1915/2110 train_time:111654ms step_avg:58.31ms
step:1916/2110 train_time:111742ms step_avg:58.32ms
step:1917/2110 train_time:111830ms step_avg:58.34ms
step:1918/2110 train_time:111917ms step_avg:58.35ms
step:1919/2110 train_time:112006ms step_avg:58.37ms
step:1920/2110 train_time:112094ms step_avg:58.38ms
step:1921/2110 train_time:112182ms step_avg:58.40ms
step:1922/2110 train_time:112270ms step_avg:58.41ms
step:1923/2110 train_time:112358ms step_avg:58.43ms
step:1924/2110 train_time:112446ms step_avg:58.44ms
step:1925/2110 train_time:112536ms step_avg:58.46ms
step:1926/2110 train_time:112624ms step_avg:58.48ms
step:1927/2110 train_time:112713ms step_avg:58.49ms
step:1928/2110 train_time:112800ms step_avg:58.51ms
step:1929/2110 train_time:112889ms step_avg:58.52ms
step:1930/2110 train_time:112976ms step_avg:58.54ms
step:1931/2110 train_time:113066ms step_avg:58.55ms
step:1932/2110 train_time:113153ms step_avg:58.57ms
step:1933/2110 train_time:113241ms step_avg:58.58ms
step:1934/2110 train_time:113328ms step_avg:58.60ms
step:1935/2110 train_time:113418ms step_avg:58.61ms
step:1936/2110 train_time:113505ms step_avg:58.63ms
step:1937/2110 train_time:113593ms step_avg:58.64ms
step:1938/2110 train_time:113680ms step_avg:58.66ms
step:1939/2110 train_time:113768ms step_avg:58.67ms
step:1940/2110 train_time:113856ms step_avg:58.69ms
step:1941/2110 train_time:113945ms step_avg:58.70ms
step:1942/2110 train_time:114032ms step_avg:58.72ms
step:1943/2110 train_time:114120ms step_avg:58.73ms
step:1944/2110 train_time:114207ms step_avg:58.75ms
step:1945/2110 train_time:114296ms step_avg:58.76ms
step:1946/2110 train_time:114385ms step_avg:58.78ms
step:1947/2110 train_time:114474ms step_avg:58.79ms
step:1948/2110 train_time:114561ms step_avg:58.81ms
step:1949/2110 train_time:114649ms step_avg:58.82ms
step:1950/2110 train_time:114737ms step_avg:58.84ms
step:1951/2110 train_time:114827ms step_avg:58.86ms
step:1952/2110 train_time:114914ms step_avg:58.87ms
step:1953/2110 train_time:115004ms step_avg:58.89ms
step:1954/2110 train_time:115091ms step_avg:58.90ms
step:1955/2110 train_time:115179ms step_avg:58.92ms
step:1956/2110 train_time:115265ms step_avg:58.93ms
step:1957/2110 train_time:115355ms step_avg:58.94ms
step:1958/2110 train_time:115443ms step_avg:58.96ms
step:1959/2110 train_time:115531ms step_avg:58.97ms
step:1960/2110 train_time:115618ms step_avg:58.99ms
step:1961/2110 train_time:115707ms step_avg:59.00ms
step:1962/2110 train_time:115794ms step_avg:59.02ms
step:1963/2110 train_time:115883ms step_avg:59.03ms
step:1964/2110 train_time:115970ms step_avg:59.05ms
step:1965/2110 train_time:116058ms step_avg:59.06ms
step:1966/2110 train_time:116146ms step_avg:59.08ms
step:1967/2110 train_time:116234ms step_avg:59.09ms
step:1968/2110 train_time:116321ms step_avg:59.11ms
step:1969/2110 train_time:116410ms step_avg:59.12ms
step:1970/2110 train_time:116498ms step_avg:59.14ms
step:1971/2110 train_time:116588ms step_avg:59.15ms
step:1972/2110 train_time:116676ms step_avg:59.17ms
step:1973/2110 train_time:116765ms step_avg:59.18ms
step:1974/2110 train_time:116852ms step_avg:59.20ms
step:1975/2110 train_time:116941ms step_avg:59.21ms
step:1976/2110 train_time:117028ms step_avg:59.22ms
step:1977/2110 train_time:117117ms step_avg:59.24ms
step:1978/2110 train_time:117204ms step_avg:59.25ms
step:1979/2110 train_time:117294ms step_avg:59.27ms
step:1980/2110 train_time:117381ms step_avg:59.28ms
step:1981/2110 train_time:117469ms step_avg:59.30ms
step:1982/2110 train_time:117557ms step_avg:59.31ms
step:1983/2110 train_time:117646ms step_avg:59.33ms
step:1984/2110 train_time:117734ms step_avg:59.34ms
step:1985/2110 train_time:117823ms step_avg:59.36ms
step:1986/2110 train_time:117910ms step_avg:59.37ms
step:1987/2110 train_time:117999ms step_avg:59.39ms
step:1988/2110 train_time:118086ms step_avg:59.40ms
step:1989/2110 train_time:118175ms step_avg:59.41ms
step:1990/2110 train_time:118262ms step_avg:59.43ms
step:1991/2110 train_time:118350ms step_avg:59.44ms
step:1992/2110 train_time:118439ms step_avg:59.46ms
step:1993/2110 train_time:118527ms step_avg:59.47ms
step:1994/2110 train_time:118613ms step_avg:59.49ms
step:1995/2110 train_time:118704ms step_avg:59.50ms
step:1996/2110 train_time:118790ms step_avg:59.51ms
step:1997/2110 train_time:118879ms step_avg:59.53ms
step:1998/2110 train_time:118966ms step_avg:59.54ms
step:1999/2110 train_time:119055ms step_avg:59.56ms
step:2000/2110 train_time:119143ms step_avg:59.57ms
step:2000/2110 val_loss:3.3046 train_time:119233ms step_avg:59.62ms
step:2001/2110 train_time:119256ms step_avg:59.60ms
step:2002/2110 train_time:119324ms step_avg:59.60ms
step:2003/2110 train_time:119417ms step_avg:59.62ms
step:2004/2110 train_time:119505ms step_avg:59.63ms
step:2005/2110 train_time:119593ms step_avg:59.65ms
step:2006/2110 train_time:119680ms step_avg:59.66ms
step:2007/2110 train_time:119767ms step_avg:59.67ms
step:2008/2110 train_time:119853ms step_avg:59.69ms
step:2009/2110 train_time:119941ms step_avg:59.70ms
step:2010/2110 train_time:120028ms step_avg:59.72ms
step:2011/2110 train_time:120116ms step_avg:59.73ms
step:2012/2110 train_time:120204ms step_avg:59.74ms
step:2013/2110 train_time:120294ms step_avg:59.76ms
step:2014/2110 train_time:120383ms step_avg:59.77ms
step:2015/2110 train_time:120473ms step_avg:59.79ms
step:2016/2110 train_time:120560ms step_avg:59.80ms
step:2017/2110 train_time:120649ms step_avg:59.82ms
step:2018/2110 train_time:120735ms step_avg:59.83ms
step:2019/2110 train_time:120823ms step_avg:59.84ms
step:2020/2110 train_time:120909ms step_avg:59.86ms
step:2021/2110 train_time:120997ms step_avg:59.87ms
step:2022/2110 train_time:121085ms step_avg:59.88ms
step:2023/2110 train_time:121174ms step_avg:59.90ms
step:2024/2110 train_time:121262ms step_avg:59.91ms
step:2025/2110 train_time:121353ms step_avg:59.93ms
step:2026/2110 train_time:121441ms step_avg:59.94ms
step:2027/2110 train_time:121530ms step_avg:59.96ms
step:2028/2110 train_time:121618ms step_avg:59.97ms
step:2029/2110 train_time:121706ms step_avg:59.98ms
step:2030/2110 train_time:121793ms step_avg:60.00ms
step:2031/2110 train_time:121881ms step_avg:60.01ms
step:2032/2110 train_time:121968ms step_avg:60.02ms
step:2033/2110 train_time:122056ms step_avg:60.04ms
step:2034/2110 train_time:122144ms step_avg:60.05ms
step:2035/2110 train_time:122232ms step_avg:60.07ms
step:2036/2110 train_time:122320ms step_avg:60.08ms
step:2037/2110 train_time:122410ms step_avg:60.09ms
step:2038/2110 train_time:122498ms step_avg:60.11ms
step:2039/2110 train_time:122589ms step_avg:60.12ms
step:2040/2110 train_time:122676ms step_avg:60.14ms
step:2041/2110 train_time:122765ms step_avg:60.15ms
step:2042/2110 train_time:122851ms step_avg:60.16ms
step:2043/2110 train_time:122939ms step_avg:60.18ms
step:2044/2110 train_time:123026ms step_avg:60.19ms
step:2045/2110 train_time:123114ms step_avg:60.20ms
step:2046/2110 train_time:123201ms step_avg:60.22ms
step:2047/2110 train_time:123290ms step_avg:60.23ms
step:2048/2110 train_time:123377ms step_avg:60.24ms
step:2049/2110 train_time:123467ms step_avg:60.26ms
step:2050/2110 train_time:123554ms step_avg:60.27ms
step:2051/2110 train_time:123643ms step_avg:60.28ms
step:2052/2110 train_time:123730ms step_avg:60.30ms
step:2053/2110 train_time:123819ms step_avg:60.31ms
step:2054/2110 train_time:123906ms step_avg:60.32ms
step:2055/2110 train_time:123994ms step_avg:60.34ms
step:2056/2110 train_time:124081ms step_avg:60.35ms
step:2057/2110 train_time:124170ms step_avg:60.36ms
step:2058/2110 train_time:124257ms step_avg:60.38ms
step:2059/2110 train_time:124347ms step_avg:60.39ms
step:2060/2110 train_time:124434ms step_avg:60.40ms
step:2061/2110 train_time:124525ms step_avg:60.42ms
step:2062/2110 train_time:124612ms step_avg:60.43ms
step:2063/2110 train_time:124701ms step_avg:60.45ms
step:2064/2110 train_time:124789ms step_avg:60.46ms
step:2065/2110 train_time:124879ms step_avg:60.47ms
step:2066/2110 train_time:124966ms step_avg:60.49ms
step:2067/2110 train_time:125054ms step_avg:60.50ms
step:2068/2110 train_time:125142ms step_avg:60.51ms
step:2069/2110 train_time:125230ms step_avg:60.53ms
step:2070/2110 train_time:125318ms step_avg:60.54ms
step:2071/2110 train_time:125408ms step_avg:60.55ms
step:2072/2110 train_time:125496ms step_avg:60.57ms
step:2073/2110 train_time:125586ms step_avg:60.58ms
step:2074/2110 train_time:125673ms step_avg:60.59ms
step:2075/2110 train_time:125763ms step_avg:60.61ms
step:2076/2110 train_time:125849ms step_avg:60.62ms
step:2077/2110 train_time:125938ms step_avg:60.63ms
step:2078/2110 train_time:126026ms step_avg:60.65ms
step:2079/2110 train_time:126114ms step_avg:60.66ms
step:2080/2110 train_time:126202ms step_avg:60.67ms
step:2081/2110 train_time:126291ms step_avg:60.69ms
step:2082/2110 train_time:126379ms step_avg:60.70ms
step:2083/2110 train_time:126469ms step_avg:60.71ms
step:2084/2110 train_time:126556ms step_avg:60.73ms
step:2085/2110 train_time:126646ms step_avg:60.74ms
step:2086/2110 train_time:126733ms step_avg:60.75ms
step:2087/2110 train_time:126822ms step_avg:60.77ms
step:2088/2110 train_time:126909ms step_avg:60.78ms
step:2089/2110 train_time:126998ms step_avg:60.79ms
step:2090/2110 train_time:127087ms step_avg:60.81ms
step:2091/2110 train_time:127177ms step_avg:60.82ms
step:2092/2110 train_time:127264ms step_avg:60.83ms
step:2093/2110 train_time:127352ms step_avg:60.85ms
step:2094/2110 train_time:127440ms step_avg:60.86ms
step:2095/2110 train_time:127529ms step_avg:60.87ms
step:2096/2110 train_time:127617ms step_avg:60.89ms
step:2097/2110 train_time:127707ms step_avg:60.90ms
step:2098/2110 train_time:127794ms step_avg:60.91ms
step:2099/2110 train_time:127884ms step_avg:60.93ms
step:2100/2110 train_time:127971ms step_avg:60.94ms
step:2101/2110 train_time:128059ms step_avg:60.95ms
step:2102/2110 train_time:128147ms step_avg:60.96ms
step:2103/2110 train_time:128236ms step_avg:60.98ms
step:2104/2110 train_time:128324ms step_avg:60.99ms
step:2105/2110 train_time:128412ms step_avg:61.00ms
step:2106/2110 train_time:128500ms step_avg:61.02ms
step:2107/2110 train_time:128590ms step_avg:61.03ms
step:2108/2110 train_time:128679ms step_avg:61.04ms
step:2109/2110 train_time:128768ms step_avg:61.06ms
step:2110/2110 train_time:128855ms step_avg:61.07ms
step:2110/2110 val_loss:3.2809 train_time:128946ms step_avg:61.11ms
peak memory allocated: 29862 MiB reserved: 44596 MiB

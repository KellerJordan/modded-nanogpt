import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens
            max_seq_len = new_max_seq_len
            grad_accum_steps = new_grad_accum_steps


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_batch_size: int = 2048 * 16 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2185  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 5  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=max(args.train_batch_size, args.val_batch_size) // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.03, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: flat, then linear decay, then flat
def get_lr(step: int):
    x = min(0.9999, step / args.num_scheduled_iterations)
    assert 0 <= x < 1
    lr = 1.0
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = w * 1.0 + (1 - w) * 0.1
    return lr

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 30
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
ws_long = ws_schedule[0]
for step in range(warmup_steps):
    inputs, targets, cum_seqlens = next(train_loader)
    # each window size is a new graph, need to warm up each with Yarn.attn_scale
    ws_idx = step % len(ws_schedule)
    if ws_idx==0:
        model.yarn.reset()
        ws_long = ws_schedule[0]
    else:
        new_ws_long = ws_schedule[ws_idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
    model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.yarn.reset() # rotary buffer is not stored in state_dict
model.load_state_dict(initial_state["model"])
optimizer2.reset() # muon momentum buffers not in state dict
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, args.train_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = next(train_loader)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec  5 21:29:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   35C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   35C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   30C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   37C    P0            124W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   29C    P0            114W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          189893      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    0   N/A  N/A          189894      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          189895      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          189896      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          189897      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          189898      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          189899      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          189900      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    1   N/A  N/A          189894      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    2   N/A  N/A          189895      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    3   N/A  N/A          189896      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    4   N/A  N/A          189897      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    5   N/A  N/A          189898      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    6   N/A  N/A          189899      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    7   N/A  N/A          189900      C   /home/ubuntu/.venv/bin/python3         1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2225 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2225 train_time:129ms step_avg:129.20ms
step:2/2225 train_time:184ms step_avg:91.93ms
step:3/2225 train_time:211ms step_avg:70.22ms
step:4/2225 train_time:245ms step_avg:61.21ms
step:5/2225 train_time:303ms step_avg:60.69ms
step:5/2225 val_loss:7.6958 train_time:361ms step_avg:72.22ms
step:6/2225 train_time:386ms step_avg:64.40ms
step:7/2225 train_time:423ms step_avg:60.45ms
step:8/2225 train_time:482ms step_avg:60.30ms
step:9/2225 train_time:551ms step_avg:61.19ms
step:10/2225 train_time:613ms step_avg:61.26ms
step:10/2225 val_loss:6.8112 train_time:674ms step_avg:67.44ms
step:11/2225 train_time:698ms step_avg:63.44ms
step:12/2225 train_time:735ms step_avg:61.28ms
step:13/2225 train_time:802ms step_avg:61.69ms
step:14/2225 train_time:867ms step_avg:61.91ms
step:15/2225 train_time:928ms step_avg:61.89ms
step:15/2225 val_loss:6.3803 train_time:987ms step_avg:65.81ms
step:16/2225 train_time:1013ms step_avg:63.30ms
step:17/2225 train_time:1049ms step_avg:61.71ms
step:18/2225 train_time:1109ms step_avg:61.62ms
step:19/2225 train_time:1175ms step_avg:61.84ms
step:20/2225 train_time:1236ms step_avg:61.79ms
step:20/2225 val_loss:6.1659 train_time:1297ms step_avg:64.86ms
step:21/2225 train_time:1324ms step_avg:63.05ms
step:22/2225 train_time:1358ms step_avg:61.72ms
step:23/2225 train_time:1422ms step_avg:61.84ms
step:24/2225 train_time:1485ms step_avg:61.89ms
step:25/2225 train_time:1548ms step_avg:61.93ms
step:25/2225 val_loss:5.9829 train_time:1608ms step_avg:64.32ms
step:26/2225 train_time:1637ms step_avg:62.97ms
step:27/2225 train_time:1672ms step_avg:61.94ms
step:28/2225 train_time:1733ms step_avg:61.89ms
step:29/2225 train_time:1800ms step_avg:62.08ms
step:30/2225 train_time:1862ms step_avg:62.08ms
step:30/2225 val_loss:5.7931 train_time:1924ms step_avg:64.14ms
step:31/2225 train_time:1948ms step_avg:62.85ms
step:32/2225 train_time:1986ms step_avg:62.06ms
step:33/2225 train_time:2049ms step_avg:62.10ms
step:34/2225 train_time:2112ms step_avg:62.11ms
step:35/2225 train_time:2174ms step_avg:62.10ms
step:35/2225 val_loss:5.6509 train_time:2233ms step_avg:63.81ms
step:36/2225 train_time:2259ms step_avg:62.74ms
step:37/2225 train_time:2299ms step_avg:62.12ms
step:38/2225 train_time:2361ms step_avg:62.14ms
step:39/2225 train_time:2427ms step_avg:62.23ms
step:40/2225 train_time:2487ms step_avg:62.18ms
step:40/2225 val_loss:5.5398 train_time:2548ms step_avg:63.71ms
step:41/2225 train_time:2577ms step_avg:62.84ms
step:42/2225 train_time:2611ms step_avg:62.16ms
step:43/2225 train_time:2673ms step_avg:62.16ms
step:44/2225 train_time:2735ms step_avg:62.16ms
step:45/2225 train_time:2797ms step_avg:62.15ms
step:45/2225 val_loss:5.4089 train_time:2857ms step_avg:63.48ms
step:46/2225 train_time:2885ms step_avg:62.71ms
step:47/2225 train_time:2919ms step_avg:62.11ms
step:48/2225 train_time:2981ms step_avg:62.10ms
step:49/2225 train_time:3044ms step_avg:62.13ms
step:50/2225 train_time:3105ms step_avg:62.10ms
step:50/2225 val_loss:5.3411 train_time:3167ms step_avg:63.33ms
step:51/2225 train_time:3190ms step_avg:62.55ms
step:52/2225 train_time:3227ms step_avg:62.06ms
step:53/2225 train_time:3292ms step_avg:62.12ms
step:54/2225 train_time:3357ms step_avg:62.16ms
step:55/2225 train_time:3418ms step_avg:62.15ms
step:55/2225 val_loss:5.2397 train_time:3479ms step_avg:63.26ms
step:56/2225 train_time:3504ms step_avg:62.57ms
step:57/2225 train_time:3541ms step_avg:62.13ms
step:58/2225 train_time:3604ms step_avg:62.13ms
step:59/2225 train_time:3673ms step_avg:62.26ms
step:60/2225 train_time:3733ms step_avg:62.22ms
step:60/2225 val_loss:5.1860 train_time:3794ms step_avg:63.24ms
step:61/2225 train_time:3818ms step_avg:62.60ms
step:62/2225 train_time:3855ms step_avg:62.18ms
step:63/2225 train_time:3919ms step_avg:62.21ms
step:64/2225 train_time:3981ms step_avg:62.20ms
step:65/2225 train_time:4042ms step_avg:62.19ms
step:65/2225 val_loss:5.1041 train_time:4101ms step_avg:63.09ms
step:66/2225 train_time:4126ms step_avg:62.52ms
step:67/2225 train_time:4164ms step_avg:62.15ms
step:68/2225 train_time:4227ms step_avg:62.16ms
step:69/2225 train_time:4294ms step_avg:62.24ms
step:70/2225 train_time:4357ms step_avg:62.24ms
step:70/2225 val_loss:5.0613 train_time:4418ms step_avg:63.11ms
step:71/2225 train_time:4442ms step_avg:62.56ms
step:72/2225 train_time:4480ms step_avg:62.22ms
step:73/2225 train_time:4543ms step_avg:62.23ms
step:74/2225 train_time:4605ms step_avg:62.23ms
step:75/2225 train_time:4667ms step_avg:62.23ms
step:75/2225 val_loss:4.9825 train_time:4726ms step_avg:63.02ms
step:76/2225 train_time:4752ms step_avg:62.52ms
step:77/2225 train_time:4789ms step_avg:62.19ms
step:78/2225 train_time:4852ms step_avg:62.20ms
step:79/2225 train_time:4917ms step_avg:62.24ms
step:80/2225 train_time:4977ms step_avg:62.21ms
step:80/2225 val_loss:4.9241 train_time:5038ms step_avg:62.98ms
step:81/2225 train_time:5064ms step_avg:62.51ms
step:82/2225 train_time:5102ms step_avg:62.22ms
step:83/2225 train_time:5165ms step_avg:62.23ms
step:84/2225 train_time:5226ms step_avg:62.22ms
step:85/2225 train_time:5288ms step_avg:62.21ms
step:85/2225 val_loss:4.8685 train_time:5347ms step_avg:62.90ms
step:86/2225 train_time:5374ms step_avg:62.49ms
step:87/2225 train_time:5412ms step_avg:62.20ms
step:88/2225 train_time:5473ms step_avg:62.19ms
step:89/2225 train_time:5538ms step_avg:62.22ms
step:90/2225 train_time:5599ms step_avg:62.21ms
step:90/2225 val_loss:4.8215 train_time:5660ms step_avg:62.89ms
step:91/2225 train_time:5684ms step_avg:62.46ms
step:92/2225 train_time:5720ms step_avg:62.18ms
step:93/2225 train_time:5782ms step_avg:62.17ms
step:94/2225 train_time:5846ms step_avg:62.19ms
step:95/2225 train_time:5909ms step_avg:62.20ms
step:95/2225 val_loss:4.7546 train_time:5969ms step_avg:62.83ms
step:96/2225 train_time:5997ms step_avg:62.46ms
step:97/2225 train_time:6031ms step_avg:62.18ms
step:98/2225 train_time:6092ms step_avg:62.16ms
step:99/2225 train_time:6158ms step_avg:62.20ms
step:100/2225 train_time:6219ms step_avg:62.19ms
step:100/2225 val_loss:4.7051 train_time:6280ms step_avg:62.80ms
step:101/2225 train_time:6304ms step_avg:62.41ms
step:102/2225 train_time:6341ms step_avg:62.17ms
step:103/2225 train_time:6406ms step_avg:62.19ms
step:104/2225 train_time:6469ms step_avg:62.21ms
step:105/2225 train_time:6532ms step_avg:62.21ms
step:105/2225 val_loss:4.6488 train_time:6591ms step_avg:62.77ms
step:106/2225 train_time:6616ms step_avg:62.42ms
step:107/2225 train_time:6654ms step_avg:62.19ms
step:108/2225 train_time:6715ms step_avg:62.17ms
step:109/2225 train_time:6780ms step_avg:62.20ms
step:110/2225 train_time:6843ms step_avg:62.21ms
step:110/2225 val_loss:4.6100 train_time:6904ms step_avg:62.77ms
step:111/2225 train_time:6928ms step_avg:62.41ms
step:112/2225 train_time:6966ms step_avg:62.19ms
step:113/2225 train_time:7032ms step_avg:62.23ms
step:114/2225 train_time:7098ms step_avg:62.27ms
step:115/2225 train_time:7160ms step_avg:62.26ms
step:115/2225 val_loss:4.5531 train_time:7218ms step_avg:62.77ms
step:116/2225 train_time:7243ms step_avg:62.44ms
step:117/2225 train_time:7281ms step_avg:62.23ms
step:118/2225 train_time:7344ms step_avg:62.24ms
step:119/2225 train_time:7408ms step_avg:62.25ms
step:120/2225 train_time:7467ms step_avg:62.23ms
step:120/2225 val_loss:4.5381 train_time:7529ms step_avg:62.74ms
step:121/2225 train_time:7552ms step_avg:62.41ms
step:122/2225 train_time:7590ms step_avg:62.21ms
step:123/2225 train_time:7654ms step_avg:62.23ms
step:124/2225 train_time:7719ms step_avg:62.25ms
step:125/2225 train_time:7781ms step_avg:62.25ms
step:125/2225 val_loss:4.4772 train_time:7840ms step_avg:62.72ms
step:126/2225 train_time:7865ms step_avg:62.42ms
step:127/2225 train_time:7905ms step_avg:62.24ms
step:128/2225 train_time:7966ms step_avg:62.23ms
step:129/2225 train_time:8032ms step_avg:62.26ms
step:130/2225 train_time:8092ms step_avg:62.24ms
step:130/2225 val_loss:4.4574 train_time:8153ms step_avg:62.71ms
step:131/2225 train_time:8176ms step_avg:62.41ms
step:132/2225 train_time:8215ms step_avg:62.23ms
step:133/2225 train_time:8278ms step_avg:62.24ms
step:134/2225 train_time:8342ms step_avg:62.25ms
step:135/2225 train_time:8403ms step_avg:62.25ms
step:135/2225 val_loss:4.4134 train_time:8463ms step_avg:62.69ms
step:136/2225 train_time:8488ms step_avg:62.41ms
step:137/2225 train_time:8525ms step_avg:62.23ms
step:138/2225 train_time:8585ms step_avg:62.21ms
step:139/2225 train_time:8649ms step_avg:62.23ms
step:140/2225 train_time:8709ms step_avg:62.20ms
step:140/2225 val_loss:4.3989 train_time:8770ms step_avg:62.64ms
step:141/2225 train_time:8793ms step_avg:62.36ms
step:142/2225 train_time:8830ms step_avg:62.18ms
step:143/2225 train_time:8893ms step_avg:62.19ms
step:144/2225 train_time:8956ms step_avg:62.19ms
step:145/2225 train_time:9019ms step_avg:62.20ms
step:145/2225 val_loss:4.3663 train_time:9078ms step_avg:62.61ms
step:146/2225 train_time:9102ms step_avg:62.34ms
step:147/2225 train_time:9140ms step_avg:62.17ms
step:148/2225 train_time:9200ms step_avg:62.17ms
step:149/2225 train_time:9266ms step_avg:62.19ms
step:150/2225 train_time:9327ms step_avg:62.18ms
step:150/2225 val_loss:4.3583 train_time:9389ms step_avg:62.59ms
step:151/2225 train_time:9413ms step_avg:62.34ms
step:152/2225 train_time:9451ms step_avg:62.18ms
step:153/2225 train_time:9513ms step_avg:62.18ms
step:154/2225 train_time:9576ms step_avg:62.18ms
step:155/2225 train_time:9637ms step_avg:62.17ms
step:155/2225 val_loss:4.3248 train_time:9695ms step_avg:62.55ms
step:156/2225 train_time:9720ms step_avg:62.31ms
step:157/2225 train_time:9759ms step_avg:62.16ms
step:158/2225 train_time:9819ms step_avg:62.14ms
step:159/2225 train_time:9882ms step_avg:62.15ms
step:160/2225 train_time:9944ms step_avg:62.15ms
step:160/2225 val_loss:4.3028 train_time:10005ms step_avg:62.53ms
step:161/2225 train_time:10029ms step_avg:62.29ms
step:162/2225 train_time:10066ms step_avg:62.14ms
step:163/2225 train_time:10129ms step_avg:62.14ms
step:164/2225 train_time:10191ms step_avg:62.14ms
step:165/2225 train_time:10253ms step_avg:62.14ms
step:165/2225 val_loss:4.2864 train_time:10312ms step_avg:62.50ms
step:166/2225 train_time:10340ms step_avg:62.29ms
step:167/2225 train_time:10375ms step_avg:62.12ms
step:168/2225 train_time:10437ms step_avg:62.13ms
step:169/2225 train_time:10502ms step_avg:62.14ms
step:170/2225 train_time:10561ms step_avg:62.13ms
step:170/2225 val_loss:4.2739 train_time:10622ms step_avg:62.49ms
step:171/2225 train_time:10646ms step_avg:62.26ms
step:172/2225 train_time:10682ms step_avg:62.11ms
step:173/2225 train_time:10745ms step_avg:62.11ms
step:174/2225 train_time:10812ms step_avg:62.14ms
step:175/2225 train_time:10873ms step_avg:62.13ms
step:175/2225 val_loss:4.2523 train_time:10932ms step_avg:62.47ms
step:176/2225 train_time:10957ms step_avg:62.26ms
step:177/2225 train_time:10995ms step_avg:62.12ms
step:178/2225 train_time:11057ms step_avg:62.12ms
step:179/2225 train_time:11122ms step_avg:62.13ms
step:180/2225 train_time:11181ms step_avg:62.12ms
step:180/2225 val_loss:4.2460 train_time:11242ms step_avg:62.46ms
step:181/2225 train_time:11265ms step_avg:62.24ms
step:182/2225 train_time:11302ms step_avg:62.10ms
step:183/2225 train_time:11366ms step_avg:62.11ms
step:184/2225 train_time:11432ms step_avg:62.13ms
step:185/2225 train_time:11494ms step_avg:62.13ms
step:185/2225 val_loss:4.2265 train_time:11552ms step_avg:62.44ms
step:186/2225 train_time:11577ms step_avg:62.24ms
step:187/2225 train_time:11617ms step_avg:62.12ms
step:188/2225 train_time:11677ms step_avg:62.11ms
step:189/2225 train_time:11742ms step_avg:62.13ms
step:190/2225 train_time:11803ms step_avg:62.12ms
step:190/2225 val_loss:4.2664 train_time:11864ms step_avg:62.44ms
step:191/2225 train_time:11887ms step_avg:62.24ms
step:192/2225 train_time:11924ms step_avg:62.10ms
step:193/2225 train_time:11988ms step_avg:62.11ms
step:194/2225 train_time:12052ms step_avg:62.12ms
step:195/2225 train_time:12114ms step_avg:62.12ms
step:195/2225 val_loss:4.2035 train_time:12173ms step_avg:62.42ms
step:196/2225 train_time:12198ms step_avg:62.23ms
step:197/2225 train_time:12236ms step_avg:62.11ms
step:198/2225 train_time:12298ms step_avg:62.11ms
step:199/2225 train_time:12362ms step_avg:62.12ms
step:200/2225 train_time:12422ms step_avg:62.11ms
step:200/2225 val_loss:4.1848 train_time:12483ms step_avg:62.41ms
step:201/2225 train_time:12506ms step_avg:62.22ms
step:202/2225 train_time:12543ms step_avg:62.09ms
step:203/2225 train_time:12608ms step_avg:62.11ms
step:204/2225 train_time:12669ms step_avg:62.11ms
step:205/2225 train_time:12731ms step_avg:62.10ms
step:205/2225 val_loss:4.1747 train_time:12790ms step_avg:62.39ms
step:206/2225 train_time:12815ms step_avg:62.21ms
step:207/2225 train_time:12855ms step_avg:62.10ms
step:208/2225 train_time:12915ms step_avg:62.09ms
step:209/2225 train_time:12979ms step_avg:62.10ms
step:210/2225 train_time:13038ms step_avg:62.09ms
step:210/2225 val_loss:4.1682 train_time:13100ms step_avg:62.38ms
step:211/2225 train_time:13123ms step_avg:62.19ms
step:212/2225 train_time:13160ms step_avg:62.08ms
step:213/2225 train_time:13223ms step_avg:62.08ms
step:214/2225 train_time:13284ms step_avg:62.08ms
step:215/2225 train_time:13345ms step_avg:62.07ms
step:215/2225 val_loss:4.1459 train_time:13404ms step_avg:62.34ms
step:216/2225 train_time:13428ms step_avg:62.17ms
step:217/2225 train_time:13468ms step_avg:62.07ms
step:218/2225 train_time:13528ms step_avg:62.06ms
step:219/2225 train_time:13591ms step_avg:62.06ms
step:220/2225 train_time:13650ms step_avg:62.05ms
step:220/2225 val_loss:4.1396 train_time:13710ms step_avg:62.32ms
step:221/2225 train_time:13733ms step_avg:62.14ms
step:222/2225 train_time:13771ms step_avg:62.03ms
step:223/2225 train_time:13834ms step_avg:62.03ms
step:224/2225 train_time:13894ms step_avg:62.03ms
step:225/2225 train_time:13955ms step_avg:62.02ms
step:225/2225 val_loss:4.1266 train_time:14014ms step_avg:62.28ms
step:226/2225 train_time:14038ms step_avg:62.12ms
step:227/2225 train_time:14076ms step_avg:62.01ms
step:228/2225 train_time:14137ms step_avg:62.01ms
step:229/2225 train_time:14202ms step_avg:62.02ms
step:230/2225 train_time:14262ms step_avg:62.01ms
step:230/2225 val_loss:4.1242 train_time:14323ms step_avg:62.27ms
step:231/2225 train_time:14346ms step_avg:62.11ms
step:232/2225 train_time:14383ms step_avg:62.00ms
step:233/2225 train_time:14448ms step_avg:62.01ms
step:234/2225 train_time:14511ms step_avg:62.01ms
step:235/2225 train_time:14572ms step_avg:62.01ms
step:235/2225 val_loss:4.1122 train_time:14631ms step_avg:62.26ms
step:236/2225 train_time:14656ms step_avg:62.10ms
step:237/2225 train_time:14694ms step_avg:62.00ms
step:238/2225 train_time:14753ms step_avg:61.99ms
step:239/2225 train_time:14816ms step_avg:61.99ms
step:240/2225 train_time:14875ms step_avg:61.98ms
step:240/2225 val_loss:4.1062 train_time:14937ms step_avg:62.24ms
step:241/2225 train_time:14960ms step_avg:62.07ms
step:242/2225 train_time:14997ms step_avg:61.97ms
step:243/2225 train_time:15063ms step_avg:61.99ms
step:244/2225 train_time:15125ms step_avg:61.99ms
step:245/2225 train_time:15186ms step_avg:61.99ms
step:245/2225 val_loss:4.0901 train_time:15245ms step_avg:62.22ms
step:246/2225 train_time:15270ms step_avg:62.07ms
step:247/2225 train_time:15308ms step_avg:61.97ms
step:248/2225 train_time:15370ms step_avg:61.97ms
step:249/2225 train_time:15436ms step_avg:61.99ms
step:250/2225 train_time:15495ms step_avg:61.98ms
step:250/2225 val_loss:4.0945 train_time:15556ms step_avg:62.22ms
step:251/2225 train_time:15579ms step_avg:62.07ms
step:252/2225 train_time:15616ms step_avg:61.97ms
step:253/2225 train_time:15677ms step_avg:61.96ms
step:254/2225 train_time:15739ms step_avg:61.96ms
step:255/2225 train_time:15801ms step_avg:61.96ms
step:255/2225 val_loss:4.0735 train_time:15859ms step_avg:62.19ms
step:256/2225 train_time:15884ms step_avg:62.05ms
step:257/2225 train_time:15922ms step_avg:61.95ms
step:258/2225 train_time:15981ms step_avg:61.94ms
step:259/2225 train_time:16046ms step_avg:61.95ms
step:260/2225 train_time:16105ms step_avg:61.94ms
step:260/2225 val_loss:4.0810 train_time:16165ms step_avg:62.17ms
step:261/2225 train_time:16189ms step_avg:62.03ms
step:262/2225 train_time:16226ms step_avg:61.93ms
step:263/2225 train_time:16290ms step_avg:61.94ms
step:264/2225 train_time:16355ms step_avg:61.95ms
step:265/2225 train_time:16416ms step_avg:61.95ms
step:265/2225 val_loss:4.0669 train_time:16474ms step_avg:62.17ms
step:266/2225 train_time:16500ms step_avg:62.03ms
step:267/2225 train_time:16537ms step_avg:61.94ms
step:268/2225 train_time:16599ms step_avg:61.94ms
step:269/2225 train_time:16664ms step_avg:61.95ms
step:270/2225 train_time:16723ms step_avg:61.94ms
step:270/2225 val_loss:4.0616 train_time:16784ms step_avg:62.16ms
step:271/2225 train_time:16807ms step_avg:62.02ms
step:272/2225 train_time:16844ms step_avg:61.93ms
step:273/2225 train_time:16907ms step_avg:61.93ms
step:274/2225 train_time:16972ms step_avg:61.94ms
step:275/2225 train_time:17034ms step_avg:61.94ms
step:275/2225 val_loss:4.0445 train_time:17092ms step_avg:62.15ms
step:276/2225 train_time:17117ms step_avg:62.02ms
step:277/2225 train_time:17156ms step_avg:61.93ms
step:278/2225 train_time:17218ms step_avg:61.93ms
step:279/2225 train_time:17285ms step_avg:61.95ms
step:280/2225 train_time:17344ms step_avg:61.94ms
step:280/2225 val_loss:4.0414 train_time:17404ms step_avg:62.16ms
step:281/2225 train_time:17429ms step_avg:62.02ms
step:282/2225 train_time:17468ms step_avg:61.94ms
step:283/2225 train_time:17529ms step_avg:61.94ms
step:284/2225 train_time:17591ms step_avg:61.94ms
step:285/2225 train_time:17652ms step_avg:61.94ms
step:285/2225 val_loss:4.0313 train_time:17711ms step_avg:62.14ms
step:286/2225 train_time:17736ms step_avg:62.01ms
step:287/2225 train_time:17774ms step_avg:61.93ms
step:288/2225 train_time:17835ms step_avg:61.93ms
step:289/2225 train_time:17901ms step_avg:61.94ms
step:290/2225 train_time:17961ms step_avg:61.94ms
step:290/2225 val_loss:4.0323 train_time:18023ms step_avg:62.15ms
step:291/2225 train_time:18046ms step_avg:62.01ms
step:292/2225 train_time:18087ms step_avg:61.94ms
step:293/2225 train_time:18149ms step_avg:61.94ms
step:294/2225 train_time:18211ms step_avg:61.94ms
step:295/2225 train_time:18272ms step_avg:61.94ms
step:295/2225 val_loss:4.0193 train_time:18331ms step_avg:62.14ms
step:296/2225 train_time:18356ms step_avg:62.01ms
step:297/2225 train_time:18394ms step_avg:61.93ms
step:298/2225 train_time:18453ms step_avg:61.92ms
step:299/2225 train_time:18515ms step_avg:61.92ms
step:300/2225 train_time:18574ms step_avg:61.91ms
step:300/2225 val_loss:4.0124 train_time:18635ms step_avg:62.12ms
step:301/2225 train_time:18658ms step_avg:61.99ms
step:302/2225 train_time:18697ms step_avg:61.91ms
step:303/2225 train_time:18760ms step_avg:61.91ms
step:304/2225 train_time:18823ms step_avg:61.92ms
step:305/2225 train_time:18884ms step_avg:61.92ms
step:305/2225 val_loss:4.0071 train_time:18942ms step_avg:62.11ms
step:306/2225 train_time:18967ms step_avg:61.98ms
step:307/2225 train_time:19006ms step_avg:61.91ms
step:308/2225 train_time:19067ms step_avg:61.91ms
step:309/2225 train_time:19131ms step_avg:61.91ms
step:310/2225 train_time:19191ms step_avg:61.91ms
step:310/2225 val_loss:4.0053 train_time:19251ms step_avg:62.10ms
step:311/2225 train_time:19275ms step_avg:61.98ms
step:312/2225 train_time:19311ms step_avg:61.89ms
step:313/2225 train_time:19376ms step_avg:61.90ms
step:314/2225 train_time:19439ms step_avg:61.91ms
step:315/2225 train_time:19500ms step_avg:61.91ms
step:315/2225 val_loss:3.9952 train_time:19558ms step_avg:62.09ms
step:316/2225 train_time:19583ms step_avg:61.97ms
step:317/2225 train_time:19621ms step_avg:61.90ms
step:318/2225 train_time:19684ms step_avg:61.90ms
step:319/2225 train_time:19748ms step_avg:61.91ms
step:320/2225 train_time:19807ms step_avg:61.90ms
step:320/2225 val_loss:3.9965 train_time:19868ms step_avg:62.09ms
step:321/2225 train_time:19891ms step_avg:61.97ms
step:322/2225 train_time:19930ms step_avg:61.89ms
step:323/2225 train_time:19993ms step_avg:61.90ms
step:324/2225 train_time:20055ms step_avg:61.90ms
step:325/2225 train_time:20116ms step_avg:61.89ms
step:325/2225 val_loss:3.9883 train_time:20175ms step_avg:62.08ms
step:326/2225 train_time:20199ms step_avg:61.96ms
step:327/2225 train_time:20238ms step_avg:61.89ms
step:328/2225 train_time:20299ms step_avg:61.89ms
step:329/2225 train_time:20364ms step_avg:61.90ms
step:330/2225 train_time:20425ms step_avg:61.89ms
step:330/2225 val_loss:3.9762 train_time:20485ms step_avg:62.08ms
step:331/2225 train_time:20510ms step_avg:61.96ms
step:332/2225 train_time:20547ms step_avg:61.89ms
step:333/2225 train_time:20611ms step_avg:61.90ms
step:334/2225 train_time:20673ms step_avg:61.90ms
step:335/2225 train_time:20734ms step_avg:61.89ms
step:335/2225 val_loss:3.9712 train_time:20792ms step_avg:62.07ms
step:336/2225 train_time:20817ms step_avg:61.96ms
step:337/2225 train_time:20856ms step_avg:61.89ms
step:338/2225 train_time:20916ms step_avg:61.88ms
step:339/2225 train_time:20981ms step_avg:61.89ms
step:340/2225 train_time:21041ms step_avg:61.88ms
step:340/2225 val_loss:3.9773 train_time:21101ms step_avg:62.06ms
step:341/2225 train_time:21125ms step_avg:61.95ms
step:342/2225 train_time:21161ms step_avg:61.88ms
step:343/2225 train_time:21226ms step_avg:61.88ms
step:344/2225 train_time:21289ms step_avg:61.89ms
step:345/2225 train_time:21350ms step_avg:61.88ms
step:345/2225 val_loss:3.9663 train_time:21409ms step_avg:62.05ms
step:346/2225 train_time:21434ms step_avg:61.95ms
step:347/2225 train_time:21471ms step_avg:61.87ms
step:348/2225 train_time:21532ms step_avg:61.87ms
step:349/2225 train_time:21598ms step_avg:61.89ms
step:350/2225 train_time:21660ms step_avg:61.89ms
step:350/2225 val_loss:3.9668 train_time:21721ms step_avg:62.06ms
step:351/2225 train_time:21745ms step_avg:61.95ms
step:352/2225 train_time:21782ms step_avg:61.88ms
step:353/2225 train_time:21845ms step_avg:61.88ms
step:354/2225 train_time:21907ms step_avg:61.88ms
step:355/2225 train_time:21968ms step_avg:61.88ms
step:355/2225 val_loss:3.9470 train_time:22027ms step_avg:62.05ms
step:356/2225 train_time:22052ms step_avg:61.94ms
step:357/2225 train_time:22091ms step_avg:61.88ms
step:358/2225 train_time:22150ms step_avg:61.87ms
step:359/2225 train_time:22213ms step_avg:61.87ms
step:360/2225 train_time:22271ms step_avg:61.87ms
step:360/2225 val_loss:3.9473 train_time:22332ms step_avg:62.03ms
step:361/2225 train_time:22355ms step_avg:61.93ms
step:362/2225 train_time:22393ms step_avg:61.86ms
step:363/2225 train_time:22457ms step_avg:61.87ms
step:364/2225 train_time:22518ms step_avg:61.86ms
step:365/2225 train_time:22579ms step_avg:61.86ms
step:365/2225 val_loss:3.9362 train_time:22638ms step_avg:62.02ms
step:366/2225 train_time:22663ms step_avg:61.92ms
step:367/2225 train_time:22701ms step_avg:61.86ms
step:368/2225 train_time:22761ms step_avg:61.85ms
step:369/2225 train_time:22826ms step_avg:61.86ms
step:370/2225 train_time:22885ms step_avg:61.85ms
step:370/2225 val_loss:3.9315 train_time:22946ms step_avg:62.02ms
step:371/2225 train_time:22969ms step_avg:61.91ms
step:372/2225 train_time:23007ms step_avg:61.85ms
step:373/2225 train_time:23069ms step_avg:61.85ms
step:374/2225 train_time:23130ms step_avg:61.84ms
step:375/2225 train_time:23191ms step_avg:61.84ms
step:375/2225 val_loss:3.9260 train_time:23249ms step_avg:62.00ms
step:376/2225 train_time:23274ms step_avg:61.90ms
step:377/2225 train_time:23312ms step_avg:61.84ms
step:378/2225 train_time:23372ms step_avg:61.83ms
step:379/2225 train_time:23434ms step_avg:61.83ms
step:380/2225 train_time:23493ms step_avg:61.82ms
step:380/2225 val_loss:3.9221 train_time:23554ms step_avg:61.98ms
step:381/2225 train_time:23578ms step_avg:61.88ms
step:382/2225 train_time:23615ms step_avg:61.82ms
step:383/2225 train_time:23678ms step_avg:61.82ms
step:384/2225 train_time:23741ms step_avg:61.83ms
step:385/2225 train_time:23804ms step_avg:61.83ms
step:385/2225 val_loss:3.9131 train_time:23863ms step_avg:61.98ms
step:386/2225 train_time:23887ms step_avg:61.88ms
step:387/2225 train_time:23926ms step_avg:61.82ms
step:388/2225 train_time:23987ms step_avg:61.82ms
step:389/2225 train_time:24052ms step_avg:61.83ms
step:390/2225 train_time:24113ms step_avg:61.83ms
step:390/2225 val_loss:3.9127 train_time:24174ms step_avg:61.99ms
step:391/2225 train_time:24198ms step_avg:61.89ms
step:392/2225 train_time:24234ms step_avg:61.82ms
step:393/2225 train_time:24296ms step_avg:61.82ms
step:394/2225 train_time:24358ms step_avg:61.82ms
step:395/2225 train_time:24420ms step_avg:61.82ms
step:395/2225 val_loss:3.9057 train_time:24479ms step_avg:61.97ms
step:396/2225 train_time:24504ms step_avg:61.88ms
step:397/2225 train_time:24542ms step_avg:61.82ms
step:398/2225 train_time:24604ms step_avg:61.82ms
step:399/2225 train_time:24667ms step_avg:61.82ms
step:400/2225 train_time:24726ms step_avg:61.82ms
step:400/2225 val_loss:3.9014 train_time:24787ms step_avg:61.97ms
step:401/2225 train_time:24810ms step_avg:61.87ms
step:402/2225 train_time:24850ms step_avg:61.82ms
step:403/2225 train_time:24912ms step_avg:61.82ms
step:404/2225 train_time:24973ms step_avg:61.81ms
step:405/2225 train_time:25034ms step_avg:61.81ms
step:405/2225 val_loss:3.8966 train_time:25093ms step_avg:61.96ms
step:406/2225 train_time:25118ms step_avg:61.87ms
step:407/2225 train_time:25158ms step_avg:61.81ms
step:408/2225 train_time:25217ms step_avg:61.81ms
step:409/2225 train_time:25283ms step_avg:61.82ms
step:410/2225 train_time:25344ms step_avg:61.81ms
step:410/2225 val_loss:3.8949 train_time:25404ms step_avg:61.96ms
step:411/2225 train_time:25428ms step_avg:61.87ms
step:412/2225 train_time:25469ms step_avg:61.82ms
step:413/2225 train_time:25530ms step_avg:61.82ms
step:414/2225 train_time:25592ms step_avg:61.82ms
step:415/2225 train_time:25653ms step_avg:61.82ms
step:415/2225 val_loss:3.8884 train_time:25712ms step_avg:61.96ms
step:416/2225 train_time:25737ms step_avg:61.87ms
step:417/2225 train_time:25775ms step_avg:61.81ms
step:418/2225 train_time:25836ms step_avg:61.81ms
step:419/2225 train_time:25900ms step_avg:61.81ms
step:420/2225 train_time:25960ms step_avg:61.81ms
step:420/2225 val_loss:3.9086 train_time:26021ms step_avg:61.95ms
step:421/2225 train_time:26044ms step_avg:61.86ms
step:422/2225 train_time:26081ms step_avg:61.80ms
step:423/2225 train_time:26144ms step_avg:61.81ms
step:424/2225 train_time:26210ms step_avg:61.82ms
step:425/2225 train_time:26272ms step_avg:61.82ms
step:425/2225 val_loss:3.8833 train_time:26331ms step_avg:61.96ms
step:426/2225 train_time:26356ms step_avg:61.87ms
step:427/2225 train_time:26393ms step_avg:61.81ms
step:428/2225 train_time:26455ms step_avg:61.81ms
step:429/2225 train_time:26519ms step_avg:61.82ms
step:430/2225 train_time:26580ms step_avg:61.81ms
step:430/2225 val_loss:3.8786 train_time:26641ms step_avg:61.96ms
step:431/2225 train_time:26664ms step_avg:61.87ms
step:432/2225 train_time:26703ms step_avg:61.81ms
step:433/2225 train_time:26766ms step_avg:61.81ms
step:434/2225 train_time:26829ms step_avg:61.82ms
step:435/2225 train_time:26891ms step_avg:61.82ms
step:435/2225 val_loss:3.8724 train_time:26949ms step_avg:61.95ms
step:436/2225 train_time:26974ms step_avg:61.87ms
step:437/2225 train_time:27012ms step_avg:61.81ms
step:438/2225 train_time:27073ms step_avg:61.81ms
step:439/2225 train_time:27138ms step_avg:61.82ms
step:440/2225 train_time:27200ms step_avg:61.82ms
step:440/2225 val_loss:3.8667 train_time:27260ms step_avg:61.95ms
step:441/2225 train_time:27283ms step_avg:61.87ms
step:442/2225 train_time:27320ms step_avg:61.81ms
step:443/2225 train_time:27383ms step_avg:61.81ms
step:444/2225 train_time:27448ms step_avg:61.82ms
step:445/2225 train_time:27512ms step_avg:61.82ms
step:445/2225 val_loss:3.8621 train_time:27570ms step_avg:61.96ms
step:446/2225 train_time:27595ms step_avg:61.87ms
step:447/2225 train_time:27636ms step_avg:61.82ms
step:448/2225 train_time:27697ms step_avg:61.82ms
step:449/2225 train_time:27760ms step_avg:61.83ms
step:450/2225 train_time:27820ms step_avg:61.82ms
step:450/2225 val_loss:3.8560 train_time:27880ms step_avg:61.96ms
step:451/2225 train_time:27903ms step_avg:61.87ms
step:452/2225 train_time:27940ms step_avg:61.81ms
step:453/2225 train_time:28008ms step_avg:61.83ms
step:454/2225 train_time:28074ms step_avg:61.84ms
step:455/2225 train_time:28136ms step_avg:61.84ms
step:455/2225 val_loss:3.8550 train_time:28194ms step_avg:61.96ms
step:456/2225 train_time:28219ms step_avg:61.88ms
step:457/2225 train_time:28257ms step_avg:61.83ms
step:458/2225 train_time:28318ms step_avg:61.83ms
step:459/2225 train_time:28383ms step_avg:61.84ms
step:460/2225 train_time:28444ms step_avg:61.83ms
step:460/2225 val_loss:3.8519 train_time:28504ms step_avg:61.96ms
step:461/2225 train_time:28529ms step_avg:61.89ms
step:462/2225 train_time:28565ms step_avg:61.83ms
step:463/2225 train_time:28627ms step_avg:61.83ms
step:464/2225 train_time:28688ms step_avg:61.83ms
step:465/2225 train_time:28750ms step_avg:61.83ms
step:465/2225 val_loss:3.8465 train_time:28808ms step_avg:61.95ms
step:466/2225 train_time:28833ms step_avg:61.87ms
step:467/2225 train_time:28873ms step_avg:61.83ms
step:468/2225 train_time:28933ms step_avg:61.82ms
step:469/2225 train_time:28996ms step_avg:61.83ms
step:470/2225 train_time:29055ms step_avg:61.82ms
step:470/2225 val_loss:3.8419 train_time:29115ms step_avg:61.95ms
step:471/2225 train_time:29138ms step_avg:61.87ms
step:472/2225 train_time:29177ms step_avg:61.82ms
step:473/2225 train_time:29240ms step_avg:61.82ms
step:474/2225 train_time:29306ms step_avg:61.83ms
step:475/2225 train_time:29368ms step_avg:61.83ms
step:475/2225 val_loss:3.8405 train_time:29426ms step_avg:61.95ms
step:476/2225 train_time:29453ms step_avg:61.88ms
step:477/2225 train_time:29489ms step_avg:61.82ms
step:478/2225 train_time:29552ms step_avg:61.82ms
step:479/2225 train_time:29620ms step_avg:61.84ms
step:480/2225 train_time:29680ms step_avg:61.83ms
step:480/2225 val_loss:3.8366 train_time:29741ms step_avg:61.96ms
step:481/2225 train_time:29764ms step_avg:61.88ms
step:482/2225 train_time:29802ms step_avg:61.83ms
step:483/2225 train_time:29865ms step_avg:61.83ms
step:484/2225 train_time:29928ms step_avg:61.83ms
step:485/2225 train_time:29991ms step_avg:61.84ms
step:485/2225 val_loss:3.8321 train_time:30049ms step_avg:61.96ms
step:486/2225 train_time:30074ms step_avg:61.88ms
step:487/2225 train_time:30111ms step_avg:61.83ms
step:488/2225 train_time:30173ms step_avg:61.83ms
step:489/2225 train_time:30237ms step_avg:61.84ms
step:490/2225 train_time:30296ms step_avg:61.83ms
step:490/2225 val_loss:3.8302 train_time:30356ms step_avg:61.95ms
step:491/2225 train_time:30380ms step_avg:61.87ms
step:492/2225 train_time:30417ms step_avg:61.82ms
step:493/2225 train_time:30480ms step_avg:61.82ms
step:494/2225 train_time:30542ms step_avg:61.83ms
step:495/2225 train_time:30604ms step_avg:61.83ms
step:495/2225 val_loss:3.8242 train_time:30663ms step_avg:61.95ms
step:496/2225 train_time:30688ms step_avg:61.87ms
step:497/2225 train_time:30726ms step_avg:61.82ms
step:498/2225 train_time:30787ms step_avg:61.82ms
step:499/2225 train_time:30854ms step_avg:61.83ms
step:500/2225 train_time:30915ms step_avg:61.83ms
step:500/2225 val_loss:3.8236 train_time:30976ms step_avg:61.95ms
step:501/2225 train_time:30999ms step_avg:61.87ms
step:502/2225 train_time:31035ms step_avg:61.82ms
step:503/2225 train_time:31097ms step_avg:61.82ms
step:504/2225 train_time:31159ms step_avg:61.82ms
step:505/2225 train_time:31222ms step_avg:61.83ms
step:505/2225 val_loss:3.8165 train_time:31281ms step_avg:61.94ms
step:506/2225 train_time:31306ms step_avg:61.87ms
step:507/2225 train_time:31344ms step_avg:61.82ms
step:508/2225 train_time:31405ms step_avg:61.82ms
step:509/2225 train_time:31468ms step_avg:61.82ms
step:510/2225 train_time:31529ms step_avg:61.82ms
step:510/2225 val_loss:3.8156 train_time:31590ms step_avg:61.94ms
step:511/2225 train_time:31613ms step_avg:61.87ms
step:512/2225 train_time:31652ms step_avg:61.82ms
step:513/2225 train_time:31713ms step_avg:61.82ms
step:514/2225 train_time:31777ms step_avg:61.82ms
step:515/2225 train_time:31839ms step_avg:61.82ms
step:515/2225 val_loss:3.8090 train_time:31897ms step_avg:61.94ms
step:516/2225 train_time:31922ms step_avg:61.86ms
step:517/2225 train_time:31961ms step_avg:61.82ms
step:518/2225 train_time:32022ms step_avg:61.82ms
step:519/2225 train_time:32087ms step_avg:61.82ms
step:520/2225 train_time:32148ms step_avg:61.82ms
step:520/2225 val_loss:3.8115 train_time:32209ms step_avg:61.94ms
step:521/2225 train_time:32232ms step_avg:61.87ms
step:522/2225 train_time:32270ms step_avg:61.82ms
step:523/2225 train_time:32331ms step_avg:61.82ms
step:524/2225 train_time:32394ms step_avg:61.82ms
step:525/2225 train_time:32457ms step_avg:61.82ms
step:525/2225 val_loss:3.8068 train_time:32517ms step_avg:61.94ms
step:526/2225 train_time:32541ms step_avg:61.87ms
step:527/2225 train_time:32580ms step_avg:61.82ms
step:528/2225 train_time:32641ms step_avg:61.82ms
step:529/2225 train_time:32704ms step_avg:61.82ms
step:530/2225 train_time:32765ms step_avg:61.82ms
step:530/2225 val_loss:3.8266 train_time:32826ms step_avg:61.94ms
step:531/2225 train_time:32849ms step_avg:61.86ms
step:532/2225 train_time:32886ms step_avg:61.82ms
step:533/2225 train_time:32949ms step_avg:61.82ms
step:534/2225 train_time:33010ms step_avg:61.82ms
step:535/2225 train_time:33073ms step_avg:61.82ms
step:535/2225 val_loss:3.8022 train_time:33131ms step_avg:61.93ms
step:536/2225 train_time:33159ms step_avg:61.86ms
step:537/2225 train_time:33194ms step_avg:61.81ms
step:538/2225 train_time:33256ms step_avg:61.81ms
step:539/2225 train_time:33320ms step_avg:61.82ms
step:540/2225 train_time:33379ms step_avg:61.81ms
step:540/2225 val_loss:3.7999 train_time:33440ms step_avg:61.93ms
step:541/2225 train_time:33463ms step_avg:61.85ms
step:542/2225 train_time:33502ms step_avg:61.81ms
step:543/2225 train_time:33564ms step_avg:61.81ms
step:544/2225 train_time:33626ms step_avg:61.81ms
step:545/2225 train_time:33686ms step_avg:61.81ms
step:545/2225 val_loss:3.7938 train_time:33745ms step_avg:61.92ms
step:546/2225 train_time:33769ms step_avg:61.85ms
step:547/2225 train_time:33808ms step_avg:61.81ms
step:548/2225 train_time:33869ms step_avg:61.81ms
step:549/2225 train_time:33936ms step_avg:61.81ms
step:550/2225 train_time:33995ms step_avg:61.81ms
step:550/2225 val_loss:3.7924 train_time:34056ms step_avg:61.92ms
step:551/2225 train_time:34079ms step_avg:61.85ms
step:552/2225 train_time:34116ms step_avg:61.80ms
step:553/2225 train_time:34178ms step_avg:61.80ms
step:554/2225 train_time:34239ms step_avg:61.80ms
step:555/2225 train_time:34300ms step_avg:61.80ms
step:555/2225 val_loss:3.7877 train_time:34359ms step_avg:61.91ms
step:556/2225 train_time:34383ms step_avg:61.84ms
step:557/2225 train_time:34423ms step_avg:61.80ms
step:558/2225 train_time:34484ms step_avg:61.80ms
step:559/2225 train_time:34547ms step_avg:61.80ms
step:560/2225 train_time:34605ms step_avg:61.80ms
step:560/2225 val_loss:3.7873 train_time:34667ms step_avg:61.90ms
step:561/2225 train_time:34690ms step_avg:61.84ms
step:562/2225 train_time:34729ms step_avg:61.80ms
step:563/2225 train_time:34791ms step_avg:61.79ms
step:564/2225 train_time:34853ms step_avg:61.80ms
step:565/2225 train_time:34916ms step_avg:61.80ms
step:565/2225 val_loss:3.7837 train_time:34974ms step_avg:61.90ms
step:566/2225 train_time:34999ms step_avg:61.84ms
step:567/2225 train_time:35037ms step_avg:61.79ms
step:568/2225 train_time:35098ms step_avg:61.79ms
step:569/2225 train_time:35163ms step_avg:61.80ms
step:570/2225 train_time:35223ms step_avg:61.79ms
step:570/2225 val_loss:3.7784 train_time:35283ms step_avg:61.90ms
step:571/2225 train_time:35307ms step_avg:61.83ms
step:572/2225 train_time:35344ms step_avg:61.79ms
step:573/2225 train_time:35406ms step_avg:61.79ms
step:574/2225 train_time:35470ms step_avg:61.79ms
step:575/2225 train_time:35532ms step_avg:61.80ms
step:575/2225 val_loss:3.7749 train_time:35591ms step_avg:61.90ms
step:576/2225 train_time:35616ms step_avg:61.83ms
step:577/2225 train_time:35657ms step_avg:61.80ms
step:578/2225 train_time:35718ms step_avg:61.80ms
step:579/2225 train_time:35780ms step_avg:61.80ms
step:580/2225 train_time:35840ms step_avg:61.79ms
step:580/2225 val_loss:3.7756 train_time:35901ms step_avg:61.90ms
step:581/2225 train_time:35924ms step_avg:61.83ms
step:582/2225 train_time:35962ms step_avg:61.79ms
step:583/2225 train_time:36025ms step_avg:61.79ms
step:584/2225 train_time:36088ms step_avg:61.79ms
step:585/2225 train_time:36151ms step_avg:61.80ms
step:585/2225 val_loss:3.7709 train_time:36209ms step_avg:61.90ms
step:586/2225 train_time:36234ms step_avg:61.83ms
step:587/2225 train_time:36272ms step_avg:61.79ms
step:588/2225 train_time:36333ms step_avg:61.79ms
step:589/2225 train_time:36397ms step_avg:61.79ms
step:590/2225 train_time:36456ms step_avg:61.79ms
step:590/2225 val_loss:3.7665 train_time:36516ms step_avg:61.89ms
step:591/2225 train_time:36540ms step_avg:61.83ms
step:592/2225 train_time:36579ms step_avg:61.79ms
step:593/2225 train_time:36639ms step_avg:61.79ms
step:594/2225 train_time:36702ms step_avg:61.79ms
step:595/2225 train_time:36765ms step_avg:61.79ms
step:595/2225 val_loss:3.7647 train_time:36824ms step_avg:61.89ms
step:596/2225 train_time:36849ms step_avg:61.83ms
step:597/2225 train_time:36886ms step_avg:61.78ms
step:598/2225 train_time:36947ms step_avg:61.78ms
step:599/2225 train_time:37011ms step_avg:61.79ms
step:600/2225 train_time:37071ms step_avg:61.79ms
step:600/2225 val_loss:3.7647 train_time:37132ms step_avg:61.89ms
step:601/2225 train_time:37155ms step_avg:61.82ms
step:602/2225 train_time:37195ms step_avg:61.79ms
step:603/2225 train_time:37259ms step_avg:61.79ms
step:604/2225 train_time:37323ms step_avg:61.79ms
step:605/2225 train_time:37385ms step_avg:61.79ms
step:605/2225 val_loss:3.7588 train_time:37444ms step_avg:61.89ms
step:606/2225 train_time:37468ms step_avg:61.83ms
step:607/2225 train_time:37508ms step_avg:61.79ms
step:608/2225 train_time:37571ms step_avg:61.79ms
step:609/2225 train_time:37637ms step_avg:61.80ms
step:610/2225 train_time:37698ms step_avg:61.80ms
step:610/2225 val_loss:3.7634 train_time:37758ms step_avg:61.90ms
step:611/2225 train_time:37782ms step_avg:61.84ms
step:612/2225 train_time:37819ms step_avg:61.80ms
step:613/2225 train_time:37883ms step_avg:61.80ms
step:614/2225 train_time:37947ms step_avg:61.80ms
step:615/2225 train_time:38008ms step_avg:61.80ms
step:615/2225 val_loss:3.7551 train_time:38066ms step_avg:61.90ms
step:616/2225 train_time:38091ms step_avg:61.84ms
step:617/2225 train_time:38128ms step_avg:61.80ms
step:618/2225 train_time:38190ms step_avg:61.80ms
step:619/2225 train_time:38256ms step_avg:61.80ms
step:620/2225 train_time:38316ms step_avg:61.80ms
step:620/2225 val_loss:3.7547 train_time:38376ms step_avg:61.90ms
step:621/2225 train_time:38399ms step_avg:61.83ms
step:622/2225 train_time:38438ms step_avg:61.80ms
step:623/2225 train_time:38501ms step_avg:61.80ms
step:624/2225 train_time:38563ms step_avg:61.80ms
step:625/2225 train_time:38624ms step_avg:61.80ms
step:625/2225 val_loss:3.7499 train_time:38683ms step_avg:61.89ms
step:626/2225 train_time:38708ms step_avg:61.83ms
step:627/2225 train_time:38748ms step_avg:61.80ms
step:628/2225 train_time:38808ms step_avg:61.80ms
step:629/2225 train_time:38874ms step_avg:61.80ms
step:630/2225 train_time:38935ms step_avg:61.80ms
step:630/2225 val_loss:3.7481 train_time:38995ms step_avg:61.90ms
step:631/2225 train_time:39018ms step_avg:61.84ms
step:632/2225 train_time:39056ms step_avg:61.80ms
step:633/2225 train_time:39120ms step_avg:61.80ms
step:634/2225 train_time:39183ms step_avg:61.80ms
step:635/2225 train_time:39244ms step_avg:61.80ms
step:635/2225 val_loss:3.7451 train_time:39302ms step_avg:61.89ms
step:636/2225 train_time:39327ms step_avg:61.84ms
step:637/2225 train_time:39366ms step_avg:61.80ms
step:638/2225 train_time:39427ms step_avg:61.80ms
step:639/2225 train_time:39491ms step_avg:61.80ms
step:640/2225 train_time:39551ms step_avg:61.80ms
step:640/2225 val_loss:3.7468 train_time:39611ms step_avg:61.89ms
step:641/2225 train_time:39635ms step_avg:61.83ms
step:642/2225 train_time:39672ms step_avg:61.79ms
step:643/2225 train_time:39735ms step_avg:61.80ms
step:644/2225 train_time:39798ms step_avg:61.80ms
step:645/2225 train_time:39861ms step_avg:61.80ms
step:645/2225 val_loss:3.7427 train_time:39921ms step_avg:61.89ms
step:646/2225 train_time:39946ms step_avg:61.84ms
step:647/2225 train_time:39986ms step_avg:61.80ms
step:648/2225 train_time:40047ms step_avg:61.80ms
step:649/2225 train_time:40113ms step_avg:61.81ms
step:650/2225 train_time:40172ms step_avg:61.80ms
step:650/2225 val_loss:3.7396 train_time:40233ms step_avg:61.90ms
step:651/2225 train_time:40256ms step_avg:61.84ms
step:652/2225 train_time:40294ms step_avg:61.80ms
step:653/2225 train_time:40357ms step_avg:61.80ms
step:654/2225 train_time:40418ms step_avg:61.80ms
step:655/2225 train_time:40482ms step_avg:61.81ms
step:655/2225 val_loss:3.7358 train_time:40543ms step_avg:61.90ms
step:656/2225 train_time:40568ms step_avg:61.84ms
step:657/2225 train_time:40606ms step_avg:61.81ms
step:658/2225 train_time:40667ms step_avg:61.80ms
step:659/2225 train_time:40731ms step_avg:61.81ms
step:660/2225 train_time:40791ms step_avg:61.80ms
step:660/2225 val_loss:3.7373 train_time:40851ms step_avg:61.90ms
step:661/2225 train_time:40876ms step_avg:61.84ms
step:662/2225 train_time:40914ms step_avg:61.80ms
step:663/2225 train_time:40977ms step_avg:61.80ms
step:664/2225 train_time:41039ms step_avg:61.81ms
step:665/2225 train_time:41100ms step_avg:61.80ms
step:665/2225 val_loss:3.7317 train_time:41159ms step_avg:61.89ms
step:666/2225 train_time:41184ms step_avg:61.84ms
step:667/2225 train_time:41222ms step_avg:61.80ms
step:668/2225 train_time:41285ms step_avg:61.80ms
step:669/2225 train_time:41348ms step_avg:61.81ms
step:670/2225 train_time:41408ms step_avg:61.80ms
step:670/2225 val_loss:3.7290 train_time:41469ms step_avg:61.89ms
step:671/2225 train_time:41492ms step_avg:61.84ms
step:672/2225 train_time:41529ms step_avg:61.80ms
step:673/2225 train_time:41592ms step_avg:61.80ms
step:674/2225 train_time:41654ms step_avg:61.80ms
step:675/2225 train_time:41714ms step_avg:61.80ms
step:675/2225 val_loss:3.7275 train_time:41773ms step_avg:61.89ms
step:676/2225 train_time:41801ms step_avg:61.84ms
step:677/2225 train_time:41835ms step_avg:61.79ms
step:678/2225 train_time:41896ms step_avg:61.79ms
step:679/2225 train_time:41960ms step_avg:61.80ms
step:680/2225 train_time:42019ms step_avg:61.79ms
step:680/2225 val_loss:3.7288 train_time:42079ms step_avg:61.88ms
step:681/2225 train_time:42103ms step_avg:61.83ms
step:682/2225 train_time:42141ms step_avg:61.79ms
step:683/2225 train_time:42204ms step_avg:61.79ms
step:684/2225 train_time:42267ms step_avg:61.79ms
step:685/2225 train_time:42328ms step_avg:61.79ms
step:685/2225 val_loss:3.7235 train_time:42386ms step_avg:61.88ms
step:686/2225 train_time:42411ms step_avg:61.82ms
step:687/2225 train_time:42448ms step_avg:61.79ms
step:688/2225 train_time:42511ms step_avg:61.79ms
step:689/2225 train_time:42576ms step_avg:61.79ms
step:690/2225 train_time:42636ms step_avg:61.79ms
step:690/2225 val_loss:3.7282 train_time:42697ms step_avg:61.88ms
step:691/2225 train_time:42720ms step_avg:61.82ms
step:692/2225 train_time:42758ms step_avg:61.79ms
step:693/2225 train_time:42821ms step_avg:61.79ms
step:694/2225 train_time:42882ms step_avg:61.79ms
step:695/2225 train_time:42943ms step_avg:61.79ms
step:695/2225 val_loss:3.7178 train_time:43002ms step_avg:61.87ms
step:696/2225 train_time:43027ms step_avg:61.82ms
step:697/2225 train_time:43064ms step_avg:61.78ms
step:698/2225 train_time:43123ms step_avg:61.78ms
step:699/2225 train_time:43189ms step_avg:61.79ms
step:700/2225 train_time:43249ms step_avg:61.78ms
step:700/2225 val_loss:3.7197 train_time:43310ms step_avg:61.87ms
step:701/2225 train_time:43333ms step_avg:61.82ms
step:702/2225 train_time:43372ms step_avg:61.78ms
step:703/2225 train_time:43434ms step_avg:61.78ms
step:704/2225 train_time:43497ms step_avg:61.79ms
step:705/2225 train_time:43559ms step_avg:61.79ms
step:705/2225 val_loss:3.7126 train_time:43618ms step_avg:61.87ms
step:706/2225 train_time:43642ms step_avg:61.82ms
step:707/2225 train_time:43681ms step_avg:61.78ms
step:708/2225 train_time:43742ms step_avg:61.78ms
step:709/2225 train_time:43807ms step_avg:61.79ms
step:710/2225 train_time:43867ms step_avg:61.78ms
step:710/2225 val_loss:3.7143 train_time:43928ms step_avg:61.87ms
step:711/2225 train_time:43951ms step_avg:61.82ms
step:712/2225 train_time:43988ms step_avg:61.78ms
step:713/2225 train_time:44050ms step_avg:61.78ms
step:714/2225 train_time:44111ms step_avg:61.78ms
step:715/2225 train_time:44171ms step_avg:61.78ms
step:715/2225 val_loss:3.7102 train_time:44230ms step_avg:61.86ms
step:716/2225 train_time:44255ms step_avg:61.81ms
step:717/2225 train_time:44293ms step_avg:61.78ms
step:718/2225 train_time:44355ms step_avg:61.78ms
step:719/2225 train_time:44419ms step_avg:61.78ms
step:720/2225 train_time:44478ms step_avg:61.78ms
step:720/2225 val_loss:3.7109 train_time:44539ms step_avg:61.86ms
step:721/2225 train_time:44562ms step_avg:61.81ms
step:722/2225 train_time:44600ms step_avg:61.77ms
step:723/2225 train_time:44663ms step_avg:61.78ms
step:724/2225 train_time:44726ms step_avg:61.78ms
step:725/2225 train_time:44787ms step_avg:61.77ms
step:725/2225 val_loss:3.7053 train_time:44845ms step_avg:61.86ms
step:726/2225 train_time:44870ms step_avg:61.80ms
step:727/2225 train_time:44908ms step_avg:61.77ms
step:728/2225 train_time:44969ms step_avg:61.77ms
step:729/2225 train_time:45034ms step_avg:61.77ms
step:730/2225 train_time:45094ms step_avg:61.77ms
step:730/2225 val_loss:3.6983 train_time:45155ms step_avg:61.86ms
step:731/2225 train_time:45178ms step_avg:61.80ms
step:732/2225 train_time:45215ms step_avg:61.77ms
step:733/2225 train_time:45277ms step_avg:61.77ms
step:734/2225 train_time:45340ms step_avg:61.77ms
step:735/2225 train_time:45403ms step_avg:61.77ms
step:735/2225 val_loss:3.6793 train_time:45461ms step_avg:61.85ms
step:736/2225 train_time:45486ms step_avg:61.80ms
step:737/2225 train_time:45524ms step_avg:61.77ms
step:738/2225 train_time:45585ms step_avg:61.77ms
step:739/2225 train_time:45650ms step_avg:61.77ms
step:740/2225 train_time:45709ms step_avg:61.77ms
step:740/2225 val_loss:3.6767 train_time:45771ms step_avg:61.85ms
step:741/2225 train_time:45794ms step_avg:61.80ms
step:742/2225 train_time:45835ms step_avg:61.77ms
step:743/2225 train_time:45897ms step_avg:61.77ms
step:744/2225 train_time:45960ms step_avg:61.77ms
step:745/2225 train_time:46022ms step_avg:61.77ms
step:745/2225 val_loss:3.6702 train_time:46082ms step_avg:61.85ms
step:746/2225 train_time:46106ms step_avg:61.80ms
step:747/2225 train_time:46149ms step_avg:61.78ms
step:748/2225 train_time:46211ms step_avg:61.78ms
step:749/2225 train_time:46275ms step_avg:61.78ms
step:750/2225 train_time:46337ms step_avg:61.78ms
step:750/2225 val_loss:3.6709 train_time:46399ms step_avg:61.87ms
step:751/2225 train_time:46422ms step_avg:61.81ms
step:752/2225 train_time:46460ms step_avg:61.78ms
step:753/2225 train_time:46525ms step_avg:61.79ms
step:754/2225 train_time:46588ms step_avg:61.79ms
step:755/2225 train_time:46649ms step_avg:61.79ms
step:755/2225 val_loss:3.6689 train_time:46708ms step_avg:61.86ms
step:756/2225 train_time:46735ms step_avg:61.82ms
step:757/2225 train_time:46774ms step_avg:61.79ms
step:758/2225 train_time:46836ms step_avg:61.79ms
step:759/2225 train_time:46901ms step_avg:61.79ms
step:760/2225 train_time:46962ms step_avg:61.79ms
step:760/2225 val_loss:3.6635 train_time:47024ms step_avg:61.87ms
step:761/2225 train_time:47047ms step_avg:61.82ms
step:762/2225 train_time:47085ms step_avg:61.79ms
step:763/2225 train_time:47148ms step_avg:61.79ms
step:764/2225 train_time:47211ms step_avg:61.79ms
step:765/2225 train_time:47273ms step_avg:61.79ms
step:765/2225 val_loss:3.6611 train_time:47332ms step_avg:61.87ms
step:766/2225 train_time:47357ms step_avg:61.82ms
step:767/2225 train_time:47395ms step_avg:61.79ms
step:768/2225 train_time:47457ms step_avg:61.79ms
step:769/2225 train_time:47521ms step_avg:61.80ms
step:770/2225 train_time:47580ms step_avg:61.79ms
step:770/2225 val_loss:3.6617 train_time:47642ms step_avg:61.87ms
step:771/2225 train_time:47665ms step_avg:61.82ms
step:772/2225 train_time:47706ms step_avg:61.80ms
step:773/2225 train_time:47769ms step_avg:61.80ms
step:774/2225 train_time:47831ms step_avg:61.80ms
step:775/2225 train_time:47894ms step_avg:61.80ms
step:775/2225 val_loss:3.6549 train_time:47954ms step_avg:61.88ms
step:776/2225 train_time:47979ms step_avg:61.83ms
step:777/2225 train_time:48017ms step_avg:61.80ms
step:778/2225 train_time:48079ms step_avg:61.80ms
step:779/2225 train_time:48145ms step_avg:61.80ms
step:780/2225 train_time:48206ms step_avg:61.80ms
step:780/2225 val_loss:3.6558 train_time:48267ms step_avg:61.88ms
step:781/2225 train_time:48290ms step_avg:61.83ms
step:782/2225 train_time:48328ms step_avg:61.80ms
step:783/2225 train_time:48392ms step_avg:61.80ms
step:784/2225 train_time:48455ms step_avg:61.80ms
step:785/2225 train_time:48516ms step_avg:61.80ms
step:785/2225 val_loss:3.6530 train_time:48576ms step_avg:61.88ms
step:786/2225 train_time:48601ms step_avg:61.83ms
step:787/2225 train_time:48643ms step_avg:61.81ms
step:788/2225 train_time:48705ms step_avg:61.81ms
step:789/2225 train_time:48769ms step_avg:61.81ms
step:790/2225 train_time:48830ms step_avg:61.81ms
step:790/2225 val_loss:3.6524 train_time:48892ms step_avg:61.89ms
step:791/2225 train_time:48916ms step_avg:61.84ms
step:792/2225 train_time:48953ms step_avg:61.81ms
step:793/2225 train_time:49018ms step_avg:61.81ms
step:794/2225 train_time:49080ms step_avg:61.81ms
step:795/2225 train_time:49142ms step_avg:61.81ms
step:795/2225 val_loss:3.6481 train_time:49201ms step_avg:61.89ms
step:796/2225 train_time:49226ms step_avg:61.84ms
step:797/2225 train_time:49265ms step_avg:61.81ms
step:798/2225 train_time:49328ms step_avg:61.81ms
step:799/2225 train_time:49393ms step_avg:61.82ms
step:800/2225 train_time:49453ms step_avg:61.82ms
step:800/2225 val_loss:3.6480 train_time:49515ms step_avg:61.89ms
step:801/2225 train_time:49538ms step_avg:61.85ms
step:802/2225 train_time:49576ms step_avg:61.82ms
step:803/2225 train_time:49642ms step_avg:61.82ms
step:804/2225 train_time:49704ms step_avg:61.82ms
step:805/2225 train_time:49766ms step_avg:61.82ms
step:805/2225 val_loss:3.6452 train_time:49825ms step_avg:61.89ms
step:806/2225 train_time:49850ms step_avg:61.85ms
step:807/2225 train_time:49890ms step_avg:61.82ms
step:808/2225 train_time:49951ms step_avg:61.82ms
step:809/2225 train_time:50015ms step_avg:61.82ms
step:810/2225 train_time:50076ms step_avg:61.82ms
step:810/2225 val_loss:3.6444 train_time:50138ms step_avg:61.90ms
step:811/2225 train_time:50161ms step_avg:61.85ms
step:812/2225 train_time:50201ms step_avg:61.82ms
step:813/2225 train_time:50263ms step_avg:61.82ms
step:814/2225 train_time:50328ms step_avg:61.83ms
step:815/2225 train_time:50390ms step_avg:61.83ms
step:815/2225 val_loss:3.6425 train_time:50450ms step_avg:61.90ms
step:816/2225 train_time:50475ms step_avg:61.86ms
step:817/2225 train_time:50514ms step_avg:61.83ms
step:818/2225 train_time:50577ms step_avg:61.83ms
step:819/2225 train_time:50640ms step_avg:61.83ms
step:820/2225 train_time:50700ms step_avg:61.83ms
step:820/2225 val_loss:3.6402 train_time:50761ms step_avg:61.90ms
step:821/2225 train_time:50784ms step_avg:61.86ms
step:822/2225 train_time:50822ms step_avg:61.83ms
step:823/2225 train_time:50886ms step_avg:61.83ms
step:824/2225 train_time:50950ms step_avg:61.83ms
step:825/2225 train_time:51014ms step_avg:61.84ms
step:825/2225 val_loss:3.6376 train_time:51073ms step_avg:61.91ms
step:826/2225 train_time:51098ms step_avg:61.86ms
step:827/2225 train_time:51137ms step_avg:61.83ms
step:828/2225 train_time:51198ms step_avg:61.83ms
step:829/2225 train_time:51263ms step_avg:61.84ms
step:830/2225 train_time:51326ms step_avg:61.84ms
step:830/2225 val_loss:3.6382 train_time:51388ms step_avg:61.91ms
step:831/2225 train_time:51411ms step_avg:61.87ms
step:832/2225 train_time:51450ms step_avg:61.84ms
step:833/2225 train_time:51514ms step_avg:61.84ms
step:834/2225 train_time:51576ms step_avg:61.84ms
step:835/2225 train_time:51641ms step_avg:61.84ms
step:835/2225 val_loss:3.6344 train_time:51700ms step_avg:61.92ms
step:836/2225 train_time:51725ms step_avg:61.87ms
step:837/2225 train_time:51763ms step_avg:61.84ms
step:838/2225 train_time:51825ms step_avg:61.84ms
step:839/2225 train_time:51891ms step_avg:61.85ms
step:840/2225 train_time:51953ms step_avg:61.85ms
step:840/2225 val_loss:3.6558 train_time:52015ms step_avg:61.92ms
step:841/2225 train_time:52038ms step_avg:61.88ms
step:842/2225 train_time:52075ms step_avg:61.85ms
step:843/2225 train_time:52141ms step_avg:61.85ms
step:844/2225 train_time:52203ms step_avg:61.85ms
step:845/2225 train_time:52264ms step_avg:61.85ms
step:845/2225 val_loss:3.6427 train_time:52324ms step_avg:61.92ms
step:846/2225 train_time:52349ms step_avg:61.88ms
step:847/2225 train_time:52387ms step_avg:61.85ms
step:848/2225 train_time:52449ms step_avg:61.85ms
step:849/2225 train_time:52515ms step_avg:61.86ms
step:850/2225 train_time:52576ms step_avg:61.85ms
step:850/2225 val_loss:3.6340 train_time:52638ms step_avg:61.93ms
step:851/2225 train_time:52661ms step_avg:61.88ms
step:852/2225 train_time:52700ms step_avg:61.85ms
step:853/2225 train_time:52763ms step_avg:61.86ms
step:854/2225 train_time:52827ms step_avg:61.86ms
step:855/2225 train_time:52889ms step_avg:61.86ms
step:855/2225 val_loss:3.6311 train_time:52948ms step_avg:61.93ms
step:856/2225 train_time:52973ms step_avg:61.88ms
step:857/2225 train_time:53012ms step_avg:61.86ms
step:858/2225 train_time:53075ms step_avg:61.86ms
step:859/2225 train_time:53140ms step_avg:61.86ms
step:860/2225 train_time:53200ms step_avg:61.86ms
step:860/2225 val_loss:3.6281 train_time:53261ms step_avg:61.93ms
step:861/2225 train_time:53286ms step_avg:61.89ms
step:862/2225 train_time:53323ms step_avg:61.86ms
step:863/2225 train_time:53387ms step_avg:61.86ms
step:864/2225 train_time:53448ms step_avg:61.86ms
step:865/2225 train_time:53509ms step_avg:61.86ms
step:865/2225 val_loss:3.6255 train_time:53569ms step_avg:61.93ms
step:866/2225 train_time:53594ms step_avg:61.89ms
step:867/2225 train_time:53634ms step_avg:61.86ms
step:868/2225 train_time:53695ms step_avg:61.86ms
step:869/2225 train_time:53761ms step_avg:61.87ms
step:870/2225 train_time:53821ms step_avg:61.86ms
step:870/2225 val_loss:3.6253 train_time:53883ms step_avg:61.93ms
step:871/2225 train_time:53906ms step_avg:61.89ms
step:872/2225 train_time:53944ms step_avg:61.86ms
step:873/2225 train_time:54010ms step_avg:61.87ms
step:874/2225 train_time:54071ms step_avg:61.87ms
step:875/2225 train_time:54133ms step_avg:61.87ms
step:875/2225 val_loss:3.6240 train_time:54193ms step_avg:61.93ms
step:876/2225 train_time:54217ms step_avg:61.89ms
step:877/2225 train_time:54255ms step_avg:61.86ms
step:878/2225 train_time:54316ms step_avg:61.86ms
step:879/2225 train_time:54383ms step_avg:61.87ms
step:880/2225 train_time:54446ms step_avg:61.87ms
step:880/2225 val_loss:3.6222 train_time:54508ms step_avg:61.94ms
step:881/2225 train_time:54534ms step_avg:61.90ms
step:882/2225 train_time:54570ms step_avg:61.87ms
step:883/2225 train_time:54634ms step_avg:61.87ms
step:884/2225 train_time:54695ms step_avg:61.87ms
step:885/2225 train_time:54757ms step_avg:61.87ms
step:885/2225 val_loss:3.6213 train_time:54815ms step_avg:61.94ms
step:886/2225 train_time:54840ms step_avg:61.90ms
step:887/2225 train_time:54880ms step_avg:61.87ms
step:888/2225 train_time:54943ms step_avg:61.87ms
step:889/2225 train_time:55008ms step_avg:61.88ms
step:890/2225 train_time:55068ms step_avg:61.87ms
step:890/2225 val_loss:3.6209 train_time:55130ms step_avg:61.94ms
step:891/2225 train_time:55153ms step_avg:61.90ms
step:892/2225 train_time:55191ms step_avg:61.87ms
step:893/2225 train_time:55258ms step_avg:61.88ms
step:894/2225 train_time:55320ms step_avg:61.88ms
step:895/2225 train_time:55382ms step_avg:61.88ms
step:895/2225 val_loss:3.6160 train_time:55441ms step_avg:61.95ms
step:896/2225 train_time:55468ms step_avg:61.91ms
step:897/2225 train_time:55505ms step_avg:61.88ms
step:898/2225 train_time:55566ms step_avg:61.88ms
step:899/2225 train_time:55631ms step_avg:61.88ms
step:900/2225 train_time:55691ms step_avg:61.88ms
step:900/2225 val_loss:3.6175 train_time:55752ms step_avg:61.95ms
step:901/2225 train_time:55776ms step_avg:61.90ms
step:902/2225 train_time:55814ms step_avg:61.88ms
step:903/2225 train_time:55878ms step_avg:61.88ms
step:904/2225 train_time:55940ms step_avg:61.88ms
step:905/2225 train_time:56002ms step_avg:61.88ms
step:905/2225 val_loss:3.6143 train_time:56061ms step_avg:61.95ms
step:906/2225 train_time:56086ms step_avg:61.90ms
step:907/2225 train_time:56124ms step_avg:61.88ms
step:908/2225 train_time:56186ms step_avg:61.88ms
step:909/2225 train_time:56250ms step_avg:61.88ms
step:910/2225 train_time:56311ms step_avg:61.88ms
step:910/2225 val_loss:3.6129 train_time:56373ms step_avg:61.95ms
step:911/2225 train_time:56397ms step_avg:61.91ms
step:912/2225 train_time:56434ms step_avg:61.88ms
step:913/2225 train_time:56500ms step_avg:61.88ms
step:914/2225 train_time:56562ms step_avg:61.88ms
step:915/2225 train_time:56623ms step_avg:61.88ms
step:915/2225 val_loss:3.6120 train_time:56682ms step_avg:61.95ms
step:916/2225 train_time:56707ms step_avg:61.91ms
step:917/2225 train_time:56749ms step_avg:61.89ms
step:918/2225 train_time:56810ms step_avg:61.88ms
step:919/2225 train_time:56875ms step_avg:61.89ms
step:920/2225 train_time:56935ms step_avg:61.89ms
step:920/2225 val_loss:3.6126 train_time:56996ms step_avg:61.95ms
step:921/2225 train_time:57020ms step_avg:61.91ms
step:922/2225 train_time:57058ms step_avg:61.88ms
step:923/2225 train_time:57124ms step_avg:61.89ms
step:924/2225 train_time:57186ms step_avg:61.89ms
step:925/2225 train_time:57247ms step_avg:61.89ms
step:925/2225 val_loss:3.6103 train_time:57306ms step_avg:61.95ms
step:926/2225 train_time:57331ms step_avg:61.91ms
step:927/2225 train_time:57370ms step_avg:61.89ms
step:928/2225 train_time:57431ms step_avg:61.89ms
step:929/2225 train_time:57496ms step_avg:61.89ms
step:930/2225 train_time:57555ms step_avg:61.89ms
step:930/2225 val_loss:3.6107 train_time:57617ms step_avg:61.95ms
step:931/2225 train_time:57640ms step_avg:61.91ms
step:932/2225 train_time:57681ms step_avg:61.89ms
step:933/2225 train_time:57745ms step_avg:61.89ms
step:934/2225 train_time:57808ms step_avg:61.89ms
step:935/2225 train_time:57871ms step_avg:61.89ms
step:935/2225 val_loss:3.6081 train_time:57930ms step_avg:61.96ms
step:936/2225 train_time:57955ms step_avg:61.92ms
step:937/2225 train_time:57994ms step_avg:61.89ms
step:938/2225 train_time:58056ms step_avg:61.89ms
step:939/2225 train_time:58119ms step_avg:61.89ms
step:940/2225 train_time:58179ms step_avg:61.89ms
step:940/2225 val_loss:3.6045 train_time:58240ms step_avg:61.96ms
step:941/2225 train_time:58264ms step_avg:61.92ms
step:942/2225 train_time:58301ms step_avg:61.89ms
step:943/2225 train_time:58365ms step_avg:61.89ms
step:944/2225 train_time:58427ms step_avg:61.89ms
step:945/2225 train_time:58489ms step_avg:61.89ms
step:945/2225 val_loss:3.6036 train_time:58549ms step_avg:61.96ms
step:946/2225 train_time:58576ms step_avg:61.92ms
step:947/2225 train_time:58613ms step_avg:61.89ms
step:948/2225 train_time:58676ms step_avg:61.89ms
step:949/2225 train_time:58741ms step_avg:61.90ms
step:950/2225 train_time:58801ms step_avg:61.90ms
step:950/2225 val_loss:3.6031 train_time:58862ms step_avg:61.96ms
step:951/2225 train_time:58885ms step_avg:61.92ms
step:952/2225 train_time:58924ms step_avg:61.90ms
step:953/2225 train_time:58988ms step_avg:61.90ms
step:954/2225 train_time:59049ms step_avg:61.90ms
step:955/2225 train_time:59111ms step_avg:61.90ms
step:955/2225 val_loss:3.6018 train_time:59170ms step_avg:61.96ms
step:956/2225 train_time:59195ms step_avg:61.92ms
step:957/2225 train_time:59234ms step_avg:61.89ms
step:958/2225 train_time:59297ms step_avg:61.90ms
step:959/2225 train_time:59366ms step_avg:61.90ms
step:960/2225 train_time:59426ms step_avg:61.90ms
step:960/2225 val_loss:3.6004 train_time:59488ms step_avg:61.97ms
step:961/2225 train_time:59511ms step_avg:61.93ms
step:962/2225 train_time:59550ms step_avg:61.90ms
step:963/2225 train_time:59614ms step_avg:61.90ms
step:964/2225 train_time:59678ms step_avg:61.91ms
step:965/2225 train_time:59741ms step_avg:61.91ms
step:965/2225 val_loss:3.5976 train_time:59800ms step_avg:61.97ms
step:966/2225 train_time:59824ms step_avg:61.93ms
step:967/2225 train_time:59867ms step_avg:61.91ms
step:968/2225 train_time:59928ms step_avg:61.91ms
step:969/2225 train_time:59992ms step_avg:61.91ms
step:970/2225 train_time:60052ms step_avg:61.91ms
step:970/2225 val_loss:3.5992 train_time:60114ms step_avg:61.97ms
step:971/2225 train_time:60138ms step_avg:61.93ms
step:972/2225 train_time:60176ms step_avg:61.91ms
step:973/2225 train_time:60242ms step_avg:61.91ms
step:974/2225 train_time:60305ms step_avg:61.91ms
step:975/2225 train_time:60367ms step_avg:61.91ms
step:975/2225 val_loss:3.5954 train_time:60427ms step_avg:61.98ms
step:976/2225 train_time:60452ms step_avg:61.94ms
step:977/2225 train_time:60490ms step_avg:61.91ms
step:978/2225 train_time:60554ms step_avg:61.92ms
step:979/2225 train_time:60620ms step_avg:61.92ms
step:980/2225 train_time:60680ms step_avg:61.92ms
step:980/2225 val_loss:3.5956 train_time:60742ms step_avg:61.98ms
step:981/2225 train_time:60766ms step_avg:61.94ms
step:982/2225 train_time:60805ms step_avg:61.92ms
step:983/2225 train_time:60868ms step_avg:61.92ms
step:984/2225 train_time:60931ms step_avg:61.92ms
step:985/2225 train_time:60992ms step_avg:61.92ms
step:985/2225 val_loss:3.5921 train_time:61051ms step_avg:61.98ms
step:986/2225 train_time:61076ms step_avg:61.94ms
step:987/2225 train_time:61116ms step_avg:61.92ms
step:988/2225 train_time:61178ms step_avg:61.92ms
step:989/2225 train_time:61243ms step_avg:61.92ms
step:990/2225 train_time:61304ms step_avg:61.92ms
step:990/2225 val_loss:3.5916 train_time:61365ms step_avg:61.98ms
step:991/2225 train_time:61388ms step_avg:61.95ms
step:992/2225 train_time:61426ms step_avg:61.92ms
step:993/2225 train_time:61491ms step_avg:61.92ms
step:994/2225 train_time:61553ms step_avg:61.92ms
step:995/2225 train_time:61615ms step_avg:61.92ms
step:995/2225 val_loss:3.5903 train_time:61674ms step_avg:61.98ms
step:996/2225 train_time:61699ms step_avg:61.95ms
step:997/2225 train_time:61738ms step_avg:61.92ms
step:998/2225 train_time:61801ms step_avg:61.92ms
step:999/2225 train_time:61865ms step_avg:61.93ms
step:1000/2225 train_time:61925ms step_avg:61.92ms
step:1000/2225 val_loss:3.5914 train_time:61987ms step_avg:61.99ms
step:1001/2225 train_time:62010ms step_avg:61.95ms
step:1002/2225 train_time:62049ms step_avg:61.92ms
step:1003/2225 train_time:62113ms step_avg:61.93ms
step:1004/2225 train_time:62177ms step_avg:61.93ms
step:1005/2225 train_time:62239ms step_avg:61.93ms
step:1005/2225 val_loss:3.5880 train_time:62298ms step_avg:61.99ms
step:1006/2225 train_time:62323ms step_avg:61.95ms
step:1007/2225 train_time:62361ms step_avg:61.93ms
step:1008/2225 train_time:62423ms step_avg:61.93ms
step:1009/2225 train_time:62490ms step_avg:61.93ms
step:1010/2225 train_time:62550ms step_avg:61.93ms
step:1010/2225 val_loss:3.5919 train_time:62613ms step_avg:61.99ms
step:1011/2225 train_time:62636ms step_avg:61.95ms
step:1012/2225 train_time:62674ms step_avg:61.93ms
step:1013/2225 train_time:62740ms step_avg:61.93ms
step:1014/2225 train_time:62803ms step_avg:61.94ms
step:1015/2225 train_time:62865ms step_avg:61.94ms
step:1015/2225 val_loss:3.5907 train_time:62924ms step_avg:61.99ms
step:1016/2225 train_time:62949ms step_avg:61.96ms
step:1017/2225 train_time:62987ms step_avg:61.93ms
step:1018/2225 train_time:63049ms step_avg:61.93ms
step:1019/2225 train_time:63113ms step_avg:61.94ms
step:1020/2225 train_time:63174ms step_avg:61.94ms
step:1020/2225 val_loss:3.5887 train_time:63236ms step_avg:62.00ms
step:1021/2225 train_time:63260ms step_avg:61.96ms
step:1022/2225 train_time:63299ms step_avg:61.94ms
step:1023/2225 train_time:63365ms step_avg:61.94ms
step:1024/2225 train_time:63431ms step_avg:61.94ms
step:1025/2225 train_time:63492ms step_avg:61.94ms
step:1025/2225 val_loss:3.5835 train_time:63551ms step_avg:62.00ms
step:1026/2225 train_time:63576ms step_avg:61.97ms
step:1027/2225 train_time:63616ms step_avg:61.94ms
step:1028/2225 train_time:63678ms step_avg:61.94ms
step:1029/2225 train_time:63745ms step_avg:61.95ms
step:1030/2225 train_time:63806ms step_avg:61.95ms
step:1030/2225 val_loss:3.5824 train_time:63868ms step_avg:62.01ms
step:1031/2225 train_time:63891ms step_avg:61.97ms
step:1032/2225 train_time:63930ms step_avg:61.95ms
step:1033/2225 train_time:63995ms step_avg:61.95ms
step:1034/2225 train_time:64057ms step_avg:61.95ms
step:1035/2225 train_time:64119ms step_avg:61.95ms
step:1035/2225 val_loss:3.5822 train_time:64179ms step_avg:62.01ms
step:1036/2225 train_time:64204ms step_avg:61.97ms
step:1037/2225 train_time:64244ms step_avg:61.95ms
step:1038/2225 train_time:64306ms step_avg:61.95ms
step:1039/2225 train_time:64371ms step_avg:61.96ms
step:1040/2225 train_time:64434ms step_avg:61.96ms
step:1040/2225 val_loss:3.5814 train_time:64495ms step_avg:62.01ms
step:1041/2225 train_time:64519ms step_avg:61.98ms
step:1042/2225 train_time:64557ms step_avg:61.95ms
step:1043/2225 train_time:64622ms step_avg:61.96ms
step:1044/2225 train_time:64685ms step_avg:61.96ms
step:1045/2225 train_time:64748ms step_avg:61.96ms
step:1045/2225 val_loss:3.5792 train_time:64808ms step_avg:62.02ms
step:1046/2225 train_time:64833ms step_avg:61.98ms
step:1047/2225 train_time:64873ms step_avg:61.96ms
step:1048/2225 train_time:64936ms step_avg:61.96ms
step:1049/2225 train_time:65002ms step_avg:61.97ms
step:1050/2225 train_time:65062ms step_avg:61.96ms
step:1050/2225 val_loss:3.5783 train_time:65123ms step_avg:62.02ms
step:1051/2225 train_time:65148ms step_avg:61.99ms
step:1052/2225 train_time:65185ms step_avg:61.96ms
step:1053/2225 train_time:65249ms step_avg:61.96ms
step:1054/2225 train_time:65313ms step_avg:61.97ms
step:1055/2225 train_time:65375ms step_avg:61.97ms
step:1055/2225 val_loss:3.5757 train_time:65435ms step_avg:62.02ms
step:1056/2225 train_time:65460ms step_avg:61.99ms
step:1057/2225 train_time:65500ms step_avg:61.97ms
step:1058/2225 train_time:65561ms step_avg:61.97ms
step:1059/2225 train_time:65627ms step_avg:61.97ms
step:1060/2225 train_time:65689ms step_avg:61.97ms
step:1060/2225 val_loss:3.5753 train_time:65752ms step_avg:62.03ms
step:1061/2225 train_time:65775ms step_avg:61.99ms
step:1062/2225 train_time:65813ms step_avg:61.97ms
step:1063/2225 train_time:65879ms step_avg:61.97ms
step:1064/2225 train_time:65941ms step_avg:61.97ms
step:1065/2225 train_time:66003ms step_avg:61.97ms
step:1065/2225 val_loss:3.5737 train_time:66062ms step_avg:62.03ms
step:1066/2225 train_time:66090ms step_avg:62.00ms
step:1067/2225 train_time:66127ms step_avg:61.98ms
step:1068/2225 train_time:66189ms step_avg:61.97ms
step:1069/2225 train_time:66255ms step_avg:61.98ms
step:1070/2225 train_time:66317ms step_avg:61.98ms
step:1070/2225 val_loss:3.5753 train_time:66379ms step_avg:62.04ms
step:1071/2225 train_time:66402ms step_avg:62.00ms
step:1072/2225 train_time:66440ms step_avg:61.98ms
step:1073/2225 train_time:66504ms step_avg:61.98ms
step:1074/2225 train_time:66567ms step_avg:61.98ms
step:1075/2225 train_time:66630ms step_avg:61.98ms
step:1075/2225 val_loss:3.5716 train_time:66690ms step_avg:62.04ms
step:1076/2225 train_time:66715ms step_avg:62.00ms
step:1077/2225 train_time:66754ms step_avg:61.98ms
step:1078/2225 train_time:66816ms step_avg:61.98ms
step:1079/2225 train_time:66880ms step_avg:61.98ms
step:1080/2225 train_time:66940ms step_avg:61.98ms
step:1080/2225 val_loss:3.5739 train_time:67001ms step_avg:62.04ms
step:1081/2225 train_time:67024ms step_avg:62.00ms
step:1082/2225 train_time:67064ms step_avg:61.98ms
step:1083/2225 train_time:67128ms step_avg:61.98ms
step:1084/2225 train_time:67191ms step_avg:61.98ms
step:1085/2225 train_time:67254ms step_avg:61.98ms
step:1085/2225 val_loss:3.5714 train_time:67314ms step_avg:62.04ms
step:1086/2225 train_time:67340ms step_avg:62.01ms
step:1087/2225 train_time:67376ms step_avg:61.98ms
step:1088/2225 train_time:67439ms step_avg:61.98ms
step:1089/2225 train_time:67504ms step_avg:61.99ms
step:1090/2225 train_time:67565ms step_avg:61.99ms
step:1090/2225 val_loss:3.5740 train_time:67627ms step_avg:62.04ms
step:1091/2225 train_time:67650ms step_avg:62.01ms
step:1092/2225 train_time:67688ms step_avg:61.99ms
step:1093/2225 train_time:67754ms step_avg:61.99ms
step:1094/2225 train_time:67816ms step_avg:61.99ms
step:1095/2225 train_time:67877ms step_avg:61.99ms
step:1095/2225 val_loss:3.5675 train_time:67937ms step_avg:62.04ms
step:1096/2225 train_time:67962ms step_avg:62.01ms
step:1097/2225 train_time:68001ms step_avg:61.99ms
step:1098/2225 train_time:68063ms step_avg:61.99ms
step:1099/2225 train_time:68128ms step_avg:61.99ms
step:1100/2225 train_time:68188ms step_avg:61.99ms
step:1100/2225 val_loss:3.5679 train_time:68250ms step_avg:62.05ms
step:1101/2225 train_time:68273ms step_avg:62.01ms
step:1102/2225 train_time:68312ms step_avg:61.99ms
step:1103/2225 train_time:68377ms step_avg:61.99ms
step:1104/2225 train_time:68439ms step_avg:61.99ms
step:1105/2225 train_time:68500ms step_avg:61.99ms
step:1105/2225 val_loss:3.5660 train_time:68560ms step_avg:62.05ms
step:1106/2225 train_time:68585ms step_avg:62.01ms
step:1107/2225 train_time:68623ms step_avg:61.99ms
step:1108/2225 train_time:68686ms step_avg:61.99ms
step:1109/2225 train_time:68752ms step_avg:61.99ms
step:1110/2225 train_time:68811ms step_avg:61.99ms
step:1110/2225 val_loss:3.5662 train_time:68873ms step_avg:62.05ms
step:1111/2225 train_time:68896ms step_avg:62.01ms
step:1112/2225 train_time:68936ms step_avg:61.99ms
step:1113/2225 train_time:69000ms step_avg:61.99ms
step:1114/2225 train_time:69063ms step_avg:62.00ms
step:1115/2225 train_time:69126ms step_avg:62.00ms
step:1115/2225 val_loss:3.5631 train_time:69186ms step_avg:62.05ms
step:1116/2225 train_time:69211ms step_avg:62.02ms
step:1117/2225 train_time:69251ms step_avg:62.00ms
step:1118/2225 train_time:69315ms step_avg:62.00ms
step:1119/2225 train_time:69379ms step_avg:62.00ms
step:1120/2225 train_time:69440ms step_avg:62.00ms
step:1120/2225 val_loss:3.5625 train_time:69502ms step_avg:62.06ms
step:1121/2225 train_time:69526ms step_avg:62.02ms
step:1122/2225 train_time:69565ms step_avg:62.00ms
step:1123/2225 train_time:69631ms step_avg:62.00ms
step:1124/2225 train_time:69694ms step_avg:62.01ms
step:1125/2225 train_time:69757ms step_avg:62.01ms
step:1125/2225 val_loss:3.5605 train_time:69818ms step_avg:62.06ms
step:1126/2225 train_time:69842ms step_avg:62.03ms
step:1127/2225 train_time:69881ms step_avg:62.01ms
step:1128/2225 train_time:69943ms step_avg:62.01ms
step:1129/2225 train_time:70010ms step_avg:62.01ms
step:1130/2225 train_time:70069ms step_avg:62.01ms
step:1130/2225 val_loss:3.5591 train_time:70132ms step_avg:62.06ms
step:1131/2225 train_time:70155ms step_avg:62.03ms
step:1132/2225 train_time:70194ms step_avg:62.01ms
step:1133/2225 train_time:70259ms step_avg:62.01ms
step:1134/2225 train_time:70322ms step_avg:62.01ms
step:1135/2225 train_time:70383ms step_avg:62.01ms
step:1135/2225 val_loss:3.5561 train_time:70443ms step_avg:62.06ms
step:1136/2225 train_time:70468ms step_avg:62.03ms
step:1137/2225 train_time:70506ms step_avg:62.01ms
step:1138/2225 train_time:70570ms step_avg:62.01ms
step:1139/2225 train_time:70635ms step_avg:62.01ms
step:1140/2225 train_time:70695ms step_avg:62.01ms
step:1140/2225 val_loss:3.5558 train_time:70757ms step_avg:62.07ms
step:1141/2225 train_time:70780ms step_avg:62.03ms
step:1142/2225 train_time:70819ms step_avg:62.01ms
step:1143/2225 train_time:70885ms step_avg:62.02ms
step:1144/2225 train_time:70946ms step_avg:62.02ms
step:1145/2225 train_time:71008ms step_avg:62.02ms
step:1145/2225 val_loss:3.5536 train_time:71067ms step_avg:62.07ms
step:1146/2225 train_time:71091ms step_avg:62.03ms
step:1147/2225 train_time:71132ms step_avg:62.02ms
step:1148/2225 train_time:71195ms step_avg:62.02ms
step:1149/2225 train_time:71261ms step_avg:62.02ms
step:1150/2225 train_time:71322ms step_avg:62.02ms
step:1150/2225 val_loss:3.5528 train_time:71384ms step_avg:62.07ms
step:1151/2225 train_time:71407ms step_avg:62.04ms
step:1152/2225 train_time:71448ms step_avg:62.02ms
step:1153/2225 train_time:71512ms step_avg:62.02ms
step:1154/2225 train_time:71576ms step_avg:62.02ms
step:1155/2225 train_time:71638ms step_avg:62.02ms
step:1155/2225 val_loss:3.5506 train_time:71698ms step_avg:62.08ms
step:1156/2225 train_time:71723ms step_avg:62.04ms
step:1157/2225 train_time:71762ms step_avg:62.02ms
step:1158/2225 train_time:71825ms step_avg:62.02ms
step:1159/2225 train_time:71890ms step_avg:62.03ms
step:1160/2225 train_time:71951ms step_avg:62.03ms
step:1160/2225 val_loss:3.5506 train_time:72014ms step_avg:62.08ms
step:1161/2225 train_time:72037ms step_avg:62.05ms
step:1162/2225 train_time:72077ms step_avg:62.03ms
step:1163/2225 train_time:72140ms step_avg:62.03ms
step:1164/2225 train_time:72203ms step_avg:62.03ms
step:1165/2225 train_time:72265ms step_avg:62.03ms
step:1165/2225 val_loss:3.5494 train_time:72325ms step_avg:62.08ms
step:1166/2225 train_time:72350ms step_avg:62.05ms
step:1167/2225 train_time:72389ms step_avg:62.03ms
step:1168/2225 train_time:72452ms step_avg:62.03ms
step:1169/2225 train_time:72515ms step_avg:62.03ms
step:1170/2225 train_time:72575ms step_avg:62.03ms
step:1170/2225 val_loss:3.5499 train_time:72636ms step_avg:62.08ms
step:1171/2225 train_time:72662ms step_avg:62.05ms
step:1172/2225 train_time:72698ms step_avg:62.03ms
step:1173/2225 train_time:72764ms step_avg:62.03ms
step:1174/2225 train_time:72827ms step_avg:62.03ms
step:1175/2225 train_time:72888ms step_avg:62.03ms
step:1175/2225 val_loss:3.5433 train_time:72947ms step_avg:62.08ms
step:1176/2225 train_time:72972ms step_avg:62.05ms
step:1177/2225 train_time:73013ms step_avg:62.03ms
step:1178/2225 train_time:73075ms step_avg:62.03ms
step:1179/2225 train_time:73140ms step_avg:62.04ms
step:1180/2225 train_time:73199ms step_avg:62.03ms
step:1180/2225 val_loss:3.5436 train_time:73261ms step_avg:62.09ms
step:1181/2225 train_time:73285ms step_avg:62.05ms
step:1182/2225 train_time:73324ms step_avg:62.03ms
step:1183/2225 train_time:73388ms step_avg:62.04ms
step:1184/2225 train_time:73451ms step_avg:62.04ms
step:1185/2225 train_time:73512ms step_avg:62.04ms
step:1185/2225 val_loss:3.5402 train_time:73572ms step_avg:62.09ms
step:1186/2225 train_time:73597ms step_avg:62.06ms
step:1187/2225 train_time:73636ms step_avg:62.04ms
step:1188/2225 train_time:73698ms step_avg:62.04ms
step:1189/2225 train_time:73763ms step_avg:62.04ms
step:1190/2225 train_time:73824ms step_avg:62.04ms
step:1190/2225 val_loss:3.5401 train_time:73886ms step_avg:62.09ms
step:1191/2225 train_time:73909ms step_avg:62.06ms
step:1192/2225 train_time:73947ms step_avg:62.04ms
step:1193/2225 train_time:74013ms step_avg:62.04ms
step:1194/2225 train_time:74075ms step_avg:62.04ms
step:1195/2225 train_time:74136ms step_avg:62.04ms
step:1195/2225 val_loss:3.5368 train_time:74196ms step_avg:62.09ms
step:1196/2225 train_time:74221ms step_avg:62.06ms
step:1197/2225 train_time:74259ms step_avg:62.04ms
step:1198/2225 train_time:74321ms step_avg:62.04ms
step:1199/2225 train_time:74387ms step_avg:62.04ms
step:1200/2225 train_time:74447ms step_avg:62.04ms
step:1200/2225 val_loss:3.5347 train_time:74509ms step_avg:62.09ms
step:1201/2225 train_time:74533ms step_avg:62.06ms
step:1202/2225 train_time:74571ms step_avg:62.04ms
step:1203/2225 train_time:74638ms step_avg:62.04ms
step:1204/2225 train_time:74700ms step_avg:62.04ms
step:1205/2225 train_time:74761ms step_avg:62.04ms
step:1205/2225 val_loss:3.5334 train_time:74821ms step_avg:62.09ms
step:1206/2225 train_time:74846ms step_avg:62.06ms
step:1207/2225 train_time:74887ms step_avg:62.04ms
step:1208/2225 train_time:74948ms step_avg:62.04ms
step:1209/2225 train_time:75012ms step_avg:62.04ms
step:1210/2225 train_time:75071ms step_avg:62.04ms
step:1210/2225 val_loss:3.5320 train_time:75133ms step_avg:62.09ms
step:1211/2225 train_time:75156ms step_avg:62.06ms
step:1212/2225 train_time:75194ms step_avg:62.04ms
step:1213/2225 train_time:75258ms step_avg:62.04ms
step:1214/2225 train_time:75322ms step_avg:62.04ms
step:1215/2225 train_time:75383ms step_avg:62.04ms
step:1215/2225 val_loss:3.5294 train_time:75442ms step_avg:62.09ms
step:1216/2225 train_time:75467ms step_avg:62.06ms
step:1217/2225 train_time:75507ms step_avg:62.04ms
step:1218/2225 train_time:75569ms step_avg:62.04ms
step:1219/2225 train_time:75635ms step_avg:62.05ms
step:1220/2225 train_time:75696ms step_avg:62.05ms
step:1220/2225 val_loss:3.5304 train_time:75758ms step_avg:62.10ms
step:1221/2225 train_time:75781ms step_avg:62.07ms
step:1222/2225 train_time:75820ms step_avg:62.05ms
step:1223/2225 train_time:75886ms step_avg:62.05ms
step:1224/2225 train_time:75948ms step_avg:62.05ms
step:1225/2225 train_time:76011ms step_avg:62.05ms
step:1225/2225 val_loss:3.5265 train_time:76070ms step_avg:62.10ms
step:1226/2225 train_time:76095ms step_avg:62.07ms
step:1227/2225 train_time:76136ms step_avg:62.05ms
step:1228/2225 train_time:76197ms step_avg:62.05ms
step:1229/2225 train_time:76263ms step_avg:62.05ms
step:1230/2225 train_time:76323ms step_avg:62.05ms
step:1230/2225 val_loss:3.5256 train_time:76384ms step_avg:62.10ms
step:1231/2225 train_time:76407ms step_avg:62.07ms
step:1232/2225 train_time:76446ms step_avg:62.05ms
step:1233/2225 train_time:76511ms step_avg:62.05ms
step:1234/2225 train_time:76575ms step_avg:62.05ms
step:1235/2225 train_time:76637ms step_avg:62.05ms
step:1235/2225 val_loss:3.5234 train_time:76696ms step_avg:62.10ms
step:1236/2225 train_time:76721ms step_avg:62.07ms
step:1237/2225 train_time:76761ms step_avg:62.05ms
step:1238/2225 train_time:76824ms step_avg:62.05ms
step:1239/2225 train_time:76888ms step_avg:62.06ms
step:1240/2225 train_time:76948ms step_avg:62.05ms
step:1240/2225 val_loss:3.5262 train_time:77010ms step_avg:62.11ms
step:1241/2225 train_time:77033ms step_avg:62.07ms
step:1242/2225 train_time:77073ms step_avg:62.06ms
step:1243/2225 train_time:77136ms step_avg:62.06ms
step:1244/2225 train_time:77200ms step_avg:62.06ms
step:1245/2225 train_time:77262ms step_avg:62.06ms
step:1245/2225 val_loss:3.5208 train_time:77322ms step_avg:62.11ms
step:1246/2225 train_time:77347ms step_avg:62.08ms
step:1247/2225 train_time:77385ms step_avg:62.06ms
step:1248/2225 train_time:77447ms step_avg:62.06ms
step:1249/2225 train_time:77511ms step_avg:62.06ms
step:1250/2225 train_time:77571ms step_avg:62.06ms
step:1250/2225 val_loss:3.5196 train_time:77633ms step_avg:62.11ms
step:1251/2225 train_time:77656ms step_avg:62.07ms
step:1252/2225 train_time:77694ms step_avg:62.06ms
step:1253/2225 train_time:77761ms step_avg:62.06ms
step:1254/2225 train_time:77824ms step_avg:62.06ms
step:1255/2225 train_time:77885ms step_avg:62.06ms
step:1255/2225 val_loss:3.5172 train_time:77944ms step_avg:62.11ms
step:1256/2225 train_time:77969ms step_avg:62.08ms
step:1257/2225 train_time:78006ms step_avg:62.06ms
step:1258/2225 train_time:78070ms step_avg:62.06ms
step:1259/2225 train_time:78136ms step_avg:62.06ms
step:1260/2225 train_time:78195ms step_avg:62.06ms
step:1260/2225 val_loss:3.5152 train_time:78257ms step_avg:62.11ms
step:1261/2225 train_time:78280ms step_avg:62.08ms
step:1262/2225 train_time:78321ms step_avg:62.06ms
step:1263/2225 train_time:78388ms step_avg:62.06ms
step:1264/2225 train_time:78451ms step_avg:62.07ms
step:1265/2225 train_time:78513ms step_avg:62.07ms
step:1265/2225 val_loss:3.5151 train_time:78572ms step_avg:62.11ms
step:1266/2225 train_time:78597ms step_avg:62.08ms
step:1267/2225 train_time:78636ms step_avg:62.06ms
step:1268/2225 train_time:78699ms step_avg:62.07ms
step:1269/2225 train_time:78763ms step_avg:62.07ms
step:1270/2225 train_time:78824ms step_avg:62.07ms
step:1270/2225 val_loss:3.5134 train_time:78885ms step_avg:62.11ms
step:1271/2225 train_time:78909ms step_avg:62.08ms
step:1272/2225 train_time:78947ms step_avg:62.07ms
step:1273/2225 train_time:79010ms step_avg:62.07ms
step:1274/2225 train_time:79072ms step_avg:62.07ms
step:1275/2225 train_time:79134ms step_avg:62.07ms
step:1275/2225 val_loss:3.5111 train_time:79193ms step_avg:62.11ms
step:1276/2225 train_time:79218ms step_avg:62.08ms
step:1277/2225 train_time:79256ms step_avg:62.06ms
step:1278/2225 train_time:79318ms step_avg:62.06ms
step:1279/2225 train_time:79386ms step_avg:62.07ms
step:1280/2225 train_time:79446ms step_avg:62.07ms
step:1280/2225 val_loss:3.5126 train_time:79508ms step_avg:62.12ms
step:1281/2225 train_time:79532ms step_avg:62.09ms
step:1282/2225 train_time:79574ms step_avg:62.07ms
step:1283/2225 train_time:79637ms step_avg:62.07ms
step:1284/2225 train_time:79698ms step_avg:62.07ms
step:1285/2225 train_time:79759ms step_avg:62.07ms
step:1285/2225 val_loss:3.5085 train_time:79819ms step_avg:62.12ms
step:1286/2225 train_time:79843ms step_avg:62.09ms
step:1287/2225 train_time:79882ms step_avg:62.07ms
step:1288/2225 train_time:79944ms step_avg:62.07ms
step:1289/2225 train_time:80008ms step_avg:62.07ms
step:1290/2225 train_time:80067ms step_avg:62.07ms
step:1290/2225 val_loss:3.5087 train_time:80129ms step_avg:62.12ms
step:1291/2225 train_time:80155ms step_avg:62.09ms
step:1292/2225 train_time:80192ms step_avg:62.07ms
step:1293/2225 train_time:80256ms step_avg:62.07ms
step:1294/2225 train_time:80319ms step_avg:62.07ms
step:1295/2225 train_time:80381ms step_avg:62.07ms
step:1295/2225 val_loss:3.5065 train_time:80441ms step_avg:62.12ms
step:1296/2225 train_time:80466ms step_avg:62.09ms
step:1297/2225 train_time:80504ms step_avg:62.07ms
step:1298/2225 train_time:80567ms step_avg:62.07ms
step:1299/2225 train_time:80631ms step_avg:62.07ms
step:1300/2225 train_time:80692ms step_avg:62.07ms
step:1300/2225 val_loss:3.5051 train_time:80754ms step_avg:62.12ms
step:1301/2225 train_time:80777ms step_avg:62.09ms
step:1302/2225 train_time:80815ms step_avg:62.07ms
step:1303/2225 train_time:80880ms step_avg:62.07ms
step:1304/2225 train_time:80944ms step_avg:62.07ms
step:1305/2225 train_time:81006ms step_avg:62.07ms
step:1305/2225 val_loss:3.5017 train_time:81066ms step_avg:62.12ms
step:1306/2225 train_time:81092ms step_avg:62.09ms
step:1307/2225 train_time:81130ms step_avg:62.07ms
step:1308/2225 train_time:81191ms step_avg:62.07ms
step:1309/2225 train_time:81257ms step_avg:62.08ms
step:1310/2225 train_time:81318ms step_avg:62.07ms
step:1310/2225 val_loss:3.5036 train_time:81379ms step_avg:62.12ms
step:1311/2225 train_time:81403ms step_avg:62.09ms
step:1312/2225 train_time:81441ms step_avg:62.07ms
step:1313/2225 train_time:81506ms step_avg:62.08ms
step:1314/2225 train_time:81567ms step_avg:62.08ms
step:1315/2225 train_time:81629ms step_avg:62.07ms
step:1315/2225 val_loss:3.4985 train_time:81689ms step_avg:62.12ms
step:1316/2225 train_time:81714ms step_avg:62.09ms
step:1317/2225 train_time:81756ms step_avg:62.08ms
step:1318/2225 train_time:81818ms step_avg:62.08ms
step:1319/2225 train_time:81883ms step_avg:62.08ms
step:1320/2225 train_time:81943ms step_avg:62.08ms
step:1320/2225 val_loss:3.4982 train_time:82005ms step_avg:62.12ms
step:1321/2225 train_time:82028ms step_avg:62.10ms
step:1322/2225 train_time:82070ms step_avg:62.08ms
step:1323/2225 train_time:82135ms step_avg:62.08ms
step:1324/2225 train_time:82198ms step_avg:62.08ms
step:1325/2225 train_time:82259ms step_avg:62.08ms
step:1325/2225 val_loss:3.4973 train_time:82319ms step_avg:62.13ms
step:1326/2225 train_time:82344ms step_avg:62.10ms
step:1327/2225 train_time:82382ms step_avg:62.08ms
step:1328/2225 train_time:82446ms step_avg:62.08ms
step:1329/2225 train_time:82512ms step_avg:62.09ms
step:1330/2225 train_time:82571ms step_avg:62.08ms
step:1330/2225 val_loss:3.4958 train_time:82633ms step_avg:62.13ms
step:1331/2225 train_time:82657ms step_avg:62.10ms
step:1332/2225 train_time:82695ms step_avg:62.08ms
step:1333/2225 train_time:82760ms step_avg:62.09ms
step:1334/2225 train_time:82821ms step_avg:62.08ms
step:1335/2225 train_time:82882ms step_avg:62.08ms
step:1335/2225 val_loss:3.4929 train_time:82942ms step_avg:62.13ms
step:1336/2225 train_time:82967ms step_avg:62.10ms
step:1337/2225 train_time:83006ms step_avg:62.08ms
step:1338/2225 train_time:83070ms step_avg:62.08ms
step:1339/2225 train_time:83135ms step_avg:62.09ms
step:1340/2225 train_time:83195ms step_avg:62.09ms
step:1340/2225 val_loss:3.4956 train_time:83257ms step_avg:62.13ms
step:1341/2225 train_time:83280ms step_avg:62.10ms
step:1342/2225 train_time:83318ms step_avg:62.08ms
step:1343/2225 train_time:83383ms step_avg:62.09ms
step:1344/2225 train_time:83445ms step_avg:62.09ms
step:1345/2225 train_time:83508ms step_avg:62.09ms
step:1345/2225 val_loss:3.4912 train_time:83567ms step_avg:62.13ms
step:1346/2225 train_time:83592ms step_avg:62.10ms
step:1347/2225 train_time:83632ms step_avg:62.09ms
step:1348/2225 train_time:83694ms step_avg:62.09ms
step:1349/2225 train_time:83758ms step_avg:62.09ms
step:1350/2225 train_time:83817ms step_avg:62.09ms
step:1350/2225 val_loss:3.4887 train_time:83880ms step_avg:62.13ms
step:1351/2225 train_time:83903ms step_avg:62.10ms
step:1352/2225 train_time:83941ms step_avg:62.09ms
step:1353/2225 train_time:84005ms step_avg:62.09ms
step:1354/2225 train_time:84070ms step_avg:62.09ms
step:1355/2225 train_time:84132ms step_avg:62.09ms
step:1355/2225 val_loss:3.4871 train_time:84192ms step_avg:62.13ms
step:1356/2225 train_time:84217ms step_avg:62.11ms
step:1357/2225 train_time:84256ms step_avg:62.09ms
step:1358/2225 train_time:84320ms step_avg:62.09ms
step:1359/2225 train_time:84386ms step_avg:62.09ms
step:1360/2225 train_time:84446ms step_avg:62.09ms
step:1360/2225 val_loss:3.4868 train_time:84507ms step_avg:62.14ms
step:1361/2225 train_time:84532ms step_avg:62.11ms
step:1362/2225 train_time:84570ms step_avg:62.09ms
step:1363/2225 train_time:84635ms step_avg:62.09ms
step:1364/2225 train_time:84698ms step_avg:62.09ms
step:1365/2225 train_time:84762ms step_avg:62.10ms
step:1365/2225 val_loss:3.4847 train_time:84822ms step_avg:62.14ms
step:1366/2225 train_time:84847ms step_avg:62.11ms
step:1367/2225 train_time:84885ms step_avg:62.10ms
step:1368/2225 train_time:84946ms step_avg:62.10ms
step:1369/2225 train_time:85009ms step_avg:62.10ms
step:1370/2225 train_time:85068ms step_avg:62.09ms
step:1370/2225 val_loss:3.4839 train_time:85130ms step_avg:62.14ms
step:1371/2225 train_time:85153ms step_avg:62.11ms
step:1372/2225 train_time:85192ms step_avg:62.09ms
step:1373/2225 train_time:85257ms step_avg:62.10ms
step:1374/2225 train_time:85320ms step_avg:62.10ms
step:1375/2225 train_time:85382ms step_avg:62.10ms
step:1375/2225 val_loss:3.4818 train_time:85441ms step_avg:62.14ms
step:1376/2225 train_time:85466ms step_avg:62.11ms
step:1377/2225 train_time:85505ms step_avg:62.09ms
step:1378/2225 train_time:85567ms step_avg:62.09ms
step:1379/2225 train_time:85634ms step_avg:62.10ms
step:1380/2225 train_time:85695ms step_avg:62.10ms
step:1380/2225 val_loss:3.4811 train_time:85756ms step_avg:62.14ms
step:1381/2225 train_time:85779ms step_avg:62.11ms
step:1382/2225 train_time:85817ms step_avg:62.10ms
step:1383/2225 train_time:85883ms step_avg:62.10ms
step:1384/2225 train_time:85946ms step_avg:62.10ms
step:1385/2225 train_time:86008ms step_avg:62.10ms
step:1385/2225 val_loss:3.4793 train_time:86068ms step_avg:62.14ms
step:1386/2225 train_time:86093ms step_avg:62.12ms
step:1387/2225 train_time:86134ms step_avg:62.10ms
step:1388/2225 train_time:86196ms step_avg:62.10ms
step:1389/2225 train_time:86264ms step_avg:62.11ms
step:1390/2225 train_time:86323ms step_avg:62.10ms
step:1390/2225 val_loss:3.4789 train_time:86385ms step_avg:62.15ms
step:1391/2225 train_time:86408ms step_avg:62.12ms
step:1392/2225 train_time:86446ms step_avg:62.10ms
step:1393/2225 train_time:86509ms step_avg:62.10ms
step:1394/2225 train_time:86573ms step_avg:62.10ms
step:1395/2225 train_time:86635ms step_avg:62.10ms
step:1395/2225 val_loss:3.4759 train_time:86694ms step_avg:62.15ms
step:1396/2225 train_time:86719ms step_avg:62.12ms
step:1397/2225 train_time:86758ms step_avg:62.10ms
step:1398/2225 train_time:86820ms step_avg:62.10ms
step:1399/2225 train_time:86883ms step_avg:62.10ms
step:1400/2225 train_time:86943ms step_avg:62.10ms
step:1400/2225 val_loss:3.4750 train_time:87004ms step_avg:62.15ms
step:1401/2225 train_time:87027ms step_avg:62.12ms
step:1402/2225 train_time:87064ms step_avg:62.10ms
step:1403/2225 train_time:87129ms step_avg:62.10ms
step:1404/2225 train_time:87194ms step_avg:62.10ms
step:1405/2225 train_time:87256ms step_avg:62.10ms
step:1405/2225 val_loss:3.4761 train_time:87316ms step_avg:62.15ms
step:1406/2225 train_time:87340ms step_avg:62.12ms
step:1407/2225 train_time:87379ms step_avg:62.10ms
step:1408/2225 train_time:87444ms step_avg:62.10ms
step:1409/2225 train_time:87508ms step_avg:62.11ms
step:1410/2225 train_time:87568ms step_avg:62.11ms
step:1410/2225 val_loss:3.4723 train_time:87630ms step_avg:62.15ms
step:1411/2225 train_time:87654ms step_avg:62.12ms
step:1412/2225 train_time:87693ms step_avg:62.11ms
step:1413/2225 train_time:87757ms step_avg:62.11ms
step:1414/2225 train_time:87818ms step_avg:62.11ms
step:1415/2225 train_time:87880ms step_avg:62.11ms
step:1415/2225 val_loss:3.4701 train_time:87939ms step_avg:62.15ms
step:1416/2225 train_time:87964ms step_avg:62.12ms
step:1417/2225 train_time:88003ms step_avg:62.11ms
step:1418/2225 train_time:88065ms step_avg:62.11ms
step:1419/2225 train_time:88130ms step_avg:62.11ms
step:1420/2225 train_time:88190ms step_avg:62.11ms
step:1420/2225 val_loss:3.4703 train_time:88251ms step_avg:62.15ms
step:1421/2225 train_time:88275ms step_avg:62.12ms
step:1422/2225 train_time:88315ms step_avg:62.11ms
step:1423/2225 train_time:88379ms step_avg:62.11ms
step:1424/2225 train_time:88443ms step_avg:62.11ms
step:1425/2225 train_time:88505ms step_avg:62.11ms
step:1425/2225 val_loss:3.4685 train_time:88565ms step_avg:62.15ms
step:1426/2225 train_time:88590ms step_avg:62.12ms
step:1427/2225 train_time:88629ms step_avg:62.11ms
step:1428/2225 train_time:88693ms step_avg:62.11ms
step:1429/2225 train_time:88759ms step_avg:62.11ms
step:1430/2225 train_time:88818ms step_avg:62.11ms
step:1430/2225 val_loss:3.4662 train_time:88881ms step_avg:62.15ms
step:1431/2225 train_time:88904ms step_avg:62.13ms
step:1432/2225 train_time:88944ms step_avg:62.11ms
step:1433/2225 train_time:89008ms step_avg:62.11ms
step:1434/2225 train_time:89071ms step_avg:62.11ms
step:1435/2225 train_time:89133ms step_avg:62.11ms
step:1435/2225 val_loss:3.4650 train_time:89192ms step_avg:62.15ms
step:1436/2225 train_time:89217ms step_avg:62.13ms
step:1437/2225 train_time:89258ms step_avg:62.11ms
step:1438/2225 train_time:89320ms step_avg:62.11ms
step:1439/2225 train_time:89386ms step_avg:62.12ms
step:1440/2225 train_time:89447ms step_avg:62.12ms
step:1440/2225 val_loss:3.4637 train_time:89509ms step_avg:62.16ms
step:1441/2225 train_time:89532ms step_avg:62.13ms
step:1442/2225 train_time:89570ms step_avg:62.12ms
step:1443/2225 train_time:89635ms step_avg:62.12ms
step:1444/2225 train_time:89698ms step_avg:62.12ms
step:1445/2225 train_time:89760ms step_avg:62.12ms
step:1445/2225 val_loss:3.4627 train_time:89820ms step_avg:62.16ms
step:1446/2225 train_time:89844ms step_avg:62.13ms
step:1447/2225 train_time:89883ms step_avg:62.12ms
step:1448/2225 train_time:89947ms step_avg:62.12ms
step:1449/2225 train_time:90012ms step_avg:62.12ms
step:1450/2225 train_time:90072ms step_avg:62.12ms
step:1450/2225 val_loss:3.4615 train_time:90135ms step_avg:62.16ms
step:1451/2225 train_time:90158ms step_avg:62.14ms
step:1452/2225 train_time:90196ms step_avg:62.12ms
step:1453/2225 train_time:90261ms step_avg:62.12ms
step:1454/2225 train_time:90323ms step_avg:62.12ms
step:1455/2225 train_time:90386ms step_avg:62.12ms
step:1455/2225 val_loss:3.4583 train_time:90445ms step_avg:62.16ms
step:1456/2225 train_time:90470ms step_avg:62.14ms
step:1457/2225 train_time:90510ms step_avg:62.12ms
step:1458/2225 train_time:90573ms step_avg:62.12ms
step:1459/2225 train_time:90640ms step_avg:62.12ms
step:1460/2225 train_time:90701ms step_avg:62.12ms
step:1460/2225 val_loss:3.4507 train_time:90764ms step_avg:62.17ms
step:1461/2225 train_time:90787ms step_avg:62.14ms
step:1462/2225 train_time:90824ms step_avg:62.12ms
step:1463/2225 train_time:90890ms step_avg:62.13ms
step:1464/2225 train_time:90955ms step_avg:62.13ms
step:1465/2225 train_time:91016ms step_avg:62.13ms
step:1465/2225 val_loss:3.4488 train_time:91076ms step_avg:62.17ms
step:1466/2225 train_time:91100ms step_avg:62.14ms
step:1467/2225 train_time:91140ms step_avg:62.13ms
step:1468/2225 train_time:91203ms step_avg:62.13ms
step:1469/2225 train_time:91266ms step_avg:62.13ms
step:1470/2225 train_time:91326ms step_avg:62.13ms
step:1470/2225 val_loss:3.4465 train_time:91388ms step_avg:62.17ms
step:1471/2225 train_time:91413ms step_avg:62.14ms
step:1472/2225 train_time:91450ms step_avg:62.13ms
step:1473/2225 train_time:91516ms step_avg:62.13ms
step:1474/2225 train_time:91577ms step_avg:62.13ms
step:1475/2225 train_time:91640ms step_avg:62.13ms
step:1475/2225 val_loss:3.4448 train_time:91700ms step_avg:62.17ms
step:1476/2225 train_time:91725ms step_avg:62.14ms
step:1477/2225 train_time:91766ms step_avg:62.13ms
step:1478/2225 train_time:91828ms step_avg:62.13ms
step:1479/2225 train_time:91893ms step_avg:62.13ms
step:1480/2225 train_time:91952ms step_avg:62.13ms
step:1480/2225 val_loss:3.4448 train_time:92014ms step_avg:62.17ms
step:1481/2225 train_time:92037ms step_avg:62.15ms
step:1482/2225 train_time:92078ms step_avg:62.13ms
step:1483/2225 train_time:92142ms step_avg:62.13ms
step:1484/2225 train_time:92208ms step_avg:62.13ms
step:1485/2225 train_time:92271ms step_avg:62.14ms
step:1485/2225 val_loss:3.4411 train_time:92330ms step_avg:62.18ms
step:1486/2225 train_time:92355ms step_avg:62.15ms
step:1487/2225 train_time:92396ms step_avg:62.14ms
step:1488/2225 train_time:92459ms step_avg:62.14ms
step:1489/2225 train_time:92523ms step_avg:62.14ms
step:1490/2225 train_time:92583ms step_avg:62.14ms
step:1490/2225 val_loss:3.4407 train_time:92646ms step_avg:62.18ms
step:1491/2225 train_time:92672ms step_avg:62.15ms
step:1492/2225 train_time:92708ms step_avg:62.14ms
step:1493/2225 train_time:92774ms step_avg:62.14ms
step:1494/2225 train_time:92840ms step_avg:62.14ms
step:1495/2225 train_time:92902ms step_avg:62.14ms
step:1495/2225 val_loss:3.4390 train_time:92961ms step_avg:62.18ms
step:1496/2225 train_time:92986ms step_avg:62.16ms
step:1497/2225 train_time:93026ms step_avg:62.14ms
step:1498/2225 train_time:93089ms step_avg:62.14ms
step:1499/2225 train_time:93153ms step_avg:62.14ms
step:1500/2225 train_time:93215ms step_avg:62.14ms
step:1500/2225 val_loss:3.4377 train_time:93276ms step_avg:62.18ms
step:1501/2225 train_time:93299ms step_avg:62.16ms
step:1502/2225 train_time:93338ms step_avg:62.14ms
step:1503/2225 train_time:93403ms step_avg:62.14ms
step:1504/2225 train_time:93466ms step_avg:62.14ms
step:1505/2225 train_time:93528ms step_avg:62.14ms
step:1505/2225 val_loss:3.4359 train_time:93589ms step_avg:62.19ms
step:1506/2225 train_time:93615ms step_avg:62.16ms
step:1507/2225 train_time:93654ms step_avg:62.15ms
step:1508/2225 train_time:93717ms step_avg:62.15ms
step:1509/2225 train_time:93783ms step_avg:62.15ms
step:1510/2225 train_time:93843ms step_avg:62.15ms
step:1510/2225 val_loss:3.4347 train_time:93905ms step_avg:62.19ms
step:1511/2225 train_time:93928ms step_avg:62.16ms
step:1512/2225 train_time:93969ms step_avg:62.15ms
step:1513/2225 train_time:94034ms step_avg:62.15ms
step:1514/2225 train_time:94096ms step_avg:62.15ms
step:1515/2225 train_time:94160ms step_avg:62.15ms
step:1515/2225 val_loss:3.4333 train_time:94219ms step_avg:62.19ms
step:1516/2225 train_time:94244ms step_avg:62.17ms
step:1517/2225 train_time:94285ms step_avg:62.15ms
step:1518/2225 train_time:94349ms step_avg:62.15ms
step:1519/2225 train_time:94414ms step_avg:62.16ms
step:1520/2225 train_time:94474ms step_avg:62.15ms
step:1520/2225 val_loss:3.4324 train_time:94537ms step_avg:62.20ms
step:1521/2225 train_time:94560ms step_avg:62.17ms
step:1522/2225 train_time:94599ms step_avg:62.15ms
step:1523/2225 train_time:94666ms step_avg:62.16ms
step:1524/2225 train_time:94730ms step_avg:62.16ms
step:1525/2225 train_time:94793ms step_avg:62.16ms
step:1525/2225 val_loss:3.4308 train_time:94854ms step_avg:62.20ms
step:1526/2225 train_time:94880ms step_avg:62.18ms
step:1527/2225 train_time:94918ms step_avg:62.16ms
step:1528/2225 train_time:94984ms step_avg:62.16ms
step:1529/2225 train_time:95049ms step_avg:62.16ms
step:1530/2225 train_time:95109ms step_avg:62.16ms
step:1530/2225 val_loss:3.4345 train_time:95171ms step_avg:62.20ms
step:1531/2225 train_time:95194ms step_avg:62.18ms
step:1532/2225 train_time:95234ms step_avg:62.16ms
step:1533/2225 train_time:95301ms step_avg:62.17ms
step:1534/2225 train_time:95364ms step_avg:62.17ms
step:1535/2225 train_time:95426ms step_avg:62.17ms
step:1535/2225 val_loss:3.4283 train_time:95486ms step_avg:62.21ms
step:1536/2225 train_time:95510ms step_avg:62.18ms
step:1537/2225 train_time:95550ms step_avg:62.17ms
step:1538/2225 train_time:95613ms step_avg:62.17ms
step:1539/2225 train_time:95677ms step_avg:62.17ms
step:1540/2225 train_time:95738ms step_avg:62.17ms
step:1540/2225 val_loss:3.4268 train_time:95800ms step_avg:62.21ms
step:1541/2225 train_time:95823ms step_avg:62.18ms
step:1542/2225 train_time:95864ms step_avg:62.17ms
step:1543/2225 train_time:95928ms step_avg:62.17ms
step:1544/2225 train_time:95989ms step_avg:62.17ms
step:1545/2225 train_time:96050ms step_avg:62.17ms
step:1545/2225 val_loss:3.4256 train_time:96110ms step_avg:62.21ms
step:1546/2225 train_time:96135ms step_avg:62.18ms
step:1547/2225 train_time:96175ms step_avg:62.17ms
step:1548/2225 train_time:96237ms step_avg:62.17ms
step:1549/2225 train_time:96303ms step_avg:62.17ms
step:1550/2225 train_time:96363ms step_avg:62.17ms
step:1550/2225 val_loss:3.4239 train_time:96425ms step_avg:62.21ms
step:1551/2225 train_time:96448ms step_avg:62.18ms
step:1552/2225 train_time:96489ms step_avg:62.17ms
step:1553/2225 train_time:96555ms step_avg:62.17ms
step:1554/2225 train_time:96618ms step_avg:62.17ms
step:1555/2225 train_time:96680ms step_avg:62.17ms
step:1555/2225 val_loss:3.4224 train_time:96739ms step_avg:62.21ms
step:1556/2225 train_time:96764ms step_avg:62.19ms
step:1557/2225 train_time:96804ms step_avg:62.17ms
step:1558/2225 train_time:96865ms step_avg:62.17ms
step:1559/2225 train_time:96928ms step_avg:62.17ms
step:1560/2225 train_time:96988ms step_avg:62.17ms
step:1560/2225 val_loss:3.4206 train_time:97050ms step_avg:62.21ms
step:1561/2225 train_time:97073ms step_avg:62.19ms
step:1562/2225 train_time:97114ms step_avg:62.17ms
step:1563/2225 train_time:97179ms step_avg:62.17ms
step:1564/2225 train_time:97243ms step_avg:62.18ms
step:1565/2225 train_time:97305ms step_avg:62.18ms
step:1565/2225 val_loss:3.4196 train_time:97365ms step_avg:62.21ms
step:1566/2225 train_time:97390ms step_avg:62.19ms
step:1567/2225 train_time:97432ms step_avg:62.18ms
step:1568/2225 train_time:97496ms step_avg:62.18ms
step:1569/2225 train_time:97561ms step_avg:62.18ms
step:1570/2225 train_time:97622ms step_avg:62.18ms
step:1570/2225 val_loss:3.4186 train_time:97684ms step_avg:62.22ms
step:1571/2225 train_time:97707ms step_avg:62.19ms
step:1572/2225 train_time:97747ms step_avg:62.18ms
step:1573/2225 train_time:97814ms step_avg:62.18ms
step:1574/2225 train_time:97878ms step_avg:62.18ms
step:1575/2225 train_time:97942ms step_avg:62.19ms
step:1575/2225 val_loss:3.4171 train_time:98003ms step_avg:62.22ms
step:1576/2225 train_time:98028ms step_avg:62.20ms
step:1577/2225 train_time:98068ms step_avg:62.19ms
step:1578/2225 train_time:98132ms step_avg:62.19ms
step:1579/2225 train_time:98197ms step_avg:62.19ms
step:1580/2225 train_time:98257ms step_avg:62.19ms
step:1580/2225 val_loss:3.4169 train_time:98319ms step_avg:62.23ms
step:1581/2225 train_time:98342ms step_avg:62.20ms
step:1582/2225 train_time:98380ms step_avg:62.19ms
step:1583/2225 train_time:98447ms step_avg:62.19ms
step:1584/2225 train_time:98512ms step_avg:62.19ms
step:1585/2225 train_time:98574ms step_avg:62.19ms
step:1585/2225 val_loss:3.4145 train_time:98634ms step_avg:62.23ms
step:1586/2225 train_time:98658ms step_avg:62.21ms
step:1587/2225 train_time:98698ms step_avg:62.19ms
step:1588/2225 train_time:98762ms step_avg:62.19ms
step:1589/2225 train_time:98826ms step_avg:62.19ms
step:1590/2225 train_time:98886ms step_avg:62.19ms
step:1590/2225 val_loss:3.4135 train_time:98948ms step_avg:62.23ms
step:1591/2225 train_time:98971ms step_avg:62.21ms
step:1592/2225 train_time:99009ms step_avg:62.19ms
step:1593/2225 train_time:99074ms step_avg:62.19ms
step:1594/2225 train_time:99136ms step_avg:62.19ms
step:1595/2225 train_time:99199ms step_avg:62.19ms
step:1595/2225 val_loss:3.4123 train_time:99259ms step_avg:62.23ms
step:1596/2225 train_time:99283ms step_avg:62.21ms
step:1597/2225 train_time:99322ms step_avg:62.19ms
step:1598/2225 train_time:99386ms step_avg:62.19ms
step:1599/2225 train_time:99450ms step_avg:62.20ms
step:1600/2225 train_time:99510ms step_avg:62.19ms
step:1600/2225 val_loss:3.4112 train_time:99572ms step_avg:62.23ms
step:1601/2225 train_time:99595ms step_avg:62.21ms
step:1602/2225 train_time:99635ms step_avg:62.19ms
step:1603/2225 train_time:99699ms step_avg:62.20ms
step:1604/2225 train_time:99762ms step_avg:62.20ms
step:1605/2225 train_time:99824ms step_avg:62.20ms
step:1605/2225 val_loss:3.4091 train_time:99884ms step_avg:62.23ms
step:1606/2225 train_time:99909ms step_avg:62.21ms
step:1607/2225 train_time:99947ms step_avg:62.19ms
step:1608/2225 train_time:100011ms step_avg:62.20ms
step:1609/2225 train_time:100075ms step_avg:62.20ms
step:1610/2225 train_time:100135ms step_avg:62.20ms
step:1610/2225 val_loss:3.4086 train_time:100196ms step_avg:62.23ms
step:1611/2225 train_time:100220ms step_avg:62.21ms
step:1612/2225 train_time:100258ms step_avg:62.19ms
step:1613/2225 train_time:100324ms step_avg:62.20ms
step:1614/2225 train_time:100387ms step_avg:62.20ms
step:1615/2225 train_time:100449ms step_avg:62.20ms
step:1615/2225 val_loss:3.4069 train_time:100508ms step_avg:62.23ms
step:1616/2225 train_time:100533ms step_avg:62.21ms
step:1617/2225 train_time:100572ms step_avg:62.20ms
step:1618/2225 train_time:100637ms step_avg:62.20ms
step:1619/2225 train_time:100702ms step_avg:62.20ms
step:1620/2225 train_time:100762ms step_avg:62.20ms
step:1620/2225 val_loss:3.4063 train_time:100823ms step_avg:62.24ms
step:1621/2225 train_time:100846ms step_avg:62.21ms
step:1622/2225 train_time:100887ms step_avg:62.20ms
step:1623/2225 train_time:100952ms step_avg:62.20ms
step:1624/2225 train_time:101015ms step_avg:62.20ms
step:1625/2225 train_time:101078ms step_avg:62.20ms
step:1625/2225 val_loss:3.4047 train_time:101137ms step_avg:62.24ms
step:1626/2225 train_time:101162ms step_avg:62.22ms
step:1627/2225 train_time:101203ms step_avg:62.20ms
step:1628/2225 train_time:101268ms step_avg:62.20ms
step:1629/2225 train_time:101332ms step_avg:62.20ms
step:1630/2225 train_time:101392ms step_avg:62.20ms
step:1630/2225 val_loss:3.4032 train_time:101454ms step_avg:62.24ms
step:1631/2225 train_time:101477ms step_avg:62.22ms
step:1632/2225 train_time:101515ms step_avg:62.20ms
step:1633/2225 train_time:101581ms step_avg:62.21ms
step:1634/2225 train_time:101647ms step_avg:62.21ms
step:1635/2225 train_time:101709ms step_avg:62.21ms
step:1635/2225 val_loss:3.4016 train_time:101769ms step_avg:62.24ms
step:1636/2225 train_time:101793ms step_avg:62.22ms
step:1637/2225 train_time:101834ms step_avg:62.21ms
step:1638/2225 train_time:101897ms step_avg:62.21ms
step:1639/2225 train_time:101960ms step_avg:62.21ms
step:1640/2225 train_time:102020ms step_avg:62.21ms
step:1640/2225 val_loss:3.4009 train_time:102081ms step_avg:62.24ms
step:1641/2225 train_time:102104ms step_avg:62.22ms
step:1642/2225 train_time:102145ms step_avg:62.21ms
step:1643/2225 train_time:102209ms step_avg:62.21ms
step:1644/2225 train_time:102272ms step_avg:62.21ms
step:1645/2225 train_time:102335ms step_avg:62.21ms
step:1645/2225 val_loss:3.3995 train_time:102395ms step_avg:62.25ms
step:1646/2225 train_time:102420ms step_avg:62.22ms
step:1647/2225 train_time:102460ms step_avg:62.21ms
step:1648/2225 train_time:102523ms step_avg:62.21ms
step:1649/2225 train_time:102589ms step_avg:62.21ms
step:1650/2225 train_time:102649ms step_avg:62.21ms
step:1650/2225 val_loss:3.3982 train_time:102711ms step_avg:62.25ms
step:1651/2225 train_time:102734ms step_avg:62.23ms
step:1652/2225 train_time:102773ms step_avg:62.21ms
step:1653/2225 train_time:102838ms step_avg:62.21ms
step:1654/2225 train_time:102900ms step_avg:62.21ms
step:1655/2225 train_time:102961ms step_avg:62.21ms
step:1655/2225 val_loss:3.3971 train_time:103022ms step_avg:62.25ms
step:1656/2225 train_time:103047ms step_avg:62.23ms
step:1657/2225 train_time:103087ms step_avg:62.21ms
step:1658/2225 train_time:103148ms step_avg:62.21ms
step:1659/2225 train_time:103215ms step_avg:62.22ms
step:1660/2225 train_time:103275ms step_avg:62.21ms
step:1660/2225 val_loss:3.3954 train_time:103337ms step_avg:62.25ms
step:1661/2225 train_time:103362ms step_avg:62.23ms
step:1662/2225 train_time:103400ms step_avg:62.21ms
step:1663/2225 train_time:103465ms step_avg:62.22ms
step:1664/2225 train_time:103529ms step_avg:62.22ms
step:1665/2225 train_time:103592ms step_avg:62.22ms
step:1665/2225 val_loss:3.3941 train_time:103652ms step_avg:62.25ms
step:1666/2225 train_time:103677ms step_avg:62.23ms
step:1667/2225 train_time:103716ms step_avg:62.22ms
step:1668/2225 train_time:103779ms step_avg:62.22ms
step:1669/2225 train_time:103844ms step_avg:62.22ms
step:1670/2225 train_time:103904ms step_avg:62.22ms
step:1670/2225 val_loss:3.3930 train_time:103967ms step_avg:62.26ms
step:1671/2225 train_time:103990ms step_avg:62.23ms
step:1672/2225 train_time:104030ms step_avg:62.22ms
step:1673/2225 train_time:104096ms step_avg:62.22ms
step:1674/2225 train_time:104158ms step_avg:62.22ms
step:1675/2225 train_time:104220ms step_avg:62.22ms
step:1675/2225 val_loss:3.3922 train_time:104281ms step_avg:62.26ms
step:1676/2225 train_time:104305ms step_avg:62.23ms
step:1677/2225 train_time:104345ms step_avg:62.22ms
step:1678/2225 train_time:104409ms step_avg:62.22ms
step:1679/2225 train_time:104474ms step_avg:62.22ms
step:1680/2225 train_time:104536ms step_avg:62.22ms
step:1680/2225 val_loss:3.3919 train_time:104598ms step_avg:62.26ms
step:1681/2225 train_time:104621ms step_avg:62.24ms
step:1682/2225 train_time:104660ms step_avg:62.22ms
step:1683/2225 train_time:104727ms step_avg:62.23ms
step:1684/2225 train_time:104790ms step_avg:62.23ms
step:1685/2225 train_time:104852ms step_avg:62.23ms
step:1685/2225 val_loss:3.3899 train_time:104912ms step_avg:62.26ms
step:1686/2225 train_time:104936ms step_avg:62.24ms
step:1687/2225 train_time:104976ms step_avg:62.23ms
step:1688/2225 train_time:105037ms step_avg:62.23ms
step:1689/2225 train_time:105102ms step_avg:62.23ms
step:1690/2225 train_time:105163ms step_avg:62.23ms
step:1690/2225 val_loss:3.3883 train_time:105224ms step_avg:62.26ms
step:1691/2225 train_time:105247ms step_avg:62.24ms
step:1692/2225 train_time:105288ms step_avg:62.23ms
step:1693/2225 train_time:105355ms step_avg:62.23ms
step:1694/2225 train_time:105418ms step_avg:62.23ms
step:1695/2225 train_time:105480ms step_avg:62.23ms
step:1695/2225 val_loss:3.3871 train_time:105540ms step_avg:62.27ms
step:1696/2225 train_time:105565ms step_avg:62.24ms
step:1697/2225 train_time:105606ms step_avg:62.23ms
step:1698/2225 train_time:105670ms step_avg:62.23ms
step:1699/2225 train_time:105734ms step_avg:62.23ms
step:1700/2225 train_time:105795ms step_avg:62.23ms
step:1700/2225 val_loss:3.3859 train_time:105856ms step_avg:62.27ms
step:1701/2225 train_time:105880ms step_avg:62.25ms
step:1702/2225 train_time:105919ms step_avg:62.23ms
step:1703/2225 train_time:105984ms step_avg:62.23ms
step:1704/2225 train_time:106050ms step_avg:62.24ms
step:1705/2225 train_time:106113ms step_avg:62.24ms
step:1705/2225 val_loss:3.3846 train_time:106172ms step_avg:62.27ms
step:1706/2225 train_time:106197ms step_avg:62.25ms
step:1707/2225 train_time:106238ms step_avg:62.24ms
step:1708/2225 train_time:106302ms step_avg:62.24ms
step:1709/2225 train_time:106366ms step_avg:62.24ms
step:1710/2225 train_time:106426ms step_avg:62.24ms
step:1710/2225 val_loss:3.3836 train_time:106488ms step_avg:62.27ms
step:1711/2225 train_time:106511ms step_avg:62.25ms
step:1712/2225 train_time:106551ms step_avg:62.24ms
step:1713/2225 train_time:106617ms step_avg:62.24ms
step:1714/2225 train_time:106680ms step_avg:62.24ms
step:1715/2225 train_time:106742ms step_avg:62.24ms
step:1715/2225 val_loss:3.3824 train_time:106802ms step_avg:62.28ms
step:1716/2225 train_time:106827ms step_avg:62.25ms
step:1717/2225 train_time:106867ms step_avg:62.24ms
step:1718/2225 train_time:106931ms step_avg:62.24ms
step:1719/2225 train_time:106995ms step_avg:62.24ms
step:1720/2225 train_time:107055ms step_avg:62.24ms
step:1720/2225 val_loss:3.3820 train_time:107117ms step_avg:62.28ms
step:1721/2225 train_time:107140ms step_avg:62.25ms
step:1722/2225 train_time:107178ms step_avg:62.24ms
step:1723/2225 train_time:107242ms step_avg:62.24ms
step:1724/2225 train_time:107304ms step_avg:62.24ms
step:1725/2225 train_time:107367ms step_avg:62.24ms
step:1725/2225 val_loss:3.3797 train_time:107427ms step_avg:62.28ms
step:1726/2225 train_time:107451ms step_avg:62.25ms
step:1727/2225 train_time:107493ms step_avg:62.24ms
step:1728/2225 train_time:107557ms step_avg:62.24ms
step:1729/2225 train_time:107623ms step_avg:62.25ms
step:1730/2225 train_time:107683ms step_avg:62.24ms
step:1730/2225 val_loss:3.3786 train_time:107745ms step_avg:62.28ms
step:1731/2225 train_time:107769ms step_avg:62.26ms
step:1732/2225 train_time:107808ms step_avg:62.24ms
step:1733/2225 train_time:107874ms step_avg:62.25ms
step:1734/2225 train_time:107937ms step_avg:62.25ms
step:1735/2225 train_time:107999ms step_avg:62.25ms
step:1735/2225 val_loss:3.3778 train_time:108058ms step_avg:62.28ms
step:1736/2225 train_time:108083ms step_avg:62.26ms
step:1737/2225 train_time:108125ms step_avg:62.25ms
step:1738/2225 train_time:108188ms step_avg:62.25ms
step:1739/2225 train_time:108253ms step_avg:62.25ms
step:1740/2225 train_time:108313ms step_avg:62.25ms
step:1740/2225 val_loss:3.3761 train_time:108375ms step_avg:62.28ms
step:1741/2225 train_time:108398ms step_avg:62.26ms
step:1742/2225 train_time:108437ms step_avg:62.25ms
step:1743/2225 train_time:108503ms step_avg:62.25ms
step:1744/2225 train_time:108564ms step_avg:62.25ms
step:1745/2225 train_time:108626ms step_avg:62.25ms
step:1745/2225 val_loss:3.3751 train_time:108687ms step_avg:62.28ms
step:1746/2225 train_time:108711ms step_avg:62.26ms
step:1747/2225 train_time:108750ms step_avg:62.25ms
step:1748/2225 train_time:108815ms step_avg:62.25ms
step:1749/2225 train_time:108881ms step_avg:62.25ms
step:1750/2225 train_time:108941ms step_avg:62.25ms
step:1750/2225 val_loss:3.3733 train_time:109003ms step_avg:62.29ms
step:1751/2225 train_time:109026ms step_avg:62.27ms
step:1752/2225 train_time:109066ms step_avg:62.25ms
step:1753/2225 train_time:109132ms step_avg:62.25ms
step:1754/2225 train_time:109195ms step_avg:62.25ms
step:1755/2225 train_time:109258ms step_avg:62.26ms
step:1755/2225 val_loss:3.3729 train_time:109317ms step_avg:62.29ms
step:1756/2225 train_time:109343ms step_avg:62.27ms
step:1757/2225 train_time:109381ms step_avg:62.25ms
step:1758/2225 train_time:109444ms step_avg:62.25ms
step:1759/2225 train_time:109509ms step_avg:62.26ms
step:1760/2225 train_time:109570ms step_avg:62.26ms
step:1760/2225 val_loss:3.3711 train_time:109633ms step_avg:62.29ms
step:1761/2225 train_time:109656ms step_avg:62.27ms
step:1762/2225 train_time:109695ms step_avg:62.26ms
step:1763/2225 train_time:109762ms step_avg:62.26ms
step:1764/2225 train_time:109827ms step_avg:62.26ms
step:1765/2225 train_time:109888ms step_avg:62.26ms
step:1765/2225 val_loss:3.3700 train_time:109948ms step_avg:62.29ms
step:1766/2225 train_time:109973ms step_avg:62.27ms
step:1767/2225 train_time:110012ms step_avg:62.26ms
step:1768/2225 train_time:110075ms step_avg:62.26ms
step:1769/2225 train_time:110140ms step_avg:62.26ms
step:1770/2225 train_time:110200ms step_avg:62.26ms
step:1770/2225 val_loss:3.3687 train_time:110262ms step_avg:62.29ms
step:1771/2225 train_time:110285ms step_avg:62.27ms
step:1772/2225 train_time:110323ms step_avg:62.26ms
step:1773/2225 train_time:110389ms step_avg:62.26ms
step:1774/2225 train_time:110452ms step_avg:62.26ms
step:1775/2225 train_time:110514ms step_avg:62.26ms
step:1775/2225 val_loss:3.3677 train_time:110574ms step_avg:62.30ms
step:1776/2225 train_time:110599ms step_avg:62.27ms
step:1777/2225 train_time:110638ms step_avg:62.26ms
step:1778/2225 train_time:110700ms step_avg:62.26ms
step:1779/2225 train_time:110765ms step_avg:62.26ms
step:1780/2225 train_time:110826ms step_avg:62.26ms
step:1780/2225 val_loss:3.3668 train_time:110887ms step_avg:62.30ms
step:1781/2225 train_time:110912ms step_avg:62.27ms
step:1782/2225 train_time:110950ms step_avg:62.26ms
step:1783/2225 train_time:111016ms step_avg:62.26ms
step:1784/2225 train_time:111079ms step_avg:62.26ms
step:1785/2225 train_time:111141ms step_avg:62.26ms
step:1785/2225 val_loss:3.3652 train_time:111200ms step_avg:62.30ms
step:1786/2225 train_time:111225ms step_avg:62.28ms
step:1787/2225 train_time:111264ms step_avg:62.26ms
step:1788/2225 train_time:111327ms step_avg:62.26ms
step:1789/2225 train_time:111393ms step_avg:62.27ms
step:1790/2225 train_time:111453ms step_avg:62.26ms
step:1790/2225 val_loss:3.3644 train_time:111515ms step_avg:62.30ms
step:1791/2225 train_time:111538ms step_avg:62.28ms
step:1792/2225 train_time:111580ms step_avg:62.27ms
step:1793/2225 train_time:111643ms step_avg:62.27ms
step:1794/2225 train_time:111706ms step_avg:62.27ms
step:1795/2225 train_time:111768ms step_avg:62.27ms
step:1795/2225 val_loss:3.3631 train_time:111828ms step_avg:62.30ms
step:1796/2225 train_time:111856ms step_avg:62.28ms
step:1797/2225 train_time:111893ms step_avg:62.27ms
step:1798/2225 train_time:111956ms step_avg:62.27ms
step:1799/2225 train_time:112020ms step_avg:62.27ms
step:1800/2225 train_time:112081ms step_avg:62.27ms
step:1800/2225 val_loss:3.3623 train_time:112143ms step_avg:62.30ms
step:1801/2225 train_time:112166ms step_avg:62.28ms
step:1802/2225 train_time:112207ms step_avg:62.27ms
step:1803/2225 train_time:112271ms step_avg:62.27ms
step:1804/2225 train_time:112335ms step_avg:62.27ms
step:1805/2225 train_time:112397ms step_avg:62.27ms
step:1805/2225 val_loss:3.3610 train_time:112457ms step_avg:62.30ms
step:1806/2225 train_time:112481ms step_avg:62.28ms
step:1807/2225 train_time:112520ms step_avg:62.27ms
step:1808/2225 train_time:112583ms step_avg:62.27ms
step:1809/2225 train_time:112649ms step_avg:62.27ms
step:1810/2225 train_time:112710ms step_avg:62.27ms
step:1810/2225 val_loss:3.3604 train_time:112772ms step_avg:62.30ms
step:1811/2225 train_time:112795ms step_avg:62.28ms
step:1812/2225 train_time:112836ms step_avg:62.27ms
step:1813/2225 train_time:112899ms step_avg:62.27ms
step:1814/2225 train_time:112961ms step_avg:62.27ms
step:1815/2225 train_time:113023ms step_avg:62.27ms
step:1815/2225 val_loss:3.3593 train_time:113083ms step_avg:62.30ms
step:1816/2225 train_time:113109ms step_avg:62.28ms
step:1817/2225 train_time:113148ms step_avg:62.27ms
step:1818/2225 train_time:113211ms step_avg:62.27ms
step:1819/2225 train_time:113276ms step_avg:62.27ms
step:1820/2225 train_time:113337ms step_avg:62.27ms
step:1820/2225 val_loss:3.3581 train_time:113399ms step_avg:62.31ms
step:1821/2225 train_time:113422ms step_avg:62.29ms
step:1822/2225 train_time:113461ms step_avg:62.27ms
step:1823/2225 train_time:113527ms step_avg:62.27ms
step:1824/2225 train_time:113591ms step_avg:62.28ms
step:1825/2225 train_time:113652ms step_avg:62.28ms
step:1825/2225 val_loss:3.3562 train_time:113712ms step_avg:62.31ms
step:1826/2225 train_time:113737ms step_avg:62.29ms
step:1827/2225 train_time:113777ms step_avg:62.28ms
step:1828/2225 train_time:113840ms step_avg:62.28ms
step:1829/2225 train_time:113903ms step_avg:62.28ms
step:1830/2225 train_time:113964ms step_avg:62.28ms
step:1830/2225 val_loss:3.3549 train_time:114026ms step_avg:62.31ms
step:1831/2225 train_time:114050ms step_avg:62.29ms
step:1832/2225 train_time:114089ms step_avg:62.28ms
step:1833/2225 train_time:114153ms step_avg:62.28ms
step:1834/2225 train_time:114216ms step_avg:62.28ms
step:1835/2225 train_time:114278ms step_avg:62.28ms
step:1835/2225 val_loss:3.3537 train_time:114338ms step_avg:62.31ms
step:1836/2225 train_time:114363ms step_avg:62.29ms
step:1837/2225 train_time:114405ms step_avg:62.28ms
step:1838/2225 train_time:114466ms step_avg:62.28ms
step:1839/2225 train_time:114530ms step_avg:62.28ms
step:1840/2225 train_time:114590ms step_avg:62.28ms
step:1840/2225 val_loss:3.3542 train_time:114652ms step_avg:62.31ms
step:1841/2225 train_time:114676ms step_avg:62.29ms
step:1842/2225 train_time:114715ms step_avg:62.28ms
step:1843/2225 train_time:114780ms step_avg:62.28ms
step:1844/2225 train_time:114843ms step_avg:62.28ms
step:1845/2225 train_time:114905ms step_avg:62.28ms
step:1845/2225 val_loss:3.3520 train_time:114965ms step_avg:62.31ms
step:1846/2225 train_time:114990ms step_avg:62.29ms
step:1847/2225 train_time:115029ms step_avg:62.28ms
step:1848/2225 train_time:115092ms step_avg:62.28ms
step:1849/2225 train_time:115156ms step_avg:62.28ms
step:1850/2225 train_time:115217ms step_avg:62.28ms
step:1850/2225 val_loss:3.3507 train_time:115278ms step_avg:62.31ms
step:1851/2225 train_time:115302ms step_avg:62.29ms
step:1852/2225 train_time:115341ms step_avg:62.28ms
step:1853/2225 train_time:115407ms step_avg:62.28ms
step:1854/2225 train_time:115469ms step_avg:62.28ms
step:1855/2225 train_time:115531ms step_avg:62.28ms
step:1855/2225 val_loss:3.3495 train_time:115591ms step_avg:62.31ms
step:1856/2225 train_time:115616ms step_avg:62.29ms
step:1857/2225 train_time:115655ms step_avg:62.28ms
step:1858/2225 train_time:115719ms step_avg:62.28ms
step:1859/2225 train_time:115783ms step_avg:62.28ms
step:1860/2225 train_time:115843ms step_avg:62.28ms
step:1860/2225 val_loss:3.3490 train_time:115905ms step_avg:62.31ms
step:1861/2225 train_time:115929ms step_avg:62.29ms
step:1862/2225 train_time:115967ms step_avg:62.28ms
step:1863/2225 train_time:116033ms step_avg:62.28ms
step:1864/2225 train_time:116096ms step_avg:62.28ms
step:1865/2225 train_time:116158ms step_avg:62.28ms
step:1865/2225 val_loss:3.3474 train_time:116218ms step_avg:62.32ms
step:1866/2225 train_time:116245ms step_avg:62.30ms
step:1867/2225 train_time:116283ms step_avg:62.28ms
step:1868/2225 train_time:116345ms step_avg:62.28ms
step:1869/2225 train_time:116410ms step_avg:62.28ms
step:1870/2225 train_time:116471ms step_avg:62.28ms
step:1870/2225 val_loss:3.3463 train_time:116533ms step_avg:62.32ms
step:1871/2225 train_time:116556ms step_avg:62.30ms
step:1872/2225 train_time:116594ms step_avg:62.28ms
step:1873/2225 train_time:116660ms step_avg:62.28ms
step:1874/2225 train_time:116721ms step_avg:62.28ms
step:1875/2225 train_time:116783ms step_avg:62.28ms
step:1875/2225 val_loss:3.3455 train_time:116843ms step_avg:62.32ms
step:1876/2225 train_time:116868ms step_avg:62.30ms
step:1877/2225 train_time:116907ms step_avg:62.28ms
step:1878/2225 train_time:116970ms step_avg:62.28ms
step:1879/2225 train_time:117034ms step_avg:62.29ms
step:1880/2225 train_time:117093ms step_avg:62.28ms
step:1880/2225 val_loss:3.3443 train_time:117156ms step_avg:62.32ms
step:1881/2225 train_time:117180ms step_avg:62.30ms
step:1882/2225 train_time:117220ms step_avg:62.28ms
step:1883/2225 train_time:117285ms step_avg:62.29ms
step:1884/2225 train_time:117348ms step_avg:62.29ms
step:1885/2225 train_time:117409ms step_avg:62.29ms
step:1885/2225 val_loss:3.3433 train_time:117469ms step_avg:62.32ms
step:1886/2225 train_time:117494ms step_avg:62.30ms
step:1887/2225 train_time:117535ms step_avg:62.29ms
step:1888/2225 train_time:117597ms step_avg:62.29ms
step:1889/2225 train_time:117661ms step_avg:62.29ms
step:1890/2225 train_time:117721ms step_avg:62.29ms
step:1890/2225 val_loss:3.3423 train_time:117783ms step_avg:62.32ms
step:1891/2225 train_time:117806ms step_avg:62.30ms
step:1892/2225 train_time:117848ms step_avg:62.29ms
step:1893/2225 train_time:117916ms step_avg:62.29ms
step:1894/2225 train_time:117978ms step_avg:62.29ms
step:1895/2225 train_time:118039ms step_avg:62.29ms
step:1895/2225 val_loss:3.3413 train_time:118099ms step_avg:62.32ms
step:1896/2225 train_time:118124ms step_avg:62.30ms
step:1897/2225 train_time:118164ms step_avg:62.29ms
step:1898/2225 train_time:118227ms step_avg:62.29ms
step:1899/2225 train_time:118293ms step_avg:62.29ms
step:1900/2225 train_time:118354ms step_avg:62.29ms
step:1900/2225 val_loss:3.3395 train_time:118416ms step_avg:62.32ms
step:1901/2225 train_time:118440ms step_avg:62.30ms
step:1902/2225 train_time:118478ms step_avg:62.29ms
step:1903/2225 train_time:118544ms step_avg:62.29ms
step:1904/2225 train_time:118606ms step_avg:62.29ms
step:1905/2225 train_time:118668ms step_avg:62.29ms
step:1905/2225 val_loss:3.3384 train_time:118727ms step_avg:62.32ms
step:1906/2225 train_time:118752ms step_avg:62.30ms
step:1907/2225 train_time:118791ms step_avg:62.29ms
step:1908/2225 train_time:118854ms step_avg:62.29ms
step:1909/2225 train_time:118919ms step_avg:62.29ms
step:1910/2225 train_time:118979ms step_avg:62.29ms
step:1910/2225 val_loss:3.3376 train_time:119041ms step_avg:62.33ms
step:1911/2225 train_time:119064ms step_avg:62.30ms
step:1912/2225 train_time:119103ms step_avg:62.29ms
step:1913/2225 train_time:119168ms step_avg:62.29ms
step:1914/2225 train_time:119232ms step_avg:62.29ms
step:1915/2225 train_time:119295ms step_avg:62.30ms
step:1915/2225 val_loss:3.3363 train_time:119356ms step_avg:62.33ms
step:1916/2225 train_time:119381ms step_avg:62.31ms
step:1917/2225 train_time:119420ms step_avg:62.30ms
step:1918/2225 train_time:119481ms step_avg:62.29ms
step:1919/2225 train_time:119545ms step_avg:62.30ms
step:1920/2225 train_time:119605ms step_avg:62.29ms
step:1920/2225 val_loss:3.3353 train_time:119668ms step_avg:62.33ms
step:1921/2225 train_time:119691ms step_avg:62.31ms
step:1922/2225 train_time:119732ms step_avg:62.30ms
step:1923/2225 train_time:119797ms step_avg:62.30ms
step:1924/2225 train_time:119859ms step_avg:62.30ms
step:1925/2225 train_time:119921ms step_avg:62.30ms
step:1925/2225 val_loss:3.3342 train_time:119981ms step_avg:62.33ms
step:1926/2225 train_time:120006ms step_avg:62.31ms
step:1927/2225 train_time:120045ms step_avg:62.30ms
step:1928/2225 train_time:120109ms step_avg:62.30ms
step:1929/2225 train_time:120177ms step_avg:62.30ms
step:1930/2225 train_time:120238ms step_avg:62.30ms
step:1930/2225 val_loss:3.3334 train_time:120300ms step_avg:62.33ms
step:1931/2225 train_time:120324ms step_avg:62.31ms
step:1932/2225 train_time:120364ms step_avg:62.30ms
step:1933/2225 train_time:120431ms step_avg:62.30ms
step:1934/2225 train_time:120494ms step_avg:62.30ms
step:1935/2225 train_time:120555ms step_avg:62.30ms
step:1935/2225 val_loss:3.3320 train_time:120615ms step_avg:62.33ms
step:1936/2225 train_time:120639ms step_avg:62.31ms
step:1937/2225 train_time:120678ms step_avg:62.30ms
step:1938/2225 train_time:120742ms step_avg:62.30ms
step:1939/2225 train_time:120807ms step_avg:62.30ms
step:1940/2225 train_time:120867ms step_avg:62.30ms
step:1940/2225 val_loss:3.3305 train_time:120929ms step_avg:62.33ms
step:1941/2225 train_time:120953ms step_avg:62.31ms
step:1942/2225 train_time:120992ms step_avg:62.30ms
step:1943/2225 train_time:121057ms step_avg:62.30ms
step:1944/2225 train_time:121120ms step_avg:62.30ms
step:1945/2225 train_time:121182ms step_avg:62.30ms
step:1945/2225 val_loss:3.3299 train_time:121242ms step_avg:62.34ms
step:1946/2225 train_time:121267ms step_avg:62.32ms
step:1947/2225 train_time:121307ms step_avg:62.30ms
step:1948/2225 train_time:121371ms step_avg:62.31ms
step:1949/2225 train_time:121437ms step_avg:62.31ms
step:1950/2225 train_time:121497ms step_avg:62.31ms
step:1950/2225 val_loss:3.3289 train_time:121559ms step_avg:62.34ms
step:1951/2225 train_time:121583ms step_avg:62.32ms
step:1952/2225 train_time:121624ms step_avg:62.31ms
step:1953/2225 train_time:121687ms step_avg:62.31ms
step:1954/2225 train_time:121750ms step_avg:62.31ms
step:1955/2225 train_time:121812ms step_avg:62.31ms
step:1955/2225 val_loss:3.3278 train_time:121872ms step_avg:62.34ms
step:1956/2225 train_time:121897ms step_avg:62.32ms
step:1957/2225 train_time:121937ms step_avg:62.31ms
step:1958/2225 train_time:122001ms step_avg:62.31ms
step:1959/2225 train_time:122066ms step_avg:62.31ms
step:1960/2225 train_time:122126ms step_avg:62.31ms
step:1960/2225 val_loss:3.3268 train_time:122188ms step_avg:62.34ms
step:1961/2225 train_time:122211ms step_avg:62.32ms
step:1962/2225 train_time:122250ms step_avg:62.31ms
step:1963/2225 train_time:122315ms step_avg:62.31ms
step:1964/2225 train_time:122377ms step_avg:62.31ms
step:1965/2225 train_time:122439ms step_avg:62.31ms
step:1965/2225 val_loss:3.3260 train_time:122499ms step_avg:62.34ms
step:1966/2225 train_time:122524ms step_avg:62.32ms
step:1967/2225 train_time:122563ms step_avg:62.31ms
step:1968/2225 train_time:122625ms step_avg:62.31ms
step:1969/2225 train_time:122691ms step_avg:62.31ms
step:1970/2225 train_time:122752ms step_avg:62.31ms
step:1970/2225 val_loss:3.3248 train_time:122815ms step_avg:62.34ms
step:1971/2225 train_time:122838ms step_avg:62.32ms
step:1972/2225 train_time:122877ms step_avg:62.31ms
step:1973/2225 train_time:122945ms step_avg:62.31ms
step:1974/2225 train_time:123008ms step_avg:62.31ms
step:1975/2225 train_time:123069ms step_avg:62.31ms
step:1975/2225 val_loss:3.3238 train_time:123128ms step_avg:62.34ms
step:1976/2225 train_time:123154ms step_avg:62.32ms
step:1977/2225 train_time:123195ms step_avg:62.31ms
step:1978/2225 train_time:123258ms step_avg:62.31ms
step:1979/2225 train_time:123323ms step_avg:62.32ms
step:1980/2225 train_time:123383ms step_avg:62.31ms
step:1980/2225 val_loss:3.3226 train_time:123446ms step_avg:62.35ms
step:1981/2225 train_time:123469ms step_avg:62.33ms
step:1982/2225 train_time:123508ms step_avg:62.31ms
step:1983/2225 train_time:123574ms step_avg:62.32ms
step:1984/2225 train_time:123637ms step_avg:62.32ms
step:1985/2225 train_time:123699ms step_avg:62.32ms
step:1985/2225 val_loss:3.3220 train_time:123759ms step_avg:62.35ms
step:1986/2225 train_time:123785ms step_avg:62.33ms
step:1987/2225 train_time:123824ms step_avg:62.32ms
step:1988/2225 train_time:123889ms step_avg:62.32ms
step:1989/2225 train_time:123953ms step_avg:62.32ms
step:1990/2225 train_time:124013ms step_avg:62.32ms
step:1990/2225 val_loss:3.3213 train_time:124076ms step_avg:62.35ms
step:1991/2225 train_time:124099ms step_avg:62.33ms
step:1992/2225 train_time:124139ms step_avg:62.32ms
step:1993/2225 train_time:124206ms step_avg:62.32ms
step:1994/2225 train_time:124270ms step_avg:62.32ms
step:1995/2225 train_time:124333ms step_avg:62.32ms
step:1995/2225 val_loss:3.3199 train_time:124392ms step_avg:62.35ms
step:1996/2225 train_time:124417ms step_avg:62.33ms
step:1997/2225 train_time:124460ms step_avg:62.32ms
step:1998/2225 train_time:124520ms step_avg:62.32ms
step:1999/2225 train_time:124584ms step_avg:62.32ms
step:2000/2225 train_time:124644ms step_avg:62.32ms
step:2000/2225 val_loss:3.3190 train_time:124706ms step_avg:62.35ms
step:2001/2225 train_time:124732ms step_avg:62.33ms
step:2002/2225 train_time:124770ms step_avg:62.32ms
step:2003/2225 train_time:124838ms step_avg:62.33ms
step:2004/2225 train_time:124900ms step_avg:62.33ms
step:2005/2225 train_time:124961ms step_avg:62.32ms
step:2005/2225 val_loss:3.3178 train_time:125020ms step_avg:62.35ms
step:2006/2225 train_time:125045ms step_avg:62.34ms
step:2007/2225 train_time:125084ms step_avg:62.32ms
step:2008/2225 train_time:125148ms step_avg:62.32ms
step:2009/2225 train_time:125212ms step_avg:62.33ms
step:2010/2225 train_time:125273ms step_avg:62.32ms
step:2010/2225 val_loss:3.3164 train_time:125335ms step_avg:62.36ms
step:2011/2225 train_time:125358ms step_avg:62.34ms
step:2012/2225 train_time:125397ms step_avg:62.32ms
step:2013/2225 train_time:125464ms step_avg:62.33ms
step:2014/2225 train_time:125527ms step_avg:62.33ms
step:2015/2225 train_time:125589ms step_avg:62.33ms
step:2015/2225 val_loss:3.3157 train_time:125650ms step_avg:62.36ms
step:2016/2225 train_time:125674ms step_avg:62.34ms
step:2017/2225 train_time:125713ms step_avg:62.33ms
step:2018/2225 train_time:125777ms step_avg:62.33ms
step:2019/2225 train_time:125840ms step_avg:62.33ms
step:2020/2225 train_time:125900ms step_avg:62.33ms
step:2020/2225 val_loss:3.3148 train_time:125962ms step_avg:62.36ms
step:2021/2225 train_time:125985ms step_avg:62.34ms
step:2022/2225 train_time:126024ms step_avg:62.33ms
step:2023/2225 train_time:126089ms step_avg:62.33ms
step:2024/2225 train_time:126150ms step_avg:62.33ms
step:2025/2225 train_time:126213ms step_avg:62.33ms
step:2025/2225 val_loss:3.3139 train_time:126273ms step_avg:62.36ms
step:2026/2225 train_time:126298ms step_avg:62.34ms
step:2027/2225 train_time:126338ms step_avg:62.33ms
step:2028/2225 train_time:126400ms step_avg:62.33ms
step:2029/2225 train_time:126463ms step_avg:62.33ms
step:2030/2225 train_time:126523ms step_avg:62.33ms
step:2030/2225 val_loss:3.3127 train_time:126585ms step_avg:62.36ms
step:2031/2225 train_time:126608ms step_avg:62.34ms
step:2032/2225 train_time:126646ms step_avg:62.33ms
step:2033/2225 train_time:126711ms step_avg:62.33ms
step:2034/2225 train_time:126773ms step_avg:62.33ms
step:2035/2225 train_time:126834ms step_avg:62.33ms
step:2035/2225 val_loss:3.3120 train_time:126894ms step_avg:62.36ms
step:2036/2225 train_time:126919ms step_avg:62.34ms
step:2037/2225 train_time:126959ms step_avg:62.33ms
step:2038/2225 train_time:127023ms step_avg:62.33ms
step:2039/2225 train_time:127089ms step_avg:62.33ms
step:2040/2225 train_time:127150ms step_avg:62.33ms
step:2040/2225 val_loss:3.3108 train_time:127211ms step_avg:62.36ms
step:2041/2225 train_time:127234ms step_avg:62.34ms
step:2042/2225 train_time:127272ms step_avg:62.33ms
step:2043/2225 train_time:127338ms step_avg:62.33ms
step:2044/2225 train_time:127401ms step_avg:62.33ms
step:2045/2225 train_time:127462ms step_avg:62.33ms
step:2045/2225 val_loss:3.3103 train_time:127523ms step_avg:62.36ms
step:2046/2225 train_time:127548ms step_avg:62.34ms
step:2047/2225 train_time:127586ms step_avg:62.33ms
step:2048/2225 train_time:127649ms step_avg:62.33ms
step:2049/2225 train_time:127714ms step_avg:62.33ms
step:2050/2225 train_time:127776ms step_avg:62.33ms
step:2050/2225 val_loss:3.3090 train_time:127840ms step_avg:62.36ms
step:2051/2225 train_time:127863ms step_avg:62.34ms
step:2052/2225 train_time:127903ms step_avg:62.33ms
step:2053/2225 train_time:127969ms step_avg:62.33ms
step:2054/2225 train_time:128031ms step_avg:62.33ms
step:2055/2225 train_time:128093ms step_avg:62.33ms
step:2055/2225 val_loss:3.3082 train_time:128153ms step_avg:62.36ms
step:2056/2225 train_time:128178ms step_avg:62.34ms
step:2057/2225 train_time:128217ms step_avg:62.33ms
step:2058/2225 train_time:128283ms step_avg:62.33ms
step:2059/2225 train_time:128348ms step_avg:62.34ms
step:2060/2225 train_time:128409ms step_avg:62.33ms
step:2060/2225 val_loss:3.3074 train_time:128471ms step_avg:62.36ms
step:2061/2225 train_time:128495ms step_avg:62.35ms
step:2062/2225 train_time:128534ms step_avg:62.33ms
step:2063/2225 train_time:128601ms step_avg:62.34ms
step:2064/2225 train_time:128665ms step_avg:62.34ms
step:2065/2225 train_time:128727ms step_avg:62.34ms
step:2065/2225 val_loss:3.3061 train_time:128788ms step_avg:62.37ms
step:2066/2225 train_time:128812ms step_avg:62.35ms
step:2067/2225 train_time:128855ms step_avg:62.34ms
step:2068/2225 train_time:128918ms step_avg:62.34ms
step:2069/2225 train_time:128982ms step_avg:62.34ms
step:2070/2225 train_time:129042ms step_avg:62.34ms
step:2070/2225 val_loss:3.3052 train_time:129104ms step_avg:62.37ms
step:2071/2225 train_time:129127ms step_avg:62.35ms
step:2072/2225 train_time:129166ms step_avg:62.34ms
step:2073/2225 train_time:129233ms step_avg:62.34ms
step:2074/2225 train_time:129296ms step_avg:62.34ms
step:2075/2225 train_time:129358ms step_avg:62.34ms
step:2075/2225 val_loss:3.3043 train_time:129417ms step_avg:62.37ms
step:2076/2225 train_time:129443ms step_avg:62.35ms
step:2077/2225 train_time:129481ms step_avg:62.34ms
step:2078/2225 train_time:129543ms step_avg:62.34ms
step:2079/2225 train_time:129607ms step_avg:62.34ms
step:2080/2225 train_time:129668ms step_avg:62.34ms
step:2080/2225 val_loss:3.3034 train_time:129730ms step_avg:62.37ms
step:2081/2225 train_time:129753ms step_avg:62.35ms
step:2082/2225 train_time:129791ms step_avg:62.34ms
step:2083/2225 train_time:129858ms step_avg:62.34ms
step:2084/2225 train_time:129926ms step_avg:62.34ms
step:2085/2225 train_time:129988ms step_avg:62.34ms
step:2085/2225 val_loss:3.3025 train_time:130048ms step_avg:62.37ms
step:2086/2225 train_time:130072ms step_avg:62.35ms
step:2087/2225 train_time:130113ms step_avg:62.34ms
step:2088/2225 train_time:130178ms step_avg:62.35ms
step:2089/2225 train_time:130244ms step_avg:62.35ms
step:2090/2225 train_time:130304ms step_avg:62.35ms
step:2090/2225 val_loss:3.3019 train_time:130366ms step_avg:62.38ms
step:2091/2225 train_time:130389ms step_avg:62.36ms
step:2092/2225 train_time:130430ms step_avg:62.35ms
step:2093/2225 train_time:130494ms step_avg:62.35ms
step:2094/2225 train_time:130557ms step_avg:62.35ms
step:2095/2225 train_time:130619ms step_avg:62.35ms
step:2095/2225 val_loss:3.3010 train_time:130679ms step_avg:62.38ms
step:2096/2225 train_time:130703ms step_avg:62.36ms
step:2097/2225 train_time:130743ms step_avg:62.35ms
step:2098/2225 train_time:130806ms step_avg:62.35ms
step:2099/2225 train_time:130871ms step_avg:62.35ms
step:2100/2225 train_time:130932ms step_avg:62.35ms
step:2100/2225 val_loss:3.3001 train_time:130995ms step_avg:62.38ms
step:2101/2225 train_time:131018ms step_avg:62.36ms
step:2102/2225 train_time:131058ms step_avg:62.35ms
step:2103/2225 train_time:131124ms step_avg:62.35ms
step:2104/2225 train_time:131188ms step_avg:62.35ms
step:2105/2225 train_time:131249ms step_avg:62.35ms
step:2105/2225 val_loss:3.2992 train_time:131309ms step_avg:62.38ms
step:2106/2225 train_time:131334ms step_avg:62.36ms
step:2107/2225 train_time:131374ms step_avg:62.35ms
step:2108/2225 train_time:131440ms step_avg:62.35ms
step:2109/2225 train_time:131505ms step_avg:62.35ms
step:2110/2225 train_time:131565ms step_avg:62.35ms
step:2110/2225 val_loss:3.2984 train_time:131627ms step_avg:62.38ms
step:2111/2225 train_time:131650ms step_avg:62.36ms
step:2112/2225 train_time:131689ms step_avg:62.35ms
step:2113/2225 train_time:131755ms step_avg:62.35ms
step:2114/2225 train_time:131816ms step_avg:62.35ms
step:2115/2225 train_time:131879ms step_avg:62.35ms
step:2115/2225 val_loss:3.2976 train_time:131938ms step_avg:62.38ms
step:2116/2225 train_time:131963ms step_avg:62.36ms
step:2117/2225 train_time:132002ms step_avg:62.35ms
step:2118/2225 train_time:132065ms step_avg:62.35ms
step:2119/2225 train_time:132128ms step_avg:62.35ms
step:2120/2225 train_time:132190ms step_avg:62.35ms
step:2120/2225 val_loss:3.2968 train_time:132253ms step_avg:62.38ms
step:2121/2225 train_time:132276ms step_avg:62.36ms
step:2122/2225 train_time:132315ms step_avg:62.35ms
step:2123/2225 train_time:132381ms step_avg:62.36ms
step:2124/2225 train_time:132445ms step_avg:62.36ms
step:2125/2225 train_time:132507ms step_avg:62.36ms
step:2125/2225 val_loss:3.2960 train_time:132567ms step_avg:62.38ms
step:2126/2225 train_time:132592ms step_avg:62.37ms
step:2127/2225 train_time:132632ms step_avg:62.36ms
step:2128/2225 train_time:132696ms step_avg:62.36ms
step:2129/2225 train_time:132762ms step_avg:62.36ms
step:2130/2225 train_time:132822ms step_avg:62.36ms
step:2130/2225 val_loss:3.2951 train_time:132884ms step_avg:62.39ms
step:2131/2225 train_time:132908ms step_avg:62.37ms
step:2132/2225 train_time:132947ms step_avg:62.36ms
step:2133/2225 train_time:133014ms step_avg:62.36ms
step:2134/2225 train_time:133078ms step_avg:62.36ms
step:2135/2225 train_time:133140ms step_avg:62.36ms
step:2135/2225 val_loss:3.2944 train_time:133200ms step_avg:62.39ms
step:2136/2225 train_time:133226ms step_avg:62.37ms
step:2137/2225 train_time:133265ms step_avg:62.36ms
step:2138/2225 train_time:133327ms step_avg:62.36ms
step:2139/2225 train_time:133390ms step_avg:62.36ms
step:2140/2225 train_time:133450ms step_avg:62.36ms
step:2140/2225 val_loss:3.2937 train_time:133512ms step_avg:62.39ms
step:2141/2225 train_time:133535ms step_avg:62.37ms
step:2142/2225 train_time:133575ms step_avg:62.36ms
step:2143/2225 train_time:133641ms step_avg:62.36ms
step:2144/2225 train_time:133703ms step_avg:62.36ms
step:2145/2225 train_time:133765ms step_avg:62.36ms
step:2145/2225 val_loss:3.2929 train_time:133826ms step_avg:62.39ms
step:2146/2225 train_time:133850ms step_avg:62.37ms
step:2147/2225 train_time:133889ms step_avg:62.36ms
step:2148/2225 train_time:133956ms step_avg:62.36ms
step:2149/2225 train_time:134021ms step_avg:62.36ms
step:2150/2225 train_time:134081ms step_avg:62.36ms
step:2150/2225 val_loss:3.2920 train_time:134142ms step_avg:62.39ms
step:2151/2225 train_time:134166ms step_avg:62.37ms
step:2152/2225 train_time:134206ms step_avg:62.36ms
step:2153/2225 train_time:134270ms step_avg:62.36ms
step:2154/2225 train_time:134332ms step_avg:62.36ms
step:2155/2225 train_time:134395ms step_avg:62.36ms
step:2155/2225 val_loss:3.2914 train_time:134456ms step_avg:62.39ms
step:2156/2225 train_time:134480ms step_avg:62.37ms
step:2157/2225 train_time:134520ms step_avg:62.36ms
step:2158/2225 train_time:134584ms step_avg:62.37ms
step:2159/2225 train_time:134649ms step_avg:62.37ms
step:2160/2225 train_time:134709ms step_avg:62.37ms
step:2160/2225 val_loss:3.2905 train_time:134772ms step_avg:62.39ms
step:2161/2225 train_time:134795ms step_avg:62.38ms
step:2162/2225 train_time:134833ms step_avg:62.37ms
step:2163/2225 train_time:134902ms step_avg:62.37ms
step:2164/2225 train_time:134964ms step_avg:62.37ms
step:2165/2225 train_time:135026ms step_avg:62.37ms
step:2165/2225 val_loss:3.2898 train_time:135086ms step_avg:62.40ms
step:2166/2225 train_time:135111ms step_avg:62.38ms
step:2167/2225 train_time:135150ms step_avg:62.37ms
step:2168/2225 train_time:135213ms step_avg:62.37ms
step:2169/2225 train_time:135277ms step_avg:62.37ms
step:2170/2225 train_time:135338ms step_avg:62.37ms
step:2170/2225 val_loss:3.2890 train_time:135399ms step_avg:62.40ms
step:2171/2225 train_time:135423ms step_avg:62.38ms
step:2172/2225 train_time:135461ms step_avg:62.37ms
step:2173/2225 train_time:135528ms step_avg:62.37ms
step:2174/2225 train_time:135592ms step_avg:62.37ms
step:2175/2225 train_time:135654ms step_avg:62.37ms
step:2175/2225 val_loss:3.2885 train_time:135713ms step_avg:62.40ms
step:2176/2225 train_time:135738ms step_avg:62.38ms
step:2177/2225 train_time:135779ms step_avg:62.37ms
step:2178/2225 train_time:135842ms step_avg:62.37ms
step:2179/2225 train_time:135906ms step_avg:62.37ms
step:2180/2225 train_time:135967ms step_avg:62.37ms
step:2180/2225 val_loss:3.2877 train_time:136029ms step_avg:62.40ms
step:2181/2225 train_time:136052ms step_avg:62.38ms
step:2182/2225 train_time:136091ms step_avg:62.37ms
step:2183/2225 train_time:136156ms step_avg:62.37ms
step:2184/2225 train_time:136219ms step_avg:62.37ms
step:2185/2225 train_time:136281ms step_avg:62.37ms
step:2185/2225 val_loss:3.2852 train_time:136342ms step_avg:62.40ms
step:2186/2225 train_time:136367ms step_avg:62.38ms
step:2187/2225 train_time:136406ms step_avg:62.37ms
step:2188/2225 train_time:136473ms step_avg:62.37ms
step:2189/2225 train_time:136539ms step_avg:62.38ms
step:2190/2225 train_time:136600ms step_avg:62.37ms
step:2190/2225 val_loss:3.2839 train_time:136662ms step_avg:62.40ms
step:2191/2225 train_time:136685ms step_avg:62.38ms
step:2192/2225 train_time:136726ms step_avg:62.37ms
step:2193/2225 train_time:136792ms step_avg:62.38ms
step:2194/2225 train_time:136854ms step_avg:62.38ms
step:2195/2225 train_time:136917ms step_avg:62.38ms
step:2195/2225 val_loss:3.2831 train_time:136977ms step_avg:62.40ms
step:2196/2225 train_time:137003ms step_avg:62.39ms
step:2197/2225 train_time:137041ms step_avg:62.38ms
step:2198/2225 train_time:137106ms step_avg:62.38ms
step:2199/2225 train_time:137170ms step_avg:62.38ms
step:2200/2225 train_time:137230ms step_avg:62.38ms
step:2200/2225 val_loss:3.2822 train_time:137292ms step_avg:62.41ms
step:2201/2225 train_time:137315ms step_avg:62.39ms
step:2202/2225 train_time:137354ms step_avg:62.38ms
step:2203/2225 train_time:137419ms step_avg:62.38ms
step:2204/2225 train_time:137483ms step_avg:62.38ms
step:2205/2225 train_time:137545ms step_avg:62.38ms
step:2205/2225 val_loss:3.2818 train_time:137605ms step_avg:62.41ms
step:2206/2225 train_time:137629ms step_avg:62.39ms
step:2207/2225 train_time:137671ms step_avg:62.38ms
step:2208/2225 train_time:137736ms step_avg:62.38ms
step:2209/2225 train_time:137801ms step_avg:62.38ms
step:2210/2225 train_time:137863ms step_avg:62.38ms
step:2210/2225 val_loss:3.2810 train_time:137926ms step_avg:62.41ms
step:2211/2225 train_time:137949ms step_avg:62.39ms
step:2212/2225 train_time:137988ms step_avg:62.38ms
step:2213/2225 train_time:138055ms step_avg:62.38ms
step:2214/2225 train_time:138117ms step_avg:62.38ms
step:2215/2225 train_time:138179ms step_avg:62.38ms
step:2215/2225 val_loss:3.2805 train_time:138239ms step_avg:62.41ms
step:2216/2225 train_time:138267ms step_avg:62.39ms
step:2217/2225 train_time:138305ms step_avg:62.38ms
step:2218/2225 train_time:138366ms step_avg:62.38ms
step:2219/2225 train_time:138429ms step_avg:62.38ms
step:2220/2225 train_time:138489ms step_avg:62.38ms
step:2220/2225 val_loss:3.2800 train_time:138551ms step_avg:62.41ms
step:2221/2225 train_time:138575ms step_avg:62.39ms
step:2222/2225 train_time:138614ms step_avg:62.38ms
step:2223/2225 train_time:138682ms step_avg:62.38ms
step:2224/2225 train_time:138749ms step_avg:62.39ms
step:2225/2225 train_time:138811ms step_avg:62.39ms
step:2225/2225 val_loss:3.2765 train_time:138871ms step_avg:62.41ms
peak memory allocated: 29244 MiB reserved: 44216 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = (16 / 8) * 0.8
    if x > 0.66:
        lr_max = (24 / 8) * 0.8
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sun Nov 30 02:36:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2160 train_time:84ms step_avg:83.86ms
step:2/2160 train_time:123ms step_avg:61.28ms
step:3/2160 train_time:142ms step_avg:47.39ms
step:4/2160 train_time:165ms step_avg:41.22ms
step:5/2160 train_time:199ms step_avg:39.75ms
step:6/2160 train_time:284ms step_avg:47.41ms
step:7/2160 train_time:304ms step_avg:43.50ms
step:8/2160 train_time:337ms step_avg:42.16ms
step:9/2160 train_time:371ms step_avg:41.22ms
step:10/2160 train_time:404ms step_avg:40.38ms
step:11/2160 train_time:438ms step_avg:39.84ms
step:12/2160 train_time:471ms step_avg:39.29ms
step:13/2160 train_time:506ms step_avg:38.89ms
step:14/2160 train_time:539ms step_avg:38.47ms
step:15/2160 train_time:573ms step_avg:38.19ms
step:16/2160 train_time:606ms step_avg:37.86ms
step:17/2160 train_time:641ms step_avg:37.68ms
step:18/2160 train_time:673ms step_avg:37.41ms
step:19/2160 train_time:708ms step_avg:37.27ms
step:20/2160 train_time:741ms step_avg:37.05ms
step:21/2160 train_time:775ms step_avg:36.92ms
step:22/2160 train_time:808ms step_avg:36.75ms
step:23/2160 train_time:843ms step_avg:36.64ms
step:24/2160 train_time:876ms step_avg:36.50ms
step:25/2160 train_time:910ms step_avg:36.40ms
step:26/2160 train_time:943ms step_avg:36.26ms
step:27/2160 train_time:977ms step_avg:36.19ms
step:28/2160 train_time:1011ms step_avg:36.09ms
step:29/2160 train_time:1045ms step_avg:36.02ms
step:30/2160 train_time:1077ms step_avg:35.91ms
step:31/2160 train_time:1112ms step_avg:35.86ms
step:32/2160 train_time:1145ms step_avg:35.77ms
step:33/2160 train_time:1179ms step_avg:35.72ms
step:34/2160 train_time:1213ms step_avg:35.67ms
step:35/2160 train_time:1247ms step_avg:35.64ms
step:36/2160 train_time:1281ms step_avg:35.57ms
step:37/2160 train_time:1315ms step_avg:35.55ms
step:38/2160 train_time:1348ms step_avg:35.48ms
step:39/2160 train_time:1383ms step_avg:35.47ms
step:40/2160 train_time:1416ms step_avg:35.41ms
step:41/2160 train_time:1451ms step_avg:35.39ms
step:42/2160 train_time:1484ms step_avg:35.33ms
step:43/2160 train_time:1518ms step_avg:35.31ms
step:44/2160 train_time:1551ms step_avg:35.26ms
step:45/2160 train_time:1586ms step_avg:35.24ms
step:46/2160 train_time:1619ms step_avg:35.20ms
step:47/2160 train_time:1653ms step_avg:35.18ms
step:48/2160 train_time:1686ms step_avg:35.13ms
step:49/2160 train_time:1721ms step_avg:35.12ms
step:50/2160 train_time:1754ms step_avg:35.08ms
step:51/2160 train_time:1788ms step_avg:35.07ms
step:52/2160 train_time:1821ms step_avg:35.03ms
step:53/2160 train_time:1856ms step_avg:35.02ms
step:54/2160 train_time:1889ms step_avg:34.98ms
step:55/2160 train_time:1923ms step_avg:34.97ms
step:56/2160 train_time:1956ms step_avg:34.94ms
step:57/2160 train_time:1990ms step_avg:34.92ms
step:58/2160 train_time:2023ms step_avg:34.89ms
step:59/2160 train_time:2058ms step_avg:34.88ms
step:60/2160 train_time:2091ms step_avg:34.85ms
step:61/2160 train_time:2125ms step_avg:34.84ms
step:62/2160 train_time:2158ms step_avg:34.80ms
step:63/2160 train_time:2192ms step_avg:34.80ms
step:64/2160 train_time:2225ms step_avg:34.77ms
step:65/2160 train_time:2260ms step_avg:34.77ms
step:66/2160 train_time:2293ms step_avg:34.74ms
step:67/2160 train_time:2328ms step_avg:34.74ms
step:68/2160 train_time:2361ms step_avg:34.72ms
step:69/2160 train_time:2395ms step_avg:34.71ms
step:70/2160 train_time:2429ms step_avg:34.69ms
step:71/2160 train_time:2463ms step_avg:34.69ms
step:72/2160 train_time:2496ms step_avg:34.67ms
step:73/2160 train_time:2530ms step_avg:34.66ms
step:74/2160 train_time:2563ms step_avg:34.64ms
step:75/2160 train_time:2598ms step_avg:34.64ms
step:76/2160 train_time:2631ms step_avg:34.62ms
step:77/2160 train_time:2666ms step_avg:34.62ms
step:78/2160 train_time:2699ms step_avg:34.60ms
step:79/2160 train_time:2733ms step_avg:34.60ms
step:80/2160 train_time:2767ms step_avg:34.58ms
step:81/2160 train_time:2801ms step_avg:34.58ms
step:82/2160 train_time:2835ms step_avg:34.57ms
step:83/2160 train_time:2869ms step_avg:34.57ms
step:84/2160 train_time:2902ms step_avg:34.55ms
step:85/2160 train_time:2936ms step_avg:34.54ms
step:86/2160 train_time:2970ms step_avg:34.53ms
step:87/2160 train_time:3003ms step_avg:34.52ms
step:88/2160 train_time:3036ms step_avg:34.51ms
step:89/2160 train_time:3070ms step_avg:34.50ms
step:90/2160 train_time:3103ms step_avg:34.48ms
step:91/2160 train_time:3138ms step_avg:34.48ms
step:92/2160 train_time:3171ms step_avg:34.46ms
step:93/2160 train_time:3205ms step_avg:34.46ms
step:94/2160 train_time:3238ms step_avg:34.44ms
step:95/2160 train_time:3272ms step_avg:34.44ms
step:96/2160 train_time:3305ms step_avg:34.42ms
step:97/2160 train_time:3339ms step_avg:34.43ms
step:98/2160 train_time:3373ms step_avg:34.42ms
step:99/2160 train_time:3407ms step_avg:34.41ms
step:100/2160 train_time:3440ms step_avg:34.40ms
step:101/2160 train_time:3475ms step_avg:34.40ms
step:102/2160 train_time:3508ms step_avg:34.39ms
step:103/2160 train_time:3542ms step_avg:34.39ms
step:104/2160 train_time:3575ms step_avg:34.38ms
step:105/2160 train_time:3610ms step_avg:34.38ms
step:106/2160 train_time:3642ms step_avg:34.36ms
step:107/2160 train_time:3677ms step_avg:34.36ms
step:108/2160 train_time:3710ms step_avg:34.35ms
step:109/2160 train_time:3744ms step_avg:34.35ms
step:110/2160 train_time:3777ms step_avg:34.34ms
step:111/2160 train_time:3811ms step_avg:34.34ms
step:112/2160 train_time:3844ms step_avg:34.32ms
step:113/2160 train_time:3879ms step_avg:34.32ms
step:114/2160 train_time:3912ms step_avg:34.31ms
step:115/2160 train_time:3946ms step_avg:34.31ms
step:116/2160 train_time:3979ms step_avg:34.30ms
step:117/2160 train_time:4013ms step_avg:34.30ms
step:118/2160 train_time:4046ms step_avg:34.29ms
step:119/2160 train_time:4080ms step_avg:34.29ms
step:120/2160 train_time:4114ms step_avg:34.28ms
step:121/2160 train_time:4148ms step_avg:34.28ms
step:122/2160 train_time:4181ms step_avg:34.27ms
step:123/2160 train_time:4215ms step_avg:34.27ms
step:124/2160 train_time:4248ms step_avg:34.26ms
step:125/2160 train_time:4282ms step_avg:34.26ms
step:126/2160 train_time:4315ms step_avg:34.25ms
step:127/2160 train_time:4350ms step_avg:34.25ms
step:128/2160 train_time:4383ms step_avg:34.24ms
step:129/2160 train_time:4417ms step_avg:34.24ms
step:130/2160 train_time:4450ms step_avg:34.23ms
step:131/2160 train_time:4485ms step_avg:34.23ms
step:132/2160 train_time:4518ms step_avg:34.22ms
step:133/2160 train_time:4552ms step_avg:34.22ms
step:134/2160 train_time:4585ms step_avg:34.21ms
step:135/2160 train_time:4619ms step_avg:34.22ms
step:136/2160 train_time:4653ms step_avg:34.21ms
step:137/2160 train_time:4687ms step_avg:34.21ms
step:138/2160 train_time:4720ms step_avg:34.21ms
step:139/2160 train_time:4755ms step_avg:34.21ms
step:140/2160 train_time:4788ms step_avg:34.20ms
step:141/2160 train_time:4822ms step_avg:34.20ms
step:142/2160 train_time:4855ms step_avg:34.19ms
step:143/2160 train_time:4889ms step_avg:34.19ms
step:144/2160 train_time:4923ms step_avg:34.18ms
step:145/2160 train_time:4957ms step_avg:34.18ms
step:146/2160 train_time:4989ms step_avg:34.17ms
step:147/2160 train_time:5024ms step_avg:34.18ms
step:148/2160 train_time:5057ms step_avg:34.17ms
step:149/2160 train_time:5091ms step_avg:34.17ms
step:150/2160 train_time:5124ms step_avg:34.16ms
step:151/2160 train_time:5158ms step_avg:34.16ms
step:152/2160 train_time:5191ms step_avg:34.15ms
step:153/2160 train_time:5225ms step_avg:34.15ms
step:154/2160 train_time:5258ms step_avg:34.15ms
step:155/2160 train_time:5293ms step_avg:34.15ms
step:156/2160 train_time:5325ms step_avg:34.14ms
step:157/2160 train_time:5360ms step_avg:34.14ms
step:158/2160 train_time:5393ms step_avg:34.13ms
step:159/2160 train_time:5427ms step_avg:34.13ms
step:160/2160 train_time:5460ms step_avg:34.13ms
step:161/2160 train_time:5494ms step_avg:34.13ms
step:162/2160 train_time:5527ms step_avg:34.12ms
step:163/2160 train_time:5562ms step_avg:34.12ms
step:164/2160 train_time:5595ms step_avg:34.12ms
step:165/2160 train_time:5629ms step_avg:34.12ms
step:166/2160 train_time:5662ms step_avg:34.11ms
step:167/2160 train_time:5696ms step_avg:34.11ms
step:168/2160 train_time:5729ms step_avg:34.10ms
step:169/2160 train_time:5764ms step_avg:34.10ms
step:170/2160 train_time:5797ms step_avg:34.10ms
step:171/2160 train_time:5831ms step_avg:34.10ms
step:172/2160 train_time:5864ms step_avg:34.09ms
step:173/2160 train_time:5898ms step_avg:34.09ms
step:174/2160 train_time:5932ms step_avg:34.09ms
step:175/2160 train_time:5966ms step_avg:34.09ms
step:176/2160 train_time:6000ms step_avg:34.09ms
step:177/2160 train_time:6034ms step_avg:34.09ms
step:178/2160 train_time:6067ms step_avg:34.09ms
step:179/2160 train_time:6101ms step_avg:34.08ms
step:180/2160 train_time:6134ms step_avg:34.08ms
step:181/2160 train_time:6168ms step_avg:34.08ms
step:182/2160 train_time:6201ms step_avg:34.07ms
step:183/2160 train_time:6235ms step_avg:34.07ms
step:184/2160 train_time:6268ms step_avg:34.07ms
step:185/2160 train_time:6302ms step_avg:34.07ms
step:186/2160 train_time:6335ms step_avg:34.06ms
step:187/2160 train_time:6369ms step_avg:34.06ms
step:188/2160 train_time:6402ms step_avg:34.06ms
step:189/2160 train_time:6436ms step_avg:34.06ms
step:190/2160 train_time:6470ms step_avg:34.05ms
step:191/2160 train_time:6504ms step_avg:34.05ms
step:192/2160 train_time:6537ms step_avg:34.05ms
step:193/2160 train_time:6571ms step_avg:34.05ms
step:194/2160 train_time:6604ms step_avg:34.04ms
step:195/2160 train_time:6638ms step_avg:34.04ms
step:196/2160 train_time:6671ms step_avg:34.03ms
step:197/2160 train_time:6705ms step_avg:34.04ms
step:198/2160 train_time:6739ms step_avg:34.03ms
step:199/2160 train_time:6772ms step_avg:34.03ms
step:200/2160 train_time:6805ms step_avg:34.03ms
step:201/2160 train_time:6840ms step_avg:34.03ms
step:202/2160 train_time:6873ms step_avg:34.02ms
step:203/2160 train_time:6907ms step_avg:34.03ms
step:204/2160 train_time:6940ms step_avg:34.02ms
step:205/2160 train_time:6974ms step_avg:34.02ms
step:206/2160 train_time:7007ms step_avg:34.02ms
step:207/2160 train_time:7042ms step_avg:34.02ms
step:208/2160 train_time:7075ms step_avg:34.01ms
step:209/2160 train_time:7109ms step_avg:34.01ms
step:210/2160 train_time:7142ms step_avg:34.01ms
step:211/2160 train_time:7176ms step_avg:34.01ms
step:212/2160 train_time:7209ms step_avg:34.00ms
step:213/2160 train_time:7243ms step_avg:34.01ms
step:214/2160 train_time:7276ms step_avg:34.00ms
step:215/2160 train_time:7310ms step_avg:34.00ms
step:216/2160 train_time:7343ms step_avg:34.00ms
step:217/2160 train_time:7377ms step_avg:34.00ms
step:218/2160 train_time:7410ms step_avg:33.99ms
step:219/2160 train_time:7444ms step_avg:33.99ms
step:220/2160 train_time:7477ms step_avg:33.99ms
step:221/2160 train_time:7511ms step_avg:33.99ms
step:222/2160 train_time:7544ms step_avg:33.98ms
step:223/2160 train_time:7578ms step_avg:33.98ms
step:224/2160 train_time:7611ms step_avg:33.98ms
step:225/2160 train_time:7646ms step_avg:33.98ms
step:226/2160 train_time:7678ms step_avg:33.97ms
step:227/2160 train_time:7713ms step_avg:33.98ms
step:228/2160 train_time:7745ms step_avg:33.97ms
step:229/2160 train_time:7780ms step_avg:33.97ms
step:230/2160 train_time:7813ms step_avg:33.97ms
step:231/2160 train_time:7847ms step_avg:33.97ms
step:232/2160 train_time:7880ms step_avg:33.97ms
step:233/2160 train_time:7915ms step_avg:33.97ms
step:234/2160 train_time:7948ms step_avg:33.96ms
step:235/2160 train_time:7982ms step_avg:33.97ms
step:236/2160 train_time:8016ms step_avg:33.96ms
step:237/2160 train_time:8050ms step_avg:33.96ms
step:238/2160 train_time:8083ms step_avg:33.96ms
step:239/2160 train_time:8117ms step_avg:33.96ms
step:240/2160 train_time:8150ms step_avg:33.96ms
step:241/2160 train_time:8185ms step_avg:33.96ms
step:242/2160 train_time:8218ms step_avg:33.96ms
step:243/2160 train_time:8252ms step_avg:33.96ms
step:244/2160 train_time:8284ms step_avg:33.95ms
step:245/2160 train_time:8319ms step_avg:33.95ms
step:246/2160 train_time:8352ms step_avg:33.95ms
step:247/2160 train_time:8386ms step_avg:33.95ms
step:248/2160 train_time:8419ms step_avg:33.95ms
step:249/2160 train_time:8453ms step_avg:33.95ms
step:250/2160 train_time:8486ms step_avg:33.94ms
step:250/2160 val_loss:4.3209 train_time:8521ms step_avg:34.09ms
step:251/2160 train_time:8542ms step_avg:34.03ms
step:252/2160 train_time:8562ms step_avg:33.98ms
step:253/2160 train_time:8592ms step_avg:33.96ms
step:254/2160 train_time:8628ms step_avg:33.97ms
step:255/2160 train_time:8664ms step_avg:33.98ms
step:256/2160 train_time:8699ms step_avg:33.98ms
step:257/2160 train_time:8733ms step_avg:33.98ms
step:258/2160 train_time:8767ms step_avg:33.98ms
step:259/2160 train_time:8802ms step_avg:33.98ms
step:260/2160 train_time:8835ms step_avg:33.98ms
step:261/2160 train_time:8869ms step_avg:33.98ms
step:262/2160 train_time:8902ms step_avg:33.98ms
step:263/2160 train_time:8936ms step_avg:33.98ms
step:264/2160 train_time:8969ms step_avg:33.97ms
step:265/2160 train_time:9003ms step_avg:33.97ms
step:266/2160 train_time:9036ms step_avg:33.97ms
step:267/2160 train_time:9070ms step_avg:33.97ms
step:268/2160 train_time:9103ms step_avg:33.97ms
step:269/2160 train_time:9137ms step_avg:33.97ms
step:270/2160 train_time:9170ms step_avg:33.96ms
step:271/2160 train_time:9204ms step_avg:33.96ms
step:272/2160 train_time:9237ms step_avg:33.96ms
step:273/2160 train_time:9271ms step_avg:33.96ms
step:274/2160 train_time:9304ms step_avg:33.95ms
step:275/2160 train_time:9338ms step_avg:33.96ms
step:276/2160 train_time:9371ms step_avg:33.95ms
step:277/2160 train_time:9405ms step_avg:33.95ms
step:278/2160 train_time:9438ms step_avg:33.95ms
step:279/2160 train_time:9471ms step_avg:33.95ms
step:280/2160 train_time:9504ms step_avg:33.94ms
step:281/2160 train_time:9539ms step_avg:33.95ms
step:282/2160 train_time:9572ms step_avg:33.94ms
step:283/2160 train_time:9607ms step_avg:33.95ms
step:284/2160 train_time:9640ms step_avg:33.94ms
step:285/2160 train_time:9674ms step_avg:33.94ms
step:286/2160 train_time:9707ms step_avg:33.94ms
step:287/2160 train_time:9741ms step_avg:33.94ms
step:288/2160 train_time:9775ms step_avg:33.94ms
step:289/2160 train_time:9809ms step_avg:33.94ms
step:290/2160 train_time:9842ms step_avg:33.94ms
step:291/2160 train_time:9877ms step_avg:33.94ms
step:292/2160 train_time:9910ms step_avg:33.94ms
step:293/2160 train_time:9944ms step_avg:33.94ms
step:294/2160 train_time:9977ms step_avg:33.94ms
step:295/2160 train_time:10011ms step_avg:33.94ms
step:296/2160 train_time:10044ms step_avg:33.93ms
step:297/2160 train_time:10078ms step_avg:33.93ms
step:298/2160 train_time:10112ms step_avg:33.93ms
step:299/2160 train_time:10146ms step_avg:33.93ms
step:300/2160 train_time:10179ms step_avg:33.93ms
step:301/2160 train_time:10213ms step_avg:33.93ms
step:302/2160 train_time:10246ms step_avg:33.93ms
step:303/2160 train_time:10280ms step_avg:33.93ms
step:304/2160 train_time:10314ms step_avg:33.93ms
step:305/2160 train_time:10348ms step_avg:33.93ms
step:306/2160 train_time:10381ms step_avg:33.92ms
step:307/2160 train_time:10415ms step_avg:33.93ms
step:308/2160 train_time:10448ms step_avg:33.92ms
step:309/2160 train_time:10482ms step_avg:33.92ms
step:310/2160 train_time:10515ms step_avg:33.92ms
step:311/2160 train_time:10549ms step_avg:33.92ms
step:312/2160 train_time:10582ms step_avg:33.92ms
step:313/2160 train_time:10616ms step_avg:33.92ms
step:314/2160 train_time:10649ms step_avg:33.91ms
step:315/2160 train_time:10683ms step_avg:33.91ms
step:316/2160 train_time:10716ms step_avg:33.91ms
step:317/2160 train_time:10750ms step_avg:33.91ms
step:318/2160 train_time:10783ms step_avg:33.91ms
step:319/2160 train_time:10818ms step_avg:33.91ms
step:320/2160 train_time:10851ms step_avg:33.91ms
step:321/2160 train_time:10885ms step_avg:33.91ms
step:322/2160 train_time:10918ms step_avg:33.91ms
step:323/2160 train_time:10952ms step_avg:33.91ms
step:324/2160 train_time:10985ms step_avg:33.91ms
step:325/2160 train_time:11019ms step_avg:33.91ms
step:326/2160 train_time:11053ms step_avg:33.90ms
step:327/2160 train_time:11087ms step_avg:33.91ms
step:328/2160 train_time:11120ms step_avg:33.90ms
step:329/2160 train_time:11154ms step_avg:33.90ms
step:330/2160 train_time:11187ms step_avg:33.90ms
step:331/2160 train_time:11221ms step_avg:33.90ms
step:332/2160 train_time:11254ms step_avg:33.90ms
step:333/2160 train_time:11288ms step_avg:33.90ms
step:334/2160 train_time:11321ms step_avg:33.90ms
step:335/2160 train_time:11355ms step_avg:33.90ms
step:336/2160 train_time:11388ms step_avg:33.89ms
step:337/2160 train_time:11422ms step_avg:33.89ms
step:338/2160 train_time:11455ms step_avg:33.89ms
step:339/2160 train_time:11489ms step_avg:33.89ms
step:340/2160 train_time:11522ms step_avg:33.89ms
step:341/2160 train_time:11556ms step_avg:33.89ms
step:342/2160 train_time:11589ms step_avg:33.89ms
step:343/2160 train_time:11623ms step_avg:33.89ms
step:344/2160 train_time:11656ms step_avg:33.88ms
step:345/2160 train_time:11691ms step_avg:33.89ms
step:346/2160 train_time:11723ms step_avg:33.88ms
step:347/2160 train_time:11758ms step_avg:33.88ms
step:348/2160 train_time:11791ms step_avg:33.88ms
step:349/2160 train_time:11825ms step_avg:33.88ms
step:350/2160 train_time:11858ms step_avg:33.88ms
step:351/2160 train_time:11892ms step_avg:33.88ms
step:352/2160 train_time:11925ms step_avg:33.88ms
step:353/2160 train_time:11960ms step_avg:33.88ms
step:354/2160 train_time:11993ms step_avg:33.88ms
step:355/2160 train_time:12027ms step_avg:33.88ms
step:356/2160 train_time:12060ms step_avg:33.88ms
step:357/2160 train_time:12094ms step_avg:33.88ms
step:358/2160 train_time:12127ms step_avg:33.88ms
step:359/2160 train_time:12162ms step_avg:33.88ms
step:360/2160 train_time:12195ms step_avg:33.87ms
step:361/2160 train_time:12229ms step_avg:33.87ms
step:362/2160 train_time:12262ms step_avg:33.87ms
step:363/2160 train_time:12296ms step_avg:33.87ms
step:364/2160 train_time:12329ms step_avg:33.87ms
step:365/2160 train_time:12363ms step_avg:33.87ms
step:366/2160 train_time:12396ms step_avg:33.87ms
step:367/2160 train_time:12430ms step_avg:33.87ms
step:368/2160 train_time:12463ms step_avg:33.87ms
step:369/2160 train_time:12497ms step_avg:33.87ms
step:370/2160 train_time:12530ms step_avg:33.87ms
step:371/2160 train_time:12565ms step_avg:33.87ms
step:372/2160 train_time:12598ms step_avg:33.86ms
step:373/2160 train_time:12632ms step_avg:33.87ms
step:374/2160 train_time:12665ms step_avg:33.86ms
step:375/2160 train_time:12699ms step_avg:33.86ms
step:376/2160 train_time:12732ms step_avg:33.86ms
step:377/2160 train_time:12766ms step_avg:33.86ms
step:378/2160 train_time:12799ms step_avg:33.86ms
step:379/2160 train_time:12833ms step_avg:33.86ms
step:380/2160 train_time:12866ms step_avg:33.86ms
step:381/2160 train_time:12901ms step_avg:33.86ms
step:382/2160 train_time:12934ms step_avg:33.86ms
step:383/2160 train_time:12968ms step_avg:33.86ms
step:384/2160 train_time:13001ms step_avg:33.86ms
step:385/2160 train_time:13035ms step_avg:33.86ms
step:386/2160 train_time:13068ms step_avg:33.85ms
step:387/2160 train_time:13102ms step_avg:33.86ms
step:388/2160 train_time:13135ms step_avg:33.85ms
step:389/2160 train_time:13169ms step_avg:33.85ms
step:390/2160 train_time:13203ms step_avg:33.85ms
step:391/2160 train_time:13237ms step_avg:33.85ms
step:392/2160 train_time:13270ms step_avg:33.85ms
step:393/2160 train_time:13304ms step_avg:33.85ms
step:394/2160 train_time:13337ms step_avg:33.85ms
step:395/2160 train_time:13371ms step_avg:33.85ms
step:396/2160 train_time:13404ms step_avg:33.85ms
step:397/2160 train_time:13439ms step_avg:33.85ms
step:398/2160 train_time:13472ms step_avg:33.85ms
step:399/2160 train_time:13506ms step_avg:33.85ms
step:400/2160 train_time:13539ms step_avg:33.85ms
step:401/2160 train_time:13573ms step_avg:33.85ms
step:402/2160 train_time:13606ms step_avg:33.85ms
step:403/2160 train_time:13641ms step_avg:33.85ms
step:404/2160 train_time:13674ms step_avg:33.85ms
step:405/2160 train_time:13708ms step_avg:33.85ms
step:406/2160 train_time:13741ms step_avg:33.84ms
step:407/2160 train_time:13775ms step_avg:33.85ms
step:408/2160 train_time:13808ms step_avg:33.84ms
step:409/2160 train_time:13842ms step_avg:33.84ms
step:410/2160 train_time:13875ms step_avg:33.84ms
step:411/2160 train_time:13909ms step_avg:33.84ms
step:412/2160 train_time:13942ms step_avg:33.84ms
step:413/2160 train_time:13977ms step_avg:33.84ms
step:414/2160 train_time:14010ms step_avg:33.84ms
step:415/2160 train_time:14044ms step_avg:33.84ms
step:416/2160 train_time:14077ms step_avg:33.84ms
step:417/2160 train_time:14111ms step_avg:33.84ms
step:418/2160 train_time:14145ms step_avg:33.84ms
step:419/2160 train_time:14179ms step_avg:33.84ms
step:420/2160 train_time:14212ms step_avg:33.84ms
step:421/2160 train_time:14246ms step_avg:33.84ms
step:422/2160 train_time:14279ms step_avg:33.84ms
step:423/2160 train_time:14313ms step_avg:33.84ms
step:424/2160 train_time:14346ms step_avg:33.84ms
step:425/2160 train_time:14381ms step_avg:33.84ms
step:426/2160 train_time:14414ms step_avg:33.84ms
step:427/2160 train_time:14448ms step_avg:33.84ms
step:428/2160 train_time:14481ms step_avg:33.83ms
step:429/2160 train_time:14515ms step_avg:33.83ms
step:430/2160 train_time:14548ms step_avg:33.83ms
step:431/2160 train_time:14583ms step_avg:33.83ms
step:432/2160 train_time:14616ms step_avg:33.83ms
step:433/2160 train_time:14650ms step_avg:33.83ms
step:434/2160 train_time:14683ms step_avg:33.83ms
step:435/2160 train_time:14717ms step_avg:33.83ms
step:436/2160 train_time:14750ms step_avg:33.83ms
step:437/2160 train_time:14785ms step_avg:33.83ms
step:438/2160 train_time:14818ms step_avg:33.83ms
step:439/2160 train_time:14852ms step_avg:33.83ms
step:440/2160 train_time:14885ms step_avg:33.83ms
step:441/2160 train_time:14919ms step_avg:33.83ms
step:442/2160 train_time:14953ms step_avg:33.83ms
step:443/2160 train_time:14987ms step_avg:33.83ms
step:444/2160 train_time:15020ms step_avg:33.83ms
step:445/2160 train_time:15054ms step_avg:33.83ms
step:446/2160 train_time:15087ms step_avg:33.83ms
step:447/2160 train_time:15121ms step_avg:33.83ms
step:448/2160 train_time:15155ms step_avg:33.83ms
step:449/2160 train_time:15189ms step_avg:33.83ms
step:450/2160 train_time:15222ms step_avg:33.83ms
step:451/2160 train_time:15256ms step_avg:33.83ms
step:452/2160 train_time:15289ms step_avg:33.83ms
step:453/2160 train_time:15323ms step_avg:33.83ms
step:454/2160 train_time:15356ms step_avg:33.82ms
step:455/2160 train_time:15390ms step_avg:33.82ms
step:456/2160 train_time:15423ms step_avg:33.82ms
step:457/2160 train_time:15457ms step_avg:33.82ms
step:458/2160 train_time:15491ms step_avg:33.82ms
step:459/2160 train_time:15525ms step_avg:33.82ms
step:460/2160 train_time:15558ms step_avg:33.82ms
step:461/2160 train_time:15592ms step_avg:33.82ms
step:462/2160 train_time:15625ms step_avg:33.82ms
step:463/2160 train_time:15659ms step_avg:33.82ms
step:464/2160 train_time:15692ms step_avg:33.82ms
step:465/2160 train_time:15727ms step_avg:33.82ms
step:466/2160 train_time:15760ms step_avg:33.82ms
step:467/2160 train_time:15794ms step_avg:33.82ms
step:468/2160 train_time:15827ms step_avg:33.82ms
step:469/2160 train_time:15862ms step_avg:33.82ms
step:470/2160 train_time:15895ms step_avg:33.82ms
step:471/2160 train_time:15929ms step_avg:33.82ms
step:472/2160 train_time:15962ms step_avg:33.82ms
step:473/2160 train_time:15996ms step_avg:33.82ms
step:474/2160 train_time:16029ms step_avg:33.82ms
step:475/2160 train_time:16063ms step_avg:33.82ms
step:476/2160 train_time:16096ms step_avg:33.82ms
step:477/2160 train_time:16130ms step_avg:33.82ms
step:478/2160 train_time:16163ms step_avg:33.81ms
step:479/2160 train_time:16198ms step_avg:33.82ms
step:480/2160 train_time:16232ms step_avg:33.82ms
step:481/2160 train_time:16266ms step_avg:33.82ms
step:482/2160 train_time:16299ms step_avg:33.82ms
step:483/2160 train_time:16334ms step_avg:33.82ms
step:484/2160 train_time:16367ms step_avg:33.82ms
step:485/2160 train_time:16401ms step_avg:33.82ms
step:486/2160 train_time:16434ms step_avg:33.82ms
step:487/2160 train_time:16468ms step_avg:33.82ms
step:488/2160 train_time:16501ms step_avg:33.81ms
step:489/2160 train_time:16535ms step_avg:33.81ms
step:490/2160 train_time:16568ms step_avg:33.81ms
step:491/2160 train_time:16603ms step_avg:33.81ms
step:492/2160 train_time:16636ms step_avg:33.81ms
step:493/2160 train_time:16670ms step_avg:33.81ms
step:494/2160 train_time:16703ms step_avg:33.81ms
step:495/2160 train_time:16737ms step_avg:33.81ms
step:496/2160 train_time:16770ms step_avg:33.81ms
step:497/2160 train_time:16805ms step_avg:33.81ms
step:498/2160 train_time:16838ms step_avg:33.81ms
step:499/2160 train_time:16872ms step_avg:33.81ms
step:500/2160 train_time:16905ms step_avg:33.81ms
step:500/2160 val_loss:4.0131 train_time:16941ms step_avg:33.88ms
step:501/2160 train_time:16960ms step_avg:33.85ms
step:502/2160 train_time:16979ms step_avg:33.82ms
step:503/2160 train_time:17011ms step_avg:33.82ms
step:504/2160 train_time:17045ms step_avg:33.82ms
step:505/2160 train_time:17080ms step_avg:33.82ms
step:506/2160 train_time:17114ms step_avg:33.82ms
step:507/2160 train_time:17149ms step_avg:33.82ms
step:508/2160 train_time:17182ms step_avg:33.82ms
step:509/2160 train_time:17216ms step_avg:33.82ms
step:510/2160 train_time:17249ms step_avg:33.82ms
step:511/2160 train_time:17283ms step_avg:33.82ms
step:512/2160 train_time:17316ms step_avg:33.82ms
step:513/2160 train_time:17350ms step_avg:33.82ms
step:514/2160 train_time:17383ms step_avg:33.82ms
step:515/2160 train_time:17417ms step_avg:33.82ms
step:516/2160 train_time:17450ms step_avg:33.82ms
step:517/2160 train_time:17484ms step_avg:33.82ms
step:518/2160 train_time:17517ms step_avg:33.82ms
step:519/2160 train_time:17551ms step_avg:33.82ms
step:520/2160 train_time:17584ms step_avg:33.81ms
step:521/2160 train_time:17617ms step_avg:33.81ms
step:522/2160 train_time:17651ms step_avg:33.81ms
step:523/2160 train_time:17685ms step_avg:33.81ms
step:524/2160 train_time:17718ms step_avg:33.81ms
step:525/2160 train_time:17752ms step_avg:33.81ms
step:526/2160 train_time:17785ms step_avg:33.81ms
step:527/2160 train_time:17819ms step_avg:33.81ms
step:528/2160 train_time:17852ms step_avg:33.81ms
step:529/2160 train_time:17886ms step_avg:33.81ms
step:530/2160 train_time:17920ms step_avg:33.81ms
step:531/2160 train_time:17954ms step_avg:33.81ms
step:532/2160 train_time:17987ms step_avg:33.81ms
step:533/2160 train_time:18022ms step_avg:33.81ms
step:534/2160 train_time:18056ms step_avg:33.81ms
step:535/2160 train_time:18090ms step_avg:33.81ms
step:536/2160 train_time:18124ms step_avg:33.81ms
step:537/2160 train_time:18158ms step_avg:33.81ms
step:538/2160 train_time:18191ms step_avg:33.81ms
step:539/2160 train_time:18226ms step_avg:33.81ms
step:540/2160 train_time:18259ms step_avg:33.81ms
step:541/2160 train_time:18293ms step_avg:33.81ms
step:542/2160 train_time:18326ms step_avg:33.81ms
step:543/2160 train_time:18360ms step_avg:33.81ms
step:544/2160 train_time:18394ms step_avg:33.81ms
step:545/2160 train_time:18428ms step_avg:33.81ms
step:546/2160 train_time:18461ms step_avg:33.81ms
step:547/2160 train_time:18495ms step_avg:33.81ms
step:548/2160 train_time:18528ms step_avg:33.81ms
step:549/2160 train_time:18562ms step_avg:33.81ms
step:550/2160 train_time:18596ms step_avg:33.81ms
step:551/2160 train_time:18629ms step_avg:33.81ms
step:552/2160 train_time:18663ms step_avg:33.81ms
step:553/2160 train_time:18697ms step_avg:33.81ms
step:554/2160 train_time:18730ms step_avg:33.81ms
step:555/2160 train_time:18764ms step_avg:33.81ms
step:556/2160 train_time:18797ms step_avg:33.81ms
step:557/2160 train_time:18831ms step_avg:33.81ms
step:558/2160 train_time:18864ms step_avg:33.81ms
step:559/2160 train_time:18899ms step_avg:33.81ms
step:560/2160 train_time:18932ms step_avg:33.81ms
step:561/2160 train_time:18966ms step_avg:33.81ms
step:562/2160 train_time:18999ms step_avg:33.81ms
step:563/2160 train_time:19034ms step_avg:33.81ms
step:564/2160 train_time:19067ms step_avg:33.81ms
step:565/2160 train_time:19102ms step_avg:33.81ms
step:566/2160 train_time:19135ms step_avg:33.81ms
step:567/2160 train_time:19170ms step_avg:33.81ms
step:568/2160 train_time:19203ms step_avg:33.81ms
step:569/2160 train_time:19237ms step_avg:33.81ms
step:570/2160 train_time:19270ms step_avg:33.81ms
step:571/2160 train_time:19305ms step_avg:33.81ms
step:572/2160 train_time:19338ms step_avg:33.81ms
step:573/2160 train_time:19372ms step_avg:33.81ms
step:574/2160 train_time:19405ms step_avg:33.81ms
step:575/2160 train_time:19439ms step_avg:33.81ms
step:576/2160 train_time:19472ms step_avg:33.81ms
step:577/2160 train_time:19506ms step_avg:33.81ms
step:578/2160 train_time:19539ms step_avg:33.80ms
step:579/2160 train_time:19573ms step_avg:33.80ms
step:580/2160 train_time:19606ms step_avg:33.80ms
step:581/2160 train_time:19640ms step_avg:33.80ms
step:582/2160 train_time:19673ms step_avg:33.80ms
step:583/2160 train_time:19707ms step_avg:33.80ms
step:584/2160 train_time:19740ms step_avg:33.80ms
step:585/2160 train_time:19774ms step_avg:33.80ms
step:586/2160 train_time:19807ms step_avg:33.80ms
step:587/2160 train_time:19842ms step_avg:33.80ms
step:588/2160 train_time:19875ms step_avg:33.80ms
step:589/2160 train_time:19909ms step_avg:33.80ms
step:590/2160 train_time:19942ms step_avg:33.80ms
step:591/2160 train_time:19976ms step_avg:33.80ms
step:592/2160 train_time:20010ms step_avg:33.80ms
step:593/2160 train_time:20044ms step_avg:33.80ms
step:594/2160 train_time:20077ms step_avg:33.80ms
step:595/2160 train_time:20111ms step_avg:33.80ms
step:596/2160 train_time:20145ms step_avg:33.80ms
step:597/2160 train_time:20179ms step_avg:33.80ms
step:598/2160 train_time:20212ms step_avg:33.80ms
step:599/2160 train_time:20246ms step_avg:33.80ms
step:600/2160 train_time:20279ms step_avg:33.80ms
step:601/2160 train_time:20314ms step_avg:33.80ms
step:602/2160 train_time:20347ms step_avg:33.80ms
step:603/2160 train_time:20381ms step_avg:33.80ms
step:604/2160 train_time:20414ms step_avg:33.80ms
step:605/2160 train_time:20448ms step_avg:33.80ms
step:606/2160 train_time:20481ms step_avg:33.80ms
step:607/2160 train_time:20516ms step_avg:33.80ms
step:608/2160 train_time:20549ms step_avg:33.80ms
step:609/2160 train_time:20584ms step_avg:33.80ms
step:610/2160 train_time:20617ms step_avg:33.80ms
step:611/2160 train_time:20652ms step_avg:33.80ms
step:612/2160 train_time:20684ms step_avg:33.80ms
step:613/2160 train_time:20719ms step_avg:33.80ms
step:614/2160 train_time:20752ms step_avg:33.80ms
step:615/2160 train_time:20786ms step_avg:33.80ms
step:616/2160 train_time:20819ms step_avg:33.80ms
step:617/2160 train_time:20853ms step_avg:33.80ms
step:618/2160 train_time:20886ms step_avg:33.80ms
step:619/2160 train_time:20920ms step_avg:33.80ms
step:620/2160 train_time:20953ms step_avg:33.80ms
step:621/2160 train_time:20988ms step_avg:33.80ms
step:622/2160 train_time:21021ms step_avg:33.80ms
step:623/2160 train_time:21055ms step_avg:33.80ms
step:624/2160 train_time:21088ms step_avg:33.80ms
step:625/2160 train_time:21123ms step_avg:33.80ms
step:626/2160 train_time:21157ms step_avg:33.80ms
step:627/2160 train_time:21191ms step_avg:33.80ms
step:628/2160 train_time:21224ms step_avg:33.80ms
step:629/2160 train_time:21258ms step_avg:33.80ms
step:630/2160 train_time:21291ms step_avg:33.80ms
step:631/2160 train_time:21326ms step_avg:33.80ms
step:632/2160 train_time:21359ms step_avg:33.80ms
step:633/2160 train_time:21393ms step_avg:33.80ms
step:634/2160 train_time:21426ms step_avg:33.79ms
step:635/2160 train_time:21460ms step_avg:33.80ms
step:636/2160 train_time:21494ms step_avg:33.79ms
step:637/2160 train_time:21528ms step_avg:33.80ms
step:638/2160 train_time:21561ms step_avg:33.80ms
step:639/2160 train_time:21596ms step_avg:33.80ms
step:640/2160 train_time:21629ms step_avg:33.80ms
step:641/2160 train_time:21663ms step_avg:33.80ms
step:642/2160 train_time:21697ms step_avg:33.80ms
step:643/2160 train_time:21731ms step_avg:33.80ms
step:644/2160 train_time:21764ms step_avg:33.80ms
step:645/2160 train_time:21798ms step_avg:33.80ms
step:646/2160 train_time:21831ms step_avg:33.79ms
step:647/2160 train_time:21866ms step_avg:33.80ms
step:648/2160 train_time:21899ms step_avg:33.79ms
step:649/2160 train_time:21933ms step_avg:33.79ms
step:650/2160 train_time:21966ms step_avg:33.79ms
step:651/2160 train_time:22000ms step_avg:33.79ms
step:652/2160 train_time:22033ms step_avg:33.79ms
step:653/2160 train_time:22067ms step_avg:33.79ms
step:654/2160 train_time:22100ms step_avg:33.79ms
step:655/2160 train_time:22134ms step_avg:33.79ms
step:656/2160 train_time:22167ms step_avg:33.79ms
step:657/2160 train_time:22202ms step_avg:33.79ms
step:658/2160 train_time:22235ms step_avg:33.79ms
step:659/2160 train_time:22269ms step_avg:33.79ms
step:660/2160 train_time:22302ms step_avg:33.79ms
step:661/2160 train_time:22337ms step_avg:33.79ms
step:662/2160 train_time:22371ms step_avg:33.79ms
step:663/2160 train_time:22405ms step_avg:33.79ms
step:664/2160 train_time:22438ms step_avg:33.79ms
step:665/2160 train_time:22472ms step_avg:33.79ms
step:666/2160 train_time:22505ms step_avg:33.79ms
step:667/2160 train_time:22540ms step_avg:33.79ms
step:668/2160 train_time:22573ms step_avg:33.79ms
step:669/2160 train_time:22608ms step_avg:33.79ms
step:670/2160 train_time:22641ms step_avg:33.79ms
step:671/2160 train_time:22675ms step_avg:33.79ms
step:672/2160 train_time:22708ms step_avg:33.79ms
step:673/2160 train_time:22742ms step_avg:33.79ms
step:674/2160 train_time:22775ms step_avg:33.79ms
step:675/2160 train_time:22810ms step_avg:33.79ms
step:676/2160 train_time:22843ms step_avg:33.79ms
step:677/2160 train_time:22878ms step_avg:33.79ms
step:678/2160 train_time:22911ms step_avg:33.79ms
step:679/2160 train_time:22945ms step_avg:33.79ms
step:680/2160 train_time:22979ms step_avg:33.79ms
step:681/2160 train_time:23013ms step_avg:33.79ms
step:682/2160 train_time:23046ms step_avg:33.79ms
step:683/2160 train_time:23081ms step_avg:33.79ms
step:684/2160 train_time:23114ms step_avg:33.79ms
step:685/2160 train_time:23148ms step_avg:33.79ms
step:686/2160 train_time:23181ms step_avg:33.79ms
step:687/2160 train_time:23215ms step_avg:33.79ms
step:688/2160 train_time:23248ms step_avg:33.79ms
step:689/2160 train_time:23282ms step_avg:33.79ms
step:690/2160 train_time:23316ms step_avg:33.79ms
step:691/2160 train_time:23350ms step_avg:33.79ms
step:692/2160 train_time:23383ms step_avg:33.79ms
step:693/2160 train_time:23418ms step_avg:33.79ms
step:694/2160 train_time:23451ms step_avg:33.79ms
step:695/2160 train_time:23485ms step_avg:33.79ms
step:696/2160 train_time:23519ms step_avg:33.79ms
step:697/2160 train_time:23553ms step_avg:33.79ms
step:698/2160 train_time:23586ms step_avg:33.79ms
step:699/2160 train_time:23620ms step_avg:33.79ms
step:700/2160 train_time:23653ms step_avg:33.79ms
step:701/2160 train_time:23687ms step_avg:33.79ms
step:702/2160 train_time:23721ms step_avg:33.79ms
step:703/2160 train_time:23755ms step_avg:33.79ms
step:704/2160 train_time:23788ms step_avg:33.79ms
step:705/2160 train_time:23822ms step_avg:33.79ms
step:706/2160 train_time:23855ms step_avg:33.79ms
step:707/2160 train_time:23890ms step_avg:33.79ms
step:708/2160 train_time:23924ms step_avg:33.79ms
step:709/2160 train_time:23984ms step_avg:33.83ms
step:710/2160 train_time:24044ms step_avg:33.86ms
step:711/2160 train_time:24104ms step_avg:33.90ms
step:712/2160 train_time:24163ms step_avg:33.94ms
step:713/2160 train_time:24224ms step_avg:33.98ms
step:714/2160 train_time:24284ms step_avg:34.01ms
step:715/2160 train_time:24345ms step_avg:34.05ms
step:716/2160 train_time:24404ms step_avg:34.08ms
step:717/2160 train_time:24465ms step_avg:34.12ms
step:718/2160 train_time:24525ms step_avg:34.16ms
step:719/2160 train_time:24586ms step_avg:34.20ms
step:720/2160 train_time:24646ms step_avg:34.23ms
step:721/2160 train_time:24707ms step_avg:34.27ms
step:722/2160 train_time:24767ms step_avg:34.30ms
step:723/2160 train_time:24828ms step_avg:34.34ms
step:724/2160 train_time:24888ms step_avg:34.38ms
step:725/2160 train_time:24949ms step_avg:34.41ms
step:726/2160 train_time:25009ms step_avg:34.45ms
step:727/2160 train_time:25071ms step_avg:34.49ms
step:728/2160 train_time:25131ms step_avg:34.52ms
step:729/2160 train_time:25193ms step_avg:34.56ms
step:730/2160 train_time:25253ms step_avg:34.59ms
step:731/2160 train_time:25315ms step_avg:34.63ms
step:732/2160 train_time:25375ms step_avg:34.66ms
step:733/2160 train_time:25436ms step_avg:34.70ms
step:734/2160 train_time:25496ms step_avg:34.74ms
step:735/2160 train_time:25558ms step_avg:34.77ms
step:736/2160 train_time:25618ms step_avg:34.81ms
step:737/2160 train_time:25680ms step_avg:34.84ms
step:738/2160 train_time:25739ms step_avg:34.88ms
step:739/2160 train_time:25800ms step_avg:34.91ms
step:740/2160 train_time:25860ms step_avg:34.95ms
step:741/2160 train_time:25921ms step_avg:34.98ms
step:742/2160 train_time:25981ms step_avg:35.02ms
step:743/2160 train_time:26043ms step_avg:35.05ms
step:744/2160 train_time:26102ms step_avg:35.08ms
step:745/2160 train_time:26164ms step_avg:35.12ms
step:746/2160 train_time:26223ms step_avg:35.15ms
step:747/2160 train_time:26284ms step_avg:35.19ms
step:748/2160 train_time:26344ms step_avg:35.22ms
step:749/2160 train_time:26405ms step_avg:35.25ms
step:750/2160 train_time:26464ms step_avg:35.29ms
step:750/2160 val_loss:3.8626 train_time:26526ms step_avg:35.37ms
step:751/2160 train_time:26547ms step_avg:35.35ms
step:752/2160 train_time:26590ms step_avg:35.36ms
step:753/2160 train_time:26650ms step_avg:35.39ms
step:754/2160 train_time:26710ms step_avg:35.42ms
step:755/2160 train_time:26772ms step_avg:35.46ms
step:756/2160 train_time:26832ms step_avg:35.49ms
step:757/2160 train_time:26892ms step_avg:35.52ms
step:758/2160 train_time:26950ms step_avg:35.55ms
step:759/2160 train_time:27010ms step_avg:35.59ms
step:760/2160 train_time:27069ms step_avg:35.62ms
step:761/2160 train_time:27129ms step_avg:35.65ms
step:762/2160 train_time:27188ms step_avg:35.68ms
step:763/2160 train_time:27248ms step_avg:35.71ms
step:764/2160 train_time:27307ms step_avg:35.74ms
step:765/2160 train_time:27367ms step_avg:35.77ms
step:766/2160 train_time:27434ms step_avg:35.81ms
step:767/2160 train_time:27500ms step_avg:35.85ms
step:768/2160 train_time:27560ms step_avg:35.89ms
step:769/2160 train_time:27622ms step_avg:35.92ms
step:770/2160 train_time:27681ms step_avg:35.95ms
step:771/2160 train_time:27743ms step_avg:35.98ms
step:772/2160 train_time:27803ms step_avg:36.01ms
step:773/2160 train_time:27864ms step_avg:36.05ms
step:774/2160 train_time:27924ms step_avg:36.08ms
step:775/2160 train_time:27985ms step_avg:36.11ms
step:776/2160 train_time:28045ms step_avg:36.14ms
step:777/2160 train_time:28106ms step_avg:36.17ms
step:778/2160 train_time:28165ms step_avg:36.20ms
step:779/2160 train_time:28226ms step_avg:36.23ms
step:780/2160 train_time:28285ms step_avg:36.26ms
step:781/2160 train_time:28347ms step_avg:36.30ms
step:782/2160 train_time:28409ms step_avg:36.33ms
step:783/2160 train_time:28473ms step_avg:36.36ms
step:784/2160 train_time:28533ms step_avg:36.39ms
step:785/2160 train_time:28595ms step_avg:36.43ms
step:786/2160 train_time:28655ms step_avg:36.46ms
step:787/2160 train_time:28716ms step_avg:36.49ms
step:788/2160 train_time:28775ms step_avg:36.52ms
step:789/2160 train_time:28836ms step_avg:36.55ms
step:790/2160 train_time:28895ms step_avg:36.58ms
step:791/2160 train_time:28956ms step_avg:36.61ms
step:792/2160 train_time:29016ms step_avg:36.64ms
step:793/2160 train_time:29077ms step_avg:36.67ms
step:794/2160 train_time:29137ms step_avg:36.70ms
step:795/2160 train_time:29198ms step_avg:36.73ms
step:796/2160 train_time:29258ms step_avg:36.76ms
step:797/2160 train_time:29320ms step_avg:36.79ms
step:798/2160 train_time:29379ms step_avg:36.82ms
step:799/2160 train_time:29441ms step_avg:36.85ms
step:800/2160 train_time:29502ms step_avg:36.88ms
step:801/2160 train_time:29564ms step_avg:36.91ms
step:802/2160 train_time:29624ms step_avg:36.94ms
step:803/2160 train_time:29686ms step_avg:36.97ms
step:804/2160 train_time:29746ms step_avg:37.00ms
step:805/2160 train_time:29807ms step_avg:37.03ms
step:806/2160 train_time:29867ms step_avg:37.06ms
step:807/2160 train_time:29927ms step_avg:37.08ms
step:808/2160 train_time:29987ms step_avg:37.11ms
step:809/2160 train_time:30048ms step_avg:37.14ms
step:810/2160 train_time:30107ms step_avg:37.17ms
step:811/2160 train_time:30168ms step_avg:37.20ms
step:812/2160 train_time:30227ms step_avg:37.23ms
step:813/2160 train_time:30288ms step_avg:37.26ms
step:814/2160 train_time:30348ms step_avg:37.28ms
step:815/2160 train_time:30410ms step_avg:37.31ms
step:816/2160 train_time:30470ms step_avg:37.34ms
step:817/2160 train_time:30532ms step_avg:37.37ms
step:818/2160 train_time:30591ms step_avg:37.40ms
step:819/2160 train_time:30652ms step_avg:37.43ms
step:820/2160 train_time:30711ms step_avg:37.45ms
step:821/2160 train_time:30772ms step_avg:37.48ms
step:822/2160 train_time:30831ms step_avg:37.51ms
step:823/2160 train_time:30892ms step_avg:37.54ms
step:824/2160 train_time:30952ms step_avg:37.56ms
step:825/2160 train_time:31014ms step_avg:37.59ms
step:826/2160 train_time:31073ms step_avg:37.62ms
step:827/2160 train_time:31134ms step_avg:37.65ms
step:828/2160 train_time:31193ms step_avg:37.67ms
step:829/2160 train_time:31254ms step_avg:37.70ms
step:830/2160 train_time:31313ms step_avg:37.73ms
step:831/2160 train_time:31375ms step_avg:37.76ms
step:832/2160 train_time:31435ms step_avg:37.78ms
step:833/2160 train_time:31497ms step_avg:37.81ms
step:834/2160 train_time:31556ms step_avg:37.84ms
step:835/2160 train_time:31618ms step_avg:37.87ms
step:836/2160 train_time:31677ms step_avg:37.89ms
step:837/2160 train_time:31739ms step_avg:37.92ms
step:838/2160 train_time:31799ms step_avg:37.95ms
step:839/2160 train_time:31861ms step_avg:37.97ms
step:840/2160 train_time:31920ms step_avg:38.00ms
step:841/2160 train_time:31982ms step_avg:38.03ms
step:842/2160 train_time:32042ms step_avg:38.05ms
step:843/2160 train_time:32104ms step_avg:38.08ms
step:844/2160 train_time:32163ms step_avg:38.11ms
step:845/2160 train_time:32225ms step_avg:38.14ms
step:846/2160 train_time:32285ms step_avg:38.16ms
step:847/2160 train_time:32346ms step_avg:38.19ms
step:848/2160 train_time:32407ms step_avg:38.22ms
step:849/2160 train_time:32469ms step_avg:38.24ms
step:850/2160 train_time:32529ms step_avg:38.27ms
step:851/2160 train_time:32591ms step_avg:38.30ms
step:852/2160 train_time:32652ms step_avg:38.32ms
step:853/2160 train_time:32713ms step_avg:38.35ms
step:854/2160 train_time:32773ms step_avg:38.38ms
step:855/2160 train_time:32833ms step_avg:38.40ms
step:856/2160 train_time:32893ms step_avg:38.43ms
step:857/2160 train_time:32954ms step_avg:38.45ms
step:858/2160 train_time:33013ms step_avg:38.48ms
step:859/2160 train_time:33074ms step_avg:38.50ms
step:860/2160 train_time:33133ms step_avg:38.53ms
step:861/2160 train_time:33194ms step_avg:38.55ms
step:862/2160 train_time:33254ms step_avg:38.58ms
step:863/2160 train_time:33315ms step_avg:38.60ms
step:864/2160 train_time:33375ms step_avg:38.63ms
step:865/2160 train_time:33437ms step_avg:38.66ms
step:866/2160 train_time:33497ms step_avg:38.68ms
step:867/2160 train_time:33559ms step_avg:38.71ms
step:868/2160 train_time:33619ms step_avg:38.73ms
step:869/2160 train_time:33681ms step_avg:38.76ms
step:870/2160 train_time:33741ms step_avg:38.78ms
step:871/2160 train_time:33803ms step_avg:38.81ms
step:872/2160 train_time:33863ms step_avg:38.83ms
step:873/2160 train_time:33925ms step_avg:38.86ms
step:874/2160 train_time:33984ms step_avg:38.88ms
step:875/2160 train_time:34045ms step_avg:38.91ms
step:876/2160 train_time:34104ms step_avg:38.93ms
step:877/2160 train_time:34165ms step_avg:38.96ms
step:878/2160 train_time:34225ms step_avg:38.98ms
step:879/2160 train_time:34287ms step_avg:39.01ms
step:880/2160 train_time:34347ms step_avg:39.03ms
step:881/2160 train_time:34409ms step_avg:39.06ms
step:882/2160 train_time:34469ms step_avg:39.08ms
step:883/2160 train_time:34530ms step_avg:39.11ms
step:884/2160 train_time:34590ms step_avg:39.13ms
step:885/2160 train_time:34652ms step_avg:39.15ms
step:886/2160 train_time:34711ms step_avg:39.18ms
step:887/2160 train_time:34772ms step_avg:39.20ms
step:888/2160 train_time:34832ms step_avg:39.22ms
step:889/2160 train_time:34892ms step_avg:39.25ms
step:890/2160 train_time:34952ms step_avg:39.27ms
step:891/2160 train_time:35013ms step_avg:39.30ms
step:892/2160 train_time:35072ms step_avg:39.32ms
step:893/2160 train_time:35133ms step_avg:39.34ms
step:894/2160 train_time:35193ms step_avg:39.37ms
step:895/2160 train_time:35255ms step_avg:39.39ms
step:896/2160 train_time:35315ms step_avg:39.41ms
step:897/2160 train_time:35377ms step_avg:39.44ms
step:898/2160 train_time:35437ms step_avg:39.46ms
step:899/2160 train_time:35498ms step_avg:39.49ms
step:900/2160 train_time:35558ms step_avg:39.51ms
step:901/2160 train_time:35619ms step_avg:39.53ms
step:902/2160 train_time:35679ms step_avg:39.56ms
step:903/2160 train_time:35740ms step_avg:39.58ms
step:904/2160 train_time:35800ms step_avg:39.60ms
step:905/2160 train_time:35862ms step_avg:39.63ms
step:906/2160 train_time:35921ms step_avg:39.65ms
step:907/2160 train_time:35982ms step_avg:39.67ms
step:908/2160 train_time:36042ms step_avg:39.69ms
step:909/2160 train_time:36104ms step_avg:39.72ms
step:910/2160 train_time:36163ms step_avg:39.74ms
step:911/2160 train_time:36225ms step_avg:39.76ms
step:912/2160 train_time:36284ms step_avg:39.79ms
step:913/2160 train_time:36346ms step_avg:39.81ms
step:914/2160 train_time:36406ms step_avg:39.83ms
step:915/2160 train_time:36468ms step_avg:39.86ms
step:916/2160 train_time:36528ms step_avg:39.88ms
step:917/2160 train_time:36589ms step_avg:39.90ms
step:918/2160 train_time:36650ms step_avg:39.92ms
step:919/2160 train_time:36711ms step_avg:39.95ms
step:920/2160 train_time:36771ms step_avg:39.97ms
step:921/2160 train_time:36832ms step_avg:39.99ms
step:922/2160 train_time:36891ms step_avg:40.01ms
step:923/2160 train_time:36952ms step_avg:40.03ms
step:924/2160 train_time:37011ms step_avg:40.05ms
step:925/2160 train_time:37072ms step_avg:40.08ms
step:926/2160 train_time:37132ms step_avg:40.10ms
step:927/2160 train_time:37193ms step_avg:40.12ms
step:928/2160 train_time:37252ms step_avg:40.14ms
step:929/2160 train_time:37314ms step_avg:40.17ms
step:930/2160 train_time:37373ms step_avg:40.19ms
step:931/2160 train_time:37436ms step_avg:40.21ms
step:932/2160 train_time:37495ms step_avg:40.23ms
step:933/2160 train_time:37557ms step_avg:40.25ms
step:934/2160 train_time:37616ms step_avg:40.27ms
step:935/2160 train_time:37678ms step_avg:40.30ms
step:936/2160 train_time:37737ms step_avg:40.32ms
step:937/2160 train_time:37799ms step_avg:40.34ms
step:938/2160 train_time:37859ms step_avg:40.36ms
step:939/2160 train_time:37921ms step_avg:40.38ms
step:940/2160 train_time:37981ms step_avg:40.41ms
step:941/2160 train_time:38042ms step_avg:40.43ms
step:942/2160 train_time:38103ms step_avg:40.45ms
step:943/2160 train_time:38165ms step_avg:40.47ms
step:944/2160 train_time:38224ms step_avg:40.49ms
step:945/2160 train_time:38286ms step_avg:40.51ms
step:946/2160 train_time:38346ms step_avg:40.54ms
step:947/2160 train_time:38408ms step_avg:40.56ms
step:948/2160 train_time:38467ms step_avg:40.58ms
step:949/2160 train_time:38528ms step_avg:40.60ms
step:950/2160 train_time:38588ms step_avg:40.62ms
step:951/2160 train_time:38649ms step_avg:40.64ms
step:952/2160 train_time:38709ms step_avg:40.66ms
step:953/2160 train_time:38771ms step_avg:40.68ms
step:954/2160 train_time:38831ms step_avg:40.70ms
step:955/2160 train_time:38892ms step_avg:40.72ms
step:956/2160 train_time:38951ms step_avg:40.74ms
step:957/2160 train_time:39011ms step_avg:40.76ms
step:958/2160 train_time:39071ms step_avg:40.78ms
step:959/2160 train_time:39132ms step_avg:40.80ms
step:960/2160 train_time:39191ms step_avg:40.82ms
step:961/2160 train_time:39252ms step_avg:40.85ms
step:962/2160 train_time:39312ms step_avg:40.86ms
step:963/2160 train_time:39373ms step_avg:40.89ms
step:964/2160 train_time:39432ms step_avg:40.90ms
step:965/2160 train_time:39494ms step_avg:40.93ms
step:966/2160 train_time:39553ms step_avg:40.95ms
step:967/2160 train_time:39615ms step_avg:40.97ms
step:968/2160 train_time:39674ms step_avg:40.99ms
step:969/2160 train_time:39735ms step_avg:41.01ms
step:970/2160 train_time:39794ms step_avg:41.03ms
step:971/2160 train_time:39855ms step_avg:41.05ms
step:972/2160 train_time:39915ms step_avg:41.06ms
step:973/2160 train_time:39976ms step_avg:41.08ms
step:974/2160 train_time:40035ms step_avg:41.10ms
step:975/2160 train_time:40097ms step_avg:41.12ms
step:976/2160 train_time:40156ms step_avg:41.14ms
step:977/2160 train_time:40218ms step_avg:41.16ms
step:978/2160 train_time:40278ms step_avg:41.18ms
step:979/2160 train_time:40340ms step_avg:41.21ms
step:980/2160 train_time:40400ms step_avg:41.22ms
step:981/2160 train_time:40461ms step_avg:41.25ms
step:982/2160 train_time:40521ms step_avg:41.26ms
step:983/2160 train_time:40583ms step_avg:41.28ms
step:984/2160 train_time:40642ms step_avg:41.30ms
step:985/2160 train_time:40704ms step_avg:41.32ms
step:986/2160 train_time:40764ms step_avg:41.34ms
step:987/2160 train_time:40826ms step_avg:41.36ms
step:988/2160 train_time:40885ms step_avg:41.38ms
step:989/2160 train_time:40947ms step_avg:41.40ms
step:990/2160 train_time:41006ms step_avg:41.42ms
step:991/2160 train_time:41067ms step_avg:41.44ms
step:992/2160 train_time:41127ms step_avg:41.46ms
step:993/2160 train_time:41188ms step_avg:41.48ms
step:994/2160 train_time:41248ms step_avg:41.50ms
step:995/2160 train_time:41309ms step_avg:41.52ms
step:996/2160 train_time:41368ms step_avg:41.53ms
step:997/2160 train_time:41429ms step_avg:41.55ms
step:998/2160 train_time:41489ms step_avg:41.57ms
step:999/2160 train_time:41550ms step_avg:41.59ms
step:1000/2160 train_time:41609ms step_avg:41.61ms
step:1000/2160 val_loss:3.6941 train_time:41671ms step_avg:41.67ms
step:1001/2160 train_time:41690ms step_avg:41.65ms
step:1002/2160 train_time:41731ms step_avg:41.65ms
step:1003/2160 train_time:41795ms step_avg:41.67ms
step:1004/2160 train_time:41855ms step_avg:41.69ms
step:1005/2160 train_time:41917ms step_avg:41.71ms
step:1006/2160 train_time:41978ms step_avg:41.73ms
step:1007/2160 train_time:42038ms step_avg:41.75ms
step:1008/2160 train_time:42097ms step_avg:41.76ms
step:1009/2160 train_time:42158ms step_avg:41.78ms
step:1010/2160 train_time:42217ms step_avg:41.80ms
step:1011/2160 train_time:42278ms step_avg:41.82ms
step:1012/2160 train_time:42337ms step_avg:41.83ms
step:1013/2160 train_time:42398ms step_avg:41.85ms
step:1014/2160 train_time:42457ms step_avg:41.87ms
step:1015/2160 train_time:42518ms step_avg:41.89ms
step:1016/2160 train_time:42578ms step_avg:41.91ms
step:1017/2160 train_time:42640ms step_avg:41.93ms
step:1018/2160 train_time:42702ms step_avg:41.95ms
step:1019/2160 train_time:42765ms step_avg:41.97ms
step:1020/2160 train_time:42825ms step_avg:41.99ms
step:1021/2160 train_time:42886ms step_avg:42.00ms
step:1022/2160 train_time:42945ms step_avg:42.02ms
step:1023/2160 train_time:43007ms step_avg:42.04ms
step:1024/2160 train_time:43065ms step_avg:42.06ms
step:1025/2160 train_time:43126ms step_avg:42.07ms
step:1026/2160 train_time:43185ms step_avg:42.09ms
step:1027/2160 train_time:43246ms step_avg:42.11ms
step:1028/2160 train_time:43305ms step_avg:42.13ms
step:1029/2160 train_time:43366ms step_avg:42.14ms
step:1030/2160 train_time:43425ms step_avg:42.16ms
step:1031/2160 train_time:43487ms step_avg:42.18ms
step:1032/2160 train_time:43546ms step_avg:42.20ms
step:1033/2160 train_time:43608ms step_avg:42.22ms
step:1034/2160 train_time:43668ms step_avg:42.23ms
step:1035/2160 train_time:43731ms step_avg:42.25ms
step:1036/2160 train_time:43791ms step_avg:42.27ms
step:1037/2160 train_time:43853ms step_avg:42.29ms
step:1038/2160 train_time:43913ms step_avg:42.31ms
step:1039/2160 train_time:43975ms step_avg:42.32ms
step:1040/2160 train_time:44034ms step_avg:42.34ms
step:1041/2160 train_time:44096ms step_avg:42.36ms
step:1042/2160 train_time:44155ms step_avg:42.38ms
step:1043/2160 train_time:44216ms step_avg:42.39ms
step:1044/2160 train_time:44275ms step_avg:42.41ms
step:1045/2160 train_time:44337ms step_avg:42.43ms
step:1046/2160 train_time:44396ms step_avg:42.44ms
step:1047/2160 train_time:44457ms step_avg:42.46ms
step:1048/2160 train_time:44517ms step_avg:42.48ms
step:1049/2160 train_time:44578ms step_avg:42.50ms
step:1050/2160 train_time:44638ms step_avg:42.51ms
step:1051/2160 train_time:44700ms step_avg:42.53ms
step:1052/2160 train_time:44760ms step_avg:42.55ms
step:1053/2160 train_time:44821ms step_avg:42.57ms
step:1054/2160 train_time:44881ms step_avg:42.58ms
step:1055/2160 train_time:44942ms step_avg:42.60ms
step:1056/2160 train_time:45002ms step_avg:42.62ms
step:1057/2160 train_time:45063ms step_avg:42.63ms
step:1058/2160 train_time:45122ms step_avg:42.65ms
step:1059/2160 train_time:45183ms step_avg:42.67ms
step:1060/2160 train_time:45242ms step_avg:42.68ms
step:1061/2160 train_time:45303ms step_avg:42.70ms
step:1062/2160 train_time:45362ms step_avg:42.71ms
step:1063/2160 train_time:45423ms step_avg:42.73ms
step:1064/2160 train_time:45483ms step_avg:42.75ms
step:1065/2160 train_time:45544ms step_avg:42.76ms
step:1066/2160 train_time:45604ms step_avg:42.78ms
step:1067/2160 train_time:45665ms step_avg:42.80ms
step:1068/2160 train_time:45725ms step_avg:42.81ms
step:1069/2160 train_time:45786ms step_avg:42.83ms
step:1070/2160 train_time:45845ms step_avg:42.85ms
step:1071/2160 train_time:45907ms step_avg:42.86ms
step:1072/2160 train_time:45966ms step_avg:42.88ms
step:1073/2160 train_time:46027ms step_avg:42.90ms
step:1074/2160 train_time:46087ms step_avg:42.91ms
step:1075/2160 train_time:46148ms step_avg:42.93ms
step:1076/2160 train_time:46207ms step_avg:42.94ms
step:1077/2160 train_time:46268ms step_avg:42.96ms
step:1078/2160 train_time:46327ms step_avg:42.98ms
step:1079/2160 train_time:46389ms step_avg:42.99ms
step:1080/2160 train_time:46448ms step_avg:43.01ms
step:1081/2160 train_time:46509ms step_avg:43.02ms
step:1082/2160 train_time:46569ms step_avg:43.04ms
step:1083/2160 train_time:46631ms step_avg:43.06ms
step:1084/2160 train_time:46691ms step_avg:43.07ms
step:1085/2160 train_time:46752ms step_avg:43.09ms
step:1086/2160 train_time:46812ms step_avg:43.10ms
step:1087/2160 train_time:46873ms step_avg:43.12ms
step:1088/2160 train_time:46933ms step_avg:43.14ms
step:1089/2160 train_time:46994ms step_avg:43.15ms
step:1090/2160 train_time:47053ms step_avg:43.17ms
step:1091/2160 train_time:47115ms step_avg:43.19ms
step:1092/2160 train_time:47175ms step_avg:43.20ms
step:1093/2160 train_time:47237ms step_avg:43.22ms
step:1094/2160 train_time:47296ms step_avg:43.23ms
step:1095/2160 train_time:47358ms step_avg:43.25ms
step:1096/2160 train_time:47418ms step_avg:43.26ms
step:1097/2160 train_time:47479ms step_avg:43.28ms
step:1098/2160 train_time:47539ms step_avg:43.30ms
step:1099/2160 train_time:47600ms step_avg:43.31ms
step:1100/2160 train_time:47660ms step_avg:43.33ms
step:1101/2160 train_time:47721ms step_avg:43.34ms
step:1102/2160 train_time:47782ms step_avg:43.36ms
step:1103/2160 train_time:47843ms step_avg:43.38ms
step:1104/2160 train_time:47904ms step_avg:43.39ms
step:1105/2160 train_time:47965ms step_avg:43.41ms
step:1106/2160 train_time:48024ms step_avg:43.42ms
step:1107/2160 train_time:48085ms step_avg:43.44ms
step:1108/2160 train_time:48144ms step_avg:43.45ms
step:1109/2160 train_time:48205ms step_avg:43.47ms
step:1110/2160 train_time:48264ms step_avg:43.48ms
step:1111/2160 train_time:48326ms step_avg:43.50ms
step:1112/2160 train_time:48385ms step_avg:43.51ms
step:1113/2160 train_time:48446ms step_avg:43.53ms
step:1114/2160 train_time:48506ms step_avg:43.54ms
step:1115/2160 train_time:48567ms step_avg:43.56ms
step:1116/2160 train_time:48627ms step_avg:43.57ms
step:1117/2160 train_time:48689ms step_avg:43.59ms
step:1118/2160 train_time:48749ms step_avg:43.60ms
step:1119/2160 train_time:48810ms step_avg:43.62ms
step:1120/2160 train_time:48870ms step_avg:43.63ms
step:1121/2160 train_time:48931ms step_avg:43.65ms
step:1122/2160 train_time:48991ms step_avg:43.66ms
step:1123/2160 train_time:49053ms step_avg:43.68ms
step:1124/2160 train_time:49113ms step_avg:43.69ms
step:1125/2160 train_time:49175ms step_avg:43.71ms
step:1126/2160 train_time:49235ms step_avg:43.73ms
step:1127/2160 train_time:49297ms step_avg:43.74ms
step:1128/2160 train_time:49356ms step_avg:43.76ms
step:1129/2160 train_time:49417ms step_avg:43.77ms
step:1130/2160 train_time:49477ms step_avg:43.79ms
step:1131/2160 train_time:49539ms step_avg:43.80ms
step:1132/2160 train_time:49599ms step_avg:43.82ms
step:1133/2160 train_time:49662ms step_avg:43.83ms
step:1134/2160 train_time:49721ms step_avg:43.85ms
step:1135/2160 train_time:49783ms step_avg:43.86ms
step:1136/2160 train_time:49842ms step_avg:43.88ms
step:1137/2160 train_time:49903ms step_avg:43.89ms
step:1138/2160 train_time:49963ms step_avg:43.90ms
step:1139/2160 train_time:50025ms step_avg:43.92ms
step:1140/2160 train_time:50085ms step_avg:43.93ms
step:1141/2160 train_time:50146ms step_avg:43.95ms
step:1142/2160 train_time:50205ms step_avg:43.96ms
step:1143/2160 train_time:50267ms step_avg:43.98ms
step:1144/2160 train_time:50327ms step_avg:43.99ms
step:1145/2160 train_time:50388ms step_avg:44.01ms
step:1146/2160 train_time:50448ms step_avg:44.02ms
step:1147/2160 train_time:50509ms step_avg:44.04ms
step:1148/2160 train_time:50569ms step_avg:44.05ms
step:1149/2160 train_time:50631ms step_avg:44.07ms
step:1150/2160 train_time:50691ms step_avg:44.08ms
step:1151/2160 train_time:50753ms step_avg:44.09ms
step:1152/2160 train_time:50813ms step_avg:44.11ms
step:1153/2160 train_time:50875ms step_avg:44.12ms
step:1154/2160 train_time:50935ms step_avg:44.14ms
step:1155/2160 train_time:50996ms step_avg:44.15ms
step:1156/2160 train_time:51055ms step_avg:44.17ms
step:1157/2160 train_time:51117ms step_avg:44.18ms
step:1158/2160 train_time:51177ms step_avg:44.19ms
step:1159/2160 train_time:51238ms step_avg:44.21ms
step:1160/2160 train_time:51298ms step_avg:44.22ms
step:1161/2160 train_time:51359ms step_avg:44.24ms
step:1162/2160 train_time:51419ms step_avg:44.25ms
step:1163/2160 train_time:51481ms step_avg:44.27ms
step:1164/2160 train_time:51541ms step_avg:44.28ms
step:1165/2160 train_time:51602ms step_avg:44.29ms
step:1166/2160 train_time:51662ms step_avg:44.31ms
step:1167/2160 train_time:51723ms step_avg:44.32ms
step:1168/2160 train_time:51783ms step_avg:44.33ms
step:1169/2160 train_time:51845ms step_avg:44.35ms
step:1170/2160 train_time:51904ms step_avg:44.36ms
step:1171/2160 train_time:51965ms step_avg:44.38ms
step:1172/2160 train_time:52024ms step_avg:44.39ms
step:1173/2160 train_time:52085ms step_avg:44.40ms
step:1174/2160 train_time:52145ms step_avg:44.42ms
step:1175/2160 train_time:52206ms step_avg:44.43ms
step:1176/2160 train_time:52265ms step_avg:44.44ms
step:1177/2160 train_time:52326ms step_avg:44.46ms
step:1178/2160 train_time:52386ms step_avg:44.47ms
step:1179/2160 train_time:52448ms step_avg:44.49ms
step:1180/2160 train_time:52508ms step_avg:44.50ms
step:1181/2160 train_time:52569ms step_avg:44.51ms
step:1182/2160 train_time:52629ms step_avg:44.53ms
step:1183/2160 train_time:52690ms step_avg:44.54ms
step:1184/2160 train_time:52750ms step_avg:44.55ms
step:1185/2160 train_time:52813ms step_avg:44.57ms
step:1186/2160 train_time:52873ms step_avg:44.58ms
step:1187/2160 train_time:52934ms step_avg:44.59ms
step:1188/2160 train_time:52994ms step_avg:44.61ms
step:1189/2160 train_time:53055ms step_avg:44.62ms
step:1190/2160 train_time:53115ms step_avg:44.63ms
step:1191/2160 train_time:53176ms step_avg:44.65ms
step:1192/2160 train_time:53235ms step_avg:44.66ms
step:1193/2160 train_time:53297ms step_avg:44.67ms
step:1194/2160 train_time:53357ms step_avg:44.69ms
step:1195/2160 train_time:53418ms step_avg:44.70ms
step:1196/2160 train_time:53478ms step_avg:44.71ms
step:1197/2160 train_time:53540ms step_avg:44.73ms
step:1198/2160 train_time:53600ms step_avg:44.74ms
step:1199/2160 train_time:53663ms step_avg:44.76ms
step:1200/2160 train_time:53722ms step_avg:44.77ms
step:1201/2160 train_time:53784ms step_avg:44.78ms
step:1202/2160 train_time:53845ms step_avg:44.80ms
step:1203/2160 train_time:53905ms step_avg:44.81ms
step:1204/2160 train_time:53964ms step_avg:44.82ms
step:1205/2160 train_time:54025ms step_avg:44.83ms
step:1206/2160 train_time:54085ms step_avg:44.85ms
step:1207/2160 train_time:54145ms step_avg:44.86ms
step:1208/2160 train_time:54205ms step_avg:44.87ms
step:1209/2160 train_time:54266ms step_avg:44.88ms
step:1210/2160 train_time:54325ms step_avg:44.90ms
step:1211/2160 train_time:54387ms step_avg:44.91ms
step:1212/2160 train_time:54446ms step_avg:44.92ms
step:1213/2160 train_time:54508ms step_avg:44.94ms
step:1214/2160 train_time:54568ms step_avg:44.95ms
step:1215/2160 train_time:54629ms step_avg:44.96ms
step:1216/2160 train_time:54689ms step_avg:44.97ms
step:1217/2160 train_time:54750ms step_avg:44.99ms
step:1218/2160 train_time:54810ms step_avg:45.00ms
step:1219/2160 train_time:54872ms step_avg:45.01ms
step:1220/2160 train_time:54932ms step_avg:45.03ms
step:1221/2160 train_time:54993ms step_avg:45.04ms
step:1222/2160 train_time:55053ms step_avg:45.05ms
step:1223/2160 train_time:55114ms step_avg:45.06ms
step:1224/2160 train_time:55174ms step_avg:45.08ms
step:1225/2160 train_time:55236ms step_avg:45.09ms
step:1226/2160 train_time:55296ms step_avg:45.10ms
step:1227/2160 train_time:55357ms step_avg:45.12ms
step:1228/2160 train_time:55417ms step_avg:45.13ms
step:1229/2160 train_time:55479ms step_avg:45.14ms
step:1230/2160 train_time:55538ms step_avg:45.15ms
step:1231/2160 train_time:55600ms step_avg:45.17ms
step:1232/2160 train_time:55660ms step_avg:45.18ms
step:1233/2160 train_time:55722ms step_avg:45.19ms
step:1234/2160 train_time:55782ms step_avg:45.20ms
step:1235/2160 train_time:55843ms step_avg:45.22ms
step:1236/2160 train_time:55903ms step_avg:45.23ms
step:1237/2160 train_time:55965ms step_avg:45.24ms
step:1238/2160 train_time:56024ms step_avg:45.25ms
step:1239/2160 train_time:56086ms step_avg:45.27ms
step:1240/2160 train_time:56145ms step_avg:45.28ms
step:1241/2160 train_time:56206ms step_avg:45.29ms
step:1242/2160 train_time:56265ms step_avg:45.30ms
step:1243/2160 train_time:56327ms step_avg:45.32ms
step:1244/2160 train_time:56386ms step_avg:45.33ms
step:1245/2160 train_time:56447ms step_avg:45.34ms
step:1246/2160 train_time:56507ms step_avg:45.35ms
step:1247/2160 train_time:56569ms step_avg:45.36ms
step:1248/2160 train_time:56630ms step_avg:45.38ms
step:1249/2160 train_time:56691ms step_avg:45.39ms
step:1250/2160 train_time:56751ms step_avg:45.40ms
step:1250/2160 val_loss:3.5765 train_time:56814ms step_avg:45.45ms
step:1251/2160 train_time:56832ms step_avg:45.43ms
step:1252/2160 train_time:56874ms step_avg:45.43ms
step:1253/2160 train_time:56939ms step_avg:45.44ms
step:1254/2160 train_time:56999ms step_avg:45.45ms
step:1255/2160 train_time:57060ms step_avg:45.47ms
step:1256/2160 train_time:57120ms step_avg:45.48ms
step:1257/2160 train_time:57180ms step_avg:45.49ms
step:1258/2160 train_time:57239ms step_avg:45.50ms
step:1259/2160 train_time:57300ms step_avg:45.51ms
step:1260/2160 train_time:57359ms step_avg:45.52ms
step:1261/2160 train_time:57420ms step_avg:45.54ms
step:1262/2160 train_time:57480ms step_avg:45.55ms
step:1263/2160 train_time:57541ms step_avg:45.56ms
step:1264/2160 train_time:57600ms step_avg:45.57ms
step:1265/2160 train_time:57661ms step_avg:45.58ms
step:1266/2160 train_time:57721ms step_avg:45.59ms
step:1267/2160 train_time:57784ms step_avg:45.61ms
step:1268/2160 train_time:57845ms step_avg:45.62ms
step:1269/2160 train_time:57909ms step_avg:45.63ms
step:1270/2160 train_time:57970ms step_avg:45.65ms
step:1271/2160 train_time:58031ms step_avg:45.66ms
step:1272/2160 train_time:58091ms step_avg:45.67ms
step:1273/2160 train_time:58151ms step_avg:45.68ms
step:1274/2160 train_time:58210ms step_avg:45.69ms
step:1275/2160 train_time:58271ms step_avg:45.70ms
step:1276/2160 train_time:58331ms step_avg:45.71ms
step:1277/2160 train_time:58391ms step_avg:45.73ms
step:1278/2160 train_time:58451ms step_avg:45.74ms
step:1279/2160 train_time:58512ms step_avg:45.75ms
step:1280/2160 train_time:58571ms step_avg:45.76ms
step:1281/2160 train_time:58632ms step_avg:45.77ms
step:1282/2160 train_time:58692ms step_avg:45.78ms
step:1283/2160 train_time:58754ms step_avg:45.79ms
step:1284/2160 train_time:58815ms step_avg:45.81ms
step:1285/2160 train_time:58877ms step_avg:45.82ms
step:1286/2160 train_time:58938ms step_avg:45.83ms
step:1287/2160 train_time:59000ms step_avg:45.84ms
step:1288/2160 train_time:59059ms step_avg:45.85ms
step:1289/2160 train_time:59121ms step_avg:45.87ms
step:1290/2160 train_time:59181ms step_avg:45.88ms
step:1291/2160 train_time:59242ms step_avg:45.89ms
step:1292/2160 train_time:59301ms step_avg:45.90ms
step:1293/2160 train_time:59362ms step_avg:45.91ms
step:1294/2160 train_time:59421ms step_avg:45.92ms
step:1295/2160 train_time:59483ms step_avg:45.93ms
step:1296/2160 train_time:59543ms step_avg:45.94ms
step:1297/2160 train_time:59604ms step_avg:45.96ms
step:1298/2160 train_time:59664ms step_avg:45.97ms
step:1299/2160 train_time:59726ms step_avg:45.98ms
step:1300/2160 train_time:59787ms step_avg:45.99ms
step:1301/2160 train_time:59849ms step_avg:46.00ms
step:1302/2160 train_time:59909ms step_avg:46.01ms
step:1303/2160 train_time:59971ms step_avg:46.03ms
step:1304/2160 train_time:60031ms step_avg:46.04ms
step:1305/2160 train_time:60092ms step_avg:46.05ms
step:1306/2160 train_time:60153ms step_avg:46.06ms
step:1307/2160 train_time:60214ms step_avg:46.07ms
step:1308/2160 train_time:60273ms step_avg:46.08ms
step:1309/2160 train_time:60335ms step_avg:46.09ms
step:1310/2160 train_time:60394ms step_avg:46.10ms
step:1311/2160 train_time:60455ms step_avg:46.11ms
step:1312/2160 train_time:60515ms step_avg:46.12ms
step:1313/2160 train_time:60575ms step_avg:46.13ms
step:1314/2160 train_time:60635ms step_avg:46.15ms
step:1315/2160 train_time:60697ms step_avg:46.16ms
step:1316/2160 train_time:60757ms step_avg:46.17ms
step:1317/2160 train_time:60818ms step_avg:46.18ms
step:1318/2160 train_time:60877ms step_avg:46.19ms
step:1319/2160 train_time:60938ms step_avg:46.20ms
step:1320/2160 train_time:60998ms step_avg:46.21ms
step:1321/2160 train_time:61059ms step_avg:46.22ms
step:1322/2160 train_time:61119ms step_avg:46.23ms
step:1323/2160 train_time:61180ms step_avg:46.24ms
step:1324/2160 train_time:61240ms step_avg:46.25ms
step:1325/2160 train_time:61302ms step_avg:46.27ms
step:1326/2160 train_time:61361ms step_avg:46.28ms
step:1327/2160 train_time:61422ms step_avg:46.29ms
step:1328/2160 train_time:61482ms step_avg:46.30ms
step:1329/2160 train_time:61543ms step_avg:46.31ms
step:1330/2160 train_time:61603ms step_avg:46.32ms
step:1331/2160 train_time:61666ms step_avg:46.33ms
step:1332/2160 train_time:61726ms step_avg:46.34ms
step:1333/2160 train_time:61788ms step_avg:46.35ms
step:1334/2160 train_time:61848ms step_avg:46.36ms
step:1335/2160 train_time:61909ms step_avg:46.37ms
step:1336/2160 train_time:61969ms step_avg:46.38ms
step:1337/2160 train_time:62031ms step_avg:46.40ms
step:1338/2160 train_time:62091ms step_avg:46.41ms
step:1339/2160 train_time:62152ms step_avg:46.42ms
step:1340/2160 train_time:62212ms step_avg:46.43ms
step:1341/2160 train_time:62273ms step_avg:46.44ms
step:1342/2160 train_time:62333ms step_avg:46.45ms
step:1343/2160 train_time:62394ms step_avg:46.46ms
step:1344/2160 train_time:62453ms step_avg:46.47ms
step:1345/2160 train_time:62514ms step_avg:46.48ms
step:1346/2160 train_time:62574ms step_avg:46.49ms
step:1347/2160 train_time:62636ms step_avg:46.50ms
step:1348/2160 train_time:62696ms step_avg:46.51ms
step:1349/2160 train_time:62757ms step_avg:46.52ms
step:1350/2160 train_time:62816ms step_avg:46.53ms
step:1351/2160 train_time:62877ms step_avg:46.54ms
step:1352/2160 train_time:62936ms step_avg:46.55ms
step:1353/2160 train_time:62998ms step_avg:46.56ms
step:1354/2160 train_time:63057ms step_avg:46.57ms
step:1355/2160 train_time:63119ms step_avg:46.58ms
step:1356/2160 train_time:63179ms step_avg:46.59ms
step:1357/2160 train_time:63240ms step_avg:46.60ms
step:1358/2160 train_time:63300ms step_avg:46.61ms
step:1359/2160 train_time:63361ms step_avg:46.62ms
step:1360/2160 train_time:63421ms step_avg:46.63ms
step:1361/2160 train_time:63482ms step_avg:46.64ms
step:1362/2160 train_time:63542ms step_avg:46.65ms
step:1363/2160 train_time:63604ms step_avg:46.66ms
step:1364/2160 train_time:63663ms step_avg:46.67ms
step:1365/2160 train_time:63725ms step_avg:46.69ms
step:1366/2160 train_time:63784ms step_avg:46.69ms
step:1367/2160 train_time:63846ms step_avg:46.71ms
step:1368/2160 train_time:63906ms step_avg:46.71ms
step:1369/2160 train_time:63968ms step_avg:46.73ms
step:1370/2160 train_time:64029ms step_avg:46.74ms
step:1371/2160 train_time:64090ms step_avg:46.75ms
step:1372/2160 train_time:64150ms step_avg:46.76ms
step:1373/2160 train_time:64211ms step_avg:46.77ms
step:1374/2160 train_time:64271ms step_avg:46.78ms
step:1375/2160 train_time:64332ms step_avg:46.79ms
step:1376/2160 train_time:64392ms step_avg:46.80ms
step:1377/2160 train_time:64454ms step_avg:46.81ms
step:1378/2160 train_time:64514ms step_avg:46.82ms
step:1379/2160 train_time:64576ms step_avg:46.83ms
step:1380/2160 train_time:64637ms step_avg:46.84ms
step:1381/2160 train_time:64697ms step_avg:46.85ms
step:1382/2160 train_time:64757ms step_avg:46.86ms
step:1383/2160 train_time:64818ms step_avg:46.87ms
step:1384/2160 train_time:64877ms step_avg:46.88ms
step:1385/2160 train_time:64938ms step_avg:46.89ms
step:1386/2160 train_time:64997ms step_avg:46.90ms
step:1387/2160 train_time:65059ms step_avg:46.91ms
step:1388/2160 train_time:65118ms step_avg:46.92ms
step:1389/2160 train_time:65180ms step_avg:46.93ms
step:1390/2160 train_time:65240ms step_avg:46.93ms
step:1391/2160 train_time:65301ms step_avg:46.95ms
step:1392/2160 train_time:65361ms step_avg:46.95ms
step:1393/2160 train_time:65423ms step_avg:46.97ms
step:1394/2160 train_time:65483ms step_avg:46.97ms
step:1395/2160 train_time:65545ms step_avg:46.99ms
step:1396/2160 train_time:65605ms step_avg:47.00ms
step:1397/2160 train_time:65666ms step_avg:47.01ms
step:1398/2160 train_time:65725ms step_avg:47.01ms
step:1399/2160 train_time:65787ms step_avg:47.02ms
step:1400/2160 train_time:65846ms step_avg:47.03ms
step:1401/2160 train_time:65908ms step_avg:47.04ms
step:1402/2160 train_time:65968ms step_avg:47.05ms
step:1403/2160 train_time:66030ms step_avg:47.06ms
step:1404/2160 train_time:66090ms step_avg:47.07ms
step:1405/2160 train_time:66151ms step_avg:47.08ms
step:1406/2160 train_time:66211ms step_avg:47.09ms
step:1407/2160 train_time:66272ms step_avg:47.10ms
step:1408/2160 train_time:66333ms step_avg:47.11ms
step:1409/2160 train_time:66395ms step_avg:47.12ms
step:1410/2160 train_time:66454ms step_avg:47.13ms
step:1411/2160 train_time:66516ms step_avg:47.14ms
step:1412/2160 train_time:66575ms step_avg:47.15ms
step:1413/2160 train_time:66636ms step_avg:47.16ms
step:1414/2160 train_time:66695ms step_avg:47.17ms
step:1415/2160 train_time:66757ms step_avg:47.18ms
step:1416/2160 train_time:66845ms step_avg:47.21ms
step:1417/2160 train_time:66933ms step_avg:47.24ms
step:1418/2160 train_time:67021ms step_avg:47.26ms
step:1419/2160 train_time:67111ms step_avg:47.29ms
step:1420/2160 train_time:67199ms step_avg:47.32ms
step:1421/2160 train_time:67288ms step_avg:47.35ms
step:1422/2160 train_time:67376ms step_avg:47.38ms
step:1423/2160 train_time:67467ms step_avg:47.41ms
step:1424/2160 train_time:67554ms step_avg:47.44ms
step:1425/2160 train_time:67643ms step_avg:47.47ms
step:1426/2160 train_time:67730ms step_avg:47.50ms
step:1427/2160 train_time:67819ms step_avg:47.53ms
step:1428/2160 train_time:67906ms step_avg:47.55ms
step:1429/2160 train_time:67996ms step_avg:47.58ms
step:1430/2160 train_time:68084ms step_avg:47.61ms
step:1431/2160 train_time:68173ms step_avg:47.64ms
step:1432/2160 train_time:68262ms step_avg:47.67ms
step:1433/2160 train_time:68351ms step_avg:47.70ms
step:1434/2160 train_time:68440ms step_avg:47.73ms
step:1435/2160 train_time:68530ms step_avg:47.76ms
step:1436/2160 train_time:68617ms step_avg:47.78ms
step:1437/2160 train_time:68706ms step_avg:47.81ms
step:1438/2160 train_time:68794ms step_avg:47.84ms
step:1439/2160 train_time:68884ms step_avg:47.87ms
step:1440/2160 train_time:68971ms step_avg:47.90ms
step:1441/2160 train_time:69060ms step_avg:47.93ms
step:1442/2160 train_time:69149ms step_avg:47.95ms
step:1443/2160 train_time:69238ms step_avg:47.98ms
step:1444/2160 train_time:69326ms step_avg:48.01ms
step:1445/2160 train_time:69416ms step_avg:48.04ms
step:1446/2160 train_time:69504ms step_avg:48.07ms
step:1447/2160 train_time:69593ms step_avg:48.09ms
step:1448/2160 train_time:69680ms step_avg:48.12ms
step:1449/2160 train_time:69770ms step_avg:48.15ms
step:1450/2160 train_time:69858ms step_avg:48.18ms
step:1451/2160 train_time:69946ms step_avg:48.21ms
step:1452/2160 train_time:70033ms step_avg:48.23ms
step:1453/2160 train_time:70122ms step_avg:48.26ms
step:1454/2160 train_time:70210ms step_avg:48.29ms
step:1455/2160 train_time:70299ms step_avg:48.32ms
step:1456/2160 train_time:70387ms step_avg:48.34ms
step:1457/2160 train_time:70476ms step_avg:48.37ms
step:1458/2160 train_time:70564ms step_avg:48.40ms
step:1459/2160 train_time:70652ms step_avg:48.43ms
step:1460/2160 train_time:70739ms step_avg:48.45ms
step:1461/2160 train_time:70828ms step_avg:48.48ms
step:1462/2160 train_time:70916ms step_avg:48.51ms
step:1463/2160 train_time:71006ms step_avg:48.53ms
step:1464/2160 train_time:71093ms step_avg:48.56ms
step:1465/2160 train_time:71183ms step_avg:48.59ms
step:1466/2160 train_time:71270ms step_avg:48.62ms
step:1467/2160 train_time:71360ms step_avg:48.64ms
step:1468/2160 train_time:71447ms step_avg:48.67ms
step:1469/2160 train_time:71536ms step_avg:48.70ms
step:1470/2160 train_time:71624ms step_avg:48.72ms
step:1471/2160 train_time:71713ms step_avg:48.75ms
step:1472/2160 train_time:71801ms step_avg:48.78ms
step:1473/2160 train_time:71891ms step_avg:48.81ms
step:1474/2160 train_time:71978ms step_avg:48.83ms
step:1475/2160 train_time:72067ms step_avg:48.86ms
step:1476/2160 train_time:72154ms step_avg:48.88ms
step:1477/2160 train_time:72244ms step_avg:48.91ms
step:1478/2160 train_time:72331ms step_avg:48.94ms
step:1479/2160 train_time:72421ms step_avg:48.97ms
step:1480/2160 train_time:72508ms step_avg:48.99ms
step:1481/2160 train_time:72598ms step_avg:49.02ms
step:1482/2160 train_time:72685ms step_avg:49.04ms
step:1483/2160 train_time:72774ms step_avg:49.07ms
step:1484/2160 train_time:72861ms step_avg:49.10ms
step:1485/2160 train_time:72950ms step_avg:49.12ms
step:1486/2160 train_time:73037ms step_avg:49.15ms
step:1487/2160 train_time:73126ms step_avg:49.18ms
step:1488/2160 train_time:73214ms step_avg:49.20ms
step:1489/2160 train_time:73303ms step_avg:49.23ms
step:1490/2160 train_time:73390ms step_avg:49.26ms
step:1491/2160 train_time:73480ms step_avg:49.28ms
step:1492/2160 train_time:73567ms step_avg:49.31ms
step:1493/2160 train_time:73656ms step_avg:49.33ms
step:1494/2160 train_time:73744ms step_avg:49.36ms
step:1495/2160 train_time:73833ms step_avg:49.39ms
step:1496/2160 train_time:73921ms step_avg:49.41ms
step:1497/2160 train_time:74010ms step_avg:49.44ms
step:1498/2160 train_time:74097ms step_avg:49.46ms
step:1499/2160 train_time:74186ms step_avg:49.49ms
step:1500/2160 train_time:74274ms step_avg:49.52ms
step:1500/2160 val_loss:3.4965 train_time:74365ms step_avg:49.58ms
step:1501/2160 train_time:74387ms step_avg:49.56ms
step:1502/2160 train_time:74458ms step_avg:49.57ms
step:1503/2160 train_time:74548ms step_avg:49.60ms
step:1504/2160 train_time:74636ms step_avg:49.63ms
step:1505/2160 train_time:74725ms step_avg:49.65ms
step:1506/2160 train_time:74811ms step_avg:49.68ms
step:1507/2160 train_time:74899ms step_avg:49.70ms
step:1508/2160 train_time:74985ms step_avg:49.72ms
step:1509/2160 train_time:75073ms step_avg:49.75ms
step:1510/2160 train_time:75160ms step_avg:49.77ms
step:1511/2160 train_time:75249ms step_avg:49.80ms
step:1512/2160 train_time:75340ms step_avg:49.83ms
step:1513/2160 train_time:75432ms step_avg:49.86ms
step:1514/2160 train_time:75521ms step_avg:49.88ms
step:1515/2160 train_time:75612ms step_avg:49.91ms
step:1516/2160 train_time:75698ms step_avg:49.93ms
step:1517/2160 train_time:75787ms step_avg:49.96ms
step:1518/2160 train_time:75873ms step_avg:49.98ms
step:1519/2160 train_time:75961ms step_avg:50.01ms
step:1520/2160 train_time:76048ms step_avg:50.03ms
step:1521/2160 train_time:76136ms step_avg:50.06ms
step:1522/2160 train_time:76223ms step_avg:50.08ms
step:1523/2160 train_time:76313ms step_avg:50.11ms
step:1524/2160 train_time:76401ms step_avg:50.13ms
step:1525/2160 train_time:76492ms step_avg:50.16ms
step:1526/2160 train_time:76581ms step_avg:50.18ms
step:1527/2160 train_time:76670ms step_avg:50.21ms
step:1528/2160 train_time:76757ms step_avg:50.23ms
step:1529/2160 train_time:76845ms step_avg:50.26ms
step:1530/2160 train_time:76931ms step_avg:50.28ms
step:1531/2160 train_time:77020ms step_avg:50.31ms
step:1532/2160 train_time:77106ms step_avg:50.33ms
step:1533/2160 train_time:77194ms step_avg:50.36ms
step:1534/2160 train_time:77282ms step_avg:50.38ms
step:1535/2160 train_time:77372ms step_avg:50.40ms
step:1536/2160 train_time:77460ms step_avg:50.43ms
step:1537/2160 train_time:77550ms step_avg:50.46ms
step:1538/2160 train_time:77639ms step_avg:50.48ms
step:1539/2160 train_time:77728ms step_avg:50.51ms
step:1540/2160 train_time:77815ms step_avg:50.53ms
step:1541/2160 train_time:77904ms step_avg:50.55ms
step:1542/2160 train_time:77991ms step_avg:50.58ms
step:1543/2160 train_time:78079ms step_avg:50.60ms
step:1544/2160 train_time:78166ms step_avg:50.63ms
step:1545/2160 train_time:78256ms step_avg:50.65ms
step:1546/2160 train_time:78343ms step_avg:50.67ms
step:1547/2160 train_time:78433ms step_avg:50.70ms
step:1548/2160 train_time:78522ms step_avg:50.72ms
step:1549/2160 train_time:78611ms step_avg:50.75ms
step:1550/2160 train_time:78699ms step_avg:50.77ms
step:1551/2160 train_time:78787ms step_avg:50.80ms
step:1552/2160 train_time:78873ms step_avg:50.82ms
step:1553/2160 train_time:78962ms step_avg:50.84ms
step:1554/2160 train_time:79049ms step_avg:50.87ms
step:1555/2160 train_time:79137ms step_avg:50.89ms
step:1556/2160 train_time:79224ms step_avg:50.92ms
step:1557/2160 train_time:79314ms step_avg:50.94ms
step:1558/2160 train_time:79402ms step_avg:50.96ms
step:1559/2160 train_time:79492ms step_avg:50.99ms
step:1560/2160 train_time:79579ms step_avg:51.01ms
step:1561/2160 train_time:79669ms step_avg:51.04ms
step:1562/2160 train_time:79757ms step_avg:51.06ms
step:1563/2160 train_time:79846ms step_avg:51.09ms
step:1564/2160 train_time:79934ms step_avg:51.11ms
step:1565/2160 train_time:80024ms step_avg:51.13ms
step:1566/2160 train_time:80111ms step_avg:51.16ms
step:1567/2160 train_time:80199ms step_avg:51.18ms
step:1568/2160 train_time:80286ms step_avg:51.20ms
step:1569/2160 train_time:80376ms step_avg:51.23ms
step:1570/2160 train_time:80463ms step_avg:51.25ms
step:1571/2160 train_time:80553ms step_avg:51.27ms
step:1572/2160 train_time:80642ms step_avg:51.30ms
step:1573/2160 train_time:80731ms step_avg:51.32ms
step:1574/2160 train_time:80819ms step_avg:51.35ms
step:1575/2160 train_time:80909ms step_avg:51.37ms
step:1576/2160 train_time:80996ms step_avg:51.39ms
step:1577/2160 train_time:81085ms step_avg:51.42ms
step:1578/2160 train_time:81172ms step_avg:51.44ms
step:1579/2160 train_time:81261ms step_avg:51.46ms
step:1580/2160 train_time:81348ms step_avg:51.49ms
step:1581/2160 train_time:81437ms step_avg:51.51ms
step:1582/2160 train_time:81525ms step_avg:51.53ms
step:1583/2160 train_time:81615ms step_avg:51.56ms
step:1584/2160 train_time:81702ms step_avg:51.58ms
step:1585/2160 train_time:81791ms step_avg:51.60ms
step:1586/2160 train_time:81879ms step_avg:51.63ms
step:1587/2160 train_time:81969ms step_avg:51.65ms
step:1588/2160 train_time:82056ms step_avg:51.67ms
step:1589/2160 train_time:82145ms step_avg:51.70ms
step:1590/2160 train_time:82232ms step_avg:51.72ms
step:1591/2160 train_time:82321ms step_avg:51.74ms
step:1592/2160 train_time:82408ms step_avg:51.76ms
step:1593/2160 train_time:82497ms step_avg:51.79ms
step:1594/2160 train_time:82585ms step_avg:51.81ms
step:1595/2160 train_time:82674ms step_avg:51.83ms
step:1596/2160 train_time:82761ms step_avg:51.86ms
step:1597/2160 train_time:82850ms step_avg:51.88ms
step:1598/2160 train_time:82938ms step_avg:51.90ms
step:1599/2160 train_time:83027ms step_avg:51.92ms
step:1600/2160 train_time:83114ms step_avg:51.95ms
step:1601/2160 train_time:83203ms step_avg:51.97ms
step:1602/2160 train_time:83291ms step_avg:51.99ms
step:1603/2160 train_time:83380ms step_avg:52.01ms
step:1604/2160 train_time:83467ms step_avg:52.04ms
step:1605/2160 train_time:83557ms step_avg:52.06ms
step:1606/2160 train_time:83644ms step_avg:52.08ms
step:1607/2160 train_time:83733ms step_avg:52.11ms
step:1608/2160 train_time:83822ms step_avg:52.13ms
step:1609/2160 train_time:83911ms step_avg:52.15ms
step:1610/2160 train_time:83998ms step_avg:52.17ms
step:1611/2160 train_time:84088ms step_avg:52.20ms
step:1612/2160 train_time:84175ms step_avg:52.22ms
step:1613/2160 train_time:84264ms step_avg:52.24ms
step:1614/2160 train_time:84352ms step_avg:52.26ms
step:1615/2160 train_time:84442ms step_avg:52.29ms
step:1616/2160 train_time:84529ms step_avg:52.31ms
step:1617/2160 train_time:84618ms step_avg:52.33ms
step:1618/2160 train_time:84705ms step_avg:52.35ms
step:1619/2160 train_time:84795ms step_avg:52.37ms
step:1620/2160 train_time:84882ms step_avg:52.40ms
step:1621/2160 train_time:84971ms step_avg:52.42ms
step:1622/2160 train_time:85058ms step_avg:52.44ms
step:1623/2160 train_time:85147ms step_avg:52.46ms
step:1624/2160 train_time:85235ms step_avg:52.48ms
step:1625/2160 train_time:85324ms step_avg:52.51ms
step:1626/2160 train_time:85412ms step_avg:52.53ms
step:1627/2160 train_time:85502ms step_avg:52.55ms
step:1628/2160 train_time:85590ms step_avg:52.57ms
step:1629/2160 train_time:85678ms step_avg:52.60ms
step:1630/2160 train_time:85765ms step_avg:52.62ms
step:1631/2160 train_time:85856ms step_avg:52.64ms
step:1632/2160 train_time:85944ms step_avg:52.66ms
step:1633/2160 train_time:86033ms step_avg:52.68ms
step:1634/2160 train_time:86120ms step_avg:52.70ms
step:1635/2160 train_time:86211ms step_avg:52.73ms
step:1636/2160 train_time:86298ms step_avg:52.75ms
step:1637/2160 train_time:86388ms step_avg:52.77ms
step:1638/2160 train_time:86475ms step_avg:52.79ms
step:1639/2160 train_time:86564ms step_avg:52.82ms
step:1640/2160 train_time:86651ms step_avg:52.84ms
step:1641/2160 train_time:86741ms step_avg:52.86ms
step:1642/2160 train_time:86828ms step_avg:52.88ms
step:1643/2160 train_time:86917ms step_avg:52.90ms
step:1644/2160 train_time:87004ms step_avg:52.92ms
step:1645/2160 train_time:87093ms step_avg:52.94ms
step:1646/2160 train_time:87182ms step_avg:52.97ms
step:1647/2160 train_time:87271ms step_avg:52.99ms
step:1648/2160 train_time:87360ms step_avg:53.01ms
step:1649/2160 train_time:87449ms step_avg:53.03ms
step:1650/2160 train_time:87536ms step_avg:53.05ms
step:1651/2160 train_time:87626ms step_avg:53.07ms
step:1652/2160 train_time:87713ms step_avg:53.10ms
step:1653/2160 train_time:87803ms step_avg:53.12ms
step:1654/2160 train_time:87890ms step_avg:53.14ms
step:1655/2160 train_time:87979ms step_avg:53.16ms
step:1656/2160 train_time:88067ms step_avg:53.18ms
step:1657/2160 train_time:88156ms step_avg:53.20ms
step:1658/2160 train_time:88245ms step_avg:53.22ms
step:1659/2160 train_time:88333ms step_avg:53.25ms
step:1660/2160 train_time:88421ms step_avg:53.27ms
step:1661/2160 train_time:88511ms step_avg:53.29ms
step:1662/2160 train_time:88598ms step_avg:53.31ms
step:1663/2160 train_time:88687ms step_avg:53.33ms
step:1664/2160 train_time:88774ms step_avg:53.35ms
step:1665/2160 train_time:88863ms step_avg:53.37ms
step:1666/2160 train_time:88951ms step_avg:53.39ms
step:1667/2160 train_time:89040ms step_avg:53.41ms
step:1668/2160 train_time:89128ms step_avg:53.43ms
step:1669/2160 train_time:89217ms step_avg:53.46ms
step:1670/2160 train_time:89305ms step_avg:53.48ms
step:1671/2160 train_time:89396ms step_avg:53.50ms
step:1672/2160 train_time:89484ms step_avg:53.52ms
step:1673/2160 train_time:89573ms step_avg:53.54ms
step:1674/2160 train_time:89660ms step_avg:53.56ms
step:1675/2160 train_time:89748ms step_avg:53.58ms
step:1676/2160 train_time:89835ms step_avg:53.60ms
step:1677/2160 train_time:89924ms step_avg:53.62ms
step:1678/2160 train_time:90012ms step_avg:53.64ms
step:1679/2160 train_time:90101ms step_avg:53.66ms
step:1680/2160 train_time:90189ms step_avg:53.68ms
step:1681/2160 train_time:90280ms step_avg:53.71ms
step:1682/2160 train_time:90367ms step_avg:53.73ms
step:1683/2160 train_time:90456ms step_avg:53.75ms
step:1684/2160 train_time:90542ms step_avg:53.77ms
step:1685/2160 train_time:90632ms step_avg:53.79ms
step:1686/2160 train_time:90719ms step_avg:53.81ms
step:1687/2160 train_time:90808ms step_avg:53.83ms
step:1688/2160 train_time:90895ms step_avg:53.85ms
step:1689/2160 train_time:90985ms step_avg:53.87ms
step:1690/2160 train_time:91072ms step_avg:53.89ms
step:1691/2160 train_time:91161ms step_avg:53.91ms
step:1692/2160 train_time:91249ms step_avg:53.93ms
step:1693/2160 train_time:91339ms step_avg:53.95ms
step:1694/2160 train_time:91426ms step_avg:53.97ms
step:1695/2160 train_time:91515ms step_avg:53.99ms
step:1696/2160 train_time:91602ms step_avg:54.01ms
step:1697/2160 train_time:91692ms step_avg:54.03ms
step:1698/2160 train_time:91779ms step_avg:54.05ms
step:1699/2160 train_time:91868ms step_avg:54.07ms
step:1700/2160 train_time:91956ms step_avg:54.09ms
step:1701/2160 train_time:92044ms step_avg:54.11ms
step:1702/2160 train_time:92131ms step_avg:54.13ms
step:1703/2160 train_time:92220ms step_avg:54.15ms
step:1704/2160 train_time:92308ms step_avg:54.17ms
step:1705/2160 train_time:92396ms step_avg:54.19ms
step:1706/2160 train_time:92483ms step_avg:54.21ms
step:1707/2160 train_time:92572ms step_avg:54.23ms
step:1708/2160 train_time:92660ms step_avg:54.25ms
step:1709/2160 train_time:92750ms step_avg:54.27ms
step:1710/2160 train_time:92837ms step_avg:54.29ms
step:1711/2160 train_time:92926ms step_avg:54.31ms
step:1712/2160 train_time:93013ms step_avg:54.33ms
step:1713/2160 train_time:93102ms step_avg:54.35ms
step:1714/2160 train_time:93189ms step_avg:54.37ms
step:1715/2160 train_time:93279ms step_avg:54.39ms
step:1716/2160 train_time:93366ms step_avg:54.41ms
step:1717/2160 train_time:93454ms step_avg:54.43ms
step:1718/2160 train_time:93542ms step_avg:54.45ms
step:1719/2160 train_time:93632ms step_avg:54.47ms
step:1720/2160 train_time:93719ms step_avg:54.49ms
step:1721/2160 train_time:93808ms step_avg:54.51ms
step:1722/2160 train_time:93895ms step_avg:54.53ms
step:1723/2160 train_time:93985ms step_avg:54.55ms
step:1724/2160 train_time:94072ms step_avg:54.57ms
step:1725/2160 train_time:94162ms step_avg:54.59ms
step:1726/2160 train_time:94249ms step_avg:54.61ms
step:1727/2160 train_time:94338ms step_avg:54.63ms
step:1728/2160 train_time:94426ms step_avg:54.64ms
step:1729/2160 train_time:94515ms step_avg:54.66ms
step:1730/2160 train_time:94603ms step_avg:54.68ms
step:1731/2160 train_time:94692ms step_avg:54.70ms
step:1732/2160 train_time:94780ms step_avg:54.72ms
step:1733/2160 train_time:94869ms step_avg:54.74ms
step:1734/2160 train_time:94957ms step_avg:54.76ms
step:1735/2160 train_time:95046ms step_avg:54.78ms
step:1736/2160 train_time:95133ms step_avg:54.80ms
step:1737/2160 train_time:95222ms step_avg:54.82ms
step:1738/2160 train_time:95310ms step_avg:54.84ms
step:1739/2160 train_time:95399ms step_avg:54.86ms
step:1740/2160 train_time:95486ms step_avg:54.88ms
step:1741/2160 train_time:95575ms step_avg:54.90ms
step:1742/2160 train_time:95663ms step_avg:54.92ms
step:1743/2160 train_time:95753ms step_avg:54.94ms
step:1744/2160 train_time:95841ms step_avg:54.95ms
step:1745/2160 train_time:95931ms step_avg:54.98ms
step:1746/2160 train_time:96019ms step_avg:54.99ms
step:1747/2160 train_time:96108ms step_avg:55.01ms
step:1748/2160 train_time:96196ms step_avg:55.03ms
step:1749/2160 train_time:96285ms step_avg:55.05ms
step:1750/2160 train_time:96372ms step_avg:55.07ms
step:1750/2160 val_loss:3.3932 train_time:96463ms step_avg:55.12ms
step:1751/2160 train_time:96482ms step_avg:55.10ms
step:1752/2160 train_time:96555ms step_avg:55.11ms
step:1753/2160 train_time:96646ms step_avg:55.13ms
step:1754/2160 train_time:96733ms step_avg:55.15ms
step:1755/2160 train_time:96821ms step_avg:55.17ms
step:1756/2160 train_time:96908ms step_avg:55.19ms
step:1757/2160 train_time:96995ms step_avg:55.21ms
step:1758/2160 train_time:97083ms step_avg:55.22ms
step:1759/2160 train_time:97171ms step_avg:55.24ms
step:1760/2160 train_time:97259ms step_avg:55.26ms
step:1761/2160 train_time:97347ms step_avg:55.28ms
step:1762/2160 train_time:97437ms step_avg:55.30ms
step:1763/2160 train_time:97528ms step_avg:55.32ms
step:1764/2160 train_time:97616ms step_avg:55.34ms
step:1765/2160 train_time:97705ms step_avg:55.36ms
step:1766/2160 train_time:97793ms step_avg:55.38ms
step:1767/2160 train_time:97881ms step_avg:55.39ms
step:1768/2160 train_time:97968ms step_avg:55.41ms
step:1769/2160 train_time:98056ms step_avg:55.43ms
step:1770/2160 train_time:98143ms step_avg:55.45ms
step:1771/2160 train_time:98232ms step_avg:55.47ms
step:1772/2160 train_time:98321ms step_avg:55.49ms
step:1773/2160 train_time:98411ms step_avg:55.51ms
step:1774/2160 train_time:98499ms step_avg:55.52ms
step:1775/2160 train_time:98588ms step_avg:55.54ms
step:1776/2160 train_time:98676ms step_avg:55.56ms
step:1777/2160 train_time:98766ms step_avg:55.58ms
step:1778/2160 train_time:98853ms step_avg:55.60ms
step:1779/2160 train_time:98941ms step_avg:55.62ms
step:1780/2160 train_time:99028ms step_avg:55.63ms
step:1781/2160 train_time:99117ms step_avg:55.65ms
step:1782/2160 train_time:99203ms step_avg:55.67ms
step:1783/2160 train_time:99292ms step_avg:55.69ms
step:1784/2160 train_time:99381ms step_avg:55.71ms
step:1785/2160 train_time:99471ms step_avg:55.73ms
step:1786/2160 train_time:99559ms step_avg:55.74ms
step:1787/2160 train_time:99649ms step_avg:55.76ms
step:1788/2160 train_time:99736ms step_avg:55.78ms
step:1789/2160 train_time:99825ms step_avg:55.80ms
step:1790/2160 train_time:99913ms step_avg:55.82ms
step:1791/2160 train_time:100004ms step_avg:55.84ms
step:1792/2160 train_time:100091ms step_avg:55.85ms
step:1793/2160 train_time:100180ms step_avg:55.87ms
step:1794/2160 train_time:100266ms step_avg:55.89ms
step:1795/2160 train_time:100356ms step_avg:55.91ms
step:1796/2160 train_time:100445ms step_avg:55.93ms
step:1797/2160 train_time:100534ms step_avg:55.95ms
step:1798/2160 train_time:100622ms step_avg:55.96ms
step:1799/2160 train_time:100712ms step_avg:55.98ms
step:1800/2160 train_time:100799ms step_avg:56.00ms
step:1801/2160 train_time:100890ms step_avg:56.02ms
step:1802/2160 train_time:100978ms step_avg:56.04ms
step:1803/2160 train_time:101066ms step_avg:56.05ms
step:1804/2160 train_time:101154ms step_avg:56.07ms
step:1805/2160 train_time:101242ms step_avg:56.09ms
step:1806/2160 train_time:101330ms step_avg:56.11ms
step:1807/2160 train_time:101419ms step_avg:56.13ms
step:1808/2160 train_time:101506ms step_avg:56.14ms
step:1809/2160 train_time:101595ms step_avg:56.16ms
step:1810/2160 train_time:101683ms step_avg:56.18ms
step:1811/2160 train_time:101772ms step_avg:56.20ms
step:1812/2160 train_time:101859ms step_avg:56.21ms
step:1813/2160 train_time:101949ms step_avg:56.23ms
step:1814/2160 train_time:102036ms step_avg:56.25ms
step:1815/2160 train_time:102125ms step_avg:56.27ms
step:1816/2160 train_time:102212ms step_avg:56.28ms
step:1817/2160 train_time:102302ms step_avg:56.30ms
step:1818/2160 train_time:102390ms step_avg:56.32ms
step:1819/2160 train_time:102479ms step_avg:56.34ms
step:1820/2160 train_time:102567ms step_avg:56.36ms
step:1821/2160 train_time:102656ms step_avg:56.37ms
step:1822/2160 train_time:102743ms step_avg:56.39ms
step:1823/2160 train_time:102833ms step_avg:56.41ms
step:1824/2160 train_time:102920ms step_avg:56.43ms
step:1825/2160 train_time:103009ms step_avg:56.44ms
step:1826/2160 train_time:103095ms step_avg:56.46ms
step:1827/2160 train_time:103185ms step_avg:56.48ms
step:1828/2160 train_time:103273ms step_avg:56.50ms
step:1829/2160 train_time:103363ms step_avg:56.51ms
step:1830/2160 train_time:103450ms step_avg:56.53ms
step:1831/2160 train_time:103540ms step_avg:56.55ms
step:1832/2160 train_time:103627ms step_avg:56.57ms
step:1833/2160 train_time:103716ms step_avg:56.58ms
step:1834/2160 train_time:103803ms step_avg:56.60ms
step:1835/2160 train_time:103893ms step_avg:56.62ms
step:1836/2160 train_time:103980ms step_avg:56.63ms
step:1837/2160 train_time:104069ms step_avg:56.65ms
step:1838/2160 train_time:104157ms step_avg:56.67ms
step:1839/2160 train_time:104246ms step_avg:56.69ms
step:1840/2160 train_time:104334ms step_avg:56.70ms
step:1841/2160 train_time:104423ms step_avg:56.72ms
step:1842/2160 train_time:104511ms step_avg:56.74ms
step:1843/2160 train_time:104601ms step_avg:56.76ms
step:1844/2160 train_time:104688ms step_avg:56.77ms
step:1845/2160 train_time:104777ms step_avg:56.79ms
step:1846/2160 train_time:104864ms step_avg:56.81ms
step:1847/2160 train_time:104954ms step_avg:56.82ms
step:1848/2160 train_time:105042ms step_avg:56.84ms
step:1849/2160 train_time:105131ms step_avg:56.86ms
step:1850/2160 train_time:105218ms step_avg:56.87ms
step:1851/2160 train_time:105307ms step_avg:56.89ms
step:1852/2160 train_time:105395ms step_avg:56.91ms
step:1853/2160 train_time:105484ms step_avg:56.93ms
step:1854/2160 train_time:105571ms step_avg:56.94ms
step:1855/2160 train_time:105660ms step_avg:56.96ms
step:1856/2160 train_time:105748ms step_avg:56.98ms
step:1857/2160 train_time:105837ms step_avg:56.99ms
step:1858/2160 train_time:105925ms step_avg:57.01ms
step:1859/2160 train_time:106015ms step_avg:57.03ms
step:1860/2160 train_time:106103ms step_avg:57.04ms
step:1861/2160 train_time:106192ms step_avg:57.06ms
step:1862/2160 train_time:106280ms step_avg:57.08ms
step:1863/2160 train_time:106370ms step_avg:57.10ms
step:1864/2160 train_time:106457ms step_avg:57.11ms
step:1865/2160 train_time:106546ms step_avg:57.13ms
step:1866/2160 train_time:106633ms step_avg:57.15ms
step:1867/2160 train_time:106722ms step_avg:57.16ms
step:1868/2160 train_time:106810ms step_avg:57.18ms
step:1869/2160 train_time:106900ms step_avg:57.20ms
step:1870/2160 train_time:106988ms step_avg:57.21ms
step:1871/2160 train_time:107077ms step_avg:57.23ms
step:1872/2160 train_time:107165ms step_avg:57.25ms
step:1873/2160 train_time:107255ms step_avg:57.26ms
step:1874/2160 train_time:107343ms step_avg:57.28ms
step:1875/2160 train_time:107432ms step_avg:57.30ms
step:1876/2160 train_time:107519ms step_avg:57.31ms
step:1877/2160 train_time:107607ms step_avg:57.33ms
step:1878/2160 train_time:107695ms step_avg:57.35ms
step:1879/2160 train_time:107784ms step_avg:57.36ms
step:1880/2160 train_time:107872ms step_avg:57.38ms
step:1881/2160 train_time:107962ms step_avg:57.40ms
step:1882/2160 train_time:108049ms step_avg:57.41ms
step:1883/2160 train_time:108138ms step_avg:57.43ms
step:1884/2160 train_time:108225ms step_avg:57.44ms
step:1885/2160 train_time:108315ms step_avg:57.46ms
step:1886/2160 train_time:108403ms step_avg:57.48ms
step:1887/2160 train_time:108493ms step_avg:57.50ms
step:1888/2160 train_time:108580ms step_avg:57.51ms
step:1889/2160 train_time:108671ms step_avg:57.53ms
step:1890/2160 train_time:108759ms step_avg:57.54ms
step:1891/2160 train_time:108848ms step_avg:57.56ms
step:1892/2160 train_time:108936ms step_avg:57.58ms
step:1893/2160 train_time:109026ms step_avg:57.59ms
step:1894/2160 train_time:109113ms step_avg:57.61ms
step:1895/2160 train_time:109202ms step_avg:57.63ms
step:1896/2160 train_time:109290ms step_avg:57.64ms
step:1897/2160 train_time:109380ms step_avg:57.66ms
step:1898/2160 train_time:109467ms step_avg:57.68ms
step:1899/2160 train_time:109557ms step_avg:57.69ms
step:1900/2160 train_time:109645ms step_avg:57.71ms
step:1901/2160 train_time:109734ms step_avg:57.72ms
step:1902/2160 train_time:109821ms step_avg:57.74ms
step:1903/2160 train_time:109911ms step_avg:57.76ms
step:1904/2160 train_time:109999ms step_avg:57.77ms
step:1905/2160 train_time:110088ms step_avg:57.79ms
step:1906/2160 train_time:110175ms step_avg:57.80ms
step:1907/2160 train_time:110265ms step_avg:57.82ms
step:1908/2160 train_time:110353ms step_avg:57.84ms
step:1909/2160 train_time:110443ms step_avg:57.85ms
step:1910/2160 train_time:110531ms step_avg:57.87ms
step:1911/2160 train_time:110620ms step_avg:57.89ms
step:1912/2160 train_time:110707ms step_avg:57.90ms
step:1913/2160 train_time:110797ms step_avg:57.92ms
step:1914/2160 train_time:110884ms step_avg:57.93ms
step:1915/2160 train_time:110974ms step_avg:57.95ms
step:1916/2160 train_time:111062ms step_avg:57.97ms
step:1917/2160 train_time:111151ms step_avg:57.98ms
step:1918/2160 train_time:111238ms step_avg:58.00ms
step:1919/2160 train_time:111328ms step_avg:58.01ms
step:1920/2160 train_time:111415ms step_avg:58.03ms
step:1921/2160 train_time:111504ms step_avg:58.04ms
step:1922/2160 train_time:111592ms step_avg:58.06ms
step:1923/2160 train_time:111681ms step_avg:58.08ms
step:1924/2160 train_time:111767ms step_avg:58.09ms
step:1925/2160 train_time:111857ms step_avg:58.11ms
step:1926/2160 train_time:111944ms step_avg:58.12ms
step:1927/2160 train_time:112033ms step_avg:58.14ms
step:1928/2160 train_time:112121ms step_avg:58.15ms
step:1929/2160 train_time:112210ms step_avg:58.17ms
step:1930/2160 train_time:112298ms step_avg:58.19ms
step:1931/2160 train_time:112388ms step_avg:58.20ms
step:1932/2160 train_time:112475ms step_avg:58.22ms
step:1933/2160 train_time:112564ms step_avg:58.23ms
step:1934/2160 train_time:112652ms step_avg:58.25ms
step:1935/2160 train_time:112742ms step_avg:58.26ms
step:1936/2160 train_time:112829ms step_avg:58.28ms
step:1937/2160 train_time:112919ms step_avg:58.30ms
step:1938/2160 train_time:113006ms step_avg:58.31ms
step:1939/2160 train_time:113096ms step_avg:58.33ms
step:1940/2160 train_time:113183ms step_avg:58.34ms
step:1941/2160 train_time:113272ms step_avg:58.36ms
step:1942/2160 train_time:113360ms step_avg:58.37ms
step:1943/2160 train_time:113450ms step_avg:58.39ms
step:1944/2160 train_time:113538ms step_avg:58.40ms
step:1945/2160 train_time:113627ms step_avg:58.42ms
step:1946/2160 train_time:113714ms step_avg:58.43ms
step:1947/2160 train_time:113804ms step_avg:58.45ms
step:1948/2160 train_time:113890ms step_avg:58.47ms
step:1949/2160 train_time:113980ms step_avg:58.48ms
step:1950/2160 train_time:114068ms step_avg:58.50ms
step:1951/2160 train_time:114158ms step_avg:58.51ms
step:1952/2160 train_time:114246ms step_avg:58.53ms
step:1953/2160 train_time:114335ms step_avg:58.54ms
step:1954/2160 train_time:114422ms step_avg:58.56ms
step:1955/2160 train_time:114512ms step_avg:58.57ms
step:1956/2160 train_time:114600ms step_avg:58.59ms
step:1957/2160 train_time:114688ms step_avg:58.60ms
step:1958/2160 train_time:114778ms step_avg:58.62ms
step:1959/2160 train_time:114866ms step_avg:58.64ms
step:1960/2160 train_time:114954ms step_avg:58.65ms
step:1961/2160 train_time:115043ms step_avg:58.67ms
step:1962/2160 train_time:115131ms step_avg:58.68ms
step:1963/2160 train_time:115221ms step_avg:58.70ms
step:1964/2160 train_time:115308ms step_avg:58.71ms
step:1965/2160 train_time:115397ms step_avg:58.73ms
step:1966/2160 train_time:115485ms step_avg:58.74ms
step:1967/2160 train_time:115574ms step_avg:58.76ms
step:1968/2160 train_time:115662ms step_avg:58.77ms
step:1969/2160 train_time:115751ms step_avg:58.79ms
step:1970/2160 train_time:115838ms step_avg:58.80ms
step:1971/2160 train_time:115928ms step_avg:58.82ms
step:1972/2160 train_time:116014ms step_avg:58.83ms
step:1973/2160 train_time:116104ms step_avg:58.85ms
step:1974/2160 train_time:116192ms step_avg:58.86ms
step:1975/2160 train_time:116281ms step_avg:58.88ms
step:1976/2160 train_time:116369ms step_avg:58.89ms
step:1977/2160 train_time:116458ms step_avg:58.91ms
step:1978/2160 train_time:116545ms step_avg:58.92ms
step:1979/2160 train_time:116634ms step_avg:58.94ms
step:1980/2160 train_time:116722ms step_avg:58.95ms
step:1981/2160 train_time:116811ms step_avg:58.97ms
step:1982/2160 train_time:116898ms step_avg:58.98ms
step:1983/2160 train_time:116988ms step_avg:59.00ms
step:1984/2160 train_time:117076ms step_avg:59.01ms
step:1985/2160 train_time:117165ms step_avg:59.03ms
step:1986/2160 train_time:117251ms step_avg:59.04ms
step:1987/2160 train_time:117341ms step_avg:59.05ms
step:1988/2160 train_time:117429ms step_avg:59.07ms
step:1989/2160 train_time:117518ms step_avg:59.08ms
step:1990/2160 train_time:117605ms step_avg:59.10ms
step:1991/2160 train_time:117695ms step_avg:59.11ms
step:1992/2160 train_time:117782ms step_avg:59.13ms
step:1993/2160 train_time:117871ms step_avg:59.14ms
step:1994/2160 train_time:117958ms step_avg:59.16ms
step:1995/2160 train_time:118048ms step_avg:59.17ms
step:1996/2160 train_time:118136ms step_avg:59.19ms
step:1997/2160 train_time:118224ms step_avg:59.20ms
step:1998/2160 train_time:118311ms step_avg:59.21ms
step:1999/2160 train_time:118402ms step_avg:59.23ms
step:2000/2160 train_time:118490ms step_avg:59.24ms
step:2000/2160 val_loss:3.3135 train_time:118579ms step_avg:59.29ms
step:2001/2160 train_time:118600ms step_avg:59.27ms
step:2002/2160 train_time:118671ms step_avg:59.28ms
step:2003/2160 train_time:118761ms step_avg:59.29ms
step:2004/2160 train_time:118849ms step_avg:59.31ms
step:2005/2160 train_time:118937ms step_avg:59.32ms
step:2006/2160 train_time:119024ms step_avg:59.33ms
step:2007/2160 train_time:119112ms step_avg:59.35ms
step:2008/2160 train_time:119199ms step_avg:59.36ms
step:2009/2160 train_time:119287ms step_avg:59.38ms
step:2010/2160 train_time:119374ms step_avg:59.39ms
step:2011/2160 train_time:119463ms step_avg:59.40ms
step:2012/2160 train_time:119552ms step_avg:59.42ms
step:2013/2160 train_time:119643ms step_avg:59.44ms
step:2014/2160 train_time:119733ms step_avg:59.45ms
step:2015/2160 train_time:119824ms step_avg:59.47ms
step:2016/2160 train_time:119911ms step_avg:59.48ms
step:2017/2160 train_time:119999ms step_avg:59.49ms
step:2018/2160 train_time:120086ms step_avg:59.51ms
step:2019/2160 train_time:120174ms step_avg:59.52ms
step:2020/2160 train_time:120261ms step_avg:59.54ms
step:2021/2160 train_time:120350ms step_avg:59.55ms
step:2022/2160 train_time:120439ms step_avg:59.56ms
step:2023/2160 train_time:120529ms step_avg:59.58ms
step:2024/2160 train_time:120618ms step_avg:59.59ms
step:2025/2160 train_time:120709ms step_avg:59.61ms
step:2026/2160 train_time:120798ms step_avg:59.62ms
step:2027/2160 train_time:120889ms step_avg:59.64ms
step:2028/2160 train_time:120977ms step_avg:59.65ms
step:2029/2160 train_time:121065ms step_avg:59.67ms
step:2030/2160 train_time:121152ms step_avg:59.68ms
step:2031/2160 train_time:121240ms step_avg:59.69ms
step:2032/2160 train_time:121327ms step_avg:59.71ms
step:2033/2160 train_time:121417ms step_avg:59.72ms
step:2034/2160 train_time:121505ms step_avg:59.74ms
step:2035/2160 train_time:121595ms step_avg:59.75ms
step:2036/2160 train_time:121683ms step_avg:59.77ms
step:2037/2160 train_time:121774ms step_avg:59.78ms
step:2038/2160 train_time:121861ms step_avg:59.79ms
step:2039/2160 train_time:121950ms step_avg:59.81ms
step:2040/2160 train_time:122037ms step_avg:59.82ms
step:2041/2160 train_time:122127ms step_avg:59.84ms
step:2042/2160 train_time:122213ms step_avg:59.85ms
step:2043/2160 train_time:122302ms step_avg:59.86ms
step:2044/2160 train_time:122391ms step_avg:59.88ms
step:2045/2160 train_time:122480ms step_avg:59.89ms
step:2046/2160 train_time:122568ms step_avg:59.91ms
step:2047/2160 train_time:122657ms step_avg:59.92ms
step:2048/2160 train_time:122745ms step_avg:59.93ms
step:2049/2160 train_time:122835ms step_avg:59.95ms
step:2050/2160 train_time:122923ms step_avg:59.96ms
step:2051/2160 train_time:123012ms step_avg:59.98ms
step:2052/2160 train_time:123099ms step_avg:59.99ms
step:2053/2160 train_time:123189ms step_avg:60.00ms
step:2054/2160 train_time:123276ms step_avg:60.02ms
step:2055/2160 train_time:123366ms step_avg:60.03ms
step:2056/2160 train_time:123453ms step_avg:60.05ms
step:2057/2160 train_time:123542ms step_avg:60.06ms
step:2058/2160 train_time:123630ms step_avg:60.07ms
step:2059/2160 train_time:123719ms step_avg:60.09ms
step:2060/2160 train_time:123807ms step_avg:60.10ms
step:2061/2160 train_time:123897ms step_avg:60.11ms
step:2062/2160 train_time:123984ms step_avg:60.13ms
step:2063/2160 train_time:124073ms step_avg:60.14ms
step:2064/2160 train_time:124160ms step_avg:60.16ms
step:2065/2160 train_time:124250ms step_avg:60.17ms
step:2066/2160 train_time:124337ms step_avg:60.18ms
step:2067/2160 train_time:124426ms step_avg:60.20ms
step:2068/2160 train_time:124515ms step_avg:60.21ms
step:2069/2160 train_time:124605ms step_avg:60.22ms
step:2070/2160 train_time:124692ms step_avg:60.24ms
step:2071/2160 train_time:124781ms step_avg:60.25ms
step:2072/2160 train_time:124869ms step_avg:60.26ms
step:2073/2160 train_time:124958ms step_avg:60.28ms
step:2074/2160 train_time:125045ms step_avg:60.29ms
step:2075/2160 train_time:125135ms step_avg:60.31ms
step:2076/2160 train_time:125222ms step_avg:60.32ms
step:2077/2160 train_time:125312ms step_avg:60.33ms
step:2078/2160 train_time:125400ms step_avg:60.35ms
step:2079/2160 train_time:125489ms step_avg:60.36ms
step:2080/2160 train_time:125576ms step_avg:60.37ms
step:2081/2160 train_time:125665ms step_avg:60.39ms
step:2082/2160 train_time:125752ms step_avg:60.40ms
step:2083/2160 train_time:125841ms step_avg:60.41ms
step:2084/2160 train_time:125929ms step_avg:60.43ms
step:2085/2160 train_time:126018ms step_avg:60.44ms
step:2086/2160 train_time:126106ms step_avg:60.45ms
step:2087/2160 train_time:126195ms step_avg:60.47ms
step:2088/2160 train_time:126283ms step_avg:60.48ms
step:2089/2160 train_time:126372ms step_avg:60.49ms
step:2090/2160 train_time:126460ms step_avg:60.51ms
step:2091/2160 train_time:126551ms step_avg:60.52ms
step:2092/2160 train_time:126638ms step_avg:60.53ms
step:2093/2160 train_time:126727ms step_avg:60.55ms
step:2094/2160 train_time:126815ms step_avg:60.56ms
step:2095/2160 train_time:126904ms step_avg:60.57ms
step:2096/2160 train_time:126992ms step_avg:60.59ms
step:2097/2160 train_time:127080ms step_avg:60.60ms
step:2098/2160 train_time:127168ms step_avg:60.61ms
step:2099/2160 train_time:127256ms step_avg:60.63ms
step:2100/2160 train_time:127344ms step_avg:60.64ms
step:2101/2160 train_time:127434ms step_avg:60.65ms
step:2102/2160 train_time:127522ms step_avg:60.67ms
step:2103/2160 train_time:127611ms step_avg:60.68ms
step:2104/2160 train_time:127698ms step_avg:60.69ms
step:2105/2160 train_time:127787ms step_avg:60.71ms
step:2106/2160 train_time:127873ms step_avg:60.72ms
step:2107/2160 train_time:127963ms step_avg:60.73ms
step:2108/2160 train_time:128050ms step_avg:60.74ms
step:2109/2160 train_time:128140ms step_avg:60.76ms
step:2110/2160 train_time:128227ms step_avg:60.77ms
step:2111/2160 train_time:128316ms step_avg:60.78ms
step:2112/2160 train_time:128404ms step_avg:60.80ms
step:2113/2160 train_time:128494ms step_avg:60.81ms
step:2114/2160 train_time:128581ms step_avg:60.82ms
step:2115/2160 train_time:128671ms step_avg:60.84ms
step:2116/2160 train_time:128759ms step_avg:60.85ms
step:2117/2160 train_time:128847ms step_avg:60.86ms
step:2118/2160 train_time:128935ms step_avg:60.88ms
step:2119/2160 train_time:129024ms step_avg:60.89ms
step:2120/2160 train_time:129111ms step_avg:60.90ms
step:2121/2160 train_time:129200ms step_avg:60.91ms
step:2122/2160 train_time:129288ms step_avg:60.93ms
step:2123/2160 train_time:129378ms step_avg:60.94ms
step:2124/2160 train_time:129465ms step_avg:60.95ms
step:2125/2160 train_time:129555ms step_avg:60.97ms
step:2126/2160 train_time:129643ms step_avg:60.98ms
step:2127/2160 train_time:129733ms step_avg:60.99ms
step:2128/2160 train_time:129820ms step_avg:61.01ms
step:2129/2160 train_time:129910ms step_avg:61.02ms
step:2130/2160 train_time:129998ms step_avg:61.03ms
step:2131/2160 train_time:130088ms step_avg:61.05ms
step:2132/2160 train_time:130176ms step_avg:61.06ms
step:2133/2160 train_time:130265ms step_avg:61.07ms
step:2134/2160 train_time:130352ms step_avg:61.08ms
step:2135/2160 train_time:130443ms step_avg:61.10ms
step:2136/2160 train_time:130531ms step_avg:61.11ms
step:2137/2160 train_time:130621ms step_avg:61.12ms
step:2138/2160 train_time:130709ms step_avg:61.14ms
step:2139/2160 train_time:130799ms step_avg:61.15ms
step:2140/2160 train_time:130887ms step_avg:61.16ms
step:2141/2160 train_time:130977ms step_avg:61.18ms
step:2142/2160 train_time:131065ms step_avg:61.19ms
step:2143/2160 train_time:131154ms step_avg:61.20ms
step:2144/2160 train_time:131242ms step_avg:61.21ms
step:2145/2160 train_time:131333ms step_avg:61.23ms
step:2146/2160 train_time:131420ms step_avg:61.24ms
step:2147/2160 train_time:131510ms step_avg:61.25ms
step:2148/2160 train_time:131599ms step_avg:61.27ms
step:2149/2160 train_time:131689ms step_avg:61.28ms
step:2150/2160 train_time:131777ms step_avg:61.29ms
step:2151/2160 train_time:131866ms step_avg:61.30ms
step:2152/2160 train_time:131954ms step_avg:61.32ms
step:2153/2160 train_time:132044ms step_avg:61.33ms
step:2154/2160 train_time:132131ms step_avg:61.34ms
step:2155/2160 train_time:132220ms step_avg:61.36ms
step:2156/2160 train_time:132308ms step_avg:61.37ms
step:2157/2160 train_time:132397ms step_avg:61.38ms
step:2158/2160 train_time:132485ms step_avg:61.39ms
step:2159/2160 train_time:132575ms step_avg:61.41ms
step:2160/2160 train_time:132662ms step_avg:61.42ms
step:2160/2160 val_loss:3.2772 train_time:132754ms step_avg:61.46ms
peak memory allocated: 29896 MiB reserved: 61784 MiB

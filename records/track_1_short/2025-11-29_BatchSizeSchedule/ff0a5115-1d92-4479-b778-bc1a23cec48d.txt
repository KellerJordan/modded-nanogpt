import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec  5 21:41:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   28C    P0            115W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   33C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   33C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   29C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   34C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   35C    P0            122W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          192486      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    0   N/A  N/A          192487      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          192488      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          192489      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          192490      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          192491      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          192492      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          192493      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    1   N/A  N/A          192487      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    2   N/A  N/A          192488      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    3   N/A  N/A          192489      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    4   N/A  N/A          192490      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    5   N/A  N/A          192491      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    6   N/A  N/A          192492      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    7   N/A  N/A          192493      C   /home/ubuntu/.venv/bin/python3         1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:110ms step_avg:110.20ms
step:2/2160 train_time:155ms step_avg:77.31ms
step:3/2160 train_time:177ms step_avg:58.91ms
step:4/2160 train_time:200ms step_avg:49.90ms
step:5/2160 train_time:221ms step_avg:44.27ms
step:6/2160 train_time:387ms step_avg:64.55ms
step:7/2160 train_time:439ms step_avg:62.70ms
step:8/2160 train_time:472ms step_avg:59.03ms
step:9/2160 train_time:506ms step_avg:56.17ms
step:10/2160 train_time:539ms step_avg:53.86ms
step:11/2160 train_time:572ms step_avg:52.02ms
step:12/2160 train_time:605ms step_avg:50.44ms
step:13/2160 train_time:639ms step_avg:49.18ms
step:14/2160 train_time:673ms step_avg:48.05ms
step:15/2160 train_time:707ms step_avg:47.10ms
step:16/2160 train_time:740ms step_avg:46.25ms
step:17/2160 train_time:774ms step_avg:45.51ms
step:18/2160 train_time:807ms step_avg:44.83ms
step:19/2160 train_time:841ms step_avg:44.25ms
step:20/2160 train_time:874ms step_avg:43.70ms
step:21/2160 train_time:908ms step_avg:43.23ms
step:22/2160 train_time:941ms step_avg:42.78ms
step:23/2160 train_time:975ms step_avg:42.38ms
step:24/2160 train_time:1008ms step_avg:42.01ms
step:25/2160 train_time:1042ms step_avg:41.67ms
step:26/2160 train_time:1075ms step_avg:41.34ms
step:27/2160 train_time:1109ms step_avg:41.07ms
step:28/2160 train_time:1142ms step_avg:40.79ms
step:29/2160 train_time:1176ms step_avg:40.55ms
step:30/2160 train_time:1209ms step_avg:40.30ms
step:31/2160 train_time:1243ms step_avg:40.10ms
step:32/2160 train_time:1276ms step_avg:39.88ms
step:33/2160 train_time:1310ms step_avg:39.70ms
step:34/2160 train_time:1344ms step_avg:39.52ms
step:35/2160 train_time:1378ms step_avg:39.37ms
step:36/2160 train_time:1411ms step_avg:39.20ms
step:37/2160 train_time:1446ms step_avg:39.08ms
step:38/2160 train_time:1479ms step_avg:38.93ms
step:39/2160 train_time:1513ms step_avg:38.80ms
step:40/2160 train_time:1547ms step_avg:38.67ms
step:41/2160 train_time:1581ms step_avg:38.56ms
step:42/2160 train_time:1614ms step_avg:38.43ms
step:43/2160 train_time:1648ms step_avg:38.32ms
step:44/2160 train_time:1681ms step_avg:38.20ms
step:45/2160 train_time:1715ms step_avg:38.11ms
step:46/2160 train_time:1749ms step_avg:38.01ms
step:47/2160 train_time:1782ms step_avg:37.93ms
step:48/2160 train_time:1816ms step_avg:37.83ms
step:49/2160 train_time:1850ms step_avg:37.75ms
step:50/2160 train_time:1883ms step_avg:37.66ms
step:51/2160 train_time:1917ms step_avg:37.59ms
step:52/2160 train_time:1950ms step_avg:37.50ms
step:53/2160 train_time:1985ms step_avg:37.44ms
step:54/2160 train_time:2018ms step_avg:37.37ms
step:55/2160 train_time:2051ms step_avg:37.30ms
step:56/2160 train_time:2085ms step_avg:37.23ms
step:57/2160 train_time:2118ms step_avg:37.16ms
step:58/2160 train_time:2152ms step_avg:37.10ms
step:59/2160 train_time:2186ms step_avg:37.04ms
step:60/2160 train_time:2219ms step_avg:36.98ms
step:61/2160 train_time:2253ms step_avg:36.93ms
step:62/2160 train_time:2286ms step_avg:36.86ms
step:63/2160 train_time:2320ms step_avg:36.83ms
step:64/2160 train_time:2353ms step_avg:36.77ms
step:65/2160 train_time:2387ms step_avg:36.73ms
step:66/2160 train_time:2421ms step_avg:36.68ms
step:67/2160 train_time:2454ms step_avg:36.63ms
step:68/2160 train_time:2488ms step_avg:36.58ms
step:69/2160 train_time:2522ms step_avg:36.55ms
step:70/2160 train_time:2555ms step_avg:36.50ms
step:71/2160 train_time:2589ms step_avg:36.46ms
step:72/2160 train_time:2622ms step_avg:36.42ms
step:73/2160 train_time:2656ms step_avg:36.39ms
step:74/2160 train_time:2689ms step_avg:36.34ms
step:75/2160 train_time:2724ms step_avg:36.32ms
step:76/2160 train_time:2757ms step_avg:36.28ms
step:77/2160 train_time:2791ms step_avg:36.24ms
step:78/2160 train_time:2824ms step_avg:36.21ms
step:79/2160 train_time:2858ms step_avg:36.18ms
step:80/2160 train_time:2891ms step_avg:36.14ms
step:81/2160 train_time:2926ms step_avg:36.12ms
step:82/2160 train_time:2959ms step_avg:36.08ms
step:83/2160 train_time:2993ms step_avg:36.06ms
step:84/2160 train_time:3026ms step_avg:36.03ms
step:85/2160 train_time:3060ms step_avg:36.00ms
step:86/2160 train_time:3093ms step_avg:35.97ms
step:87/2160 train_time:3128ms step_avg:35.95ms
step:88/2160 train_time:3161ms step_avg:35.91ms
step:89/2160 train_time:3194ms step_avg:35.89ms
step:90/2160 train_time:3228ms step_avg:35.86ms
step:91/2160 train_time:3262ms step_avg:35.84ms
step:92/2160 train_time:3295ms step_avg:35.81ms
step:93/2160 train_time:3329ms step_avg:35.79ms
step:94/2160 train_time:3362ms step_avg:35.76ms
step:95/2160 train_time:3396ms step_avg:35.74ms
step:96/2160 train_time:3429ms step_avg:35.72ms
step:97/2160 train_time:3463ms step_avg:35.70ms
step:98/2160 train_time:3496ms step_avg:35.68ms
step:99/2160 train_time:3530ms step_avg:35.66ms
step:100/2160 train_time:3564ms step_avg:35.64ms
step:101/2160 train_time:3598ms step_avg:35.62ms
step:102/2160 train_time:3631ms step_avg:35.60ms
step:103/2160 train_time:3665ms step_avg:35.58ms
step:104/2160 train_time:3698ms step_avg:35.56ms
step:105/2160 train_time:3732ms step_avg:35.54ms
step:106/2160 train_time:3766ms step_avg:35.52ms
step:107/2160 train_time:3800ms step_avg:35.51ms
step:108/2160 train_time:3833ms step_avg:35.49ms
step:109/2160 train_time:3867ms step_avg:35.47ms
step:110/2160 train_time:3900ms step_avg:35.45ms
step:111/2160 train_time:3934ms step_avg:35.44ms
step:112/2160 train_time:3967ms step_avg:35.42ms
step:113/2160 train_time:4001ms step_avg:35.41ms
step:114/2160 train_time:4034ms step_avg:35.39ms
step:115/2160 train_time:4068ms step_avg:35.37ms
step:116/2160 train_time:4101ms step_avg:35.35ms
step:117/2160 train_time:4135ms step_avg:35.34ms
step:118/2160 train_time:4169ms step_avg:35.33ms
step:119/2160 train_time:4202ms step_avg:35.31ms
step:120/2160 train_time:4236ms step_avg:35.30ms
step:121/2160 train_time:4270ms step_avg:35.29ms
step:122/2160 train_time:4303ms step_avg:35.27ms
step:123/2160 train_time:4336ms step_avg:35.26ms
step:124/2160 train_time:4370ms step_avg:35.24ms
step:125/2160 train_time:4404ms step_avg:35.23ms
step:126/2160 train_time:4437ms step_avg:35.21ms
step:127/2160 train_time:4471ms step_avg:35.20ms
step:128/2160 train_time:4504ms step_avg:35.19ms
step:129/2160 train_time:4538ms step_avg:35.18ms
step:130/2160 train_time:4571ms step_avg:35.16ms
step:131/2160 train_time:4605ms step_avg:35.15ms
step:132/2160 train_time:4638ms step_avg:35.13ms
step:133/2160 train_time:4672ms step_avg:35.12ms
step:134/2160 train_time:4705ms step_avg:35.11ms
step:135/2160 train_time:4739ms step_avg:35.10ms
step:136/2160 train_time:4772ms step_avg:35.09ms
step:137/2160 train_time:4806ms step_avg:35.08ms
step:138/2160 train_time:4839ms step_avg:35.06ms
step:139/2160 train_time:4872ms step_avg:35.05ms
step:140/2160 train_time:4906ms step_avg:35.04ms
step:141/2160 train_time:4940ms step_avg:35.03ms
step:142/2160 train_time:4973ms step_avg:35.02ms
step:143/2160 train_time:5007ms step_avg:35.01ms
step:144/2160 train_time:5040ms step_avg:35.00ms
step:145/2160 train_time:5074ms step_avg:34.99ms
step:146/2160 train_time:5107ms step_avg:34.98ms
step:147/2160 train_time:5141ms step_avg:34.97ms
step:148/2160 train_time:5174ms step_avg:34.96ms
step:149/2160 train_time:5208ms step_avg:34.95ms
step:150/2160 train_time:5241ms step_avg:34.94ms
step:151/2160 train_time:5274ms step_avg:34.93ms
step:152/2160 train_time:5308ms step_avg:34.92ms
step:153/2160 train_time:5341ms step_avg:34.91ms
step:154/2160 train_time:5374ms step_avg:34.90ms
step:155/2160 train_time:5408ms step_avg:34.89ms
step:156/2160 train_time:5442ms step_avg:34.88ms
step:157/2160 train_time:5475ms step_avg:34.88ms
step:158/2160 train_time:5509ms step_avg:34.86ms
step:159/2160 train_time:5543ms step_avg:34.86ms
step:160/2160 train_time:5576ms step_avg:34.85ms
step:161/2160 train_time:5610ms step_avg:34.84ms
step:162/2160 train_time:5643ms step_avg:34.83ms
step:163/2160 train_time:5677ms step_avg:34.83ms
step:164/2160 train_time:5710ms step_avg:34.82ms
step:165/2160 train_time:5744ms step_avg:34.81ms
step:166/2160 train_time:5777ms step_avg:34.80ms
step:167/2160 train_time:5811ms step_avg:34.79ms
step:168/2160 train_time:5844ms step_avg:34.78ms
step:169/2160 train_time:5878ms step_avg:34.78ms
step:170/2160 train_time:5911ms step_avg:34.77ms
step:171/2160 train_time:5945ms step_avg:34.76ms
step:172/2160 train_time:5978ms step_avg:34.76ms
step:173/2160 train_time:6012ms step_avg:34.75ms
step:174/2160 train_time:6045ms step_avg:34.74ms
step:175/2160 train_time:6079ms step_avg:34.74ms
step:176/2160 train_time:6112ms step_avg:34.73ms
step:177/2160 train_time:6146ms step_avg:34.72ms
step:178/2160 train_time:6179ms step_avg:34.71ms
step:179/2160 train_time:6213ms step_avg:34.71ms
step:180/2160 train_time:6247ms step_avg:34.70ms
step:181/2160 train_time:6280ms step_avg:34.70ms
step:182/2160 train_time:6313ms step_avg:34.69ms
step:183/2160 train_time:6347ms step_avg:34.68ms
step:184/2160 train_time:6380ms step_avg:34.68ms
step:185/2160 train_time:6414ms step_avg:34.67ms
step:186/2160 train_time:6447ms step_avg:34.66ms
step:187/2160 train_time:6481ms step_avg:34.66ms
step:188/2160 train_time:6514ms step_avg:34.65ms
step:189/2160 train_time:6548ms step_avg:34.65ms
step:190/2160 train_time:6582ms step_avg:34.64ms
step:191/2160 train_time:6615ms step_avg:34.63ms
step:192/2160 train_time:6648ms step_avg:34.63ms
step:193/2160 train_time:6682ms step_avg:34.62ms
step:194/2160 train_time:6715ms step_avg:34.61ms
step:195/2160 train_time:6749ms step_avg:34.61ms
step:196/2160 train_time:6782ms step_avg:34.60ms
step:197/2160 train_time:6816ms step_avg:34.60ms
step:198/2160 train_time:6849ms step_avg:34.59ms
step:199/2160 train_time:6883ms step_avg:34.59ms
step:200/2160 train_time:6916ms step_avg:34.58ms
step:201/2160 train_time:6950ms step_avg:34.58ms
step:202/2160 train_time:6983ms step_avg:34.57ms
step:203/2160 train_time:7017ms step_avg:34.57ms
step:204/2160 train_time:7050ms step_avg:34.56ms
step:205/2160 train_time:7084ms step_avg:34.56ms
step:206/2160 train_time:7117ms step_avg:34.55ms
step:207/2160 train_time:7151ms step_avg:34.55ms
step:208/2160 train_time:7184ms step_avg:34.54ms
step:209/2160 train_time:7218ms step_avg:34.54ms
step:210/2160 train_time:7251ms step_avg:34.53ms
step:211/2160 train_time:7285ms step_avg:34.53ms
step:212/2160 train_time:7318ms step_avg:34.52ms
step:213/2160 train_time:7352ms step_avg:34.52ms
step:214/2160 train_time:7385ms step_avg:34.51ms
step:215/2160 train_time:7420ms step_avg:34.51ms
step:216/2160 train_time:7453ms step_avg:34.50ms
step:217/2160 train_time:7486ms step_avg:34.50ms
step:218/2160 train_time:7520ms step_avg:34.49ms
step:219/2160 train_time:7553ms step_avg:34.49ms
step:220/2160 train_time:7587ms step_avg:34.49ms
step:221/2160 train_time:7620ms step_avg:34.48ms
step:222/2160 train_time:7653ms step_avg:34.48ms
step:223/2160 train_time:7687ms step_avg:34.47ms
step:224/2160 train_time:7720ms step_avg:34.47ms
step:225/2160 train_time:7754ms step_avg:34.46ms
step:226/2160 train_time:7788ms step_avg:34.46ms
step:227/2160 train_time:7821ms step_avg:34.46ms
step:228/2160 train_time:7855ms step_avg:34.45ms
step:229/2160 train_time:7889ms step_avg:34.45ms
step:230/2160 train_time:7922ms step_avg:34.44ms
step:231/2160 train_time:7956ms step_avg:34.44ms
step:232/2160 train_time:7989ms step_avg:34.43ms
step:233/2160 train_time:8023ms step_avg:34.43ms
step:234/2160 train_time:8056ms step_avg:34.43ms
step:235/2160 train_time:8090ms step_avg:34.43ms
step:236/2160 train_time:8124ms step_avg:34.42ms
step:237/2160 train_time:8158ms step_avg:34.42ms
step:238/2160 train_time:8191ms step_avg:34.41ms
step:239/2160 train_time:8225ms step_avg:34.41ms
step:240/2160 train_time:8258ms step_avg:34.41ms
step:241/2160 train_time:8292ms step_avg:34.41ms
step:242/2160 train_time:8325ms step_avg:34.40ms
step:243/2160 train_time:8359ms step_avg:34.40ms
step:244/2160 train_time:8392ms step_avg:34.39ms
step:245/2160 train_time:8426ms step_avg:34.39ms
step:246/2160 train_time:8459ms step_avg:34.38ms
step:247/2160 train_time:8493ms step_avg:34.38ms
step:248/2160 train_time:8526ms step_avg:34.38ms
step:249/2160 train_time:8560ms step_avg:34.38ms
step:250/2160 train_time:8593ms step_avg:34.37ms
step:250/2160 val_loss:4.2996 train_time:8628ms step_avg:34.51ms
step:251/2160 train_time:8651ms step_avg:34.47ms
step:252/2160 train_time:8673ms step_avg:34.42ms
step:253/2160 train_time:8698ms step_avg:34.38ms
step:254/2160 train_time:8731ms step_avg:34.37ms
step:255/2160 train_time:8768ms step_avg:34.38ms
step:256/2160 train_time:8803ms step_avg:34.39ms
step:257/2160 train_time:8839ms step_avg:34.39ms
step:258/2160 train_time:8872ms step_avg:34.39ms
step:259/2160 train_time:8907ms step_avg:34.39ms
step:260/2160 train_time:8940ms step_avg:34.38ms
step:261/2160 train_time:8975ms step_avg:34.39ms
step:262/2160 train_time:9008ms step_avg:34.38ms
step:263/2160 train_time:9042ms step_avg:34.38ms
step:264/2160 train_time:9075ms step_avg:34.37ms
step:265/2160 train_time:9109ms step_avg:34.37ms
step:266/2160 train_time:9142ms step_avg:34.37ms
step:267/2160 train_time:9176ms step_avg:34.37ms
step:268/2160 train_time:9209ms step_avg:34.36ms
step:269/2160 train_time:9242ms step_avg:34.36ms
step:270/2160 train_time:9275ms step_avg:34.35ms
step:271/2160 train_time:9309ms step_avg:34.35ms
step:272/2160 train_time:9342ms step_avg:34.34ms
step:273/2160 train_time:9375ms step_avg:34.34ms
step:274/2160 train_time:9408ms step_avg:34.34ms
step:275/2160 train_time:9442ms step_avg:34.33ms
step:276/2160 train_time:9475ms step_avg:34.33ms
step:277/2160 train_time:9509ms step_avg:34.33ms
step:278/2160 train_time:9542ms step_avg:34.32ms
step:279/2160 train_time:9575ms step_avg:34.32ms
step:280/2160 train_time:9608ms step_avg:34.32ms
step:281/2160 train_time:9642ms step_avg:34.31ms
step:282/2160 train_time:9675ms step_avg:34.31ms
step:283/2160 train_time:9709ms step_avg:34.31ms
step:284/2160 train_time:9742ms step_avg:34.30ms
step:285/2160 train_time:9777ms step_avg:34.30ms
step:286/2160 train_time:9810ms step_avg:34.30ms
step:287/2160 train_time:9844ms step_avg:34.30ms
step:288/2160 train_time:9877ms step_avg:34.30ms
step:289/2160 train_time:9911ms step_avg:34.29ms
step:290/2160 train_time:9944ms step_avg:34.29ms
step:291/2160 train_time:9978ms step_avg:34.29ms
step:292/2160 train_time:10011ms step_avg:34.28ms
step:293/2160 train_time:10045ms step_avg:34.28ms
step:294/2160 train_time:10078ms step_avg:34.28ms
step:295/2160 train_time:10112ms step_avg:34.28ms
step:296/2160 train_time:10145ms step_avg:34.27ms
step:297/2160 train_time:10180ms step_avg:34.28ms
step:298/2160 train_time:10213ms step_avg:34.27ms
step:299/2160 train_time:10247ms step_avg:34.27ms
step:300/2160 train_time:10280ms step_avg:34.27ms
step:301/2160 train_time:10313ms step_avg:34.26ms
step:302/2160 train_time:10347ms step_avg:34.26ms
step:303/2160 train_time:10381ms step_avg:34.26ms
step:304/2160 train_time:10414ms step_avg:34.26ms
step:305/2160 train_time:10448ms step_avg:34.25ms
step:306/2160 train_time:10481ms step_avg:34.25ms
step:307/2160 train_time:10514ms step_avg:34.25ms
step:308/2160 train_time:10548ms step_avg:34.25ms
step:309/2160 train_time:10582ms step_avg:34.24ms
step:310/2160 train_time:10615ms step_avg:34.24ms
step:311/2160 train_time:10648ms step_avg:34.24ms
step:312/2160 train_time:10681ms step_avg:34.24ms
step:313/2160 train_time:10715ms step_avg:34.23ms
step:314/2160 train_time:10748ms step_avg:34.23ms
step:315/2160 train_time:10783ms step_avg:34.23ms
step:316/2160 train_time:10816ms step_avg:34.23ms
step:317/2160 train_time:10850ms step_avg:34.23ms
step:318/2160 train_time:10883ms step_avg:34.22ms
step:319/2160 train_time:10916ms step_avg:34.22ms
step:320/2160 train_time:10950ms step_avg:34.22ms
step:321/2160 train_time:10984ms step_avg:34.22ms
step:322/2160 train_time:11017ms step_avg:34.21ms
step:323/2160 train_time:11050ms step_avg:34.21ms
step:324/2160 train_time:11083ms step_avg:34.21ms
step:325/2160 train_time:11117ms step_avg:34.21ms
step:326/2160 train_time:11151ms step_avg:34.20ms
step:327/2160 train_time:11184ms step_avg:34.20ms
step:328/2160 train_time:11217ms step_avg:34.20ms
step:329/2160 train_time:11251ms step_avg:34.20ms
step:330/2160 train_time:11284ms step_avg:34.19ms
step:331/2160 train_time:11318ms step_avg:34.19ms
step:332/2160 train_time:11351ms step_avg:34.19ms
step:333/2160 train_time:11385ms step_avg:34.19ms
step:334/2160 train_time:11418ms step_avg:34.18ms
step:335/2160 train_time:11452ms step_avg:34.18ms
step:336/2160 train_time:11484ms step_avg:34.18ms
step:337/2160 train_time:11519ms step_avg:34.18ms
step:338/2160 train_time:11552ms step_avg:34.18ms
step:339/2160 train_time:11585ms step_avg:34.17ms
step:340/2160 train_time:11618ms step_avg:34.17ms
step:341/2160 train_time:11652ms step_avg:34.17ms
step:342/2160 train_time:11685ms step_avg:34.17ms
step:343/2160 train_time:11719ms step_avg:34.17ms
step:344/2160 train_time:11752ms step_avg:34.16ms
step:345/2160 train_time:11786ms step_avg:34.16ms
step:346/2160 train_time:11819ms step_avg:34.16ms
step:347/2160 train_time:11853ms step_avg:34.16ms
step:348/2160 train_time:11886ms step_avg:34.16ms
step:349/2160 train_time:11920ms step_avg:34.15ms
step:350/2160 train_time:11953ms step_avg:34.15ms
step:351/2160 train_time:11987ms step_avg:34.15ms
step:352/2160 train_time:12020ms step_avg:34.15ms
step:353/2160 train_time:12054ms step_avg:34.15ms
step:354/2160 train_time:12087ms step_avg:34.14ms
step:355/2160 train_time:12121ms step_avg:34.14ms
step:356/2160 train_time:12154ms step_avg:34.14ms
step:357/2160 train_time:12188ms step_avg:34.14ms
step:358/2160 train_time:12221ms step_avg:34.14ms
step:359/2160 train_time:12255ms step_avg:34.14ms
step:360/2160 train_time:12288ms step_avg:34.13ms
step:361/2160 train_time:12322ms step_avg:34.13ms
step:362/2160 train_time:12355ms step_avg:34.13ms
step:363/2160 train_time:12389ms step_avg:34.13ms
step:364/2160 train_time:12422ms step_avg:34.13ms
step:365/2160 train_time:12456ms step_avg:34.13ms
step:366/2160 train_time:12489ms step_avg:34.12ms
step:367/2160 train_time:12523ms step_avg:34.12ms
step:368/2160 train_time:12556ms step_avg:34.12ms
step:369/2160 train_time:12590ms step_avg:34.12ms
step:370/2160 train_time:12623ms step_avg:34.12ms
step:371/2160 train_time:12657ms step_avg:34.12ms
step:372/2160 train_time:12690ms step_avg:34.11ms
step:373/2160 train_time:12724ms step_avg:34.11ms
step:374/2160 train_time:12757ms step_avg:34.11ms
step:375/2160 train_time:12790ms step_avg:34.11ms
step:376/2160 train_time:12823ms step_avg:34.10ms
step:377/2160 train_time:12857ms step_avg:34.10ms
step:378/2160 train_time:12890ms step_avg:34.10ms
step:379/2160 train_time:12924ms step_avg:34.10ms
step:380/2160 train_time:12957ms step_avg:34.10ms
step:381/2160 train_time:12991ms step_avg:34.10ms
step:382/2160 train_time:13024ms step_avg:34.09ms
step:383/2160 train_time:13058ms step_avg:34.09ms
step:384/2160 train_time:13091ms step_avg:34.09ms
step:385/2160 train_time:13125ms step_avg:34.09ms
step:386/2160 train_time:13158ms step_avg:34.09ms
step:387/2160 train_time:13192ms step_avg:34.09ms
step:388/2160 train_time:13225ms step_avg:34.08ms
step:389/2160 train_time:13259ms step_avg:34.09ms
step:390/2160 train_time:13292ms step_avg:34.08ms
step:391/2160 train_time:13326ms step_avg:34.08ms
step:392/2160 train_time:13359ms step_avg:34.08ms
step:393/2160 train_time:13393ms step_avg:34.08ms
step:394/2160 train_time:13426ms step_avg:34.08ms
step:395/2160 train_time:13460ms step_avg:34.08ms
step:396/2160 train_time:13493ms step_avg:34.07ms
step:397/2160 train_time:13527ms step_avg:34.07ms
step:398/2160 train_time:13560ms step_avg:34.07ms
step:399/2160 train_time:13594ms step_avg:34.07ms
step:400/2160 train_time:13627ms step_avg:34.07ms
step:401/2160 train_time:13662ms step_avg:34.07ms
step:402/2160 train_time:13695ms step_avg:34.07ms
step:403/2160 train_time:13728ms step_avg:34.07ms
step:404/2160 train_time:13761ms step_avg:34.06ms
step:405/2160 train_time:13795ms step_avg:34.06ms
step:406/2160 train_time:13828ms step_avg:34.06ms
step:407/2160 train_time:13862ms step_avg:34.06ms
step:408/2160 train_time:13895ms step_avg:34.06ms
step:409/2160 train_time:13929ms step_avg:34.06ms
step:410/2160 train_time:13962ms step_avg:34.05ms
step:411/2160 train_time:13996ms step_avg:34.05ms
step:412/2160 train_time:14029ms step_avg:34.05ms
step:413/2160 train_time:14063ms step_avg:34.05ms
step:414/2160 train_time:14096ms step_avg:34.05ms
step:415/2160 train_time:14130ms step_avg:34.05ms
step:416/2160 train_time:14163ms step_avg:34.05ms
step:417/2160 train_time:14197ms step_avg:34.05ms
step:418/2160 train_time:14230ms step_avg:34.04ms
step:419/2160 train_time:14264ms step_avg:34.04ms
step:420/2160 train_time:14297ms step_avg:34.04ms
step:421/2160 train_time:14331ms step_avg:34.04ms
step:422/2160 train_time:14364ms step_avg:34.04ms
step:423/2160 train_time:14398ms step_avg:34.04ms
step:424/2160 train_time:14431ms step_avg:34.04ms
step:425/2160 train_time:14465ms step_avg:34.03ms
step:426/2160 train_time:14498ms step_avg:34.03ms
step:427/2160 train_time:14531ms step_avg:34.03ms
step:428/2160 train_time:14564ms step_avg:34.03ms
step:429/2160 train_time:14599ms step_avg:34.03ms
step:430/2160 train_time:14632ms step_avg:34.03ms
step:431/2160 train_time:14666ms step_avg:34.03ms
step:432/2160 train_time:14699ms step_avg:34.02ms
step:433/2160 train_time:14733ms step_avg:34.02ms
step:434/2160 train_time:14766ms step_avg:34.02ms
step:435/2160 train_time:14800ms step_avg:34.02ms
step:436/2160 train_time:14833ms step_avg:34.02ms
step:437/2160 train_time:14867ms step_avg:34.02ms
step:438/2160 train_time:14900ms step_avg:34.02ms
step:439/2160 train_time:14934ms step_avg:34.02ms
step:440/2160 train_time:14967ms step_avg:34.02ms
step:441/2160 train_time:15001ms step_avg:34.02ms
step:442/2160 train_time:15035ms step_avg:34.01ms
step:443/2160 train_time:15068ms step_avg:34.01ms
step:444/2160 train_time:15101ms step_avg:34.01ms
step:445/2160 train_time:15135ms step_avg:34.01ms
step:446/2160 train_time:15168ms step_avg:34.01ms
step:447/2160 train_time:15202ms step_avg:34.01ms
step:448/2160 train_time:15236ms step_avg:34.01ms
step:449/2160 train_time:15269ms step_avg:34.01ms
step:450/2160 train_time:15302ms step_avg:34.01ms
step:451/2160 train_time:15336ms step_avg:34.00ms
step:452/2160 train_time:15369ms step_avg:34.00ms
step:453/2160 train_time:15403ms step_avg:34.00ms
step:454/2160 train_time:15436ms step_avg:34.00ms
step:455/2160 train_time:15470ms step_avg:34.00ms
step:456/2160 train_time:15503ms step_avg:34.00ms
step:457/2160 train_time:15537ms step_avg:34.00ms
step:458/2160 train_time:15570ms step_avg:34.00ms
step:459/2160 train_time:15605ms step_avg:34.00ms
step:460/2160 train_time:15638ms step_avg:33.99ms
step:461/2160 train_time:15671ms step_avg:33.99ms
step:462/2160 train_time:15704ms step_avg:33.99ms
step:463/2160 train_time:15739ms step_avg:33.99ms
step:464/2160 train_time:15772ms step_avg:33.99ms
step:465/2160 train_time:15806ms step_avg:33.99ms
step:466/2160 train_time:15839ms step_avg:33.99ms
step:467/2160 train_time:15872ms step_avg:33.99ms
step:468/2160 train_time:15906ms step_avg:33.99ms
step:469/2160 train_time:15940ms step_avg:33.99ms
step:470/2160 train_time:15973ms step_avg:33.98ms
step:471/2160 train_time:16006ms step_avg:33.98ms
step:472/2160 train_time:16039ms step_avg:33.98ms
step:473/2160 train_time:16073ms step_avg:33.98ms
step:474/2160 train_time:16106ms step_avg:33.98ms
step:475/2160 train_time:16140ms step_avg:33.98ms
step:476/2160 train_time:16173ms step_avg:33.98ms
step:477/2160 train_time:16207ms step_avg:33.98ms
step:478/2160 train_time:16240ms step_avg:33.98ms
step:479/2160 train_time:16274ms step_avg:33.98ms
step:480/2160 train_time:16307ms step_avg:33.97ms
step:481/2160 train_time:16341ms step_avg:33.97ms
step:482/2160 train_time:16374ms step_avg:33.97ms
step:483/2160 train_time:16408ms step_avg:33.97ms
step:484/2160 train_time:16441ms step_avg:33.97ms
step:485/2160 train_time:16475ms step_avg:33.97ms
step:486/2160 train_time:16508ms step_avg:33.97ms
step:487/2160 train_time:16542ms step_avg:33.97ms
step:488/2160 train_time:16575ms step_avg:33.96ms
step:489/2160 train_time:16609ms step_avg:33.97ms
step:490/2160 train_time:16642ms step_avg:33.96ms
step:491/2160 train_time:16676ms step_avg:33.96ms
step:492/2160 train_time:16709ms step_avg:33.96ms
step:493/2160 train_time:16743ms step_avg:33.96ms
step:494/2160 train_time:16776ms step_avg:33.96ms
step:495/2160 train_time:16810ms step_avg:33.96ms
step:496/2160 train_time:16843ms step_avg:33.96ms
step:497/2160 train_time:16877ms step_avg:33.96ms
step:498/2160 train_time:16910ms step_avg:33.96ms
step:499/2160 train_time:16944ms step_avg:33.96ms
step:500/2160 train_time:16977ms step_avg:33.95ms
step:500/2160 val_loss:4.0081 train_time:17011ms step_avg:34.02ms
step:501/2160 train_time:17034ms step_avg:34.00ms
step:502/2160 train_time:17057ms step_avg:33.98ms
step:503/2160 train_time:17081ms step_avg:33.96ms
step:504/2160 train_time:17114ms step_avg:33.96ms
step:505/2160 train_time:17150ms step_avg:33.96ms
step:506/2160 train_time:17184ms step_avg:33.96ms
step:507/2160 train_time:17220ms step_avg:33.96ms
step:508/2160 train_time:17253ms step_avg:33.96ms
step:509/2160 train_time:17287ms step_avg:33.96ms
step:510/2160 train_time:17320ms step_avg:33.96ms
step:511/2160 train_time:17354ms step_avg:33.96ms
step:512/2160 train_time:17387ms step_avg:33.96ms
step:513/2160 train_time:17421ms step_avg:33.96ms
step:514/2160 train_time:17454ms step_avg:33.96ms
step:515/2160 train_time:17488ms step_avg:33.96ms
step:516/2160 train_time:17521ms step_avg:33.95ms
step:517/2160 train_time:17554ms step_avg:33.95ms
step:518/2160 train_time:17587ms step_avg:33.95ms
step:519/2160 train_time:17621ms step_avg:33.95ms
step:520/2160 train_time:17654ms step_avg:33.95ms
step:521/2160 train_time:17687ms step_avg:33.95ms
step:522/2160 train_time:17721ms step_avg:33.95ms
step:523/2160 train_time:17754ms step_avg:33.95ms
step:524/2160 train_time:17787ms step_avg:33.94ms
step:525/2160 train_time:17821ms step_avg:33.94ms
step:526/2160 train_time:17854ms step_avg:33.94ms
step:527/2160 train_time:17888ms step_avg:33.94ms
step:528/2160 train_time:17920ms step_avg:33.94ms
step:529/2160 train_time:17954ms step_avg:33.94ms
step:530/2160 train_time:17987ms step_avg:33.94ms
step:531/2160 train_time:18021ms step_avg:33.94ms
step:532/2160 train_time:18054ms step_avg:33.94ms
step:533/2160 train_time:18088ms step_avg:33.94ms
step:534/2160 train_time:18121ms step_avg:33.93ms
step:535/2160 train_time:18155ms step_avg:33.94ms
step:536/2160 train_time:18189ms step_avg:33.93ms
step:537/2160 train_time:18223ms step_avg:33.93ms
step:538/2160 train_time:18256ms step_avg:33.93ms
step:539/2160 train_time:18290ms step_avg:33.93ms
step:540/2160 train_time:18323ms step_avg:33.93ms
step:541/2160 train_time:18357ms step_avg:33.93ms
step:542/2160 train_time:18390ms step_avg:33.93ms
step:543/2160 train_time:18425ms step_avg:33.93ms
step:544/2160 train_time:18458ms step_avg:33.93ms
step:545/2160 train_time:18491ms step_avg:33.93ms
step:546/2160 train_time:18524ms step_avg:33.93ms
step:547/2160 train_time:18558ms step_avg:33.93ms
step:548/2160 train_time:18591ms step_avg:33.93ms
step:549/2160 train_time:18625ms step_avg:33.93ms
step:550/2160 train_time:18658ms step_avg:33.92ms
step:551/2160 train_time:18692ms step_avg:33.92ms
step:552/2160 train_time:18725ms step_avg:33.92ms
step:553/2160 train_time:18759ms step_avg:33.92ms
step:554/2160 train_time:18792ms step_avg:33.92ms
step:555/2160 train_time:18826ms step_avg:33.92ms
step:556/2160 train_time:18859ms step_avg:33.92ms
step:557/2160 train_time:18893ms step_avg:33.92ms
step:558/2160 train_time:18926ms step_avg:33.92ms
step:559/2160 train_time:18960ms step_avg:33.92ms
step:560/2160 train_time:18993ms step_avg:33.92ms
step:561/2160 train_time:19027ms step_avg:33.92ms
step:562/2160 train_time:19060ms step_avg:33.91ms
step:563/2160 train_time:19094ms step_avg:33.91ms
step:564/2160 train_time:19127ms step_avg:33.91ms
step:565/2160 train_time:19161ms step_avg:33.91ms
step:566/2160 train_time:19194ms step_avg:33.91ms
step:567/2160 train_time:19228ms step_avg:33.91ms
step:568/2160 train_time:19261ms step_avg:33.91ms
step:569/2160 train_time:19295ms step_avg:33.91ms
step:570/2160 train_time:19329ms step_avg:33.91ms
step:571/2160 train_time:19363ms step_avg:33.91ms
step:572/2160 train_time:19396ms step_avg:33.91ms
step:573/2160 train_time:19429ms step_avg:33.91ms
step:574/2160 train_time:19463ms step_avg:33.91ms
step:575/2160 train_time:19497ms step_avg:33.91ms
step:576/2160 train_time:19530ms step_avg:33.91ms
step:577/2160 train_time:19564ms step_avg:33.91ms
step:578/2160 train_time:19597ms step_avg:33.91ms
step:579/2160 train_time:19631ms step_avg:33.91ms
step:580/2160 train_time:19664ms step_avg:33.90ms
step:581/2160 train_time:19698ms step_avg:33.90ms
step:582/2160 train_time:19732ms step_avg:33.90ms
step:583/2160 train_time:19765ms step_avg:33.90ms
step:584/2160 train_time:19799ms step_avg:33.90ms
step:585/2160 train_time:19832ms step_avg:33.90ms
step:586/2160 train_time:19866ms step_avg:33.90ms
step:587/2160 train_time:19900ms step_avg:33.90ms
step:588/2160 train_time:19933ms step_avg:33.90ms
step:589/2160 train_time:19966ms step_avg:33.90ms
step:590/2160 train_time:19999ms step_avg:33.90ms
step:591/2160 train_time:20033ms step_avg:33.90ms
step:592/2160 train_time:20066ms step_avg:33.90ms
step:593/2160 train_time:20100ms step_avg:33.90ms
step:594/2160 train_time:20133ms step_avg:33.89ms
step:595/2160 train_time:20167ms step_avg:33.89ms
step:596/2160 train_time:20200ms step_avg:33.89ms
step:597/2160 train_time:20234ms step_avg:33.89ms
step:598/2160 train_time:20267ms step_avg:33.89ms
step:599/2160 train_time:20301ms step_avg:33.89ms
step:600/2160 train_time:20334ms step_avg:33.89ms
step:601/2160 train_time:20368ms step_avg:33.89ms
step:602/2160 train_time:20401ms step_avg:33.89ms
step:603/2160 train_time:20435ms step_avg:33.89ms
step:604/2160 train_time:20468ms step_avg:33.89ms
step:605/2160 train_time:20502ms step_avg:33.89ms
step:606/2160 train_time:20535ms step_avg:33.89ms
step:607/2160 train_time:20569ms step_avg:33.89ms
step:608/2160 train_time:20602ms step_avg:33.89ms
step:609/2160 train_time:20636ms step_avg:33.88ms
step:610/2160 train_time:20669ms step_avg:33.88ms
step:611/2160 train_time:20702ms step_avg:33.88ms
step:612/2160 train_time:20736ms step_avg:33.88ms
step:613/2160 train_time:20769ms step_avg:33.88ms
step:614/2160 train_time:20803ms step_avg:33.88ms
step:615/2160 train_time:20836ms step_avg:33.88ms
step:616/2160 train_time:20869ms step_avg:33.88ms
step:617/2160 train_time:20903ms step_avg:33.88ms
step:618/2160 train_time:20937ms step_avg:33.88ms
step:619/2160 train_time:20970ms step_avg:33.88ms
step:620/2160 train_time:21003ms step_avg:33.88ms
step:621/2160 train_time:21037ms step_avg:33.88ms
step:622/2160 train_time:21070ms step_avg:33.87ms
step:623/2160 train_time:21104ms step_avg:33.88ms
step:624/2160 train_time:21137ms step_avg:33.87ms
step:625/2160 train_time:21171ms step_avg:33.87ms
step:626/2160 train_time:21204ms step_avg:33.87ms
step:627/2160 train_time:21238ms step_avg:33.87ms
step:628/2160 train_time:21271ms step_avg:33.87ms
step:629/2160 train_time:21304ms step_avg:33.87ms
step:630/2160 train_time:21337ms step_avg:33.87ms
step:631/2160 train_time:21371ms step_avg:33.87ms
step:632/2160 train_time:21404ms step_avg:33.87ms
step:633/2160 train_time:21438ms step_avg:33.87ms
step:634/2160 train_time:21471ms step_avg:33.87ms
step:635/2160 train_time:21506ms step_avg:33.87ms
step:636/2160 train_time:21538ms step_avg:33.87ms
step:637/2160 train_time:21572ms step_avg:33.87ms
step:638/2160 train_time:21606ms step_avg:33.86ms
step:639/2160 train_time:21639ms step_avg:33.86ms
step:640/2160 train_time:21672ms step_avg:33.86ms
step:641/2160 train_time:21706ms step_avg:33.86ms
step:642/2160 train_time:21740ms step_avg:33.86ms
step:643/2160 train_time:21773ms step_avg:33.86ms
step:644/2160 train_time:21807ms step_avg:33.86ms
step:645/2160 train_time:21840ms step_avg:33.86ms
step:646/2160 train_time:21873ms step_avg:33.86ms
step:647/2160 train_time:21907ms step_avg:33.86ms
step:648/2160 train_time:21940ms step_avg:33.86ms
step:649/2160 train_time:21975ms step_avg:33.86ms
step:650/2160 train_time:22008ms step_avg:33.86ms
step:651/2160 train_time:22041ms step_avg:33.86ms
step:652/2160 train_time:22074ms step_avg:33.86ms
step:653/2160 train_time:22108ms step_avg:33.86ms
step:654/2160 train_time:22141ms step_avg:33.85ms
step:655/2160 train_time:22175ms step_avg:33.85ms
step:656/2160 train_time:22208ms step_avg:33.85ms
step:657/2160 train_time:22242ms step_avg:33.85ms
step:658/2160 train_time:22275ms step_avg:33.85ms
step:659/2160 train_time:22309ms step_avg:33.85ms
step:660/2160 train_time:22342ms step_avg:33.85ms
step:661/2160 train_time:22376ms step_avg:33.85ms
step:662/2160 train_time:22409ms step_avg:33.85ms
step:663/2160 train_time:22443ms step_avg:33.85ms
step:664/2160 train_time:22476ms step_avg:33.85ms
step:665/2160 train_time:22510ms step_avg:33.85ms
step:666/2160 train_time:22543ms step_avg:33.85ms
step:667/2160 train_time:22577ms step_avg:33.85ms
step:668/2160 train_time:22610ms step_avg:33.85ms
step:669/2160 train_time:22644ms step_avg:33.85ms
step:670/2160 train_time:22677ms step_avg:33.85ms
step:671/2160 train_time:22711ms step_avg:33.85ms
step:672/2160 train_time:22744ms step_avg:33.85ms
step:673/2160 train_time:22778ms step_avg:33.85ms
step:674/2160 train_time:22811ms step_avg:33.84ms
step:675/2160 train_time:22845ms step_avg:33.84ms
step:676/2160 train_time:22878ms step_avg:33.84ms
step:677/2160 train_time:22912ms step_avg:33.84ms
step:678/2160 train_time:22945ms step_avg:33.84ms
step:679/2160 train_time:22979ms step_avg:33.84ms
step:680/2160 train_time:23012ms step_avg:33.84ms
step:681/2160 train_time:23046ms step_avg:33.84ms
step:682/2160 train_time:23079ms step_avg:33.84ms
step:683/2160 train_time:23113ms step_avg:33.84ms
step:684/2160 train_time:23147ms step_avg:33.84ms
step:685/2160 train_time:23181ms step_avg:33.84ms
step:686/2160 train_time:23214ms step_avg:33.84ms
step:687/2160 train_time:23247ms step_avg:33.84ms
step:688/2160 train_time:23280ms step_avg:33.84ms
step:689/2160 train_time:23315ms step_avg:33.84ms
step:690/2160 train_time:23348ms step_avg:33.84ms
step:691/2160 train_time:23382ms step_avg:33.84ms
step:692/2160 train_time:23414ms step_avg:33.84ms
step:693/2160 train_time:23449ms step_avg:33.84ms
step:694/2160 train_time:23482ms step_avg:33.84ms
step:695/2160 train_time:23515ms step_avg:33.84ms
step:696/2160 train_time:23549ms step_avg:33.83ms
step:697/2160 train_time:23582ms step_avg:33.83ms
step:698/2160 train_time:23615ms step_avg:33.83ms
step:699/2160 train_time:23649ms step_avg:33.83ms
step:700/2160 train_time:23683ms step_avg:33.83ms
step:701/2160 train_time:23716ms step_avg:33.83ms
step:702/2160 train_time:23749ms step_avg:33.83ms
step:703/2160 train_time:23783ms step_avg:33.83ms
step:704/2160 train_time:23816ms step_avg:33.83ms
step:705/2160 train_time:23850ms step_avg:33.83ms
step:706/2160 train_time:23884ms step_avg:33.83ms
step:707/2160 train_time:23918ms step_avg:33.83ms
step:708/2160 train_time:23952ms step_avg:33.83ms
step:709/2160 train_time:24011ms step_avg:33.87ms
step:710/2160 train_time:24071ms step_avg:33.90ms
step:711/2160 train_time:24132ms step_avg:33.94ms
step:712/2160 train_time:24191ms step_avg:33.98ms
step:713/2160 train_time:24253ms step_avg:34.01ms
step:714/2160 train_time:24312ms step_avg:34.05ms
step:715/2160 train_time:24373ms step_avg:34.09ms
step:716/2160 train_time:24432ms step_avg:34.12ms
step:717/2160 train_time:24494ms step_avg:34.16ms
step:718/2160 train_time:24553ms step_avg:34.20ms
step:719/2160 train_time:24616ms step_avg:34.24ms
step:720/2160 train_time:24676ms step_avg:34.27ms
step:721/2160 train_time:24737ms step_avg:34.31ms
step:722/2160 train_time:24796ms step_avg:34.34ms
step:723/2160 train_time:24857ms step_avg:34.38ms
step:724/2160 train_time:24916ms step_avg:34.41ms
step:725/2160 train_time:24977ms step_avg:34.45ms
step:726/2160 train_time:25036ms step_avg:34.48ms
step:727/2160 train_time:25096ms step_avg:34.52ms
step:728/2160 train_time:25155ms step_avg:34.55ms
step:729/2160 train_time:25216ms step_avg:34.59ms
step:730/2160 train_time:25275ms step_avg:34.62ms
step:731/2160 train_time:25336ms step_avg:34.66ms
step:732/2160 train_time:25395ms step_avg:34.69ms
step:733/2160 train_time:25456ms step_avg:34.73ms
step:734/2160 train_time:25516ms step_avg:34.76ms
step:735/2160 train_time:25577ms step_avg:34.80ms
step:736/2160 train_time:25636ms step_avg:34.83ms
step:737/2160 train_time:25697ms step_avg:34.87ms
step:738/2160 train_time:25757ms step_avg:34.90ms
step:739/2160 train_time:25818ms step_avg:34.94ms
step:740/2160 train_time:25878ms step_avg:34.97ms
step:741/2160 train_time:25939ms step_avg:35.00ms
step:742/2160 train_time:25998ms step_avg:35.04ms
step:743/2160 train_time:26059ms step_avg:35.07ms
step:744/2160 train_time:26117ms step_avg:35.10ms
step:745/2160 train_time:26178ms step_avg:35.14ms
step:746/2160 train_time:26238ms step_avg:35.17ms
step:747/2160 train_time:26299ms step_avg:35.21ms
step:748/2160 train_time:26358ms step_avg:35.24ms
step:749/2160 train_time:26419ms step_avg:35.27ms
step:750/2160 train_time:26478ms step_avg:35.30ms
step:750/2160 val_loss:3.8439 train_time:26539ms step_avg:35.39ms
step:751/2160 train_time:26562ms step_avg:35.37ms
step:752/2160 train_time:26600ms step_avg:35.37ms
step:753/2160 train_time:26664ms step_avg:35.41ms
step:754/2160 train_time:26726ms step_avg:35.45ms
step:755/2160 train_time:26788ms step_avg:35.48ms
step:756/2160 train_time:26847ms step_avg:35.51ms
step:757/2160 train_time:26908ms step_avg:35.55ms
step:758/2160 train_time:26966ms step_avg:35.58ms
step:759/2160 train_time:27026ms step_avg:35.61ms
step:760/2160 train_time:27085ms step_avg:35.64ms
step:761/2160 train_time:27145ms step_avg:35.67ms
step:762/2160 train_time:27203ms step_avg:35.70ms
step:763/2160 train_time:27263ms step_avg:35.73ms
step:764/2160 train_time:27322ms step_avg:35.76ms
step:765/2160 train_time:27382ms step_avg:35.79ms
step:766/2160 train_time:27442ms step_avg:35.82ms
step:767/2160 train_time:27504ms step_avg:35.86ms
step:768/2160 train_time:27564ms step_avg:35.89ms
step:769/2160 train_time:27626ms step_avg:35.93ms
step:770/2160 train_time:27686ms step_avg:35.96ms
step:771/2160 train_time:27748ms step_avg:35.99ms
step:772/2160 train_time:27807ms step_avg:36.02ms
step:773/2160 train_time:27868ms step_avg:36.05ms
step:774/2160 train_time:27926ms step_avg:36.08ms
step:775/2160 train_time:27987ms step_avg:36.11ms
step:776/2160 train_time:28046ms step_avg:36.14ms
step:777/2160 train_time:28106ms step_avg:36.17ms
step:778/2160 train_time:28165ms step_avg:36.20ms
step:779/2160 train_time:28226ms step_avg:36.23ms
step:780/2160 train_time:28284ms step_avg:36.26ms
step:781/2160 train_time:28345ms step_avg:36.29ms
step:782/2160 train_time:28405ms step_avg:36.32ms
step:783/2160 train_time:28467ms step_avg:36.36ms
step:784/2160 train_time:28528ms step_avg:36.39ms
step:785/2160 train_time:28590ms step_avg:36.42ms
step:786/2160 train_time:28650ms step_avg:36.45ms
step:787/2160 train_time:28711ms step_avg:36.48ms
step:788/2160 train_time:28770ms step_avg:36.51ms
step:789/2160 train_time:28831ms step_avg:36.54ms
step:790/2160 train_time:28891ms step_avg:36.57ms
step:791/2160 train_time:28952ms step_avg:36.60ms
step:792/2160 train_time:29011ms step_avg:36.63ms
step:793/2160 train_time:29073ms step_avg:36.66ms
step:794/2160 train_time:29132ms step_avg:36.69ms
step:795/2160 train_time:29193ms step_avg:36.72ms
step:796/2160 train_time:29253ms step_avg:36.75ms
step:797/2160 train_time:29314ms step_avg:36.78ms
step:798/2160 train_time:29373ms step_avg:36.81ms
step:799/2160 train_time:29435ms step_avg:36.84ms
step:800/2160 train_time:29495ms step_avg:36.87ms
step:801/2160 train_time:29557ms step_avg:36.90ms
step:802/2160 train_time:29617ms step_avg:36.93ms
step:803/2160 train_time:29678ms step_avg:36.96ms
step:804/2160 train_time:29739ms step_avg:36.99ms
step:805/2160 train_time:29801ms step_avg:37.02ms
step:806/2160 train_time:29860ms step_avg:37.05ms
step:807/2160 train_time:29922ms step_avg:37.08ms
step:808/2160 train_time:29982ms step_avg:37.11ms
step:809/2160 train_time:30043ms step_avg:37.14ms
step:810/2160 train_time:30102ms step_avg:37.16ms
step:811/2160 train_time:30164ms step_avg:37.19ms
step:812/2160 train_time:30223ms step_avg:37.22ms
step:813/2160 train_time:30284ms step_avg:37.25ms
step:814/2160 train_time:30343ms step_avg:37.28ms
step:815/2160 train_time:30404ms step_avg:37.31ms
step:816/2160 train_time:30464ms step_avg:37.33ms
step:817/2160 train_time:30525ms step_avg:37.36ms
step:818/2160 train_time:30585ms step_avg:37.39ms
step:819/2160 train_time:30647ms step_avg:37.42ms
step:820/2160 train_time:30706ms step_avg:37.45ms
step:821/2160 train_time:30768ms step_avg:37.48ms
step:822/2160 train_time:30827ms step_avg:37.50ms
step:823/2160 train_time:30888ms step_avg:37.53ms
step:824/2160 train_time:30947ms step_avg:37.56ms
step:825/2160 train_time:31008ms step_avg:37.59ms
step:826/2160 train_time:31067ms step_avg:37.61ms
step:827/2160 train_time:31128ms step_avg:37.64ms
step:828/2160 train_time:31187ms step_avg:37.67ms
step:829/2160 train_time:31248ms step_avg:37.69ms
step:830/2160 train_time:31306ms step_avg:37.72ms
step:831/2160 train_time:31367ms step_avg:37.75ms
step:832/2160 train_time:31426ms step_avg:37.77ms
step:833/2160 train_time:31488ms step_avg:37.80ms
step:834/2160 train_time:31547ms step_avg:37.83ms
step:835/2160 train_time:31608ms step_avg:37.85ms
step:836/2160 train_time:31668ms step_avg:37.88ms
step:837/2160 train_time:31730ms step_avg:37.91ms
step:838/2160 train_time:31789ms step_avg:37.93ms
step:839/2160 train_time:31850ms step_avg:37.96ms
step:840/2160 train_time:31909ms step_avg:37.99ms
step:841/2160 train_time:31970ms step_avg:38.01ms
step:842/2160 train_time:32029ms step_avg:38.04ms
step:843/2160 train_time:32090ms step_avg:38.07ms
step:844/2160 train_time:32148ms step_avg:38.09ms
step:845/2160 train_time:32209ms step_avg:38.12ms
step:846/2160 train_time:32268ms step_avg:38.14ms
step:847/2160 train_time:32329ms step_avg:38.17ms
step:848/2160 train_time:32389ms step_avg:38.19ms
step:849/2160 train_time:32450ms step_avg:38.22ms
step:850/2160 train_time:32509ms step_avg:38.25ms
step:851/2160 train_time:32571ms step_avg:38.27ms
step:852/2160 train_time:32630ms step_avg:38.30ms
step:853/2160 train_time:32692ms step_avg:38.33ms
step:854/2160 train_time:32751ms step_avg:38.35ms
step:855/2160 train_time:32812ms step_avg:38.38ms
step:856/2160 train_time:32871ms step_avg:38.40ms
step:857/2160 train_time:32932ms step_avg:38.43ms
step:858/2160 train_time:32991ms step_avg:38.45ms
step:859/2160 train_time:33052ms step_avg:38.48ms
step:860/2160 train_time:33111ms step_avg:38.50ms
step:861/2160 train_time:33173ms step_avg:38.53ms
step:862/2160 train_time:33231ms step_avg:38.55ms
step:863/2160 train_time:33293ms step_avg:38.58ms
step:864/2160 train_time:33352ms step_avg:38.60ms
step:865/2160 train_time:33414ms step_avg:38.63ms
step:866/2160 train_time:33473ms step_avg:38.65ms
step:867/2160 train_time:33534ms step_avg:38.68ms
step:868/2160 train_time:33594ms step_avg:38.70ms
step:869/2160 train_time:33655ms step_avg:38.73ms
step:870/2160 train_time:33714ms step_avg:38.75ms
step:871/2160 train_time:33776ms step_avg:38.78ms
step:872/2160 train_time:33835ms step_avg:38.80ms
step:873/2160 train_time:33897ms step_avg:38.83ms
step:874/2160 train_time:33956ms step_avg:38.85ms
step:875/2160 train_time:34017ms step_avg:38.88ms
step:876/2160 train_time:34077ms step_avg:38.90ms
step:877/2160 train_time:34139ms step_avg:38.93ms
step:878/2160 train_time:34198ms step_avg:38.95ms
step:879/2160 train_time:34260ms step_avg:38.98ms
step:880/2160 train_time:34320ms step_avg:39.00ms
step:881/2160 train_time:34381ms step_avg:39.02ms
step:882/2160 train_time:34440ms step_avg:39.05ms
step:883/2160 train_time:34501ms step_avg:39.07ms
step:884/2160 train_time:34561ms step_avg:39.10ms
step:885/2160 train_time:34622ms step_avg:39.12ms
step:886/2160 train_time:34681ms step_avg:39.14ms
step:887/2160 train_time:34742ms step_avg:39.17ms
step:888/2160 train_time:34801ms step_avg:39.19ms
step:889/2160 train_time:34862ms step_avg:39.22ms
step:890/2160 train_time:34921ms step_avg:39.24ms
step:891/2160 train_time:34983ms step_avg:39.26ms
step:892/2160 train_time:35042ms step_avg:39.28ms
step:893/2160 train_time:35103ms step_avg:39.31ms
step:894/2160 train_time:35163ms step_avg:39.33ms
step:895/2160 train_time:35225ms step_avg:39.36ms
step:896/2160 train_time:35284ms step_avg:39.38ms
step:897/2160 train_time:35345ms step_avg:39.40ms
step:898/2160 train_time:35404ms step_avg:39.43ms
step:899/2160 train_time:35465ms step_avg:39.45ms
step:900/2160 train_time:35524ms step_avg:39.47ms
step:901/2160 train_time:35585ms step_avg:39.49ms
step:902/2160 train_time:35644ms step_avg:39.52ms
step:903/2160 train_time:35705ms step_avg:39.54ms
step:904/2160 train_time:35764ms step_avg:39.56ms
step:905/2160 train_time:35825ms step_avg:39.59ms
step:906/2160 train_time:35884ms step_avg:39.61ms
step:907/2160 train_time:35945ms step_avg:39.63ms
step:908/2160 train_time:36004ms step_avg:39.65ms
step:909/2160 train_time:36065ms step_avg:39.68ms
step:910/2160 train_time:36125ms step_avg:39.70ms
step:911/2160 train_time:36186ms step_avg:39.72ms
step:912/2160 train_time:36245ms step_avg:39.74ms
step:913/2160 train_time:36306ms step_avg:39.77ms
step:914/2160 train_time:36365ms step_avg:39.79ms
step:915/2160 train_time:36426ms step_avg:39.81ms
step:916/2160 train_time:36485ms step_avg:39.83ms
step:917/2160 train_time:36545ms step_avg:39.85ms
step:918/2160 train_time:36604ms step_avg:39.87ms
step:919/2160 train_time:36665ms step_avg:39.90ms
step:920/2160 train_time:36724ms step_avg:39.92ms
step:921/2160 train_time:36785ms step_avg:39.94ms
step:922/2160 train_time:36844ms step_avg:39.96ms
step:923/2160 train_time:36905ms step_avg:39.98ms
step:924/2160 train_time:36965ms step_avg:40.01ms
step:925/2160 train_time:37026ms step_avg:40.03ms
step:926/2160 train_time:37086ms step_avg:40.05ms
step:927/2160 train_time:37147ms step_avg:40.07ms
step:928/2160 train_time:37206ms step_avg:40.09ms
step:929/2160 train_time:37267ms step_avg:40.11ms
step:930/2160 train_time:37326ms step_avg:40.14ms
step:931/2160 train_time:37387ms step_avg:40.16ms
step:932/2160 train_time:37446ms step_avg:40.18ms
step:933/2160 train_time:37507ms step_avg:40.20ms
step:934/2160 train_time:37566ms step_avg:40.22ms
step:935/2160 train_time:37627ms step_avg:40.24ms
step:936/2160 train_time:37686ms step_avg:40.26ms
step:937/2160 train_time:37747ms step_avg:40.28ms
step:938/2160 train_time:37806ms step_avg:40.30ms
step:939/2160 train_time:37867ms step_avg:40.33ms
step:940/2160 train_time:37926ms step_avg:40.35ms
step:941/2160 train_time:37987ms step_avg:40.37ms
step:942/2160 train_time:38046ms step_avg:40.39ms
step:943/2160 train_time:38107ms step_avg:40.41ms
step:944/2160 train_time:38166ms step_avg:40.43ms
step:945/2160 train_time:38227ms step_avg:40.45ms
step:946/2160 train_time:38287ms step_avg:40.47ms
step:947/2160 train_time:38347ms step_avg:40.49ms
step:948/2160 train_time:38406ms step_avg:40.51ms
step:949/2160 train_time:38467ms step_avg:40.53ms
step:950/2160 train_time:38527ms step_avg:40.55ms
step:951/2160 train_time:38588ms step_avg:40.58ms
step:952/2160 train_time:38647ms step_avg:40.60ms
step:953/2160 train_time:38707ms step_avg:40.62ms
step:954/2160 train_time:38767ms step_avg:40.64ms
step:955/2160 train_time:38828ms step_avg:40.66ms
step:956/2160 train_time:38887ms step_avg:40.68ms
step:957/2160 train_time:38948ms step_avg:40.70ms
step:958/2160 train_time:39007ms step_avg:40.72ms
step:959/2160 train_time:39068ms step_avg:40.74ms
step:960/2160 train_time:39128ms step_avg:40.76ms
step:961/2160 train_time:39189ms step_avg:40.78ms
step:962/2160 train_time:39248ms step_avg:40.80ms
step:963/2160 train_time:39309ms step_avg:40.82ms
step:964/2160 train_time:39368ms step_avg:40.84ms
step:965/2160 train_time:39429ms step_avg:40.86ms
step:966/2160 train_time:39489ms step_avg:40.88ms
step:967/2160 train_time:39550ms step_avg:40.90ms
step:968/2160 train_time:39609ms step_avg:40.92ms
step:969/2160 train_time:39670ms step_avg:40.94ms
step:970/2160 train_time:39729ms step_avg:40.96ms
step:971/2160 train_time:39790ms step_avg:40.98ms
step:972/2160 train_time:39849ms step_avg:41.00ms
step:973/2160 train_time:39910ms step_avg:41.02ms
step:974/2160 train_time:39969ms step_avg:41.04ms
step:975/2160 train_time:40030ms step_avg:41.06ms
step:976/2160 train_time:40090ms step_avg:41.08ms
step:977/2160 train_time:40151ms step_avg:41.10ms
step:978/2160 train_time:40210ms step_avg:41.11ms
step:979/2160 train_time:40270ms step_avg:41.13ms
step:980/2160 train_time:40330ms step_avg:41.15ms
step:981/2160 train_time:40392ms step_avg:41.17ms
step:982/2160 train_time:40450ms step_avg:41.19ms
step:983/2160 train_time:40512ms step_avg:41.21ms
step:984/2160 train_time:40571ms step_avg:41.23ms
step:985/2160 train_time:40632ms step_avg:41.25ms
step:986/2160 train_time:40692ms step_avg:41.27ms
step:987/2160 train_time:40753ms step_avg:41.29ms
step:988/2160 train_time:40812ms step_avg:41.31ms
step:989/2160 train_time:40874ms step_avg:41.33ms
step:990/2160 train_time:40934ms step_avg:41.35ms
step:991/2160 train_time:40995ms step_avg:41.37ms
step:992/2160 train_time:41055ms step_avg:41.39ms
step:993/2160 train_time:41116ms step_avg:41.41ms
step:994/2160 train_time:41176ms step_avg:41.42ms
step:995/2160 train_time:41237ms step_avg:41.44ms
step:996/2160 train_time:41296ms step_avg:41.46ms
step:997/2160 train_time:41358ms step_avg:41.48ms
step:998/2160 train_time:41417ms step_avg:41.50ms
step:999/2160 train_time:41479ms step_avg:41.52ms
step:1000/2160 train_time:41539ms step_avg:41.54ms
step:1000/2160 val_loss:3.6921 train_time:41601ms step_avg:41.60ms
step:1001/2160 train_time:41624ms step_avg:41.58ms
step:1002/2160 train_time:41661ms step_avg:41.58ms
step:1003/2160 train_time:41726ms step_avg:41.60ms
step:1004/2160 train_time:41788ms step_avg:41.62ms
step:1005/2160 train_time:41849ms step_avg:41.64ms
step:1006/2160 train_time:41909ms step_avg:41.66ms
step:1007/2160 train_time:41969ms step_avg:41.68ms
step:1008/2160 train_time:42028ms step_avg:41.69ms
step:1009/2160 train_time:42088ms step_avg:41.71ms
step:1010/2160 train_time:42147ms step_avg:41.73ms
step:1011/2160 train_time:42207ms step_avg:41.75ms
step:1012/2160 train_time:42266ms step_avg:41.76ms
step:1013/2160 train_time:42326ms step_avg:41.78ms
step:1014/2160 train_time:42385ms step_avg:41.80ms
step:1015/2160 train_time:42446ms step_avg:41.82ms
step:1016/2160 train_time:42505ms step_avg:41.84ms
step:1017/2160 train_time:42567ms step_avg:41.86ms
step:1018/2160 train_time:42628ms step_avg:41.87ms
step:1019/2160 train_time:42691ms step_avg:41.89ms
step:1020/2160 train_time:42751ms step_avg:41.91ms
step:1021/2160 train_time:42813ms step_avg:41.93ms
step:1022/2160 train_time:42873ms step_avg:41.95ms
step:1023/2160 train_time:42934ms step_avg:41.97ms
step:1024/2160 train_time:42992ms step_avg:41.98ms
step:1025/2160 train_time:43053ms step_avg:42.00ms
step:1026/2160 train_time:43112ms step_avg:42.02ms
step:1027/2160 train_time:43172ms step_avg:42.04ms
step:1028/2160 train_time:43231ms step_avg:42.05ms
step:1029/2160 train_time:43291ms step_avg:42.07ms
step:1030/2160 train_time:43350ms step_avg:42.09ms
step:1031/2160 train_time:43410ms step_avg:42.11ms
step:1032/2160 train_time:43470ms step_avg:42.12ms
step:1033/2160 train_time:43531ms step_avg:42.14ms
step:1034/2160 train_time:43591ms step_avg:42.16ms
step:1035/2160 train_time:43652ms step_avg:42.18ms
step:1036/2160 train_time:43713ms step_avg:42.19ms
step:1037/2160 train_time:43776ms step_avg:42.21ms
step:1038/2160 train_time:43835ms step_avg:42.23ms
step:1039/2160 train_time:43895ms step_avg:42.25ms
step:1040/2160 train_time:43955ms step_avg:42.26ms
step:1041/2160 train_time:44016ms step_avg:42.28ms
step:1042/2160 train_time:44075ms step_avg:42.30ms
step:1043/2160 train_time:44136ms step_avg:42.32ms
step:1044/2160 train_time:44196ms step_avg:42.33ms
step:1045/2160 train_time:44257ms step_avg:42.35ms
step:1046/2160 train_time:44316ms step_avg:42.37ms
step:1047/2160 train_time:44377ms step_avg:42.39ms
step:1048/2160 train_time:44437ms step_avg:42.40ms
step:1049/2160 train_time:44498ms step_avg:42.42ms
step:1050/2160 train_time:44558ms step_avg:42.44ms
step:1051/2160 train_time:44620ms step_avg:42.45ms
step:1052/2160 train_time:44680ms step_avg:42.47ms
step:1053/2160 train_time:44741ms step_avg:42.49ms
step:1054/2160 train_time:44801ms step_avg:42.51ms
step:1055/2160 train_time:44863ms step_avg:42.52ms
step:1056/2160 train_time:44923ms step_avg:42.54ms
step:1057/2160 train_time:44984ms step_avg:42.56ms
step:1058/2160 train_time:45044ms step_avg:42.57ms
step:1059/2160 train_time:45104ms step_avg:42.59ms
step:1060/2160 train_time:45164ms step_avg:42.61ms
step:1061/2160 train_time:45226ms step_avg:42.63ms
step:1062/2160 train_time:45285ms step_avg:42.64ms
step:1063/2160 train_time:45347ms step_avg:42.66ms
step:1064/2160 train_time:45407ms step_avg:42.68ms
step:1065/2160 train_time:45468ms step_avg:42.69ms
step:1066/2160 train_time:45527ms step_avg:42.71ms
step:1067/2160 train_time:45588ms step_avg:42.73ms
step:1068/2160 train_time:45648ms step_avg:42.74ms
step:1069/2160 train_time:45709ms step_avg:42.76ms
step:1070/2160 train_time:45769ms step_avg:42.78ms
step:1071/2160 train_time:45831ms step_avg:42.79ms
step:1072/2160 train_time:45891ms step_avg:42.81ms
step:1073/2160 train_time:45952ms step_avg:42.83ms
step:1074/2160 train_time:46012ms step_avg:42.84ms
step:1075/2160 train_time:46073ms step_avg:42.86ms
step:1076/2160 train_time:46132ms step_avg:42.87ms
step:1077/2160 train_time:46192ms step_avg:42.89ms
step:1078/2160 train_time:46251ms step_avg:42.90ms
step:1079/2160 train_time:46312ms step_avg:42.92ms
step:1080/2160 train_time:46371ms step_avg:42.94ms
step:1081/2160 train_time:46431ms step_avg:42.95ms
step:1082/2160 train_time:46490ms step_avg:42.97ms
step:1083/2160 train_time:46551ms step_avg:42.98ms
step:1084/2160 train_time:46610ms step_avg:43.00ms
step:1085/2160 train_time:46672ms step_avg:43.02ms
step:1086/2160 train_time:46732ms step_avg:43.03ms
step:1087/2160 train_time:46794ms step_avg:43.05ms
step:1088/2160 train_time:46853ms step_avg:43.06ms
step:1089/2160 train_time:46914ms step_avg:43.08ms
step:1090/2160 train_time:46974ms step_avg:43.10ms
step:1091/2160 train_time:47034ms step_avg:43.11ms
step:1092/2160 train_time:47093ms step_avg:43.13ms
step:1093/2160 train_time:47154ms step_avg:43.14ms
step:1094/2160 train_time:47213ms step_avg:43.16ms
step:1095/2160 train_time:47275ms step_avg:43.17ms
step:1096/2160 train_time:47334ms step_avg:43.19ms
step:1097/2160 train_time:47395ms step_avg:43.20ms
step:1098/2160 train_time:47454ms step_avg:43.22ms
step:1099/2160 train_time:47515ms step_avg:43.24ms
step:1100/2160 train_time:47575ms step_avg:43.25ms
step:1101/2160 train_time:47637ms step_avg:43.27ms
step:1102/2160 train_time:47696ms step_avg:43.28ms
step:1103/2160 train_time:47758ms step_avg:43.30ms
step:1104/2160 train_time:47818ms step_avg:43.31ms
step:1105/2160 train_time:47880ms step_avg:43.33ms
step:1106/2160 train_time:47939ms step_avg:43.34ms
step:1107/2160 train_time:48000ms step_avg:43.36ms
step:1108/2160 train_time:48059ms step_avg:43.37ms
step:1109/2160 train_time:48121ms step_avg:43.39ms
step:1110/2160 train_time:48180ms step_avg:43.41ms
step:1111/2160 train_time:48242ms step_avg:43.42ms
step:1112/2160 train_time:48301ms step_avg:43.44ms
step:1113/2160 train_time:48363ms step_avg:43.45ms
step:1114/2160 train_time:48423ms step_avg:43.47ms
step:1115/2160 train_time:48485ms step_avg:43.48ms
step:1116/2160 train_time:48544ms step_avg:43.50ms
step:1117/2160 train_time:48606ms step_avg:43.51ms
step:1118/2160 train_time:48665ms step_avg:43.53ms
step:1119/2160 train_time:48727ms step_avg:43.55ms
step:1120/2160 train_time:48786ms step_avg:43.56ms
step:1121/2160 train_time:48848ms step_avg:43.58ms
step:1122/2160 train_time:48907ms step_avg:43.59ms
step:1123/2160 train_time:48968ms step_avg:43.60ms
step:1124/2160 train_time:49028ms step_avg:43.62ms
step:1125/2160 train_time:49089ms step_avg:43.63ms
step:1126/2160 train_time:49148ms step_avg:43.65ms
step:1127/2160 train_time:49210ms step_avg:43.66ms
step:1128/2160 train_time:49269ms step_avg:43.68ms
step:1129/2160 train_time:49330ms step_avg:43.69ms
step:1130/2160 train_time:49389ms step_avg:43.71ms
step:1131/2160 train_time:49450ms step_avg:43.72ms
step:1132/2160 train_time:49509ms step_avg:43.74ms
step:1133/2160 train_time:49570ms step_avg:43.75ms
step:1134/2160 train_time:49629ms step_avg:43.76ms
step:1135/2160 train_time:49689ms step_avg:43.78ms
step:1136/2160 train_time:49748ms step_avg:43.79ms
step:1137/2160 train_time:49810ms step_avg:43.81ms
step:1138/2160 train_time:49869ms step_avg:43.82ms
step:1139/2160 train_time:49930ms step_avg:43.84ms
step:1140/2160 train_time:49989ms step_avg:43.85ms
step:1141/2160 train_time:50050ms step_avg:43.87ms
step:1142/2160 train_time:50110ms step_avg:43.88ms
step:1143/2160 train_time:50171ms step_avg:43.89ms
step:1144/2160 train_time:50230ms step_avg:43.91ms
step:1145/2160 train_time:50291ms step_avg:43.92ms
step:1146/2160 train_time:50350ms step_avg:43.94ms
step:1147/2160 train_time:50411ms step_avg:43.95ms
step:1148/2160 train_time:50470ms step_avg:43.96ms
step:1149/2160 train_time:50531ms step_avg:43.98ms
step:1150/2160 train_time:50590ms step_avg:43.99ms
step:1151/2160 train_time:50651ms step_avg:44.01ms
step:1152/2160 train_time:50710ms step_avg:44.02ms
step:1153/2160 train_time:50771ms step_avg:44.03ms
step:1154/2160 train_time:50831ms step_avg:44.05ms
step:1155/2160 train_time:50891ms step_avg:44.06ms
step:1156/2160 train_time:50950ms step_avg:44.07ms
step:1157/2160 train_time:51011ms step_avg:44.09ms
step:1158/2160 train_time:51071ms step_avg:44.10ms
step:1159/2160 train_time:51132ms step_avg:44.12ms
step:1160/2160 train_time:51191ms step_avg:44.13ms
step:1161/2160 train_time:51253ms step_avg:44.15ms
step:1162/2160 train_time:51312ms step_avg:44.16ms
step:1163/2160 train_time:51373ms step_avg:44.17ms
step:1164/2160 train_time:51432ms step_avg:44.19ms
step:1165/2160 train_time:51493ms step_avg:44.20ms
step:1166/2160 train_time:51552ms step_avg:44.21ms
step:1167/2160 train_time:51612ms step_avg:44.23ms
step:1168/2160 train_time:51672ms step_avg:44.24ms
step:1169/2160 train_time:51732ms step_avg:44.25ms
step:1170/2160 train_time:51792ms step_avg:44.27ms
step:1171/2160 train_time:51852ms step_avg:44.28ms
step:1172/2160 train_time:51911ms step_avg:44.29ms
step:1173/2160 train_time:51973ms step_avg:44.31ms
step:1174/2160 train_time:52032ms step_avg:44.32ms
step:1175/2160 train_time:52093ms step_avg:44.33ms
step:1176/2160 train_time:52153ms step_avg:44.35ms
step:1177/2160 train_time:52213ms step_avg:44.36ms
step:1178/2160 train_time:52273ms step_avg:44.37ms
step:1179/2160 train_time:52334ms step_avg:44.39ms
step:1180/2160 train_time:52394ms step_avg:44.40ms
step:1181/2160 train_time:52455ms step_avg:44.42ms
step:1182/2160 train_time:52514ms step_avg:44.43ms
step:1183/2160 train_time:52576ms step_avg:44.44ms
step:1184/2160 train_time:52635ms step_avg:44.46ms
step:1185/2160 train_time:52696ms step_avg:44.47ms
step:1186/2160 train_time:52756ms step_avg:44.48ms
step:1187/2160 train_time:52817ms step_avg:44.50ms
step:1188/2160 train_time:52876ms step_avg:44.51ms
step:1189/2160 train_time:52937ms step_avg:44.52ms
step:1190/2160 train_time:52997ms step_avg:44.54ms
step:1191/2160 train_time:53059ms step_avg:44.55ms
step:1192/2160 train_time:53119ms step_avg:44.56ms
step:1193/2160 train_time:53180ms step_avg:44.58ms
step:1194/2160 train_time:53239ms step_avg:44.59ms
step:1195/2160 train_time:53301ms step_avg:44.60ms
step:1196/2160 train_time:53361ms step_avg:44.62ms
step:1197/2160 train_time:53422ms step_avg:44.63ms
step:1198/2160 train_time:53482ms step_avg:44.64ms
step:1199/2160 train_time:53544ms step_avg:44.66ms
step:1200/2160 train_time:53604ms step_avg:44.67ms
step:1201/2160 train_time:53665ms step_avg:44.68ms
step:1202/2160 train_time:53725ms step_avg:44.70ms
step:1203/2160 train_time:53786ms step_avg:44.71ms
step:1204/2160 train_time:53845ms step_avg:44.72ms
step:1205/2160 train_time:53907ms step_avg:44.74ms
step:1206/2160 train_time:53967ms step_avg:44.75ms
step:1207/2160 train_time:54029ms step_avg:44.76ms
step:1208/2160 train_time:54087ms step_avg:44.77ms
step:1209/2160 train_time:54148ms step_avg:44.79ms
step:1210/2160 train_time:54208ms step_avg:44.80ms
step:1211/2160 train_time:54269ms step_avg:44.81ms
step:1212/2160 train_time:54329ms step_avg:44.83ms
step:1213/2160 train_time:54390ms step_avg:44.84ms
step:1214/2160 train_time:54449ms step_avg:44.85ms
step:1215/2160 train_time:54511ms step_avg:44.86ms
step:1216/2160 train_time:54570ms step_avg:44.88ms
step:1217/2160 train_time:54632ms step_avg:44.89ms
step:1218/2160 train_time:54691ms step_avg:44.90ms
step:1219/2160 train_time:54751ms step_avg:44.92ms
step:1220/2160 train_time:54810ms step_avg:44.93ms
step:1221/2160 train_time:54871ms step_avg:44.94ms
step:1222/2160 train_time:54931ms step_avg:44.95ms
step:1223/2160 train_time:54992ms step_avg:44.96ms
step:1224/2160 train_time:55050ms step_avg:44.98ms
step:1225/2160 train_time:55112ms step_avg:44.99ms
step:1226/2160 train_time:55171ms step_avg:45.00ms
step:1227/2160 train_time:55233ms step_avg:45.01ms
step:1228/2160 train_time:55292ms step_avg:45.03ms
step:1229/2160 train_time:55352ms step_avg:45.04ms
step:1230/2160 train_time:55411ms step_avg:45.05ms
step:1231/2160 train_time:55472ms step_avg:45.06ms
step:1232/2160 train_time:55532ms step_avg:45.07ms
step:1233/2160 train_time:55592ms step_avg:45.09ms
step:1234/2160 train_time:55652ms step_avg:45.10ms
step:1235/2160 train_time:55712ms step_avg:45.11ms
step:1236/2160 train_time:55771ms step_avg:45.12ms
step:1237/2160 train_time:55832ms step_avg:45.14ms
step:1238/2160 train_time:55891ms step_avg:45.15ms
step:1239/2160 train_time:55953ms step_avg:45.16ms
step:1240/2160 train_time:56012ms step_avg:45.17ms
step:1241/2160 train_time:56073ms step_avg:45.18ms
step:1242/2160 train_time:56133ms step_avg:45.20ms
step:1243/2160 train_time:56194ms step_avg:45.21ms
step:1244/2160 train_time:56253ms step_avg:45.22ms
step:1245/2160 train_time:56313ms step_avg:45.23ms
step:1246/2160 train_time:56373ms step_avg:45.24ms
step:1247/2160 train_time:56434ms step_avg:45.26ms
step:1248/2160 train_time:56494ms step_avg:45.27ms
step:1249/2160 train_time:56555ms step_avg:45.28ms
step:1250/2160 train_time:56615ms step_avg:45.29ms
step:1250/2160 val_loss:3.5680 train_time:56676ms step_avg:45.34ms
step:1251/2160 train_time:56699ms step_avg:45.32ms
step:1252/2160 train_time:56739ms step_avg:45.32ms
step:1253/2160 train_time:56804ms step_avg:45.33ms
step:1254/2160 train_time:56868ms step_avg:45.35ms
step:1255/2160 train_time:56929ms step_avg:45.36ms
step:1256/2160 train_time:56989ms step_avg:45.37ms
step:1257/2160 train_time:57050ms step_avg:45.39ms
step:1258/2160 train_time:57108ms step_avg:45.40ms
step:1259/2160 train_time:57169ms step_avg:45.41ms
step:1260/2160 train_time:57227ms step_avg:45.42ms
step:1261/2160 train_time:57288ms step_avg:45.43ms
step:1262/2160 train_time:57346ms step_avg:45.44ms
step:1263/2160 train_time:57407ms step_avg:45.45ms
step:1264/2160 train_time:57466ms step_avg:45.46ms
step:1265/2160 train_time:57526ms step_avg:45.48ms
step:1266/2160 train_time:57585ms step_avg:45.49ms
step:1267/2160 train_time:57647ms step_avg:45.50ms
step:1268/2160 train_time:57708ms step_avg:45.51ms
step:1269/2160 train_time:57772ms step_avg:45.53ms
step:1270/2160 train_time:57832ms step_avg:45.54ms
step:1271/2160 train_time:57894ms step_avg:45.55ms
step:1272/2160 train_time:57954ms step_avg:45.56ms
step:1273/2160 train_time:58015ms step_avg:45.57ms
step:1274/2160 train_time:58074ms step_avg:45.58ms
step:1275/2160 train_time:58135ms step_avg:45.60ms
step:1276/2160 train_time:58194ms step_avg:45.61ms
step:1277/2160 train_time:58254ms step_avg:45.62ms
step:1278/2160 train_time:58313ms step_avg:45.63ms
step:1279/2160 train_time:58373ms step_avg:45.64ms
step:1280/2160 train_time:58432ms step_avg:45.65ms
step:1281/2160 train_time:58492ms step_avg:45.66ms
step:1282/2160 train_time:58551ms step_avg:45.67ms
step:1283/2160 train_time:58612ms step_avg:45.68ms
step:1284/2160 train_time:58672ms step_avg:45.69ms
step:1285/2160 train_time:58733ms step_avg:45.71ms
step:1286/2160 train_time:58793ms step_avg:45.72ms
step:1287/2160 train_time:58855ms step_avg:45.73ms
step:1288/2160 train_time:58914ms step_avg:45.74ms
step:1289/2160 train_time:58976ms step_avg:45.75ms
step:1290/2160 train_time:59035ms step_avg:45.76ms
step:1291/2160 train_time:59096ms step_avg:45.78ms
step:1292/2160 train_time:59155ms step_avg:45.79ms
step:1293/2160 train_time:59215ms step_avg:45.80ms
step:1294/2160 train_time:59274ms step_avg:45.81ms
step:1295/2160 train_time:59334ms step_avg:45.82ms
step:1296/2160 train_time:59393ms step_avg:45.83ms
step:1297/2160 train_time:59454ms step_avg:45.84ms
step:1298/2160 train_time:59512ms step_avg:45.85ms
step:1299/2160 train_time:59573ms step_avg:45.86ms
step:1300/2160 train_time:59633ms step_avg:45.87ms
step:1301/2160 train_time:59694ms step_avg:45.88ms
step:1302/2160 train_time:59754ms step_avg:45.89ms
step:1303/2160 train_time:59816ms step_avg:45.91ms
step:1304/2160 train_time:59875ms step_avg:45.92ms
step:1305/2160 train_time:59936ms step_avg:45.93ms
step:1306/2160 train_time:59996ms step_avg:45.94ms
step:1307/2160 train_time:60057ms step_avg:45.95ms
step:1308/2160 train_time:60116ms step_avg:45.96ms
step:1309/2160 train_time:60177ms step_avg:45.97ms
step:1310/2160 train_time:60235ms step_avg:45.98ms
step:1311/2160 train_time:60297ms step_avg:45.99ms
step:1312/2160 train_time:60356ms step_avg:46.00ms
step:1313/2160 train_time:60416ms step_avg:46.01ms
step:1314/2160 train_time:60476ms step_avg:46.02ms
step:1315/2160 train_time:60537ms step_avg:46.04ms
step:1316/2160 train_time:60597ms step_avg:46.05ms
step:1317/2160 train_time:60658ms step_avg:46.06ms
step:1318/2160 train_time:60717ms step_avg:46.07ms
step:1319/2160 train_time:60779ms step_avg:46.08ms
step:1320/2160 train_time:60838ms step_avg:46.09ms
step:1321/2160 train_time:60899ms step_avg:46.10ms
step:1322/2160 train_time:60959ms step_avg:46.11ms
step:1323/2160 train_time:61020ms step_avg:46.12ms
step:1324/2160 train_time:61079ms step_avg:46.13ms
step:1325/2160 train_time:61141ms step_avg:46.14ms
step:1326/2160 train_time:61201ms step_avg:46.15ms
step:1327/2160 train_time:61262ms step_avg:46.17ms
step:1328/2160 train_time:61322ms step_avg:46.18ms
step:1329/2160 train_time:61383ms step_avg:46.19ms
step:1330/2160 train_time:61443ms step_avg:46.20ms
step:1331/2160 train_time:61505ms step_avg:46.21ms
step:1332/2160 train_time:61564ms step_avg:46.22ms
step:1333/2160 train_time:61625ms step_avg:46.23ms
step:1334/2160 train_time:61685ms step_avg:46.24ms
step:1335/2160 train_time:61747ms step_avg:46.25ms
step:1336/2160 train_time:61806ms step_avg:46.26ms
step:1337/2160 train_time:61868ms step_avg:46.27ms
step:1338/2160 train_time:61927ms step_avg:46.28ms
step:1339/2160 train_time:61988ms step_avg:46.29ms
step:1340/2160 train_time:62048ms step_avg:46.30ms
step:1341/2160 train_time:62109ms step_avg:46.32ms
step:1342/2160 train_time:62168ms step_avg:46.33ms
step:1343/2160 train_time:62230ms step_avg:46.34ms
step:1344/2160 train_time:62289ms step_avg:46.35ms
step:1345/2160 train_time:62350ms step_avg:46.36ms
step:1346/2160 train_time:62410ms step_avg:46.37ms
step:1347/2160 train_time:62470ms step_avg:46.38ms
step:1348/2160 train_time:62530ms step_avg:46.39ms
step:1349/2160 train_time:62590ms step_avg:46.40ms
step:1350/2160 train_time:62650ms step_avg:46.41ms
step:1351/2160 train_time:62712ms step_avg:46.42ms
step:1352/2160 train_time:62771ms step_avg:46.43ms
step:1353/2160 train_time:62832ms step_avg:46.44ms
step:1354/2160 train_time:62891ms step_avg:46.45ms
step:1355/2160 train_time:62952ms step_avg:46.46ms
step:1356/2160 train_time:63011ms step_avg:46.47ms
step:1357/2160 train_time:63071ms step_avg:46.48ms
step:1358/2160 train_time:63131ms step_avg:46.49ms
step:1359/2160 train_time:63191ms step_avg:46.50ms
step:1360/2160 train_time:63250ms step_avg:46.51ms
step:1361/2160 train_time:63312ms step_avg:46.52ms
step:1362/2160 train_time:63371ms step_avg:46.53ms
step:1363/2160 train_time:63432ms step_avg:46.54ms
step:1364/2160 train_time:63491ms step_avg:46.55ms
step:1365/2160 train_time:63552ms step_avg:46.56ms
step:1366/2160 train_time:63611ms step_avg:46.57ms
step:1367/2160 train_time:63672ms step_avg:46.58ms
step:1368/2160 train_time:63732ms step_avg:46.59ms
step:1369/2160 train_time:63793ms step_avg:46.60ms
step:1370/2160 train_time:63851ms step_avg:46.61ms
step:1371/2160 train_time:63912ms step_avg:46.62ms
step:1372/2160 train_time:63971ms step_avg:46.63ms
step:1373/2160 train_time:64032ms step_avg:46.64ms
step:1374/2160 train_time:64091ms step_avg:46.65ms
step:1375/2160 train_time:64152ms step_avg:46.66ms
step:1376/2160 train_time:64211ms step_avg:46.67ms
step:1377/2160 train_time:64273ms step_avg:46.68ms
step:1378/2160 train_time:64332ms step_avg:46.69ms
step:1379/2160 train_time:64393ms step_avg:46.70ms
step:1380/2160 train_time:64453ms step_avg:46.70ms
step:1381/2160 train_time:64514ms step_avg:46.72ms
step:1382/2160 train_time:64573ms step_avg:46.72ms
step:1383/2160 train_time:64634ms step_avg:46.73ms
step:1384/2160 train_time:64693ms step_avg:46.74ms
step:1385/2160 train_time:64754ms step_avg:46.75ms
step:1386/2160 train_time:64813ms step_avg:46.76ms
step:1387/2160 train_time:64874ms step_avg:46.77ms
step:1388/2160 train_time:64933ms step_avg:46.78ms
step:1389/2160 train_time:64994ms step_avg:46.79ms
step:1390/2160 train_time:65053ms step_avg:46.80ms
step:1391/2160 train_time:65114ms step_avg:46.81ms
step:1392/2160 train_time:65173ms step_avg:46.82ms
step:1393/2160 train_time:65234ms step_avg:46.83ms
step:1394/2160 train_time:65293ms step_avg:46.84ms
step:1395/2160 train_time:65354ms step_avg:46.85ms
step:1396/2160 train_time:65414ms step_avg:46.86ms
step:1397/2160 train_time:65475ms step_avg:46.87ms
step:1398/2160 train_time:65534ms step_avg:46.88ms
step:1399/2160 train_time:65595ms step_avg:46.89ms
step:1400/2160 train_time:65654ms step_avg:46.90ms
step:1401/2160 train_time:65715ms step_avg:46.91ms
step:1402/2160 train_time:65774ms step_avg:46.91ms
step:1403/2160 train_time:65835ms step_avg:46.92ms
step:1404/2160 train_time:65894ms step_avg:46.93ms
step:1405/2160 train_time:65955ms step_avg:46.94ms
step:1406/2160 train_time:66014ms step_avg:46.95ms
step:1407/2160 train_time:66075ms step_avg:46.96ms
step:1408/2160 train_time:66134ms step_avg:46.97ms
step:1409/2160 train_time:66195ms step_avg:46.98ms
step:1410/2160 train_time:66254ms step_avg:46.99ms
step:1411/2160 train_time:66316ms step_avg:47.00ms
step:1412/2160 train_time:66375ms step_avg:47.01ms
step:1413/2160 train_time:66437ms step_avg:47.02ms
step:1414/2160 train_time:66495ms step_avg:47.03ms
step:1415/2160 train_time:66556ms step_avg:47.04ms
step:1416/2160 train_time:66644ms step_avg:47.06ms
step:1417/2160 train_time:66733ms step_avg:47.09ms
step:1418/2160 train_time:66820ms step_avg:47.12ms
step:1419/2160 train_time:66908ms step_avg:47.15ms
step:1420/2160 train_time:66996ms step_avg:47.18ms
step:1421/2160 train_time:67084ms step_avg:47.21ms
step:1422/2160 train_time:67172ms step_avg:47.24ms
step:1423/2160 train_time:67261ms step_avg:47.27ms
step:1424/2160 train_time:67347ms step_avg:47.29ms
step:1425/2160 train_time:67437ms step_avg:47.32ms
step:1426/2160 train_time:67524ms step_avg:47.35ms
step:1427/2160 train_time:67613ms step_avg:47.38ms
step:1428/2160 train_time:67700ms step_avg:47.41ms
step:1429/2160 train_time:67789ms step_avg:47.44ms
step:1430/2160 train_time:67876ms step_avg:47.47ms
step:1431/2160 train_time:67964ms step_avg:47.49ms
step:1432/2160 train_time:68051ms step_avg:47.52ms
step:1433/2160 train_time:68140ms step_avg:47.55ms
step:1434/2160 train_time:68227ms step_avg:47.58ms
step:1435/2160 train_time:68316ms step_avg:47.61ms
step:1436/2160 train_time:68403ms step_avg:47.63ms
step:1437/2160 train_time:68491ms step_avg:47.66ms
step:1438/2160 train_time:68578ms step_avg:47.69ms
step:1439/2160 train_time:68667ms step_avg:47.72ms
step:1440/2160 train_time:68754ms step_avg:47.75ms
step:1441/2160 train_time:68842ms step_avg:47.77ms
step:1442/2160 train_time:68929ms step_avg:47.80ms
step:1443/2160 train_time:69018ms step_avg:47.83ms
step:1444/2160 train_time:69104ms step_avg:47.86ms
step:1445/2160 train_time:69193ms step_avg:47.88ms
step:1446/2160 train_time:69281ms step_avg:47.91ms
step:1447/2160 train_time:69369ms step_avg:47.94ms
step:1448/2160 train_time:69457ms step_avg:47.97ms
step:1449/2160 train_time:69546ms step_avg:48.00ms
step:1450/2160 train_time:69633ms step_avg:48.02ms
step:1451/2160 train_time:69723ms step_avg:48.05ms
step:1452/2160 train_time:69810ms step_avg:48.08ms
step:1453/2160 train_time:69899ms step_avg:48.11ms
step:1454/2160 train_time:69986ms step_avg:48.13ms
step:1455/2160 train_time:70074ms step_avg:48.16ms
step:1456/2160 train_time:70161ms step_avg:48.19ms
step:1457/2160 train_time:70250ms step_avg:48.22ms
step:1458/2160 train_time:70337ms step_avg:48.24ms
step:1459/2160 train_time:70426ms step_avg:48.27ms
step:1460/2160 train_time:70514ms step_avg:48.30ms
step:1461/2160 train_time:70602ms step_avg:48.32ms
step:1462/2160 train_time:70689ms step_avg:48.35ms
step:1463/2160 train_time:70779ms step_avg:48.38ms
step:1464/2160 train_time:70865ms step_avg:48.41ms
step:1465/2160 train_time:70954ms step_avg:48.43ms
step:1466/2160 train_time:71040ms step_avg:48.46ms
step:1467/2160 train_time:71130ms step_avg:48.49ms
step:1468/2160 train_time:71217ms step_avg:48.51ms
step:1469/2160 train_time:71306ms step_avg:48.54ms
step:1470/2160 train_time:71393ms step_avg:48.57ms
step:1471/2160 train_time:71482ms step_avg:48.59ms
step:1472/2160 train_time:71569ms step_avg:48.62ms
step:1473/2160 train_time:71659ms step_avg:48.65ms
step:1474/2160 train_time:71746ms step_avg:48.67ms
step:1475/2160 train_time:71834ms step_avg:48.70ms
step:1476/2160 train_time:71921ms step_avg:48.73ms
step:1477/2160 train_time:72009ms step_avg:48.75ms
step:1478/2160 train_time:72096ms step_avg:48.78ms
step:1479/2160 train_time:72185ms step_avg:48.81ms
step:1480/2160 train_time:72272ms step_avg:48.83ms
step:1481/2160 train_time:72362ms step_avg:48.86ms
step:1482/2160 train_time:72449ms step_avg:48.89ms
step:1483/2160 train_time:72538ms step_avg:48.91ms
step:1484/2160 train_time:72625ms step_avg:48.94ms
step:1485/2160 train_time:72715ms step_avg:48.97ms
step:1486/2160 train_time:72802ms step_avg:48.99ms
step:1487/2160 train_time:72892ms step_avg:49.02ms
step:1488/2160 train_time:72979ms step_avg:49.05ms
step:1489/2160 train_time:73067ms step_avg:49.07ms
step:1490/2160 train_time:73154ms step_avg:49.10ms
step:1491/2160 train_time:73242ms step_avg:49.12ms
step:1492/2160 train_time:73330ms step_avg:49.15ms
step:1493/2160 train_time:73419ms step_avg:49.18ms
step:1494/2160 train_time:73506ms step_avg:49.20ms
step:1495/2160 train_time:73595ms step_avg:49.23ms
step:1496/2160 train_time:73682ms step_avg:49.25ms
step:1497/2160 train_time:73773ms step_avg:49.28ms
step:1498/2160 train_time:73860ms step_avg:49.31ms
step:1499/2160 train_time:73948ms step_avg:49.33ms
step:1500/2160 train_time:74035ms step_avg:49.36ms
step:1500/2160 val_loss:3.4681 train_time:74123ms step_avg:49.42ms
step:1501/2160 train_time:74147ms step_avg:49.40ms
step:1502/2160 train_time:74215ms step_avg:49.41ms
step:1503/2160 train_time:74310ms step_avg:49.44ms
step:1504/2160 train_time:74397ms step_avg:49.47ms
step:1505/2160 train_time:74484ms step_avg:49.49ms
step:1506/2160 train_time:74570ms step_avg:49.52ms
step:1507/2160 train_time:74658ms step_avg:49.54ms
step:1508/2160 train_time:74744ms step_avg:49.57ms
step:1509/2160 train_time:74831ms step_avg:49.59ms
step:1510/2160 train_time:74917ms step_avg:49.61ms
step:1511/2160 train_time:75004ms step_avg:49.64ms
step:1512/2160 train_time:75095ms step_avg:49.67ms
step:1513/2160 train_time:75186ms step_avg:49.69ms
step:1514/2160 train_time:75277ms step_avg:49.72ms
step:1515/2160 train_time:75366ms step_avg:49.75ms
step:1516/2160 train_time:75453ms step_avg:49.77ms
step:1517/2160 train_time:75541ms step_avg:49.80ms
step:1518/2160 train_time:75627ms step_avg:49.82ms
step:1519/2160 train_time:75716ms step_avg:49.85ms
step:1520/2160 train_time:75802ms step_avg:49.87ms
step:1521/2160 train_time:75889ms step_avg:49.89ms
step:1522/2160 train_time:75976ms step_avg:49.92ms
step:1523/2160 train_time:76065ms step_avg:49.94ms
step:1524/2160 train_time:76153ms step_avg:49.97ms
step:1525/2160 train_time:76245ms step_avg:50.00ms
step:1526/2160 train_time:76333ms step_avg:50.02ms
step:1527/2160 train_time:76422ms step_avg:50.05ms
step:1528/2160 train_time:76510ms step_avg:50.07ms
step:1529/2160 train_time:76598ms step_avg:50.10ms
step:1530/2160 train_time:76685ms step_avg:50.12ms
step:1531/2160 train_time:76773ms step_avg:50.15ms
step:1532/2160 train_time:76859ms step_avg:50.17ms
step:1533/2160 train_time:76948ms step_avg:50.19ms
step:1534/2160 train_time:77036ms step_avg:50.22ms
step:1535/2160 train_time:77125ms step_avg:50.24ms
step:1536/2160 train_time:77213ms step_avg:50.27ms
step:1537/2160 train_time:77302ms step_avg:50.29ms
step:1538/2160 train_time:77390ms step_avg:50.32ms
step:1539/2160 train_time:77480ms step_avg:50.34ms
step:1540/2160 train_time:77567ms step_avg:50.37ms
step:1541/2160 train_time:77655ms step_avg:50.39ms
step:1542/2160 train_time:77742ms step_avg:50.42ms
step:1543/2160 train_time:77830ms step_avg:50.44ms
step:1544/2160 train_time:77916ms step_avg:50.46ms
step:1545/2160 train_time:78005ms step_avg:50.49ms
step:1546/2160 train_time:78092ms step_avg:50.51ms
step:1547/2160 train_time:78181ms step_avg:50.54ms
step:1548/2160 train_time:78269ms step_avg:50.56ms
step:1549/2160 train_time:78358ms step_avg:50.59ms
step:1550/2160 train_time:78446ms step_avg:50.61ms
step:1551/2160 train_time:78534ms step_avg:50.63ms
step:1552/2160 train_time:78622ms step_avg:50.66ms
step:1553/2160 train_time:78710ms step_avg:50.68ms
step:1554/2160 train_time:78798ms step_avg:50.71ms
step:1555/2160 train_time:78886ms step_avg:50.73ms
step:1556/2160 train_time:78973ms step_avg:50.75ms
step:1557/2160 train_time:79062ms step_avg:50.78ms
step:1558/2160 train_time:79149ms step_avg:50.80ms
step:1559/2160 train_time:79239ms step_avg:50.83ms
step:1560/2160 train_time:79326ms step_avg:50.85ms
step:1561/2160 train_time:79415ms step_avg:50.87ms
step:1562/2160 train_time:79502ms step_avg:50.90ms
step:1563/2160 train_time:79590ms step_avg:50.92ms
step:1564/2160 train_time:79677ms step_avg:50.94ms
step:1565/2160 train_time:79766ms step_avg:50.97ms
step:1566/2160 train_time:79852ms step_avg:50.99ms
step:1567/2160 train_time:79941ms step_avg:51.02ms
step:1568/2160 train_time:80028ms step_avg:51.04ms
step:1569/2160 train_time:80118ms step_avg:51.06ms
step:1570/2160 train_time:80204ms step_avg:51.09ms
step:1571/2160 train_time:80294ms step_avg:51.11ms
step:1572/2160 train_time:80382ms step_avg:51.13ms
step:1573/2160 train_time:80470ms step_avg:51.16ms
step:1574/2160 train_time:80558ms step_avg:51.18ms
step:1575/2160 train_time:80646ms step_avg:51.20ms
step:1576/2160 train_time:80733ms step_avg:51.23ms
step:1577/2160 train_time:80823ms step_avg:51.25ms
step:1578/2160 train_time:80909ms step_avg:51.27ms
step:1579/2160 train_time:80999ms step_avg:51.30ms
step:1580/2160 train_time:81085ms step_avg:51.32ms
step:1581/2160 train_time:81174ms step_avg:51.34ms
step:1582/2160 train_time:81262ms step_avg:51.37ms
step:1583/2160 train_time:81350ms step_avg:51.39ms
step:1584/2160 train_time:81438ms step_avg:51.41ms
step:1585/2160 train_time:81527ms step_avg:51.44ms
step:1586/2160 train_time:81614ms step_avg:51.46ms
step:1587/2160 train_time:81702ms step_avg:51.48ms
step:1588/2160 train_time:81789ms step_avg:51.50ms
step:1589/2160 train_time:81878ms step_avg:51.53ms
step:1590/2160 train_time:81965ms step_avg:51.55ms
step:1591/2160 train_time:82054ms step_avg:51.57ms
step:1592/2160 train_time:82141ms step_avg:51.60ms
step:1593/2160 train_time:82231ms step_avg:51.62ms
step:1594/2160 train_time:82319ms step_avg:51.64ms
step:1595/2160 train_time:82408ms step_avg:51.67ms
step:1596/2160 train_time:82496ms step_avg:51.69ms
step:1597/2160 train_time:82585ms step_avg:51.71ms
step:1598/2160 train_time:82672ms step_avg:51.73ms
step:1599/2160 train_time:82760ms step_avg:51.76ms
step:1600/2160 train_time:82847ms step_avg:51.78ms
step:1601/2160 train_time:82936ms step_avg:51.80ms
step:1602/2160 train_time:83023ms step_avg:51.82ms
step:1603/2160 train_time:83111ms step_avg:51.85ms
step:1604/2160 train_time:83198ms step_avg:51.87ms
step:1605/2160 train_time:83287ms step_avg:51.89ms
step:1606/2160 train_time:83375ms step_avg:51.91ms
step:1607/2160 train_time:83464ms step_avg:51.94ms
step:1608/2160 train_time:83551ms step_avg:51.96ms
step:1609/2160 train_time:83640ms step_avg:51.98ms
step:1610/2160 train_time:83727ms step_avg:52.00ms
step:1611/2160 train_time:83816ms step_avg:52.03ms
step:1612/2160 train_time:83902ms step_avg:52.05ms
step:1613/2160 train_time:83992ms step_avg:52.07ms
step:1614/2160 train_time:84078ms step_avg:52.09ms
step:1615/2160 train_time:84167ms step_avg:52.12ms
step:1616/2160 train_time:84255ms step_avg:52.14ms
step:1617/2160 train_time:84344ms step_avg:52.16ms
step:1618/2160 train_time:84432ms step_avg:52.18ms
step:1619/2160 train_time:84522ms step_avg:52.21ms
step:1620/2160 train_time:84608ms step_avg:52.23ms
step:1621/2160 train_time:84697ms step_avg:52.25ms
step:1622/2160 train_time:84784ms step_avg:52.27ms
step:1623/2160 train_time:84873ms step_avg:52.29ms
step:1624/2160 train_time:84961ms step_avg:52.32ms
step:1625/2160 train_time:85049ms step_avg:52.34ms
step:1626/2160 train_time:85137ms step_avg:52.36ms
step:1627/2160 train_time:85225ms step_avg:52.38ms
step:1628/2160 train_time:85313ms step_avg:52.40ms
step:1629/2160 train_time:85403ms step_avg:52.43ms
step:1630/2160 train_time:85491ms step_avg:52.45ms
step:1631/2160 train_time:85580ms step_avg:52.47ms
step:1632/2160 train_time:85666ms step_avg:52.49ms
step:1633/2160 train_time:85755ms step_avg:52.51ms
step:1634/2160 train_time:85842ms step_avg:52.53ms
step:1635/2160 train_time:85931ms step_avg:52.56ms
step:1636/2160 train_time:86017ms step_avg:52.58ms
step:1637/2160 train_time:86107ms step_avg:52.60ms
step:1638/2160 train_time:86194ms step_avg:52.62ms
step:1639/2160 train_time:86283ms step_avg:52.64ms
step:1640/2160 train_time:86370ms step_avg:52.66ms
step:1641/2160 train_time:86459ms step_avg:52.69ms
step:1642/2160 train_time:86545ms step_avg:52.71ms
step:1643/2160 train_time:86634ms step_avg:52.73ms
step:1644/2160 train_time:86722ms step_avg:52.75ms
step:1645/2160 train_time:86810ms step_avg:52.77ms
step:1646/2160 train_time:86899ms step_avg:52.79ms
step:1647/2160 train_time:86987ms step_avg:52.82ms
step:1648/2160 train_time:87074ms step_avg:52.84ms
step:1649/2160 train_time:87163ms step_avg:52.86ms
step:1650/2160 train_time:87250ms step_avg:52.88ms
step:1651/2160 train_time:87340ms step_avg:52.90ms
step:1652/2160 train_time:87427ms step_avg:52.92ms
step:1653/2160 train_time:87515ms step_avg:52.94ms
step:1654/2160 train_time:87602ms step_avg:52.96ms
step:1655/2160 train_time:87691ms step_avg:52.99ms
step:1656/2160 train_time:87779ms step_avg:53.01ms
step:1657/2160 train_time:87867ms step_avg:53.03ms
step:1658/2160 train_time:87955ms step_avg:53.05ms
step:1659/2160 train_time:88044ms step_avg:53.07ms
step:1660/2160 train_time:88130ms step_avg:53.09ms
step:1661/2160 train_time:88220ms step_avg:53.11ms
step:1662/2160 train_time:88307ms step_avg:53.13ms
step:1663/2160 train_time:88396ms step_avg:53.15ms
step:1664/2160 train_time:88484ms step_avg:53.18ms
step:1665/2160 train_time:88572ms step_avg:53.20ms
step:1666/2160 train_time:88659ms step_avg:53.22ms
step:1667/2160 train_time:88748ms step_avg:53.24ms
step:1668/2160 train_time:88835ms step_avg:53.26ms
step:1669/2160 train_time:88924ms step_avg:53.28ms
step:1670/2160 train_time:89011ms step_avg:53.30ms
step:1671/2160 train_time:89100ms step_avg:53.32ms
step:1672/2160 train_time:89188ms step_avg:53.34ms
step:1673/2160 train_time:89276ms step_avg:53.36ms
step:1674/2160 train_time:89363ms step_avg:53.38ms
step:1675/2160 train_time:89452ms step_avg:53.40ms
step:1676/2160 train_time:89539ms step_avg:53.42ms
step:1677/2160 train_time:89628ms step_avg:53.45ms
step:1678/2160 train_time:89715ms step_avg:53.47ms
step:1679/2160 train_time:89804ms step_avg:53.49ms
step:1680/2160 train_time:89892ms step_avg:53.51ms
step:1681/2160 train_time:89981ms step_avg:53.53ms
step:1682/2160 train_time:90067ms step_avg:53.55ms
step:1683/2160 train_time:90157ms step_avg:53.57ms
step:1684/2160 train_time:90243ms step_avg:53.59ms
step:1685/2160 train_time:90332ms step_avg:53.61ms
step:1686/2160 train_time:90420ms step_avg:53.63ms
step:1687/2160 train_time:90508ms step_avg:53.65ms
step:1688/2160 train_time:90596ms step_avg:53.67ms
step:1689/2160 train_time:90684ms step_avg:53.69ms
step:1690/2160 train_time:90772ms step_avg:53.71ms
step:1691/2160 train_time:90862ms step_avg:53.73ms
step:1692/2160 train_time:90949ms step_avg:53.75ms
step:1693/2160 train_time:91038ms step_avg:53.77ms
step:1694/2160 train_time:91125ms step_avg:53.79ms
step:1695/2160 train_time:91215ms step_avg:53.81ms
step:1696/2160 train_time:91302ms step_avg:53.83ms
step:1697/2160 train_time:91390ms step_avg:53.85ms
step:1698/2160 train_time:91477ms step_avg:53.87ms
step:1699/2160 train_time:91566ms step_avg:53.89ms
step:1700/2160 train_time:91653ms step_avg:53.91ms
step:1701/2160 train_time:91742ms step_avg:53.93ms
step:1702/2160 train_time:91829ms step_avg:53.95ms
step:1703/2160 train_time:91918ms step_avg:53.97ms
step:1704/2160 train_time:92004ms step_avg:53.99ms
step:1705/2160 train_time:92093ms step_avg:54.01ms
step:1706/2160 train_time:92181ms step_avg:54.03ms
step:1707/2160 train_time:92269ms step_avg:54.05ms
step:1708/2160 train_time:92357ms step_avg:54.07ms
step:1709/2160 train_time:92446ms step_avg:54.09ms
step:1710/2160 train_time:92533ms step_avg:54.11ms
step:1711/2160 train_time:92623ms step_avg:54.13ms
step:1712/2160 train_time:92711ms step_avg:54.15ms
step:1713/2160 train_time:92800ms step_avg:54.17ms
step:1714/2160 train_time:92887ms step_avg:54.19ms
step:1715/2160 train_time:92976ms step_avg:54.21ms
step:1716/2160 train_time:93063ms step_avg:54.23ms
step:1717/2160 train_time:93153ms step_avg:54.25ms
step:1718/2160 train_time:93240ms step_avg:54.27ms
step:1719/2160 train_time:93329ms step_avg:54.29ms
step:1720/2160 train_time:93417ms step_avg:54.31ms
step:1721/2160 train_time:93505ms step_avg:54.33ms
step:1722/2160 train_time:93593ms step_avg:54.35ms
step:1723/2160 train_time:93682ms step_avg:54.37ms
step:1724/2160 train_time:93769ms step_avg:54.39ms
step:1725/2160 train_time:93859ms step_avg:54.41ms
step:1726/2160 train_time:93946ms step_avg:54.43ms
step:1727/2160 train_time:94035ms step_avg:54.45ms
step:1728/2160 train_time:94122ms step_avg:54.47ms
step:1729/2160 train_time:94211ms step_avg:54.49ms
step:1730/2160 train_time:94298ms step_avg:54.51ms
step:1731/2160 train_time:94387ms step_avg:54.53ms
step:1732/2160 train_time:94473ms step_avg:54.55ms
step:1733/2160 train_time:94562ms step_avg:54.57ms
step:1734/2160 train_time:94649ms step_avg:54.58ms
step:1735/2160 train_time:94738ms step_avg:54.60ms
step:1736/2160 train_time:94826ms step_avg:54.62ms
step:1737/2160 train_time:94914ms step_avg:54.64ms
step:1738/2160 train_time:95001ms step_avg:54.66ms
step:1739/2160 train_time:95090ms step_avg:54.68ms
step:1740/2160 train_time:95177ms step_avg:54.70ms
step:1741/2160 train_time:95266ms step_avg:54.72ms
step:1742/2160 train_time:95354ms step_avg:54.74ms
step:1743/2160 train_time:95444ms step_avg:54.76ms
step:1744/2160 train_time:95531ms step_avg:54.78ms
step:1745/2160 train_time:95620ms step_avg:54.80ms
step:1746/2160 train_time:95707ms step_avg:54.82ms
step:1747/2160 train_time:95796ms step_avg:54.83ms
step:1748/2160 train_time:95883ms step_avg:54.85ms
step:1749/2160 train_time:95972ms step_avg:54.87ms
step:1750/2160 train_time:96060ms step_avg:54.89ms
step:1750/2160 val_loss:3.3780 train_time:96148ms step_avg:54.94ms
step:1751/2160 train_time:96172ms step_avg:54.92ms
step:1752/2160 train_time:96239ms step_avg:54.93ms
step:1753/2160 train_time:96335ms step_avg:54.95ms
step:1754/2160 train_time:96422ms step_avg:54.97ms
step:1755/2160 train_time:96511ms step_avg:54.99ms
step:1756/2160 train_time:96597ms step_avg:55.01ms
step:1757/2160 train_time:96685ms step_avg:55.03ms
step:1758/2160 train_time:96771ms step_avg:55.05ms
step:1759/2160 train_time:96858ms step_avg:55.06ms
step:1760/2160 train_time:96944ms step_avg:55.08ms
step:1761/2160 train_time:97032ms step_avg:55.10ms
step:1762/2160 train_time:97121ms step_avg:55.12ms
step:1763/2160 train_time:97212ms step_avg:55.14ms
step:1764/2160 train_time:97301ms step_avg:55.16ms
step:1765/2160 train_time:97392ms step_avg:55.18ms
step:1766/2160 train_time:97479ms step_avg:55.20ms
step:1767/2160 train_time:97567ms step_avg:55.22ms
step:1768/2160 train_time:97654ms step_avg:55.23ms
step:1769/2160 train_time:97741ms step_avg:55.25ms
step:1770/2160 train_time:97828ms step_avg:55.27ms
step:1771/2160 train_time:97915ms step_avg:55.29ms
step:1772/2160 train_time:98002ms step_avg:55.31ms
step:1773/2160 train_time:98091ms step_avg:55.32ms
step:1774/2160 train_time:98178ms step_avg:55.34ms
step:1775/2160 train_time:98269ms step_avg:55.36ms
step:1776/2160 train_time:98358ms step_avg:55.38ms
step:1777/2160 train_time:98447ms step_avg:55.40ms
step:1778/2160 train_time:98533ms step_avg:55.42ms
step:1779/2160 train_time:98622ms step_avg:55.44ms
step:1780/2160 train_time:98708ms step_avg:55.45ms
step:1781/2160 train_time:98796ms step_avg:55.47ms
step:1782/2160 train_time:98882ms step_avg:55.49ms
step:1783/2160 train_time:98970ms step_avg:55.51ms
step:1784/2160 train_time:99056ms step_avg:55.52ms
step:1785/2160 train_time:99146ms step_avg:55.54ms
step:1786/2160 train_time:99233ms step_avg:55.56ms
step:1787/2160 train_time:99323ms step_avg:55.58ms
step:1788/2160 train_time:99411ms step_avg:55.60ms
step:1789/2160 train_time:99500ms step_avg:55.62ms
step:1790/2160 train_time:99587ms step_avg:55.63ms
step:1791/2160 train_time:99675ms step_avg:55.65ms
step:1792/2160 train_time:99761ms step_avg:55.67ms
step:1793/2160 train_time:99850ms step_avg:55.69ms
step:1794/2160 train_time:99936ms step_avg:55.71ms
step:1795/2160 train_time:100025ms step_avg:55.72ms
step:1796/2160 train_time:100112ms step_avg:55.74ms
step:1797/2160 train_time:100201ms step_avg:55.76ms
step:1798/2160 train_time:100289ms step_avg:55.78ms
step:1799/2160 train_time:100379ms step_avg:55.80ms
step:1800/2160 train_time:100467ms step_avg:55.81ms
step:1801/2160 train_time:100556ms step_avg:55.83ms
step:1802/2160 train_time:100643ms step_avg:55.85ms
step:1803/2160 train_time:100732ms step_avg:55.87ms
step:1804/2160 train_time:100818ms step_avg:55.89ms
step:1805/2160 train_time:100907ms step_avg:55.90ms
step:1806/2160 train_time:100994ms step_avg:55.92ms
step:1807/2160 train_time:101082ms step_avg:55.94ms
step:1808/2160 train_time:101169ms step_avg:55.96ms
step:1809/2160 train_time:101258ms step_avg:55.97ms
step:1810/2160 train_time:101346ms step_avg:55.99ms
step:1811/2160 train_time:101434ms step_avg:56.01ms
step:1812/2160 train_time:101521ms step_avg:56.03ms
step:1813/2160 train_time:101611ms step_avg:56.05ms
step:1814/2160 train_time:101697ms step_avg:56.06ms
step:1815/2160 train_time:101786ms step_avg:56.08ms
step:1816/2160 train_time:101872ms step_avg:56.10ms
step:1817/2160 train_time:101961ms step_avg:56.11ms
step:1818/2160 train_time:102047ms step_avg:56.13ms
step:1819/2160 train_time:102137ms step_avg:56.15ms
step:1820/2160 train_time:102223ms step_avg:56.17ms
step:1821/2160 train_time:102313ms step_avg:56.19ms
step:1822/2160 train_time:102400ms step_avg:56.20ms
step:1823/2160 train_time:102490ms step_avg:56.22ms
step:1824/2160 train_time:102576ms step_avg:56.24ms
step:1825/2160 train_time:102665ms step_avg:56.26ms
step:1826/2160 train_time:102753ms step_avg:56.27ms
step:1827/2160 train_time:102842ms step_avg:56.29ms
step:1828/2160 train_time:102929ms step_avg:56.31ms
step:1829/2160 train_time:103017ms step_avg:56.32ms
step:1830/2160 train_time:103104ms step_avg:56.34ms
step:1831/2160 train_time:103193ms step_avg:56.36ms
step:1832/2160 train_time:103280ms step_avg:56.38ms
step:1833/2160 train_time:103370ms step_avg:56.39ms
step:1834/2160 train_time:103457ms step_avg:56.41ms
step:1835/2160 train_time:103547ms step_avg:56.43ms
step:1836/2160 train_time:103634ms step_avg:56.45ms
step:1837/2160 train_time:103723ms step_avg:56.46ms
step:1838/2160 train_time:103810ms step_avg:56.48ms
step:1839/2160 train_time:103898ms step_avg:56.50ms
step:1840/2160 train_time:103985ms step_avg:56.51ms
step:1841/2160 train_time:104074ms step_avg:56.53ms
step:1842/2160 train_time:104161ms step_avg:56.55ms
step:1843/2160 train_time:104250ms step_avg:56.57ms
step:1844/2160 train_time:104337ms step_avg:56.58ms
step:1845/2160 train_time:104426ms step_avg:56.60ms
step:1846/2160 train_time:104513ms step_avg:56.62ms
step:1847/2160 train_time:104602ms step_avg:56.63ms
step:1848/2160 train_time:104690ms step_avg:56.65ms
step:1849/2160 train_time:104779ms step_avg:56.67ms
step:1850/2160 train_time:104867ms step_avg:56.68ms
step:1851/2160 train_time:104956ms step_avg:56.70ms
step:1852/2160 train_time:105043ms step_avg:56.72ms
step:1853/2160 train_time:105131ms step_avg:56.74ms
step:1854/2160 train_time:105217ms step_avg:56.75ms
step:1855/2160 train_time:105307ms step_avg:56.77ms
step:1856/2160 train_time:105395ms step_avg:56.79ms
step:1857/2160 train_time:105484ms step_avg:56.80ms
step:1858/2160 train_time:105572ms step_avg:56.82ms
step:1859/2160 train_time:105661ms step_avg:56.84ms
step:1860/2160 train_time:105748ms step_avg:56.85ms
step:1861/2160 train_time:105838ms step_avg:56.87ms
step:1862/2160 train_time:105925ms step_avg:56.89ms
step:1863/2160 train_time:106014ms step_avg:56.90ms
step:1864/2160 train_time:106100ms step_avg:56.92ms
step:1865/2160 train_time:106190ms step_avg:56.94ms
step:1866/2160 train_time:106277ms step_avg:56.95ms
step:1867/2160 train_time:106366ms step_avg:56.97ms
step:1868/2160 train_time:106453ms step_avg:56.99ms
step:1869/2160 train_time:106542ms step_avg:57.00ms
step:1870/2160 train_time:106629ms step_avg:57.02ms
step:1871/2160 train_time:106718ms step_avg:57.04ms
step:1872/2160 train_time:106805ms step_avg:57.05ms
step:1873/2160 train_time:106894ms step_avg:57.07ms
step:1874/2160 train_time:106981ms step_avg:57.09ms
step:1875/2160 train_time:107070ms step_avg:57.10ms
step:1876/2160 train_time:107157ms step_avg:57.12ms
step:1877/2160 train_time:107245ms step_avg:57.14ms
step:1878/2160 train_time:107333ms step_avg:57.15ms
step:1879/2160 train_time:107421ms step_avg:57.17ms
step:1880/2160 train_time:107509ms step_avg:57.19ms
step:1881/2160 train_time:107598ms step_avg:57.20ms
step:1882/2160 train_time:107685ms step_avg:57.22ms
step:1883/2160 train_time:107774ms step_avg:57.24ms
step:1884/2160 train_time:107861ms step_avg:57.25ms
step:1885/2160 train_time:107950ms step_avg:57.27ms
step:1886/2160 train_time:108036ms step_avg:57.28ms
step:1887/2160 train_time:108126ms step_avg:57.30ms
step:1888/2160 train_time:108212ms step_avg:57.32ms
step:1889/2160 train_time:108301ms step_avg:57.33ms
step:1890/2160 train_time:108388ms step_avg:57.35ms
step:1891/2160 train_time:108477ms step_avg:57.37ms
step:1892/2160 train_time:108565ms step_avg:57.38ms
step:1893/2160 train_time:108654ms step_avg:57.40ms
step:1894/2160 train_time:108741ms step_avg:57.41ms
step:1895/2160 train_time:108831ms step_avg:57.43ms
step:1896/2160 train_time:108917ms step_avg:57.45ms
step:1897/2160 train_time:109007ms step_avg:57.46ms
step:1898/2160 train_time:109094ms step_avg:57.48ms
step:1899/2160 train_time:109182ms step_avg:57.49ms
step:1900/2160 train_time:109269ms step_avg:57.51ms
step:1901/2160 train_time:109358ms step_avg:57.53ms
step:1902/2160 train_time:109445ms step_avg:57.54ms
step:1903/2160 train_time:109534ms step_avg:57.56ms
step:1904/2160 train_time:109621ms step_avg:57.57ms
step:1905/2160 train_time:109709ms step_avg:57.59ms
step:1906/2160 train_time:109796ms step_avg:57.61ms
step:1907/2160 train_time:109885ms step_avg:57.62ms
step:1908/2160 train_time:109972ms step_avg:57.64ms
step:1909/2160 train_time:110061ms step_avg:57.65ms
step:1910/2160 train_time:110149ms step_avg:57.67ms
step:1911/2160 train_time:110237ms step_avg:57.69ms
step:1912/2160 train_time:110325ms step_avg:57.70ms
step:1913/2160 train_time:110414ms step_avg:57.72ms
step:1914/2160 train_time:110500ms step_avg:57.73ms
step:1915/2160 train_time:110589ms step_avg:57.75ms
step:1916/2160 train_time:110676ms step_avg:57.76ms
step:1917/2160 train_time:110766ms step_avg:57.78ms
step:1918/2160 train_time:110853ms step_avg:57.80ms
step:1919/2160 train_time:110942ms step_avg:57.81ms
step:1920/2160 train_time:111029ms step_avg:57.83ms
step:1921/2160 train_time:111118ms step_avg:57.84ms
step:1922/2160 train_time:111205ms step_avg:57.86ms
step:1923/2160 train_time:111294ms step_avg:57.88ms
step:1924/2160 train_time:111381ms step_avg:57.89ms
step:1925/2160 train_time:111470ms step_avg:57.91ms
step:1926/2160 train_time:111558ms step_avg:57.92ms
step:1927/2160 train_time:111648ms step_avg:57.94ms
step:1928/2160 train_time:111735ms step_avg:57.95ms
step:1929/2160 train_time:111824ms step_avg:57.97ms
step:1930/2160 train_time:111910ms step_avg:57.98ms
step:1931/2160 train_time:111999ms step_avg:58.00ms
step:1932/2160 train_time:112086ms step_avg:58.02ms
step:1933/2160 train_time:112175ms step_avg:58.03ms
step:1934/2160 train_time:112261ms step_avg:58.05ms
step:1935/2160 train_time:112350ms step_avg:58.06ms
step:1936/2160 train_time:112437ms step_avg:58.08ms
step:1937/2160 train_time:112526ms step_avg:58.09ms
step:1938/2160 train_time:112613ms step_avg:58.11ms
step:1939/2160 train_time:112703ms step_avg:58.12ms
step:1940/2160 train_time:112791ms step_avg:58.14ms
step:1941/2160 train_time:112879ms step_avg:58.16ms
step:1942/2160 train_time:112967ms step_avg:58.17ms
step:1943/2160 train_time:113057ms step_avg:58.19ms
step:1944/2160 train_time:113144ms step_avg:58.20ms
step:1945/2160 train_time:113233ms step_avg:58.22ms
step:1946/2160 train_time:113319ms step_avg:58.23ms
step:1947/2160 train_time:113408ms step_avg:58.25ms
step:1948/2160 train_time:113494ms step_avg:58.26ms
step:1949/2160 train_time:113583ms step_avg:58.28ms
step:1950/2160 train_time:113671ms step_avg:58.29ms
step:1951/2160 train_time:113760ms step_avg:58.31ms
step:1952/2160 train_time:113847ms step_avg:58.32ms
step:1953/2160 train_time:113935ms step_avg:58.34ms
step:1954/2160 train_time:114022ms step_avg:58.35ms
step:1955/2160 train_time:114111ms step_avg:58.37ms
step:1956/2160 train_time:114198ms step_avg:58.38ms
step:1957/2160 train_time:114287ms step_avg:58.40ms
step:1958/2160 train_time:114374ms step_avg:58.41ms
step:1959/2160 train_time:114463ms step_avg:58.43ms
step:1960/2160 train_time:114550ms step_avg:58.44ms
step:1961/2160 train_time:114638ms step_avg:58.46ms
step:1962/2160 train_time:114725ms step_avg:58.47ms
step:1963/2160 train_time:114815ms step_avg:58.49ms
step:1964/2160 train_time:114902ms step_avg:58.50ms
step:1965/2160 train_time:114991ms step_avg:58.52ms
step:1966/2160 train_time:115078ms step_avg:58.53ms
step:1967/2160 train_time:115167ms step_avg:58.55ms
step:1968/2160 train_time:115255ms step_avg:58.56ms
step:1969/2160 train_time:115344ms step_avg:58.58ms
step:1970/2160 train_time:115431ms step_avg:58.59ms
step:1971/2160 train_time:115519ms step_avg:58.61ms
step:1972/2160 train_time:115606ms step_avg:58.62ms
step:1973/2160 train_time:115697ms step_avg:58.64ms
step:1974/2160 train_time:115783ms step_avg:58.65ms
step:1975/2160 train_time:115873ms step_avg:58.67ms
step:1976/2160 train_time:115960ms step_avg:58.68ms
step:1977/2160 train_time:116049ms step_avg:58.70ms
step:1978/2160 train_time:116135ms step_avg:58.71ms
step:1979/2160 train_time:116224ms step_avg:58.73ms
step:1980/2160 train_time:116313ms step_avg:58.74ms
step:1981/2160 train_time:116402ms step_avg:58.76ms
step:1982/2160 train_time:116489ms step_avg:58.77ms
step:1983/2160 train_time:116577ms step_avg:58.79ms
step:1984/2160 train_time:116665ms step_avg:58.80ms
step:1985/2160 train_time:116754ms step_avg:58.82ms
step:1986/2160 train_time:116841ms step_avg:58.83ms
step:1987/2160 train_time:116930ms step_avg:58.85ms
step:1988/2160 train_time:117017ms step_avg:58.86ms
step:1989/2160 train_time:117106ms step_avg:58.88ms
step:1990/2160 train_time:117194ms step_avg:58.89ms
step:1991/2160 train_time:117283ms step_avg:58.91ms
step:1992/2160 train_time:117371ms step_avg:58.92ms
step:1993/2160 train_time:117460ms step_avg:58.94ms
step:1994/2160 train_time:117548ms step_avg:58.95ms
step:1995/2160 train_time:117636ms step_avg:58.97ms
step:1996/2160 train_time:117724ms step_avg:58.98ms
step:1997/2160 train_time:117814ms step_avg:59.00ms
step:1998/2160 train_time:117900ms step_avg:59.01ms
step:1999/2160 train_time:117989ms step_avg:59.02ms
step:2000/2160 train_time:118075ms step_avg:59.04ms
step:2000/2160 val_loss:3.3087 train_time:118164ms step_avg:59.08ms
step:2001/2160 train_time:118188ms step_avg:59.06ms
step:2002/2160 train_time:118254ms step_avg:59.07ms
step:2003/2160 train_time:118348ms step_avg:59.09ms
step:2004/2160 train_time:118437ms step_avg:59.10ms
step:2005/2160 train_time:118526ms step_avg:59.12ms
step:2006/2160 train_time:118612ms step_avg:59.13ms
step:2007/2160 train_time:118700ms step_avg:59.14ms
step:2008/2160 train_time:118786ms step_avg:59.16ms
step:2009/2160 train_time:118873ms step_avg:59.17ms
step:2010/2160 train_time:118960ms step_avg:59.18ms
step:2011/2160 train_time:119047ms step_avg:59.20ms
step:2012/2160 train_time:119135ms step_avg:59.21ms
step:2013/2160 train_time:119227ms step_avg:59.23ms
step:2014/2160 train_time:119317ms step_avg:59.24ms
step:2015/2160 train_time:119407ms step_avg:59.26ms
step:2016/2160 train_time:119495ms step_avg:59.27ms
step:2017/2160 train_time:119584ms step_avg:59.29ms
step:2018/2160 train_time:119670ms step_avg:59.30ms
step:2019/2160 train_time:119758ms step_avg:59.32ms
step:2020/2160 train_time:119844ms step_avg:59.33ms
step:2021/2160 train_time:119932ms step_avg:59.34ms
step:2022/2160 train_time:120019ms step_avg:59.36ms
step:2023/2160 train_time:120107ms step_avg:59.37ms
step:2024/2160 train_time:120197ms step_avg:59.39ms
step:2025/2160 train_time:120288ms step_avg:59.40ms
step:2026/2160 train_time:120376ms step_avg:59.42ms
step:2027/2160 train_time:120466ms step_avg:59.43ms
step:2028/2160 train_time:120553ms step_avg:59.44ms
step:2029/2160 train_time:120642ms step_avg:59.46ms
step:2030/2160 train_time:120729ms step_avg:59.47ms
step:2031/2160 train_time:120817ms step_avg:59.49ms
step:2032/2160 train_time:120903ms step_avg:59.50ms
step:2033/2160 train_time:120991ms step_avg:59.51ms
step:2034/2160 train_time:121078ms step_avg:59.53ms
step:2035/2160 train_time:121167ms step_avg:59.54ms
step:2036/2160 train_time:121256ms step_avg:59.56ms
step:2037/2160 train_time:121345ms step_avg:59.57ms
step:2038/2160 train_time:121432ms step_avg:59.58ms
step:2039/2160 train_time:121522ms step_avg:59.60ms
step:2040/2160 train_time:121608ms step_avg:59.61ms
step:2041/2160 train_time:121697ms step_avg:59.63ms
step:2042/2160 train_time:121784ms step_avg:59.64ms
step:2043/2160 train_time:121872ms step_avg:59.65ms
step:2044/2160 train_time:121959ms step_avg:59.67ms
step:2045/2160 train_time:122047ms step_avg:59.68ms
step:2046/2160 train_time:122134ms step_avg:59.69ms
step:2047/2160 train_time:122224ms step_avg:59.71ms
step:2048/2160 train_time:122311ms step_avg:59.72ms
step:2049/2160 train_time:122400ms step_avg:59.74ms
step:2050/2160 train_time:122487ms step_avg:59.75ms
step:2051/2160 train_time:122576ms step_avg:59.76ms
step:2052/2160 train_time:122663ms step_avg:59.78ms
step:2053/2160 train_time:122752ms step_avg:59.79ms
step:2054/2160 train_time:122839ms step_avg:59.80ms
step:2055/2160 train_time:122927ms step_avg:59.82ms
step:2056/2160 train_time:123014ms step_avg:59.83ms
step:2057/2160 train_time:123104ms step_avg:59.85ms
step:2058/2160 train_time:123192ms step_avg:59.86ms
step:2059/2160 train_time:123280ms step_avg:59.87ms
step:2060/2160 train_time:123368ms step_avg:59.89ms
step:2061/2160 train_time:123457ms step_avg:59.90ms
step:2062/2160 train_time:123544ms step_avg:59.91ms
step:2063/2160 train_time:123632ms step_avg:59.93ms
step:2064/2160 train_time:123720ms step_avg:59.94ms
step:2065/2160 train_time:123809ms step_avg:59.96ms
step:2066/2160 train_time:123897ms step_avg:59.97ms
step:2067/2160 train_time:123986ms step_avg:59.98ms
step:2068/2160 train_time:124072ms step_avg:60.00ms
step:2069/2160 train_time:124162ms step_avg:60.01ms
step:2070/2160 train_time:124250ms step_avg:60.02ms
step:2071/2160 train_time:124339ms step_avg:60.04ms
step:2072/2160 train_time:124426ms step_avg:60.05ms
step:2073/2160 train_time:124516ms step_avg:60.07ms
step:2074/2160 train_time:124603ms step_avg:60.08ms
step:2075/2160 train_time:124692ms step_avg:60.09ms
step:2076/2160 train_time:124778ms step_avg:60.11ms
step:2077/2160 train_time:124867ms step_avg:60.12ms
step:2078/2160 train_time:124954ms step_avg:60.13ms
step:2079/2160 train_time:125043ms step_avg:60.15ms
step:2080/2160 train_time:125129ms step_avg:60.16ms
step:2081/2160 train_time:125218ms step_avg:60.17ms
step:2082/2160 train_time:125306ms step_avg:60.19ms
step:2083/2160 train_time:125396ms step_avg:60.20ms
step:2084/2160 train_time:125483ms step_avg:60.21ms
step:2085/2160 train_time:125572ms step_avg:60.23ms
step:2086/2160 train_time:125659ms step_avg:60.24ms
step:2087/2160 train_time:125747ms step_avg:60.25ms
step:2088/2160 train_time:125834ms step_avg:60.27ms
step:2089/2160 train_time:125923ms step_avg:60.28ms
step:2090/2160 train_time:126010ms step_avg:60.29ms
step:2091/2160 train_time:126099ms step_avg:60.31ms
step:2092/2160 train_time:126185ms step_avg:60.32ms
step:2093/2160 train_time:126275ms step_avg:60.33ms
step:2094/2160 train_time:126363ms step_avg:60.35ms
step:2095/2160 train_time:126451ms step_avg:60.36ms
step:2096/2160 train_time:126539ms step_avg:60.37ms
step:2097/2160 train_time:126627ms step_avg:60.38ms
step:2098/2160 train_time:126715ms step_avg:60.40ms
step:2099/2160 train_time:126803ms step_avg:60.41ms
step:2100/2160 train_time:126890ms step_avg:60.42ms
step:2101/2160 train_time:126978ms step_avg:60.44ms
step:2102/2160 train_time:127066ms step_avg:60.45ms
step:2103/2160 train_time:127154ms step_avg:60.46ms
step:2104/2160 train_time:127242ms step_avg:60.48ms
step:2105/2160 train_time:127330ms step_avg:60.49ms
step:2106/2160 train_time:127418ms step_avg:60.50ms
step:2107/2160 train_time:127508ms step_avg:60.52ms
step:2108/2160 train_time:127595ms step_avg:60.53ms
step:2109/2160 train_time:127683ms step_avg:60.54ms
step:2110/2160 train_time:127770ms step_avg:60.55ms
step:2111/2160 train_time:127860ms step_avg:60.57ms
step:2112/2160 train_time:127947ms step_avg:60.58ms
step:2113/2160 train_time:128036ms step_avg:60.59ms
step:2114/2160 train_time:128123ms step_avg:60.61ms
step:2115/2160 train_time:128212ms step_avg:60.62ms
step:2116/2160 train_time:128299ms step_avg:60.63ms
step:2117/2160 train_time:128388ms step_avg:60.65ms
step:2118/2160 train_time:128475ms step_avg:60.66ms
step:2119/2160 train_time:128565ms step_avg:60.67ms
step:2120/2160 train_time:128651ms step_avg:60.68ms
step:2121/2160 train_time:128740ms step_avg:60.70ms
step:2122/2160 train_time:128827ms step_avg:60.71ms
step:2123/2160 train_time:128917ms step_avg:60.72ms
step:2124/2160 train_time:129004ms step_avg:60.74ms
step:2125/2160 train_time:129093ms step_avg:60.75ms
step:2126/2160 train_time:129181ms step_avg:60.76ms
step:2127/2160 train_time:129270ms step_avg:60.78ms
step:2128/2160 train_time:129358ms step_avg:60.79ms
step:2129/2160 train_time:129448ms step_avg:60.80ms
step:2130/2160 train_time:129535ms step_avg:60.81ms
step:2131/2160 train_time:129624ms step_avg:60.83ms
step:2132/2160 train_time:129711ms step_avg:60.84ms
step:2133/2160 train_time:129801ms step_avg:60.85ms
step:2134/2160 train_time:129888ms step_avg:60.87ms
step:2135/2160 train_time:129977ms step_avg:60.88ms
step:2136/2160 train_time:130064ms step_avg:60.89ms
step:2137/2160 train_time:130153ms step_avg:60.90ms
step:2138/2160 train_time:130241ms step_avg:60.92ms
step:2139/2160 train_time:130330ms step_avg:60.93ms
step:2140/2160 train_time:130418ms step_avg:60.94ms
step:2141/2160 train_time:130507ms step_avg:60.96ms
step:2142/2160 train_time:130595ms step_avg:60.97ms
step:2143/2160 train_time:130684ms step_avg:60.98ms
step:2144/2160 train_time:130771ms step_avg:60.99ms
step:2145/2160 train_time:130861ms step_avg:61.01ms
step:2146/2160 train_time:130948ms step_avg:61.02ms
step:2147/2160 train_time:131037ms step_avg:61.03ms
step:2148/2160 train_time:131125ms step_avg:61.05ms
step:2149/2160 train_time:131214ms step_avg:61.06ms
step:2150/2160 train_time:131302ms step_avg:61.07ms
step:2151/2160 train_time:131392ms step_avg:61.08ms
step:2152/2160 train_time:131479ms step_avg:61.10ms
step:2153/2160 train_time:131568ms step_avg:61.11ms
step:2154/2160 train_time:131655ms step_avg:61.12ms
step:2155/2160 train_time:131744ms step_avg:61.13ms
step:2156/2160 train_time:131832ms step_avg:61.15ms
step:2157/2160 train_time:131921ms step_avg:61.16ms
step:2158/2160 train_time:132008ms step_avg:61.17ms
step:2159/2160 train_time:132098ms step_avg:61.18ms
step:2160/2160 train_time:132186ms step_avg:61.20ms
step:2160/2160 val_loss:3.2765 train_time:132275ms step_avg:61.24ms
peak memory allocated: 29707 MiB reserved: 44776 MiB

import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = (16 / 8) * 0.8
    if x > 0.66:
        lr_max = (24 / 8) * 0.8
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sun Nov 30 02:00:09 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:92ms step_avg:91.59ms
step:2/2160 train_time:130ms step_avg:64.97ms
step:3/2160 train_time:150ms step_avg:49.84ms
step:4/2160 train_time:170ms step_avg:42.53ms
step:5/2160 train_time:203ms step_avg:40.63ms
step:6/2160 train_time:286ms step_avg:47.59ms
step:7/2160 train_time:304ms step_avg:43.43ms
step:8/2160 train_time:337ms step_avg:42.09ms
step:9/2160 train_time:370ms step_avg:41.16ms
step:10/2160 train_time:403ms step_avg:40.34ms
step:11/2160 train_time:437ms step_avg:39.77ms
step:12/2160 train_time:470ms step_avg:39.19ms
step:13/2160 train_time:504ms step_avg:38.80ms
step:14/2160 train_time:537ms step_avg:38.38ms
step:15/2160 train_time:572ms step_avg:38.12ms
step:16/2160 train_time:605ms step_avg:37.82ms
step:17/2160 train_time:639ms step_avg:37.60ms
step:18/2160 train_time:672ms step_avg:37.34ms
step:19/2160 train_time:706ms step_avg:37.17ms
step:20/2160 train_time:739ms step_avg:36.95ms
step:21/2160 train_time:773ms step_avg:36.83ms
step:22/2160 train_time:806ms step_avg:36.66ms
step:23/2160 train_time:840ms step_avg:36.54ms
step:24/2160 train_time:873ms step_avg:36.39ms
step:25/2160 train_time:907ms step_avg:36.30ms
step:26/2160 train_time:940ms step_avg:36.16ms
step:27/2160 train_time:975ms step_avg:36.09ms
step:28/2160 train_time:1008ms step_avg:35.99ms
step:29/2160 train_time:1042ms step_avg:35.92ms
step:30/2160 train_time:1075ms step_avg:35.82ms
step:31/2160 train_time:1109ms step_avg:35.76ms
step:32/2160 train_time:1142ms step_avg:35.68ms
step:33/2160 train_time:1176ms step_avg:35.63ms
step:34/2160 train_time:1209ms step_avg:35.56ms
step:35/2160 train_time:1243ms step_avg:35.52ms
step:36/2160 train_time:1277ms step_avg:35.46ms
step:37/2160 train_time:1311ms step_avg:35.44ms
step:38/2160 train_time:1345ms step_avg:35.38ms
step:39/2160 train_time:1379ms step_avg:35.35ms
step:40/2160 train_time:1412ms step_avg:35.29ms
step:41/2160 train_time:1446ms step_avg:35.27ms
step:42/2160 train_time:1479ms step_avg:35.21ms
step:43/2160 train_time:1514ms step_avg:35.20ms
step:44/2160 train_time:1547ms step_avg:35.16ms
step:45/2160 train_time:1581ms step_avg:35.13ms
step:46/2160 train_time:1614ms step_avg:35.09ms
step:47/2160 train_time:1648ms step_avg:35.06ms
step:48/2160 train_time:1681ms step_avg:35.02ms
step:49/2160 train_time:1715ms step_avg:35.01ms
step:50/2160 train_time:1748ms step_avg:34.97ms
step:51/2160 train_time:1782ms step_avg:34.95ms
step:52/2160 train_time:1815ms step_avg:34.91ms
step:53/2160 train_time:1849ms step_avg:34.90ms
step:54/2160 train_time:1883ms step_avg:34.86ms
step:55/2160 train_time:1917ms step_avg:34.85ms
step:56/2160 train_time:1950ms step_avg:34.81ms
step:57/2160 train_time:1984ms step_avg:34.80ms
step:58/2160 train_time:2016ms step_avg:34.77ms
step:59/2160 train_time:2051ms step_avg:34.76ms
step:60/2160 train_time:2084ms step_avg:34.73ms
step:61/2160 train_time:2119ms step_avg:34.73ms
step:62/2160 train_time:2151ms step_avg:34.70ms
step:63/2160 train_time:2186ms step_avg:34.70ms
step:64/2160 train_time:2219ms step_avg:34.67ms
step:65/2160 train_time:2253ms step_avg:34.66ms
step:66/2160 train_time:2286ms step_avg:34.64ms
step:67/2160 train_time:2320ms step_avg:34.63ms
step:68/2160 train_time:2353ms step_avg:34.61ms
step:69/2160 train_time:2387ms step_avg:34.60ms
step:70/2160 train_time:2420ms step_avg:34.58ms
step:71/2160 train_time:2455ms step_avg:34.58ms
step:72/2160 train_time:2488ms step_avg:34.55ms
step:73/2160 train_time:2522ms step_avg:34.55ms
step:74/2160 train_time:2555ms step_avg:34.53ms
step:75/2160 train_time:2589ms step_avg:34.52ms
step:76/2160 train_time:2622ms step_avg:34.50ms
step:77/2160 train_time:2657ms step_avg:34.50ms
step:78/2160 train_time:2690ms step_avg:34.48ms
step:79/2160 train_time:2724ms step_avg:34.48ms
step:80/2160 train_time:2756ms step_avg:34.45ms
step:81/2160 train_time:2791ms step_avg:34.46ms
step:82/2160 train_time:2824ms step_avg:34.44ms
step:83/2160 train_time:2859ms step_avg:34.44ms
step:84/2160 train_time:2892ms step_avg:34.43ms
step:85/2160 train_time:2926ms step_avg:34.42ms
step:86/2160 train_time:2959ms step_avg:34.40ms
step:87/2160 train_time:2993ms step_avg:34.40ms
step:88/2160 train_time:3026ms step_avg:34.38ms
step:89/2160 train_time:3060ms step_avg:34.38ms
step:90/2160 train_time:3093ms step_avg:34.37ms
step:91/2160 train_time:3127ms step_avg:34.37ms
step:92/2160 train_time:3160ms step_avg:34.35ms
step:93/2160 train_time:3195ms step_avg:34.35ms
step:94/2160 train_time:3228ms step_avg:34.34ms
step:95/2160 train_time:3262ms step_avg:34.34ms
step:96/2160 train_time:3295ms step_avg:34.32ms
step:97/2160 train_time:3329ms step_avg:34.32ms
step:98/2160 train_time:3363ms step_avg:34.31ms
step:99/2160 train_time:3397ms step_avg:34.31ms
step:100/2160 train_time:3430ms step_avg:34.30ms
step:101/2160 train_time:3464ms step_avg:34.30ms
step:102/2160 train_time:3497ms step_avg:34.28ms
step:103/2160 train_time:3531ms step_avg:34.28ms
step:104/2160 train_time:3564ms step_avg:34.27ms
step:105/2160 train_time:3598ms step_avg:34.27ms
step:106/2160 train_time:3631ms step_avg:34.26ms
step:107/2160 train_time:3666ms step_avg:34.26ms
step:108/2160 train_time:3698ms step_avg:34.24ms
step:109/2160 train_time:3733ms step_avg:34.25ms
step:110/2160 train_time:3766ms step_avg:34.23ms
step:111/2160 train_time:3800ms step_avg:34.24ms
step:112/2160 train_time:3833ms step_avg:34.22ms
step:113/2160 train_time:3867ms step_avg:34.22ms
step:114/2160 train_time:3900ms step_avg:34.21ms
step:115/2160 train_time:3934ms step_avg:34.21ms
step:116/2160 train_time:3967ms step_avg:34.20ms
step:117/2160 train_time:4001ms step_avg:34.20ms
step:118/2160 train_time:4034ms step_avg:34.19ms
step:119/2160 train_time:4068ms step_avg:34.19ms
step:120/2160 train_time:4101ms step_avg:34.17ms
step:121/2160 train_time:4135ms step_avg:34.17ms
step:122/2160 train_time:4168ms step_avg:34.16ms
step:123/2160 train_time:4202ms step_avg:34.16ms
step:124/2160 train_time:4235ms step_avg:34.15ms
step:125/2160 train_time:4269ms step_avg:34.15ms
step:126/2160 train_time:4302ms step_avg:34.14ms
step:127/2160 train_time:4336ms step_avg:34.14ms
step:128/2160 train_time:4369ms step_avg:34.14ms
step:129/2160 train_time:4403ms step_avg:34.13ms
step:130/2160 train_time:4436ms step_avg:34.12ms
step:131/2160 train_time:4470ms step_avg:34.12ms
step:132/2160 train_time:4503ms step_avg:34.12ms
step:133/2160 train_time:4538ms step_avg:34.12ms
step:134/2160 train_time:4570ms step_avg:34.11ms
step:135/2160 train_time:4604ms step_avg:34.11ms
step:136/2160 train_time:4637ms step_avg:34.10ms
step:137/2160 train_time:4671ms step_avg:34.10ms
step:138/2160 train_time:4704ms step_avg:34.09ms
step:139/2160 train_time:4739ms step_avg:34.09ms
step:140/2160 train_time:4771ms step_avg:34.08ms
step:141/2160 train_time:4806ms step_avg:34.08ms
step:142/2160 train_time:4839ms step_avg:34.07ms
step:143/2160 train_time:4873ms step_avg:34.08ms
step:144/2160 train_time:4906ms step_avg:34.07ms
step:145/2160 train_time:4940ms step_avg:34.07ms
step:146/2160 train_time:4973ms step_avg:34.06ms
step:147/2160 train_time:5008ms step_avg:34.07ms
step:148/2160 train_time:5040ms step_avg:34.06ms
step:149/2160 train_time:5074ms step_avg:34.06ms
step:150/2160 train_time:5107ms step_avg:34.05ms
step:151/2160 train_time:5141ms step_avg:34.05ms
step:152/2160 train_time:5174ms step_avg:34.04ms
step:153/2160 train_time:5209ms step_avg:34.04ms
step:154/2160 train_time:5241ms step_avg:34.04ms
step:155/2160 train_time:5276ms step_avg:34.04ms
step:156/2160 train_time:5309ms step_avg:34.03ms
step:157/2160 train_time:5343ms step_avg:34.03ms
step:158/2160 train_time:5376ms step_avg:34.03ms
step:159/2160 train_time:5410ms step_avg:34.03ms
step:160/2160 train_time:5443ms step_avg:34.02ms
step:161/2160 train_time:5477ms step_avg:34.02ms
step:162/2160 train_time:5510ms step_avg:34.01ms
step:163/2160 train_time:5544ms step_avg:34.01ms
step:164/2160 train_time:5577ms step_avg:34.00ms
step:165/2160 train_time:5611ms step_avg:34.01ms
step:166/2160 train_time:5644ms step_avg:34.00ms
step:167/2160 train_time:5678ms step_avg:34.00ms
step:168/2160 train_time:5711ms step_avg:33.99ms
step:169/2160 train_time:5745ms step_avg:33.99ms
step:170/2160 train_time:5778ms step_avg:33.99ms
step:171/2160 train_time:5812ms step_avg:33.99ms
step:172/2160 train_time:5845ms step_avg:33.98ms
step:173/2160 train_time:5880ms step_avg:33.99ms
step:174/2160 train_time:5912ms step_avg:33.98ms
step:175/2160 train_time:5946ms step_avg:33.98ms
step:176/2160 train_time:5979ms step_avg:33.97ms
step:177/2160 train_time:6014ms step_avg:33.98ms
step:178/2160 train_time:6047ms step_avg:33.97ms
step:179/2160 train_time:6081ms step_avg:33.97ms
step:180/2160 train_time:6114ms step_avg:33.97ms
step:181/2160 train_time:6148ms step_avg:33.96ms
step:182/2160 train_time:6180ms step_avg:33.96ms
step:183/2160 train_time:6214ms step_avg:33.96ms
step:184/2160 train_time:6247ms step_avg:33.95ms
step:185/2160 train_time:6281ms step_avg:33.95ms
step:186/2160 train_time:6314ms step_avg:33.95ms
step:187/2160 train_time:6348ms step_avg:33.95ms
step:188/2160 train_time:6381ms step_avg:33.94ms
step:189/2160 train_time:6415ms step_avg:33.94ms
step:190/2160 train_time:6448ms step_avg:33.94ms
step:191/2160 train_time:6482ms step_avg:33.94ms
step:192/2160 train_time:6515ms step_avg:33.93ms
step:193/2160 train_time:6549ms step_avg:33.93ms
step:194/2160 train_time:6582ms step_avg:33.93ms
step:195/2160 train_time:6616ms step_avg:33.93ms
step:196/2160 train_time:6649ms step_avg:33.93ms
step:197/2160 train_time:6683ms step_avg:33.93ms
step:198/2160 train_time:6716ms step_avg:33.92ms
step:199/2160 train_time:6750ms step_avg:33.92ms
step:200/2160 train_time:6783ms step_avg:33.92ms
step:201/2160 train_time:6818ms step_avg:33.92ms
step:202/2160 train_time:6850ms step_avg:33.91ms
step:203/2160 train_time:6885ms step_avg:33.91ms
step:204/2160 train_time:6917ms step_avg:33.91ms
step:205/2160 train_time:6952ms step_avg:33.91ms
step:206/2160 train_time:6985ms step_avg:33.91ms
step:207/2160 train_time:7019ms step_avg:33.91ms
step:208/2160 train_time:7052ms step_avg:33.90ms
step:209/2160 train_time:7086ms step_avg:33.91ms
step:210/2160 train_time:7119ms step_avg:33.90ms
step:211/2160 train_time:7153ms step_avg:33.90ms
step:212/2160 train_time:7186ms step_avg:33.90ms
step:213/2160 train_time:7220ms step_avg:33.90ms
step:214/2160 train_time:7253ms step_avg:33.89ms
step:215/2160 train_time:7287ms step_avg:33.89ms
step:216/2160 train_time:7319ms step_avg:33.89ms
step:217/2160 train_time:7354ms step_avg:33.89ms
step:218/2160 train_time:7386ms step_avg:33.88ms
step:219/2160 train_time:7420ms step_avg:33.88ms
step:220/2160 train_time:7453ms step_avg:33.88ms
step:221/2160 train_time:7487ms step_avg:33.88ms
step:222/2160 train_time:7520ms step_avg:33.87ms
step:223/2160 train_time:7554ms step_avg:33.88ms
step:224/2160 train_time:7587ms step_avg:33.87ms
step:225/2160 train_time:7621ms step_avg:33.87ms
step:226/2160 train_time:7655ms step_avg:33.87ms
step:227/2160 train_time:7689ms step_avg:33.87ms
step:228/2160 train_time:7722ms step_avg:33.87ms
step:229/2160 train_time:7756ms step_avg:33.87ms
step:230/2160 train_time:7789ms step_avg:33.86ms
step:231/2160 train_time:7823ms step_avg:33.86ms
step:232/2160 train_time:7855ms step_avg:33.86ms
step:233/2160 train_time:7890ms step_avg:33.86ms
step:234/2160 train_time:7923ms step_avg:33.86ms
step:235/2160 train_time:7957ms step_avg:33.86ms
step:236/2160 train_time:7990ms step_avg:33.86ms
step:237/2160 train_time:8024ms step_avg:33.86ms
step:238/2160 train_time:8058ms step_avg:33.86ms
step:239/2160 train_time:8091ms step_avg:33.86ms
step:240/2160 train_time:8125ms step_avg:33.85ms
step:241/2160 train_time:8159ms step_avg:33.85ms
step:242/2160 train_time:8192ms step_avg:33.85ms
step:243/2160 train_time:8226ms step_avg:33.85ms
step:244/2160 train_time:8259ms step_avg:33.85ms
step:245/2160 train_time:8293ms step_avg:33.85ms
step:246/2160 train_time:8326ms step_avg:33.85ms
step:247/2160 train_time:8360ms step_avg:33.85ms
step:248/2160 train_time:8393ms step_avg:33.84ms
step:249/2160 train_time:8427ms step_avg:33.84ms
step:250/2160 train_time:8460ms step_avg:33.84ms
step:250/2160 val_loss:4.3118 train_time:8496ms step_avg:33.98ms
step:251/2160 train_time:8516ms step_avg:33.93ms
step:252/2160 train_time:8534ms step_avg:33.87ms
step:253/2160 train_time:8565ms step_avg:33.86ms
step:254/2160 train_time:8599ms step_avg:33.86ms
step:255/2160 train_time:8636ms step_avg:33.87ms
step:256/2160 train_time:8671ms step_avg:33.87ms
step:257/2160 train_time:8706ms step_avg:33.87ms
step:258/2160 train_time:8739ms step_avg:33.87ms
step:259/2160 train_time:8773ms step_avg:33.87ms
step:260/2160 train_time:8806ms step_avg:33.87ms
step:261/2160 train_time:8840ms step_avg:33.87ms
step:262/2160 train_time:8873ms step_avg:33.87ms
step:263/2160 train_time:8907ms step_avg:33.87ms
step:264/2160 train_time:8940ms step_avg:33.86ms
step:265/2160 train_time:8974ms step_avg:33.86ms
step:266/2160 train_time:9006ms step_avg:33.86ms
step:267/2160 train_time:9040ms step_avg:33.86ms
step:268/2160 train_time:9073ms step_avg:33.85ms
step:269/2160 train_time:9107ms step_avg:33.86ms
step:270/2160 train_time:9140ms step_avg:33.85ms
step:271/2160 train_time:9174ms step_avg:33.85ms
step:272/2160 train_time:9206ms step_avg:33.85ms
step:273/2160 train_time:9240ms step_avg:33.85ms
step:274/2160 train_time:9273ms step_avg:33.84ms
step:275/2160 train_time:9307ms step_avg:33.84ms
step:276/2160 train_time:9340ms step_avg:33.84ms
step:277/2160 train_time:9374ms step_avg:33.84ms
step:278/2160 train_time:9406ms step_avg:33.84ms
step:279/2160 train_time:9440ms step_avg:33.84ms
step:280/2160 train_time:9473ms step_avg:33.83ms
step:281/2160 train_time:9507ms step_avg:33.83ms
step:282/2160 train_time:9540ms step_avg:33.83ms
step:283/2160 train_time:9574ms step_avg:33.83ms
step:284/2160 train_time:9608ms step_avg:33.83ms
step:285/2160 train_time:9642ms step_avg:33.83ms
step:286/2160 train_time:9675ms step_avg:33.83ms
step:287/2160 train_time:9710ms step_avg:33.83ms
step:288/2160 train_time:9743ms step_avg:33.83ms
step:289/2160 train_time:9778ms step_avg:33.83ms
step:290/2160 train_time:9811ms step_avg:33.83ms
step:291/2160 train_time:9845ms step_avg:33.83ms
step:292/2160 train_time:9878ms step_avg:33.83ms
step:293/2160 train_time:9913ms step_avg:33.83ms
step:294/2160 train_time:9945ms step_avg:33.83ms
step:295/2160 train_time:9980ms step_avg:33.83ms
step:296/2160 train_time:10012ms step_avg:33.82ms
step:297/2160 train_time:10046ms step_avg:33.83ms
step:298/2160 train_time:10079ms step_avg:33.82ms
step:299/2160 train_time:10113ms step_avg:33.82ms
step:300/2160 train_time:10146ms step_avg:33.82ms
step:301/2160 train_time:10180ms step_avg:33.82ms
step:302/2160 train_time:10214ms step_avg:33.82ms
step:303/2160 train_time:10247ms step_avg:33.82ms
step:304/2160 train_time:10280ms step_avg:33.82ms
step:305/2160 train_time:10314ms step_avg:33.82ms
step:306/2160 train_time:10347ms step_avg:33.81ms
step:307/2160 train_time:10382ms step_avg:33.82ms
step:308/2160 train_time:10415ms step_avg:33.81ms
step:309/2160 train_time:10448ms step_avg:33.81ms
step:310/2160 train_time:10481ms step_avg:33.81ms
step:311/2160 train_time:10516ms step_avg:33.81ms
step:312/2160 train_time:10548ms step_avg:33.81ms
step:313/2160 train_time:10583ms step_avg:33.81ms
step:314/2160 train_time:10616ms step_avg:33.81ms
step:315/2160 train_time:10650ms step_avg:33.81ms
step:316/2160 train_time:10683ms step_avg:33.81ms
step:317/2160 train_time:10717ms step_avg:33.81ms
step:318/2160 train_time:10750ms step_avg:33.81ms
step:319/2160 train_time:10784ms step_avg:33.81ms
step:320/2160 train_time:10817ms step_avg:33.80ms
step:321/2160 train_time:10851ms step_avg:33.80ms
step:322/2160 train_time:10884ms step_avg:33.80ms
step:323/2160 train_time:10919ms step_avg:33.80ms
step:324/2160 train_time:10952ms step_avg:33.80ms
step:325/2160 train_time:10986ms step_avg:33.80ms
step:326/2160 train_time:11019ms step_avg:33.80ms
step:327/2160 train_time:11053ms step_avg:33.80ms
step:328/2160 train_time:11086ms step_avg:33.80ms
step:329/2160 train_time:11120ms step_avg:33.80ms
step:330/2160 train_time:11153ms step_avg:33.80ms
step:331/2160 train_time:11187ms step_avg:33.80ms
step:332/2160 train_time:11220ms step_avg:33.80ms
step:333/2160 train_time:11254ms step_avg:33.80ms
step:334/2160 train_time:11287ms step_avg:33.79ms
step:335/2160 train_time:11321ms step_avg:33.79ms
step:336/2160 train_time:11354ms step_avg:33.79ms
step:337/2160 train_time:11388ms step_avg:33.79ms
step:338/2160 train_time:11421ms step_avg:33.79ms
step:339/2160 train_time:11455ms step_avg:33.79ms
step:340/2160 train_time:11488ms step_avg:33.79ms
step:341/2160 train_time:11522ms step_avg:33.79ms
step:342/2160 train_time:11555ms step_avg:33.79ms
step:343/2160 train_time:11589ms step_avg:33.79ms
step:344/2160 train_time:11622ms step_avg:33.79ms
step:345/2160 train_time:11656ms step_avg:33.79ms
step:346/2160 train_time:11689ms step_avg:33.78ms
step:347/2160 train_time:11724ms step_avg:33.79ms
step:348/2160 train_time:11757ms step_avg:33.78ms
step:349/2160 train_time:11791ms step_avg:33.78ms
step:350/2160 train_time:11824ms step_avg:33.78ms
step:351/2160 train_time:11858ms step_avg:33.78ms
step:352/2160 train_time:11891ms step_avg:33.78ms
step:353/2160 train_time:11925ms step_avg:33.78ms
step:354/2160 train_time:11958ms step_avg:33.78ms
step:355/2160 train_time:11993ms step_avg:33.78ms
step:356/2160 train_time:12025ms step_avg:33.78ms
step:357/2160 train_time:12060ms step_avg:33.78ms
step:358/2160 train_time:12093ms step_avg:33.78ms
step:359/2160 train_time:12127ms step_avg:33.78ms
step:360/2160 train_time:12160ms step_avg:33.78ms
step:361/2160 train_time:12194ms step_avg:33.78ms
step:362/2160 train_time:12227ms step_avg:33.78ms
step:363/2160 train_time:12261ms step_avg:33.78ms
step:364/2160 train_time:12294ms step_avg:33.78ms
step:365/2160 train_time:12328ms step_avg:33.78ms
step:366/2160 train_time:12361ms step_avg:33.77ms
step:367/2160 train_time:12395ms step_avg:33.77ms
step:368/2160 train_time:12428ms step_avg:33.77ms
step:369/2160 train_time:12462ms step_avg:33.77ms
step:370/2160 train_time:12495ms step_avg:33.77ms
step:371/2160 train_time:12529ms step_avg:33.77ms
step:372/2160 train_time:12562ms step_avg:33.77ms
step:373/2160 train_time:12596ms step_avg:33.77ms
step:374/2160 train_time:12629ms step_avg:33.77ms
step:375/2160 train_time:12663ms step_avg:33.77ms
step:376/2160 train_time:12696ms step_avg:33.77ms
step:377/2160 train_time:12730ms step_avg:33.77ms
step:378/2160 train_time:12763ms step_avg:33.76ms
step:379/2160 train_time:12797ms step_avg:33.77ms
step:380/2160 train_time:12830ms step_avg:33.76ms
step:381/2160 train_time:12865ms step_avg:33.77ms
step:382/2160 train_time:12898ms step_avg:33.76ms
step:383/2160 train_time:12932ms step_avg:33.76ms
step:384/2160 train_time:12964ms step_avg:33.76ms
step:385/2160 train_time:12999ms step_avg:33.76ms
step:386/2160 train_time:13031ms step_avg:33.76ms
step:387/2160 train_time:13066ms step_avg:33.76ms
step:388/2160 train_time:13099ms step_avg:33.76ms
step:389/2160 train_time:13133ms step_avg:33.76ms
step:390/2160 train_time:13165ms step_avg:33.76ms
step:391/2160 train_time:13200ms step_avg:33.76ms
step:392/2160 train_time:13233ms step_avg:33.76ms
step:393/2160 train_time:13266ms step_avg:33.76ms
step:394/2160 train_time:13300ms step_avg:33.76ms
step:395/2160 train_time:13333ms step_avg:33.76ms
step:396/2160 train_time:13367ms step_avg:33.75ms
step:397/2160 train_time:13401ms step_avg:33.75ms
step:398/2160 train_time:13434ms step_avg:33.75ms
step:399/2160 train_time:13468ms step_avg:33.75ms
step:400/2160 train_time:13501ms step_avg:33.75ms
step:401/2160 train_time:13535ms step_avg:33.75ms
step:402/2160 train_time:13568ms step_avg:33.75ms
step:403/2160 train_time:13602ms step_avg:33.75ms
step:404/2160 train_time:13635ms step_avg:33.75ms
step:405/2160 train_time:13669ms step_avg:33.75ms
step:406/2160 train_time:13702ms step_avg:33.75ms
step:407/2160 train_time:13736ms step_avg:33.75ms
step:408/2160 train_time:13768ms step_avg:33.75ms
step:409/2160 train_time:13803ms step_avg:33.75ms
step:410/2160 train_time:13836ms step_avg:33.75ms
step:411/2160 train_time:13870ms step_avg:33.75ms
step:412/2160 train_time:13903ms step_avg:33.75ms
step:413/2160 train_time:13937ms step_avg:33.75ms
step:414/2160 train_time:13970ms step_avg:33.74ms
step:415/2160 train_time:14004ms step_avg:33.75ms
step:416/2160 train_time:14037ms step_avg:33.74ms
step:417/2160 train_time:14071ms step_avg:33.74ms
step:418/2160 train_time:14104ms step_avg:33.74ms
step:419/2160 train_time:14139ms step_avg:33.74ms
step:420/2160 train_time:14172ms step_avg:33.74ms
step:421/2160 train_time:14206ms step_avg:33.74ms
step:422/2160 train_time:14239ms step_avg:33.74ms
step:423/2160 train_time:14273ms step_avg:33.74ms
step:424/2160 train_time:14306ms step_avg:33.74ms
step:425/2160 train_time:14340ms step_avg:33.74ms
step:426/2160 train_time:14373ms step_avg:33.74ms
step:427/2160 train_time:14407ms step_avg:33.74ms
step:428/2160 train_time:14440ms step_avg:33.74ms
step:429/2160 train_time:14474ms step_avg:33.74ms
step:430/2160 train_time:14507ms step_avg:33.74ms
step:431/2160 train_time:14541ms step_avg:33.74ms
step:432/2160 train_time:14574ms step_avg:33.74ms
step:433/2160 train_time:14609ms step_avg:33.74ms
step:434/2160 train_time:14642ms step_avg:33.74ms
step:435/2160 train_time:14676ms step_avg:33.74ms
step:436/2160 train_time:14709ms step_avg:33.74ms
step:437/2160 train_time:14743ms step_avg:33.74ms
step:438/2160 train_time:14776ms step_avg:33.74ms
step:439/2160 train_time:14810ms step_avg:33.74ms
step:440/2160 train_time:14843ms step_avg:33.73ms
step:441/2160 train_time:14877ms step_avg:33.73ms
step:442/2160 train_time:14910ms step_avg:33.73ms
step:443/2160 train_time:14944ms step_avg:33.73ms
step:444/2160 train_time:14977ms step_avg:33.73ms
step:445/2160 train_time:15011ms step_avg:33.73ms
step:446/2160 train_time:15044ms step_avg:33.73ms
step:447/2160 train_time:15078ms step_avg:33.73ms
step:448/2160 train_time:15111ms step_avg:33.73ms
step:449/2160 train_time:15145ms step_avg:33.73ms
step:450/2160 train_time:15178ms step_avg:33.73ms
step:451/2160 train_time:15212ms step_avg:33.73ms
step:452/2160 train_time:15245ms step_avg:33.73ms
step:453/2160 train_time:15280ms step_avg:33.73ms
step:454/2160 train_time:15313ms step_avg:33.73ms
step:455/2160 train_time:15347ms step_avg:33.73ms
step:456/2160 train_time:15380ms step_avg:33.73ms
step:457/2160 train_time:15414ms step_avg:33.73ms
step:458/2160 train_time:15447ms step_avg:33.73ms
step:459/2160 train_time:15481ms step_avg:33.73ms
step:460/2160 train_time:15514ms step_avg:33.73ms
step:461/2160 train_time:15548ms step_avg:33.73ms
step:462/2160 train_time:15581ms step_avg:33.72ms
step:463/2160 train_time:15615ms step_avg:33.73ms
step:464/2160 train_time:15648ms step_avg:33.72ms
step:465/2160 train_time:15682ms step_avg:33.73ms
step:466/2160 train_time:15715ms step_avg:33.72ms
step:467/2160 train_time:15749ms step_avg:33.72ms
step:468/2160 train_time:15782ms step_avg:33.72ms
step:469/2160 train_time:15816ms step_avg:33.72ms
step:470/2160 train_time:15850ms step_avg:33.72ms
step:471/2160 train_time:15884ms step_avg:33.72ms
step:472/2160 train_time:15917ms step_avg:33.72ms
step:473/2160 train_time:15951ms step_avg:33.72ms
step:474/2160 train_time:15984ms step_avg:33.72ms
step:475/2160 train_time:16018ms step_avg:33.72ms
step:476/2160 train_time:16051ms step_avg:33.72ms
step:477/2160 train_time:16085ms step_avg:33.72ms
step:478/2160 train_time:16118ms step_avg:33.72ms
step:479/2160 train_time:16152ms step_avg:33.72ms
step:480/2160 train_time:16184ms step_avg:33.72ms
step:481/2160 train_time:16219ms step_avg:33.72ms
step:482/2160 train_time:16252ms step_avg:33.72ms
step:483/2160 train_time:16286ms step_avg:33.72ms
step:484/2160 train_time:16319ms step_avg:33.72ms
step:485/2160 train_time:16353ms step_avg:33.72ms
step:486/2160 train_time:16386ms step_avg:33.72ms
step:487/2160 train_time:16420ms step_avg:33.72ms
step:488/2160 train_time:16453ms step_avg:33.71ms
step:489/2160 train_time:16487ms step_avg:33.72ms
step:490/2160 train_time:16520ms step_avg:33.72ms
step:491/2160 train_time:16554ms step_avg:33.72ms
step:492/2160 train_time:16587ms step_avg:33.71ms
step:493/2160 train_time:16621ms step_avg:33.71ms
step:494/2160 train_time:16654ms step_avg:33.71ms
step:495/2160 train_time:16688ms step_avg:33.71ms
step:496/2160 train_time:16721ms step_avg:33.71ms
step:497/2160 train_time:16755ms step_avg:33.71ms
step:498/2160 train_time:16788ms step_avg:33.71ms
step:499/2160 train_time:16822ms step_avg:33.71ms
step:500/2160 train_time:16856ms step_avg:33.71ms
step:500/2160 val_loss:4.0150 train_time:16891ms step_avg:33.78ms
step:501/2160 train_time:16910ms step_avg:33.75ms
step:502/2160 train_time:16928ms step_avg:33.72ms
step:503/2160 train_time:16963ms step_avg:33.72ms
step:504/2160 train_time:16996ms step_avg:33.72ms
step:505/2160 train_time:17031ms step_avg:33.73ms
step:506/2160 train_time:17064ms step_avg:33.72ms
step:507/2160 train_time:17099ms step_avg:33.73ms
step:508/2160 train_time:17132ms step_avg:33.72ms
step:509/2160 train_time:17166ms step_avg:33.72ms
step:510/2160 train_time:17199ms step_avg:33.72ms
step:511/2160 train_time:17233ms step_avg:33.72ms
step:512/2160 train_time:17266ms step_avg:33.72ms
step:513/2160 train_time:17299ms step_avg:33.72ms
step:514/2160 train_time:17332ms step_avg:33.72ms
step:515/2160 train_time:17367ms step_avg:33.72ms
step:516/2160 train_time:17399ms step_avg:33.72ms
step:517/2160 train_time:17433ms step_avg:33.72ms
step:518/2160 train_time:17466ms step_avg:33.72ms
step:519/2160 train_time:17500ms step_avg:33.72ms
step:520/2160 train_time:17533ms step_avg:33.72ms
step:521/2160 train_time:17567ms step_avg:33.72ms
step:522/2160 train_time:17600ms step_avg:33.72ms
step:523/2160 train_time:17634ms step_avg:33.72ms
step:524/2160 train_time:17667ms step_avg:33.72ms
step:525/2160 train_time:17701ms step_avg:33.72ms
step:526/2160 train_time:17734ms step_avg:33.71ms
step:527/2160 train_time:17768ms step_avg:33.71ms
step:528/2160 train_time:17801ms step_avg:33.71ms
step:529/2160 train_time:17835ms step_avg:33.71ms
step:530/2160 train_time:17868ms step_avg:33.71ms
step:531/2160 train_time:17903ms step_avg:33.72ms
step:532/2160 train_time:17936ms step_avg:33.71ms
step:533/2160 train_time:17971ms step_avg:33.72ms
step:534/2160 train_time:18004ms step_avg:33.72ms
step:535/2160 train_time:18038ms step_avg:33.72ms
step:536/2160 train_time:18071ms step_avg:33.71ms
step:537/2160 train_time:18105ms step_avg:33.72ms
step:538/2160 train_time:18138ms step_avg:33.71ms
step:539/2160 train_time:18173ms step_avg:33.72ms
step:540/2160 train_time:18205ms step_avg:33.71ms
step:541/2160 train_time:18239ms step_avg:33.71ms
step:542/2160 train_time:18272ms step_avg:33.71ms
step:543/2160 train_time:18306ms step_avg:33.71ms
step:544/2160 train_time:18339ms step_avg:33.71ms
step:545/2160 train_time:18374ms step_avg:33.71ms
step:546/2160 train_time:18406ms step_avg:33.71ms
step:547/2160 train_time:18440ms step_avg:33.71ms
step:548/2160 train_time:18473ms step_avg:33.71ms
step:549/2160 train_time:18507ms step_avg:33.71ms
step:550/2160 train_time:18540ms step_avg:33.71ms
step:551/2160 train_time:18574ms step_avg:33.71ms
step:552/2160 train_time:18607ms step_avg:33.71ms
step:553/2160 train_time:18641ms step_avg:33.71ms
step:554/2160 train_time:18674ms step_avg:33.71ms
step:555/2160 train_time:18708ms step_avg:33.71ms
step:556/2160 train_time:18741ms step_avg:33.71ms
step:557/2160 train_time:18775ms step_avg:33.71ms
step:558/2160 train_time:18808ms step_avg:33.71ms
step:559/2160 train_time:18842ms step_avg:33.71ms
step:560/2160 train_time:18875ms step_avg:33.71ms
step:561/2160 train_time:18910ms step_avg:33.71ms
step:562/2160 train_time:18943ms step_avg:33.71ms
step:563/2160 train_time:18977ms step_avg:33.71ms
step:564/2160 train_time:19011ms step_avg:33.71ms
step:565/2160 train_time:19045ms step_avg:33.71ms
step:566/2160 train_time:19078ms step_avg:33.71ms
step:567/2160 train_time:19113ms step_avg:33.71ms
step:568/2160 train_time:19146ms step_avg:33.71ms
step:569/2160 train_time:19180ms step_avg:33.71ms
step:570/2160 train_time:19213ms step_avg:33.71ms
step:571/2160 train_time:19247ms step_avg:33.71ms
step:572/2160 train_time:19281ms step_avg:33.71ms
step:573/2160 train_time:19314ms step_avg:33.71ms
step:574/2160 train_time:19348ms step_avg:33.71ms
step:575/2160 train_time:19382ms step_avg:33.71ms
step:576/2160 train_time:19415ms step_avg:33.71ms
step:577/2160 train_time:19449ms step_avg:33.71ms
step:578/2160 train_time:19482ms step_avg:33.71ms
step:579/2160 train_time:19516ms step_avg:33.71ms
step:580/2160 train_time:19549ms step_avg:33.70ms
step:581/2160 train_time:19583ms step_avg:33.71ms
step:582/2160 train_time:19615ms step_avg:33.70ms
step:583/2160 train_time:19650ms step_avg:33.70ms
step:584/2160 train_time:19683ms step_avg:33.70ms
step:585/2160 train_time:19717ms step_avg:33.70ms
step:586/2160 train_time:19750ms step_avg:33.70ms
step:587/2160 train_time:19784ms step_avg:33.70ms
step:588/2160 train_time:19817ms step_avg:33.70ms
step:589/2160 train_time:19851ms step_avg:33.70ms
step:590/2160 train_time:19884ms step_avg:33.70ms
step:591/2160 train_time:19918ms step_avg:33.70ms
step:592/2160 train_time:19951ms step_avg:33.70ms
step:593/2160 train_time:19986ms step_avg:33.70ms
step:594/2160 train_time:20019ms step_avg:33.70ms
step:595/2160 train_time:20053ms step_avg:33.70ms
step:596/2160 train_time:20086ms step_avg:33.70ms
step:597/2160 train_time:20120ms step_avg:33.70ms
step:598/2160 train_time:20153ms step_avg:33.70ms
step:599/2160 train_time:20187ms step_avg:33.70ms
step:600/2160 train_time:20220ms step_avg:33.70ms
step:601/2160 train_time:20254ms step_avg:33.70ms
step:602/2160 train_time:20288ms step_avg:33.70ms
step:603/2160 train_time:20322ms step_avg:33.70ms
step:604/2160 train_time:20355ms step_avg:33.70ms
step:605/2160 train_time:20389ms step_avg:33.70ms
step:606/2160 train_time:20422ms step_avg:33.70ms
step:607/2160 train_time:20456ms step_avg:33.70ms
step:608/2160 train_time:20489ms step_avg:33.70ms
step:609/2160 train_time:20523ms step_avg:33.70ms
step:610/2160 train_time:20556ms step_avg:33.70ms
step:611/2160 train_time:20590ms step_avg:33.70ms
step:612/2160 train_time:20623ms step_avg:33.70ms
step:613/2160 train_time:20657ms step_avg:33.70ms
step:614/2160 train_time:20691ms step_avg:33.70ms
step:615/2160 train_time:20725ms step_avg:33.70ms
step:616/2160 train_time:20758ms step_avg:33.70ms
step:617/2160 train_time:20792ms step_avg:33.70ms
step:618/2160 train_time:20825ms step_avg:33.70ms
step:619/2160 train_time:20859ms step_avg:33.70ms
step:620/2160 train_time:20892ms step_avg:33.70ms
step:621/2160 train_time:20927ms step_avg:33.70ms
step:622/2160 train_time:20960ms step_avg:33.70ms
step:623/2160 train_time:20994ms step_avg:33.70ms
step:624/2160 train_time:21027ms step_avg:33.70ms
step:625/2160 train_time:21061ms step_avg:33.70ms
step:626/2160 train_time:21094ms step_avg:33.70ms
step:627/2160 train_time:21128ms step_avg:33.70ms
step:628/2160 train_time:21162ms step_avg:33.70ms
step:629/2160 train_time:21196ms step_avg:33.70ms
step:630/2160 train_time:21228ms step_avg:33.70ms
step:631/2160 train_time:21262ms step_avg:33.70ms
step:632/2160 train_time:21295ms step_avg:33.69ms
step:633/2160 train_time:21330ms step_avg:33.70ms
step:634/2160 train_time:21362ms step_avg:33.69ms
step:635/2160 train_time:21397ms step_avg:33.70ms
step:636/2160 train_time:21430ms step_avg:33.69ms
step:637/2160 train_time:21464ms step_avg:33.69ms
step:638/2160 train_time:21497ms step_avg:33.69ms
step:639/2160 train_time:21531ms step_avg:33.69ms
step:640/2160 train_time:21564ms step_avg:33.69ms
step:641/2160 train_time:21598ms step_avg:33.69ms
step:642/2160 train_time:21631ms step_avg:33.69ms
step:643/2160 train_time:21666ms step_avg:33.69ms
step:644/2160 train_time:21699ms step_avg:33.69ms
step:645/2160 train_time:21733ms step_avg:33.69ms
step:646/2160 train_time:21766ms step_avg:33.69ms
step:647/2160 train_time:21800ms step_avg:33.69ms
step:648/2160 train_time:21833ms step_avg:33.69ms
step:649/2160 train_time:21867ms step_avg:33.69ms
step:650/2160 train_time:21900ms step_avg:33.69ms
step:651/2160 train_time:21935ms step_avg:33.69ms
step:652/2160 train_time:21968ms step_avg:33.69ms
step:653/2160 train_time:22002ms step_avg:33.69ms
step:654/2160 train_time:22035ms step_avg:33.69ms
step:655/2160 train_time:22069ms step_avg:33.69ms
step:656/2160 train_time:22103ms step_avg:33.69ms
step:657/2160 train_time:22137ms step_avg:33.69ms
step:658/2160 train_time:22170ms step_avg:33.69ms
step:659/2160 train_time:22204ms step_avg:33.69ms
step:660/2160 train_time:22237ms step_avg:33.69ms
step:661/2160 train_time:22272ms step_avg:33.69ms
step:662/2160 train_time:22305ms step_avg:33.69ms
step:663/2160 train_time:22339ms step_avg:33.69ms
step:664/2160 train_time:22372ms step_avg:33.69ms
step:665/2160 train_time:22407ms step_avg:33.69ms
step:666/2160 train_time:22440ms step_avg:33.69ms
step:667/2160 train_time:22474ms step_avg:33.69ms
step:668/2160 train_time:22507ms step_avg:33.69ms
step:669/2160 train_time:22541ms step_avg:33.69ms
step:670/2160 train_time:22573ms step_avg:33.69ms
step:671/2160 train_time:22608ms step_avg:33.69ms
step:672/2160 train_time:22641ms step_avg:33.69ms
step:673/2160 train_time:22675ms step_avg:33.69ms
step:674/2160 train_time:22708ms step_avg:33.69ms
step:675/2160 train_time:22742ms step_avg:33.69ms
step:676/2160 train_time:22775ms step_avg:33.69ms
step:677/2160 train_time:22809ms step_avg:33.69ms
step:678/2160 train_time:22843ms step_avg:33.69ms
step:679/2160 train_time:22877ms step_avg:33.69ms
step:680/2160 train_time:22910ms step_avg:33.69ms
step:681/2160 train_time:22944ms step_avg:33.69ms
step:682/2160 train_time:22977ms step_avg:33.69ms
step:683/2160 train_time:23011ms step_avg:33.69ms
step:684/2160 train_time:23044ms step_avg:33.69ms
step:685/2160 train_time:23078ms step_avg:33.69ms
step:686/2160 train_time:23111ms step_avg:33.69ms
step:687/2160 train_time:23146ms step_avg:33.69ms
step:688/2160 train_time:23178ms step_avg:33.69ms
step:689/2160 train_time:23213ms step_avg:33.69ms
step:690/2160 train_time:23246ms step_avg:33.69ms
step:691/2160 train_time:23280ms step_avg:33.69ms
step:692/2160 train_time:23313ms step_avg:33.69ms
step:693/2160 train_time:23348ms step_avg:33.69ms
step:694/2160 train_time:23381ms step_avg:33.69ms
step:695/2160 train_time:23415ms step_avg:33.69ms
step:696/2160 train_time:23448ms step_avg:33.69ms
step:697/2160 train_time:23482ms step_avg:33.69ms
step:698/2160 train_time:23515ms step_avg:33.69ms
step:699/2160 train_time:23550ms step_avg:33.69ms
step:700/2160 train_time:23583ms step_avg:33.69ms
step:701/2160 train_time:23617ms step_avg:33.69ms
step:702/2160 train_time:23650ms step_avg:33.69ms
step:703/2160 train_time:23685ms step_avg:33.69ms
step:704/2160 train_time:23718ms step_avg:33.69ms
step:705/2160 train_time:23752ms step_avg:33.69ms
step:706/2160 train_time:23785ms step_avg:33.69ms
step:707/2160 train_time:23819ms step_avg:33.69ms
step:708/2160 train_time:23853ms step_avg:33.69ms
step:709/2160 train_time:23913ms step_avg:33.73ms
step:710/2160 train_time:23973ms step_avg:33.76ms
step:711/2160 train_time:24034ms step_avg:33.80ms
step:712/2160 train_time:24094ms step_avg:33.84ms
step:713/2160 train_time:24156ms step_avg:33.88ms
step:714/2160 train_time:24215ms step_avg:33.91ms
step:715/2160 train_time:24277ms step_avg:33.95ms
step:716/2160 train_time:24337ms step_avg:33.99ms
step:717/2160 train_time:24399ms step_avg:34.03ms
step:718/2160 train_time:24459ms step_avg:34.07ms
step:719/2160 train_time:24521ms step_avg:34.10ms
step:720/2160 train_time:24580ms step_avg:34.14ms
step:721/2160 train_time:24642ms step_avg:34.18ms
step:722/2160 train_time:24702ms step_avg:34.21ms
step:723/2160 train_time:24763ms step_avg:34.25ms
step:724/2160 train_time:24823ms step_avg:34.29ms
step:725/2160 train_time:24884ms step_avg:34.32ms
step:726/2160 train_time:24943ms step_avg:34.36ms
step:727/2160 train_time:25005ms step_avg:34.39ms
step:728/2160 train_time:25064ms step_avg:34.43ms
step:729/2160 train_time:25126ms step_avg:34.47ms
step:730/2160 train_time:25185ms step_avg:34.50ms
step:731/2160 train_time:25247ms step_avg:34.54ms
step:732/2160 train_time:25308ms step_avg:34.57ms
step:733/2160 train_time:25369ms step_avg:34.61ms
step:734/2160 train_time:25429ms step_avg:34.64ms
step:735/2160 train_time:25491ms step_avg:34.68ms
step:736/2160 train_time:25550ms step_avg:34.71ms
step:737/2160 train_time:25611ms step_avg:34.75ms
step:738/2160 train_time:25671ms step_avg:34.78ms
step:739/2160 train_time:25732ms step_avg:34.82ms
step:740/2160 train_time:25791ms step_avg:34.85ms
step:741/2160 train_time:25853ms step_avg:34.89ms
step:742/2160 train_time:25912ms step_avg:34.92ms
step:743/2160 train_time:25974ms step_avg:34.96ms
step:744/2160 train_time:26034ms step_avg:34.99ms
step:745/2160 train_time:26095ms step_avg:35.03ms
step:746/2160 train_time:26155ms step_avg:35.06ms
step:747/2160 train_time:26217ms step_avg:35.10ms
step:748/2160 train_time:26277ms step_avg:35.13ms
step:749/2160 train_time:26339ms step_avg:35.17ms
step:750/2160 train_time:26398ms step_avg:35.20ms
step:750/2160 val_loss:3.8672 train_time:26462ms step_avg:35.28ms
step:751/2160 train_time:26483ms step_avg:35.26ms
step:752/2160 train_time:26523ms step_avg:35.27ms
step:753/2160 train_time:26583ms step_avg:35.30ms
step:754/2160 train_time:26642ms step_avg:35.33ms
step:755/2160 train_time:26703ms step_avg:35.37ms
step:756/2160 train_time:26763ms step_avg:35.40ms
step:757/2160 train_time:26823ms step_avg:35.43ms
step:758/2160 train_time:26882ms step_avg:35.46ms
step:759/2160 train_time:26943ms step_avg:35.50ms
step:760/2160 train_time:27001ms step_avg:35.53ms
step:761/2160 train_time:27062ms step_avg:35.56ms
step:762/2160 train_time:27121ms step_avg:35.59ms
step:763/2160 train_time:27181ms step_avg:35.62ms
step:764/2160 train_time:27240ms step_avg:35.65ms
step:765/2160 train_time:27300ms step_avg:35.69ms
step:766/2160 train_time:27365ms step_avg:35.72ms
step:767/2160 train_time:27433ms step_avg:35.77ms
step:768/2160 train_time:27494ms step_avg:35.80ms
step:769/2160 train_time:27555ms step_avg:35.83ms
step:770/2160 train_time:27615ms step_avg:35.86ms
step:771/2160 train_time:27676ms step_avg:35.90ms
step:772/2160 train_time:27736ms step_avg:35.93ms
step:773/2160 train_time:27796ms step_avg:35.96ms
step:774/2160 train_time:27855ms step_avg:35.99ms
step:775/2160 train_time:27916ms step_avg:36.02ms
step:776/2160 train_time:27974ms step_avg:36.05ms
step:777/2160 train_time:28035ms step_avg:36.08ms
step:778/2160 train_time:28094ms step_avg:36.11ms
step:779/2160 train_time:28154ms step_avg:36.14ms
step:780/2160 train_time:28214ms step_avg:36.17ms
step:781/2160 train_time:28275ms step_avg:36.20ms
step:782/2160 train_time:28337ms step_avg:36.24ms
step:783/2160 train_time:28400ms step_avg:36.27ms
step:784/2160 train_time:28462ms step_avg:36.30ms
step:785/2160 train_time:28525ms step_avg:36.34ms
step:786/2160 train_time:28585ms step_avg:36.37ms
step:787/2160 train_time:28646ms step_avg:36.40ms
step:788/2160 train_time:28705ms step_avg:36.43ms
step:789/2160 train_time:28766ms step_avg:36.46ms
step:790/2160 train_time:28825ms step_avg:36.49ms
step:791/2160 train_time:28886ms step_avg:36.52ms
step:792/2160 train_time:28946ms step_avg:36.55ms
step:793/2160 train_time:29007ms step_avg:36.58ms
step:794/2160 train_time:29066ms step_avg:36.61ms
step:795/2160 train_time:29127ms step_avg:36.64ms
step:796/2160 train_time:29187ms step_avg:36.67ms
step:797/2160 train_time:29249ms step_avg:36.70ms
step:798/2160 train_time:29310ms step_avg:36.73ms
step:799/2160 train_time:29372ms step_avg:36.76ms
step:800/2160 train_time:29432ms step_avg:36.79ms
step:801/2160 train_time:29494ms step_avg:36.82ms
step:802/2160 train_time:29553ms step_avg:36.85ms
step:803/2160 train_time:29615ms step_avg:36.88ms
step:804/2160 train_time:29674ms step_avg:36.91ms
step:805/2160 train_time:29736ms step_avg:36.94ms
step:806/2160 train_time:29795ms step_avg:36.97ms
step:807/2160 train_time:29856ms step_avg:37.00ms
step:808/2160 train_time:29915ms step_avg:37.02ms
step:809/2160 train_time:29976ms step_avg:37.05ms
step:810/2160 train_time:30035ms step_avg:37.08ms
step:811/2160 train_time:30096ms step_avg:37.11ms
step:812/2160 train_time:30156ms step_avg:37.14ms
step:813/2160 train_time:30217ms step_avg:37.17ms
step:814/2160 train_time:30277ms step_avg:37.20ms
step:815/2160 train_time:30339ms step_avg:37.23ms
step:816/2160 train_time:30401ms step_avg:37.26ms
step:817/2160 train_time:30464ms step_avg:37.29ms
step:818/2160 train_time:30523ms step_avg:37.31ms
step:819/2160 train_time:30585ms step_avg:37.34ms
step:820/2160 train_time:30644ms step_avg:37.37ms
step:821/2160 train_time:30706ms step_avg:37.40ms
step:822/2160 train_time:30765ms step_avg:37.43ms
step:823/2160 train_time:30826ms step_avg:37.46ms
step:824/2160 train_time:30886ms step_avg:37.48ms
step:825/2160 train_time:30947ms step_avg:37.51ms
step:826/2160 train_time:31006ms step_avg:37.54ms
step:827/2160 train_time:31067ms step_avg:37.57ms
step:828/2160 train_time:31126ms step_avg:37.59ms
step:829/2160 train_time:31187ms step_avg:37.62ms
step:830/2160 train_time:31247ms step_avg:37.65ms
step:831/2160 train_time:31309ms step_avg:37.68ms
step:832/2160 train_time:31370ms step_avg:37.70ms
step:833/2160 train_time:31432ms step_avg:37.73ms
step:834/2160 train_time:31492ms step_avg:37.76ms
step:835/2160 train_time:31553ms step_avg:37.79ms
step:836/2160 train_time:31613ms step_avg:37.81ms
step:837/2160 train_time:31674ms step_avg:37.84ms
step:838/2160 train_time:31733ms step_avg:37.87ms
step:839/2160 train_time:31794ms step_avg:37.89ms
step:840/2160 train_time:31853ms step_avg:37.92ms
step:841/2160 train_time:31914ms step_avg:37.95ms
step:842/2160 train_time:31973ms step_avg:37.97ms
step:843/2160 train_time:32034ms step_avg:38.00ms
step:844/2160 train_time:32094ms step_avg:38.03ms
step:845/2160 train_time:32155ms step_avg:38.05ms
step:846/2160 train_time:32214ms step_avg:38.08ms
step:847/2160 train_time:32276ms step_avg:38.11ms
step:848/2160 train_time:32336ms step_avg:38.13ms
step:849/2160 train_time:32397ms step_avg:38.16ms
step:850/2160 train_time:32458ms step_avg:38.19ms
step:851/2160 train_time:32521ms step_avg:38.21ms
step:852/2160 train_time:32580ms step_avg:38.24ms
step:853/2160 train_time:32641ms step_avg:38.27ms
step:854/2160 train_time:32701ms step_avg:38.29ms
step:855/2160 train_time:32762ms step_avg:38.32ms
step:856/2160 train_time:32822ms step_avg:38.34ms
step:857/2160 train_time:32884ms step_avg:38.37ms
step:858/2160 train_time:32943ms step_avg:38.40ms
step:859/2160 train_time:33005ms step_avg:38.42ms
step:860/2160 train_time:33065ms step_avg:38.45ms
step:861/2160 train_time:33126ms step_avg:38.47ms
step:862/2160 train_time:33185ms step_avg:38.50ms
step:863/2160 train_time:33247ms step_avg:38.53ms
step:864/2160 train_time:33307ms step_avg:38.55ms
step:865/2160 train_time:33369ms step_avg:38.58ms
step:866/2160 train_time:33429ms step_avg:38.60ms
step:867/2160 train_time:33491ms step_avg:38.63ms
step:868/2160 train_time:33551ms step_avg:38.65ms
step:869/2160 train_time:33612ms step_avg:38.68ms
step:870/2160 train_time:33672ms step_avg:38.70ms
step:871/2160 train_time:33733ms step_avg:38.73ms
step:872/2160 train_time:33792ms step_avg:38.75ms
step:873/2160 train_time:33854ms step_avg:38.78ms
step:874/2160 train_time:33913ms step_avg:38.80ms
step:875/2160 train_time:33974ms step_avg:38.83ms
step:876/2160 train_time:34033ms step_avg:38.85ms
step:877/2160 train_time:34094ms step_avg:38.88ms
step:878/2160 train_time:34154ms step_avg:38.90ms
step:879/2160 train_time:34215ms step_avg:38.93ms
step:880/2160 train_time:34275ms step_avg:38.95ms
step:881/2160 train_time:34337ms step_avg:38.98ms
step:882/2160 train_time:34397ms step_avg:39.00ms
step:883/2160 train_time:34459ms step_avg:39.02ms
step:884/2160 train_time:34519ms step_avg:39.05ms
step:885/2160 train_time:34581ms step_avg:39.07ms
step:886/2160 train_time:34640ms step_avg:39.10ms
step:887/2160 train_time:34702ms step_avg:39.12ms
step:888/2160 train_time:34762ms step_avg:39.15ms
step:889/2160 train_time:34824ms step_avg:39.17ms
step:890/2160 train_time:34884ms step_avg:39.20ms
step:891/2160 train_time:34945ms step_avg:39.22ms
step:892/2160 train_time:35004ms step_avg:39.24ms
step:893/2160 train_time:35066ms step_avg:39.27ms
step:894/2160 train_time:35125ms step_avg:39.29ms
step:895/2160 train_time:35186ms step_avg:39.31ms
step:896/2160 train_time:35246ms step_avg:39.34ms
step:897/2160 train_time:35308ms step_avg:39.36ms
step:898/2160 train_time:35368ms step_avg:39.39ms
step:899/2160 train_time:35430ms step_avg:39.41ms
step:900/2160 train_time:35491ms step_avg:39.43ms
step:901/2160 train_time:35552ms step_avg:39.46ms
step:902/2160 train_time:35612ms step_avg:39.48ms
step:903/2160 train_time:35674ms step_avg:39.51ms
step:904/2160 train_time:35733ms step_avg:39.53ms
step:905/2160 train_time:35794ms step_avg:39.55ms
step:906/2160 train_time:35853ms step_avg:39.57ms
step:907/2160 train_time:35914ms step_avg:39.60ms
step:908/2160 train_time:35974ms step_avg:39.62ms
step:909/2160 train_time:36036ms step_avg:39.64ms
step:910/2160 train_time:36095ms step_avg:39.66ms
step:911/2160 train_time:36156ms step_avg:39.69ms
step:912/2160 train_time:36216ms step_avg:39.71ms
step:913/2160 train_time:36277ms step_avg:39.73ms
step:914/2160 train_time:36337ms step_avg:39.76ms
step:915/2160 train_time:36399ms step_avg:39.78ms
step:916/2160 train_time:36459ms step_avg:39.80ms
step:917/2160 train_time:36521ms step_avg:39.83ms
step:918/2160 train_time:36582ms step_avg:39.85ms
step:919/2160 train_time:36644ms step_avg:39.87ms
step:920/2160 train_time:36704ms step_avg:39.90ms
step:921/2160 train_time:36766ms step_avg:39.92ms
step:922/2160 train_time:36825ms step_avg:39.94ms
step:923/2160 train_time:36886ms step_avg:39.96ms
step:924/2160 train_time:36945ms step_avg:39.98ms
step:925/2160 train_time:37007ms step_avg:40.01ms
step:926/2160 train_time:37066ms step_avg:40.03ms
step:927/2160 train_time:37127ms step_avg:40.05ms
step:928/2160 train_time:37186ms step_avg:40.07ms
step:929/2160 train_time:37248ms step_avg:40.09ms
step:930/2160 train_time:37308ms step_avg:40.12ms
step:931/2160 train_time:37369ms step_avg:40.14ms
step:932/2160 train_time:37429ms step_avg:40.16ms
step:933/2160 train_time:37491ms step_avg:40.18ms
step:934/2160 train_time:37551ms step_avg:40.20ms
step:935/2160 train_time:37613ms step_avg:40.23ms
step:936/2160 train_time:37673ms step_avg:40.25ms
step:937/2160 train_time:37734ms step_avg:40.27ms
step:938/2160 train_time:37793ms step_avg:40.29ms
step:939/2160 train_time:37854ms step_avg:40.31ms
step:940/2160 train_time:37913ms step_avg:40.33ms
step:941/2160 train_time:37974ms step_avg:40.35ms
step:942/2160 train_time:38033ms step_avg:40.37ms
step:943/2160 train_time:38094ms step_avg:40.40ms
step:944/2160 train_time:38154ms step_avg:40.42ms
step:945/2160 train_time:38215ms step_avg:40.44ms
step:946/2160 train_time:38275ms step_avg:40.46ms
step:947/2160 train_time:38337ms step_avg:40.48ms
step:948/2160 train_time:38397ms step_avg:40.50ms
step:949/2160 train_time:38459ms step_avg:40.53ms
step:950/2160 train_time:38519ms step_avg:40.55ms
step:951/2160 train_time:38580ms step_avg:40.57ms
step:952/2160 train_time:38640ms step_avg:40.59ms
step:953/2160 train_time:38702ms step_avg:40.61ms
step:954/2160 train_time:38762ms step_avg:40.63ms
step:955/2160 train_time:38823ms step_avg:40.65ms
step:956/2160 train_time:38882ms step_avg:40.67ms
step:957/2160 train_time:38944ms step_avg:40.69ms
step:958/2160 train_time:39003ms step_avg:40.71ms
step:959/2160 train_time:39065ms step_avg:40.73ms
step:960/2160 train_time:39125ms step_avg:40.75ms
step:961/2160 train_time:39186ms step_avg:40.78ms
step:962/2160 train_time:39246ms step_avg:40.80ms
step:963/2160 train_time:39308ms step_avg:40.82ms
step:964/2160 train_time:39367ms step_avg:40.84ms
step:965/2160 train_time:39429ms step_avg:40.86ms
step:966/2160 train_time:39489ms step_avg:40.88ms
step:967/2160 train_time:39550ms step_avg:40.90ms
step:968/2160 train_time:39610ms step_avg:40.92ms
step:969/2160 train_time:39672ms step_avg:40.94ms
step:970/2160 train_time:39731ms step_avg:40.96ms
step:971/2160 train_time:39792ms step_avg:40.98ms
step:972/2160 train_time:39852ms step_avg:41.00ms
step:973/2160 train_time:39914ms step_avg:41.02ms
step:974/2160 train_time:39973ms step_avg:41.04ms
step:975/2160 train_time:40033ms step_avg:41.06ms
step:976/2160 train_time:40093ms step_avg:41.08ms
step:977/2160 train_time:40154ms step_avg:41.10ms
step:978/2160 train_time:40214ms step_avg:41.12ms
step:979/2160 train_time:40275ms step_avg:41.14ms
step:980/2160 train_time:40335ms step_avg:41.16ms
step:981/2160 train_time:40397ms step_avg:41.18ms
step:982/2160 train_time:40457ms step_avg:41.20ms
step:983/2160 train_time:40519ms step_avg:41.22ms
step:984/2160 train_time:40578ms step_avg:41.24ms
step:985/2160 train_time:40640ms step_avg:41.26ms
step:986/2160 train_time:40700ms step_avg:41.28ms
step:987/2160 train_time:40763ms step_avg:41.30ms
step:988/2160 train_time:40822ms step_avg:41.32ms
step:989/2160 train_time:40884ms step_avg:41.34ms
step:990/2160 train_time:40944ms step_avg:41.36ms
step:991/2160 train_time:41005ms step_avg:41.38ms
step:992/2160 train_time:41065ms step_avg:41.40ms
step:993/2160 train_time:41126ms step_avg:41.42ms
step:994/2160 train_time:41186ms step_avg:41.43ms
step:995/2160 train_time:41248ms step_avg:41.45ms
step:996/2160 train_time:41307ms step_avg:41.47ms
step:997/2160 train_time:41368ms step_avg:41.49ms
step:998/2160 train_time:41428ms step_avg:41.51ms
step:999/2160 train_time:41490ms step_avg:41.53ms
step:1000/2160 train_time:41550ms step_avg:41.55ms
step:1000/2160 val_loss:3.6966 train_time:41612ms step_avg:41.61ms
step:1001/2160 train_time:41631ms step_avg:41.59ms
step:1002/2160 train_time:41672ms step_avg:41.59ms
step:1003/2160 train_time:41736ms step_avg:41.61ms
step:1004/2160 train_time:41798ms step_avg:41.63ms
step:1005/2160 train_time:41858ms step_avg:41.65ms
step:1006/2160 train_time:41918ms step_avg:41.67ms
step:1007/2160 train_time:41978ms step_avg:41.69ms
step:1008/2160 train_time:42037ms step_avg:41.70ms
step:1009/2160 train_time:42098ms step_avg:41.72ms
step:1010/2160 train_time:42157ms step_avg:41.74ms
step:1011/2160 train_time:42217ms step_avg:41.76ms
step:1012/2160 train_time:42276ms step_avg:41.77ms
step:1013/2160 train_time:42337ms step_avg:41.79ms
step:1014/2160 train_time:42396ms step_avg:41.81ms
step:1015/2160 train_time:42457ms step_avg:41.83ms
step:1016/2160 train_time:42517ms step_avg:41.85ms
step:1017/2160 train_time:42580ms step_avg:41.87ms
step:1018/2160 train_time:42641ms step_avg:41.89ms
step:1019/2160 train_time:42706ms step_avg:41.91ms
step:1020/2160 train_time:42767ms step_avg:41.93ms
step:1021/2160 train_time:42828ms step_avg:41.95ms
step:1022/2160 train_time:42887ms step_avg:41.96ms
step:1023/2160 train_time:42948ms step_avg:41.98ms
step:1024/2160 train_time:43007ms step_avg:42.00ms
step:1025/2160 train_time:43068ms step_avg:42.02ms
step:1026/2160 train_time:43128ms step_avg:42.03ms
step:1027/2160 train_time:43189ms step_avg:42.05ms
step:1028/2160 train_time:43248ms step_avg:42.07ms
step:1029/2160 train_time:43309ms step_avg:42.09ms
step:1030/2160 train_time:43369ms step_avg:42.11ms
step:1031/2160 train_time:43430ms step_avg:42.12ms
step:1032/2160 train_time:43489ms step_avg:42.14ms
step:1033/2160 train_time:43551ms step_avg:42.16ms
step:1034/2160 train_time:43612ms step_avg:42.18ms
step:1035/2160 train_time:43674ms step_avg:42.20ms
step:1036/2160 train_time:43734ms step_avg:42.21ms
step:1037/2160 train_time:43797ms step_avg:42.23ms
step:1038/2160 train_time:43857ms step_avg:42.25ms
step:1039/2160 train_time:43919ms step_avg:42.27ms
step:1040/2160 train_time:43977ms step_avg:42.29ms
step:1041/2160 train_time:44039ms step_avg:42.30ms
step:1042/2160 train_time:44098ms step_avg:42.32ms
step:1043/2160 train_time:44159ms step_avg:42.34ms
step:1044/2160 train_time:44218ms step_avg:42.35ms
step:1045/2160 train_time:44279ms step_avg:42.37ms
step:1046/2160 train_time:44339ms step_avg:42.39ms
step:1047/2160 train_time:44399ms step_avg:42.41ms
step:1048/2160 train_time:44458ms step_avg:42.42ms
step:1049/2160 train_time:44520ms step_avg:42.44ms
step:1050/2160 train_time:44580ms step_avg:42.46ms
step:1051/2160 train_time:44642ms step_avg:42.48ms
step:1052/2160 train_time:44702ms step_avg:42.49ms
step:1053/2160 train_time:44763ms step_avg:42.51ms
step:1054/2160 train_time:44822ms step_avg:42.53ms
step:1055/2160 train_time:44883ms step_avg:42.54ms
step:1056/2160 train_time:44943ms step_avg:42.56ms
step:1057/2160 train_time:45004ms step_avg:42.58ms
step:1058/2160 train_time:45063ms step_avg:42.59ms
step:1059/2160 train_time:45125ms step_avg:42.61ms
step:1060/2160 train_time:45184ms step_avg:42.63ms
step:1061/2160 train_time:45245ms step_avg:42.64ms
step:1062/2160 train_time:45304ms step_avg:42.66ms
step:1063/2160 train_time:45366ms step_avg:42.68ms
step:1064/2160 train_time:45425ms step_avg:42.69ms
step:1065/2160 train_time:45486ms step_avg:42.71ms
step:1066/2160 train_time:45546ms step_avg:42.73ms
step:1067/2160 train_time:45608ms step_avg:42.74ms
step:1068/2160 train_time:45668ms step_avg:42.76ms
step:1069/2160 train_time:45729ms step_avg:42.78ms
step:1070/2160 train_time:45789ms step_avg:42.79ms
step:1071/2160 train_time:45850ms step_avg:42.81ms
step:1072/2160 train_time:45910ms step_avg:42.83ms
step:1073/2160 train_time:45971ms step_avg:42.84ms
step:1074/2160 train_time:46031ms step_avg:42.86ms
step:1075/2160 train_time:46092ms step_avg:42.88ms
step:1076/2160 train_time:46152ms step_avg:42.89ms
step:1077/2160 train_time:46214ms step_avg:42.91ms
step:1078/2160 train_time:46273ms step_avg:42.93ms
step:1079/2160 train_time:46335ms step_avg:42.94ms
step:1080/2160 train_time:46394ms step_avg:42.96ms
step:1081/2160 train_time:46455ms step_avg:42.97ms
step:1082/2160 train_time:46515ms step_avg:42.99ms
step:1083/2160 train_time:46576ms step_avg:43.01ms
step:1084/2160 train_time:46636ms step_avg:43.02ms
step:1085/2160 train_time:46698ms step_avg:43.04ms
step:1086/2160 train_time:46758ms step_avg:43.05ms
step:1087/2160 train_time:46819ms step_avg:43.07ms
step:1088/2160 train_time:46879ms step_avg:43.09ms
step:1089/2160 train_time:46940ms step_avg:43.10ms
step:1090/2160 train_time:47000ms step_avg:43.12ms
step:1091/2160 train_time:47061ms step_avg:43.14ms
step:1092/2160 train_time:47121ms step_avg:43.15ms
step:1093/2160 train_time:47181ms step_avg:43.17ms
step:1094/2160 train_time:47241ms step_avg:43.18ms
step:1095/2160 train_time:47301ms step_avg:43.20ms
step:1096/2160 train_time:47361ms step_avg:43.21ms
step:1097/2160 train_time:47422ms step_avg:43.23ms
step:1098/2160 train_time:47482ms step_avg:43.24ms
step:1099/2160 train_time:47543ms step_avg:43.26ms
step:1100/2160 train_time:47603ms step_avg:43.28ms
step:1101/2160 train_time:47664ms step_avg:43.29ms
step:1102/2160 train_time:47724ms step_avg:43.31ms
step:1103/2160 train_time:47785ms step_avg:43.32ms
step:1104/2160 train_time:47845ms step_avg:43.34ms
step:1105/2160 train_time:47907ms step_avg:43.35ms
step:1106/2160 train_time:47966ms step_avg:43.37ms
step:1107/2160 train_time:48028ms step_avg:43.39ms
step:1108/2160 train_time:48088ms step_avg:43.40ms
step:1109/2160 train_time:48149ms step_avg:43.42ms
step:1110/2160 train_time:48210ms step_avg:43.43ms
step:1111/2160 train_time:48271ms step_avg:43.45ms
step:1112/2160 train_time:48331ms step_avg:43.46ms
step:1113/2160 train_time:48392ms step_avg:43.48ms
step:1114/2160 train_time:48452ms step_avg:43.49ms
step:1115/2160 train_time:48514ms step_avg:43.51ms
step:1116/2160 train_time:48574ms step_avg:43.52ms
step:1117/2160 train_time:48635ms step_avg:43.54ms
step:1118/2160 train_time:48695ms step_avg:43.56ms
step:1119/2160 train_time:48757ms step_avg:43.57ms
step:1120/2160 train_time:48817ms step_avg:43.59ms
step:1121/2160 train_time:48878ms step_avg:43.60ms
step:1122/2160 train_time:48938ms step_avg:43.62ms
step:1123/2160 train_time:48999ms step_avg:43.63ms
step:1124/2160 train_time:49058ms step_avg:43.65ms
step:1125/2160 train_time:49119ms step_avg:43.66ms
step:1126/2160 train_time:49179ms step_avg:43.68ms
step:1127/2160 train_time:49241ms step_avg:43.69ms
step:1128/2160 train_time:49301ms step_avg:43.71ms
step:1129/2160 train_time:49362ms step_avg:43.72ms
step:1130/2160 train_time:49422ms step_avg:43.74ms
step:1131/2160 train_time:49484ms step_avg:43.75ms
step:1132/2160 train_time:49543ms step_avg:43.77ms
step:1133/2160 train_time:49604ms step_avg:43.78ms
step:1134/2160 train_time:49663ms step_avg:43.79ms
step:1135/2160 train_time:49725ms step_avg:43.81ms
step:1136/2160 train_time:49785ms step_avg:43.82ms
step:1137/2160 train_time:49846ms step_avg:43.84ms
step:1138/2160 train_time:49906ms step_avg:43.85ms
step:1139/2160 train_time:49968ms step_avg:43.87ms
step:1140/2160 train_time:50027ms step_avg:43.88ms
step:1141/2160 train_time:50089ms step_avg:43.90ms
step:1142/2160 train_time:50149ms step_avg:43.91ms
step:1143/2160 train_time:50211ms step_avg:43.93ms
step:1144/2160 train_time:50271ms step_avg:43.94ms
step:1145/2160 train_time:50333ms step_avg:43.96ms
step:1146/2160 train_time:50392ms step_avg:43.97ms
step:1147/2160 train_time:50454ms step_avg:43.99ms
step:1148/2160 train_time:50514ms step_avg:44.00ms
step:1149/2160 train_time:50576ms step_avg:44.02ms
step:1150/2160 train_time:50635ms step_avg:44.03ms
step:1151/2160 train_time:50698ms step_avg:44.05ms
step:1152/2160 train_time:50757ms step_avg:44.06ms
step:1153/2160 train_time:50818ms step_avg:44.07ms
step:1154/2160 train_time:50878ms step_avg:44.09ms
step:1155/2160 train_time:50940ms step_avg:44.10ms
step:1156/2160 train_time:50999ms step_avg:44.12ms
step:1157/2160 train_time:51060ms step_avg:44.13ms
step:1158/2160 train_time:51120ms step_avg:44.15ms
step:1159/2160 train_time:51181ms step_avg:44.16ms
step:1160/2160 train_time:51242ms step_avg:44.17ms
step:1161/2160 train_time:51304ms step_avg:44.19ms
step:1162/2160 train_time:51364ms step_avg:44.20ms
step:1163/2160 train_time:51425ms step_avg:44.22ms
step:1164/2160 train_time:51484ms step_avg:44.23ms
step:1165/2160 train_time:51546ms step_avg:44.25ms
step:1166/2160 train_time:51605ms step_avg:44.26ms
step:1167/2160 train_time:51666ms step_avg:44.27ms
step:1168/2160 train_time:51726ms step_avg:44.29ms
step:1169/2160 train_time:51788ms step_avg:44.30ms
step:1170/2160 train_time:51847ms step_avg:44.31ms
step:1171/2160 train_time:51909ms step_avg:44.33ms
step:1172/2160 train_time:51970ms step_avg:44.34ms
step:1173/2160 train_time:52032ms step_avg:44.36ms
step:1174/2160 train_time:52092ms step_avg:44.37ms
step:1175/2160 train_time:52154ms step_avg:44.39ms
step:1176/2160 train_time:52214ms step_avg:44.40ms
step:1177/2160 train_time:52276ms step_avg:44.41ms
step:1178/2160 train_time:52335ms step_avg:44.43ms
step:1179/2160 train_time:52397ms step_avg:44.44ms
step:1180/2160 train_time:52456ms step_avg:44.45ms
step:1181/2160 train_time:52518ms step_avg:44.47ms
step:1182/2160 train_time:52577ms step_avg:44.48ms
step:1183/2160 train_time:52639ms step_avg:44.50ms
step:1184/2160 train_time:52698ms step_avg:44.51ms
step:1185/2160 train_time:52759ms step_avg:44.52ms
step:1186/2160 train_time:52818ms step_avg:44.53ms
step:1187/2160 train_time:52879ms step_avg:44.55ms
step:1188/2160 train_time:52939ms step_avg:44.56ms
step:1189/2160 train_time:53001ms step_avg:44.58ms
step:1190/2160 train_time:53061ms step_avg:44.59ms
step:1191/2160 train_time:53122ms step_avg:44.60ms
step:1192/2160 train_time:53182ms step_avg:44.62ms
step:1193/2160 train_time:53244ms step_avg:44.63ms
step:1194/2160 train_time:53304ms step_avg:44.64ms
step:1195/2160 train_time:53364ms step_avg:44.66ms
step:1196/2160 train_time:53424ms step_avg:44.67ms
step:1197/2160 train_time:53485ms step_avg:44.68ms
step:1198/2160 train_time:53544ms step_avg:44.69ms
step:1199/2160 train_time:53605ms step_avg:44.71ms
step:1200/2160 train_time:53665ms step_avg:44.72ms
step:1201/2160 train_time:53726ms step_avg:44.73ms
step:1202/2160 train_time:53786ms step_avg:44.75ms
step:1203/2160 train_time:53847ms step_avg:44.76ms
step:1204/2160 train_time:53907ms step_avg:44.77ms
step:1205/2160 train_time:53968ms step_avg:44.79ms
step:1206/2160 train_time:54028ms step_avg:44.80ms
step:1207/2160 train_time:54090ms step_avg:44.81ms
step:1208/2160 train_time:54150ms step_avg:44.83ms
step:1209/2160 train_time:54211ms step_avg:44.84ms
step:1210/2160 train_time:54272ms step_avg:44.85ms
step:1211/2160 train_time:54333ms step_avg:44.87ms
step:1212/2160 train_time:54393ms step_avg:44.88ms
step:1213/2160 train_time:54455ms step_avg:44.89ms
step:1214/2160 train_time:54515ms step_avg:44.90ms
step:1215/2160 train_time:54576ms step_avg:44.92ms
step:1216/2160 train_time:54636ms step_avg:44.93ms
step:1217/2160 train_time:54699ms step_avg:44.95ms
step:1218/2160 train_time:54758ms step_avg:44.96ms
step:1219/2160 train_time:54819ms step_avg:44.97ms
step:1220/2160 train_time:54878ms step_avg:44.98ms
step:1221/2160 train_time:54939ms step_avg:45.00ms
step:1222/2160 train_time:54999ms step_avg:45.01ms
step:1223/2160 train_time:55060ms step_avg:45.02ms
step:1224/2160 train_time:55120ms step_avg:45.03ms
step:1225/2160 train_time:55182ms step_avg:45.05ms
step:1226/2160 train_time:55242ms step_avg:45.06ms
step:1227/2160 train_time:55303ms step_avg:45.07ms
step:1228/2160 train_time:55364ms step_avg:45.08ms
step:1229/2160 train_time:55425ms step_avg:45.10ms
step:1230/2160 train_time:55484ms step_avg:45.11ms
step:1231/2160 train_time:55545ms step_avg:45.12ms
step:1232/2160 train_time:55604ms step_avg:45.13ms
step:1233/2160 train_time:55666ms step_avg:45.15ms
step:1234/2160 train_time:55726ms step_avg:45.16ms
step:1235/2160 train_time:55787ms step_avg:45.17ms
step:1236/2160 train_time:55847ms step_avg:45.18ms
step:1237/2160 train_time:55908ms step_avg:45.20ms
step:1238/2160 train_time:55968ms step_avg:45.21ms
step:1239/2160 train_time:56029ms step_avg:45.22ms
step:1240/2160 train_time:56090ms step_avg:45.23ms
step:1241/2160 train_time:56152ms step_avg:45.25ms
step:1242/2160 train_time:56212ms step_avg:45.26ms
step:1243/2160 train_time:56273ms step_avg:45.27ms
step:1244/2160 train_time:56333ms step_avg:45.28ms
step:1245/2160 train_time:56395ms step_avg:45.30ms
step:1246/2160 train_time:56454ms step_avg:45.31ms
step:1247/2160 train_time:56516ms step_avg:45.32ms
step:1248/2160 train_time:56575ms step_avg:45.33ms
step:1249/2160 train_time:56637ms step_avg:45.35ms
step:1250/2160 train_time:56697ms step_avg:45.36ms
step:1250/2160 val_loss:3.5795 train_time:56759ms step_avg:45.41ms
step:1251/2160 train_time:56778ms step_avg:45.39ms
step:1252/2160 train_time:56820ms step_avg:45.38ms
step:1253/2160 train_time:56884ms step_avg:45.40ms
step:1254/2160 train_time:56944ms step_avg:45.41ms
step:1255/2160 train_time:57006ms step_avg:45.42ms
step:1256/2160 train_time:57066ms step_avg:45.43ms
step:1257/2160 train_time:57126ms step_avg:45.45ms
step:1258/2160 train_time:57185ms step_avg:45.46ms
step:1259/2160 train_time:57247ms step_avg:45.47ms
step:1260/2160 train_time:57305ms step_avg:45.48ms
step:1261/2160 train_time:57366ms step_avg:45.49ms
step:1262/2160 train_time:57425ms step_avg:45.50ms
step:1263/2160 train_time:57486ms step_avg:45.52ms
step:1264/2160 train_time:57545ms step_avg:45.53ms
step:1265/2160 train_time:57606ms step_avg:45.54ms
step:1266/2160 train_time:57665ms step_avg:45.55ms
step:1267/2160 train_time:57728ms step_avg:45.56ms
step:1268/2160 train_time:57788ms step_avg:45.57ms
step:1269/2160 train_time:57851ms step_avg:45.59ms
step:1270/2160 train_time:57911ms step_avg:45.60ms
step:1271/2160 train_time:57973ms step_avg:45.61ms
step:1272/2160 train_time:58033ms step_avg:45.62ms
step:1273/2160 train_time:58093ms step_avg:45.64ms
step:1274/2160 train_time:58153ms step_avg:45.65ms
step:1275/2160 train_time:58214ms step_avg:45.66ms
step:1276/2160 train_time:58273ms step_avg:45.67ms
step:1277/2160 train_time:58334ms step_avg:45.68ms
step:1278/2160 train_time:58393ms step_avg:45.69ms
step:1279/2160 train_time:58453ms step_avg:45.70ms
step:1280/2160 train_time:58512ms step_avg:45.71ms
step:1281/2160 train_time:58573ms step_avg:45.72ms
step:1282/2160 train_time:58633ms step_avg:45.74ms
step:1283/2160 train_time:58694ms step_avg:45.75ms
step:1284/2160 train_time:58755ms step_avg:45.76ms
step:1285/2160 train_time:58816ms step_avg:45.77ms
step:1286/2160 train_time:58876ms step_avg:45.78ms
step:1287/2160 train_time:58938ms step_avg:45.79ms
step:1288/2160 train_time:58997ms step_avg:45.81ms
step:1289/2160 train_time:59058ms step_avg:45.82ms
step:1290/2160 train_time:59118ms step_avg:45.83ms
step:1291/2160 train_time:59180ms step_avg:45.84ms
step:1292/2160 train_time:59239ms step_avg:45.85ms
step:1293/2160 train_time:59301ms step_avg:45.86ms
step:1294/2160 train_time:59361ms step_avg:45.87ms
step:1295/2160 train_time:59422ms step_avg:45.89ms
step:1296/2160 train_time:59482ms step_avg:45.90ms
step:1297/2160 train_time:59543ms step_avg:45.91ms
step:1298/2160 train_time:59603ms step_avg:45.92ms
step:1299/2160 train_time:59664ms step_avg:45.93ms
step:1300/2160 train_time:59725ms step_avg:45.94ms
step:1301/2160 train_time:59787ms step_avg:45.95ms
step:1302/2160 train_time:59846ms step_avg:45.96ms
step:1303/2160 train_time:59908ms step_avg:45.98ms
step:1304/2160 train_time:59968ms step_avg:45.99ms
step:1305/2160 train_time:60030ms step_avg:46.00ms
step:1306/2160 train_time:60090ms step_avg:46.01ms
step:1307/2160 train_time:60151ms step_avg:46.02ms
step:1308/2160 train_time:60210ms step_avg:46.03ms
step:1309/2160 train_time:60271ms step_avg:46.04ms
step:1310/2160 train_time:60330ms step_avg:46.05ms
step:1311/2160 train_time:60391ms step_avg:46.06ms
step:1312/2160 train_time:60451ms step_avg:46.08ms
step:1313/2160 train_time:60512ms step_avg:46.09ms
step:1314/2160 train_time:60571ms step_avg:46.10ms
step:1315/2160 train_time:60633ms step_avg:46.11ms
step:1316/2160 train_time:60693ms step_avg:46.12ms
step:1317/2160 train_time:60754ms step_avg:46.13ms
step:1318/2160 train_time:60813ms step_avg:46.14ms
step:1319/2160 train_time:60874ms step_avg:46.15ms
step:1320/2160 train_time:60934ms step_avg:46.16ms
step:1321/2160 train_time:60995ms step_avg:46.17ms
step:1322/2160 train_time:61055ms step_avg:46.18ms
step:1323/2160 train_time:61117ms step_avg:46.20ms
step:1324/2160 train_time:61177ms step_avg:46.21ms
step:1325/2160 train_time:61238ms step_avg:46.22ms
step:1326/2160 train_time:61297ms step_avg:46.23ms
step:1327/2160 train_time:61359ms step_avg:46.24ms
step:1328/2160 train_time:61419ms step_avg:46.25ms
step:1329/2160 train_time:61481ms step_avg:46.26ms
step:1330/2160 train_time:61540ms step_avg:46.27ms
step:1331/2160 train_time:61601ms step_avg:46.28ms
step:1332/2160 train_time:61662ms step_avg:46.29ms
step:1333/2160 train_time:61723ms step_avg:46.30ms
step:1334/2160 train_time:61783ms step_avg:46.31ms
step:1335/2160 train_time:61845ms step_avg:46.33ms
step:1336/2160 train_time:61905ms step_avg:46.34ms
step:1337/2160 train_time:61967ms step_avg:46.35ms
step:1338/2160 train_time:62026ms step_avg:46.36ms
step:1339/2160 train_time:62088ms step_avg:46.37ms
step:1340/2160 train_time:62149ms step_avg:46.38ms
step:1341/2160 train_time:62210ms step_avg:46.39ms
step:1342/2160 train_time:62270ms step_avg:46.40ms
step:1343/2160 train_time:62331ms step_avg:46.41ms
step:1344/2160 train_time:62390ms step_avg:46.42ms
step:1345/2160 train_time:62452ms step_avg:46.43ms
step:1346/2160 train_time:62512ms step_avg:46.44ms
step:1347/2160 train_time:62574ms step_avg:46.45ms
step:1348/2160 train_time:62634ms step_avg:46.46ms
step:1349/2160 train_time:62696ms step_avg:46.48ms
step:1350/2160 train_time:62755ms step_avg:46.49ms
step:1351/2160 train_time:62817ms step_avg:46.50ms
step:1352/2160 train_time:62876ms step_avg:46.51ms
step:1353/2160 train_time:62938ms step_avg:46.52ms
step:1354/2160 train_time:62998ms step_avg:46.53ms
step:1355/2160 train_time:63060ms step_avg:46.54ms
step:1356/2160 train_time:63119ms step_avg:46.55ms
step:1357/2160 train_time:63181ms step_avg:46.56ms
step:1358/2160 train_time:63242ms step_avg:46.57ms
step:1359/2160 train_time:63303ms step_avg:46.58ms
step:1360/2160 train_time:63363ms step_avg:46.59ms
step:1361/2160 train_time:63424ms step_avg:46.60ms
step:1362/2160 train_time:63484ms step_avg:46.61ms
step:1363/2160 train_time:63546ms step_avg:46.62ms
step:1364/2160 train_time:63606ms step_avg:46.63ms
step:1365/2160 train_time:63667ms step_avg:46.64ms
step:1366/2160 train_time:63726ms step_avg:46.65ms
step:1367/2160 train_time:63788ms step_avg:46.66ms
step:1368/2160 train_time:63847ms step_avg:46.67ms
step:1369/2160 train_time:63909ms step_avg:46.68ms
step:1370/2160 train_time:63968ms step_avg:46.69ms
step:1371/2160 train_time:64029ms step_avg:46.70ms
step:1372/2160 train_time:64089ms step_avg:46.71ms
step:1373/2160 train_time:64151ms step_avg:46.72ms
step:1374/2160 train_time:64211ms step_avg:46.73ms
step:1375/2160 train_time:64272ms step_avg:46.74ms
step:1376/2160 train_time:64332ms step_avg:46.75ms
step:1377/2160 train_time:64393ms step_avg:46.76ms
step:1378/2160 train_time:64453ms step_avg:46.77ms
step:1379/2160 train_time:64515ms step_avg:46.78ms
step:1380/2160 train_time:64574ms step_avg:46.79ms
step:1381/2160 train_time:64635ms step_avg:46.80ms
step:1382/2160 train_time:64694ms step_avg:46.81ms
step:1383/2160 train_time:64756ms step_avg:46.82ms
step:1384/2160 train_time:64815ms step_avg:46.83ms
step:1385/2160 train_time:64876ms step_avg:46.84ms
step:1386/2160 train_time:64936ms step_avg:46.85ms
step:1387/2160 train_time:64998ms step_avg:46.86ms
step:1388/2160 train_time:65058ms step_avg:46.87ms
step:1389/2160 train_time:65120ms step_avg:46.88ms
step:1390/2160 train_time:65179ms step_avg:46.89ms
step:1391/2160 train_time:65240ms step_avg:46.90ms
step:1392/2160 train_time:65299ms step_avg:46.91ms
step:1393/2160 train_time:65361ms step_avg:46.92ms
step:1394/2160 train_time:65421ms step_avg:46.93ms
step:1395/2160 train_time:65483ms step_avg:46.94ms
step:1396/2160 train_time:65543ms step_avg:46.95ms
step:1397/2160 train_time:65604ms step_avg:46.96ms
step:1398/2160 train_time:65664ms step_avg:46.97ms
step:1399/2160 train_time:65725ms step_avg:46.98ms
step:1400/2160 train_time:65785ms step_avg:46.99ms
step:1401/2160 train_time:65847ms step_avg:47.00ms
step:1402/2160 train_time:65907ms step_avg:47.01ms
step:1403/2160 train_time:65969ms step_avg:47.02ms
step:1404/2160 train_time:66028ms step_avg:47.03ms
step:1405/2160 train_time:66090ms step_avg:47.04ms
step:1406/2160 train_time:66151ms step_avg:47.05ms
step:1407/2160 train_time:66212ms step_avg:47.06ms
step:1408/2160 train_time:66271ms step_avg:47.07ms
step:1409/2160 train_time:66332ms step_avg:47.08ms
step:1410/2160 train_time:66393ms step_avg:47.09ms
step:1411/2160 train_time:66454ms step_avg:47.10ms
step:1412/2160 train_time:66514ms step_avg:47.11ms
step:1413/2160 train_time:66575ms step_avg:47.12ms
step:1414/2160 train_time:66635ms step_avg:47.12ms
step:1415/2160 train_time:66696ms step_avg:47.13ms
step:1416/2160 train_time:66783ms step_avg:47.16ms
step:1417/2160 train_time:66872ms step_avg:47.19ms
step:1418/2160 train_time:66960ms step_avg:47.22ms
step:1419/2160 train_time:67049ms step_avg:47.25ms
step:1420/2160 train_time:67136ms step_avg:47.28ms
step:1421/2160 train_time:67226ms step_avg:47.31ms
step:1422/2160 train_time:67314ms step_avg:47.34ms
step:1423/2160 train_time:67403ms step_avg:47.37ms
step:1424/2160 train_time:67491ms step_avg:47.40ms
step:1425/2160 train_time:67580ms step_avg:47.42ms
step:1426/2160 train_time:67667ms step_avg:47.45ms
step:1427/2160 train_time:67756ms step_avg:47.48ms
step:1428/2160 train_time:67843ms step_avg:47.51ms
step:1429/2160 train_time:67932ms step_avg:47.54ms
step:1430/2160 train_time:68020ms step_avg:47.57ms
step:1431/2160 train_time:68110ms step_avg:47.60ms
step:1432/2160 train_time:68198ms step_avg:47.62ms
step:1433/2160 train_time:68287ms step_avg:47.65ms
step:1434/2160 train_time:68375ms step_avg:47.68ms
step:1435/2160 train_time:68464ms step_avg:47.71ms
step:1436/2160 train_time:68551ms step_avg:47.74ms
step:1437/2160 train_time:68640ms step_avg:47.77ms
step:1438/2160 train_time:68727ms step_avg:47.79ms
step:1439/2160 train_time:68816ms step_avg:47.82ms
step:1440/2160 train_time:68904ms step_avg:47.85ms
step:1441/2160 train_time:68993ms step_avg:47.88ms
step:1442/2160 train_time:69081ms step_avg:47.91ms
step:1443/2160 train_time:69170ms step_avg:47.93ms
step:1444/2160 train_time:69257ms step_avg:47.96ms
step:1445/2160 train_time:69347ms step_avg:47.99ms
step:1446/2160 train_time:69434ms step_avg:48.02ms
step:1447/2160 train_time:69523ms step_avg:48.05ms
step:1448/2160 train_time:69610ms step_avg:48.07ms
step:1449/2160 train_time:69699ms step_avg:48.10ms
step:1450/2160 train_time:69786ms step_avg:48.13ms
step:1451/2160 train_time:69875ms step_avg:48.16ms
step:1452/2160 train_time:69962ms step_avg:48.18ms
step:1453/2160 train_time:70052ms step_avg:48.21ms
step:1454/2160 train_time:70139ms step_avg:48.24ms
step:1455/2160 train_time:70228ms step_avg:48.27ms
step:1456/2160 train_time:70316ms step_avg:48.29ms
step:1457/2160 train_time:70405ms step_avg:48.32ms
step:1458/2160 train_time:70493ms step_avg:48.35ms
step:1459/2160 train_time:70581ms step_avg:48.38ms
step:1460/2160 train_time:70669ms step_avg:48.40ms
step:1461/2160 train_time:70757ms step_avg:48.43ms
step:1462/2160 train_time:70844ms step_avg:48.46ms
step:1463/2160 train_time:70933ms step_avg:48.48ms
step:1464/2160 train_time:71021ms step_avg:48.51ms
step:1465/2160 train_time:71111ms step_avg:48.54ms
step:1466/2160 train_time:71198ms step_avg:48.57ms
step:1467/2160 train_time:71287ms step_avg:48.59ms
step:1468/2160 train_time:71375ms step_avg:48.62ms
step:1469/2160 train_time:71464ms step_avg:48.65ms
step:1470/2160 train_time:71552ms step_avg:48.67ms
step:1471/2160 train_time:71641ms step_avg:48.70ms
step:1472/2160 train_time:71728ms step_avg:48.73ms
step:1473/2160 train_time:71817ms step_avg:48.76ms
step:1474/2160 train_time:71904ms step_avg:48.78ms
step:1475/2160 train_time:71993ms step_avg:48.81ms
step:1476/2160 train_time:72081ms step_avg:48.84ms
step:1477/2160 train_time:72170ms step_avg:48.86ms
step:1478/2160 train_time:72257ms step_avg:48.89ms
step:1479/2160 train_time:72346ms step_avg:48.92ms
step:1480/2160 train_time:72434ms step_avg:48.94ms
step:1481/2160 train_time:72523ms step_avg:48.97ms
step:1482/2160 train_time:72611ms step_avg:49.00ms
step:1483/2160 train_time:72701ms step_avg:49.02ms
step:1484/2160 train_time:72789ms step_avg:49.05ms
step:1485/2160 train_time:72878ms step_avg:49.08ms
step:1486/2160 train_time:72965ms step_avg:49.10ms
step:1487/2160 train_time:73055ms step_avg:49.13ms
step:1488/2160 train_time:73143ms step_avg:49.15ms
step:1489/2160 train_time:73232ms step_avg:49.18ms
step:1490/2160 train_time:73319ms step_avg:49.21ms
step:1491/2160 train_time:73408ms step_avg:49.23ms
step:1492/2160 train_time:73497ms step_avg:49.26ms
step:1493/2160 train_time:73586ms step_avg:49.29ms
step:1494/2160 train_time:73672ms step_avg:49.31ms
step:1495/2160 train_time:73761ms step_avg:49.34ms
step:1496/2160 train_time:73848ms step_avg:49.36ms
step:1497/2160 train_time:73938ms step_avg:49.39ms
step:1498/2160 train_time:74025ms step_avg:49.42ms
step:1499/2160 train_time:74113ms step_avg:49.44ms
step:1500/2160 train_time:74201ms step_avg:49.47ms
step:1500/2160 val_loss:3.4977 train_time:74291ms step_avg:49.53ms
step:1501/2160 train_time:74314ms step_avg:49.51ms
step:1502/2160 train_time:74379ms step_avg:49.52ms
step:1503/2160 train_time:74469ms step_avg:49.55ms
step:1504/2160 train_time:74558ms step_avg:49.57ms
step:1505/2160 train_time:74646ms step_avg:49.60ms
step:1506/2160 train_time:74732ms step_avg:49.62ms
step:1507/2160 train_time:74820ms step_avg:49.65ms
step:1508/2160 train_time:74905ms step_avg:49.67ms
step:1509/2160 train_time:74994ms step_avg:49.70ms
step:1510/2160 train_time:75081ms step_avg:49.72ms
step:1511/2160 train_time:75171ms step_avg:49.75ms
step:1512/2160 train_time:75265ms step_avg:49.78ms
step:1513/2160 train_time:75354ms step_avg:49.80ms
step:1514/2160 train_time:75442ms step_avg:49.83ms
step:1515/2160 train_time:75532ms step_avg:49.86ms
step:1516/2160 train_time:75619ms step_avg:49.88ms
step:1517/2160 train_time:75708ms step_avg:49.91ms
step:1518/2160 train_time:75794ms step_avg:49.93ms
step:1519/2160 train_time:75882ms step_avg:49.96ms
step:1520/2160 train_time:75968ms step_avg:49.98ms
step:1521/2160 train_time:76056ms step_avg:50.00ms
step:1522/2160 train_time:76144ms step_avg:50.03ms
step:1523/2160 train_time:76235ms step_avg:50.06ms
step:1524/2160 train_time:76324ms step_avg:50.08ms
step:1525/2160 train_time:76414ms step_avg:50.11ms
step:1526/2160 train_time:76503ms step_avg:50.13ms
step:1527/2160 train_time:76592ms step_avg:50.16ms
step:1528/2160 train_time:76678ms step_avg:50.18ms
step:1529/2160 train_time:76767ms step_avg:50.21ms
step:1530/2160 train_time:76853ms step_avg:50.23ms
step:1531/2160 train_time:76941ms step_avg:50.26ms
step:1532/2160 train_time:77027ms step_avg:50.28ms
step:1533/2160 train_time:77116ms step_avg:50.30ms
step:1534/2160 train_time:77204ms step_avg:50.33ms
step:1535/2160 train_time:77295ms step_avg:50.36ms
step:1536/2160 train_time:77383ms step_avg:50.38ms
step:1537/2160 train_time:77473ms step_avg:50.41ms
step:1538/2160 train_time:77561ms step_avg:50.43ms
step:1539/2160 train_time:77650ms step_avg:50.45ms
step:1540/2160 train_time:77736ms step_avg:50.48ms
step:1541/2160 train_time:77824ms step_avg:50.50ms
step:1542/2160 train_time:77912ms step_avg:50.53ms
step:1543/2160 train_time:78000ms step_avg:50.55ms
step:1544/2160 train_time:78087ms step_avg:50.57ms
step:1545/2160 train_time:78176ms step_avg:50.60ms
step:1546/2160 train_time:78265ms step_avg:50.62ms
step:1547/2160 train_time:78355ms step_avg:50.65ms
step:1548/2160 train_time:78443ms step_avg:50.67ms
step:1549/2160 train_time:78533ms step_avg:50.70ms
step:1550/2160 train_time:78621ms step_avg:50.72ms
step:1551/2160 train_time:78710ms step_avg:50.75ms
step:1552/2160 train_time:78796ms step_avg:50.77ms
step:1553/2160 train_time:78885ms step_avg:50.80ms
step:1554/2160 train_time:78972ms step_avg:50.82ms
step:1555/2160 train_time:79060ms step_avg:50.84ms
step:1556/2160 train_time:79147ms step_avg:50.87ms
step:1557/2160 train_time:79237ms step_avg:50.89ms
step:1558/2160 train_time:79325ms step_avg:50.91ms
step:1559/2160 train_time:79414ms step_avg:50.94ms
step:1560/2160 train_time:79502ms step_avg:50.96ms
step:1561/2160 train_time:79591ms step_avg:50.99ms
step:1562/2160 train_time:79678ms step_avg:51.01ms
step:1563/2160 train_time:79766ms step_avg:51.03ms
step:1564/2160 train_time:79853ms step_avg:51.06ms
step:1565/2160 train_time:79941ms step_avg:51.08ms
step:1566/2160 train_time:80029ms step_avg:51.10ms
step:1567/2160 train_time:80118ms step_avg:51.13ms
step:1568/2160 train_time:80205ms step_avg:51.15ms
step:1569/2160 train_time:80294ms step_avg:51.18ms
step:1570/2160 train_time:80382ms step_avg:51.20ms
step:1571/2160 train_time:80471ms step_avg:51.22ms
step:1572/2160 train_time:80559ms step_avg:51.25ms
step:1573/2160 train_time:80648ms step_avg:51.27ms
step:1574/2160 train_time:80735ms step_avg:51.29ms
step:1575/2160 train_time:80824ms step_avg:51.32ms
step:1576/2160 train_time:80911ms step_avg:51.34ms
step:1577/2160 train_time:81000ms step_avg:51.36ms
step:1578/2160 train_time:81087ms step_avg:51.39ms
step:1579/2160 train_time:81176ms step_avg:51.41ms
step:1580/2160 train_time:81265ms step_avg:51.43ms
step:1581/2160 train_time:81354ms step_avg:51.46ms
step:1582/2160 train_time:81443ms step_avg:51.48ms
step:1583/2160 train_time:81532ms step_avg:51.50ms
step:1584/2160 train_time:81619ms step_avg:51.53ms
step:1585/2160 train_time:81708ms step_avg:51.55ms
step:1586/2160 train_time:81795ms step_avg:51.57ms
step:1587/2160 train_time:81884ms step_avg:51.60ms
step:1588/2160 train_time:81971ms step_avg:51.62ms
step:1589/2160 train_time:82060ms step_avg:51.64ms
step:1590/2160 train_time:82148ms step_avg:51.67ms
step:1591/2160 train_time:82238ms step_avg:51.69ms
step:1592/2160 train_time:82325ms step_avg:51.71ms
step:1593/2160 train_time:82415ms step_avg:51.74ms
step:1594/2160 train_time:82502ms step_avg:51.76ms
step:1595/2160 train_time:82591ms step_avg:51.78ms
step:1596/2160 train_time:82679ms step_avg:51.80ms
step:1597/2160 train_time:82767ms step_avg:51.83ms
step:1598/2160 train_time:82855ms step_avg:51.85ms
step:1599/2160 train_time:82944ms step_avg:51.87ms
step:1600/2160 train_time:83031ms step_avg:51.89ms
step:1601/2160 train_time:83120ms step_avg:51.92ms
step:1602/2160 train_time:83208ms step_avg:51.94ms
step:1603/2160 train_time:83297ms step_avg:51.96ms
step:1604/2160 train_time:83385ms step_avg:51.99ms
step:1605/2160 train_time:83476ms step_avg:52.01ms
step:1606/2160 train_time:83564ms step_avg:52.03ms
step:1607/2160 train_time:83654ms step_avg:52.06ms
step:1608/2160 train_time:83740ms step_avg:52.08ms
step:1609/2160 train_time:83830ms step_avg:52.10ms
step:1610/2160 train_time:83917ms step_avg:52.12ms
step:1611/2160 train_time:84007ms step_avg:52.15ms
step:1612/2160 train_time:84094ms step_avg:52.17ms
step:1613/2160 train_time:84183ms step_avg:52.19ms
step:1614/2160 train_time:84270ms step_avg:52.21ms
step:1615/2160 train_time:84359ms step_avg:52.23ms
step:1616/2160 train_time:84447ms step_avg:52.26ms
step:1617/2160 train_time:84537ms step_avg:52.28ms
step:1618/2160 train_time:84624ms step_avg:52.30ms
step:1619/2160 train_time:84714ms step_avg:52.32ms
step:1620/2160 train_time:84801ms step_avg:52.35ms
step:1621/2160 train_time:84890ms step_avg:52.37ms
step:1622/2160 train_time:84977ms step_avg:52.39ms
step:1623/2160 train_time:85066ms step_avg:52.41ms
step:1624/2160 train_time:85153ms step_avg:52.43ms
step:1625/2160 train_time:85243ms step_avg:52.46ms
step:1626/2160 train_time:85330ms step_avg:52.48ms
step:1627/2160 train_time:85419ms step_avg:52.50ms
step:1628/2160 train_time:85507ms step_avg:52.52ms
step:1629/2160 train_time:85597ms step_avg:52.55ms
step:1630/2160 train_time:85684ms step_avg:52.57ms
step:1631/2160 train_time:85773ms step_avg:52.59ms
step:1632/2160 train_time:85861ms step_avg:52.61ms
step:1633/2160 train_time:85951ms step_avg:52.63ms
step:1634/2160 train_time:86039ms step_avg:52.66ms
step:1635/2160 train_time:86129ms step_avg:52.68ms
step:1636/2160 train_time:86216ms step_avg:52.70ms
step:1637/2160 train_time:86305ms step_avg:52.72ms
step:1638/2160 train_time:86392ms step_avg:52.74ms
step:1639/2160 train_time:86482ms step_avg:52.76ms
step:1640/2160 train_time:86570ms step_avg:52.79ms
step:1641/2160 train_time:86660ms step_avg:52.81ms
step:1642/2160 train_time:86747ms step_avg:52.83ms
step:1643/2160 train_time:86837ms step_avg:52.85ms
step:1644/2160 train_time:86924ms step_avg:52.87ms
step:1645/2160 train_time:87014ms step_avg:52.90ms
step:1646/2160 train_time:87102ms step_avg:52.92ms
step:1647/2160 train_time:87191ms step_avg:52.94ms
step:1648/2160 train_time:87279ms step_avg:52.96ms
step:1649/2160 train_time:87367ms step_avg:52.98ms
step:1650/2160 train_time:87455ms step_avg:53.00ms
step:1651/2160 train_time:87544ms step_avg:53.02ms
step:1652/2160 train_time:87630ms step_avg:53.05ms
step:1653/2160 train_time:87719ms step_avg:53.07ms
step:1654/2160 train_time:87806ms step_avg:53.09ms
step:1655/2160 train_time:87895ms step_avg:53.11ms
step:1656/2160 train_time:87982ms step_avg:53.13ms
step:1657/2160 train_time:88071ms step_avg:53.15ms
step:1658/2160 train_time:88159ms step_avg:53.17ms
step:1659/2160 train_time:88249ms step_avg:53.19ms
step:1660/2160 train_time:88336ms step_avg:53.21ms
step:1661/2160 train_time:88426ms step_avg:53.24ms
step:1662/2160 train_time:88513ms step_avg:53.26ms
step:1663/2160 train_time:88602ms step_avg:53.28ms
step:1664/2160 train_time:88689ms step_avg:53.30ms
step:1665/2160 train_time:88778ms step_avg:53.32ms
step:1666/2160 train_time:88865ms step_avg:53.34ms
step:1667/2160 train_time:88954ms step_avg:53.36ms
step:1668/2160 train_time:89041ms step_avg:53.38ms
step:1669/2160 train_time:89131ms step_avg:53.40ms
step:1670/2160 train_time:89218ms step_avg:53.42ms
step:1671/2160 train_time:89307ms step_avg:53.45ms
step:1672/2160 train_time:89395ms step_avg:53.47ms
step:1673/2160 train_time:89485ms step_avg:53.49ms
step:1674/2160 train_time:89572ms step_avg:53.51ms
step:1675/2160 train_time:89661ms step_avg:53.53ms
step:1676/2160 train_time:89748ms step_avg:53.55ms
step:1677/2160 train_time:89837ms step_avg:53.57ms
step:1678/2160 train_time:89925ms step_avg:53.59ms
step:1679/2160 train_time:90013ms step_avg:53.61ms
step:1680/2160 train_time:90101ms step_avg:53.63ms
step:1681/2160 train_time:90191ms step_avg:53.65ms
step:1682/2160 train_time:90279ms step_avg:53.67ms
step:1683/2160 train_time:90368ms step_avg:53.69ms
step:1684/2160 train_time:90455ms step_avg:53.71ms
step:1685/2160 train_time:90544ms step_avg:53.74ms
step:1686/2160 train_time:90631ms step_avg:53.76ms
step:1687/2160 train_time:90720ms step_avg:53.78ms
step:1688/2160 train_time:90807ms step_avg:53.80ms
step:1689/2160 train_time:90896ms step_avg:53.82ms
step:1690/2160 train_time:90983ms step_avg:53.84ms
step:1691/2160 train_time:91073ms step_avg:53.86ms
step:1692/2160 train_time:91161ms step_avg:53.88ms
step:1693/2160 train_time:91250ms step_avg:53.90ms
step:1694/2160 train_time:91337ms step_avg:53.92ms
step:1695/2160 train_time:91426ms step_avg:53.94ms
step:1696/2160 train_time:91513ms step_avg:53.96ms
step:1697/2160 train_time:91602ms step_avg:53.98ms
step:1698/2160 train_time:91689ms step_avg:54.00ms
step:1699/2160 train_time:91778ms step_avg:54.02ms
step:1700/2160 train_time:91865ms step_avg:54.04ms
step:1701/2160 train_time:91954ms step_avg:54.06ms
step:1702/2160 train_time:92041ms step_avg:54.08ms
step:1703/2160 train_time:92132ms step_avg:54.10ms
step:1704/2160 train_time:92219ms step_avg:54.12ms
step:1705/2160 train_time:92309ms step_avg:54.14ms
step:1706/2160 train_time:92396ms step_avg:54.16ms
step:1707/2160 train_time:92485ms step_avg:54.18ms
step:1708/2160 train_time:92572ms step_avg:54.20ms
step:1709/2160 train_time:92662ms step_avg:54.22ms
step:1710/2160 train_time:92749ms step_avg:54.24ms
step:1711/2160 train_time:92838ms step_avg:54.26ms
step:1712/2160 train_time:92924ms step_avg:54.28ms
step:1713/2160 train_time:93014ms step_avg:54.30ms
step:1714/2160 train_time:93101ms step_avg:54.32ms
step:1715/2160 train_time:93191ms step_avg:54.34ms
step:1716/2160 train_time:93278ms step_avg:54.36ms
step:1717/2160 train_time:93367ms step_avg:54.38ms
step:1718/2160 train_time:93454ms step_avg:54.40ms
step:1719/2160 train_time:93543ms step_avg:54.42ms
step:1720/2160 train_time:93631ms step_avg:54.44ms
step:1721/2160 train_time:93719ms step_avg:54.46ms
step:1722/2160 train_time:93807ms step_avg:54.48ms
step:1723/2160 train_time:93897ms step_avg:54.50ms
step:1724/2160 train_time:93984ms step_avg:54.52ms
step:1725/2160 train_time:94074ms step_avg:54.54ms
step:1726/2160 train_time:94162ms step_avg:54.56ms
step:1727/2160 train_time:94251ms step_avg:54.57ms
step:1728/2160 train_time:94337ms step_avg:54.59ms
step:1729/2160 train_time:94427ms step_avg:54.61ms
step:1730/2160 train_time:94513ms step_avg:54.63ms
step:1731/2160 train_time:94602ms step_avg:54.65ms
step:1732/2160 train_time:94690ms step_avg:54.67ms
step:1733/2160 train_time:94779ms step_avg:54.69ms
step:1734/2160 train_time:94866ms step_avg:54.71ms
step:1735/2160 train_time:94955ms step_avg:54.73ms
step:1736/2160 train_time:95043ms step_avg:54.75ms
step:1737/2160 train_time:95132ms step_avg:54.77ms
step:1738/2160 train_time:95220ms step_avg:54.79ms
step:1739/2160 train_time:95309ms step_avg:54.81ms
step:1740/2160 train_time:95397ms step_avg:54.83ms
step:1741/2160 train_time:95486ms step_avg:54.85ms
step:1742/2160 train_time:95573ms step_avg:54.86ms
step:1743/2160 train_time:95662ms step_avg:54.88ms
step:1744/2160 train_time:95749ms step_avg:54.90ms
step:1745/2160 train_time:95838ms step_avg:54.92ms
step:1746/2160 train_time:95925ms step_avg:54.94ms
step:1747/2160 train_time:96015ms step_avg:54.96ms
step:1748/2160 train_time:96103ms step_avg:54.98ms
step:1749/2160 train_time:96193ms step_avg:55.00ms
step:1750/2160 train_time:96280ms step_avg:55.02ms
step:1750/2160 val_loss:3.3953 train_time:96372ms step_avg:55.07ms
step:1751/2160 train_time:96391ms step_avg:55.05ms
step:1752/2160 train_time:96462ms step_avg:55.06ms
step:1753/2160 train_time:96553ms step_avg:55.08ms
step:1754/2160 train_time:96641ms step_avg:55.10ms
step:1755/2160 train_time:96730ms step_avg:55.12ms
step:1756/2160 train_time:96817ms step_avg:55.13ms
step:1757/2160 train_time:96906ms step_avg:55.15ms
step:1758/2160 train_time:96992ms step_avg:55.17ms
step:1759/2160 train_time:97080ms step_avg:55.19ms
step:1760/2160 train_time:97167ms step_avg:55.21ms
step:1761/2160 train_time:97256ms step_avg:55.23ms
step:1762/2160 train_time:97344ms step_avg:55.25ms
step:1763/2160 train_time:97435ms step_avg:55.27ms
step:1764/2160 train_time:97524ms step_avg:55.29ms
step:1765/2160 train_time:97613ms step_avg:55.30ms
step:1766/2160 train_time:97700ms step_avg:55.32ms
step:1767/2160 train_time:97789ms step_avg:55.34ms
step:1768/2160 train_time:97876ms step_avg:55.36ms
step:1769/2160 train_time:97965ms step_avg:55.38ms
step:1770/2160 train_time:98052ms step_avg:55.40ms
step:1771/2160 train_time:98141ms step_avg:55.42ms
step:1772/2160 train_time:98228ms step_avg:55.43ms
step:1773/2160 train_time:98317ms step_avg:55.45ms
step:1774/2160 train_time:98407ms step_avg:55.47ms
step:1775/2160 train_time:98497ms step_avg:55.49ms
step:1776/2160 train_time:98585ms step_avg:55.51ms
step:1777/2160 train_time:98675ms step_avg:55.53ms
step:1778/2160 train_time:98762ms step_avg:55.55ms
step:1779/2160 train_time:98850ms step_avg:55.56ms
step:1780/2160 train_time:98937ms step_avg:55.58ms
step:1781/2160 train_time:99026ms step_avg:55.60ms
step:1782/2160 train_time:99112ms step_avg:55.62ms
step:1783/2160 train_time:99201ms step_avg:55.64ms
step:1784/2160 train_time:99288ms step_avg:55.65ms
step:1785/2160 train_time:99378ms step_avg:55.67ms
step:1786/2160 train_time:99467ms step_avg:55.69ms
step:1787/2160 train_time:99556ms step_avg:55.71ms
step:1788/2160 train_time:99644ms step_avg:55.73ms
step:1789/2160 train_time:99734ms step_avg:55.75ms
step:1790/2160 train_time:99822ms step_avg:55.77ms
step:1791/2160 train_time:99910ms step_avg:55.78ms
step:1792/2160 train_time:99998ms step_avg:55.80ms
step:1793/2160 train_time:100086ms step_avg:55.82ms
step:1794/2160 train_time:100173ms step_avg:55.84ms
step:1795/2160 train_time:100263ms step_avg:55.86ms
step:1796/2160 train_time:100351ms step_avg:55.87ms
step:1797/2160 train_time:100439ms step_avg:55.89ms
step:1798/2160 train_time:100527ms step_avg:55.91ms
step:1799/2160 train_time:100616ms step_avg:55.93ms
step:1800/2160 train_time:100704ms step_avg:55.95ms
step:1801/2160 train_time:100794ms step_avg:55.97ms
step:1802/2160 train_time:100881ms step_avg:55.98ms
step:1803/2160 train_time:100970ms step_avg:56.00ms
step:1804/2160 train_time:101056ms step_avg:56.02ms
step:1805/2160 train_time:101145ms step_avg:56.04ms
step:1806/2160 train_time:101232ms step_avg:56.05ms
step:1807/2160 train_time:101321ms step_avg:56.07ms
step:1808/2160 train_time:101408ms step_avg:56.09ms
step:1809/2160 train_time:101497ms step_avg:56.11ms
step:1810/2160 train_time:101586ms step_avg:56.12ms
step:1811/2160 train_time:101675ms step_avg:56.14ms
step:1812/2160 train_time:101763ms step_avg:56.16ms
step:1813/2160 train_time:101852ms step_avg:56.18ms
step:1814/2160 train_time:101939ms step_avg:56.20ms
step:1815/2160 train_time:102028ms step_avg:56.21ms
step:1816/2160 train_time:102115ms step_avg:56.23ms
step:1817/2160 train_time:102205ms step_avg:56.25ms
step:1818/2160 train_time:102293ms step_avg:56.27ms
step:1819/2160 train_time:102382ms step_avg:56.28ms
step:1820/2160 train_time:102469ms step_avg:56.30ms
step:1821/2160 train_time:102559ms step_avg:56.32ms
step:1822/2160 train_time:102647ms step_avg:56.34ms
step:1823/2160 train_time:102737ms step_avg:56.36ms
step:1824/2160 train_time:102825ms step_avg:56.37ms
step:1825/2160 train_time:102914ms step_avg:56.39ms
step:1826/2160 train_time:103001ms step_avg:56.41ms
step:1827/2160 train_time:103091ms step_avg:56.43ms
step:1828/2160 train_time:103178ms step_avg:56.44ms
step:1829/2160 train_time:103268ms step_avg:56.46ms
step:1830/2160 train_time:103356ms step_avg:56.48ms
step:1831/2160 train_time:103445ms step_avg:56.50ms
step:1832/2160 train_time:103532ms step_avg:56.51ms
step:1833/2160 train_time:103622ms step_avg:56.53ms
step:1834/2160 train_time:103710ms step_avg:56.55ms
step:1835/2160 train_time:103799ms step_avg:56.57ms
step:1836/2160 train_time:103887ms step_avg:56.58ms
step:1837/2160 train_time:103975ms step_avg:56.60ms
step:1838/2160 train_time:104063ms step_avg:56.62ms
step:1839/2160 train_time:104152ms step_avg:56.63ms
step:1840/2160 train_time:104239ms step_avg:56.65ms
step:1841/2160 train_time:104329ms step_avg:56.67ms
step:1842/2160 train_time:104416ms step_avg:56.69ms
step:1843/2160 train_time:104506ms step_avg:56.70ms
step:1844/2160 train_time:104594ms step_avg:56.72ms
step:1845/2160 train_time:104683ms step_avg:56.74ms
step:1846/2160 train_time:104770ms step_avg:56.76ms
step:1847/2160 train_time:104859ms step_avg:56.77ms
step:1848/2160 train_time:104947ms step_avg:56.79ms
step:1849/2160 train_time:105036ms step_avg:56.81ms
step:1850/2160 train_time:105122ms step_avg:56.82ms
step:1851/2160 train_time:105211ms step_avg:56.84ms
step:1852/2160 train_time:105298ms step_avg:56.86ms
step:1853/2160 train_time:105388ms step_avg:56.87ms
step:1854/2160 train_time:105476ms step_avg:56.89ms
step:1855/2160 train_time:105565ms step_avg:56.91ms
step:1856/2160 train_time:105652ms step_avg:56.92ms
step:1857/2160 train_time:105741ms step_avg:56.94ms
step:1858/2160 train_time:105829ms step_avg:56.96ms
step:1859/2160 train_time:105918ms step_avg:56.98ms
step:1860/2160 train_time:106005ms step_avg:56.99ms
step:1861/2160 train_time:106094ms step_avg:57.01ms
step:1862/2160 train_time:106181ms step_avg:57.03ms
step:1863/2160 train_time:106270ms step_avg:57.04ms
step:1864/2160 train_time:106358ms step_avg:57.06ms
step:1865/2160 train_time:106447ms step_avg:57.08ms
step:1866/2160 train_time:106535ms step_avg:57.09ms
step:1867/2160 train_time:106624ms step_avg:57.11ms
step:1868/2160 train_time:106712ms step_avg:57.13ms
step:1869/2160 train_time:106800ms step_avg:57.14ms
step:1870/2160 train_time:106888ms step_avg:57.16ms
step:1871/2160 train_time:106978ms step_avg:57.18ms
step:1872/2160 train_time:107065ms step_avg:57.19ms
step:1873/2160 train_time:107154ms step_avg:57.21ms
step:1874/2160 train_time:107241ms step_avg:57.23ms
step:1875/2160 train_time:107330ms step_avg:57.24ms
step:1876/2160 train_time:107417ms step_avg:57.26ms
step:1877/2160 train_time:107506ms step_avg:57.28ms
step:1878/2160 train_time:107594ms step_avg:57.29ms
step:1879/2160 train_time:107682ms step_avg:57.31ms
step:1880/2160 train_time:107769ms step_avg:57.32ms
step:1881/2160 train_time:107859ms step_avg:57.34ms
step:1882/2160 train_time:107947ms step_avg:57.36ms
step:1883/2160 train_time:108035ms step_avg:57.37ms
step:1884/2160 train_time:108122ms step_avg:57.39ms
step:1885/2160 train_time:108213ms step_avg:57.41ms
step:1886/2160 train_time:108300ms step_avg:57.42ms
step:1887/2160 train_time:108389ms step_avg:57.44ms
step:1888/2160 train_time:108477ms step_avg:57.46ms
step:1889/2160 train_time:108567ms step_avg:57.47ms
step:1890/2160 train_time:108654ms step_avg:57.49ms
step:1891/2160 train_time:108744ms step_avg:57.51ms
step:1892/2160 train_time:108832ms step_avg:57.52ms
step:1893/2160 train_time:108921ms step_avg:57.54ms
step:1894/2160 train_time:109009ms step_avg:57.55ms
step:1895/2160 train_time:109098ms step_avg:57.57ms
step:1896/2160 train_time:109185ms step_avg:57.59ms
step:1897/2160 train_time:109277ms step_avg:57.61ms
step:1898/2160 train_time:109365ms step_avg:57.62ms
step:1899/2160 train_time:109455ms step_avg:57.64ms
step:1900/2160 train_time:109542ms step_avg:57.65ms
step:1901/2160 train_time:109632ms step_avg:57.67ms
step:1902/2160 train_time:109719ms step_avg:57.69ms
step:1903/2160 train_time:109809ms step_avg:57.70ms
step:1904/2160 train_time:109896ms step_avg:57.72ms
step:1905/2160 train_time:109986ms step_avg:57.74ms
step:1906/2160 train_time:110073ms step_avg:57.75ms
step:1907/2160 train_time:110162ms step_avg:57.77ms
step:1908/2160 train_time:110250ms step_avg:57.78ms
step:1909/2160 train_time:110338ms step_avg:57.80ms
step:1910/2160 train_time:110426ms step_avg:57.81ms
step:1911/2160 train_time:110515ms step_avg:57.83ms
step:1912/2160 train_time:110603ms step_avg:57.85ms
step:1913/2160 train_time:110692ms step_avg:57.86ms
step:1914/2160 train_time:110779ms step_avg:57.88ms
step:1915/2160 train_time:110868ms step_avg:57.89ms
step:1916/2160 train_time:110955ms step_avg:57.91ms
step:1917/2160 train_time:111045ms step_avg:57.93ms
step:1918/2160 train_time:111132ms step_avg:57.94ms
step:1919/2160 train_time:111221ms step_avg:57.96ms
step:1920/2160 train_time:111308ms step_avg:57.97ms
step:1921/2160 train_time:111397ms step_avg:57.99ms
step:1922/2160 train_time:111485ms step_avg:58.00ms
step:1923/2160 train_time:111574ms step_avg:58.02ms
step:1924/2160 train_time:111662ms step_avg:58.04ms
step:1925/2160 train_time:111751ms step_avg:58.05ms
step:1926/2160 train_time:111838ms step_avg:58.07ms
step:1927/2160 train_time:111927ms step_avg:58.08ms
step:1928/2160 train_time:112015ms step_avg:58.10ms
step:1929/2160 train_time:112104ms step_avg:58.12ms
step:1930/2160 train_time:112192ms step_avg:58.13ms
step:1931/2160 train_time:112281ms step_avg:58.15ms
step:1932/2160 train_time:112368ms step_avg:58.16ms
step:1933/2160 train_time:112458ms step_avg:58.18ms
step:1934/2160 train_time:112545ms step_avg:58.19ms
step:1935/2160 train_time:112634ms step_avg:58.21ms
step:1936/2160 train_time:112721ms step_avg:58.22ms
step:1937/2160 train_time:112811ms step_avg:58.24ms
step:1938/2160 train_time:112898ms step_avg:58.25ms
step:1939/2160 train_time:112988ms step_avg:58.27ms
step:1940/2160 train_time:113075ms step_avg:58.29ms
step:1941/2160 train_time:113164ms step_avg:58.30ms
step:1942/2160 train_time:113251ms step_avg:58.32ms
step:1943/2160 train_time:113340ms step_avg:58.33ms
step:1944/2160 train_time:113427ms step_avg:58.35ms
step:1945/2160 train_time:113518ms step_avg:58.36ms
step:1946/2160 train_time:113605ms step_avg:58.38ms
step:1947/2160 train_time:113695ms step_avg:58.39ms
step:1948/2160 train_time:113781ms step_avg:58.41ms
step:1949/2160 train_time:113871ms step_avg:58.43ms
step:1950/2160 train_time:113959ms step_avg:58.44ms
step:1951/2160 train_time:114049ms step_avg:58.46ms
step:1952/2160 train_time:114137ms step_avg:58.47ms
step:1953/2160 train_time:114227ms step_avg:58.49ms
step:1954/2160 train_time:114314ms step_avg:58.50ms
step:1955/2160 train_time:114404ms step_avg:58.52ms
step:1956/2160 train_time:114491ms step_avg:58.53ms
step:1957/2160 train_time:114580ms step_avg:58.55ms
step:1958/2160 train_time:114667ms step_avg:58.56ms
step:1959/2160 train_time:114756ms step_avg:58.58ms
step:1960/2160 train_time:114843ms step_avg:58.59ms
step:1961/2160 train_time:114933ms step_avg:58.61ms
step:1962/2160 train_time:115020ms step_avg:58.62ms
step:1963/2160 train_time:115109ms step_avg:58.64ms
step:1964/2160 train_time:115197ms step_avg:58.65ms
step:1965/2160 train_time:115286ms step_avg:58.67ms
step:1966/2160 train_time:115373ms step_avg:58.68ms
step:1967/2160 train_time:115463ms step_avg:58.70ms
step:1968/2160 train_time:115550ms step_avg:58.71ms
step:1969/2160 train_time:115640ms step_avg:58.73ms
step:1970/2160 train_time:115727ms step_avg:58.74ms
step:1971/2160 train_time:115817ms step_avg:58.76ms
step:1972/2160 train_time:115906ms step_avg:58.78ms
step:1973/2160 train_time:115994ms step_avg:58.79ms
step:1974/2160 train_time:116082ms step_avg:58.81ms
step:1975/2160 train_time:116171ms step_avg:58.82ms
step:1976/2160 train_time:116258ms step_avg:58.83ms
step:1977/2160 train_time:116347ms step_avg:58.85ms
step:1978/2160 train_time:116433ms step_avg:58.86ms
step:1979/2160 train_time:116523ms step_avg:58.88ms
step:1980/2160 train_time:116610ms step_avg:58.89ms
step:1981/2160 train_time:116699ms step_avg:58.91ms
step:1982/2160 train_time:116786ms step_avg:58.92ms
step:1983/2160 train_time:116876ms step_avg:58.94ms
step:1984/2160 train_time:116963ms step_avg:58.95ms
step:1985/2160 train_time:117052ms step_avg:58.97ms
step:1986/2160 train_time:117140ms step_avg:58.98ms
step:1987/2160 train_time:117228ms step_avg:59.00ms
step:1988/2160 train_time:117315ms step_avg:59.01ms
step:1989/2160 train_time:117405ms step_avg:59.03ms
step:1990/2160 train_time:117491ms step_avg:59.04ms
step:1991/2160 train_time:117581ms step_avg:59.06ms
step:1992/2160 train_time:117668ms step_avg:59.07ms
step:1993/2160 train_time:117756ms step_avg:59.08ms
step:1994/2160 train_time:117844ms step_avg:59.10ms
step:1995/2160 train_time:117934ms step_avg:59.11ms
step:1996/2160 train_time:118021ms step_avg:59.13ms
step:1997/2160 train_time:118110ms step_avg:59.14ms
step:1998/2160 train_time:118197ms step_avg:59.16ms
step:1999/2160 train_time:118286ms step_avg:59.17ms
step:2000/2160 train_time:118374ms step_avg:59.19ms
step:2000/2160 val_loss:3.3157 train_time:118464ms step_avg:59.23ms
step:2001/2160 train_time:118484ms step_avg:59.21ms
step:2002/2160 train_time:118553ms step_avg:59.22ms
step:2003/2160 train_time:118648ms step_avg:59.23ms
step:2004/2160 train_time:118735ms step_avg:59.25ms
step:2005/2160 train_time:118823ms step_avg:59.26ms
step:2006/2160 train_time:118910ms step_avg:59.28ms
step:2007/2160 train_time:118998ms step_avg:59.29ms
step:2008/2160 train_time:119086ms step_avg:59.31ms
step:2009/2160 train_time:119177ms step_avg:59.32ms
step:2010/2160 train_time:119264ms step_avg:59.34ms
step:2011/2160 train_time:119352ms step_avg:59.35ms
step:2012/2160 train_time:119440ms step_avg:59.36ms
step:2013/2160 train_time:119532ms step_avg:59.38ms
step:2014/2160 train_time:119620ms step_avg:59.39ms
step:2015/2160 train_time:119710ms step_avg:59.41ms
step:2016/2160 train_time:119798ms step_avg:59.42ms
step:2017/2160 train_time:119886ms step_avg:59.44ms
step:2018/2160 train_time:119972ms step_avg:59.45ms
step:2019/2160 train_time:120062ms step_avg:59.47ms
step:2020/2160 train_time:120149ms step_avg:59.48ms
step:2021/2160 train_time:120237ms step_avg:59.49ms
step:2022/2160 train_time:120325ms step_avg:59.51ms
step:2023/2160 train_time:120415ms step_avg:59.52ms
step:2024/2160 train_time:120503ms step_avg:59.54ms
step:2025/2160 train_time:120594ms step_avg:59.55ms
step:2026/2160 train_time:120681ms step_avg:59.57ms
step:2027/2160 train_time:120771ms step_avg:59.58ms
step:2028/2160 train_time:120858ms step_avg:59.59ms
step:2029/2160 train_time:120947ms step_avg:59.61ms
step:2030/2160 train_time:121034ms step_avg:59.62ms
step:2031/2160 train_time:121123ms step_avg:59.64ms
step:2032/2160 train_time:121210ms step_avg:59.65ms
step:2033/2160 train_time:121299ms step_avg:59.67ms
step:2034/2160 train_time:121387ms step_avg:59.68ms
step:2035/2160 train_time:121476ms step_avg:59.69ms
step:2036/2160 train_time:121564ms step_avg:59.71ms
step:2037/2160 train_time:121654ms step_avg:59.72ms
step:2038/2160 train_time:121742ms step_avg:59.74ms
step:2039/2160 train_time:121831ms step_avg:59.75ms
step:2040/2160 train_time:121918ms step_avg:59.76ms
step:2041/2160 train_time:122008ms step_avg:59.78ms
step:2042/2160 train_time:122095ms step_avg:59.79ms
step:2043/2160 train_time:122184ms step_avg:59.81ms
step:2044/2160 train_time:122271ms step_avg:59.82ms
step:2045/2160 train_time:122360ms step_avg:59.83ms
step:2046/2160 train_time:122447ms step_avg:59.85ms
step:2047/2160 train_time:122536ms step_avg:59.86ms
step:2048/2160 train_time:122624ms step_avg:59.88ms
step:2049/2160 train_time:122713ms step_avg:59.89ms
step:2050/2160 train_time:122801ms step_avg:59.90ms
step:2051/2160 train_time:122889ms step_avg:59.92ms
step:2052/2160 train_time:122977ms step_avg:59.93ms
step:2053/2160 train_time:123066ms step_avg:59.94ms
step:2054/2160 train_time:123153ms step_avg:59.96ms
step:2055/2160 train_time:123242ms step_avg:59.97ms
step:2056/2160 train_time:123329ms step_avg:59.99ms
step:2057/2160 train_time:123418ms step_avg:60.00ms
step:2058/2160 train_time:123506ms step_avg:60.01ms
step:2059/2160 train_time:123596ms step_avg:60.03ms
step:2060/2160 train_time:123683ms step_avg:60.04ms
step:2061/2160 train_time:123773ms step_avg:60.05ms
step:2062/2160 train_time:123860ms step_avg:60.07ms
step:2063/2160 train_time:123950ms step_avg:60.08ms
step:2064/2160 train_time:124037ms step_avg:60.10ms
step:2065/2160 train_time:124126ms step_avg:60.11ms
step:2066/2160 train_time:124213ms step_avg:60.12ms
step:2067/2160 train_time:124303ms step_avg:60.14ms
step:2068/2160 train_time:124391ms step_avg:60.15ms
step:2069/2160 train_time:124480ms step_avg:60.16ms
step:2070/2160 train_time:124568ms step_avg:60.18ms
step:2071/2160 train_time:124658ms step_avg:60.19ms
step:2072/2160 train_time:124746ms step_avg:60.21ms
step:2073/2160 train_time:124835ms step_avg:60.22ms
step:2074/2160 train_time:124923ms step_avg:60.23ms
step:2075/2160 train_time:125013ms step_avg:60.25ms
step:2076/2160 train_time:125101ms step_avg:60.26ms
step:2077/2160 train_time:125190ms step_avg:60.27ms
step:2078/2160 train_time:125277ms step_avg:60.29ms
step:2079/2160 train_time:125366ms step_avg:60.30ms
step:2080/2160 train_time:125453ms step_avg:60.31ms
step:2081/2160 train_time:125544ms step_avg:60.33ms
step:2082/2160 train_time:125631ms step_avg:60.34ms
step:2083/2160 train_time:125720ms step_avg:60.36ms
step:2084/2160 train_time:125808ms step_avg:60.37ms
step:2085/2160 train_time:125898ms step_avg:60.38ms
step:2086/2160 train_time:125985ms step_avg:60.40ms
step:2087/2160 train_time:126074ms step_avg:60.41ms
step:2088/2160 train_time:126162ms step_avg:60.42ms
step:2089/2160 train_time:126250ms step_avg:60.44ms
step:2090/2160 train_time:126337ms step_avg:60.45ms
step:2091/2160 train_time:126427ms step_avg:60.46ms
step:2092/2160 train_time:126515ms step_avg:60.48ms
step:2093/2160 train_time:126605ms step_avg:60.49ms
step:2094/2160 train_time:126692ms step_avg:60.50ms
step:2095/2160 train_time:126781ms step_avg:60.52ms
step:2096/2160 train_time:126869ms step_avg:60.53ms
step:2097/2160 train_time:126958ms step_avg:60.54ms
step:2098/2160 train_time:127045ms step_avg:60.56ms
step:2099/2160 train_time:127134ms step_avg:60.57ms
step:2100/2160 train_time:127221ms step_avg:60.58ms
step:2101/2160 train_time:127310ms step_avg:60.59ms
step:2102/2160 train_time:127397ms step_avg:60.61ms
step:2103/2160 train_time:127486ms step_avg:60.62ms
step:2104/2160 train_time:127573ms step_avg:60.63ms
step:2105/2160 train_time:127663ms step_avg:60.65ms
step:2106/2160 train_time:127751ms step_avg:60.66ms
step:2107/2160 train_time:127840ms step_avg:60.67ms
step:2108/2160 train_time:127928ms step_avg:60.69ms
step:2109/2160 train_time:128017ms step_avg:60.70ms
step:2110/2160 train_time:128105ms step_avg:60.71ms
step:2111/2160 train_time:128195ms step_avg:60.73ms
step:2112/2160 train_time:128282ms step_avg:60.74ms
step:2113/2160 train_time:128371ms step_avg:60.75ms
step:2114/2160 train_time:128458ms step_avg:60.77ms
step:2115/2160 train_time:128547ms step_avg:60.78ms
step:2116/2160 train_time:128636ms step_avg:60.79ms
step:2117/2160 train_time:128725ms step_avg:60.81ms
step:2118/2160 train_time:128814ms step_avg:60.82ms
step:2119/2160 train_time:128904ms step_avg:60.83ms
step:2120/2160 train_time:128991ms step_avg:60.84ms
step:2121/2160 train_time:129080ms step_avg:60.86ms
step:2122/2160 train_time:129168ms step_avg:60.87ms
step:2123/2160 train_time:129257ms step_avg:60.88ms
step:2124/2160 train_time:129345ms step_avg:60.90ms
step:2125/2160 train_time:129434ms step_avg:60.91ms
step:2126/2160 train_time:129522ms step_avg:60.92ms
step:2127/2160 train_time:129611ms step_avg:60.94ms
step:2128/2160 train_time:129700ms step_avg:60.95ms
step:2129/2160 train_time:129790ms step_avg:60.96ms
step:2130/2160 train_time:129877ms step_avg:60.97ms
step:2131/2160 train_time:129966ms step_avg:60.99ms
step:2132/2160 train_time:130054ms step_avg:61.00ms
step:2133/2160 train_time:130143ms step_avg:61.01ms
step:2134/2160 train_time:130231ms step_avg:61.03ms
step:2135/2160 train_time:130320ms step_avg:61.04ms
step:2136/2160 train_time:130408ms step_avg:61.05ms
step:2137/2160 train_time:130497ms step_avg:61.07ms
step:2138/2160 train_time:130585ms step_avg:61.08ms
step:2139/2160 train_time:130675ms step_avg:61.09ms
step:2140/2160 train_time:130763ms step_avg:61.10ms
step:2141/2160 train_time:130853ms step_avg:61.12ms
step:2142/2160 train_time:130941ms step_avg:61.13ms
step:2143/2160 train_time:131030ms step_avg:61.14ms
step:2144/2160 train_time:131117ms step_avg:61.16ms
step:2145/2160 train_time:131206ms step_avg:61.17ms
step:2146/2160 train_time:131294ms step_avg:61.18ms
step:2147/2160 train_time:131384ms step_avg:61.19ms
step:2148/2160 train_time:131471ms step_avg:61.21ms
step:2149/2160 train_time:131561ms step_avg:61.22ms
step:2150/2160 train_time:131648ms step_avg:61.23ms
step:2151/2160 train_time:131738ms step_avg:61.24ms
step:2152/2160 train_time:131826ms step_avg:61.26ms
step:2153/2160 train_time:131916ms step_avg:61.27ms
step:2154/2160 train_time:132004ms step_avg:61.28ms
step:2155/2160 train_time:132094ms step_avg:61.30ms
step:2156/2160 train_time:132181ms step_avg:61.31ms
step:2157/2160 train_time:132271ms step_avg:61.32ms
step:2158/2160 train_time:132359ms step_avg:61.33ms
step:2159/2160 train_time:132448ms step_avg:61.35ms
step:2160/2160 train_time:132537ms step_avg:61.36ms
step:2160/2160 val_loss:3.2795 train_time:132627ms step_avg:61.40ms
peak memory allocated: 29896 MiB reserved: 61774 MiB

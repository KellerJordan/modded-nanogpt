import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = (16 / 8) * 0.8
    if x > 0.66:
        lr_max = (24 / 8) * 0.8
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sun Nov 30 02:31:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            122W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0            108W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   35C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:89ms step_avg:88.88ms
step:2/2160 train_time:131ms step_avg:65.68ms
step:3/2160 train_time:152ms step_avg:50.72ms
step:4/2160 train_time:181ms step_avg:45.13ms
step:5/2160 train_time:214ms step_avg:42.87ms
step:6/2160 train_time:276ms step_avg:46.04ms
step:7/2160 train_time:301ms step_avg:42.97ms
step:8/2160 train_time:334ms step_avg:41.70ms
step:9/2160 train_time:367ms step_avg:40.82ms
step:10/2160 train_time:400ms step_avg:40.03ms
step:11/2160 train_time:435ms step_avg:39.52ms
step:12/2160 train_time:467ms step_avg:38.94ms
step:13/2160 train_time:502ms step_avg:38.59ms
step:14/2160 train_time:535ms step_avg:38.19ms
step:15/2160 train_time:569ms step_avg:37.93ms
step:16/2160 train_time:602ms step_avg:37.63ms
step:17/2160 train_time:636ms step_avg:37.42ms
step:18/2160 train_time:669ms step_avg:37.18ms
step:19/2160 train_time:704ms step_avg:37.03ms
step:20/2160 train_time:737ms step_avg:36.84ms
step:21/2160 train_time:771ms step_avg:36.71ms
step:22/2160 train_time:804ms step_avg:36.54ms
step:23/2160 train_time:838ms step_avg:36.43ms
step:24/2160 train_time:871ms step_avg:36.29ms
step:25/2160 train_time:905ms step_avg:36.20ms
step:26/2160 train_time:938ms step_avg:36.08ms
step:27/2160 train_time:972ms step_avg:36.01ms
step:28/2160 train_time:1005ms step_avg:35.91ms
step:29/2160 train_time:1040ms step_avg:35.85ms
step:30/2160 train_time:1073ms step_avg:35.76ms
step:31/2160 train_time:1107ms step_avg:35.70ms
step:32/2160 train_time:1140ms step_avg:35.63ms
step:33/2160 train_time:1174ms step_avg:35.57ms
step:34/2160 train_time:1207ms step_avg:35.51ms
step:35/2160 train_time:1242ms step_avg:35.48ms
step:36/2160 train_time:1275ms step_avg:35.43ms
step:37/2160 train_time:1310ms step_avg:35.40ms
step:38/2160 train_time:1343ms step_avg:35.35ms
step:39/2160 train_time:1378ms step_avg:35.33ms
step:40/2160 train_time:1411ms step_avg:35.27ms
step:41/2160 train_time:1445ms step_avg:35.25ms
step:42/2160 train_time:1478ms step_avg:35.20ms
step:43/2160 train_time:1512ms step_avg:35.17ms
step:44/2160 train_time:1545ms step_avg:35.12ms
step:45/2160 train_time:1580ms step_avg:35.10ms
step:46/2160 train_time:1613ms step_avg:35.06ms
step:47/2160 train_time:1647ms step_avg:35.04ms
step:48/2160 train_time:1680ms step_avg:35.01ms
step:49/2160 train_time:1714ms step_avg:34.98ms
step:50/2160 train_time:1747ms step_avg:34.94ms
step:51/2160 train_time:1782ms step_avg:34.94ms
step:52/2160 train_time:1815ms step_avg:34.90ms
step:53/2160 train_time:1849ms step_avg:34.89ms
step:54/2160 train_time:1882ms step_avg:34.85ms
step:55/2160 train_time:1916ms step_avg:34.84ms
step:56/2160 train_time:1949ms step_avg:34.81ms
step:57/2160 train_time:1984ms step_avg:34.80ms
step:58/2160 train_time:2017ms step_avg:34.78ms
step:59/2160 train_time:2051ms step_avg:34.76ms
step:60/2160 train_time:2084ms step_avg:34.73ms
step:61/2160 train_time:2118ms step_avg:34.72ms
step:62/2160 train_time:2151ms step_avg:34.70ms
step:63/2160 train_time:2186ms step_avg:34.69ms
step:64/2160 train_time:2219ms step_avg:34.67ms
step:65/2160 train_time:2253ms step_avg:34.66ms
step:66/2160 train_time:2286ms step_avg:34.64ms
step:67/2160 train_time:2320ms step_avg:34.63ms
step:68/2160 train_time:2354ms step_avg:34.61ms
step:69/2160 train_time:2388ms step_avg:34.61ms
step:70/2160 train_time:2421ms step_avg:34.59ms
step:71/2160 train_time:2455ms step_avg:34.58ms
step:72/2160 train_time:2489ms step_avg:34.56ms
step:73/2160 train_time:2523ms step_avg:34.56ms
step:74/2160 train_time:2556ms step_avg:34.54ms
step:75/2160 train_time:2590ms step_avg:34.54ms
step:76/2160 train_time:2623ms step_avg:34.52ms
step:77/2160 train_time:2657ms step_avg:34.51ms
step:78/2160 train_time:2691ms step_avg:34.49ms
step:79/2160 train_time:2725ms step_avg:34.49ms
step:80/2160 train_time:2758ms step_avg:34.47ms
step:81/2160 train_time:2792ms step_avg:34.47ms
step:82/2160 train_time:2825ms step_avg:34.45ms
step:83/2160 train_time:2859ms step_avg:34.45ms
step:84/2160 train_time:2893ms step_avg:34.44ms
step:85/2160 train_time:2927ms step_avg:34.43ms
step:86/2160 train_time:2960ms step_avg:34.41ms
step:87/2160 train_time:2994ms step_avg:34.41ms
step:88/2160 train_time:3027ms step_avg:34.39ms
step:89/2160 train_time:3061ms step_avg:34.40ms
step:90/2160 train_time:3095ms step_avg:34.39ms
step:91/2160 train_time:3129ms step_avg:34.38ms
step:92/2160 train_time:3162ms step_avg:34.37ms
step:93/2160 train_time:3196ms step_avg:34.37ms
step:94/2160 train_time:3229ms step_avg:34.35ms
step:95/2160 train_time:3264ms step_avg:34.36ms
step:96/2160 train_time:3297ms step_avg:34.34ms
step:97/2160 train_time:3332ms step_avg:34.35ms
step:98/2160 train_time:3365ms step_avg:34.33ms
step:99/2160 train_time:3399ms step_avg:34.33ms
step:100/2160 train_time:3432ms step_avg:34.32ms
step:101/2160 train_time:3466ms step_avg:34.32ms
step:102/2160 train_time:3499ms step_avg:34.31ms
step:103/2160 train_time:3533ms step_avg:34.31ms
step:104/2160 train_time:3567ms step_avg:34.30ms
step:105/2160 train_time:3601ms step_avg:34.29ms
step:106/2160 train_time:3634ms step_avg:34.28ms
step:107/2160 train_time:3668ms step_avg:34.28ms
step:108/2160 train_time:3701ms step_avg:34.27ms
step:109/2160 train_time:3735ms step_avg:34.27ms
step:110/2160 train_time:3768ms step_avg:34.26ms
step:111/2160 train_time:3803ms step_avg:34.26ms
step:112/2160 train_time:3836ms step_avg:34.25ms
step:113/2160 train_time:3870ms step_avg:34.25ms
step:114/2160 train_time:3903ms step_avg:34.24ms
step:115/2160 train_time:3937ms step_avg:34.24ms
step:116/2160 train_time:3971ms step_avg:34.23ms
step:117/2160 train_time:4005ms step_avg:34.23ms
step:118/2160 train_time:4038ms step_avg:34.22ms
step:119/2160 train_time:4072ms step_avg:34.22ms
step:120/2160 train_time:4105ms step_avg:34.21ms
step:121/2160 train_time:4140ms step_avg:34.21ms
step:122/2160 train_time:4173ms step_avg:34.20ms
step:123/2160 train_time:4207ms step_avg:34.20ms
step:124/2160 train_time:4240ms step_avg:34.19ms
step:125/2160 train_time:4274ms step_avg:34.19ms
step:126/2160 train_time:4308ms step_avg:34.19ms
step:127/2160 train_time:4342ms step_avg:34.19ms
step:128/2160 train_time:4375ms step_avg:34.18ms
step:129/2160 train_time:4409ms step_avg:34.18ms
step:130/2160 train_time:4442ms step_avg:34.17ms
step:131/2160 train_time:4476ms step_avg:34.17ms
step:132/2160 train_time:4509ms step_avg:34.16ms
step:133/2160 train_time:4543ms step_avg:34.16ms
step:134/2160 train_time:4576ms step_avg:34.15ms
step:135/2160 train_time:4610ms step_avg:34.15ms
step:136/2160 train_time:4643ms step_avg:34.14ms
step:137/2160 train_time:4678ms step_avg:34.14ms
step:138/2160 train_time:4711ms step_avg:34.13ms
step:139/2160 train_time:4745ms step_avg:34.14ms
step:140/2160 train_time:4778ms step_avg:34.13ms
step:141/2160 train_time:4812ms step_avg:34.13ms
step:142/2160 train_time:4845ms step_avg:34.12ms
step:143/2160 train_time:4879ms step_avg:34.12ms
step:144/2160 train_time:4912ms step_avg:34.11ms
step:145/2160 train_time:4946ms step_avg:34.11ms
step:146/2160 train_time:4979ms step_avg:34.10ms
step:147/2160 train_time:5013ms step_avg:34.10ms
step:148/2160 train_time:5046ms step_avg:34.10ms
step:149/2160 train_time:5080ms step_avg:34.10ms
step:150/2160 train_time:5114ms step_avg:34.09ms
step:151/2160 train_time:5148ms step_avg:34.09ms
step:152/2160 train_time:5181ms step_avg:34.09ms
step:153/2160 train_time:5215ms step_avg:34.09ms
step:154/2160 train_time:5248ms step_avg:34.08ms
step:155/2160 train_time:5282ms step_avg:34.08ms
step:156/2160 train_time:5316ms step_avg:34.08ms
step:157/2160 train_time:5350ms step_avg:34.08ms
step:158/2160 train_time:5383ms step_avg:34.07ms
step:159/2160 train_time:5417ms step_avg:34.07ms
step:160/2160 train_time:5450ms step_avg:34.06ms
step:161/2160 train_time:5484ms step_avg:34.06ms
step:162/2160 train_time:5518ms step_avg:34.06ms
step:163/2160 train_time:5552ms step_avg:34.06ms
step:164/2160 train_time:5585ms step_avg:34.05ms
step:165/2160 train_time:5619ms step_avg:34.05ms
step:166/2160 train_time:5652ms step_avg:34.05ms
step:167/2160 train_time:5686ms step_avg:34.05ms
step:168/2160 train_time:5719ms step_avg:34.04ms
step:169/2160 train_time:5753ms step_avg:34.04ms
step:170/2160 train_time:5786ms step_avg:34.03ms
step:171/2160 train_time:5820ms step_avg:34.04ms
step:172/2160 train_time:5853ms step_avg:34.03ms
step:173/2160 train_time:5887ms step_avg:34.03ms
step:174/2160 train_time:5921ms step_avg:34.03ms
step:175/2160 train_time:5954ms step_avg:34.02ms
step:176/2160 train_time:5987ms step_avg:34.02ms
step:177/2160 train_time:6021ms step_avg:34.02ms
step:178/2160 train_time:6055ms step_avg:34.02ms
step:179/2160 train_time:6089ms step_avg:34.01ms
step:180/2160 train_time:6122ms step_avg:34.01ms
step:181/2160 train_time:6156ms step_avg:34.01ms
step:182/2160 train_time:6189ms step_avg:34.00ms
step:183/2160 train_time:6223ms step_avg:34.01ms
step:184/2160 train_time:6257ms step_avg:34.00ms
step:185/2160 train_time:6291ms step_avg:34.00ms
step:186/2160 train_time:6324ms step_avg:34.00ms
step:187/2160 train_time:6358ms step_avg:34.00ms
step:188/2160 train_time:6391ms step_avg:34.00ms
step:189/2160 train_time:6425ms step_avg:34.00ms
step:190/2160 train_time:6459ms step_avg:33.99ms
step:191/2160 train_time:6492ms step_avg:33.99ms
step:192/2160 train_time:6526ms step_avg:33.99ms
step:193/2160 train_time:6560ms step_avg:33.99ms
step:194/2160 train_time:6593ms step_avg:33.98ms
step:195/2160 train_time:6627ms step_avg:33.98ms
step:196/2160 train_time:6660ms step_avg:33.98ms
step:197/2160 train_time:6694ms step_avg:33.98ms
step:198/2160 train_time:6728ms step_avg:33.98ms
step:199/2160 train_time:6762ms step_avg:33.98ms
step:200/2160 train_time:6795ms step_avg:33.97ms
step:201/2160 train_time:6828ms step_avg:33.97ms
step:202/2160 train_time:6861ms step_avg:33.97ms
step:203/2160 train_time:6895ms step_avg:33.97ms
step:204/2160 train_time:6928ms step_avg:33.96ms
step:205/2160 train_time:6963ms step_avg:33.96ms
step:206/2160 train_time:6996ms step_avg:33.96ms
step:207/2160 train_time:7030ms step_avg:33.96ms
step:208/2160 train_time:7062ms step_avg:33.95ms
step:209/2160 train_time:7097ms step_avg:33.96ms
step:210/2160 train_time:7130ms step_avg:33.95ms
step:211/2160 train_time:7164ms step_avg:33.95ms
step:212/2160 train_time:7197ms step_avg:33.95ms
step:213/2160 train_time:7231ms step_avg:33.95ms
step:214/2160 train_time:7264ms step_avg:33.94ms
step:215/2160 train_time:7298ms step_avg:33.94ms
step:216/2160 train_time:7332ms step_avg:33.94ms
step:217/2160 train_time:7366ms step_avg:33.94ms
step:218/2160 train_time:7399ms step_avg:33.94ms
step:219/2160 train_time:7433ms step_avg:33.94ms
step:220/2160 train_time:7466ms step_avg:33.94ms
step:221/2160 train_time:7500ms step_avg:33.94ms
step:222/2160 train_time:7533ms step_avg:33.93ms
step:223/2160 train_time:7567ms step_avg:33.93ms
step:224/2160 train_time:7600ms step_avg:33.93ms
step:225/2160 train_time:7634ms step_avg:33.93ms
step:226/2160 train_time:7667ms step_avg:33.93ms
step:227/2160 train_time:7701ms step_avg:33.93ms
step:228/2160 train_time:7734ms step_avg:33.92ms
step:229/2160 train_time:7768ms step_avg:33.92ms
step:230/2160 train_time:7801ms step_avg:33.92ms
step:231/2160 train_time:7835ms step_avg:33.92ms
step:232/2160 train_time:7869ms step_avg:33.92ms
step:233/2160 train_time:7903ms step_avg:33.92ms
step:234/2160 train_time:7936ms step_avg:33.92ms
step:235/2160 train_time:7970ms step_avg:33.92ms
step:236/2160 train_time:8003ms step_avg:33.91ms
step:237/2160 train_time:8037ms step_avg:33.91ms
step:238/2160 train_time:8070ms step_avg:33.91ms
step:239/2160 train_time:8104ms step_avg:33.91ms
step:240/2160 train_time:8138ms step_avg:33.91ms
step:241/2160 train_time:8172ms step_avg:33.91ms
step:242/2160 train_time:8204ms step_avg:33.90ms
step:243/2160 train_time:8239ms step_avg:33.91ms
step:244/2160 train_time:8272ms step_avg:33.90ms
step:245/2160 train_time:8307ms step_avg:33.90ms
step:246/2160 train_time:8339ms step_avg:33.90ms
step:247/2160 train_time:8373ms step_avg:33.90ms
step:248/2160 train_time:8406ms step_avg:33.90ms
step:249/2160 train_time:8440ms step_avg:33.90ms
step:250/2160 train_time:8473ms step_avg:33.89ms
step:250/2160 val_loss:4.3121 train_time:8509ms step_avg:34.04ms
step:251/2160 train_time:8531ms step_avg:33.99ms
step:252/2160 train_time:8550ms step_avg:33.93ms
step:253/2160 train_time:8578ms step_avg:33.90ms
step:254/2160 train_time:8613ms step_avg:33.91ms
step:255/2160 train_time:8650ms step_avg:33.92ms
step:256/2160 train_time:8685ms step_avg:33.93ms
step:257/2160 train_time:8721ms step_avg:33.93ms
step:258/2160 train_time:8754ms step_avg:33.93ms
step:259/2160 train_time:8788ms step_avg:33.93ms
step:260/2160 train_time:8821ms step_avg:33.93ms
step:261/2160 train_time:8856ms step_avg:33.93ms
step:262/2160 train_time:8889ms step_avg:33.93ms
step:263/2160 train_time:8922ms step_avg:33.93ms
step:264/2160 train_time:8955ms step_avg:33.92ms
step:265/2160 train_time:8989ms step_avg:33.92ms
step:266/2160 train_time:9022ms step_avg:33.92ms
step:267/2160 train_time:9056ms step_avg:33.92ms
step:268/2160 train_time:9089ms step_avg:33.91ms
step:269/2160 train_time:9123ms step_avg:33.91ms
step:270/2160 train_time:9156ms step_avg:33.91ms
step:271/2160 train_time:9189ms step_avg:33.91ms
step:272/2160 train_time:9222ms step_avg:33.90ms
step:273/2160 train_time:9256ms step_avg:33.91ms
step:274/2160 train_time:9289ms step_avg:33.90ms
step:275/2160 train_time:9324ms step_avg:33.90ms
step:276/2160 train_time:9356ms step_avg:33.90ms
step:277/2160 train_time:9390ms step_avg:33.90ms
step:278/2160 train_time:9423ms step_avg:33.89ms
step:279/2160 train_time:9457ms step_avg:33.89ms
step:280/2160 train_time:9490ms step_avg:33.89ms
step:281/2160 train_time:9524ms step_avg:33.89ms
step:282/2160 train_time:9557ms step_avg:33.89ms
step:283/2160 train_time:9592ms step_avg:33.89ms
step:284/2160 train_time:9625ms step_avg:33.89ms
step:285/2160 train_time:9660ms step_avg:33.89ms
step:286/2160 train_time:9693ms step_avg:33.89ms
step:287/2160 train_time:9728ms step_avg:33.89ms
step:288/2160 train_time:9761ms step_avg:33.89ms
step:289/2160 train_time:9795ms step_avg:33.89ms
step:290/2160 train_time:9828ms step_avg:33.89ms
step:291/2160 train_time:9862ms step_avg:33.89ms
step:292/2160 train_time:9895ms step_avg:33.89ms
step:293/2160 train_time:9929ms step_avg:33.89ms
step:294/2160 train_time:9962ms step_avg:33.89ms
step:295/2160 train_time:9997ms step_avg:33.89ms
step:296/2160 train_time:10030ms step_avg:33.88ms
step:297/2160 train_time:10064ms step_avg:33.88ms
step:298/2160 train_time:10097ms step_avg:33.88ms
step:299/2160 train_time:10131ms step_avg:33.88ms
step:300/2160 train_time:10164ms step_avg:33.88ms
step:301/2160 train_time:10198ms step_avg:33.88ms
step:302/2160 train_time:10231ms step_avg:33.88ms
step:303/2160 train_time:10265ms step_avg:33.88ms
step:304/2160 train_time:10298ms step_avg:33.87ms
step:305/2160 train_time:10331ms step_avg:33.87ms
step:306/2160 train_time:10364ms step_avg:33.87ms
step:307/2160 train_time:10398ms step_avg:33.87ms
step:308/2160 train_time:10431ms step_avg:33.87ms
step:309/2160 train_time:10466ms step_avg:33.87ms
step:310/2160 train_time:10499ms step_avg:33.87ms
step:311/2160 train_time:10533ms step_avg:33.87ms
step:312/2160 train_time:10566ms step_avg:33.87ms
step:313/2160 train_time:10600ms step_avg:33.87ms
step:314/2160 train_time:10633ms step_avg:33.86ms
step:315/2160 train_time:10667ms step_avg:33.86ms
step:316/2160 train_time:10700ms step_avg:33.86ms
step:317/2160 train_time:10734ms step_avg:33.86ms
step:318/2160 train_time:10767ms step_avg:33.86ms
step:319/2160 train_time:10802ms step_avg:33.86ms
step:320/2160 train_time:10835ms step_avg:33.86ms
step:321/2160 train_time:10869ms step_avg:33.86ms
step:322/2160 train_time:10902ms step_avg:33.86ms
step:323/2160 train_time:10936ms step_avg:33.86ms
step:324/2160 train_time:10969ms step_avg:33.85ms
step:325/2160 train_time:11003ms step_avg:33.86ms
step:326/2160 train_time:11036ms step_avg:33.85ms
step:327/2160 train_time:11070ms step_avg:33.85ms
step:328/2160 train_time:11103ms step_avg:33.85ms
step:329/2160 train_time:11138ms step_avg:33.85ms
step:330/2160 train_time:11171ms step_avg:33.85ms
step:331/2160 train_time:11205ms step_avg:33.85ms
step:332/2160 train_time:11238ms step_avg:33.85ms
step:333/2160 train_time:11272ms step_avg:33.85ms
step:334/2160 train_time:11305ms step_avg:33.85ms
step:335/2160 train_time:11339ms step_avg:33.85ms
step:336/2160 train_time:11372ms step_avg:33.85ms
step:337/2160 train_time:11406ms step_avg:33.85ms
step:338/2160 train_time:11439ms step_avg:33.84ms
step:339/2160 train_time:11474ms step_avg:33.85ms
step:340/2160 train_time:11506ms step_avg:33.84ms
step:341/2160 train_time:11541ms step_avg:33.84ms
step:342/2160 train_time:11574ms step_avg:33.84ms
step:343/2160 train_time:11608ms step_avg:33.84ms
step:344/2160 train_time:11641ms step_avg:33.84ms
step:345/2160 train_time:11675ms step_avg:33.84ms
step:346/2160 train_time:11708ms step_avg:33.84ms
step:347/2160 train_time:11742ms step_avg:33.84ms
step:348/2160 train_time:11775ms step_avg:33.84ms
step:349/2160 train_time:11809ms step_avg:33.84ms
step:350/2160 train_time:11842ms step_avg:33.83ms
step:351/2160 train_time:11876ms step_avg:33.84ms
step:352/2160 train_time:11909ms step_avg:33.83ms
step:353/2160 train_time:11944ms step_avg:33.83ms
step:354/2160 train_time:11977ms step_avg:33.83ms
step:355/2160 train_time:12011ms step_avg:33.83ms
step:356/2160 train_time:12044ms step_avg:33.83ms
step:357/2160 train_time:12078ms step_avg:33.83ms
step:358/2160 train_time:12111ms step_avg:33.83ms
step:359/2160 train_time:12145ms step_avg:33.83ms
step:360/2160 train_time:12178ms step_avg:33.83ms
step:361/2160 train_time:12213ms step_avg:33.83ms
step:362/2160 train_time:12246ms step_avg:33.83ms
step:363/2160 train_time:12280ms step_avg:33.83ms
step:364/2160 train_time:12313ms step_avg:33.83ms
step:365/2160 train_time:12347ms step_avg:33.83ms
step:366/2160 train_time:12380ms step_avg:33.83ms
step:367/2160 train_time:12415ms step_avg:33.83ms
step:368/2160 train_time:12448ms step_avg:33.82ms
step:369/2160 train_time:12482ms step_avg:33.83ms
step:370/2160 train_time:12515ms step_avg:33.82ms
step:371/2160 train_time:12549ms step_avg:33.82ms
step:372/2160 train_time:12582ms step_avg:33.82ms
step:373/2160 train_time:12617ms step_avg:33.82ms
step:374/2160 train_time:12649ms step_avg:33.82ms
step:375/2160 train_time:12684ms step_avg:33.82ms
step:376/2160 train_time:12717ms step_avg:33.82ms
step:377/2160 train_time:12751ms step_avg:33.82ms
step:378/2160 train_time:12784ms step_avg:33.82ms
step:379/2160 train_time:12818ms step_avg:33.82ms
step:380/2160 train_time:12851ms step_avg:33.82ms
step:381/2160 train_time:12885ms step_avg:33.82ms
step:382/2160 train_time:12918ms step_avg:33.82ms
step:383/2160 train_time:12952ms step_avg:33.82ms
step:384/2160 train_time:12985ms step_avg:33.82ms
step:385/2160 train_time:13019ms step_avg:33.82ms
step:386/2160 train_time:13052ms step_avg:33.81ms
step:387/2160 train_time:13086ms step_avg:33.81ms
step:388/2160 train_time:13119ms step_avg:33.81ms
step:389/2160 train_time:13153ms step_avg:33.81ms
step:390/2160 train_time:13187ms step_avg:33.81ms
step:391/2160 train_time:13221ms step_avg:33.81ms
step:392/2160 train_time:13254ms step_avg:33.81ms
step:393/2160 train_time:13288ms step_avg:33.81ms
step:394/2160 train_time:13321ms step_avg:33.81ms
step:395/2160 train_time:13355ms step_avg:33.81ms
step:396/2160 train_time:13389ms step_avg:33.81ms
step:397/2160 train_time:13423ms step_avg:33.81ms
step:398/2160 train_time:13456ms step_avg:33.81ms
step:399/2160 train_time:13490ms step_avg:33.81ms
step:400/2160 train_time:13523ms step_avg:33.81ms
step:401/2160 train_time:13557ms step_avg:33.81ms
step:402/2160 train_time:13590ms step_avg:33.81ms
step:403/2160 train_time:13624ms step_avg:33.81ms
step:404/2160 train_time:13657ms step_avg:33.80ms
step:405/2160 train_time:13691ms step_avg:33.81ms
step:406/2160 train_time:13724ms step_avg:33.80ms
step:407/2160 train_time:13759ms step_avg:33.80ms
step:408/2160 train_time:13792ms step_avg:33.80ms
step:409/2160 train_time:13825ms step_avg:33.80ms
step:410/2160 train_time:13858ms step_avg:33.80ms
step:411/2160 train_time:13893ms step_avg:33.80ms
step:412/2160 train_time:13926ms step_avg:33.80ms
step:413/2160 train_time:13960ms step_avg:33.80ms
step:414/2160 train_time:13993ms step_avg:33.80ms
step:415/2160 train_time:14027ms step_avg:33.80ms
step:416/2160 train_time:14060ms step_avg:33.80ms
step:417/2160 train_time:14094ms step_avg:33.80ms
step:418/2160 train_time:14127ms step_avg:33.80ms
step:419/2160 train_time:14161ms step_avg:33.80ms
step:420/2160 train_time:14194ms step_avg:33.80ms
step:421/2160 train_time:14228ms step_avg:33.80ms
step:422/2160 train_time:14261ms step_avg:33.79ms
step:423/2160 train_time:14295ms step_avg:33.79ms
step:424/2160 train_time:14328ms step_avg:33.79ms
step:425/2160 train_time:14362ms step_avg:33.79ms
step:426/2160 train_time:14395ms step_avg:33.79ms
step:427/2160 train_time:14430ms step_avg:33.79ms
step:428/2160 train_time:14463ms step_avg:33.79ms
step:429/2160 train_time:14497ms step_avg:33.79ms
step:430/2160 train_time:14531ms step_avg:33.79ms
step:431/2160 train_time:14565ms step_avg:33.79ms
step:432/2160 train_time:14598ms step_avg:33.79ms
step:433/2160 train_time:14632ms step_avg:33.79ms
step:434/2160 train_time:14665ms step_avg:33.79ms
step:435/2160 train_time:14699ms step_avg:33.79ms
step:436/2160 train_time:14732ms step_avg:33.79ms
step:437/2160 train_time:14766ms step_avg:33.79ms
step:438/2160 train_time:14799ms step_avg:33.79ms
step:439/2160 train_time:14833ms step_avg:33.79ms
step:440/2160 train_time:14866ms step_avg:33.79ms
step:441/2160 train_time:14900ms step_avg:33.79ms
step:442/2160 train_time:14934ms step_avg:33.79ms
step:443/2160 train_time:14968ms step_avg:33.79ms
step:444/2160 train_time:15001ms step_avg:33.79ms
step:445/2160 train_time:15035ms step_avg:33.79ms
step:446/2160 train_time:15068ms step_avg:33.79ms
step:447/2160 train_time:15102ms step_avg:33.79ms
step:448/2160 train_time:15135ms step_avg:33.78ms
step:449/2160 train_time:15169ms step_avg:33.78ms
step:450/2160 train_time:15202ms step_avg:33.78ms
step:451/2160 train_time:15236ms step_avg:33.78ms
step:452/2160 train_time:15269ms step_avg:33.78ms
step:453/2160 train_time:15303ms step_avg:33.78ms
step:454/2160 train_time:15336ms step_avg:33.78ms
step:455/2160 train_time:15370ms step_avg:33.78ms
step:456/2160 train_time:15404ms step_avg:33.78ms
step:457/2160 train_time:15437ms step_avg:33.78ms
step:458/2160 train_time:15471ms step_avg:33.78ms
step:459/2160 train_time:15505ms step_avg:33.78ms
step:460/2160 train_time:15537ms step_avg:33.78ms
step:461/2160 train_time:15571ms step_avg:33.78ms
step:462/2160 train_time:15605ms step_avg:33.78ms
step:463/2160 train_time:15639ms step_avg:33.78ms
step:464/2160 train_time:15672ms step_avg:33.78ms
step:465/2160 train_time:15706ms step_avg:33.78ms
step:466/2160 train_time:15738ms step_avg:33.77ms
step:467/2160 train_time:15772ms step_avg:33.77ms
step:468/2160 train_time:15806ms step_avg:33.77ms
step:469/2160 train_time:15840ms step_avg:33.77ms
step:470/2160 train_time:15873ms step_avg:33.77ms
step:471/2160 train_time:15907ms step_avg:33.77ms
step:472/2160 train_time:15940ms step_avg:33.77ms
step:473/2160 train_time:15974ms step_avg:33.77ms
step:474/2160 train_time:16007ms step_avg:33.77ms
step:475/2160 train_time:16041ms step_avg:33.77ms
step:476/2160 train_time:16074ms step_avg:33.77ms
step:477/2160 train_time:16108ms step_avg:33.77ms
step:478/2160 train_time:16141ms step_avg:33.77ms
step:479/2160 train_time:16175ms step_avg:33.77ms
step:480/2160 train_time:16208ms step_avg:33.77ms
step:481/2160 train_time:16242ms step_avg:33.77ms
step:482/2160 train_time:16275ms step_avg:33.77ms
step:483/2160 train_time:16309ms step_avg:33.77ms
step:484/2160 train_time:16342ms step_avg:33.76ms
step:485/2160 train_time:16376ms step_avg:33.77ms
step:486/2160 train_time:16410ms step_avg:33.77ms
step:487/2160 train_time:16444ms step_avg:33.77ms
step:488/2160 train_time:16477ms step_avg:33.76ms
step:489/2160 train_time:16511ms step_avg:33.77ms
step:490/2160 train_time:16544ms step_avg:33.76ms
step:491/2160 train_time:16579ms step_avg:33.76ms
step:492/2160 train_time:16612ms step_avg:33.76ms
step:493/2160 train_time:16646ms step_avg:33.76ms
step:494/2160 train_time:16679ms step_avg:33.76ms
step:495/2160 train_time:16713ms step_avg:33.76ms
step:496/2160 train_time:16746ms step_avg:33.76ms
step:497/2160 train_time:16781ms step_avg:33.76ms
step:498/2160 train_time:16814ms step_avg:33.76ms
step:499/2160 train_time:16848ms step_avg:33.76ms
step:500/2160 train_time:16881ms step_avg:33.76ms
step:500/2160 val_loss:4.0087 train_time:16917ms step_avg:33.83ms
step:501/2160 train_time:16936ms step_avg:33.80ms
step:502/2160 train_time:16954ms step_avg:33.77ms
step:503/2160 train_time:16986ms step_avg:33.77ms
step:504/2160 train_time:17020ms step_avg:33.77ms
step:505/2160 train_time:17055ms step_avg:33.77ms
step:506/2160 train_time:17089ms step_avg:33.77ms
step:507/2160 train_time:17123ms step_avg:33.77ms
step:508/2160 train_time:17156ms step_avg:33.77ms
step:509/2160 train_time:17191ms step_avg:33.77ms
step:510/2160 train_time:17224ms step_avg:33.77ms
step:511/2160 train_time:17258ms step_avg:33.77ms
step:512/2160 train_time:17291ms step_avg:33.77ms
step:513/2160 train_time:17325ms step_avg:33.77ms
step:514/2160 train_time:17358ms step_avg:33.77ms
step:515/2160 train_time:17392ms step_avg:33.77ms
step:516/2160 train_time:17425ms step_avg:33.77ms
step:517/2160 train_time:17459ms step_avg:33.77ms
step:518/2160 train_time:17492ms step_avg:33.77ms
step:519/2160 train_time:17525ms step_avg:33.77ms
step:520/2160 train_time:17558ms step_avg:33.77ms
step:521/2160 train_time:17592ms step_avg:33.77ms
step:522/2160 train_time:17625ms step_avg:33.76ms
step:523/2160 train_time:17659ms step_avg:33.76ms
step:524/2160 train_time:17692ms step_avg:33.76ms
step:525/2160 train_time:17726ms step_avg:33.76ms
step:526/2160 train_time:17758ms step_avg:33.76ms
step:527/2160 train_time:17792ms step_avg:33.76ms
step:528/2160 train_time:17826ms step_avg:33.76ms
step:529/2160 train_time:17860ms step_avg:33.76ms
step:530/2160 train_time:17893ms step_avg:33.76ms
step:531/2160 train_time:17927ms step_avg:33.76ms
step:532/2160 train_time:17961ms step_avg:33.76ms
step:533/2160 train_time:17995ms step_avg:33.76ms
step:534/2160 train_time:18029ms step_avg:33.76ms
step:535/2160 train_time:18064ms step_avg:33.76ms
step:536/2160 train_time:18097ms step_avg:33.76ms
step:537/2160 train_time:18131ms step_avg:33.76ms
step:538/2160 train_time:18164ms step_avg:33.76ms
step:539/2160 train_time:18198ms step_avg:33.76ms
step:540/2160 train_time:18231ms step_avg:33.76ms
step:541/2160 train_time:18265ms step_avg:33.76ms
step:542/2160 train_time:18299ms step_avg:33.76ms
step:543/2160 train_time:18333ms step_avg:33.76ms
step:544/2160 train_time:18366ms step_avg:33.76ms
step:545/2160 train_time:18400ms step_avg:33.76ms
step:546/2160 train_time:18433ms step_avg:33.76ms
step:547/2160 train_time:18467ms step_avg:33.76ms
step:548/2160 train_time:18500ms step_avg:33.76ms
step:549/2160 train_time:18534ms step_avg:33.76ms
step:550/2160 train_time:18567ms step_avg:33.76ms
step:551/2160 train_time:18601ms step_avg:33.76ms
step:552/2160 train_time:18634ms step_avg:33.76ms
step:553/2160 train_time:18668ms step_avg:33.76ms
step:554/2160 train_time:18701ms step_avg:33.76ms
step:555/2160 train_time:18735ms step_avg:33.76ms
step:556/2160 train_time:18768ms step_avg:33.76ms
step:557/2160 train_time:18802ms step_avg:33.76ms
step:558/2160 train_time:18835ms step_avg:33.75ms
step:559/2160 train_time:18869ms step_avg:33.76ms
step:560/2160 train_time:18902ms step_avg:33.75ms
step:561/2160 train_time:18936ms step_avg:33.75ms
step:562/2160 train_time:18969ms step_avg:33.75ms
step:563/2160 train_time:19004ms step_avg:33.76ms
step:564/2160 train_time:19037ms step_avg:33.75ms
step:565/2160 train_time:19071ms step_avg:33.75ms
step:566/2160 train_time:19105ms step_avg:33.75ms
step:567/2160 train_time:19139ms step_avg:33.75ms
step:568/2160 train_time:19172ms step_avg:33.75ms
step:569/2160 train_time:19206ms step_avg:33.75ms
step:570/2160 train_time:19239ms step_avg:33.75ms
step:571/2160 train_time:19274ms step_avg:33.75ms
step:572/2160 train_time:19307ms step_avg:33.75ms
step:573/2160 train_time:19341ms step_avg:33.75ms
step:574/2160 train_time:19374ms step_avg:33.75ms
step:575/2160 train_time:19408ms step_avg:33.75ms
step:576/2160 train_time:19441ms step_avg:33.75ms
step:577/2160 train_time:19475ms step_avg:33.75ms
step:578/2160 train_time:19508ms step_avg:33.75ms
step:579/2160 train_time:19543ms step_avg:33.75ms
step:580/2160 train_time:19576ms step_avg:33.75ms
step:581/2160 train_time:19610ms step_avg:33.75ms
step:582/2160 train_time:19643ms step_avg:33.75ms
step:583/2160 train_time:19677ms step_avg:33.75ms
step:584/2160 train_time:19710ms step_avg:33.75ms
step:585/2160 train_time:19744ms step_avg:33.75ms
step:586/2160 train_time:19777ms step_avg:33.75ms
step:587/2160 train_time:19811ms step_avg:33.75ms
step:588/2160 train_time:19844ms step_avg:33.75ms
step:589/2160 train_time:19879ms step_avg:33.75ms
step:590/2160 train_time:19912ms step_avg:33.75ms
step:591/2160 train_time:19946ms step_avg:33.75ms
step:592/2160 train_time:19979ms step_avg:33.75ms
step:593/2160 train_time:20014ms step_avg:33.75ms
step:594/2160 train_time:20047ms step_avg:33.75ms
step:595/2160 train_time:20081ms step_avg:33.75ms
step:596/2160 train_time:20114ms step_avg:33.75ms
step:597/2160 train_time:20148ms step_avg:33.75ms
step:598/2160 train_time:20182ms step_avg:33.75ms
step:599/2160 train_time:20216ms step_avg:33.75ms
step:600/2160 train_time:20249ms step_avg:33.75ms
step:601/2160 train_time:20284ms step_avg:33.75ms
step:602/2160 train_time:20317ms step_avg:33.75ms
step:603/2160 train_time:20351ms step_avg:33.75ms
step:604/2160 train_time:20384ms step_avg:33.75ms
step:605/2160 train_time:20418ms step_avg:33.75ms
step:606/2160 train_time:20451ms step_avg:33.75ms
step:607/2160 train_time:20485ms step_avg:33.75ms
step:608/2160 train_time:20518ms step_avg:33.75ms
step:609/2160 train_time:20552ms step_avg:33.75ms
step:610/2160 train_time:20586ms step_avg:33.75ms
step:611/2160 train_time:20620ms step_avg:33.75ms
step:612/2160 train_time:20653ms step_avg:33.75ms
step:613/2160 train_time:20687ms step_avg:33.75ms
step:614/2160 train_time:20720ms step_avg:33.75ms
step:615/2160 train_time:20754ms step_avg:33.75ms
step:616/2160 train_time:20787ms step_avg:33.75ms
step:617/2160 train_time:20821ms step_avg:33.75ms
step:618/2160 train_time:20854ms step_avg:33.74ms
step:619/2160 train_time:20888ms step_avg:33.74ms
step:620/2160 train_time:20921ms step_avg:33.74ms
step:621/2160 train_time:20955ms step_avg:33.74ms
step:622/2160 train_time:20988ms step_avg:33.74ms
step:623/2160 train_time:21023ms step_avg:33.74ms
step:624/2160 train_time:21056ms step_avg:33.74ms
step:625/2160 train_time:21090ms step_avg:33.74ms
step:626/2160 train_time:21123ms step_avg:33.74ms
step:627/2160 train_time:21157ms step_avg:33.74ms
step:628/2160 train_time:21191ms step_avg:33.74ms
step:629/2160 train_time:21225ms step_avg:33.74ms
step:630/2160 train_time:21258ms step_avg:33.74ms
step:631/2160 train_time:21291ms step_avg:33.74ms
step:632/2160 train_time:21325ms step_avg:33.74ms
step:633/2160 train_time:21359ms step_avg:33.74ms
step:634/2160 train_time:21392ms step_avg:33.74ms
step:635/2160 train_time:21426ms step_avg:33.74ms
step:636/2160 train_time:21460ms step_avg:33.74ms
step:637/2160 train_time:21494ms step_avg:33.74ms
step:638/2160 train_time:21527ms step_avg:33.74ms
step:639/2160 train_time:21561ms step_avg:33.74ms
step:640/2160 train_time:21594ms step_avg:33.74ms
step:641/2160 train_time:21628ms step_avg:33.74ms
step:642/2160 train_time:21661ms step_avg:33.74ms
step:643/2160 train_time:21695ms step_avg:33.74ms
step:644/2160 train_time:21728ms step_avg:33.74ms
step:645/2160 train_time:21762ms step_avg:33.74ms
step:646/2160 train_time:21795ms step_avg:33.74ms
step:647/2160 train_time:21829ms step_avg:33.74ms
step:648/2160 train_time:21863ms step_avg:33.74ms
step:649/2160 train_time:21896ms step_avg:33.74ms
step:650/2160 train_time:21929ms step_avg:33.74ms
step:651/2160 train_time:21964ms step_avg:33.74ms
step:652/2160 train_time:21997ms step_avg:33.74ms
step:653/2160 train_time:22031ms step_avg:33.74ms
step:654/2160 train_time:22064ms step_avg:33.74ms
step:655/2160 train_time:22098ms step_avg:33.74ms
step:656/2160 train_time:22131ms step_avg:33.74ms
step:657/2160 train_time:22166ms step_avg:33.74ms
step:658/2160 train_time:22199ms step_avg:33.74ms
step:659/2160 train_time:22233ms step_avg:33.74ms
step:660/2160 train_time:22266ms step_avg:33.74ms
step:661/2160 train_time:22300ms step_avg:33.74ms
step:662/2160 train_time:22333ms step_avg:33.74ms
step:663/2160 train_time:22368ms step_avg:33.74ms
step:664/2160 train_time:22401ms step_avg:33.74ms
step:665/2160 train_time:22435ms step_avg:33.74ms
step:666/2160 train_time:22468ms step_avg:33.74ms
step:667/2160 train_time:22503ms step_avg:33.74ms
step:668/2160 train_time:22536ms step_avg:33.74ms
step:669/2160 train_time:22570ms step_avg:33.74ms
step:670/2160 train_time:22604ms step_avg:33.74ms
step:671/2160 train_time:22638ms step_avg:33.74ms
step:672/2160 train_time:22671ms step_avg:33.74ms
step:673/2160 train_time:22705ms step_avg:33.74ms
step:674/2160 train_time:22738ms step_avg:33.74ms
step:675/2160 train_time:22772ms step_avg:33.74ms
step:676/2160 train_time:22805ms step_avg:33.74ms
step:677/2160 train_time:22839ms step_avg:33.74ms
step:678/2160 train_time:22873ms step_avg:33.74ms
step:679/2160 train_time:22907ms step_avg:33.74ms
step:680/2160 train_time:22940ms step_avg:33.73ms
step:681/2160 train_time:22974ms step_avg:33.74ms
step:682/2160 train_time:23007ms step_avg:33.73ms
step:683/2160 train_time:23041ms step_avg:33.73ms
step:684/2160 train_time:23074ms step_avg:33.73ms
step:685/2160 train_time:23108ms step_avg:33.73ms
step:686/2160 train_time:23141ms step_avg:33.73ms
step:687/2160 train_time:23175ms step_avg:33.73ms
step:688/2160 train_time:23209ms step_avg:33.73ms
step:689/2160 train_time:23243ms step_avg:33.73ms
step:690/2160 train_time:23276ms step_avg:33.73ms
step:691/2160 train_time:23310ms step_avg:33.73ms
step:692/2160 train_time:23344ms step_avg:33.73ms
step:693/2160 train_time:23378ms step_avg:33.73ms
step:694/2160 train_time:23411ms step_avg:33.73ms
step:695/2160 train_time:23445ms step_avg:33.73ms
step:696/2160 train_time:23479ms step_avg:33.73ms
step:697/2160 train_time:23513ms step_avg:33.73ms
step:698/2160 train_time:23546ms step_avg:33.73ms
step:699/2160 train_time:23581ms step_avg:33.73ms
step:700/2160 train_time:23614ms step_avg:33.73ms
step:701/2160 train_time:23648ms step_avg:33.73ms
step:702/2160 train_time:23681ms step_avg:33.73ms
step:703/2160 train_time:23715ms step_avg:33.73ms
step:704/2160 train_time:23748ms step_avg:33.73ms
step:705/2160 train_time:23782ms step_avg:33.73ms
step:706/2160 train_time:23815ms step_avg:33.73ms
step:707/2160 train_time:23850ms step_avg:33.73ms
step:708/2160 train_time:23883ms step_avg:33.73ms
step:709/2160 train_time:23943ms step_avg:33.77ms
step:710/2160 train_time:24003ms step_avg:33.81ms
step:711/2160 train_time:24064ms step_avg:33.84ms
step:712/2160 train_time:24123ms step_avg:33.88ms
step:713/2160 train_time:24184ms step_avg:33.92ms
step:714/2160 train_time:24244ms step_avg:33.96ms
step:715/2160 train_time:24306ms step_avg:33.99ms
step:716/2160 train_time:24365ms step_avg:34.03ms
step:717/2160 train_time:24427ms step_avg:34.07ms
step:718/2160 train_time:24487ms step_avg:34.10ms
step:719/2160 train_time:24548ms step_avg:34.14ms
step:720/2160 train_time:24608ms step_avg:34.18ms
step:721/2160 train_time:24671ms step_avg:34.22ms
step:722/2160 train_time:24731ms step_avg:34.25ms
step:723/2160 train_time:24792ms step_avg:34.29ms
step:724/2160 train_time:24851ms step_avg:34.32ms
step:725/2160 train_time:24912ms step_avg:34.36ms
step:726/2160 train_time:24972ms step_avg:34.40ms
step:727/2160 train_time:25034ms step_avg:34.43ms
step:728/2160 train_time:25094ms step_avg:34.47ms
step:729/2160 train_time:25155ms step_avg:34.51ms
step:730/2160 train_time:25215ms step_avg:34.54ms
step:731/2160 train_time:25277ms step_avg:34.58ms
step:732/2160 train_time:25337ms step_avg:34.61ms
step:733/2160 train_time:25399ms step_avg:34.65ms
step:734/2160 train_time:25458ms step_avg:34.68ms
step:735/2160 train_time:25519ms step_avg:34.72ms
step:736/2160 train_time:25579ms step_avg:34.75ms
step:737/2160 train_time:25641ms step_avg:34.79ms
step:738/2160 train_time:25700ms step_avg:34.82ms
step:739/2160 train_time:25762ms step_avg:34.86ms
step:740/2160 train_time:25822ms step_avg:34.89ms
step:741/2160 train_time:25884ms step_avg:34.93ms
step:742/2160 train_time:25944ms step_avg:34.96ms
step:743/2160 train_time:26005ms step_avg:35.00ms
step:744/2160 train_time:26065ms step_avg:35.03ms
step:745/2160 train_time:26126ms step_avg:35.07ms
step:746/2160 train_time:26186ms step_avg:35.10ms
step:747/2160 train_time:26248ms step_avg:35.14ms
step:748/2160 train_time:26308ms step_avg:35.17ms
step:749/2160 train_time:26370ms step_avg:35.21ms
step:750/2160 train_time:26430ms step_avg:35.24ms
step:750/2160 val_loss:3.8620 train_time:26492ms step_avg:35.32ms
step:751/2160 train_time:26514ms step_avg:35.30ms
step:752/2160 train_time:26557ms step_avg:35.32ms
step:753/2160 train_time:26618ms step_avg:35.35ms
step:754/2160 train_time:26678ms step_avg:35.38ms
step:755/2160 train_time:26742ms step_avg:35.42ms
step:756/2160 train_time:26802ms step_avg:35.45ms
step:757/2160 train_time:26863ms step_avg:35.49ms
step:758/2160 train_time:26922ms step_avg:35.52ms
step:759/2160 train_time:26983ms step_avg:35.55ms
step:760/2160 train_time:27041ms step_avg:35.58ms
step:761/2160 train_time:27102ms step_avg:35.61ms
step:762/2160 train_time:27160ms step_avg:35.64ms
step:763/2160 train_time:27221ms step_avg:35.68ms
step:764/2160 train_time:27281ms step_avg:35.71ms
step:765/2160 train_time:27342ms step_avg:35.74ms
step:766/2160 train_time:27404ms step_avg:35.78ms
step:767/2160 train_time:27471ms step_avg:35.82ms
step:768/2160 train_time:27531ms step_avg:35.85ms
step:769/2160 train_time:27593ms step_avg:35.88ms
step:770/2160 train_time:27653ms step_avg:35.91ms
step:771/2160 train_time:27715ms step_avg:35.95ms
step:772/2160 train_time:27774ms step_avg:35.98ms
step:773/2160 train_time:27834ms step_avg:36.01ms
step:774/2160 train_time:27893ms step_avg:36.04ms
step:775/2160 train_time:27954ms step_avg:36.07ms
step:776/2160 train_time:28014ms step_avg:36.10ms
step:777/2160 train_time:28074ms step_avg:36.13ms
step:778/2160 train_time:28133ms step_avg:36.16ms
step:779/2160 train_time:28194ms step_avg:36.19ms
step:780/2160 train_time:28254ms step_avg:36.22ms
step:781/2160 train_time:28315ms step_avg:36.26ms
step:782/2160 train_time:28375ms step_avg:36.29ms
step:783/2160 train_time:28438ms step_avg:36.32ms
step:784/2160 train_time:28499ms step_avg:36.35ms
step:785/2160 train_time:28561ms step_avg:36.38ms
step:786/2160 train_time:28621ms step_avg:36.41ms
step:787/2160 train_time:28683ms step_avg:36.45ms
step:788/2160 train_time:28743ms step_avg:36.48ms
step:789/2160 train_time:28805ms step_avg:36.51ms
step:790/2160 train_time:28865ms step_avg:36.54ms
step:791/2160 train_time:28926ms step_avg:36.57ms
step:792/2160 train_time:28985ms step_avg:36.60ms
step:793/2160 train_time:29047ms step_avg:36.63ms
step:794/2160 train_time:29106ms step_avg:36.66ms
step:795/2160 train_time:29167ms step_avg:36.69ms
step:796/2160 train_time:29227ms step_avg:36.72ms
step:797/2160 train_time:29288ms step_avg:36.75ms
step:798/2160 train_time:29348ms step_avg:36.78ms
step:799/2160 train_time:29410ms step_avg:36.81ms
step:800/2160 train_time:29471ms step_avg:36.84ms
step:801/2160 train_time:29533ms step_avg:36.87ms
step:802/2160 train_time:29593ms step_avg:36.90ms
step:803/2160 train_time:29655ms step_avg:36.93ms
step:804/2160 train_time:29715ms step_avg:36.96ms
step:805/2160 train_time:29776ms step_avg:36.99ms
step:806/2160 train_time:29835ms step_avg:37.02ms
step:807/2160 train_time:29896ms step_avg:37.05ms
step:808/2160 train_time:29955ms step_avg:37.07ms
step:809/2160 train_time:30017ms step_avg:37.10ms
step:810/2160 train_time:30076ms step_avg:37.13ms
step:811/2160 train_time:30137ms step_avg:37.16ms
step:812/2160 train_time:30197ms step_avg:37.19ms
step:813/2160 train_time:30257ms step_avg:37.22ms
step:814/2160 train_time:30317ms step_avg:37.24ms
step:815/2160 train_time:30378ms step_avg:37.27ms
step:816/2160 train_time:30438ms step_avg:37.30ms
step:817/2160 train_time:30499ms step_avg:37.33ms
step:818/2160 train_time:30560ms step_avg:37.36ms
step:819/2160 train_time:30622ms step_avg:37.39ms
step:820/2160 train_time:30681ms step_avg:37.42ms
step:821/2160 train_time:30743ms step_avg:37.45ms
step:822/2160 train_time:30803ms step_avg:37.47ms
step:823/2160 train_time:30865ms step_avg:37.50ms
step:824/2160 train_time:30924ms step_avg:37.53ms
step:825/2160 train_time:30986ms step_avg:37.56ms
step:826/2160 train_time:31046ms step_avg:37.59ms
step:827/2160 train_time:31107ms step_avg:37.61ms
step:828/2160 train_time:31167ms step_avg:37.64ms
step:829/2160 train_time:31229ms step_avg:37.67ms
step:830/2160 train_time:31289ms step_avg:37.70ms
step:831/2160 train_time:31350ms step_avg:37.73ms
step:832/2160 train_time:31410ms step_avg:37.75ms
step:833/2160 train_time:31473ms step_avg:37.78ms
step:834/2160 train_time:31533ms step_avg:37.81ms
step:835/2160 train_time:31595ms step_avg:37.84ms
step:836/2160 train_time:31655ms step_avg:37.86ms
step:837/2160 train_time:31716ms step_avg:37.89ms
step:838/2160 train_time:31775ms step_avg:37.92ms
step:839/2160 train_time:31836ms step_avg:37.94ms
step:840/2160 train_time:31895ms step_avg:37.97ms
step:841/2160 train_time:31956ms step_avg:38.00ms
step:842/2160 train_time:32015ms step_avg:38.02ms
step:843/2160 train_time:32077ms step_avg:38.05ms
step:844/2160 train_time:32136ms step_avg:38.08ms
step:845/2160 train_time:32197ms step_avg:38.10ms
step:846/2160 train_time:32257ms step_avg:38.13ms
step:847/2160 train_time:32319ms step_avg:38.16ms
step:848/2160 train_time:32378ms step_avg:38.18ms
step:849/2160 train_time:32440ms step_avg:38.21ms
step:850/2160 train_time:32499ms step_avg:38.23ms
step:851/2160 train_time:32561ms step_avg:38.26ms
step:852/2160 train_time:32620ms step_avg:38.29ms
step:853/2160 train_time:32682ms step_avg:38.31ms
step:854/2160 train_time:32741ms step_avg:38.34ms
step:855/2160 train_time:32803ms step_avg:38.37ms
step:856/2160 train_time:32862ms step_avg:38.39ms
step:857/2160 train_time:32924ms step_avg:38.42ms
step:858/2160 train_time:32984ms step_avg:38.44ms
step:859/2160 train_time:33046ms step_avg:38.47ms
step:860/2160 train_time:33106ms step_avg:38.49ms
step:861/2160 train_time:33167ms step_avg:38.52ms
step:862/2160 train_time:33227ms step_avg:38.55ms
step:863/2160 train_time:33288ms step_avg:38.57ms
step:864/2160 train_time:33348ms step_avg:38.60ms
step:865/2160 train_time:33409ms step_avg:38.62ms
step:866/2160 train_time:33469ms step_avg:38.65ms
step:867/2160 train_time:33530ms step_avg:38.67ms
step:868/2160 train_time:33590ms step_avg:38.70ms
step:869/2160 train_time:33652ms step_avg:38.72ms
step:870/2160 train_time:33711ms step_avg:38.75ms
step:871/2160 train_time:33773ms step_avg:38.78ms
step:872/2160 train_time:33833ms step_avg:38.80ms
step:873/2160 train_time:33894ms step_avg:38.82ms
step:874/2160 train_time:33953ms step_avg:38.85ms
step:875/2160 train_time:34014ms step_avg:38.87ms
step:876/2160 train_time:34073ms step_avg:38.90ms
step:877/2160 train_time:34134ms step_avg:38.92ms
step:878/2160 train_time:34193ms step_avg:38.94ms
step:879/2160 train_time:34254ms step_avg:38.97ms
step:880/2160 train_time:34313ms step_avg:38.99ms
step:881/2160 train_time:34375ms step_avg:39.02ms
step:882/2160 train_time:34435ms step_avg:39.04ms
step:883/2160 train_time:34496ms step_avg:39.07ms
step:884/2160 train_time:34555ms step_avg:39.09ms
step:885/2160 train_time:34617ms step_avg:39.12ms
step:886/2160 train_time:34677ms step_avg:39.14ms
step:887/2160 train_time:34739ms step_avg:39.16ms
step:888/2160 train_time:34799ms step_avg:39.19ms
step:889/2160 train_time:34860ms step_avg:39.21ms
step:890/2160 train_time:34919ms step_avg:39.23ms
step:891/2160 train_time:34979ms step_avg:39.26ms
step:892/2160 train_time:35039ms step_avg:39.28ms
step:893/2160 train_time:35101ms step_avg:39.31ms
step:894/2160 train_time:35160ms step_avg:39.33ms
step:895/2160 train_time:35223ms step_avg:39.35ms
step:896/2160 train_time:35282ms step_avg:39.38ms
step:897/2160 train_time:35344ms step_avg:39.40ms
step:898/2160 train_time:35404ms step_avg:39.43ms
step:899/2160 train_time:35466ms step_avg:39.45ms
step:900/2160 train_time:35526ms step_avg:39.47ms
step:901/2160 train_time:35587ms step_avg:39.50ms
step:902/2160 train_time:35647ms step_avg:39.52ms
step:903/2160 train_time:35709ms step_avg:39.54ms
step:904/2160 train_time:35769ms step_avg:39.57ms
step:905/2160 train_time:35830ms step_avg:39.59ms
step:906/2160 train_time:35889ms step_avg:39.61ms
step:907/2160 train_time:35951ms step_avg:39.64ms
step:908/2160 train_time:36010ms step_avg:39.66ms
step:909/2160 train_time:36071ms step_avg:39.68ms
step:910/2160 train_time:36131ms step_avg:39.70ms
step:911/2160 train_time:36193ms step_avg:39.73ms
step:912/2160 train_time:36253ms step_avg:39.75ms
step:913/2160 train_time:36314ms step_avg:39.77ms
step:914/2160 train_time:36374ms step_avg:39.80ms
step:915/2160 train_time:36434ms step_avg:39.82ms
step:916/2160 train_time:36494ms step_avg:39.84ms
step:917/2160 train_time:36556ms step_avg:39.86ms
step:918/2160 train_time:36615ms step_avg:39.89ms
step:919/2160 train_time:36676ms step_avg:39.91ms
step:920/2160 train_time:36736ms step_avg:39.93ms
step:921/2160 train_time:36798ms step_avg:39.95ms
step:922/2160 train_time:36858ms step_avg:39.98ms
step:923/2160 train_time:36919ms step_avg:40.00ms
step:924/2160 train_time:36979ms step_avg:40.02ms
step:925/2160 train_time:37040ms step_avg:40.04ms
step:926/2160 train_time:37099ms step_avg:40.06ms
step:927/2160 train_time:37160ms step_avg:40.09ms
step:928/2160 train_time:37220ms step_avg:40.11ms
step:929/2160 train_time:37281ms step_avg:40.13ms
step:930/2160 train_time:37341ms step_avg:40.15ms
step:931/2160 train_time:37402ms step_avg:40.17ms
step:932/2160 train_time:37462ms step_avg:40.20ms
step:933/2160 train_time:37524ms step_avg:40.22ms
step:934/2160 train_time:37584ms step_avg:40.24ms
step:935/2160 train_time:37646ms step_avg:40.26ms
step:936/2160 train_time:37706ms step_avg:40.28ms
step:937/2160 train_time:37768ms step_avg:40.31ms
step:938/2160 train_time:37827ms step_avg:40.33ms
step:939/2160 train_time:37889ms step_avg:40.35ms
step:940/2160 train_time:37948ms step_avg:40.37ms
step:941/2160 train_time:38010ms step_avg:40.39ms
step:942/2160 train_time:38069ms step_avg:40.41ms
step:943/2160 train_time:38131ms step_avg:40.44ms
step:944/2160 train_time:38191ms step_avg:40.46ms
step:945/2160 train_time:38252ms step_avg:40.48ms
step:946/2160 train_time:38312ms step_avg:40.50ms
step:947/2160 train_time:38373ms step_avg:40.52ms
step:948/2160 train_time:38433ms step_avg:40.54ms
step:949/2160 train_time:38494ms step_avg:40.56ms
step:950/2160 train_time:38553ms step_avg:40.58ms
step:951/2160 train_time:38614ms step_avg:40.60ms
step:952/2160 train_time:38673ms step_avg:40.62ms
step:953/2160 train_time:38734ms step_avg:40.64ms
step:954/2160 train_time:38794ms step_avg:40.66ms
step:955/2160 train_time:38855ms step_avg:40.69ms
step:956/2160 train_time:38914ms step_avg:40.71ms
step:957/2160 train_time:38976ms step_avg:40.73ms
step:958/2160 train_time:39035ms step_avg:40.75ms
step:959/2160 train_time:39097ms step_avg:40.77ms
step:960/2160 train_time:39156ms step_avg:40.79ms
step:961/2160 train_time:39218ms step_avg:40.81ms
step:962/2160 train_time:39277ms step_avg:40.83ms
step:963/2160 train_time:39338ms step_avg:40.85ms
step:964/2160 train_time:39397ms step_avg:40.87ms
step:965/2160 train_time:39458ms step_avg:40.89ms
step:966/2160 train_time:39518ms step_avg:40.91ms
step:967/2160 train_time:39579ms step_avg:40.93ms
step:968/2160 train_time:39639ms step_avg:40.95ms
step:969/2160 train_time:39700ms step_avg:40.97ms
step:970/2160 train_time:39760ms step_avg:40.99ms
step:971/2160 train_time:39821ms step_avg:41.01ms
step:972/2160 train_time:39881ms step_avg:41.03ms
step:973/2160 train_time:39943ms step_avg:41.05ms
step:974/2160 train_time:40003ms step_avg:41.07ms
step:975/2160 train_time:40064ms step_avg:41.09ms
step:976/2160 train_time:40124ms step_avg:41.11ms
step:977/2160 train_time:40186ms step_avg:41.13ms
step:978/2160 train_time:40245ms step_avg:41.15ms
step:979/2160 train_time:40307ms step_avg:41.17ms
step:980/2160 train_time:40368ms step_avg:41.19ms
step:981/2160 train_time:40429ms step_avg:41.21ms
step:982/2160 train_time:40489ms step_avg:41.23ms
step:983/2160 train_time:40551ms step_avg:41.25ms
step:984/2160 train_time:40611ms step_avg:41.27ms
step:985/2160 train_time:40672ms step_avg:41.29ms
step:986/2160 train_time:40733ms step_avg:41.31ms
step:987/2160 train_time:40795ms step_avg:41.33ms
step:988/2160 train_time:40854ms step_avg:41.35ms
step:989/2160 train_time:40914ms step_avg:41.37ms
step:990/2160 train_time:40974ms step_avg:41.39ms
step:991/2160 train_time:41035ms step_avg:41.41ms
step:992/2160 train_time:41094ms step_avg:41.43ms
step:993/2160 train_time:41155ms step_avg:41.45ms
step:994/2160 train_time:41215ms step_avg:41.46ms
step:995/2160 train_time:41276ms step_avg:41.48ms
step:996/2160 train_time:41335ms step_avg:41.50ms
step:997/2160 train_time:41396ms step_avg:41.52ms
step:998/2160 train_time:41456ms step_avg:41.54ms
step:999/2160 train_time:41517ms step_avg:41.56ms
step:1000/2160 train_time:41577ms step_avg:41.58ms
step:1000/2160 val_loss:3.6917 train_time:41639ms step_avg:41.64ms
step:1001/2160 train_time:41658ms step_avg:41.62ms
step:1002/2160 train_time:41700ms step_avg:41.62ms
step:1003/2160 train_time:41765ms step_avg:41.64ms
step:1004/2160 train_time:41824ms step_avg:41.66ms
step:1005/2160 train_time:41885ms step_avg:41.68ms
step:1006/2160 train_time:41945ms step_avg:41.69ms
step:1007/2160 train_time:42005ms step_avg:41.71ms
step:1008/2160 train_time:42064ms step_avg:41.73ms
step:1009/2160 train_time:42124ms step_avg:41.75ms
step:1010/2160 train_time:42184ms step_avg:41.77ms
step:1011/2160 train_time:42244ms step_avg:41.78ms
step:1012/2160 train_time:42303ms step_avg:41.80ms
step:1013/2160 train_time:42364ms step_avg:41.82ms
step:1014/2160 train_time:42423ms step_avg:41.84ms
step:1015/2160 train_time:42484ms step_avg:41.86ms
step:1016/2160 train_time:42544ms step_avg:41.87ms
step:1017/2160 train_time:42606ms step_avg:41.89ms
step:1018/2160 train_time:42666ms step_avg:41.91ms
step:1019/2160 train_time:42729ms step_avg:41.93ms
step:1020/2160 train_time:42789ms step_avg:41.95ms
step:1021/2160 train_time:42851ms step_avg:41.97ms
step:1022/2160 train_time:42910ms step_avg:41.99ms
step:1023/2160 train_time:42971ms step_avg:42.01ms
step:1024/2160 train_time:43031ms step_avg:42.02ms
step:1025/2160 train_time:43092ms step_avg:42.04ms
step:1026/2160 train_time:43152ms step_avg:42.06ms
step:1027/2160 train_time:43213ms step_avg:42.08ms
step:1028/2160 train_time:43272ms step_avg:42.09ms
step:1029/2160 train_time:43334ms step_avg:42.11ms
step:1030/2160 train_time:43394ms step_avg:42.13ms
step:1031/2160 train_time:43455ms step_avg:42.15ms
step:1032/2160 train_time:43514ms step_avg:42.16ms
step:1033/2160 train_time:43576ms step_avg:42.18ms
step:1034/2160 train_time:43637ms step_avg:42.20ms
step:1035/2160 train_time:43699ms step_avg:42.22ms
step:1036/2160 train_time:43759ms step_avg:42.24ms
step:1037/2160 train_time:43820ms step_avg:42.26ms
step:1038/2160 train_time:43880ms step_avg:42.27ms
step:1039/2160 train_time:43941ms step_avg:42.29ms
step:1040/2160 train_time:44001ms step_avg:42.31ms
step:1041/2160 train_time:44062ms step_avg:42.33ms
step:1042/2160 train_time:44121ms step_avg:42.34ms
step:1043/2160 train_time:44183ms step_avg:42.36ms
step:1044/2160 train_time:44242ms step_avg:42.38ms
step:1045/2160 train_time:44303ms step_avg:42.39ms
step:1046/2160 train_time:44362ms step_avg:42.41ms
step:1047/2160 train_time:44423ms step_avg:42.43ms
step:1048/2160 train_time:44483ms step_avg:42.45ms
step:1049/2160 train_time:44543ms step_avg:42.46ms
step:1050/2160 train_time:44602ms step_avg:42.48ms
step:1051/2160 train_time:44664ms step_avg:42.50ms
step:1052/2160 train_time:44724ms step_avg:42.51ms
step:1053/2160 train_time:44786ms step_avg:42.53ms
step:1054/2160 train_time:44846ms step_avg:42.55ms
step:1055/2160 train_time:44907ms step_avg:42.57ms
step:1056/2160 train_time:44966ms step_avg:42.58ms
step:1057/2160 train_time:45027ms step_avg:42.60ms
step:1058/2160 train_time:45086ms step_avg:42.61ms
step:1059/2160 train_time:45147ms step_avg:42.63ms
step:1060/2160 train_time:45207ms step_avg:42.65ms
step:1061/2160 train_time:45268ms step_avg:42.67ms
step:1062/2160 train_time:45327ms step_avg:42.68ms
step:1063/2160 train_time:45388ms step_avg:42.70ms
step:1064/2160 train_time:45448ms step_avg:42.71ms
step:1065/2160 train_time:45509ms step_avg:42.73ms
step:1066/2160 train_time:45568ms step_avg:42.75ms
step:1067/2160 train_time:45630ms step_avg:42.77ms
step:1068/2160 train_time:45690ms step_avg:42.78ms
step:1069/2160 train_time:45751ms step_avg:42.80ms
step:1070/2160 train_time:45811ms step_avg:42.81ms
step:1071/2160 train_time:45873ms step_avg:42.83ms
step:1072/2160 train_time:45933ms step_avg:42.85ms
step:1073/2160 train_time:45994ms step_avg:42.87ms
step:1074/2160 train_time:46055ms step_avg:42.88ms
step:1075/2160 train_time:46116ms step_avg:42.90ms
step:1076/2160 train_time:46176ms step_avg:42.91ms
step:1077/2160 train_time:46238ms step_avg:42.93ms
step:1078/2160 train_time:46298ms step_avg:42.95ms
step:1079/2160 train_time:46359ms step_avg:42.97ms
step:1080/2160 train_time:46419ms step_avg:42.98ms
step:1081/2160 train_time:46480ms step_avg:43.00ms
step:1082/2160 train_time:46540ms step_avg:43.01ms
step:1083/2160 train_time:46601ms step_avg:43.03ms
step:1084/2160 train_time:46662ms step_avg:43.05ms
step:1085/2160 train_time:46724ms step_avg:43.06ms
step:1086/2160 train_time:46783ms step_avg:43.08ms
step:1087/2160 train_time:46844ms step_avg:43.10ms
step:1088/2160 train_time:46905ms step_avg:43.11ms
step:1089/2160 train_time:46966ms step_avg:43.13ms
step:1090/2160 train_time:47025ms step_avg:43.14ms
step:1091/2160 train_time:47086ms step_avg:43.16ms
step:1092/2160 train_time:47145ms step_avg:43.17ms
step:1093/2160 train_time:47206ms step_avg:43.19ms
step:1094/2160 train_time:47266ms step_avg:43.20ms
step:1095/2160 train_time:47327ms step_avg:43.22ms
step:1096/2160 train_time:47386ms step_avg:43.24ms
step:1097/2160 train_time:47448ms step_avg:43.25ms
step:1098/2160 train_time:47507ms step_avg:43.27ms
step:1099/2160 train_time:47570ms step_avg:43.28ms
step:1100/2160 train_time:47629ms step_avg:43.30ms
step:1101/2160 train_time:47690ms step_avg:43.32ms
step:1102/2160 train_time:47750ms step_avg:43.33ms
step:1103/2160 train_time:47811ms step_avg:43.35ms
step:1104/2160 train_time:47871ms step_avg:43.36ms
step:1105/2160 train_time:47933ms step_avg:43.38ms
step:1106/2160 train_time:47992ms step_avg:43.39ms
step:1107/2160 train_time:48055ms step_avg:43.41ms
step:1108/2160 train_time:48114ms step_avg:43.42ms
step:1109/2160 train_time:48176ms step_avg:43.44ms
step:1110/2160 train_time:48236ms step_avg:43.46ms
step:1111/2160 train_time:48297ms step_avg:43.47ms
step:1112/2160 train_time:48358ms step_avg:43.49ms
step:1113/2160 train_time:48419ms step_avg:43.50ms
step:1114/2160 train_time:48479ms step_avg:43.52ms
step:1115/2160 train_time:48540ms step_avg:43.53ms
step:1116/2160 train_time:48600ms step_avg:43.55ms
step:1117/2160 train_time:48661ms step_avg:43.56ms
step:1118/2160 train_time:48721ms step_avg:43.58ms
step:1119/2160 train_time:48783ms step_avg:43.60ms
step:1120/2160 train_time:48843ms step_avg:43.61ms
step:1121/2160 train_time:48905ms step_avg:43.63ms
step:1122/2160 train_time:48965ms step_avg:43.64ms
step:1123/2160 train_time:49026ms step_avg:43.66ms
step:1124/2160 train_time:49086ms step_avg:43.67ms
step:1125/2160 train_time:49146ms step_avg:43.69ms
step:1126/2160 train_time:49206ms step_avg:43.70ms
step:1127/2160 train_time:49267ms step_avg:43.71ms
step:1128/2160 train_time:49326ms step_avg:43.73ms
step:1129/2160 train_time:49388ms step_avg:43.74ms
step:1130/2160 train_time:49448ms step_avg:43.76ms
step:1131/2160 train_time:49510ms step_avg:43.78ms
step:1132/2160 train_time:49569ms step_avg:43.79ms
step:1133/2160 train_time:49631ms step_avg:43.80ms
step:1134/2160 train_time:49691ms step_avg:43.82ms
step:1135/2160 train_time:49752ms step_avg:43.83ms
step:1136/2160 train_time:49812ms step_avg:43.85ms
step:1137/2160 train_time:49873ms step_avg:43.86ms
step:1138/2160 train_time:49933ms step_avg:43.88ms
step:1139/2160 train_time:49994ms step_avg:43.89ms
step:1140/2160 train_time:50055ms step_avg:43.91ms
step:1141/2160 train_time:50116ms step_avg:43.92ms
step:1142/2160 train_time:50176ms step_avg:43.94ms
step:1143/2160 train_time:50237ms step_avg:43.95ms
step:1144/2160 train_time:50297ms step_avg:43.97ms
step:1145/2160 train_time:50359ms step_avg:43.98ms
step:1146/2160 train_time:50419ms step_avg:44.00ms
step:1147/2160 train_time:50481ms step_avg:44.01ms
step:1148/2160 train_time:50540ms step_avg:44.02ms
step:1149/2160 train_time:50602ms step_avg:44.04ms
step:1150/2160 train_time:50662ms step_avg:44.05ms
step:1151/2160 train_time:50723ms step_avg:44.07ms
step:1152/2160 train_time:50783ms step_avg:44.08ms
step:1153/2160 train_time:50844ms step_avg:44.10ms
step:1154/2160 train_time:50904ms step_avg:44.11ms
step:1155/2160 train_time:50966ms step_avg:44.13ms
step:1156/2160 train_time:51026ms step_avg:44.14ms
step:1157/2160 train_time:51086ms step_avg:44.15ms
step:1158/2160 train_time:51145ms step_avg:44.17ms
step:1159/2160 train_time:51207ms step_avg:44.18ms
step:1160/2160 train_time:51266ms step_avg:44.19ms
step:1161/2160 train_time:51327ms step_avg:44.21ms
step:1162/2160 train_time:51386ms step_avg:44.22ms
step:1163/2160 train_time:51448ms step_avg:44.24ms
step:1164/2160 train_time:51507ms step_avg:44.25ms
step:1165/2160 train_time:51569ms step_avg:44.27ms
step:1166/2160 train_time:51629ms step_avg:44.28ms
step:1167/2160 train_time:51690ms step_avg:44.29ms
step:1168/2160 train_time:51750ms step_avg:44.31ms
step:1169/2160 train_time:51811ms step_avg:44.32ms
step:1170/2160 train_time:51871ms step_avg:44.33ms
step:1171/2160 train_time:51933ms step_avg:44.35ms
step:1172/2160 train_time:51992ms step_avg:44.36ms
step:1173/2160 train_time:52054ms step_avg:44.38ms
step:1174/2160 train_time:52113ms step_avg:44.39ms
step:1175/2160 train_time:52175ms step_avg:44.40ms
step:1176/2160 train_time:52234ms step_avg:44.42ms
step:1177/2160 train_time:52296ms step_avg:44.43ms
step:1178/2160 train_time:52357ms step_avg:44.45ms
step:1179/2160 train_time:52419ms step_avg:44.46ms
step:1180/2160 train_time:52479ms step_avg:44.47ms
step:1181/2160 train_time:52540ms step_avg:44.49ms
step:1182/2160 train_time:52600ms step_avg:44.50ms
step:1183/2160 train_time:52661ms step_avg:44.51ms
step:1184/2160 train_time:52721ms step_avg:44.53ms
step:1185/2160 train_time:52783ms step_avg:44.54ms
step:1186/2160 train_time:52843ms step_avg:44.56ms
step:1187/2160 train_time:52905ms step_avg:44.57ms
step:1188/2160 train_time:52965ms step_avg:44.58ms
step:1189/2160 train_time:53026ms step_avg:44.60ms
step:1190/2160 train_time:53086ms step_avg:44.61ms
step:1191/2160 train_time:53146ms step_avg:44.62ms
step:1192/2160 train_time:53205ms step_avg:44.64ms
step:1193/2160 train_time:53266ms step_avg:44.65ms
step:1194/2160 train_time:53326ms step_avg:44.66ms
step:1195/2160 train_time:53387ms step_avg:44.68ms
step:1196/2160 train_time:53447ms step_avg:44.69ms
step:1197/2160 train_time:53508ms step_avg:44.70ms
step:1198/2160 train_time:53568ms step_avg:44.71ms
step:1199/2160 train_time:53630ms step_avg:44.73ms
step:1200/2160 train_time:53690ms step_avg:44.74ms
step:1201/2160 train_time:53752ms step_avg:44.76ms
step:1202/2160 train_time:53811ms step_avg:44.77ms
step:1203/2160 train_time:53873ms step_avg:44.78ms
step:1204/2160 train_time:53932ms step_avg:44.79ms
step:1205/2160 train_time:53994ms step_avg:44.81ms
step:1206/2160 train_time:54054ms step_avg:44.82ms
step:1207/2160 train_time:54116ms step_avg:44.83ms
step:1208/2160 train_time:54175ms step_avg:44.85ms
step:1209/2160 train_time:54237ms step_avg:44.86ms
step:1210/2160 train_time:54297ms step_avg:44.87ms
step:1211/2160 train_time:54358ms step_avg:44.89ms
step:1212/2160 train_time:54418ms step_avg:44.90ms
step:1213/2160 train_time:54479ms step_avg:44.91ms
step:1214/2160 train_time:54539ms step_avg:44.93ms
step:1215/2160 train_time:54601ms step_avg:44.94ms
step:1216/2160 train_time:54661ms step_avg:44.95ms
step:1217/2160 train_time:54723ms step_avg:44.97ms
step:1218/2160 train_time:54784ms step_avg:44.98ms
step:1219/2160 train_time:54844ms step_avg:44.99ms
step:1220/2160 train_time:54905ms step_avg:45.00ms
step:1221/2160 train_time:54966ms step_avg:45.02ms
step:1222/2160 train_time:55025ms step_avg:45.03ms
step:1223/2160 train_time:55086ms step_avg:45.04ms
step:1224/2160 train_time:55145ms step_avg:45.05ms
step:1225/2160 train_time:55206ms step_avg:45.07ms
step:1226/2160 train_time:55265ms step_avg:45.08ms
step:1227/2160 train_time:55326ms step_avg:45.09ms
step:1228/2160 train_time:55386ms step_avg:45.10ms
step:1229/2160 train_time:55447ms step_avg:45.12ms
step:1230/2160 train_time:55507ms step_avg:45.13ms
step:1231/2160 train_time:55569ms step_avg:45.14ms
step:1232/2160 train_time:55629ms step_avg:45.15ms
step:1233/2160 train_time:55690ms step_avg:45.17ms
step:1234/2160 train_time:55750ms step_avg:45.18ms
step:1235/2160 train_time:55811ms step_avg:45.19ms
step:1236/2160 train_time:55871ms step_avg:45.20ms
step:1237/2160 train_time:55932ms step_avg:45.22ms
step:1238/2160 train_time:55992ms step_avg:45.23ms
step:1239/2160 train_time:56053ms step_avg:45.24ms
step:1240/2160 train_time:56113ms step_avg:45.25ms
step:1241/2160 train_time:56175ms step_avg:45.27ms
step:1242/2160 train_time:56234ms step_avg:45.28ms
step:1243/2160 train_time:56296ms step_avg:45.29ms
step:1244/2160 train_time:56356ms step_avg:45.30ms
step:1245/2160 train_time:56417ms step_avg:45.32ms
step:1246/2160 train_time:56477ms step_avg:45.33ms
step:1247/2160 train_time:56538ms step_avg:45.34ms
step:1248/2160 train_time:56598ms step_avg:45.35ms
step:1249/2160 train_time:56660ms step_avg:45.36ms
step:1250/2160 train_time:56720ms step_avg:45.38ms
step:1250/2160 val_loss:3.5759 train_time:56782ms step_avg:45.43ms
step:1251/2160 train_time:56801ms step_avg:45.40ms
step:1252/2160 train_time:56842ms step_avg:45.40ms
step:1253/2160 train_time:56905ms step_avg:45.41ms
step:1254/2160 train_time:56964ms step_avg:45.43ms
step:1255/2160 train_time:57025ms step_avg:45.44ms
step:1256/2160 train_time:57086ms step_avg:45.45ms
step:1257/2160 train_time:57146ms step_avg:45.46ms
step:1258/2160 train_time:57206ms step_avg:45.47ms
step:1259/2160 train_time:57267ms step_avg:45.49ms
step:1260/2160 train_time:57327ms step_avg:45.50ms
step:1261/2160 train_time:57389ms step_avg:45.51ms
step:1262/2160 train_time:57448ms step_avg:45.52ms
step:1263/2160 train_time:57509ms step_avg:45.53ms
step:1264/2160 train_time:57568ms step_avg:45.54ms
step:1265/2160 train_time:57629ms step_avg:45.56ms
step:1266/2160 train_time:57689ms step_avg:45.57ms
step:1267/2160 train_time:57753ms step_avg:45.58ms
step:1268/2160 train_time:57814ms step_avg:45.59ms
step:1269/2160 train_time:57876ms step_avg:45.61ms
step:1270/2160 train_time:57935ms step_avg:45.62ms
step:1271/2160 train_time:57996ms step_avg:45.63ms
step:1272/2160 train_time:58055ms step_avg:45.64ms
step:1273/2160 train_time:58116ms step_avg:45.65ms
step:1274/2160 train_time:58175ms step_avg:45.66ms
step:1275/2160 train_time:58236ms step_avg:45.68ms
step:1276/2160 train_time:58295ms step_avg:45.69ms
step:1277/2160 train_time:58356ms step_avg:45.70ms
step:1278/2160 train_time:58416ms step_avg:45.71ms
step:1279/2160 train_time:58478ms step_avg:45.72ms
step:1280/2160 train_time:58537ms step_avg:45.73ms
step:1281/2160 train_time:58599ms step_avg:45.74ms
step:1282/2160 train_time:58659ms step_avg:45.76ms
step:1283/2160 train_time:58721ms step_avg:45.77ms
step:1284/2160 train_time:58781ms step_avg:45.78ms
step:1285/2160 train_time:58843ms step_avg:45.79ms
step:1286/2160 train_time:58903ms step_avg:45.80ms
step:1287/2160 train_time:58964ms step_avg:45.82ms
step:1288/2160 train_time:59024ms step_avg:45.83ms
step:1289/2160 train_time:59085ms step_avg:45.84ms
step:1290/2160 train_time:59145ms step_avg:45.85ms
step:1291/2160 train_time:59207ms step_avg:45.86ms
step:1292/2160 train_time:59266ms step_avg:45.87ms
step:1293/2160 train_time:59327ms step_avg:45.88ms
step:1294/2160 train_time:59387ms step_avg:45.89ms
step:1295/2160 train_time:59448ms step_avg:45.91ms
step:1296/2160 train_time:59508ms step_avg:45.92ms
step:1297/2160 train_time:59569ms step_avg:45.93ms
step:1298/2160 train_time:59629ms step_avg:45.94ms
step:1299/2160 train_time:59690ms step_avg:45.95ms
step:1300/2160 train_time:59751ms step_avg:45.96ms
step:1301/2160 train_time:59812ms step_avg:45.97ms
step:1302/2160 train_time:59873ms step_avg:45.99ms
step:1303/2160 train_time:59933ms step_avg:46.00ms
step:1304/2160 train_time:59992ms step_avg:46.01ms
step:1305/2160 train_time:60053ms step_avg:46.02ms
step:1306/2160 train_time:60113ms step_avg:46.03ms
step:1307/2160 train_time:60174ms step_avg:46.04ms
step:1308/2160 train_time:60233ms step_avg:46.05ms
step:1309/2160 train_time:60295ms step_avg:46.06ms
step:1310/2160 train_time:60354ms step_avg:46.07ms
step:1311/2160 train_time:60415ms step_avg:46.08ms
step:1312/2160 train_time:60475ms step_avg:46.09ms
step:1313/2160 train_time:60536ms step_avg:46.11ms
step:1314/2160 train_time:60596ms step_avg:46.12ms
step:1315/2160 train_time:60657ms step_avg:46.13ms
step:1316/2160 train_time:60716ms step_avg:46.14ms
step:1317/2160 train_time:60778ms step_avg:46.15ms
step:1318/2160 train_time:60837ms step_avg:46.16ms
step:1319/2160 train_time:60898ms step_avg:46.17ms
step:1320/2160 train_time:60958ms step_avg:46.18ms
step:1321/2160 train_time:61019ms step_avg:46.19ms
step:1322/2160 train_time:61079ms step_avg:46.20ms
step:1323/2160 train_time:61140ms step_avg:46.21ms
step:1324/2160 train_time:61200ms step_avg:46.22ms
step:1325/2160 train_time:61262ms step_avg:46.24ms
step:1326/2160 train_time:61322ms step_avg:46.25ms
step:1327/2160 train_time:61383ms step_avg:46.26ms
step:1328/2160 train_time:61443ms step_avg:46.27ms
step:1329/2160 train_time:61504ms step_avg:46.28ms
step:1330/2160 train_time:61565ms step_avg:46.29ms
step:1331/2160 train_time:61627ms step_avg:46.30ms
step:1332/2160 train_time:61687ms step_avg:46.31ms
step:1333/2160 train_time:61748ms step_avg:46.32ms
step:1334/2160 train_time:61808ms step_avg:46.33ms
step:1335/2160 train_time:61869ms step_avg:46.34ms
step:1336/2160 train_time:61929ms step_avg:46.35ms
step:1337/2160 train_time:61991ms step_avg:46.37ms
step:1338/2160 train_time:62050ms step_avg:46.38ms
step:1339/2160 train_time:62111ms step_avg:46.39ms
step:1340/2160 train_time:62171ms step_avg:46.40ms
step:1341/2160 train_time:62233ms step_avg:46.41ms
step:1342/2160 train_time:62292ms step_avg:46.42ms
step:1343/2160 train_time:62353ms step_avg:46.43ms
step:1344/2160 train_time:62413ms step_avg:46.44ms
step:1345/2160 train_time:62474ms step_avg:46.45ms
step:1346/2160 train_time:62533ms step_avg:46.46ms
step:1347/2160 train_time:62594ms step_avg:46.47ms
step:1348/2160 train_time:62653ms step_avg:46.48ms
step:1349/2160 train_time:62714ms step_avg:46.49ms
step:1350/2160 train_time:62774ms step_avg:46.50ms
step:1351/2160 train_time:62835ms step_avg:46.51ms
step:1352/2160 train_time:62894ms step_avg:46.52ms
step:1353/2160 train_time:62956ms step_avg:46.53ms
step:1354/2160 train_time:63015ms step_avg:46.54ms
step:1355/2160 train_time:63077ms step_avg:46.55ms
step:1356/2160 train_time:63136ms step_avg:46.56ms
step:1357/2160 train_time:63197ms step_avg:46.57ms
step:1358/2160 train_time:63256ms step_avg:46.58ms
step:1359/2160 train_time:63317ms step_avg:46.59ms
step:1360/2160 train_time:63376ms step_avg:46.60ms
step:1361/2160 train_time:63437ms step_avg:46.61ms
step:1362/2160 train_time:63497ms step_avg:46.62ms
step:1363/2160 train_time:63558ms step_avg:46.63ms
step:1364/2160 train_time:63618ms step_avg:46.64ms
step:1365/2160 train_time:63680ms step_avg:46.65ms
step:1366/2160 train_time:63739ms step_avg:46.66ms
step:1367/2160 train_time:63800ms step_avg:46.67ms
step:1368/2160 train_time:63860ms step_avg:46.68ms
step:1369/2160 train_time:63921ms step_avg:46.69ms
step:1370/2160 train_time:63981ms step_avg:46.70ms
step:1371/2160 train_time:64043ms step_avg:46.71ms
step:1372/2160 train_time:64103ms step_avg:46.72ms
step:1373/2160 train_time:64164ms step_avg:46.73ms
step:1374/2160 train_time:64224ms step_avg:46.74ms
step:1375/2160 train_time:64286ms step_avg:46.75ms
step:1376/2160 train_time:64346ms step_avg:46.76ms
step:1377/2160 train_time:64408ms step_avg:46.77ms
step:1378/2160 train_time:64467ms step_avg:46.78ms
step:1379/2160 train_time:64529ms step_avg:46.79ms
step:1380/2160 train_time:64588ms step_avg:46.80ms
step:1381/2160 train_time:64649ms step_avg:46.81ms
step:1382/2160 train_time:64709ms step_avg:46.82ms
step:1383/2160 train_time:64770ms step_avg:46.83ms
step:1384/2160 train_time:64830ms step_avg:46.84ms
step:1385/2160 train_time:64891ms step_avg:46.85ms
step:1386/2160 train_time:64952ms step_avg:46.86ms
step:1387/2160 train_time:65013ms step_avg:46.87ms
step:1388/2160 train_time:65073ms step_avg:46.88ms
step:1389/2160 train_time:65134ms step_avg:46.89ms
step:1390/2160 train_time:65193ms step_avg:46.90ms
step:1391/2160 train_time:65254ms step_avg:46.91ms
step:1392/2160 train_time:65313ms step_avg:46.92ms
step:1393/2160 train_time:65375ms step_avg:46.93ms
step:1394/2160 train_time:65434ms step_avg:46.94ms
step:1395/2160 train_time:65495ms step_avg:46.95ms
step:1396/2160 train_time:65554ms step_avg:46.96ms
step:1397/2160 train_time:65615ms step_avg:46.97ms
step:1398/2160 train_time:65675ms step_avg:46.98ms
step:1399/2160 train_time:65737ms step_avg:46.99ms
step:1400/2160 train_time:65796ms step_avg:47.00ms
step:1401/2160 train_time:65857ms step_avg:47.01ms
step:1402/2160 train_time:65916ms step_avg:47.02ms
step:1403/2160 train_time:65977ms step_avg:47.03ms
step:1404/2160 train_time:66037ms step_avg:47.03ms
step:1405/2160 train_time:66098ms step_avg:47.04ms
step:1406/2160 train_time:66158ms step_avg:47.05ms
step:1407/2160 train_time:66219ms step_avg:47.06ms
step:1408/2160 train_time:66279ms step_avg:47.07ms
step:1409/2160 train_time:66341ms step_avg:47.08ms
step:1410/2160 train_time:66401ms step_avg:47.09ms
step:1411/2160 train_time:66462ms step_avg:47.10ms
step:1412/2160 train_time:66522ms step_avg:47.11ms
step:1413/2160 train_time:66584ms step_avg:47.12ms
step:1414/2160 train_time:66644ms step_avg:47.13ms
step:1415/2160 train_time:66707ms step_avg:47.14ms
step:1416/2160 train_time:66794ms step_avg:47.17ms
step:1417/2160 train_time:66884ms step_avg:47.20ms
step:1418/2160 train_time:66972ms step_avg:47.23ms
step:1419/2160 train_time:67061ms step_avg:47.26ms
step:1420/2160 train_time:67148ms step_avg:47.29ms
step:1421/2160 train_time:67238ms step_avg:47.32ms
step:1422/2160 train_time:67325ms step_avg:47.35ms
step:1423/2160 train_time:67414ms step_avg:47.37ms
step:1424/2160 train_time:67501ms step_avg:47.40ms
step:1425/2160 train_time:67590ms step_avg:47.43ms
step:1426/2160 train_time:67678ms step_avg:47.46ms
step:1427/2160 train_time:67767ms step_avg:47.49ms
step:1428/2160 train_time:67854ms step_avg:47.52ms
step:1429/2160 train_time:67943ms step_avg:47.55ms
step:1430/2160 train_time:68029ms step_avg:47.57ms
step:1431/2160 train_time:68119ms step_avg:47.60ms
step:1432/2160 train_time:68207ms step_avg:47.63ms
step:1433/2160 train_time:68296ms step_avg:47.66ms
step:1434/2160 train_time:68383ms step_avg:47.69ms
step:1435/2160 train_time:68472ms step_avg:47.72ms
step:1436/2160 train_time:68560ms step_avg:47.74ms
step:1437/2160 train_time:68649ms step_avg:47.77ms
step:1438/2160 train_time:68737ms step_avg:47.80ms
step:1439/2160 train_time:68826ms step_avg:47.83ms
step:1440/2160 train_time:68913ms step_avg:47.86ms
step:1441/2160 train_time:69002ms step_avg:47.89ms
step:1442/2160 train_time:69089ms step_avg:47.91ms
step:1443/2160 train_time:69180ms step_avg:47.94ms
step:1444/2160 train_time:69268ms step_avg:47.97ms
step:1445/2160 train_time:69358ms step_avg:48.00ms
step:1446/2160 train_time:69445ms step_avg:48.03ms
step:1447/2160 train_time:69535ms step_avg:48.05ms
step:1448/2160 train_time:69622ms step_avg:48.08ms
step:1449/2160 train_time:69711ms step_avg:48.11ms
step:1450/2160 train_time:69798ms step_avg:48.14ms
step:1451/2160 train_time:69888ms step_avg:48.17ms
step:1452/2160 train_time:69975ms step_avg:48.19ms
step:1453/2160 train_time:70064ms step_avg:48.22ms
step:1454/2160 train_time:70151ms step_avg:48.25ms
step:1455/2160 train_time:70240ms step_avg:48.27ms
step:1456/2160 train_time:70328ms step_avg:48.30ms
step:1457/2160 train_time:70417ms step_avg:48.33ms
step:1458/2160 train_time:70504ms step_avg:48.36ms
step:1459/2160 train_time:70593ms step_avg:48.38ms
step:1460/2160 train_time:70680ms step_avg:48.41ms
step:1461/2160 train_time:70770ms step_avg:48.44ms
step:1462/2160 train_time:70857ms step_avg:48.47ms
step:1463/2160 train_time:70946ms step_avg:48.49ms
step:1464/2160 train_time:71033ms step_avg:48.52ms
step:1465/2160 train_time:71122ms step_avg:48.55ms
step:1466/2160 train_time:71209ms step_avg:48.57ms
step:1467/2160 train_time:71300ms step_avg:48.60ms
step:1468/2160 train_time:71387ms step_avg:48.63ms
step:1469/2160 train_time:71476ms step_avg:48.66ms
step:1470/2160 train_time:71563ms step_avg:48.68ms
step:1471/2160 train_time:71653ms step_avg:48.71ms
step:1472/2160 train_time:71741ms step_avg:48.74ms
step:1473/2160 train_time:71830ms step_avg:48.76ms
step:1474/2160 train_time:71917ms step_avg:48.79ms
step:1475/2160 train_time:72006ms step_avg:48.82ms
step:1476/2160 train_time:72093ms step_avg:48.84ms
step:1477/2160 train_time:72183ms step_avg:48.87ms
step:1478/2160 train_time:72269ms step_avg:48.90ms
step:1479/2160 train_time:72359ms step_avg:48.92ms
step:1480/2160 train_time:72446ms step_avg:48.95ms
step:1481/2160 train_time:72536ms step_avg:48.98ms
step:1482/2160 train_time:72623ms step_avg:49.00ms
step:1483/2160 train_time:72713ms step_avg:49.03ms
step:1484/2160 train_time:72800ms step_avg:49.06ms
step:1485/2160 train_time:72889ms step_avg:49.08ms
step:1486/2160 train_time:72977ms step_avg:49.11ms
step:1487/2160 train_time:73065ms step_avg:49.14ms
step:1488/2160 train_time:73152ms step_avg:49.16ms
step:1489/2160 train_time:73241ms step_avg:49.19ms
step:1490/2160 train_time:73328ms step_avg:49.21ms
step:1491/2160 train_time:73418ms step_avg:49.24ms
step:1492/2160 train_time:73505ms step_avg:49.27ms
step:1493/2160 train_time:73594ms step_avg:49.29ms
step:1494/2160 train_time:73682ms step_avg:49.32ms
step:1495/2160 train_time:73771ms step_avg:49.35ms
step:1496/2160 train_time:73859ms step_avg:49.37ms
step:1497/2160 train_time:73948ms step_avg:49.40ms
step:1498/2160 train_time:74035ms step_avg:49.42ms
step:1499/2160 train_time:74124ms step_avg:49.45ms
step:1500/2160 train_time:74212ms step_avg:49.47ms
step:1500/2160 val_loss:3.4933 train_time:74301ms step_avg:49.53ms
step:1501/2160 train_time:74321ms step_avg:49.51ms
step:1502/2160 train_time:74391ms step_avg:49.53ms
step:1503/2160 train_time:74484ms step_avg:49.56ms
step:1504/2160 train_time:74572ms step_avg:49.58ms
step:1505/2160 train_time:74660ms step_avg:49.61ms
step:1506/2160 train_time:74747ms step_avg:49.63ms
step:1507/2160 train_time:74834ms step_avg:49.66ms
step:1508/2160 train_time:74921ms step_avg:49.68ms
step:1509/2160 train_time:75009ms step_avg:49.71ms
step:1510/2160 train_time:75095ms step_avg:49.73ms
step:1511/2160 train_time:75187ms step_avg:49.76ms
step:1512/2160 train_time:75281ms step_avg:49.79ms
step:1513/2160 train_time:75371ms step_avg:49.82ms
step:1514/2160 train_time:75460ms step_avg:49.84ms
step:1515/2160 train_time:75550ms step_avg:49.87ms
step:1516/2160 train_time:75637ms step_avg:49.89ms
step:1517/2160 train_time:75726ms step_avg:49.92ms
step:1518/2160 train_time:75812ms step_avg:49.94ms
step:1519/2160 train_time:75900ms step_avg:49.97ms
step:1520/2160 train_time:75987ms step_avg:49.99ms
step:1521/2160 train_time:76076ms step_avg:50.02ms
step:1522/2160 train_time:76164ms step_avg:50.04ms
step:1523/2160 train_time:76257ms step_avg:50.07ms
step:1524/2160 train_time:76345ms step_avg:50.10ms
step:1525/2160 train_time:76435ms step_avg:50.12ms
step:1526/2160 train_time:76524ms step_avg:50.15ms
step:1527/2160 train_time:76612ms step_avg:50.17ms
step:1528/2160 train_time:76699ms step_avg:50.20ms
step:1529/2160 train_time:76789ms step_avg:50.22ms
step:1530/2160 train_time:76875ms step_avg:50.24ms
step:1531/2160 train_time:76964ms step_avg:50.27ms
step:1532/2160 train_time:77050ms step_avg:50.29ms
step:1533/2160 train_time:77140ms step_avg:50.32ms
step:1534/2160 train_time:77229ms step_avg:50.34ms
step:1535/2160 train_time:77318ms step_avg:50.37ms
step:1536/2160 train_time:77407ms step_avg:50.39ms
step:1537/2160 train_time:77496ms step_avg:50.42ms
step:1538/2160 train_time:77584ms step_avg:50.44ms
step:1539/2160 train_time:77673ms step_avg:50.47ms
step:1540/2160 train_time:77760ms step_avg:50.49ms
step:1541/2160 train_time:77848ms step_avg:50.52ms
step:1542/2160 train_time:77935ms step_avg:50.54ms
step:1543/2160 train_time:78024ms step_avg:50.57ms
step:1544/2160 train_time:78111ms step_avg:50.59ms
step:1545/2160 train_time:78201ms step_avg:50.62ms
step:1546/2160 train_time:78290ms step_avg:50.64ms
step:1547/2160 train_time:78380ms step_avg:50.67ms
step:1548/2160 train_time:78468ms step_avg:50.69ms
step:1549/2160 train_time:78558ms step_avg:50.72ms
step:1550/2160 train_time:78646ms step_avg:50.74ms
step:1551/2160 train_time:78734ms step_avg:50.76ms
step:1552/2160 train_time:78822ms step_avg:50.79ms
step:1553/2160 train_time:78910ms step_avg:50.81ms
step:1554/2160 train_time:78997ms step_avg:50.83ms
step:1555/2160 train_time:79086ms step_avg:50.86ms
step:1556/2160 train_time:79174ms step_avg:50.88ms
step:1557/2160 train_time:79263ms step_avg:50.91ms
step:1558/2160 train_time:79351ms step_avg:50.93ms
step:1559/2160 train_time:79441ms step_avg:50.96ms
step:1560/2160 train_time:79528ms step_avg:50.98ms
step:1561/2160 train_time:79617ms step_avg:51.00ms
step:1562/2160 train_time:79705ms step_avg:51.03ms
step:1563/2160 train_time:79793ms step_avg:51.05ms
step:1564/2160 train_time:79880ms step_avg:51.07ms
step:1565/2160 train_time:79969ms step_avg:51.10ms
step:1566/2160 train_time:80056ms step_avg:51.12ms
step:1567/2160 train_time:80145ms step_avg:51.15ms
step:1568/2160 train_time:80232ms step_avg:51.17ms
step:1569/2160 train_time:80322ms step_avg:51.19ms
step:1570/2160 train_time:80410ms step_avg:51.22ms
step:1571/2160 train_time:80499ms step_avg:51.24ms
step:1572/2160 train_time:80586ms step_avg:51.26ms
step:1573/2160 train_time:80675ms step_avg:51.29ms
step:1574/2160 train_time:80762ms step_avg:51.31ms
step:1575/2160 train_time:80851ms step_avg:51.33ms
step:1576/2160 train_time:80938ms step_avg:51.36ms
step:1577/2160 train_time:81027ms step_avg:51.38ms
step:1578/2160 train_time:81114ms step_avg:51.40ms
step:1579/2160 train_time:81204ms step_avg:51.43ms
step:1580/2160 train_time:81292ms step_avg:51.45ms
step:1581/2160 train_time:81381ms step_avg:51.47ms
step:1582/2160 train_time:81469ms step_avg:51.50ms
step:1583/2160 train_time:81558ms step_avg:51.52ms
step:1584/2160 train_time:81646ms step_avg:51.54ms
step:1585/2160 train_time:81735ms step_avg:51.57ms
step:1586/2160 train_time:81823ms step_avg:51.59ms
step:1587/2160 train_time:81912ms step_avg:51.61ms
step:1588/2160 train_time:81999ms step_avg:51.64ms
step:1589/2160 train_time:82088ms step_avg:51.66ms
step:1590/2160 train_time:82175ms step_avg:51.68ms
step:1591/2160 train_time:82267ms step_avg:51.71ms
step:1592/2160 train_time:82354ms step_avg:51.73ms
step:1593/2160 train_time:82443ms step_avg:51.75ms
step:1594/2160 train_time:82531ms step_avg:51.78ms
step:1595/2160 train_time:82620ms step_avg:51.80ms
step:1596/2160 train_time:82707ms step_avg:51.82ms
step:1597/2160 train_time:82796ms step_avg:51.84ms
step:1598/2160 train_time:82882ms step_avg:51.87ms
step:1599/2160 train_time:82972ms step_avg:51.89ms
step:1600/2160 train_time:83060ms step_avg:51.91ms
step:1601/2160 train_time:83149ms step_avg:51.94ms
step:1602/2160 train_time:83237ms step_avg:51.96ms
step:1603/2160 train_time:83326ms step_avg:51.98ms
step:1604/2160 train_time:83414ms step_avg:52.00ms
step:1605/2160 train_time:83503ms step_avg:52.03ms
step:1606/2160 train_time:83591ms step_avg:52.05ms
step:1607/2160 train_time:83679ms step_avg:52.07ms
step:1608/2160 train_time:83768ms step_avg:52.09ms
step:1609/2160 train_time:83857ms step_avg:52.12ms
step:1610/2160 train_time:83945ms step_avg:52.14ms
step:1611/2160 train_time:84035ms step_avg:52.16ms
step:1612/2160 train_time:84122ms step_avg:52.19ms
step:1613/2160 train_time:84211ms step_avg:52.21ms
step:1614/2160 train_time:84299ms step_avg:52.23ms
step:1615/2160 train_time:84388ms step_avg:52.25ms
step:1616/2160 train_time:84475ms step_avg:52.27ms
step:1617/2160 train_time:84564ms step_avg:52.30ms
step:1618/2160 train_time:84652ms step_avg:52.32ms
step:1619/2160 train_time:84741ms step_avg:52.34ms
step:1620/2160 train_time:84829ms step_avg:52.36ms
step:1621/2160 train_time:84918ms step_avg:52.39ms
step:1622/2160 train_time:85006ms step_avg:52.41ms
step:1623/2160 train_time:85095ms step_avg:52.43ms
step:1624/2160 train_time:85182ms step_avg:52.45ms
step:1625/2160 train_time:85271ms step_avg:52.47ms
step:1626/2160 train_time:85358ms step_avg:52.50ms
step:1627/2160 train_time:85448ms step_avg:52.52ms
step:1628/2160 train_time:85536ms step_avg:52.54ms
step:1629/2160 train_time:85625ms step_avg:52.56ms
step:1630/2160 train_time:85713ms step_avg:52.58ms
step:1631/2160 train_time:85802ms step_avg:52.61ms
step:1632/2160 train_time:85890ms step_avg:52.63ms
step:1633/2160 train_time:85980ms step_avg:52.65ms
step:1634/2160 train_time:86067ms step_avg:52.67ms
step:1635/2160 train_time:86157ms step_avg:52.70ms
step:1636/2160 train_time:86245ms step_avg:52.72ms
step:1637/2160 train_time:86334ms step_avg:52.74ms
step:1638/2160 train_time:86421ms step_avg:52.76ms
step:1639/2160 train_time:86510ms step_avg:52.78ms
step:1640/2160 train_time:86597ms step_avg:52.80ms
step:1641/2160 train_time:86688ms step_avg:52.83ms
step:1642/2160 train_time:86776ms step_avg:52.85ms
step:1643/2160 train_time:86865ms step_avg:52.87ms
step:1644/2160 train_time:86953ms step_avg:52.89ms
step:1645/2160 train_time:87042ms step_avg:52.91ms
step:1646/2160 train_time:87131ms step_avg:52.93ms
step:1647/2160 train_time:87219ms step_avg:52.96ms
step:1648/2160 train_time:87308ms step_avg:52.98ms
step:1649/2160 train_time:87397ms step_avg:53.00ms
step:1650/2160 train_time:87484ms step_avg:53.02ms
step:1651/2160 train_time:87573ms step_avg:53.04ms
step:1652/2160 train_time:87661ms step_avg:53.06ms
step:1653/2160 train_time:87750ms step_avg:53.09ms
step:1654/2160 train_time:87838ms step_avg:53.11ms
step:1655/2160 train_time:87927ms step_avg:53.13ms
step:1656/2160 train_time:88014ms step_avg:53.15ms
step:1657/2160 train_time:88104ms step_avg:53.17ms
step:1658/2160 train_time:88192ms step_avg:53.19ms
step:1659/2160 train_time:88280ms step_avg:53.21ms
step:1660/2160 train_time:88368ms step_avg:53.23ms
step:1661/2160 train_time:88457ms step_avg:53.26ms
step:1662/2160 train_time:88545ms step_avg:53.28ms
step:1663/2160 train_time:88635ms step_avg:53.30ms
step:1664/2160 train_time:88722ms step_avg:53.32ms
step:1665/2160 train_time:88811ms step_avg:53.34ms
step:1666/2160 train_time:88899ms step_avg:53.36ms
step:1667/2160 train_time:88989ms step_avg:53.38ms
step:1668/2160 train_time:89076ms step_avg:53.40ms
step:1669/2160 train_time:89166ms step_avg:53.42ms
step:1670/2160 train_time:89254ms step_avg:53.45ms
step:1671/2160 train_time:89344ms step_avg:53.47ms
step:1672/2160 train_time:89430ms step_avg:53.49ms
step:1673/2160 train_time:89520ms step_avg:53.51ms
step:1674/2160 train_time:89607ms step_avg:53.53ms
step:1675/2160 train_time:89697ms step_avg:53.55ms
step:1676/2160 train_time:89784ms step_avg:53.57ms
step:1677/2160 train_time:89873ms step_avg:53.59ms
step:1678/2160 train_time:89960ms step_avg:53.61ms
step:1679/2160 train_time:90049ms step_avg:53.63ms
step:1680/2160 train_time:90138ms step_avg:53.65ms
step:1681/2160 train_time:90227ms step_avg:53.67ms
step:1682/2160 train_time:90315ms step_avg:53.69ms
step:1683/2160 train_time:90404ms step_avg:53.72ms
step:1684/2160 train_time:90491ms step_avg:53.74ms
step:1685/2160 train_time:90580ms step_avg:53.76ms
step:1686/2160 train_time:90668ms step_avg:53.78ms
step:1687/2160 train_time:90758ms step_avg:53.80ms
step:1688/2160 train_time:90845ms step_avg:53.82ms
step:1689/2160 train_time:90935ms step_avg:53.84ms
step:1690/2160 train_time:91022ms step_avg:53.86ms
step:1691/2160 train_time:91112ms step_avg:53.88ms
step:1692/2160 train_time:91199ms step_avg:53.90ms
step:1693/2160 train_time:91289ms step_avg:53.92ms
step:1694/2160 train_time:91376ms step_avg:53.94ms
step:1695/2160 train_time:91466ms step_avg:53.96ms
step:1696/2160 train_time:91554ms step_avg:53.98ms
step:1697/2160 train_time:91643ms step_avg:54.00ms
step:1698/2160 train_time:91731ms step_avg:54.02ms
step:1699/2160 train_time:91820ms step_avg:54.04ms
step:1700/2160 train_time:91907ms step_avg:54.06ms
step:1701/2160 train_time:91996ms step_avg:54.08ms
step:1702/2160 train_time:92083ms step_avg:54.10ms
step:1703/2160 train_time:92174ms step_avg:54.12ms
step:1704/2160 train_time:92261ms step_avg:54.14ms
step:1705/2160 train_time:92350ms step_avg:54.16ms
step:1706/2160 train_time:92438ms step_avg:54.18ms
step:1707/2160 train_time:92527ms step_avg:54.20ms
step:1708/2160 train_time:92613ms step_avg:54.22ms
step:1709/2160 train_time:92703ms step_avg:54.24ms
step:1710/2160 train_time:92790ms step_avg:54.26ms
step:1711/2160 train_time:92879ms step_avg:54.28ms
step:1712/2160 train_time:92968ms step_avg:54.30ms
step:1713/2160 train_time:93057ms step_avg:54.32ms
step:1714/2160 train_time:93145ms step_avg:54.34ms
step:1715/2160 train_time:93235ms step_avg:54.36ms
step:1716/2160 train_time:93323ms step_avg:54.38ms
step:1717/2160 train_time:93412ms step_avg:54.40ms
step:1718/2160 train_time:93500ms step_avg:54.42ms
step:1719/2160 train_time:93589ms step_avg:54.44ms
step:1720/2160 train_time:93677ms step_avg:54.46ms
step:1721/2160 train_time:93766ms step_avg:54.48ms
step:1722/2160 train_time:93854ms step_avg:54.50ms
step:1723/2160 train_time:93944ms step_avg:54.52ms
step:1724/2160 train_time:94031ms step_avg:54.54ms
step:1725/2160 train_time:94120ms step_avg:54.56ms
step:1726/2160 train_time:94207ms step_avg:54.58ms
step:1727/2160 train_time:94297ms step_avg:54.60ms
step:1728/2160 train_time:94384ms step_avg:54.62ms
step:1729/2160 train_time:94473ms step_avg:54.64ms
step:1730/2160 train_time:94560ms step_avg:54.66ms
step:1731/2160 train_time:94650ms step_avg:54.68ms
step:1732/2160 train_time:94737ms step_avg:54.70ms
step:1733/2160 train_time:94827ms step_avg:54.72ms
step:1734/2160 train_time:94914ms step_avg:54.74ms
step:1735/2160 train_time:95004ms step_avg:54.76ms
step:1736/2160 train_time:95092ms step_avg:54.78ms
step:1737/2160 train_time:95180ms step_avg:54.80ms
step:1738/2160 train_time:95268ms step_avg:54.81ms
step:1739/2160 train_time:95357ms step_avg:54.83ms
step:1740/2160 train_time:95444ms step_avg:54.85ms
step:1741/2160 train_time:95534ms step_avg:54.87ms
step:1742/2160 train_time:95621ms step_avg:54.89ms
step:1743/2160 train_time:95712ms step_avg:54.91ms
step:1744/2160 train_time:95800ms step_avg:54.93ms
step:1745/2160 train_time:95890ms step_avg:54.95ms
step:1746/2160 train_time:95977ms step_avg:54.97ms
step:1747/2160 train_time:96067ms step_avg:54.99ms
step:1748/2160 train_time:96154ms step_avg:55.01ms
step:1749/2160 train_time:96244ms step_avg:55.03ms
step:1750/2160 train_time:96331ms step_avg:55.05ms
step:1750/2160 val_loss:3.3911 train_time:96421ms step_avg:55.10ms
step:1751/2160 train_time:96441ms step_avg:55.08ms
step:1752/2160 train_time:96512ms step_avg:55.09ms
step:1753/2160 train_time:96601ms step_avg:55.11ms
step:1754/2160 train_time:96689ms step_avg:55.12ms
step:1755/2160 train_time:96777ms step_avg:55.14ms
step:1756/2160 train_time:96864ms step_avg:55.16ms
step:1757/2160 train_time:96952ms step_avg:55.18ms
step:1758/2160 train_time:97038ms step_avg:55.20ms
step:1759/2160 train_time:97127ms step_avg:55.22ms
step:1760/2160 train_time:97215ms step_avg:55.24ms
step:1761/2160 train_time:97303ms step_avg:55.25ms
step:1762/2160 train_time:97395ms step_avg:55.28ms
step:1763/2160 train_time:97486ms step_avg:55.30ms
step:1764/2160 train_time:97575ms step_avg:55.31ms
step:1765/2160 train_time:97664ms step_avg:55.33ms
step:1766/2160 train_time:97751ms step_avg:55.35ms
step:1767/2160 train_time:97840ms step_avg:55.37ms
step:1768/2160 train_time:97927ms step_avg:55.39ms
step:1769/2160 train_time:98015ms step_avg:55.41ms
step:1770/2160 train_time:98102ms step_avg:55.42ms
step:1771/2160 train_time:98191ms step_avg:55.44ms
step:1772/2160 train_time:98278ms step_avg:55.46ms
step:1773/2160 train_time:98369ms step_avg:55.48ms
step:1774/2160 train_time:98458ms step_avg:55.50ms
step:1775/2160 train_time:98548ms step_avg:55.52ms
step:1776/2160 train_time:98635ms step_avg:55.54ms
step:1777/2160 train_time:98725ms step_avg:55.56ms
step:1778/2160 train_time:98812ms step_avg:55.57ms
step:1779/2160 train_time:98900ms step_avg:55.59ms
step:1780/2160 train_time:98988ms step_avg:55.61ms
step:1781/2160 train_time:99076ms step_avg:55.63ms
step:1782/2160 train_time:99163ms step_avg:55.65ms
step:1783/2160 train_time:99252ms step_avg:55.67ms
step:1784/2160 train_time:99340ms step_avg:55.68ms
step:1785/2160 train_time:99430ms step_avg:55.70ms
step:1786/2160 train_time:99519ms step_avg:55.72ms
step:1787/2160 train_time:99608ms step_avg:55.74ms
step:1788/2160 train_time:99696ms step_avg:55.76ms
step:1789/2160 train_time:99785ms step_avg:55.78ms
step:1790/2160 train_time:99873ms step_avg:55.79ms
step:1791/2160 train_time:99963ms step_avg:55.81ms
step:1792/2160 train_time:100049ms step_avg:55.83ms
step:1793/2160 train_time:100138ms step_avg:55.85ms
step:1794/2160 train_time:100226ms step_avg:55.87ms
step:1795/2160 train_time:100315ms step_avg:55.89ms
step:1796/2160 train_time:100403ms step_avg:55.90ms
step:1797/2160 train_time:100492ms step_avg:55.92ms
step:1798/2160 train_time:100580ms step_avg:55.94ms
step:1799/2160 train_time:100670ms step_avg:55.96ms
step:1800/2160 train_time:100758ms step_avg:55.98ms
step:1801/2160 train_time:100847ms step_avg:56.00ms
step:1802/2160 train_time:100934ms step_avg:56.01ms
step:1803/2160 train_time:101023ms step_avg:56.03ms
step:1804/2160 train_time:101110ms step_avg:56.05ms
step:1805/2160 train_time:101199ms step_avg:56.07ms
step:1806/2160 train_time:101287ms step_avg:56.08ms
step:1807/2160 train_time:101376ms step_avg:56.10ms
step:1808/2160 train_time:101464ms step_avg:56.12ms
step:1809/2160 train_time:101555ms step_avg:56.14ms
step:1810/2160 train_time:101642ms step_avg:56.16ms
step:1811/2160 train_time:101733ms step_avg:56.17ms
step:1812/2160 train_time:101821ms step_avg:56.19ms
step:1813/2160 train_time:101910ms step_avg:56.21ms
step:1814/2160 train_time:101997ms step_avg:56.23ms
step:1815/2160 train_time:102086ms step_avg:56.25ms
step:1816/2160 train_time:102173ms step_avg:56.26ms
step:1817/2160 train_time:102262ms step_avg:56.28ms
step:1818/2160 train_time:102349ms step_avg:56.30ms
step:1819/2160 train_time:102438ms step_avg:56.32ms
step:1820/2160 train_time:102525ms step_avg:56.33ms
step:1821/2160 train_time:102615ms step_avg:56.35ms
step:1822/2160 train_time:102703ms step_avg:56.37ms
step:1823/2160 train_time:102792ms step_avg:56.39ms
step:1824/2160 train_time:102879ms step_avg:56.40ms
step:1825/2160 train_time:102969ms step_avg:56.42ms
step:1826/2160 train_time:103056ms step_avg:56.44ms
step:1827/2160 train_time:103145ms step_avg:56.46ms
step:1828/2160 train_time:103232ms step_avg:56.47ms
step:1829/2160 train_time:103322ms step_avg:56.49ms
step:1830/2160 train_time:103409ms step_avg:56.51ms
step:1831/2160 train_time:103498ms step_avg:56.53ms
step:1832/2160 train_time:103587ms step_avg:56.54ms
step:1833/2160 train_time:103676ms step_avg:56.56ms
step:1834/2160 train_time:103764ms step_avg:56.58ms
step:1835/2160 train_time:103853ms step_avg:56.60ms
step:1836/2160 train_time:103940ms step_avg:56.61ms
step:1837/2160 train_time:104029ms step_avg:56.63ms
step:1838/2160 train_time:104116ms step_avg:56.65ms
step:1839/2160 train_time:104206ms step_avg:56.66ms
step:1840/2160 train_time:104293ms step_avg:56.68ms
step:1841/2160 train_time:104382ms step_avg:56.70ms
step:1842/2160 train_time:104470ms step_avg:56.72ms
step:1843/2160 train_time:104559ms step_avg:56.73ms
step:1844/2160 train_time:104647ms step_avg:56.75ms
step:1845/2160 train_time:104736ms step_avg:56.77ms
step:1846/2160 train_time:104824ms step_avg:56.78ms
step:1847/2160 train_time:104914ms step_avg:56.80ms
step:1848/2160 train_time:105001ms step_avg:56.82ms
step:1849/2160 train_time:105090ms step_avg:56.84ms
step:1850/2160 train_time:105178ms step_avg:56.85ms
step:1851/2160 train_time:105267ms step_avg:56.87ms
step:1852/2160 train_time:105355ms step_avg:56.89ms
step:1853/2160 train_time:105444ms step_avg:56.90ms
step:1854/2160 train_time:105531ms step_avg:56.92ms
step:1855/2160 train_time:105621ms step_avg:56.94ms
step:1856/2160 train_time:105709ms step_avg:56.96ms
step:1857/2160 train_time:105800ms step_avg:56.97ms
step:1858/2160 train_time:105887ms step_avg:56.99ms
step:1859/2160 train_time:105977ms step_avg:57.01ms
step:1860/2160 train_time:106065ms step_avg:57.02ms
step:1861/2160 train_time:106154ms step_avg:57.04ms
step:1862/2160 train_time:106242ms step_avg:57.06ms
step:1863/2160 train_time:106331ms step_avg:57.08ms
step:1864/2160 train_time:106418ms step_avg:57.09ms
step:1865/2160 train_time:106507ms step_avg:57.11ms
step:1866/2160 train_time:106594ms step_avg:57.12ms
step:1867/2160 train_time:106683ms step_avg:57.14ms
step:1868/2160 train_time:106770ms step_avg:57.16ms
step:1869/2160 train_time:106860ms step_avg:57.17ms
step:1870/2160 train_time:106947ms step_avg:57.19ms
step:1871/2160 train_time:107037ms step_avg:57.21ms
step:1872/2160 train_time:107123ms step_avg:57.22ms
step:1873/2160 train_time:107213ms step_avg:57.24ms
step:1874/2160 train_time:107301ms step_avg:57.26ms
step:1875/2160 train_time:107391ms step_avg:57.28ms
step:1876/2160 train_time:107477ms step_avg:57.29ms
step:1877/2160 train_time:107566ms step_avg:57.31ms
step:1878/2160 train_time:107654ms step_avg:57.32ms
step:1879/2160 train_time:107743ms step_avg:57.34ms
step:1880/2160 train_time:107831ms step_avg:57.36ms
step:1881/2160 train_time:107921ms step_avg:57.37ms
step:1882/2160 train_time:108008ms step_avg:57.39ms
step:1883/2160 train_time:108097ms step_avg:57.41ms
step:1884/2160 train_time:108185ms step_avg:57.42ms
step:1885/2160 train_time:108274ms step_avg:57.44ms
step:1886/2160 train_time:108361ms step_avg:57.46ms
step:1887/2160 train_time:108450ms step_avg:57.47ms
step:1888/2160 train_time:108537ms step_avg:57.49ms
step:1889/2160 train_time:108627ms step_avg:57.51ms
step:1890/2160 train_time:108715ms step_avg:57.52ms
step:1891/2160 train_time:108804ms step_avg:57.54ms
step:1892/2160 train_time:108892ms step_avg:57.55ms
step:1893/2160 train_time:108981ms step_avg:57.57ms
step:1894/2160 train_time:109068ms step_avg:57.59ms
step:1895/2160 train_time:109157ms step_avg:57.60ms
step:1896/2160 train_time:109245ms step_avg:57.62ms
step:1897/2160 train_time:109334ms step_avg:57.64ms
step:1898/2160 train_time:109422ms step_avg:57.65ms
step:1899/2160 train_time:109511ms step_avg:57.67ms
step:1900/2160 train_time:109599ms step_avg:57.68ms
step:1901/2160 train_time:109688ms step_avg:57.70ms
step:1902/2160 train_time:109775ms step_avg:57.72ms
step:1903/2160 train_time:109865ms step_avg:57.73ms
step:1904/2160 train_time:109953ms step_avg:57.75ms
step:1905/2160 train_time:110042ms step_avg:57.76ms
step:1906/2160 train_time:110129ms step_avg:57.78ms
step:1907/2160 train_time:110219ms step_avg:57.80ms
step:1908/2160 train_time:110306ms step_avg:57.81ms
step:1909/2160 train_time:110396ms step_avg:57.83ms
step:1910/2160 train_time:110483ms step_avg:57.84ms
step:1911/2160 train_time:110572ms step_avg:57.86ms
step:1912/2160 train_time:110660ms step_avg:57.88ms
step:1913/2160 train_time:110749ms step_avg:57.89ms
step:1914/2160 train_time:110836ms step_avg:57.91ms
step:1915/2160 train_time:110925ms step_avg:57.92ms
step:1916/2160 train_time:111013ms step_avg:57.94ms
step:1917/2160 train_time:111103ms step_avg:57.96ms
step:1918/2160 train_time:111190ms step_avg:57.97ms
step:1919/2160 train_time:111280ms step_avg:57.99ms
step:1920/2160 train_time:111366ms step_avg:58.00ms
step:1921/2160 train_time:111455ms step_avg:58.02ms
step:1922/2160 train_time:111542ms step_avg:58.03ms
step:1923/2160 train_time:111631ms step_avg:58.05ms
step:1924/2160 train_time:111719ms step_avg:58.07ms
step:1925/2160 train_time:111808ms step_avg:58.08ms
step:1926/2160 train_time:111895ms step_avg:58.10ms
step:1927/2160 train_time:111985ms step_avg:58.11ms
step:1928/2160 train_time:112072ms step_avg:58.13ms
step:1929/2160 train_time:112161ms step_avg:58.14ms
step:1930/2160 train_time:112248ms step_avg:58.16ms
step:1931/2160 train_time:112338ms step_avg:58.18ms
step:1932/2160 train_time:112426ms step_avg:58.19ms
step:1933/2160 train_time:112515ms step_avg:58.21ms
step:1934/2160 train_time:112602ms step_avg:58.22ms
step:1935/2160 train_time:112691ms step_avg:58.24ms
step:1936/2160 train_time:112778ms step_avg:58.25ms
step:1937/2160 train_time:112868ms step_avg:58.27ms
step:1938/2160 train_time:112955ms step_avg:58.28ms
step:1939/2160 train_time:113045ms step_avg:58.30ms
step:1940/2160 train_time:113132ms step_avg:58.32ms
step:1941/2160 train_time:113222ms step_avg:58.33ms
step:1942/2160 train_time:113309ms step_avg:58.35ms
step:1943/2160 train_time:113399ms step_avg:58.36ms
step:1944/2160 train_time:113487ms step_avg:58.38ms
step:1945/2160 train_time:113576ms step_avg:58.39ms
step:1946/2160 train_time:113663ms step_avg:58.41ms
step:1947/2160 train_time:113753ms step_avg:58.42ms
step:1948/2160 train_time:113840ms step_avg:58.44ms
step:1949/2160 train_time:113929ms step_avg:58.46ms
step:1950/2160 train_time:114016ms step_avg:58.47ms
step:1951/2160 train_time:114106ms step_avg:58.49ms
step:1952/2160 train_time:114195ms step_avg:58.50ms
step:1953/2160 train_time:114284ms step_avg:58.52ms
step:1954/2160 train_time:114371ms step_avg:58.53ms
step:1955/2160 train_time:114460ms step_avg:58.55ms
step:1956/2160 train_time:114547ms step_avg:58.56ms
step:1957/2160 train_time:114636ms step_avg:58.58ms
step:1958/2160 train_time:114725ms step_avg:58.59ms
step:1959/2160 train_time:114813ms step_avg:58.61ms
step:1960/2160 train_time:114900ms step_avg:58.62ms
step:1961/2160 train_time:114989ms step_avg:58.64ms
step:1962/2160 train_time:115078ms step_avg:58.65ms
step:1963/2160 train_time:115167ms step_avg:58.67ms
step:1964/2160 train_time:115254ms step_avg:58.68ms
step:1965/2160 train_time:115344ms step_avg:58.70ms
step:1966/2160 train_time:115431ms step_avg:58.71ms
step:1967/2160 train_time:115520ms step_avg:58.73ms
step:1968/2160 train_time:115608ms step_avg:58.74ms
step:1969/2160 train_time:115697ms step_avg:58.76ms
step:1970/2160 train_time:115783ms step_avg:58.77ms
step:1971/2160 train_time:115873ms step_avg:58.79ms
step:1972/2160 train_time:115960ms step_avg:58.80ms
step:1973/2160 train_time:116050ms step_avg:58.82ms
step:1974/2160 train_time:116137ms step_avg:58.83ms
step:1975/2160 train_time:116227ms step_avg:58.85ms
step:1976/2160 train_time:116314ms step_avg:58.86ms
step:1977/2160 train_time:116403ms step_avg:58.88ms
step:1978/2160 train_time:116491ms step_avg:58.89ms
step:1979/2160 train_time:116580ms step_avg:58.91ms
step:1980/2160 train_time:116669ms step_avg:58.92ms
step:1981/2160 train_time:116758ms step_avg:58.94ms
step:1982/2160 train_time:116845ms step_avg:58.95ms
step:1983/2160 train_time:116937ms step_avg:58.97ms
step:1984/2160 train_time:117024ms step_avg:58.98ms
step:1985/2160 train_time:117114ms step_avg:59.00ms
step:1986/2160 train_time:117201ms step_avg:59.01ms
step:1987/2160 train_time:117291ms step_avg:59.03ms
step:1988/2160 train_time:117378ms step_avg:59.04ms
step:1989/2160 train_time:117468ms step_avg:59.06ms
step:1990/2160 train_time:117555ms step_avg:59.07ms
step:1991/2160 train_time:117645ms step_avg:59.09ms
step:1992/2160 train_time:117732ms step_avg:59.10ms
step:1993/2160 train_time:117821ms step_avg:59.12ms
step:1994/2160 train_time:117910ms step_avg:59.13ms
step:1995/2160 train_time:117999ms step_avg:59.15ms
step:1996/2160 train_time:118087ms step_avg:59.16ms
step:1997/2160 train_time:118176ms step_avg:59.18ms
step:1998/2160 train_time:118264ms step_avg:59.19ms
step:1999/2160 train_time:118353ms step_avg:59.21ms
step:2000/2160 train_time:118440ms step_avg:59.22ms
step:2000/2160 val_loss:3.3121 train_time:118531ms step_avg:59.27ms
step:2001/2160 train_time:118550ms step_avg:59.25ms
step:2002/2160 train_time:118622ms step_avg:59.25ms
step:2003/2160 train_time:118712ms step_avg:59.27ms
step:2004/2160 train_time:118800ms step_avg:59.28ms
step:2005/2160 train_time:118888ms step_avg:59.30ms
step:2006/2160 train_time:118975ms step_avg:59.31ms
step:2007/2160 train_time:119064ms step_avg:59.32ms
step:2008/2160 train_time:119150ms step_avg:59.34ms
step:2009/2160 train_time:119238ms step_avg:59.35ms
step:2010/2160 train_time:119327ms step_avg:59.37ms
step:2011/2160 train_time:119416ms step_avg:59.38ms
step:2012/2160 train_time:119507ms step_avg:59.40ms
step:2013/2160 train_time:119598ms step_avg:59.41ms
step:2014/2160 train_time:119685ms step_avg:59.43ms
step:2015/2160 train_time:119775ms step_avg:59.44ms
step:2016/2160 train_time:119863ms step_avg:59.46ms
step:2017/2160 train_time:119951ms step_avg:59.47ms
step:2018/2160 train_time:120038ms step_avg:59.48ms
step:2019/2160 train_time:120127ms step_avg:59.50ms
step:2020/2160 train_time:120214ms step_avg:59.51ms
step:2021/2160 train_time:120304ms step_avg:59.53ms
step:2022/2160 train_time:120392ms step_avg:59.54ms
step:2023/2160 train_time:120481ms step_avg:59.56ms
step:2024/2160 train_time:120571ms step_avg:59.57ms
step:2025/2160 train_time:120660ms step_avg:59.59ms
step:2026/2160 train_time:120748ms step_avg:59.60ms
step:2027/2160 train_time:120838ms step_avg:59.61ms
step:2028/2160 train_time:120926ms step_avg:59.63ms
step:2029/2160 train_time:121014ms step_avg:59.64ms
step:2030/2160 train_time:121101ms step_avg:59.66ms
step:2031/2160 train_time:121191ms step_avg:59.67ms
step:2032/2160 train_time:121277ms step_avg:59.68ms
step:2033/2160 train_time:121368ms step_avg:59.70ms
step:2034/2160 train_time:121455ms step_avg:59.71ms
step:2035/2160 train_time:121544ms step_avg:59.73ms
step:2036/2160 train_time:121632ms step_avg:59.74ms
step:2037/2160 train_time:121722ms step_avg:59.76ms
step:2038/2160 train_time:121812ms step_avg:59.77ms
step:2039/2160 train_time:121901ms step_avg:59.78ms
step:2040/2160 train_time:121988ms step_avg:59.80ms
step:2041/2160 train_time:122077ms step_avg:59.81ms
step:2042/2160 train_time:122165ms step_avg:59.83ms
step:2043/2160 train_time:122254ms step_avg:59.84ms
step:2044/2160 train_time:122341ms step_avg:59.85ms
step:2045/2160 train_time:122430ms step_avg:59.87ms
step:2046/2160 train_time:122518ms step_avg:59.88ms
step:2047/2160 train_time:122608ms step_avg:59.90ms
step:2048/2160 train_time:122696ms step_avg:59.91ms
step:2049/2160 train_time:122786ms step_avg:59.92ms
step:2050/2160 train_time:122874ms step_avg:59.94ms
step:2051/2160 train_time:122963ms step_avg:59.95ms
step:2052/2160 train_time:123050ms step_avg:59.97ms
step:2053/2160 train_time:123140ms step_avg:59.98ms
step:2054/2160 train_time:123228ms step_avg:59.99ms
step:2055/2160 train_time:123316ms step_avg:60.01ms
step:2056/2160 train_time:123403ms step_avg:60.02ms
step:2057/2160 train_time:123493ms step_avg:60.04ms
step:2058/2160 train_time:123581ms step_avg:60.05ms
step:2059/2160 train_time:123670ms step_avg:60.06ms
step:2060/2160 train_time:123758ms step_avg:60.08ms
step:2061/2160 train_time:123847ms step_avg:60.09ms
step:2062/2160 train_time:123935ms step_avg:60.10ms
step:2063/2160 train_time:124024ms step_avg:60.12ms
step:2064/2160 train_time:124111ms step_avg:60.13ms
step:2065/2160 train_time:124200ms step_avg:60.15ms
step:2066/2160 train_time:124288ms step_avg:60.16ms
step:2067/2160 train_time:124376ms step_avg:60.17ms
step:2068/2160 train_time:124464ms step_avg:60.19ms
step:2069/2160 train_time:124554ms step_avg:60.20ms
step:2070/2160 train_time:124642ms step_avg:60.21ms
step:2071/2160 train_time:124732ms step_avg:60.23ms
step:2072/2160 train_time:124820ms step_avg:60.24ms
step:2073/2160 train_time:124909ms step_avg:60.26ms
step:2074/2160 train_time:124997ms step_avg:60.27ms
step:2075/2160 train_time:125085ms step_avg:60.28ms
step:2076/2160 train_time:125173ms step_avg:60.30ms
step:2077/2160 train_time:125262ms step_avg:60.31ms
step:2078/2160 train_time:125350ms step_avg:60.32ms
step:2079/2160 train_time:125438ms step_avg:60.34ms
step:2080/2160 train_time:125526ms step_avg:60.35ms
step:2081/2160 train_time:125615ms step_avg:60.36ms
step:2082/2160 train_time:125703ms step_avg:60.38ms
step:2083/2160 train_time:125793ms step_avg:60.39ms
step:2084/2160 train_time:125880ms step_avg:60.40ms
step:2085/2160 train_time:125971ms step_avg:60.42ms
step:2086/2160 train_time:126058ms step_avg:60.43ms
step:2087/2160 train_time:126148ms step_avg:60.44ms
step:2088/2160 train_time:126235ms step_avg:60.46ms
step:2089/2160 train_time:126327ms step_avg:60.47ms
step:2090/2160 train_time:126413ms step_avg:60.48ms
step:2091/2160 train_time:126503ms step_avg:60.50ms
step:2092/2160 train_time:126590ms step_avg:60.51ms
step:2093/2160 train_time:126681ms step_avg:60.53ms
step:2094/2160 train_time:126768ms step_avg:60.54ms
step:2095/2160 train_time:126857ms step_avg:60.55ms
step:2096/2160 train_time:126945ms step_avg:60.57ms
step:2097/2160 train_time:127034ms step_avg:60.58ms
step:2098/2160 train_time:127121ms step_avg:60.59ms
step:2099/2160 train_time:127210ms step_avg:60.61ms
step:2100/2160 train_time:127298ms step_avg:60.62ms
step:2101/2160 train_time:127387ms step_avg:60.63ms
step:2102/2160 train_time:127475ms step_avg:60.64ms
step:2103/2160 train_time:127563ms step_avg:60.66ms
step:2104/2160 train_time:127650ms step_avg:60.67ms
step:2105/2160 train_time:127740ms step_avg:60.68ms
step:2106/2160 train_time:127829ms step_avg:60.70ms
step:2107/2160 train_time:127918ms step_avg:60.71ms
step:2108/2160 train_time:128006ms step_avg:60.72ms
step:2109/2160 train_time:128095ms step_avg:60.74ms
step:2110/2160 train_time:128182ms step_avg:60.75ms
step:2111/2160 train_time:128272ms step_avg:60.76ms
step:2112/2160 train_time:128359ms step_avg:60.78ms
step:2113/2160 train_time:128449ms step_avg:60.79ms
step:2114/2160 train_time:128536ms step_avg:60.80ms
step:2115/2160 train_time:128625ms step_avg:60.82ms
step:2116/2160 train_time:128712ms step_avg:60.83ms
step:2117/2160 train_time:128802ms step_avg:60.84ms
step:2118/2160 train_time:128889ms step_avg:60.85ms
step:2119/2160 train_time:128978ms step_avg:60.87ms
step:2120/2160 train_time:129065ms step_avg:60.88ms
step:2121/2160 train_time:129154ms step_avg:60.89ms
step:2122/2160 train_time:129242ms step_avg:60.91ms
step:2123/2160 train_time:129331ms step_avg:60.92ms
step:2124/2160 train_time:129419ms step_avg:60.93ms
step:2125/2160 train_time:129509ms step_avg:60.95ms
step:2126/2160 train_time:129597ms step_avg:60.96ms
step:2127/2160 train_time:129686ms step_avg:60.97ms
step:2128/2160 train_time:129775ms step_avg:60.98ms
step:2129/2160 train_time:129864ms step_avg:61.00ms
step:2130/2160 train_time:129951ms step_avg:61.01ms
step:2131/2160 train_time:130042ms step_avg:61.02ms
step:2132/2160 train_time:130130ms step_avg:61.04ms
step:2133/2160 train_time:130220ms step_avg:61.05ms
step:2134/2160 train_time:130307ms step_avg:61.06ms
step:2135/2160 train_time:130397ms step_avg:61.08ms
step:2136/2160 train_time:130485ms step_avg:61.09ms
step:2137/2160 train_time:130575ms step_avg:61.10ms
step:2138/2160 train_time:130663ms step_avg:61.11ms
step:2139/2160 train_time:130753ms step_avg:61.13ms
step:2140/2160 train_time:130841ms step_avg:61.14ms
step:2141/2160 train_time:130931ms step_avg:61.15ms
step:2142/2160 train_time:131019ms step_avg:61.17ms
step:2143/2160 train_time:131108ms step_avg:61.18ms
step:2144/2160 train_time:131196ms step_avg:61.19ms
step:2145/2160 train_time:131286ms step_avg:61.21ms
step:2146/2160 train_time:131374ms step_avg:61.22ms
step:2147/2160 train_time:131463ms step_avg:61.23ms
step:2148/2160 train_time:131551ms step_avg:61.24ms
step:2149/2160 train_time:131640ms step_avg:61.26ms
step:2150/2160 train_time:131728ms step_avg:61.27ms
step:2151/2160 train_time:131818ms step_avg:61.28ms
step:2152/2160 train_time:131906ms step_avg:61.29ms
step:2153/2160 train_time:131995ms step_avg:61.31ms
step:2154/2160 train_time:132083ms step_avg:61.32ms
step:2155/2160 train_time:132172ms step_avg:61.33ms
step:2156/2160 train_time:132260ms step_avg:61.35ms
step:2157/2160 train_time:132350ms step_avg:61.36ms
step:2158/2160 train_time:132437ms step_avg:61.37ms
step:2159/2160 train_time:132528ms step_avg:61.38ms
step:2160/2160 train_time:132615ms step_avg:61.40ms
step:2160/2160 val_loss:3.2761 train_time:132706ms step_avg:61.44ms
peak memory allocated: 29896 MiB reserved: 61784 MiB

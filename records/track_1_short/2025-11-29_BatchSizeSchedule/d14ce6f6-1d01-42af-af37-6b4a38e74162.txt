import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = (16 / 8) * 0.8
    if x > 0.66:
        lr_max = (24 / 8) * 0.8
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sun Nov 30 02:44:38 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   35C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2160 train_time:107ms step_avg:106.90ms
step:2/2160 train_time:145ms step_avg:72.41ms
step:3/2160 train_time:163ms step_avg:54.49ms
step:4/2160 train_time:183ms step_avg:45.76ms
step:5/2160 train_time:217ms step_avg:43.42ms
step:6/2160 train_time:299ms step_avg:49.89ms
step:7/2160 train_time:328ms step_avg:46.80ms
step:8/2160 train_time:360ms step_avg:45.05ms
step:9/2160 train_time:394ms step_avg:43.80ms
step:10/2160 train_time:427ms step_avg:42.71ms
step:11/2160 train_time:461ms step_avg:41.92ms
step:12/2160 train_time:494ms step_avg:41.17ms
step:13/2160 train_time:529ms step_avg:40.65ms
step:14/2160 train_time:562ms step_avg:40.11ms
step:15/2160 train_time:596ms step_avg:39.71ms
step:16/2160 train_time:629ms step_avg:39.29ms
step:17/2160 train_time:663ms step_avg:38.98ms
step:18/2160 train_time:696ms step_avg:38.64ms
step:19/2160 train_time:730ms step_avg:38.43ms
step:20/2160 train_time:763ms step_avg:38.17ms
step:21/2160 train_time:798ms step_avg:38.00ms
step:22/2160 train_time:831ms step_avg:37.77ms
step:23/2160 train_time:865ms step_avg:37.63ms
step:24/2160 train_time:898ms step_avg:37.43ms
step:25/2160 train_time:933ms step_avg:37.31ms
step:26/2160 train_time:965ms step_avg:37.13ms
step:27/2160 train_time:1000ms step_avg:37.02ms
step:28/2160 train_time:1033ms step_avg:36.89ms
step:29/2160 train_time:1067ms step_avg:36.80ms
step:30/2160 train_time:1100ms step_avg:36.67ms
step:31/2160 train_time:1134ms step_avg:36.59ms
step:32/2160 train_time:1167ms step_avg:36.48ms
step:33/2160 train_time:1201ms step_avg:36.41ms
step:34/2160 train_time:1235ms step_avg:36.33ms
step:35/2160 train_time:1270ms step_avg:36.30ms
step:36/2160 train_time:1304ms step_avg:36.21ms
step:37/2160 train_time:1338ms step_avg:36.17ms
step:38/2160 train_time:1372ms step_avg:36.09ms
step:39/2160 train_time:1406ms step_avg:36.06ms
step:40/2160 train_time:1439ms step_avg:35.98ms
step:41/2160 train_time:1474ms step_avg:35.95ms
step:42/2160 train_time:1507ms step_avg:35.89ms
step:43/2160 train_time:1541ms step_avg:35.85ms
step:44/2160 train_time:1574ms step_avg:35.78ms
step:45/2160 train_time:1609ms step_avg:35.76ms
step:46/2160 train_time:1642ms step_avg:35.70ms
step:47/2160 train_time:1677ms step_avg:35.67ms
step:48/2160 train_time:1710ms step_avg:35.62ms
step:49/2160 train_time:1744ms step_avg:35.59ms
step:50/2160 train_time:1777ms step_avg:35.54ms
step:51/2160 train_time:1811ms step_avg:35.52ms
step:52/2160 train_time:1844ms step_avg:35.47ms
step:53/2160 train_time:1879ms step_avg:35.44ms
step:54/2160 train_time:1912ms step_avg:35.40ms
step:55/2160 train_time:1946ms step_avg:35.38ms
step:56/2160 train_time:1979ms step_avg:35.34ms
step:57/2160 train_time:2013ms step_avg:35.32ms
step:58/2160 train_time:2046ms step_avg:35.28ms
step:59/2160 train_time:2080ms step_avg:35.26ms
step:60/2160 train_time:2113ms step_avg:35.22ms
step:61/2160 train_time:2148ms step_avg:35.21ms
step:62/2160 train_time:2181ms step_avg:35.17ms
step:63/2160 train_time:2216ms step_avg:35.17ms
step:64/2160 train_time:2249ms step_avg:35.14ms
step:65/2160 train_time:2283ms step_avg:35.13ms
step:66/2160 train_time:2316ms step_avg:35.09ms
step:67/2160 train_time:2351ms step_avg:35.09ms
step:68/2160 train_time:2384ms step_avg:35.06ms
step:69/2160 train_time:2418ms step_avg:35.05ms
step:70/2160 train_time:2452ms step_avg:35.03ms
step:71/2160 train_time:2486ms step_avg:35.02ms
step:72/2160 train_time:2519ms step_avg:34.99ms
step:73/2160 train_time:2553ms step_avg:34.98ms
step:74/2160 train_time:2587ms step_avg:34.95ms
step:75/2160 train_time:2621ms step_avg:34.95ms
step:76/2160 train_time:2654ms step_avg:34.92ms
step:77/2160 train_time:2688ms step_avg:34.91ms
step:78/2160 train_time:2721ms step_avg:34.89ms
step:79/2160 train_time:2756ms step_avg:34.88ms
step:80/2160 train_time:2789ms step_avg:34.86ms
step:81/2160 train_time:2823ms step_avg:34.85ms
step:82/2160 train_time:2856ms step_avg:34.83ms
step:83/2160 train_time:2890ms step_avg:34.82ms
step:84/2160 train_time:2923ms step_avg:34.80ms
step:85/2160 train_time:2958ms step_avg:34.80ms
step:86/2160 train_time:2991ms step_avg:34.78ms
step:87/2160 train_time:3025ms step_avg:34.78ms
step:88/2160 train_time:3058ms step_avg:34.75ms
step:89/2160 train_time:3093ms step_avg:34.75ms
step:90/2160 train_time:3126ms step_avg:34.73ms
step:91/2160 train_time:3160ms step_avg:34.73ms
step:92/2160 train_time:3193ms step_avg:34.71ms
step:93/2160 train_time:3228ms step_avg:34.71ms
step:94/2160 train_time:3261ms step_avg:34.69ms
step:95/2160 train_time:3296ms step_avg:34.69ms
step:96/2160 train_time:3329ms step_avg:34.67ms
step:97/2160 train_time:3363ms step_avg:34.67ms
step:98/2160 train_time:3396ms step_avg:34.65ms
step:99/2160 train_time:3431ms step_avg:34.65ms
step:100/2160 train_time:3464ms step_avg:34.64ms
step:101/2160 train_time:3498ms step_avg:34.63ms
step:102/2160 train_time:3531ms step_avg:34.62ms
step:103/2160 train_time:3565ms step_avg:34.61ms
step:104/2160 train_time:3598ms step_avg:34.60ms
step:105/2160 train_time:3633ms step_avg:34.60ms
step:106/2160 train_time:3665ms step_avg:34.58ms
step:107/2160 train_time:3700ms step_avg:34.58ms
step:108/2160 train_time:3733ms step_avg:34.56ms
step:109/2160 train_time:3767ms step_avg:34.56ms
step:110/2160 train_time:3800ms step_avg:34.55ms
step:111/2160 train_time:3835ms step_avg:34.55ms
step:112/2160 train_time:3868ms step_avg:34.54ms
step:113/2160 train_time:3902ms step_avg:34.53ms
step:114/2160 train_time:3935ms step_avg:34.52ms
step:115/2160 train_time:3969ms step_avg:34.52ms
step:116/2160 train_time:4003ms step_avg:34.51ms
step:117/2160 train_time:4037ms step_avg:34.51ms
step:118/2160 train_time:4070ms step_avg:34.49ms
step:119/2160 train_time:4104ms step_avg:34.49ms
step:120/2160 train_time:4137ms step_avg:34.48ms
step:121/2160 train_time:4172ms step_avg:34.48ms
step:122/2160 train_time:4205ms step_avg:34.46ms
step:123/2160 train_time:4239ms step_avg:34.46ms
step:124/2160 train_time:4272ms step_avg:34.45ms
step:125/2160 train_time:4306ms step_avg:34.45ms
step:126/2160 train_time:4339ms step_avg:34.44ms
step:127/2160 train_time:4373ms step_avg:34.44ms
step:128/2160 train_time:4406ms step_avg:34.42ms
step:129/2160 train_time:4441ms step_avg:34.42ms
step:130/2160 train_time:4473ms step_avg:34.41ms
step:131/2160 train_time:4508ms step_avg:34.41ms
step:132/2160 train_time:4541ms step_avg:34.40ms
step:133/2160 train_time:4576ms step_avg:34.40ms
step:134/2160 train_time:4609ms step_avg:34.39ms
step:135/2160 train_time:4643ms step_avg:34.39ms
step:136/2160 train_time:4676ms step_avg:34.38ms
step:137/2160 train_time:4710ms step_avg:34.38ms
step:138/2160 train_time:4743ms step_avg:34.37ms
step:139/2160 train_time:4777ms step_avg:34.37ms
step:140/2160 train_time:4810ms step_avg:34.36ms
step:141/2160 train_time:4845ms step_avg:34.36ms
step:142/2160 train_time:4877ms step_avg:34.35ms
step:143/2160 train_time:4912ms step_avg:34.35ms
step:144/2160 train_time:4945ms step_avg:34.34ms
step:145/2160 train_time:4979ms step_avg:34.34ms
step:146/2160 train_time:5012ms step_avg:34.33ms
step:147/2160 train_time:5046ms step_avg:34.33ms
step:148/2160 train_time:5079ms step_avg:34.32ms
step:149/2160 train_time:5114ms step_avg:34.32ms
step:150/2160 train_time:5147ms step_avg:34.31ms
step:151/2160 train_time:5181ms step_avg:34.31ms
step:152/2160 train_time:5214ms step_avg:34.30ms
step:153/2160 train_time:5248ms step_avg:34.30ms
step:154/2160 train_time:5281ms step_avg:34.29ms
step:155/2160 train_time:5315ms step_avg:34.29ms
step:156/2160 train_time:5348ms step_avg:34.28ms
step:157/2160 train_time:5382ms step_avg:34.28ms
step:158/2160 train_time:5415ms step_avg:34.28ms
step:159/2160 train_time:5450ms step_avg:34.28ms
step:160/2160 train_time:5483ms step_avg:34.27ms
step:161/2160 train_time:5517ms step_avg:34.27ms
step:162/2160 train_time:5551ms step_avg:34.26ms
step:163/2160 train_time:5585ms step_avg:34.26ms
step:164/2160 train_time:5617ms step_avg:34.25ms
step:165/2160 train_time:5652ms step_avg:34.26ms
step:166/2160 train_time:5685ms step_avg:34.25ms
step:167/2160 train_time:5719ms step_avg:34.25ms
step:168/2160 train_time:5752ms step_avg:34.24ms
step:169/2160 train_time:5786ms step_avg:34.24ms
step:170/2160 train_time:5819ms step_avg:34.23ms
step:171/2160 train_time:5854ms step_avg:34.23ms
step:172/2160 train_time:5887ms step_avg:34.23ms
step:173/2160 train_time:5921ms step_avg:34.22ms
step:174/2160 train_time:5954ms step_avg:34.22ms
step:175/2160 train_time:5988ms step_avg:34.22ms
step:176/2160 train_time:6021ms step_avg:34.21ms
step:177/2160 train_time:6056ms step_avg:34.21ms
step:178/2160 train_time:6088ms step_avg:34.20ms
step:179/2160 train_time:6123ms step_avg:34.21ms
step:180/2160 train_time:6156ms step_avg:34.20ms
step:181/2160 train_time:6190ms step_avg:34.20ms
step:182/2160 train_time:6223ms step_avg:34.19ms
step:183/2160 train_time:6258ms step_avg:34.19ms
step:184/2160 train_time:6291ms step_avg:34.19ms
step:185/2160 train_time:6325ms step_avg:34.19ms
step:186/2160 train_time:6358ms step_avg:34.18ms
step:187/2160 train_time:6392ms step_avg:34.18ms
step:188/2160 train_time:6425ms step_avg:34.18ms
step:189/2160 train_time:6459ms step_avg:34.18ms
step:190/2160 train_time:6492ms step_avg:34.17ms
step:191/2160 train_time:6526ms step_avg:34.17ms
step:192/2160 train_time:6559ms step_avg:34.16ms
step:193/2160 train_time:6594ms step_avg:34.17ms
step:194/2160 train_time:6627ms step_avg:34.16ms
step:195/2160 train_time:6662ms step_avg:34.16ms
step:196/2160 train_time:6694ms step_avg:34.15ms
step:197/2160 train_time:6729ms step_avg:34.16ms
step:198/2160 train_time:6762ms step_avg:34.15ms
step:199/2160 train_time:6796ms step_avg:34.15ms
step:200/2160 train_time:6829ms step_avg:34.14ms
step:201/2160 train_time:6863ms step_avg:34.14ms
step:202/2160 train_time:6895ms step_avg:34.14ms
step:203/2160 train_time:6930ms step_avg:34.14ms
step:204/2160 train_time:6963ms step_avg:34.13ms
step:205/2160 train_time:6997ms step_avg:34.13ms
step:206/2160 train_time:7030ms step_avg:34.13ms
step:207/2160 train_time:7064ms step_avg:34.13ms
step:208/2160 train_time:7097ms step_avg:34.12ms
step:209/2160 train_time:7131ms step_avg:34.12ms
step:210/2160 train_time:7164ms step_avg:34.11ms
step:211/2160 train_time:7198ms step_avg:34.11ms
step:212/2160 train_time:7231ms step_avg:34.11ms
step:213/2160 train_time:7265ms step_avg:34.11ms
step:214/2160 train_time:7298ms step_avg:34.10ms
step:215/2160 train_time:7333ms step_avg:34.11ms
step:216/2160 train_time:7366ms step_avg:34.10ms
step:217/2160 train_time:7400ms step_avg:34.10ms
step:218/2160 train_time:7433ms step_avg:34.10ms
step:219/2160 train_time:7467ms step_avg:34.10ms
step:220/2160 train_time:7501ms step_avg:34.09ms
step:221/2160 train_time:7535ms step_avg:34.09ms
step:222/2160 train_time:7568ms step_avg:34.09ms
step:223/2160 train_time:7602ms step_avg:34.09ms
step:224/2160 train_time:7635ms step_avg:34.08ms
step:225/2160 train_time:7669ms step_avg:34.09ms
step:226/2160 train_time:7703ms step_avg:34.08ms
step:227/2160 train_time:7737ms step_avg:34.08ms
step:228/2160 train_time:7770ms step_avg:34.08ms
step:229/2160 train_time:7804ms step_avg:34.08ms
step:230/2160 train_time:7837ms step_avg:34.07ms
step:231/2160 train_time:7871ms step_avg:34.07ms
step:232/2160 train_time:7904ms step_avg:34.07ms
step:233/2160 train_time:7938ms step_avg:34.07ms
step:234/2160 train_time:7971ms step_avg:34.07ms
step:235/2160 train_time:8005ms step_avg:34.07ms
step:236/2160 train_time:8038ms step_avg:34.06ms
step:237/2160 train_time:8073ms step_avg:34.06ms
step:238/2160 train_time:8106ms step_avg:34.06ms
step:239/2160 train_time:8140ms step_avg:34.06ms
step:240/2160 train_time:8173ms step_avg:34.05ms
step:241/2160 train_time:8207ms step_avg:34.05ms
step:242/2160 train_time:8240ms step_avg:34.05ms
step:243/2160 train_time:8274ms step_avg:34.05ms
step:244/2160 train_time:8308ms step_avg:34.05ms
step:245/2160 train_time:8342ms step_avg:34.05ms
step:246/2160 train_time:8375ms step_avg:34.04ms
step:247/2160 train_time:8409ms step_avg:34.05ms
step:248/2160 train_time:8443ms step_avg:34.04ms
step:249/2160 train_time:8477ms step_avg:34.04ms
step:250/2160 train_time:8510ms step_avg:34.04ms
step:250/2160 val_loss:4.3018 train_time:8545ms step_avg:34.18ms
step:251/2160 train_time:8564ms step_avg:34.12ms
step:252/2160 train_time:8582ms step_avg:34.06ms
step:253/2160 train_time:8614ms step_avg:34.05ms
step:254/2160 train_time:8648ms step_avg:34.05ms
step:255/2160 train_time:8685ms step_avg:34.06ms
step:256/2160 train_time:8720ms step_avg:34.06ms
step:257/2160 train_time:8756ms step_avg:34.07ms
step:258/2160 train_time:8790ms step_avg:34.07ms
step:259/2160 train_time:8824ms step_avg:34.07ms
step:260/2160 train_time:8857ms step_avg:34.07ms
step:261/2160 train_time:8891ms step_avg:34.07ms
step:262/2160 train_time:8924ms step_avg:34.06ms
step:263/2160 train_time:8958ms step_avg:34.06ms
step:264/2160 train_time:8991ms step_avg:34.06ms
step:265/2160 train_time:9025ms step_avg:34.06ms
step:266/2160 train_time:9058ms step_avg:34.05ms
step:267/2160 train_time:9092ms step_avg:34.05ms
step:268/2160 train_time:9125ms step_avg:34.05ms
step:269/2160 train_time:9159ms step_avg:34.05ms
step:270/2160 train_time:9192ms step_avg:34.05ms
step:271/2160 train_time:9226ms step_avg:34.04ms
step:272/2160 train_time:9259ms step_avg:34.04ms
step:273/2160 train_time:9293ms step_avg:34.04ms
step:274/2160 train_time:9326ms step_avg:34.04ms
step:275/2160 train_time:9360ms step_avg:34.04ms
step:276/2160 train_time:9393ms step_avg:34.03ms
step:277/2160 train_time:9427ms step_avg:34.03ms
step:278/2160 train_time:9460ms step_avg:34.03ms
step:279/2160 train_time:9494ms step_avg:34.03ms
step:280/2160 train_time:9527ms step_avg:34.03ms
step:281/2160 train_time:9561ms step_avg:34.02ms
step:282/2160 train_time:9595ms step_avg:34.02ms
step:283/2160 train_time:9629ms step_avg:34.02ms
step:284/2160 train_time:9662ms step_avg:34.02ms
step:285/2160 train_time:9697ms step_avg:34.02ms
step:286/2160 train_time:9730ms step_avg:34.02ms
step:287/2160 train_time:9764ms step_avg:34.02ms
step:288/2160 train_time:9798ms step_avg:34.02ms
step:289/2160 train_time:9832ms step_avg:34.02ms
step:290/2160 train_time:9865ms step_avg:34.02ms
step:291/2160 train_time:9900ms step_avg:34.02ms
step:292/2160 train_time:9933ms step_avg:34.02ms
step:293/2160 train_time:9967ms step_avg:34.02ms
step:294/2160 train_time:10000ms step_avg:34.01ms
step:295/2160 train_time:10034ms step_avg:34.01ms
step:296/2160 train_time:10067ms step_avg:34.01ms
step:297/2160 train_time:10102ms step_avg:34.01ms
step:298/2160 train_time:10134ms step_avg:34.01ms
step:299/2160 train_time:10168ms step_avg:34.01ms
step:300/2160 train_time:10201ms step_avg:34.00ms
step:301/2160 train_time:10235ms step_avg:34.00ms
step:302/2160 train_time:10268ms step_avg:34.00ms
step:303/2160 train_time:10302ms step_avg:34.00ms
step:304/2160 train_time:10335ms step_avg:34.00ms
step:305/2160 train_time:10369ms step_avg:34.00ms
step:306/2160 train_time:10402ms step_avg:33.99ms
step:307/2160 train_time:10436ms step_avg:33.99ms
step:308/2160 train_time:10469ms step_avg:33.99ms
step:309/2160 train_time:10503ms step_avg:33.99ms
step:310/2160 train_time:10536ms step_avg:33.99ms
step:311/2160 train_time:10570ms step_avg:33.99ms
step:312/2160 train_time:10603ms step_avg:33.98ms
step:313/2160 train_time:10637ms step_avg:33.98ms
step:314/2160 train_time:10670ms step_avg:33.98ms
step:315/2160 train_time:10704ms step_avg:33.98ms
step:316/2160 train_time:10738ms step_avg:33.98ms
step:317/2160 train_time:10772ms step_avg:33.98ms
step:318/2160 train_time:10805ms step_avg:33.98ms
step:319/2160 train_time:10840ms step_avg:33.98ms
step:320/2160 train_time:10873ms step_avg:33.98ms
step:321/2160 train_time:10907ms step_avg:33.98ms
step:322/2160 train_time:10940ms step_avg:33.97ms
step:323/2160 train_time:10974ms step_avg:33.98ms
step:324/2160 train_time:11007ms step_avg:33.97ms
step:325/2160 train_time:11041ms step_avg:33.97ms
step:326/2160 train_time:11075ms step_avg:33.97ms
step:327/2160 train_time:11109ms step_avg:33.97ms
step:328/2160 train_time:11142ms step_avg:33.97ms
step:329/2160 train_time:11176ms step_avg:33.97ms
step:330/2160 train_time:11210ms step_avg:33.97ms
step:331/2160 train_time:11244ms step_avg:33.97ms
step:332/2160 train_time:11277ms step_avg:33.97ms
step:333/2160 train_time:11311ms step_avg:33.97ms
step:334/2160 train_time:11344ms step_avg:33.96ms
step:335/2160 train_time:11378ms step_avg:33.96ms
step:336/2160 train_time:11411ms step_avg:33.96ms
step:337/2160 train_time:11445ms step_avg:33.96ms
step:338/2160 train_time:11478ms step_avg:33.96ms
step:339/2160 train_time:11512ms step_avg:33.96ms
step:340/2160 train_time:11545ms step_avg:33.96ms
step:341/2160 train_time:11579ms step_avg:33.96ms
step:342/2160 train_time:11612ms step_avg:33.95ms
step:343/2160 train_time:11646ms step_avg:33.95ms
step:344/2160 train_time:11679ms step_avg:33.95ms
step:345/2160 train_time:11714ms step_avg:33.95ms
step:346/2160 train_time:11747ms step_avg:33.95ms
step:347/2160 train_time:11781ms step_avg:33.95ms
step:348/2160 train_time:11815ms step_avg:33.95ms
step:349/2160 train_time:11849ms step_avg:33.95ms
step:350/2160 train_time:11882ms step_avg:33.95ms
step:351/2160 train_time:11916ms step_avg:33.95ms
step:352/2160 train_time:11949ms step_avg:33.95ms
step:353/2160 train_time:11984ms step_avg:33.95ms
step:354/2160 train_time:12017ms step_avg:33.95ms
step:355/2160 train_time:12051ms step_avg:33.95ms
step:356/2160 train_time:12084ms step_avg:33.94ms
step:357/2160 train_time:12119ms step_avg:33.95ms
step:358/2160 train_time:12151ms step_avg:33.94ms
step:359/2160 train_time:12186ms step_avg:33.94ms
step:360/2160 train_time:12219ms step_avg:33.94ms
step:361/2160 train_time:12253ms step_avg:33.94ms
step:362/2160 train_time:12286ms step_avg:33.94ms
step:363/2160 train_time:12320ms step_avg:33.94ms
step:364/2160 train_time:12353ms step_avg:33.94ms
step:365/2160 train_time:12387ms step_avg:33.94ms
step:366/2160 train_time:12420ms step_avg:33.94ms
step:367/2160 train_time:12455ms step_avg:33.94ms
step:368/2160 train_time:12488ms step_avg:33.94ms
step:369/2160 train_time:12522ms step_avg:33.94ms
step:370/2160 train_time:12555ms step_avg:33.93ms
step:371/2160 train_time:12589ms step_avg:33.93ms
step:372/2160 train_time:12622ms step_avg:33.93ms
step:373/2160 train_time:12656ms step_avg:33.93ms
step:374/2160 train_time:12690ms step_avg:33.93ms
step:375/2160 train_time:12724ms step_avg:33.93ms
step:376/2160 train_time:12757ms step_avg:33.93ms
step:377/2160 train_time:12792ms step_avg:33.93ms
step:378/2160 train_time:12824ms step_avg:33.93ms
step:379/2160 train_time:12859ms step_avg:33.93ms
step:380/2160 train_time:12892ms step_avg:33.93ms
step:381/2160 train_time:12926ms step_avg:33.93ms
step:382/2160 train_time:12959ms step_avg:33.92ms
step:383/2160 train_time:12993ms step_avg:33.93ms
step:384/2160 train_time:13026ms step_avg:33.92ms
step:385/2160 train_time:13061ms step_avg:33.92ms
step:386/2160 train_time:13094ms step_avg:33.92ms
step:387/2160 train_time:13128ms step_avg:33.92ms
step:388/2160 train_time:13161ms step_avg:33.92ms
step:389/2160 train_time:13196ms step_avg:33.92ms
step:390/2160 train_time:13229ms step_avg:33.92ms
step:391/2160 train_time:13264ms step_avg:33.92ms
step:392/2160 train_time:13297ms step_avg:33.92ms
step:393/2160 train_time:13331ms step_avg:33.92ms
step:394/2160 train_time:13364ms step_avg:33.92ms
step:395/2160 train_time:13398ms step_avg:33.92ms
step:396/2160 train_time:13431ms step_avg:33.92ms
step:397/2160 train_time:13465ms step_avg:33.92ms
step:398/2160 train_time:13499ms step_avg:33.92ms
step:399/2160 train_time:13533ms step_avg:33.92ms
step:400/2160 train_time:13566ms step_avg:33.91ms
step:401/2160 train_time:13600ms step_avg:33.91ms
step:402/2160 train_time:13633ms step_avg:33.91ms
step:403/2160 train_time:13667ms step_avg:33.91ms
step:404/2160 train_time:13700ms step_avg:33.91ms
step:405/2160 train_time:13734ms step_avg:33.91ms
step:406/2160 train_time:13768ms step_avg:33.91ms
step:407/2160 train_time:13802ms step_avg:33.91ms
step:408/2160 train_time:13835ms step_avg:33.91ms
step:409/2160 train_time:13869ms step_avg:33.91ms
step:410/2160 train_time:13903ms step_avg:33.91ms
step:411/2160 train_time:13937ms step_avg:33.91ms
step:412/2160 train_time:13970ms step_avg:33.91ms
step:413/2160 train_time:14004ms step_avg:33.91ms
step:414/2160 train_time:14037ms step_avg:33.91ms
step:415/2160 train_time:14071ms step_avg:33.91ms
step:416/2160 train_time:14104ms step_avg:33.90ms
step:417/2160 train_time:14139ms step_avg:33.91ms
step:418/2160 train_time:14172ms step_avg:33.90ms
step:419/2160 train_time:14206ms step_avg:33.91ms
step:420/2160 train_time:14239ms step_avg:33.90ms
step:421/2160 train_time:14273ms step_avg:33.90ms
step:422/2160 train_time:14306ms step_avg:33.90ms
step:423/2160 train_time:14340ms step_avg:33.90ms
step:424/2160 train_time:14373ms step_avg:33.90ms
step:425/2160 train_time:14408ms step_avg:33.90ms
step:426/2160 train_time:14441ms step_avg:33.90ms
step:427/2160 train_time:14475ms step_avg:33.90ms
step:428/2160 train_time:14508ms step_avg:33.90ms
step:429/2160 train_time:14542ms step_avg:33.90ms
step:430/2160 train_time:14575ms step_avg:33.90ms
step:431/2160 train_time:14610ms step_avg:33.90ms
step:432/2160 train_time:14643ms step_avg:33.89ms
step:433/2160 train_time:14677ms step_avg:33.90ms
step:434/2160 train_time:14710ms step_avg:33.89ms
step:435/2160 train_time:14744ms step_avg:33.89ms
step:436/2160 train_time:14777ms step_avg:33.89ms
step:437/2160 train_time:14811ms step_avg:33.89ms
step:438/2160 train_time:14844ms step_avg:33.89ms
step:439/2160 train_time:14879ms step_avg:33.89ms
step:440/2160 train_time:14912ms step_avg:33.89ms
step:441/2160 train_time:14946ms step_avg:33.89ms
step:442/2160 train_time:14979ms step_avg:33.89ms
step:443/2160 train_time:15013ms step_avg:33.89ms
step:444/2160 train_time:15047ms step_avg:33.89ms
step:445/2160 train_time:15081ms step_avg:33.89ms
step:446/2160 train_time:15114ms step_avg:33.89ms
step:447/2160 train_time:15148ms step_avg:33.89ms
step:448/2160 train_time:15181ms step_avg:33.89ms
step:449/2160 train_time:15216ms step_avg:33.89ms
step:450/2160 train_time:15248ms step_avg:33.89ms
step:451/2160 train_time:15283ms step_avg:33.89ms
step:452/2160 train_time:15316ms step_avg:33.88ms
step:453/2160 train_time:15350ms step_avg:33.89ms
step:454/2160 train_time:15383ms step_avg:33.88ms
step:455/2160 train_time:15418ms step_avg:33.89ms
step:456/2160 train_time:15451ms step_avg:33.88ms
step:457/2160 train_time:15486ms step_avg:33.89ms
step:458/2160 train_time:15519ms step_avg:33.88ms
step:459/2160 train_time:15553ms step_avg:33.89ms
step:460/2160 train_time:15587ms step_avg:33.88ms
step:461/2160 train_time:15621ms step_avg:33.88ms
step:462/2160 train_time:15654ms step_avg:33.88ms
step:463/2160 train_time:15688ms step_avg:33.88ms
step:464/2160 train_time:15721ms step_avg:33.88ms
step:465/2160 train_time:15756ms step_avg:33.88ms
step:466/2160 train_time:15789ms step_avg:33.88ms
step:467/2160 train_time:15823ms step_avg:33.88ms
step:468/2160 train_time:15856ms step_avg:33.88ms
step:469/2160 train_time:15890ms step_avg:33.88ms
step:470/2160 train_time:15923ms step_avg:33.88ms
step:471/2160 train_time:15957ms step_avg:33.88ms
step:472/2160 train_time:15990ms step_avg:33.88ms
step:473/2160 train_time:16024ms step_avg:33.88ms
step:474/2160 train_time:16058ms step_avg:33.88ms
step:475/2160 train_time:16092ms step_avg:33.88ms
step:476/2160 train_time:16125ms step_avg:33.88ms
step:477/2160 train_time:16160ms step_avg:33.88ms
step:478/2160 train_time:16193ms step_avg:33.88ms
step:479/2160 train_time:16227ms step_avg:33.88ms
step:480/2160 train_time:16260ms step_avg:33.87ms
step:481/2160 train_time:16294ms step_avg:33.88ms
step:482/2160 train_time:16327ms step_avg:33.87ms
step:483/2160 train_time:16362ms step_avg:33.87ms
step:484/2160 train_time:16395ms step_avg:33.87ms
step:485/2160 train_time:16429ms step_avg:33.87ms
step:486/2160 train_time:16462ms step_avg:33.87ms
step:487/2160 train_time:16496ms step_avg:33.87ms
step:488/2160 train_time:16529ms step_avg:33.87ms
step:489/2160 train_time:16564ms step_avg:33.87ms
step:490/2160 train_time:16597ms step_avg:33.87ms
step:491/2160 train_time:16631ms step_avg:33.87ms
step:492/2160 train_time:16664ms step_avg:33.87ms
step:493/2160 train_time:16698ms step_avg:33.87ms
step:494/2160 train_time:16731ms step_avg:33.87ms
step:495/2160 train_time:16766ms step_avg:33.87ms
step:496/2160 train_time:16799ms step_avg:33.87ms
step:497/2160 train_time:16833ms step_avg:33.87ms
step:498/2160 train_time:16866ms step_avg:33.87ms
step:499/2160 train_time:16900ms step_avg:33.87ms
step:500/2160 train_time:16934ms step_avg:33.87ms
step:500/2160 val_loss:4.0102 train_time:16970ms step_avg:33.94ms
step:501/2160 train_time:16990ms step_avg:33.91ms
step:502/2160 train_time:17008ms step_avg:33.88ms
step:503/2160 train_time:17042ms step_avg:33.88ms
step:504/2160 train_time:17076ms step_avg:33.88ms
step:505/2160 train_time:17111ms step_avg:33.88ms
step:506/2160 train_time:17145ms step_avg:33.88ms
step:507/2160 train_time:17180ms step_avg:33.88ms
step:508/2160 train_time:17213ms step_avg:33.88ms
step:509/2160 train_time:17247ms step_avg:33.88ms
step:510/2160 train_time:17280ms step_avg:33.88ms
step:511/2160 train_time:17314ms step_avg:33.88ms
step:512/2160 train_time:17347ms step_avg:33.88ms
step:513/2160 train_time:17381ms step_avg:33.88ms
step:514/2160 train_time:17414ms step_avg:33.88ms
step:515/2160 train_time:17448ms step_avg:33.88ms
step:516/2160 train_time:17481ms step_avg:33.88ms
step:517/2160 train_time:17515ms step_avg:33.88ms
step:518/2160 train_time:17547ms step_avg:33.88ms
step:519/2160 train_time:17581ms step_avg:33.87ms
step:520/2160 train_time:17614ms step_avg:33.87ms
step:521/2160 train_time:17648ms step_avg:33.87ms
step:522/2160 train_time:17681ms step_avg:33.87ms
step:523/2160 train_time:17716ms step_avg:33.87ms
step:524/2160 train_time:17748ms step_avg:33.87ms
step:525/2160 train_time:17783ms step_avg:33.87ms
step:526/2160 train_time:17815ms step_avg:33.87ms
step:527/2160 train_time:17849ms step_avg:33.87ms
step:528/2160 train_time:17882ms step_avg:33.87ms
step:529/2160 train_time:17917ms step_avg:33.87ms
step:530/2160 train_time:17950ms step_avg:33.87ms
step:531/2160 train_time:17984ms step_avg:33.87ms
step:532/2160 train_time:18018ms step_avg:33.87ms
step:533/2160 train_time:18053ms step_avg:33.87ms
step:534/2160 train_time:18086ms step_avg:33.87ms
step:535/2160 train_time:18121ms step_avg:33.87ms
step:536/2160 train_time:18154ms step_avg:33.87ms
step:537/2160 train_time:18189ms step_avg:33.87ms
step:538/2160 train_time:18222ms step_avg:33.87ms
step:539/2160 train_time:18256ms step_avg:33.87ms
step:540/2160 train_time:18290ms step_avg:33.87ms
step:541/2160 train_time:18324ms step_avg:33.87ms
step:542/2160 train_time:18357ms step_avg:33.87ms
step:543/2160 train_time:18391ms step_avg:33.87ms
step:544/2160 train_time:18424ms step_avg:33.87ms
step:545/2160 train_time:18459ms step_avg:33.87ms
step:546/2160 train_time:18492ms step_avg:33.87ms
step:547/2160 train_time:18526ms step_avg:33.87ms
step:548/2160 train_time:18559ms step_avg:33.87ms
step:549/2160 train_time:18593ms step_avg:33.87ms
step:550/2160 train_time:18626ms step_avg:33.87ms
step:551/2160 train_time:18660ms step_avg:33.87ms
step:552/2160 train_time:18693ms step_avg:33.86ms
step:553/2160 train_time:18728ms step_avg:33.87ms
step:554/2160 train_time:18761ms step_avg:33.86ms
step:555/2160 train_time:18795ms step_avg:33.86ms
step:556/2160 train_time:18828ms step_avg:33.86ms
step:557/2160 train_time:18862ms step_avg:33.86ms
step:558/2160 train_time:18895ms step_avg:33.86ms
step:559/2160 train_time:18929ms step_avg:33.86ms
step:560/2160 train_time:18962ms step_avg:33.86ms
step:561/2160 train_time:18997ms step_avg:33.86ms
step:562/2160 train_time:19030ms step_avg:33.86ms
step:563/2160 train_time:19064ms step_avg:33.86ms
step:564/2160 train_time:19097ms step_avg:33.86ms
step:565/2160 train_time:19132ms step_avg:33.86ms
step:566/2160 train_time:19165ms step_avg:33.86ms
step:567/2160 train_time:19199ms step_avg:33.86ms
step:568/2160 train_time:19232ms step_avg:33.86ms
step:569/2160 train_time:19267ms step_avg:33.86ms
step:570/2160 train_time:19300ms step_avg:33.86ms
step:571/2160 train_time:19335ms step_avg:33.86ms
step:572/2160 train_time:19368ms step_avg:33.86ms
step:573/2160 train_time:19402ms step_avg:33.86ms
step:574/2160 train_time:19435ms step_avg:33.86ms
step:575/2160 train_time:19470ms step_avg:33.86ms
step:576/2160 train_time:19502ms step_avg:33.86ms
step:577/2160 train_time:19537ms step_avg:33.86ms
step:578/2160 train_time:19570ms step_avg:33.86ms
step:579/2160 train_time:19604ms step_avg:33.86ms
step:580/2160 train_time:19637ms step_avg:33.86ms
step:581/2160 train_time:19671ms step_avg:33.86ms
step:582/2160 train_time:19704ms step_avg:33.86ms
step:583/2160 train_time:19738ms step_avg:33.86ms
step:584/2160 train_time:19771ms step_avg:33.86ms
step:585/2160 train_time:19806ms step_avg:33.86ms
step:586/2160 train_time:19839ms step_avg:33.86ms
step:587/2160 train_time:19873ms step_avg:33.86ms
step:588/2160 train_time:19907ms step_avg:33.86ms
step:589/2160 train_time:19941ms step_avg:33.86ms
step:590/2160 train_time:19974ms step_avg:33.85ms
step:591/2160 train_time:20008ms step_avg:33.85ms
step:592/2160 train_time:20041ms step_avg:33.85ms
step:593/2160 train_time:20076ms step_avg:33.85ms
step:594/2160 train_time:20109ms step_avg:33.85ms
step:595/2160 train_time:20143ms step_avg:33.85ms
step:596/2160 train_time:20176ms step_avg:33.85ms
step:597/2160 train_time:20211ms step_avg:33.85ms
step:598/2160 train_time:20244ms step_avg:33.85ms
step:599/2160 train_time:20278ms step_avg:33.85ms
step:600/2160 train_time:20311ms step_avg:33.85ms
step:601/2160 train_time:20346ms step_avg:33.85ms
step:602/2160 train_time:20379ms step_avg:33.85ms
step:603/2160 train_time:20413ms step_avg:33.85ms
step:604/2160 train_time:20446ms step_avg:33.85ms
step:605/2160 train_time:20480ms step_avg:33.85ms
step:606/2160 train_time:20513ms step_avg:33.85ms
step:607/2160 train_time:20548ms step_avg:33.85ms
step:608/2160 train_time:20581ms step_avg:33.85ms
step:609/2160 train_time:20615ms step_avg:33.85ms
step:610/2160 train_time:20648ms step_avg:33.85ms
step:611/2160 train_time:20683ms step_avg:33.85ms
step:612/2160 train_time:20716ms step_avg:33.85ms
step:613/2160 train_time:20751ms step_avg:33.85ms
step:614/2160 train_time:20784ms step_avg:33.85ms
step:615/2160 train_time:20818ms step_avg:33.85ms
step:616/2160 train_time:20851ms step_avg:33.85ms
step:617/2160 train_time:20885ms step_avg:33.85ms
step:618/2160 train_time:20918ms step_avg:33.85ms
step:619/2160 train_time:20952ms step_avg:33.85ms
step:620/2160 train_time:20985ms step_avg:33.85ms
step:621/2160 train_time:21020ms step_avg:33.85ms
step:622/2160 train_time:21053ms step_avg:33.85ms
step:623/2160 train_time:21087ms step_avg:33.85ms
step:624/2160 train_time:21120ms step_avg:33.85ms
step:625/2160 train_time:21155ms step_avg:33.85ms
step:626/2160 train_time:21187ms step_avg:33.85ms
step:627/2160 train_time:21222ms step_avg:33.85ms
step:628/2160 train_time:21255ms step_avg:33.85ms
step:629/2160 train_time:21289ms step_avg:33.85ms
step:630/2160 train_time:21322ms step_avg:33.85ms
step:631/2160 train_time:21357ms step_avg:33.85ms
step:632/2160 train_time:21390ms step_avg:33.84ms
step:633/2160 train_time:21424ms step_avg:33.85ms
step:634/2160 train_time:21457ms step_avg:33.84ms
step:635/2160 train_time:21492ms step_avg:33.85ms
step:636/2160 train_time:21524ms step_avg:33.84ms
step:637/2160 train_time:21559ms step_avg:33.84ms
step:638/2160 train_time:21592ms step_avg:33.84ms
step:639/2160 train_time:21626ms step_avg:33.84ms
step:640/2160 train_time:21659ms step_avg:33.84ms
step:641/2160 train_time:21693ms step_avg:33.84ms
step:642/2160 train_time:21726ms step_avg:33.84ms
step:643/2160 train_time:21761ms step_avg:33.84ms
step:644/2160 train_time:21794ms step_avg:33.84ms
step:645/2160 train_time:21828ms step_avg:33.84ms
step:646/2160 train_time:21861ms step_avg:33.84ms
step:647/2160 train_time:21895ms step_avg:33.84ms
step:648/2160 train_time:21928ms step_avg:33.84ms
step:649/2160 train_time:21962ms step_avg:33.84ms
step:650/2160 train_time:21995ms step_avg:33.84ms
step:651/2160 train_time:22030ms step_avg:33.84ms
step:652/2160 train_time:22063ms step_avg:33.84ms
step:653/2160 train_time:22097ms step_avg:33.84ms
step:654/2160 train_time:22130ms step_avg:33.84ms
step:655/2160 train_time:22164ms step_avg:33.84ms
step:656/2160 train_time:22198ms step_avg:33.84ms
step:657/2160 train_time:22232ms step_avg:33.84ms
step:658/2160 train_time:22265ms step_avg:33.84ms
step:659/2160 train_time:22299ms step_avg:33.84ms
step:660/2160 train_time:22332ms step_avg:33.84ms
step:661/2160 train_time:22367ms step_avg:33.84ms
step:662/2160 train_time:22400ms step_avg:33.84ms
step:663/2160 train_time:22435ms step_avg:33.84ms
step:664/2160 train_time:22468ms step_avg:33.84ms
step:665/2160 train_time:22502ms step_avg:33.84ms
step:666/2160 train_time:22535ms step_avg:33.84ms
step:667/2160 train_time:22569ms step_avg:33.84ms
step:668/2160 train_time:22602ms step_avg:33.84ms
step:669/2160 train_time:22636ms step_avg:33.84ms
step:670/2160 train_time:22670ms step_avg:33.84ms
step:671/2160 train_time:22704ms step_avg:33.84ms
step:672/2160 train_time:22736ms step_avg:33.83ms
step:673/2160 train_time:22771ms step_avg:33.84ms
step:674/2160 train_time:22804ms step_avg:33.83ms
step:675/2160 train_time:22838ms step_avg:33.83ms
step:676/2160 train_time:22871ms step_avg:33.83ms
step:677/2160 train_time:22905ms step_avg:33.83ms
step:678/2160 train_time:22938ms step_avg:33.83ms
step:679/2160 train_time:22973ms step_avg:33.83ms
step:680/2160 train_time:23006ms step_avg:33.83ms
step:681/2160 train_time:23040ms step_avg:33.83ms
step:682/2160 train_time:23073ms step_avg:33.83ms
step:683/2160 train_time:23107ms step_avg:33.83ms
step:684/2160 train_time:23140ms step_avg:33.83ms
step:685/2160 train_time:23175ms step_avg:33.83ms
step:686/2160 train_time:23208ms step_avg:33.83ms
step:687/2160 train_time:23242ms step_avg:33.83ms
step:688/2160 train_time:23275ms step_avg:33.83ms
step:689/2160 train_time:23309ms step_avg:33.83ms
step:690/2160 train_time:23342ms step_avg:33.83ms
step:691/2160 train_time:23377ms step_avg:33.83ms
step:692/2160 train_time:23410ms step_avg:33.83ms
step:693/2160 train_time:23444ms step_avg:33.83ms
step:694/2160 train_time:23477ms step_avg:33.83ms
step:695/2160 train_time:23512ms step_avg:33.83ms
step:696/2160 train_time:23544ms step_avg:33.83ms
step:697/2160 train_time:23579ms step_avg:33.83ms
step:698/2160 train_time:23612ms step_avg:33.83ms
step:699/2160 train_time:23646ms step_avg:33.83ms
step:700/2160 train_time:23679ms step_avg:33.83ms
step:701/2160 train_time:23713ms step_avg:33.83ms
step:702/2160 train_time:23746ms step_avg:33.83ms
step:703/2160 train_time:23781ms step_avg:33.83ms
step:704/2160 train_time:23813ms step_avg:33.83ms
step:705/2160 train_time:23848ms step_avg:33.83ms
step:706/2160 train_time:23881ms step_avg:33.83ms
step:707/2160 train_time:23915ms step_avg:33.83ms
step:708/2160 train_time:23949ms step_avg:33.83ms
step:709/2160 train_time:24009ms step_avg:33.86ms
step:710/2160 train_time:24069ms step_avg:33.90ms
step:711/2160 train_time:24129ms step_avg:33.94ms
step:712/2160 train_time:24189ms step_avg:33.97ms
step:713/2160 train_time:24251ms step_avg:34.01ms
step:714/2160 train_time:24310ms step_avg:34.05ms
step:715/2160 train_time:24372ms step_avg:34.09ms
step:716/2160 train_time:24431ms step_avg:34.12ms
step:717/2160 train_time:24492ms step_avg:34.16ms
step:718/2160 train_time:24552ms step_avg:34.20ms
step:719/2160 train_time:24614ms step_avg:34.23ms
step:720/2160 train_time:24673ms step_avg:34.27ms
step:721/2160 train_time:24735ms step_avg:34.31ms
step:722/2160 train_time:24795ms step_avg:34.34ms
step:723/2160 train_time:24856ms step_avg:34.38ms
step:724/2160 train_time:24915ms step_avg:34.41ms
step:725/2160 train_time:24977ms step_avg:34.45ms
step:726/2160 train_time:25037ms step_avg:34.49ms
step:727/2160 train_time:25099ms step_avg:34.52ms
step:728/2160 train_time:25160ms step_avg:34.56ms
step:729/2160 train_time:25222ms step_avg:34.60ms
step:730/2160 train_time:25282ms step_avg:34.63ms
step:731/2160 train_time:25343ms step_avg:34.67ms
step:732/2160 train_time:25403ms step_avg:34.70ms
step:733/2160 train_time:25465ms step_avg:34.74ms
step:734/2160 train_time:25524ms step_avg:34.77ms
step:735/2160 train_time:25586ms step_avg:34.81ms
step:736/2160 train_time:25646ms step_avg:34.85ms
step:737/2160 train_time:25708ms step_avg:34.88ms
step:738/2160 train_time:25768ms step_avg:34.92ms
step:739/2160 train_time:25831ms step_avg:34.95ms
step:740/2160 train_time:25890ms step_avg:34.99ms
step:741/2160 train_time:25951ms step_avg:35.02ms
step:742/2160 train_time:26011ms step_avg:35.05ms
step:743/2160 train_time:26072ms step_avg:35.09ms
step:744/2160 train_time:26132ms step_avg:35.12ms
step:745/2160 train_time:26193ms step_avg:35.16ms
step:746/2160 train_time:26252ms step_avg:35.19ms
step:747/2160 train_time:26314ms step_avg:35.23ms
step:748/2160 train_time:26374ms step_avg:35.26ms
step:749/2160 train_time:26435ms step_avg:35.29ms
step:750/2160 train_time:26496ms step_avg:35.33ms
step:750/2160 val_loss:3.8627 train_time:26559ms step_avg:35.41ms
step:751/2160 train_time:26578ms step_avg:35.39ms
step:752/2160 train_time:26624ms step_avg:35.40ms
step:753/2160 train_time:26684ms step_avg:35.44ms
step:754/2160 train_time:26743ms step_avg:35.47ms
step:755/2160 train_time:26805ms step_avg:35.50ms
step:756/2160 train_time:26865ms step_avg:35.54ms
step:757/2160 train_time:26925ms step_avg:35.57ms
step:758/2160 train_time:26984ms step_avg:35.60ms
step:759/2160 train_time:27045ms step_avg:35.63ms
step:760/2160 train_time:27104ms step_avg:35.66ms
step:761/2160 train_time:27164ms step_avg:35.70ms
step:762/2160 train_time:27223ms step_avg:35.73ms
step:763/2160 train_time:27284ms step_avg:35.76ms
step:764/2160 train_time:27342ms step_avg:35.79ms
step:765/2160 train_time:27403ms step_avg:35.82ms
step:766/2160 train_time:27468ms step_avg:35.86ms
step:767/2160 train_time:27534ms step_avg:35.90ms
step:768/2160 train_time:27595ms step_avg:35.93ms
step:769/2160 train_time:27658ms step_avg:35.97ms
step:770/2160 train_time:27717ms step_avg:36.00ms
step:771/2160 train_time:27779ms step_avg:36.03ms
step:772/2160 train_time:27838ms step_avg:36.06ms
step:773/2160 train_time:27899ms step_avg:36.09ms
step:774/2160 train_time:27959ms step_avg:36.12ms
step:775/2160 train_time:28019ms step_avg:36.15ms
step:776/2160 train_time:28078ms step_avg:36.18ms
step:777/2160 train_time:28139ms step_avg:36.22ms
step:778/2160 train_time:28198ms step_avg:36.24ms
step:779/2160 train_time:28259ms step_avg:36.28ms
step:780/2160 train_time:28318ms step_avg:36.31ms
step:781/2160 train_time:28380ms step_avg:36.34ms
step:782/2160 train_time:28442ms step_avg:36.37ms
step:783/2160 train_time:28505ms step_avg:36.41ms
step:784/2160 train_time:28567ms step_avg:36.44ms
step:785/2160 train_time:28630ms step_avg:36.47ms
step:786/2160 train_time:28690ms step_avg:36.50ms
step:787/2160 train_time:28751ms step_avg:36.53ms
step:788/2160 train_time:28811ms step_avg:36.56ms
step:789/2160 train_time:28872ms step_avg:36.59ms
step:790/2160 train_time:28932ms step_avg:36.62ms
step:791/2160 train_time:28992ms step_avg:36.65ms
step:792/2160 train_time:29051ms step_avg:36.68ms
step:793/2160 train_time:29112ms step_avg:36.71ms
step:794/2160 train_time:29172ms step_avg:36.74ms
step:795/2160 train_time:29233ms step_avg:36.77ms
step:796/2160 train_time:29293ms step_avg:36.80ms
step:797/2160 train_time:29354ms step_avg:36.83ms
step:798/2160 train_time:29416ms step_avg:36.86ms
step:799/2160 train_time:29478ms step_avg:36.89ms
step:800/2160 train_time:29538ms step_avg:36.92ms
step:801/2160 train_time:29600ms step_avg:36.95ms
step:802/2160 train_time:29660ms step_avg:36.98ms
step:803/2160 train_time:29721ms step_avg:37.01ms
step:804/2160 train_time:29781ms step_avg:37.04ms
step:805/2160 train_time:29843ms step_avg:37.07ms
step:806/2160 train_time:29902ms step_avg:37.10ms
step:807/2160 train_time:29964ms step_avg:37.13ms
step:808/2160 train_time:30023ms step_avg:37.16ms
step:809/2160 train_time:30085ms step_avg:37.19ms
step:810/2160 train_time:30145ms step_avg:37.22ms
step:811/2160 train_time:30206ms step_avg:37.25ms
step:812/2160 train_time:30265ms step_avg:37.27ms
step:813/2160 train_time:30327ms step_avg:37.30ms
step:814/2160 train_time:30388ms step_avg:37.33ms
step:815/2160 train_time:30450ms step_avg:37.36ms
step:816/2160 train_time:30509ms step_avg:37.39ms
step:817/2160 train_time:30571ms step_avg:37.42ms
step:818/2160 train_time:30630ms step_avg:37.45ms
step:819/2160 train_time:30691ms step_avg:37.47ms
step:820/2160 train_time:30751ms step_avg:37.50ms
step:821/2160 train_time:30812ms step_avg:37.53ms
step:822/2160 train_time:30872ms step_avg:37.56ms
step:823/2160 train_time:30933ms step_avg:37.59ms
step:824/2160 train_time:30992ms step_avg:37.61ms
step:825/2160 train_time:31053ms step_avg:37.64ms
step:826/2160 train_time:31112ms step_avg:37.67ms
step:827/2160 train_time:31173ms step_avg:37.69ms
step:828/2160 train_time:31232ms step_avg:37.72ms
step:829/2160 train_time:31293ms step_avg:37.75ms
step:830/2160 train_time:31353ms step_avg:37.77ms
step:831/2160 train_time:31414ms step_avg:37.80ms
step:832/2160 train_time:31474ms step_avg:37.83ms
step:833/2160 train_time:31535ms step_avg:37.86ms
step:834/2160 train_time:31594ms step_avg:37.88ms
step:835/2160 train_time:31656ms step_avg:37.91ms
step:836/2160 train_time:31715ms step_avg:37.94ms
step:837/2160 train_time:31776ms step_avg:37.96ms
step:838/2160 train_time:31835ms step_avg:37.99ms
step:839/2160 train_time:31896ms step_avg:38.02ms
step:840/2160 train_time:31956ms step_avg:38.04ms
step:841/2160 train_time:32017ms step_avg:38.07ms
step:842/2160 train_time:32077ms step_avg:38.10ms
step:843/2160 train_time:32138ms step_avg:38.12ms
step:844/2160 train_time:32197ms step_avg:38.15ms
step:845/2160 train_time:32259ms step_avg:38.18ms
step:846/2160 train_time:32319ms step_avg:38.20ms
step:847/2160 train_time:32380ms step_avg:38.23ms
step:848/2160 train_time:32440ms step_avg:38.25ms
step:849/2160 train_time:32501ms step_avg:38.28ms
step:850/2160 train_time:32561ms step_avg:38.31ms
step:851/2160 train_time:32623ms step_avg:38.33ms
step:852/2160 train_time:32683ms step_avg:38.36ms
step:853/2160 train_time:32744ms step_avg:38.39ms
step:854/2160 train_time:32804ms step_avg:38.41ms
step:855/2160 train_time:32865ms step_avg:38.44ms
step:856/2160 train_time:32925ms step_avg:38.46ms
step:857/2160 train_time:32987ms step_avg:38.49ms
step:858/2160 train_time:33046ms step_avg:38.52ms
step:859/2160 train_time:33108ms step_avg:38.54ms
step:860/2160 train_time:33167ms step_avg:38.57ms
step:861/2160 train_time:33229ms step_avg:38.59ms
step:862/2160 train_time:33288ms step_avg:38.62ms
step:863/2160 train_time:33349ms step_avg:38.64ms
step:864/2160 train_time:33409ms step_avg:38.67ms
step:865/2160 train_time:33471ms step_avg:38.69ms
step:866/2160 train_time:33530ms step_avg:38.72ms
step:867/2160 train_time:33592ms step_avg:38.75ms
step:868/2160 train_time:33651ms step_avg:38.77ms
step:869/2160 train_time:33713ms step_avg:38.79ms
step:870/2160 train_time:33772ms step_avg:38.82ms
step:871/2160 train_time:33833ms step_avg:38.84ms
step:872/2160 train_time:33893ms step_avg:38.87ms
step:873/2160 train_time:33954ms step_avg:38.89ms
step:874/2160 train_time:34014ms step_avg:38.92ms
step:875/2160 train_time:34075ms step_avg:38.94ms
step:876/2160 train_time:34134ms step_avg:38.97ms
step:877/2160 train_time:34195ms step_avg:38.99ms
step:878/2160 train_time:34254ms step_avg:39.01ms
step:879/2160 train_time:34315ms step_avg:39.04ms
step:880/2160 train_time:34374ms step_avg:39.06ms
step:881/2160 train_time:34436ms step_avg:39.09ms
step:882/2160 train_time:34495ms step_avg:39.11ms
step:883/2160 train_time:34556ms step_avg:39.13ms
step:884/2160 train_time:34616ms step_avg:39.16ms
step:885/2160 train_time:34677ms step_avg:39.18ms
step:886/2160 train_time:34736ms step_avg:39.21ms
step:887/2160 train_time:34797ms step_avg:39.23ms
step:888/2160 train_time:34858ms step_avg:39.25ms
step:889/2160 train_time:34919ms step_avg:39.28ms
step:890/2160 train_time:34979ms step_avg:39.30ms
step:891/2160 train_time:35040ms step_avg:39.33ms
step:892/2160 train_time:35100ms step_avg:39.35ms
step:893/2160 train_time:35162ms step_avg:39.37ms
step:894/2160 train_time:35222ms step_avg:39.40ms
step:895/2160 train_time:35284ms step_avg:39.42ms
step:896/2160 train_time:35344ms step_avg:39.45ms
step:897/2160 train_time:35405ms step_avg:39.47ms
step:898/2160 train_time:35465ms step_avg:39.49ms
step:899/2160 train_time:35527ms step_avg:39.52ms
step:900/2160 train_time:35587ms step_avg:39.54ms
step:901/2160 train_time:35648ms step_avg:39.57ms
step:902/2160 train_time:35708ms step_avg:39.59ms
step:903/2160 train_time:35769ms step_avg:39.61ms
step:904/2160 train_time:35829ms step_avg:39.63ms
step:905/2160 train_time:35890ms step_avg:39.66ms
step:906/2160 train_time:35949ms step_avg:39.68ms
step:907/2160 train_time:36010ms step_avg:39.70ms
step:908/2160 train_time:36071ms step_avg:39.73ms
step:909/2160 train_time:36132ms step_avg:39.75ms
step:910/2160 train_time:36192ms step_avg:39.77ms
step:911/2160 train_time:36253ms step_avg:39.79ms
step:912/2160 train_time:36312ms step_avg:39.82ms
step:913/2160 train_time:36373ms step_avg:39.84ms
step:914/2160 train_time:36433ms step_avg:39.86ms
step:915/2160 train_time:36494ms step_avg:39.88ms
step:916/2160 train_time:36553ms step_avg:39.91ms
step:917/2160 train_time:36614ms step_avg:39.93ms
step:918/2160 train_time:36673ms step_avg:39.95ms
step:919/2160 train_time:36735ms step_avg:39.97ms
step:920/2160 train_time:36794ms step_avg:39.99ms
step:921/2160 train_time:36855ms step_avg:40.02ms
step:922/2160 train_time:36914ms step_avg:40.04ms
step:923/2160 train_time:36975ms step_avg:40.06ms
step:924/2160 train_time:37035ms step_avg:40.08ms
step:925/2160 train_time:37096ms step_avg:40.10ms
step:926/2160 train_time:37156ms step_avg:40.13ms
step:927/2160 train_time:37217ms step_avg:40.15ms
step:928/2160 train_time:37276ms step_avg:40.17ms
step:929/2160 train_time:37338ms step_avg:40.19ms
step:930/2160 train_time:37398ms step_avg:40.21ms
step:931/2160 train_time:37460ms step_avg:40.24ms
step:932/2160 train_time:37520ms step_avg:40.26ms
step:933/2160 train_time:37581ms step_avg:40.28ms
step:934/2160 train_time:37640ms step_avg:40.30ms
step:935/2160 train_time:37702ms step_avg:40.32ms
step:936/2160 train_time:37762ms step_avg:40.34ms
step:937/2160 train_time:37824ms step_avg:40.37ms
step:938/2160 train_time:37884ms step_avg:40.39ms
step:939/2160 train_time:37947ms step_avg:40.41ms
step:940/2160 train_time:38008ms step_avg:40.43ms
step:941/2160 train_time:38069ms step_avg:40.46ms
step:942/2160 train_time:38129ms step_avg:40.48ms
step:943/2160 train_time:38190ms step_avg:40.50ms
step:944/2160 train_time:38249ms step_avg:40.52ms
step:945/2160 train_time:38311ms step_avg:40.54ms
step:946/2160 train_time:38370ms step_avg:40.56ms
step:947/2160 train_time:38431ms step_avg:40.58ms
step:948/2160 train_time:38491ms step_avg:40.60ms
step:949/2160 train_time:38552ms step_avg:40.62ms
step:950/2160 train_time:38612ms step_avg:40.64ms
step:951/2160 train_time:38673ms step_avg:40.67ms
step:952/2160 train_time:38733ms step_avg:40.69ms
step:953/2160 train_time:38794ms step_avg:40.71ms
step:954/2160 train_time:38854ms step_avg:40.73ms
step:955/2160 train_time:38916ms step_avg:40.75ms
step:956/2160 train_time:38976ms step_avg:40.77ms
step:957/2160 train_time:39037ms step_avg:40.79ms
step:958/2160 train_time:39096ms step_avg:40.81ms
step:959/2160 train_time:39158ms step_avg:40.83ms
step:960/2160 train_time:39217ms step_avg:40.85ms
step:961/2160 train_time:39278ms step_avg:40.87ms
step:962/2160 train_time:39337ms step_avg:40.89ms
step:963/2160 train_time:39399ms step_avg:40.91ms
step:964/2160 train_time:39459ms step_avg:40.93ms
step:965/2160 train_time:39521ms step_avg:40.95ms
step:966/2160 train_time:39580ms step_avg:40.97ms
step:967/2160 train_time:39642ms step_avg:40.99ms
step:968/2160 train_time:39702ms step_avg:41.01ms
step:969/2160 train_time:39763ms step_avg:41.04ms
step:970/2160 train_time:39823ms step_avg:41.05ms
step:971/2160 train_time:39884ms step_avg:41.08ms
step:972/2160 train_time:39944ms step_avg:41.09ms
step:973/2160 train_time:40006ms step_avg:41.12ms
step:974/2160 train_time:40066ms step_avg:41.14ms
step:975/2160 train_time:40127ms step_avg:41.16ms
step:976/2160 train_time:40187ms step_avg:41.18ms
step:977/2160 train_time:40249ms step_avg:41.20ms
step:978/2160 train_time:40308ms step_avg:41.21ms
step:979/2160 train_time:40370ms step_avg:41.24ms
step:980/2160 train_time:40429ms step_avg:41.25ms
step:981/2160 train_time:40491ms step_avg:41.28ms
step:982/2160 train_time:40551ms step_avg:41.29ms
step:983/2160 train_time:40612ms step_avg:41.31ms
step:984/2160 train_time:40672ms step_avg:41.33ms
step:985/2160 train_time:40734ms step_avg:41.35ms
step:986/2160 train_time:40793ms step_avg:41.37ms
step:987/2160 train_time:40854ms step_avg:41.39ms
step:988/2160 train_time:40914ms step_avg:41.41ms
step:989/2160 train_time:40975ms step_avg:41.43ms
step:990/2160 train_time:41034ms step_avg:41.45ms
step:991/2160 train_time:41096ms step_avg:41.47ms
step:992/2160 train_time:41156ms step_avg:41.49ms
step:993/2160 train_time:41217ms step_avg:41.51ms
step:994/2160 train_time:41276ms step_avg:41.53ms
step:995/2160 train_time:41337ms step_avg:41.54ms
step:996/2160 train_time:41396ms step_avg:41.56ms
step:997/2160 train_time:41458ms step_avg:41.58ms
step:998/2160 train_time:41518ms step_avg:41.60ms
step:999/2160 train_time:41580ms step_avg:41.62ms
step:1000/2160 train_time:41639ms step_avg:41.64ms
step:1000/2160 val_loss:3.6929 train_time:41702ms step_avg:41.70ms
step:1001/2160 train_time:41720ms step_avg:41.68ms
step:1002/2160 train_time:41765ms step_avg:41.68ms
step:1003/2160 train_time:41829ms step_avg:41.70ms
step:1004/2160 train_time:41891ms step_avg:41.72ms
step:1005/2160 train_time:41952ms step_avg:41.74ms
step:1006/2160 train_time:42012ms step_avg:41.76ms
step:1007/2160 train_time:42073ms step_avg:41.78ms
step:1008/2160 train_time:42131ms step_avg:41.80ms
step:1009/2160 train_time:42192ms step_avg:41.82ms
step:1010/2160 train_time:42251ms step_avg:41.83ms
step:1011/2160 train_time:42312ms step_avg:41.85ms
step:1012/2160 train_time:42370ms step_avg:41.87ms
step:1013/2160 train_time:42431ms step_avg:41.89ms
step:1014/2160 train_time:42490ms step_avg:41.90ms
step:1015/2160 train_time:42550ms step_avg:41.92ms
step:1016/2160 train_time:42610ms step_avg:41.94ms
step:1017/2160 train_time:42674ms step_avg:41.96ms
step:1018/2160 train_time:42736ms step_avg:41.98ms
step:1019/2160 train_time:42799ms step_avg:42.00ms
step:1020/2160 train_time:42859ms step_avg:42.02ms
step:1021/2160 train_time:42921ms step_avg:42.04ms
step:1022/2160 train_time:42981ms step_avg:42.06ms
step:1023/2160 train_time:43041ms step_avg:42.07ms
step:1024/2160 train_time:43101ms step_avg:42.09ms
step:1025/2160 train_time:43162ms step_avg:42.11ms
step:1026/2160 train_time:43222ms step_avg:42.13ms
step:1027/2160 train_time:43283ms step_avg:42.14ms
step:1028/2160 train_time:43343ms step_avg:42.16ms
step:1029/2160 train_time:43404ms step_avg:42.18ms
step:1030/2160 train_time:43463ms step_avg:42.20ms
step:1031/2160 train_time:43524ms step_avg:42.22ms
step:1032/2160 train_time:43584ms step_avg:42.23ms
step:1033/2160 train_time:43646ms step_avg:42.25ms
step:1034/2160 train_time:43707ms step_avg:42.27ms
step:1035/2160 train_time:43770ms step_avg:42.29ms
step:1036/2160 train_time:43830ms step_avg:42.31ms
step:1037/2160 train_time:43892ms step_avg:42.33ms
step:1038/2160 train_time:43951ms step_avg:42.34ms
step:1039/2160 train_time:44012ms step_avg:42.36ms
step:1040/2160 train_time:44071ms step_avg:42.38ms
step:1041/2160 train_time:44133ms step_avg:42.39ms
step:1042/2160 train_time:44192ms step_avg:42.41ms
step:1043/2160 train_time:44253ms step_avg:42.43ms
step:1044/2160 train_time:44314ms step_avg:42.45ms
step:1045/2160 train_time:44374ms step_avg:42.46ms
step:1046/2160 train_time:44434ms step_avg:42.48ms
step:1047/2160 train_time:44495ms step_avg:42.50ms
step:1048/2160 train_time:44555ms step_avg:42.51ms
step:1049/2160 train_time:44616ms step_avg:42.53ms
step:1050/2160 train_time:44676ms step_avg:42.55ms
step:1051/2160 train_time:44738ms step_avg:42.57ms
step:1052/2160 train_time:44799ms step_avg:42.58ms
step:1053/2160 train_time:44860ms step_avg:42.60ms
step:1054/2160 train_time:44920ms step_avg:42.62ms
step:1055/2160 train_time:44981ms step_avg:42.64ms
step:1056/2160 train_time:45041ms step_avg:42.65ms
step:1057/2160 train_time:45102ms step_avg:42.67ms
step:1058/2160 train_time:45161ms step_avg:42.69ms
step:1059/2160 train_time:45223ms step_avg:42.70ms
step:1060/2160 train_time:45282ms step_avg:42.72ms
step:1061/2160 train_time:45344ms step_avg:42.74ms
step:1062/2160 train_time:45404ms step_avg:42.75ms
step:1063/2160 train_time:45465ms step_avg:42.77ms
step:1064/2160 train_time:45525ms step_avg:42.79ms
step:1065/2160 train_time:45587ms step_avg:42.81ms
step:1066/2160 train_time:45647ms step_avg:42.82ms
step:1067/2160 train_time:45709ms step_avg:42.84ms
step:1068/2160 train_time:45770ms step_avg:42.86ms
step:1069/2160 train_time:45832ms step_avg:42.87ms
step:1070/2160 train_time:45891ms step_avg:42.89ms
step:1071/2160 train_time:45952ms step_avg:42.91ms
step:1072/2160 train_time:46012ms step_avg:42.92ms
step:1073/2160 train_time:46073ms step_avg:42.94ms
step:1074/2160 train_time:46134ms step_avg:42.95ms
step:1075/2160 train_time:46195ms step_avg:42.97ms
step:1076/2160 train_time:46255ms step_avg:42.99ms
step:1077/2160 train_time:46317ms step_avg:43.01ms
step:1078/2160 train_time:46376ms step_avg:43.02ms
step:1079/2160 train_time:46438ms step_avg:43.04ms
step:1080/2160 train_time:46498ms step_avg:43.05ms
step:1081/2160 train_time:46559ms step_avg:43.07ms
step:1082/2160 train_time:46618ms step_avg:43.09ms
step:1083/2160 train_time:46680ms step_avg:43.10ms
step:1084/2160 train_time:46740ms step_avg:43.12ms
step:1085/2160 train_time:46802ms step_avg:43.14ms
step:1086/2160 train_time:46861ms step_avg:43.15ms
step:1087/2160 train_time:46922ms step_avg:43.17ms
step:1088/2160 train_time:46982ms step_avg:43.18ms
step:1089/2160 train_time:47044ms step_avg:43.20ms
step:1090/2160 train_time:47104ms step_avg:43.21ms
step:1091/2160 train_time:47165ms step_avg:43.23ms
step:1092/2160 train_time:47225ms step_avg:43.25ms
step:1093/2160 train_time:47287ms step_avg:43.26ms
step:1094/2160 train_time:47347ms step_avg:43.28ms
step:1095/2160 train_time:47408ms step_avg:43.30ms
step:1096/2160 train_time:47468ms step_avg:43.31ms
step:1097/2160 train_time:47529ms step_avg:43.33ms
step:1098/2160 train_time:47589ms step_avg:43.34ms
step:1099/2160 train_time:47651ms step_avg:43.36ms
step:1100/2160 train_time:47711ms step_avg:43.37ms
step:1101/2160 train_time:47773ms step_avg:43.39ms
step:1102/2160 train_time:47832ms step_avg:43.40ms
step:1103/2160 train_time:47893ms step_avg:43.42ms
step:1104/2160 train_time:47952ms step_avg:43.44ms
step:1105/2160 train_time:48014ms step_avg:43.45ms
step:1106/2160 train_time:48074ms step_avg:43.47ms
step:1107/2160 train_time:48136ms step_avg:43.48ms
step:1108/2160 train_time:48195ms step_avg:43.50ms
step:1109/2160 train_time:48256ms step_avg:43.51ms
step:1110/2160 train_time:48316ms step_avg:43.53ms
step:1111/2160 train_time:48378ms step_avg:43.54ms
step:1112/2160 train_time:48438ms step_avg:43.56ms
step:1113/2160 train_time:48499ms step_avg:43.57ms
step:1114/2160 train_time:48558ms step_avg:43.59ms
step:1115/2160 train_time:48620ms step_avg:43.61ms
step:1116/2160 train_time:48680ms step_avg:43.62ms
step:1117/2160 train_time:48741ms step_avg:43.64ms
step:1118/2160 train_time:48801ms step_avg:43.65ms
step:1119/2160 train_time:48862ms step_avg:43.67ms
step:1120/2160 train_time:48922ms step_avg:43.68ms
step:1121/2160 train_time:48984ms step_avg:43.70ms
step:1122/2160 train_time:49044ms step_avg:43.71ms
step:1123/2160 train_time:49105ms step_avg:43.73ms
step:1124/2160 train_time:49165ms step_avg:43.74ms
step:1125/2160 train_time:49227ms step_avg:43.76ms
step:1126/2160 train_time:49288ms step_avg:43.77ms
step:1127/2160 train_time:49349ms step_avg:43.79ms
step:1128/2160 train_time:49409ms step_avg:43.80ms
step:1129/2160 train_time:49471ms step_avg:43.82ms
step:1130/2160 train_time:49531ms step_avg:43.83ms
step:1131/2160 train_time:49592ms step_avg:43.85ms
step:1132/2160 train_time:49652ms step_avg:43.86ms
step:1133/2160 train_time:49714ms step_avg:43.88ms
step:1134/2160 train_time:49773ms step_avg:43.89ms
step:1135/2160 train_time:49834ms step_avg:43.91ms
step:1136/2160 train_time:49894ms step_avg:43.92ms
step:1137/2160 train_time:49956ms step_avg:43.94ms
step:1138/2160 train_time:50016ms step_avg:43.95ms
step:1139/2160 train_time:50077ms step_avg:43.97ms
step:1140/2160 train_time:50137ms step_avg:43.98ms
step:1141/2160 train_time:50199ms step_avg:44.00ms
step:1142/2160 train_time:50258ms step_avg:44.01ms
step:1143/2160 train_time:50320ms step_avg:44.02ms
step:1144/2160 train_time:50380ms step_avg:44.04ms
step:1145/2160 train_time:50441ms step_avg:44.05ms
step:1146/2160 train_time:50502ms step_avg:44.07ms
step:1147/2160 train_time:50563ms step_avg:44.08ms
step:1148/2160 train_time:50623ms step_avg:44.10ms
step:1149/2160 train_time:50684ms step_avg:44.11ms
step:1150/2160 train_time:50744ms step_avg:44.13ms
step:1151/2160 train_time:50806ms step_avg:44.14ms
step:1152/2160 train_time:50865ms step_avg:44.15ms
step:1153/2160 train_time:50927ms step_avg:44.17ms
step:1154/2160 train_time:50987ms step_avg:44.18ms
step:1155/2160 train_time:51049ms step_avg:44.20ms
step:1156/2160 train_time:51109ms step_avg:44.21ms
step:1157/2160 train_time:51170ms step_avg:44.23ms
step:1158/2160 train_time:51230ms step_avg:44.24ms
step:1159/2160 train_time:51291ms step_avg:44.25ms
step:1160/2160 train_time:51351ms step_avg:44.27ms
step:1161/2160 train_time:51412ms step_avg:44.28ms
step:1162/2160 train_time:51472ms step_avg:44.30ms
step:1163/2160 train_time:51533ms step_avg:44.31ms
step:1164/2160 train_time:51592ms step_avg:44.32ms
step:1165/2160 train_time:51654ms step_avg:44.34ms
step:1166/2160 train_time:51713ms step_avg:44.35ms
step:1167/2160 train_time:51774ms step_avg:44.37ms
step:1168/2160 train_time:51833ms step_avg:44.38ms
step:1169/2160 train_time:51895ms step_avg:44.39ms
step:1170/2160 train_time:51954ms step_avg:44.41ms
step:1171/2160 train_time:52016ms step_avg:44.42ms
step:1172/2160 train_time:52076ms step_avg:44.43ms
step:1173/2160 train_time:52138ms step_avg:44.45ms
step:1174/2160 train_time:52198ms step_avg:44.46ms
step:1175/2160 train_time:52260ms step_avg:44.48ms
step:1176/2160 train_time:52319ms step_avg:44.49ms
step:1177/2160 train_time:52381ms step_avg:44.50ms
step:1178/2160 train_time:52440ms step_avg:44.52ms
step:1179/2160 train_time:52502ms step_avg:44.53ms
step:1180/2160 train_time:52561ms step_avg:44.54ms
step:1181/2160 train_time:52623ms step_avg:44.56ms
step:1182/2160 train_time:52683ms step_avg:44.57ms
step:1183/2160 train_time:52745ms step_avg:44.59ms
step:1184/2160 train_time:52805ms step_avg:44.60ms
step:1185/2160 train_time:52867ms step_avg:44.61ms
step:1186/2160 train_time:52927ms step_avg:44.63ms
step:1187/2160 train_time:52989ms step_avg:44.64ms
step:1188/2160 train_time:53050ms step_avg:44.65ms
step:1189/2160 train_time:53112ms step_avg:44.67ms
step:1190/2160 train_time:53171ms step_avg:44.68ms
step:1191/2160 train_time:53233ms step_avg:44.70ms
step:1192/2160 train_time:53293ms step_avg:44.71ms
step:1193/2160 train_time:53354ms step_avg:44.72ms
step:1194/2160 train_time:53415ms step_avg:44.74ms
step:1195/2160 train_time:53476ms step_avg:44.75ms
step:1196/2160 train_time:53535ms step_avg:44.76ms
step:1197/2160 train_time:53597ms step_avg:44.78ms
step:1198/2160 train_time:53657ms step_avg:44.79ms
step:1199/2160 train_time:53718ms step_avg:44.80ms
step:1200/2160 train_time:53777ms step_avg:44.81ms
step:1201/2160 train_time:53838ms step_avg:44.83ms
step:1202/2160 train_time:53899ms step_avg:44.84ms
step:1203/2160 train_time:53960ms step_avg:44.85ms
step:1204/2160 train_time:54020ms step_avg:44.87ms
step:1205/2160 train_time:54081ms step_avg:44.88ms
step:1206/2160 train_time:54141ms step_avg:44.89ms
step:1207/2160 train_time:54203ms step_avg:44.91ms
step:1208/2160 train_time:54263ms step_avg:44.92ms
step:1209/2160 train_time:54325ms step_avg:44.93ms
step:1210/2160 train_time:54385ms step_avg:44.95ms
step:1211/2160 train_time:54446ms step_avg:44.96ms
step:1212/2160 train_time:54506ms step_avg:44.97ms
step:1213/2160 train_time:54567ms step_avg:44.99ms
step:1214/2160 train_time:54627ms step_avg:45.00ms
step:1215/2160 train_time:54689ms step_avg:45.01ms
step:1216/2160 train_time:54749ms step_avg:45.02ms
step:1217/2160 train_time:54811ms step_avg:45.04ms
step:1218/2160 train_time:54870ms step_avg:45.05ms
step:1219/2160 train_time:54931ms step_avg:45.06ms
step:1220/2160 train_time:54991ms step_avg:45.07ms
step:1221/2160 train_time:55052ms step_avg:45.09ms
step:1222/2160 train_time:55112ms step_avg:45.10ms
step:1223/2160 train_time:55174ms step_avg:45.11ms
step:1224/2160 train_time:55233ms step_avg:45.13ms
step:1225/2160 train_time:55294ms step_avg:45.14ms
step:1226/2160 train_time:55354ms step_avg:45.15ms
step:1227/2160 train_time:55415ms step_avg:45.16ms
step:1228/2160 train_time:55475ms step_avg:45.18ms
step:1229/2160 train_time:55536ms step_avg:45.19ms
step:1230/2160 train_time:55595ms step_avg:45.20ms
step:1231/2160 train_time:55657ms step_avg:45.21ms
step:1232/2160 train_time:55717ms step_avg:45.23ms
step:1233/2160 train_time:55778ms step_avg:45.24ms
step:1234/2160 train_time:55837ms step_avg:45.25ms
step:1235/2160 train_time:55899ms step_avg:45.26ms
step:1236/2160 train_time:55959ms step_avg:45.27ms
step:1237/2160 train_time:56020ms step_avg:45.29ms
step:1238/2160 train_time:56079ms step_avg:45.30ms
step:1239/2160 train_time:56142ms step_avg:45.31ms
step:1240/2160 train_time:56201ms step_avg:45.32ms
step:1241/2160 train_time:56262ms step_avg:45.34ms
step:1242/2160 train_time:56322ms step_avg:45.35ms
step:1243/2160 train_time:56384ms step_avg:45.36ms
step:1244/2160 train_time:56444ms step_avg:45.37ms
step:1245/2160 train_time:56506ms step_avg:45.39ms
step:1246/2160 train_time:56565ms step_avg:45.40ms
step:1247/2160 train_time:56628ms step_avg:45.41ms
step:1248/2160 train_time:56689ms step_avg:45.42ms
step:1249/2160 train_time:56751ms step_avg:45.44ms
step:1250/2160 train_time:56810ms step_avg:45.45ms
step:1250/2160 val_loss:3.5778 train_time:56872ms step_avg:45.50ms
step:1251/2160 train_time:56891ms step_avg:45.48ms
step:1252/2160 train_time:56934ms step_avg:45.47ms
step:1253/2160 train_time:57000ms step_avg:45.49ms
step:1254/2160 train_time:57062ms step_avg:45.50ms
step:1255/2160 train_time:57124ms step_avg:45.52ms
step:1256/2160 train_time:57184ms step_avg:45.53ms
step:1257/2160 train_time:57245ms step_avg:45.54ms
step:1258/2160 train_time:57304ms step_avg:45.55ms
step:1259/2160 train_time:57365ms step_avg:45.56ms
step:1260/2160 train_time:57424ms step_avg:45.57ms
step:1261/2160 train_time:57485ms step_avg:45.59ms
step:1262/2160 train_time:57545ms step_avg:45.60ms
step:1263/2160 train_time:57605ms step_avg:45.61ms
step:1264/2160 train_time:57665ms step_avg:45.62ms
step:1265/2160 train_time:57725ms step_avg:45.63ms
step:1266/2160 train_time:57785ms step_avg:45.64ms
step:1267/2160 train_time:57847ms step_avg:45.66ms
step:1268/2160 train_time:57909ms step_avg:45.67ms
step:1269/2160 train_time:57972ms step_avg:45.68ms
step:1270/2160 train_time:58033ms step_avg:45.69ms
step:1271/2160 train_time:58095ms step_avg:45.71ms
step:1272/2160 train_time:58154ms step_avg:45.72ms
step:1273/2160 train_time:58215ms step_avg:45.73ms
step:1274/2160 train_time:58275ms step_avg:45.74ms
step:1275/2160 train_time:58337ms step_avg:45.75ms
step:1276/2160 train_time:58396ms step_avg:45.76ms
step:1277/2160 train_time:58457ms step_avg:45.78ms
step:1278/2160 train_time:58516ms step_avg:45.79ms
step:1279/2160 train_time:58576ms step_avg:45.80ms
step:1280/2160 train_time:58635ms step_avg:45.81ms
step:1281/2160 train_time:58696ms step_avg:45.82ms
step:1282/2160 train_time:58755ms step_avg:45.83ms
step:1283/2160 train_time:58817ms step_avg:45.84ms
step:1284/2160 train_time:58877ms step_avg:45.85ms
step:1285/2160 train_time:58939ms step_avg:45.87ms
step:1286/2160 train_time:59000ms step_avg:45.88ms
step:1287/2160 train_time:59062ms step_avg:45.89ms
step:1288/2160 train_time:59123ms step_avg:45.90ms
step:1289/2160 train_time:59184ms step_avg:45.91ms
step:1290/2160 train_time:59244ms step_avg:45.93ms
step:1291/2160 train_time:59306ms step_avg:45.94ms
step:1292/2160 train_time:59365ms step_avg:45.95ms
step:1293/2160 train_time:59426ms step_avg:45.96ms
step:1294/2160 train_time:59485ms step_avg:45.97ms
step:1295/2160 train_time:59546ms step_avg:45.98ms
step:1296/2160 train_time:59606ms step_avg:45.99ms
step:1297/2160 train_time:59667ms step_avg:46.00ms
step:1298/2160 train_time:59727ms step_avg:46.01ms
step:1299/2160 train_time:59788ms step_avg:46.03ms
step:1300/2160 train_time:59848ms step_avg:46.04ms
step:1301/2160 train_time:59910ms step_avg:46.05ms
step:1302/2160 train_time:59971ms step_avg:46.06ms
step:1303/2160 train_time:60033ms step_avg:46.07ms
step:1304/2160 train_time:60093ms step_avg:46.08ms
step:1305/2160 train_time:60154ms step_avg:46.10ms
step:1306/2160 train_time:60214ms step_avg:46.11ms
step:1307/2160 train_time:60275ms step_avg:46.12ms
step:1308/2160 train_time:60335ms step_avg:46.13ms
step:1309/2160 train_time:60396ms step_avg:46.14ms
step:1310/2160 train_time:60456ms step_avg:46.15ms
step:1311/2160 train_time:60517ms step_avg:46.16ms
step:1312/2160 train_time:60577ms step_avg:46.17ms
step:1313/2160 train_time:60639ms step_avg:46.18ms
step:1314/2160 train_time:60698ms step_avg:46.19ms
step:1315/2160 train_time:60759ms step_avg:46.20ms
step:1316/2160 train_time:60819ms step_avg:46.22ms
step:1317/2160 train_time:60882ms step_avg:46.23ms
step:1318/2160 train_time:60942ms step_avg:46.24ms
step:1319/2160 train_time:61004ms step_avg:46.25ms
step:1320/2160 train_time:61064ms step_avg:46.26ms
step:1321/2160 train_time:61125ms step_avg:46.27ms
step:1322/2160 train_time:61186ms step_avg:46.28ms
step:1323/2160 train_time:61247ms step_avg:46.29ms
step:1324/2160 train_time:61307ms step_avg:46.30ms
step:1325/2160 train_time:61369ms step_avg:46.32ms
step:1326/2160 train_time:61428ms step_avg:46.33ms
step:1327/2160 train_time:61490ms step_avg:46.34ms
step:1328/2160 train_time:61549ms step_avg:46.35ms
step:1329/2160 train_time:61610ms step_avg:46.36ms
step:1330/2160 train_time:61669ms step_avg:46.37ms
step:1331/2160 train_time:61731ms step_avg:46.38ms
step:1332/2160 train_time:61790ms step_avg:46.39ms
step:1333/2160 train_time:61852ms step_avg:46.40ms
step:1334/2160 train_time:61911ms step_avg:46.41ms
step:1335/2160 train_time:61973ms step_avg:46.42ms
step:1336/2160 train_time:62033ms step_avg:46.43ms
step:1337/2160 train_time:62095ms step_avg:46.44ms
step:1338/2160 train_time:62155ms step_avg:46.45ms
step:1339/2160 train_time:62216ms step_avg:46.46ms
step:1340/2160 train_time:62276ms step_avg:46.47ms
step:1341/2160 train_time:62337ms step_avg:46.49ms
step:1342/2160 train_time:62397ms step_avg:46.50ms
step:1343/2160 train_time:62458ms step_avg:46.51ms
step:1344/2160 train_time:62518ms step_avg:46.52ms
step:1345/2160 train_time:62579ms step_avg:46.53ms
step:1346/2160 train_time:62638ms step_avg:46.54ms
step:1347/2160 train_time:62700ms step_avg:46.55ms
step:1348/2160 train_time:62760ms step_avg:46.56ms
step:1349/2160 train_time:62822ms step_avg:46.57ms
step:1350/2160 train_time:62882ms step_avg:46.58ms
step:1351/2160 train_time:62944ms step_avg:46.59ms
step:1352/2160 train_time:63004ms step_avg:46.60ms
step:1353/2160 train_time:63065ms step_avg:46.61ms
step:1354/2160 train_time:63126ms step_avg:46.62ms
step:1355/2160 train_time:63187ms step_avg:46.63ms
step:1356/2160 train_time:63247ms step_avg:46.64ms
step:1357/2160 train_time:63308ms step_avg:46.65ms
step:1358/2160 train_time:63368ms step_avg:46.66ms
step:1359/2160 train_time:63429ms step_avg:46.67ms
step:1360/2160 train_time:63488ms step_avg:46.68ms
step:1361/2160 train_time:63550ms step_avg:46.69ms
step:1362/2160 train_time:63609ms step_avg:46.70ms
step:1363/2160 train_time:63671ms step_avg:46.71ms
step:1364/2160 train_time:63731ms step_avg:46.72ms
step:1365/2160 train_time:63792ms step_avg:46.73ms
step:1366/2160 train_time:63852ms step_avg:46.74ms
step:1367/2160 train_time:63913ms step_avg:46.75ms
step:1368/2160 train_time:63973ms step_avg:46.76ms
step:1369/2160 train_time:64035ms step_avg:46.78ms
step:1370/2160 train_time:64095ms step_avg:46.78ms
step:1371/2160 train_time:64157ms step_avg:46.80ms
step:1372/2160 train_time:64217ms step_avg:46.81ms
step:1373/2160 train_time:64278ms step_avg:46.82ms
step:1374/2160 train_time:64337ms step_avg:46.82ms
step:1375/2160 train_time:64398ms step_avg:46.83ms
step:1376/2160 train_time:64458ms step_avg:46.84ms
step:1377/2160 train_time:64519ms step_avg:46.85ms
step:1378/2160 train_time:64578ms step_avg:46.86ms
step:1379/2160 train_time:64640ms step_avg:46.87ms
step:1380/2160 train_time:64699ms step_avg:46.88ms
step:1381/2160 train_time:64761ms step_avg:46.89ms
step:1382/2160 train_time:64821ms step_avg:46.90ms
step:1383/2160 train_time:64884ms step_avg:46.92ms
step:1384/2160 train_time:64944ms step_avg:46.92ms
step:1385/2160 train_time:65006ms step_avg:46.94ms
step:1386/2160 train_time:65066ms step_avg:46.94ms
step:1387/2160 train_time:65128ms step_avg:46.96ms
step:1388/2160 train_time:65187ms step_avg:46.97ms
step:1389/2160 train_time:65249ms step_avg:46.98ms
step:1390/2160 train_time:65308ms step_avg:46.98ms
step:1391/2160 train_time:65370ms step_avg:46.99ms
step:1392/2160 train_time:65428ms step_avg:47.00ms
step:1393/2160 train_time:65489ms step_avg:47.01ms
step:1394/2160 train_time:65549ms step_avg:47.02ms
step:1395/2160 train_time:65610ms step_avg:47.03ms
step:1396/2160 train_time:65670ms step_avg:47.04ms
step:1397/2160 train_time:65731ms step_avg:47.05ms
step:1398/2160 train_time:65791ms step_avg:47.06ms
step:1399/2160 train_time:65853ms step_avg:47.07ms
step:1400/2160 train_time:65913ms step_avg:47.08ms
step:1401/2160 train_time:65974ms step_avg:47.09ms
step:1402/2160 train_time:66034ms step_avg:47.10ms
step:1403/2160 train_time:66095ms step_avg:47.11ms
step:1404/2160 train_time:66155ms step_avg:47.12ms
step:1405/2160 train_time:66216ms step_avg:47.13ms
step:1406/2160 train_time:66275ms step_avg:47.14ms
step:1407/2160 train_time:66336ms step_avg:47.15ms
step:1408/2160 train_time:66396ms step_avg:47.16ms
step:1409/2160 train_time:66457ms step_avg:47.17ms
step:1410/2160 train_time:66517ms step_avg:47.17ms
step:1411/2160 train_time:66578ms step_avg:47.18ms
step:1412/2160 train_time:66637ms step_avg:47.19ms
step:1413/2160 train_time:66699ms step_avg:47.20ms
step:1414/2160 train_time:66759ms step_avg:47.21ms
step:1415/2160 train_time:66821ms step_avg:47.22ms
step:1416/2160 train_time:66909ms step_avg:47.25ms
step:1417/2160 train_time:67000ms step_avg:47.28ms
step:1418/2160 train_time:67088ms step_avg:47.31ms
step:1419/2160 train_time:67178ms step_avg:47.34ms
step:1420/2160 train_time:67265ms step_avg:47.37ms
step:1421/2160 train_time:67354ms step_avg:47.40ms
step:1422/2160 train_time:67441ms step_avg:47.43ms
step:1423/2160 train_time:67530ms step_avg:47.46ms
step:1424/2160 train_time:67618ms step_avg:47.48ms
step:1425/2160 train_time:67706ms step_avg:47.51ms
step:1426/2160 train_time:67794ms step_avg:47.54ms
step:1427/2160 train_time:67883ms step_avg:47.57ms
step:1428/2160 train_time:67972ms step_avg:47.60ms
step:1429/2160 train_time:68062ms step_avg:47.63ms
step:1430/2160 train_time:68150ms step_avg:47.66ms
step:1431/2160 train_time:68240ms step_avg:47.69ms
step:1432/2160 train_time:68327ms step_avg:47.71ms
step:1433/2160 train_time:68416ms step_avg:47.74ms
step:1434/2160 train_time:68504ms step_avg:47.77ms
step:1435/2160 train_time:68593ms step_avg:47.80ms
step:1436/2160 train_time:68681ms step_avg:47.83ms
step:1437/2160 train_time:68770ms step_avg:47.86ms
step:1438/2160 train_time:68858ms step_avg:47.88ms
step:1439/2160 train_time:68947ms step_avg:47.91ms
step:1440/2160 train_time:69034ms step_avg:47.94ms
step:1441/2160 train_time:69123ms step_avg:47.97ms
step:1442/2160 train_time:69211ms step_avg:48.00ms
step:1443/2160 train_time:69301ms step_avg:48.03ms
step:1444/2160 train_time:69389ms step_avg:48.05ms
step:1445/2160 train_time:69478ms step_avg:48.08ms
step:1446/2160 train_time:69565ms step_avg:48.11ms
step:1447/2160 train_time:69655ms step_avg:48.14ms
step:1448/2160 train_time:69742ms step_avg:48.16ms
step:1449/2160 train_time:69831ms step_avg:48.19ms
step:1450/2160 train_time:69919ms step_avg:48.22ms
step:1451/2160 train_time:70009ms step_avg:48.25ms
step:1452/2160 train_time:70097ms step_avg:48.28ms
step:1453/2160 train_time:70186ms step_avg:48.30ms
step:1454/2160 train_time:70274ms step_avg:48.33ms
step:1455/2160 train_time:70363ms step_avg:48.36ms
step:1456/2160 train_time:70450ms step_avg:48.39ms
step:1457/2160 train_time:70540ms step_avg:48.41ms
step:1458/2160 train_time:70627ms step_avg:48.44ms
step:1459/2160 train_time:70716ms step_avg:48.47ms
step:1460/2160 train_time:70804ms step_avg:48.50ms
step:1461/2160 train_time:70893ms step_avg:48.52ms
step:1462/2160 train_time:70980ms step_avg:48.55ms
step:1463/2160 train_time:71070ms step_avg:48.58ms
step:1464/2160 train_time:71159ms step_avg:48.61ms
step:1465/2160 train_time:71249ms step_avg:48.63ms
step:1466/2160 train_time:71336ms step_avg:48.66ms
step:1467/2160 train_time:71425ms step_avg:48.69ms
step:1468/2160 train_time:71513ms step_avg:48.71ms
step:1469/2160 train_time:71602ms step_avg:48.74ms
step:1470/2160 train_time:71689ms step_avg:48.77ms
step:1471/2160 train_time:71778ms step_avg:48.80ms
step:1472/2160 train_time:71865ms step_avg:48.82ms
step:1473/2160 train_time:71955ms step_avg:48.85ms
step:1474/2160 train_time:72042ms step_avg:48.88ms
step:1475/2160 train_time:72132ms step_avg:48.90ms
step:1476/2160 train_time:72219ms step_avg:48.93ms
step:1477/2160 train_time:72309ms step_avg:48.96ms
step:1478/2160 train_time:72397ms step_avg:48.98ms
step:1479/2160 train_time:72486ms step_avg:49.01ms
step:1480/2160 train_time:72574ms step_avg:49.04ms
step:1481/2160 train_time:72663ms step_avg:49.06ms
step:1482/2160 train_time:72750ms step_avg:49.09ms
step:1483/2160 train_time:72839ms step_avg:49.12ms
step:1484/2160 train_time:72927ms step_avg:49.14ms
step:1485/2160 train_time:73016ms step_avg:49.17ms
step:1486/2160 train_time:73103ms step_avg:49.19ms
step:1487/2160 train_time:73192ms step_avg:49.22ms
step:1488/2160 train_time:73280ms step_avg:49.25ms
step:1489/2160 train_time:73369ms step_avg:49.27ms
step:1490/2160 train_time:73457ms step_avg:49.30ms
step:1491/2160 train_time:73546ms step_avg:49.33ms
step:1492/2160 train_time:73633ms step_avg:49.35ms
step:1493/2160 train_time:73722ms step_avg:49.38ms
step:1494/2160 train_time:73809ms step_avg:49.40ms
step:1495/2160 train_time:73898ms step_avg:49.43ms
step:1496/2160 train_time:73985ms step_avg:49.46ms
step:1497/2160 train_time:74075ms step_avg:49.48ms
step:1498/2160 train_time:74163ms step_avg:49.51ms
step:1499/2160 train_time:74252ms step_avg:49.53ms
step:1500/2160 train_time:74339ms step_avg:49.56ms
step:1500/2160 val_loss:3.4930 train_time:74429ms step_avg:49.62ms
step:1501/2160 train_time:74447ms step_avg:49.60ms
step:1502/2160 train_time:74518ms step_avg:49.61ms
step:1503/2160 train_time:74609ms step_avg:49.64ms
step:1504/2160 train_time:74699ms step_avg:49.67ms
step:1505/2160 train_time:74788ms step_avg:49.69ms
step:1506/2160 train_time:74874ms step_avg:49.72ms
step:1507/2160 train_time:74962ms step_avg:49.74ms
step:1508/2160 train_time:75048ms step_avg:49.77ms
step:1509/2160 train_time:75136ms step_avg:49.79ms
step:1510/2160 train_time:75224ms step_avg:49.82ms
step:1511/2160 train_time:75314ms step_avg:49.84ms
step:1512/2160 train_time:75405ms step_avg:49.87ms
step:1513/2160 train_time:75495ms step_avg:49.90ms
step:1514/2160 train_time:75584ms step_avg:49.92ms
step:1515/2160 train_time:75673ms step_avg:49.95ms
step:1516/2160 train_time:75760ms step_avg:49.97ms
step:1517/2160 train_time:75849ms step_avg:50.00ms
step:1518/2160 train_time:75936ms step_avg:50.02ms
step:1519/2160 train_time:76024ms step_avg:50.05ms
step:1520/2160 train_time:76110ms step_avg:50.07ms
step:1521/2160 train_time:76200ms step_avg:50.10ms
step:1522/2160 train_time:76288ms step_avg:50.12ms
step:1523/2160 train_time:76379ms step_avg:50.15ms
step:1524/2160 train_time:76467ms step_avg:50.18ms
step:1525/2160 train_time:76557ms step_avg:50.20ms
step:1526/2160 train_time:76644ms step_avg:50.23ms
step:1527/2160 train_time:76733ms step_avg:50.25ms
step:1528/2160 train_time:76820ms step_avg:50.27ms
step:1529/2160 train_time:76909ms step_avg:50.30ms
step:1530/2160 train_time:76996ms step_avg:50.32ms
step:1531/2160 train_time:77085ms step_avg:50.35ms
step:1532/2160 train_time:77171ms step_avg:50.37ms
step:1533/2160 train_time:77260ms step_avg:50.40ms
step:1534/2160 train_time:77349ms step_avg:50.42ms
step:1535/2160 train_time:77438ms step_avg:50.45ms
step:1536/2160 train_time:77528ms step_avg:50.47ms
step:1537/2160 train_time:77617ms step_avg:50.50ms
step:1538/2160 train_time:77705ms step_avg:50.52ms
step:1539/2160 train_time:77794ms step_avg:50.55ms
step:1540/2160 train_time:77881ms step_avg:50.57ms
step:1541/2160 train_time:77970ms step_avg:50.60ms
step:1542/2160 train_time:78057ms step_avg:50.62ms
step:1543/2160 train_time:78146ms step_avg:50.65ms
step:1544/2160 train_time:78233ms step_avg:50.67ms
step:1545/2160 train_time:78323ms step_avg:50.69ms
step:1546/2160 train_time:78412ms step_avg:50.72ms
step:1547/2160 train_time:78503ms step_avg:50.75ms
step:1548/2160 train_time:78591ms step_avg:50.77ms
step:1549/2160 train_time:78681ms step_avg:50.79ms
step:1550/2160 train_time:78768ms step_avg:50.82ms
step:1551/2160 train_time:78857ms step_avg:50.84ms
step:1552/2160 train_time:78944ms step_avg:50.87ms
step:1553/2160 train_time:79032ms step_avg:50.89ms
step:1554/2160 train_time:79119ms step_avg:50.91ms
step:1555/2160 train_time:79208ms step_avg:50.94ms
step:1556/2160 train_time:79295ms step_avg:50.96ms
step:1557/2160 train_time:79385ms step_avg:50.99ms
step:1558/2160 train_time:79473ms step_avg:51.01ms
step:1559/2160 train_time:79563ms step_avg:51.03ms
step:1560/2160 train_time:79651ms step_avg:51.06ms
step:1561/2160 train_time:79741ms step_avg:51.08ms
step:1562/2160 train_time:79829ms step_avg:51.11ms
step:1563/2160 train_time:79917ms step_avg:51.13ms
step:1564/2160 train_time:80004ms step_avg:51.15ms
step:1565/2160 train_time:80093ms step_avg:51.18ms
step:1566/2160 train_time:80181ms step_avg:51.20ms
step:1567/2160 train_time:80270ms step_avg:51.23ms
step:1568/2160 train_time:80357ms step_avg:51.25ms
step:1569/2160 train_time:80447ms step_avg:51.27ms
step:1570/2160 train_time:80535ms step_avg:51.30ms
step:1571/2160 train_time:80625ms step_avg:51.32ms
step:1572/2160 train_time:80713ms step_avg:51.34ms
step:1573/2160 train_time:80802ms step_avg:51.37ms
step:1574/2160 train_time:80891ms step_avg:51.39ms
step:1575/2160 train_time:80979ms step_avg:51.42ms
step:1576/2160 train_time:81066ms step_avg:51.44ms
step:1577/2160 train_time:81155ms step_avg:51.46ms
step:1578/2160 train_time:81242ms step_avg:51.48ms
step:1579/2160 train_time:81332ms step_avg:51.51ms
step:1580/2160 train_time:81420ms step_avg:51.53ms
step:1581/2160 train_time:81510ms step_avg:51.56ms
step:1582/2160 train_time:81598ms step_avg:51.58ms
step:1583/2160 train_time:81688ms step_avg:51.60ms
step:1584/2160 train_time:81776ms step_avg:51.63ms
step:1585/2160 train_time:81865ms step_avg:51.65ms
step:1586/2160 train_time:81952ms step_avg:51.67ms
step:1587/2160 train_time:82041ms step_avg:51.70ms
step:1588/2160 train_time:82129ms step_avg:51.72ms
step:1589/2160 train_time:82218ms step_avg:51.74ms
step:1590/2160 train_time:82305ms step_avg:51.76ms
step:1591/2160 train_time:82395ms step_avg:51.79ms
step:1592/2160 train_time:82483ms step_avg:51.81ms
step:1593/2160 train_time:82573ms step_avg:51.84ms
step:1594/2160 train_time:82661ms step_avg:51.86ms
step:1595/2160 train_time:82750ms step_avg:51.88ms
step:1596/2160 train_time:82837ms step_avg:51.90ms
step:1597/2160 train_time:82926ms step_avg:51.93ms
step:1598/2160 train_time:83013ms step_avg:51.95ms
step:1599/2160 train_time:83103ms step_avg:51.97ms
step:1600/2160 train_time:83189ms step_avg:51.99ms
step:1601/2160 train_time:83278ms step_avg:52.02ms
step:1602/2160 train_time:83365ms step_avg:52.04ms
step:1603/2160 train_time:83455ms step_avg:52.06ms
step:1604/2160 train_time:83543ms step_avg:52.08ms
step:1605/2160 train_time:83633ms step_avg:52.11ms
step:1606/2160 train_time:83720ms step_avg:52.13ms
step:1607/2160 train_time:83810ms step_avg:52.15ms
step:1608/2160 train_time:83897ms step_avg:52.17ms
step:1609/2160 train_time:83986ms step_avg:52.20ms
step:1610/2160 train_time:84073ms step_avg:52.22ms
step:1611/2160 train_time:84163ms step_avg:52.24ms
step:1612/2160 train_time:84250ms step_avg:52.26ms
step:1613/2160 train_time:84340ms step_avg:52.29ms
step:1614/2160 train_time:84427ms step_avg:52.31ms
step:1615/2160 train_time:84516ms step_avg:52.33ms
step:1616/2160 train_time:84604ms step_avg:52.35ms
step:1617/2160 train_time:84694ms step_avg:52.38ms
step:1618/2160 train_time:84781ms step_avg:52.40ms
step:1619/2160 train_time:84870ms step_avg:52.42ms
step:1620/2160 train_time:84959ms step_avg:52.44ms
step:1621/2160 train_time:85048ms step_avg:52.47ms
step:1622/2160 train_time:85134ms step_avg:52.49ms
step:1623/2160 train_time:85224ms step_avg:52.51ms
step:1624/2160 train_time:85311ms step_avg:52.53ms
step:1625/2160 train_time:85401ms step_avg:52.55ms
step:1626/2160 train_time:85489ms step_avg:52.58ms
step:1627/2160 train_time:85579ms step_avg:52.60ms
step:1628/2160 train_time:85667ms step_avg:52.62ms
step:1629/2160 train_time:85756ms step_avg:52.64ms
step:1630/2160 train_time:85843ms step_avg:52.66ms
step:1631/2160 train_time:85934ms step_avg:52.69ms
step:1632/2160 train_time:86021ms step_avg:52.71ms
step:1633/2160 train_time:86111ms step_avg:52.73ms
step:1634/2160 train_time:86198ms step_avg:52.75ms
step:1635/2160 train_time:86287ms step_avg:52.77ms
step:1636/2160 train_time:86374ms step_avg:52.80ms
step:1637/2160 train_time:86464ms step_avg:52.82ms
step:1638/2160 train_time:86552ms step_avg:52.84ms
step:1639/2160 train_time:86642ms step_avg:52.86ms
step:1640/2160 train_time:86730ms step_avg:52.88ms
step:1641/2160 train_time:86819ms step_avg:52.91ms
step:1642/2160 train_time:86907ms step_avg:52.93ms
step:1643/2160 train_time:86996ms step_avg:52.95ms
step:1644/2160 train_time:87083ms step_avg:52.97ms
step:1645/2160 train_time:87173ms step_avg:52.99ms
step:1646/2160 train_time:87260ms step_avg:53.01ms
step:1647/2160 train_time:87349ms step_avg:53.04ms
step:1648/2160 train_time:87437ms step_avg:53.06ms
step:1649/2160 train_time:87526ms step_avg:53.08ms
step:1650/2160 train_time:87614ms step_avg:53.10ms
step:1651/2160 train_time:87704ms step_avg:53.12ms
step:1652/2160 train_time:87791ms step_avg:53.14ms
step:1653/2160 train_time:87880ms step_avg:53.16ms
step:1654/2160 train_time:87967ms step_avg:53.18ms
step:1655/2160 train_time:88056ms step_avg:53.21ms
step:1656/2160 train_time:88143ms step_avg:53.23ms
step:1657/2160 train_time:88233ms step_avg:53.25ms
step:1658/2160 train_time:88321ms step_avg:53.27ms
step:1659/2160 train_time:88410ms step_avg:53.29ms
step:1660/2160 train_time:88498ms step_avg:53.31ms
step:1661/2160 train_time:88588ms step_avg:53.33ms
step:1662/2160 train_time:88675ms step_avg:53.35ms
step:1663/2160 train_time:88764ms step_avg:53.38ms
step:1664/2160 train_time:88851ms step_avg:53.40ms
step:1665/2160 train_time:88941ms step_avg:53.42ms
step:1666/2160 train_time:89029ms step_avg:53.44ms
step:1667/2160 train_time:89118ms step_avg:53.46ms
step:1668/2160 train_time:89205ms step_avg:53.48ms
step:1669/2160 train_time:89295ms step_avg:53.50ms
step:1670/2160 train_time:89382ms step_avg:53.52ms
step:1671/2160 train_time:89472ms step_avg:53.54ms
step:1672/2160 train_time:89559ms step_avg:53.56ms
step:1673/2160 train_time:89648ms step_avg:53.59ms
step:1674/2160 train_time:89735ms step_avg:53.61ms
step:1675/2160 train_time:89824ms step_avg:53.63ms
step:1676/2160 train_time:89911ms step_avg:53.65ms
step:1677/2160 train_time:90000ms step_avg:53.67ms
step:1678/2160 train_time:90089ms step_avg:53.69ms
step:1679/2160 train_time:90177ms step_avg:53.71ms
step:1680/2160 train_time:90265ms step_avg:53.73ms
step:1681/2160 train_time:90354ms step_avg:53.75ms
step:1682/2160 train_time:90442ms step_avg:53.77ms
step:1683/2160 train_time:90532ms step_avg:53.79ms
step:1684/2160 train_time:90618ms step_avg:53.81ms
step:1685/2160 train_time:90709ms step_avg:53.83ms
step:1686/2160 train_time:90796ms step_avg:53.85ms
step:1687/2160 train_time:90886ms step_avg:53.87ms
step:1688/2160 train_time:90973ms step_avg:53.89ms
step:1689/2160 train_time:91061ms step_avg:53.91ms
step:1690/2160 train_time:91149ms step_avg:53.93ms
step:1691/2160 train_time:91239ms step_avg:53.96ms
step:1692/2160 train_time:91326ms step_avg:53.98ms
step:1693/2160 train_time:91415ms step_avg:54.00ms
step:1694/2160 train_time:91504ms step_avg:54.02ms
step:1695/2160 train_time:91592ms step_avg:54.04ms
step:1696/2160 train_time:91681ms step_avg:54.06ms
step:1697/2160 train_time:91771ms step_avg:54.08ms
step:1698/2160 train_time:91858ms step_avg:54.10ms
step:1699/2160 train_time:91948ms step_avg:54.12ms
step:1700/2160 train_time:92036ms step_avg:54.14ms
step:1701/2160 train_time:92126ms step_avg:54.16ms
step:1702/2160 train_time:92214ms step_avg:54.18ms
step:1703/2160 train_time:92303ms step_avg:54.20ms
step:1704/2160 train_time:92391ms step_avg:54.22ms
step:1705/2160 train_time:92480ms step_avg:54.24ms
step:1706/2160 train_time:92568ms step_avg:54.26ms
step:1707/2160 train_time:92656ms step_avg:54.28ms
step:1708/2160 train_time:92745ms step_avg:54.30ms
step:1709/2160 train_time:92834ms step_avg:54.32ms
step:1710/2160 train_time:92921ms step_avg:54.34ms
step:1711/2160 train_time:93012ms step_avg:54.36ms
step:1712/2160 train_time:93099ms step_avg:54.38ms
step:1713/2160 train_time:93189ms step_avg:54.40ms
step:1714/2160 train_time:93276ms step_avg:54.42ms
step:1715/2160 train_time:93365ms step_avg:54.44ms
step:1716/2160 train_time:93452ms step_avg:54.46ms
step:1717/2160 train_time:93542ms step_avg:54.48ms
step:1718/2160 train_time:93630ms step_avg:54.50ms
step:1719/2160 train_time:93720ms step_avg:54.52ms
step:1720/2160 train_time:93809ms step_avg:54.54ms
step:1721/2160 train_time:93897ms step_avg:54.56ms
step:1722/2160 train_time:93985ms step_avg:54.58ms
step:1723/2160 train_time:94074ms step_avg:54.60ms
step:1724/2160 train_time:94162ms step_avg:54.62ms
step:1725/2160 train_time:94253ms step_avg:54.64ms
step:1726/2160 train_time:94341ms step_avg:54.66ms
step:1727/2160 train_time:94431ms step_avg:54.68ms
step:1728/2160 train_time:94518ms step_avg:54.70ms
step:1729/2160 train_time:94608ms step_avg:54.72ms
step:1730/2160 train_time:94695ms step_avg:54.74ms
step:1731/2160 train_time:94784ms step_avg:54.76ms
step:1732/2160 train_time:94872ms step_avg:54.78ms
step:1733/2160 train_time:94961ms step_avg:54.80ms
step:1734/2160 train_time:95049ms step_avg:54.81ms
step:1735/2160 train_time:95138ms step_avg:54.83ms
step:1736/2160 train_time:95226ms step_avg:54.85ms
step:1737/2160 train_time:95315ms step_avg:54.87ms
step:1738/2160 train_time:95403ms step_avg:54.89ms
step:1739/2160 train_time:95492ms step_avg:54.91ms
step:1740/2160 train_time:95581ms step_avg:54.93ms
step:1741/2160 train_time:95672ms step_avg:54.95ms
step:1742/2160 train_time:95759ms step_avg:54.97ms
step:1743/2160 train_time:95848ms step_avg:54.99ms
step:1744/2160 train_time:95935ms step_avg:55.01ms
step:1745/2160 train_time:96024ms step_avg:55.03ms
step:1746/2160 train_time:96111ms step_avg:55.05ms
step:1747/2160 train_time:96201ms step_avg:55.07ms
step:1748/2160 train_time:96289ms step_avg:55.09ms
step:1749/2160 train_time:96377ms step_avg:55.10ms
step:1750/2160 train_time:96464ms step_avg:55.12ms
step:1750/2160 val_loss:3.3922 train_time:96554ms step_avg:55.17ms
step:1751/2160 train_time:96574ms step_avg:55.15ms
step:1752/2160 train_time:96646ms step_avg:55.16ms
step:1753/2160 train_time:96739ms step_avg:55.18ms
step:1754/2160 train_time:96826ms step_avg:55.20ms
step:1755/2160 train_time:96915ms step_avg:55.22ms
step:1756/2160 train_time:97002ms step_avg:55.24ms
step:1757/2160 train_time:97090ms step_avg:55.26ms
step:1758/2160 train_time:97176ms step_avg:55.28ms
step:1759/2160 train_time:97264ms step_avg:55.29ms
step:1760/2160 train_time:97351ms step_avg:55.31ms
step:1761/2160 train_time:97439ms step_avg:55.33ms
step:1762/2160 train_time:97528ms step_avg:55.35ms
step:1763/2160 train_time:97621ms step_avg:55.37ms
step:1764/2160 train_time:97712ms step_avg:55.39ms
step:1765/2160 train_time:97801ms step_avg:55.41ms
step:1766/2160 train_time:97889ms step_avg:55.43ms
step:1767/2160 train_time:97979ms step_avg:55.45ms
step:1768/2160 train_time:98065ms step_avg:55.47ms
step:1769/2160 train_time:98153ms step_avg:55.49ms
step:1770/2160 train_time:98239ms step_avg:55.50ms
step:1771/2160 train_time:98328ms step_avg:55.52ms
step:1772/2160 train_time:98416ms step_avg:55.54ms
step:1773/2160 train_time:98505ms step_avg:55.56ms
step:1774/2160 train_time:98594ms step_avg:55.58ms
step:1775/2160 train_time:98684ms step_avg:55.60ms
step:1776/2160 train_time:98773ms step_avg:55.62ms
step:1777/2160 train_time:98864ms step_avg:55.64ms
step:1778/2160 train_time:98951ms step_avg:55.65ms
step:1779/2160 train_time:99040ms step_avg:55.67ms
step:1780/2160 train_time:99126ms step_avg:55.69ms
step:1781/2160 train_time:99215ms step_avg:55.71ms
step:1782/2160 train_time:99301ms step_avg:55.72ms
step:1783/2160 train_time:99390ms step_avg:55.74ms
step:1784/2160 train_time:99477ms step_avg:55.76ms
step:1785/2160 train_time:99567ms step_avg:55.78ms
step:1786/2160 train_time:99655ms step_avg:55.80ms
step:1787/2160 train_time:99746ms step_avg:55.82ms
step:1788/2160 train_time:99834ms step_avg:55.84ms
step:1789/2160 train_time:99925ms step_avg:55.86ms
step:1790/2160 train_time:100013ms step_avg:55.87ms
step:1791/2160 train_time:100103ms step_avg:55.89ms
step:1792/2160 train_time:100189ms step_avg:55.91ms
step:1793/2160 train_time:100278ms step_avg:55.93ms
step:1794/2160 train_time:100365ms step_avg:55.94ms
step:1795/2160 train_time:100454ms step_avg:55.96ms
step:1796/2160 train_time:100542ms step_avg:55.98ms
step:1797/2160 train_time:100632ms step_avg:56.00ms
step:1798/2160 train_time:100720ms step_avg:56.02ms
step:1799/2160 train_time:100809ms step_avg:56.04ms
step:1800/2160 train_time:100897ms step_avg:56.05ms
step:1801/2160 train_time:100987ms step_avg:56.07ms
step:1802/2160 train_time:101075ms step_avg:56.09ms
step:1803/2160 train_time:101164ms step_avg:56.11ms
step:1804/2160 train_time:101251ms step_avg:56.13ms
step:1805/2160 train_time:101340ms step_avg:56.14ms
step:1806/2160 train_time:101427ms step_avg:56.16ms
step:1807/2160 train_time:101515ms step_avg:56.18ms
step:1808/2160 train_time:101603ms step_avg:56.20ms
step:1809/2160 train_time:101693ms step_avg:56.22ms
step:1810/2160 train_time:101781ms step_avg:56.23ms
step:1811/2160 train_time:101869ms step_avg:56.25ms
step:1812/2160 train_time:101957ms step_avg:56.27ms
step:1813/2160 train_time:102046ms step_avg:56.29ms
step:1814/2160 train_time:102134ms step_avg:56.30ms
step:1815/2160 train_time:102223ms step_avg:56.32ms
step:1816/2160 train_time:102310ms step_avg:56.34ms
step:1817/2160 train_time:102400ms step_avg:56.36ms
step:1818/2160 train_time:102487ms step_avg:56.37ms
step:1819/2160 train_time:102577ms step_avg:56.39ms
step:1820/2160 train_time:102664ms step_avg:56.41ms
step:1821/2160 train_time:102754ms step_avg:56.43ms
step:1822/2160 train_time:102842ms step_avg:56.44ms
step:1823/2160 train_time:102931ms step_avg:56.46ms
step:1824/2160 train_time:103018ms step_avg:56.48ms
step:1825/2160 train_time:103107ms step_avg:56.50ms
step:1826/2160 train_time:103193ms step_avg:56.51ms
step:1827/2160 train_time:103283ms step_avg:56.53ms
step:1828/2160 train_time:103370ms step_avg:56.55ms
step:1829/2160 train_time:103459ms step_avg:56.57ms
step:1830/2160 train_time:103546ms step_avg:56.58ms
step:1831/2160 train_time:103636ms step_avg:56.60ms
step:1832/2160 train_time:103724ms step_avg:56.62ms
step:1833/2160 train_time:103814ms step_avg:56.64ms
step:1834/2160 train_time:103901ms step_avg:56.65ms
step:1835/2160 train_time:103990ms step_avg:56.67ms
step:1836/2160 train_time:104077ms step_avg:56.69ms
step:1837/2160 train_time:104166ms step_avg:56.70ms
step:1838/2160 train_time:104254ms step_avg:56.72ms
step:1839/2160 train_time:104343ms step_avg:56.74ms
step:1840/2160 train_time:104431ms step_avg:56.76ms
step:1841/2160 train_time:104521ms step_avg:56.77ms
step:1842/2160 train_time:104608ms step_avg:56.79ms
step:1843/2160 train_time:104698ms step_avg:56.81ms
step:1844/2160 train_time:104786ms step_avg:56.83ms
step:1845/2160 train_time:104876ms step_avg:56.84ms
step:1846/2160 train_time:104963ms step_avg:56.86ms
step:1847/2160 train_time:105052ms step_avg:56.88ms
step:1848/2160 train_time:105140ms step_avg:56.89ms
step:1849/2160 train_time:105229ms step_avg:56.91ms
step:1850/2160 train_time:105317ms step_avg:56.93ms
step:1851/2160 train_time:105406ms step_avg:56.95ms
step:1852/2160 train_time:105493ms step_avg:56.96ms
step:1853/2160 train_time:105583ms step_avg:56.98ms
step:1854/2160 train_time:105669ms step_avg:57.00ms
step:1855/2160 train_time:105759ms step_avg:57.01ms
step:1856/2160 train_time:105846ms step_avg:57.03ms
step:1857/2160 train_time:105936ms step_avg:57.05ms
step:1858/2160 train_time:106025ms step_avg:57.06ms
step:1859/2160 train_time:106115ms step_avg:57.08ms
step:1860/2160 train_time:106202ms step_avg:57.10ms
step:1861/2160 train_time:106291ms step_avg:57.12ms
step:1862/2160 train_time:106380ms step_avg:57.13ms
step:1863/2160 train_time:106469ms step_avg:57.15ms
step:1864/2160 train_time:106557ms step_avg:57.17ms
step:1865/2160 train_time:106646ms step_avg:57.18ms
step:1866/2160 train_time:106734ms step_avg:57.20ms
step:1867/2160 train_time:106824ms step_avg:57.22ms
step:1868/2160 train_time:106912ms step_avg:57.23ms
step:1869/2160 train_time:107002ms step_avg:57.25ms
step:1870/2160 train_time:107090ms step_avg:57.27ms
step:1871/2160 train_time:107180ms step_avg:57.28ms
step:1872/2160 train_time:107267ms step_avg:57.30ms
step:1873/2160 train_time:107357ms step_avg:57.32ms
step:1874/2160 train_time:107445ms step_avg:57.33ms
step:1875/2160 train_time:107534ms step_avg:57.35ms
step:1876/2160 train_time:107621ms step_avg:57.37ms
step:1877/2160 train_time:107710ms step_avg:57.38ms
step:1878/2160 train_time:107798ms step_avg:57.40ms
step:1879/2160 train_time:107886ms step_avg:57.42ms
step:1880/2160 train_time:107974ms step_avg:57.43ms
step:1881/2160 train_time:108063ms step_avg:57.45ms
step:1882/2160 train_time:108150ms step_avg:57.47ms
step:1883/2160 train_time:108240ms step_avg:57.48ms
step:1884/2160 train_time:108327ms step_avg:57.50ms
step:1885/2160 train_time:108416ms step_avg:57.52ms
step:1886/2160 train_time:108504ms step_avg:57.53ms
step:1887/2160 train_time:108594ms step_avg:57.55ms
step:1888/2160 train_time:108682ms step_avg:57.56ms
step:1889/2160 train_time:108772ms step_avg:57.58ms
step:1890/2160 train_time:108861ms step_avg:57.60ms
step:1891/2160 train_time:108949ms step_avg:57.61ms
step:1892/2160 train_time:109038ms step_avg:57.63ms
step:1893/2160 train_time:109127ms step_avg:57.65ms
step:1894/2160 train_time:109215ms step_avg:57.66ms
step:1895/2160 train_time:109304ms step_avg:57.68ms
step:1896/2160 train_time:109391ms step_avg:57.70ms
step:1897/2160 train_time:109481ms step_avg:57.71ms
step:1898/2160 train_time:109568ms step_avg:57.73ms
step:1899/2160 train_time:109658ms step_avg:57.74ms
step:1900/2160 train_time:109745ms step_avg:57.76ms
step:1901/2160 train_time:109834ms step_avg:57.78ms
step:1902/2160 train_time:109922ms step_avg:57.79ms
step:1903/2160 train_time:110011ms step_avg:57.81ms
step:1904/2160 train_time:110099ms step_avg:57.83ms
step:1905/2160 train_time:110188ms step_avg:57.84ms
step:1906/2160 train_time:110276ms step_avg:57.86ms
step:1907/2160 train_time:110365ms step_avg:57.87ms
step:1908/2160 train_time:110452ms step_avg:57.89ms
step:1909/2160 train_time:110542ms step_avg:57.91ms
step:1910/2160 train_time:110629ms step_avg:57.92ms
step:1911/2160 train_time:110719ms step_avg:57.94ms
step:1912/2160 train_time:110806ms step_avg:57.95ms
step:1913/2160 train_time:110896ms step_avg:57.97ms
step:1914/2160 train_time:110983ms step_avg:57.98ms
step:1915/2160 train_time:111072ms step_avg:58.00ms
step:1916/2160 train_time:111159ms step_avg:58.02ms
step:1917/2160 train_time:111248ms step_avg:58.03ms
step:1918/2160 train_time:111336ms step_avg:58.05ms
step:1919/2160 train_time:111427ms step_avg:58.07ms
step:1920/2160 train_time:111515ms step_avg:58.08ms
step:1921/2160 train_time:111605ms step_avg:58.10ms
step:1922/2160 train_time:111692ms step_avg:58.11ms
step:1923/2160 train_time:111781ms step_avg:58.13ms
step:1924/2160 train_time:111869ms step_avg:58.14ms
step:1925/2160 train_time:111958ms step_avg:58.16ms
step:1926/2160 train_time:112045ms step_avg:58.18ms
step:1927/2160 train_time:112135ms step_avg:58.19ms
step:1928/2160 train_time:112222ms step_avg:58.21ms
step:1929/2160 train_time:112311ms step_avg:58.22ms
step:1930/2160 train_time:112399ms step_avg:58.24ms
step:1931/2160 train_time:112488ms step_avg:58.25ms
step:1932/2160 train_time:112576ms step_avg:58.27ms
step:1933/2160 train_time:112667ms step_avg:58.29ms
step:1934/2160 train_time:112756ms step_avg:58.30ms
step:1935/2160 train_time:112846ms step_avg:58.32ms
step:1936/2160 train_time:112933ms step_avg:58.33ms
step:1937/2160 train_time:113023ms step_avg:58.35ms
step:1938/2160 train_time:113110ms step_avg:58.36ms
step:1939/2160 train_time:113199ms step_avg:58.38ms
step:1940/2160 train_time:113286ms step_avg:58.39ms
step:1941/2160 train_time:113375ms step_avg:58.41ms
step:1942/2160 train_time:113463ms step_avg:58.43ms
step:1943/2160 train_time:113552ms step_avg:58.44ms
step:1944/2160 train_time:113641ms step_avg:58.46ms
step:1945/2160 train_time:113730ms step_avg:58.47ms
step:1946/2160 train_time:113818ms step_avg:58.49ms
step:1947/2160 train_time:113907ms step_avg:58.50ms
step:1948/2160 train_time:113994ms step_avg:58.52ms
step:1949/2160 train_time:114084ms step_avg:58.53ms
step:1950/2160 train_time:114172ms step_avg:58.55ms
step:1951/2160 train_time:114260ms step_avg:58.57ms
step:1952/2160 train_time:114349ms step_avg:58.58ms
step:1953/2160 train_time:114438ms step_avg:58.60ms
step:1954/2160 train_time:114525ms step_avg:58.61ms
step:1955/2160 train_time:114615ms step_avg:58.63ms
step:1956/2160 train_time:114702ms step_avg:58.64ms
step:1957/2160 train_time:114792ms step_avg:58.66ms
step:1958/2160 train_time:114880ms step_avg:58.67ms
step:1959/2160 train_time:114969ms step_avg:58.69ms
step:1960/2160 train_time:115057ms step_avg:58.70ms
step:1961/2160 train_time:115146ms step_avg:58.72ms
step:1962/2160 train_time:115233ms step_avg:58.73ms
step:1963/2160 train_time:115323ms step_avg:58.75ms
step:1964/2160 train_time:115410ms step_avg:58.76ms
step:1965/2160 train_time:115500ms step_avg:58.78ms
step:1966/2160 train_time:115587ms step_avg:58.79ms
step:1967/2160 train_time:115677ms step_avg:58.81ms
step:1968/2160 train_time:115765ms step_avg:58.82ms
step:1969/2160 train_time:115854ms step_avg:58.84ms
step:1970/2160 train_time:115942ms step_avg:58.85ms
step:1971/2160 train_time:116031ms step_avg:58.87ms
step:1972/2160 train_time:116118ms step_avg:58.88ms
step:1973/2160 train_time:116207ms step_avg:58.90ms
step:1974/2160 train_time:116294ms step_avg:58.91ms
step:1975/2160 train_time:116383ms step_avg:58.93ms
step:1976/2160 train_time:116470ms step_avg:58.94ms
step:1977/2160 train_time:116560ms step_avg:58.96ms
step:1978/2160 train_time:116647ms step_avg:58.97ms
step:1979/2160 train_time:116738ms step_avg:58.99ms
step:1980/2160 train_time:116825ms step_avg:59.00ms
step:1981/2160 train_time:116915ms step_avg:59.02ms
step:1982/2160 train_time:117002ms step_avg:59.03ms
step:1983/2160 train_time:117090ms step_avg:59.05ms
step:1984/2160 train_time:117178ms step_avg:59.06ms
step:1985/2160 train_time:117266ms step_avg:59.08ms
step:1986/2160 train_time:117354ms step_avg:59.09ms
step:1987/2160 train_time:117443ms step_avg:59.11ms
step:1988/2160 train_time:117531ms step_avg:59.12ms
step:1989/2160 train_time:117620ms step_avg:59.14ms
step:1990/2160 train_time:117708ms step_avg:59.15ms
step:1991/2160 train_time:117797ms step_avg:59.16ms
step:1992/2160 train_time:117884ms step_avg:59.18ms
step:1993/2160 train_time:117973ms step_avg:59.19ms
step:1994/2160 train_time:118061ms step_avg:59.21ms
step:1995/2160 train_time:118151ms step_avg:59.22ms
step:1996/2160 train_time:118238ms step_avg:59.24ms
step:1997/2160 train_time:118327ms step_avg:59.25ms
step:1998/2160 train_time:118415ms step_avg:59.27ms
step:1999/2160 train_time:118504ms step_avg:59.28ms
step:2000/2160 train_time:118592ms step_avg:59.30ms
step:2000/2160 val_loss:3.3128 train_time:118682ms step_avg:59.34ms
step:2001/2160 train_time:118701ms step_avg:59.32ms
step:2002/2160 train_time:118773ms step_avg:59.33ms
step:2003/2160 train_time:118865ms step_avg:59.34ms
step:2004/2160 train_time:118954ms step_avg:59.36ms
step:2005/2160 train_time:119042ms step_avg:59.37ms
step:2006/2160 train_time:119128ms step_avg:59.39ms
step:2007/2160 train_time:119216ms step_avg:59.40ms
step:2008/2160 train_time:119303ms step_avg:59.41ms
step:2009/2160 train_time:119390ms step_avg:59.43ms
step:2010/2160 train_time:119477ms step_avg:59.44ms
step:2011/2160 train_time:119566ms step_avg:59.46ms
step:2012/2160 train_time:119655ms step_avg:59.47ms
step:2013/2160 train_time:119748ms step_avg:59.49ms
step:2014/2160 train_time:119838ms step_avg:59.50ms
step:2015/2160 train_time:119929ms step_avg:59.52ms
step:2016/2160 train_time:120017ms step_avg:59.53ms
step:2017/2160 train_time:120106ms step_avg:59.55ms
step:2018/2160 train_time:120192ms step_avg:59.56ms
step:2019/2160 train_time:120281ms step_avg:59.57ms
step:2020/2160 train_time:120367ms step_avg:59.59ms
step:2021/2160 train_time:120455ms step_avg:59.60ms
step:2022/2160 train_time:120543ms step_avg:59.62ms
step:2023/2160 train_time:120633ms step_avg:59.63ms
step:2024/2160 train_time:120722ms step_avg:59.65ms
step:2025/2160 train_time:120813ms step_avg:59.66ms
step:2026/2160 train_time:120901ms step_avg:59.67ms
step:2027/2160 train_time:120992ms step_avg:59.69ms
step:2028/2160 train_time:121079ms step_avg:59.70ms
step:2029/2160 train_time:121168ms step_avg:59.72ms
step:2030/2160 train_time:121256ms step_avg:59.73ms
step:2031/2160 train_time:121344ms step_avg:59.75ms
step:2032/2160 train_time:121431ms step_avg:59.76ms
step:2033/2160 train_time:121520ms step_avg:59.77ms
step:2034/2160 train_time:121609ms step_avg:59.79ms
step:2035/2160 train_time:121699ms step_avg:59.80ms
step:2036/2160 train_time:121788ms step_avg:59.82ms
step:2037/2160 train_time:121879ms step_avg:59.83ms
step:2038/2160 train_time:121968ms step_avg:59.85ms
step:2039/2160 train_time:122058ms step_avg:59.86ms
step:2040/2160 train_time:122146ms step_avg:59.88ms
step:2041/2160 train_time:122235ms step_avg:59.89ms
step:2042/2160 train_time:122322ms step_avg:59.90ms
step:2043/2160 train_time:122410ms step_avg:59.92ms
step:2044/2160 train_time:122497ms step_avg:59.93ms
step:2045/2160 train_time:122587ms step_avg:59.94ms
step:2046/2160 train_time:122674ms step_avg:59.96ms
step:2047/2160 train_time:122765ms step_avg:59.97ms
step:2048/2160 train_time:122854ms step_avg:59.99ms
step:2049/2160 train_time:122944ms step_avg:60.00ms
step:2050/2160 train_time:123031ms step_avg:60.02ms
step:2051/2160 train_time:123120ms step_avg:60.03ms
step:2052/2160 train_time:123207ms step_avg:60.04ms
step:2053/2160 train_time:123297ms step_avg:60.06ms
step:2054/2160 train_time:123385ms step_avg:60.07ms
step:2055/2160 train_time:123474ms step_avg:60.08ms
step:2056/2160 train_time:123561ms step_avg:60.10ms
step:2057/2160 train_time:123650ms step_avg:60.11ms
step:2058/2160 train_time:123737ms step_avg:60.12ms
step:2059/2160 train_time:123826ms step_avg:60.14ms
step:2060/2160 train_time:123915ms step_avg:60.15ms
step:2061/2160 train_time:124004ms step_avg:60.17ms
step:2062/2160 train_time:124093ms step_avg:60.18ms
step:2063/2160 train_time:124181ms step_avg:60.19ms
step:2064/2160 train_time:124269ms step_avg:60.21ms
step:2065/2160 train_time:124358ms step_avg:60.22ms
step:2066/2160 train_time:124445ms step_avg:60.23ms
step:2067/2160 train_time:124533ms step_avg:60.25ms
step:2068/2160 train_time:124620ms step_avg:60.26ms
step:2069/2160 train_time:124710ms step_avg:60.28ms
step:2070/2160 train_time:124798ms step_avg:60.29ms
step:2071/2160 train_time:124889ms step_avg:60.30ms
step:2072/2160 train_time:124977ms step_avg:60.32ms
step:2073/2160 train_time:125066ms step_avg:60.33ms
step:2074/2160 train_time:125155ms step_avg:60.34ms
step:2075/2160 train_time:125244ms step_avg:60.36ms
step:2076/2160 train_time:125331ms step_avg:60.37ms
step:2077/2160 train_time:125420ms step_avg:60.39ms
step:2078/2160 train_time:125507ms step_avg:60.40ms
step:2079/2160 train_time:125596ms step_avg:60.41ms
step:2080/2160 train_time:125684ms step_avg:60.43ms
step:2081/2160 train_time:125775ms step_avg:60.44ms
step:2082/2160 train_time:125863ms step_avg:60.45ms
step:2083/2160 train_time:125951ms step_avg:60.47ms
step:2084/2160 train_time:126040ms step_avg:60.48ms
step:2085/2160 train_time:126129ms step_avg:60.49ms
step:2086/2160 train_time:126217ms step_avg:60.51ms
step:2087/2160 train_time:126306ms step_avg:60.52ms
step:2088/2160 train_time:126393ms step_avg:60.53ms
step:2089/2160 train_time:126482ms step_avg:60.55ms
step:2090/2160 train_time:126570ms step_avg:60.56ms
step:2091/2160 train_time:126659ms step_avg:60.57ms
step:2092/2160 train_time:126747ms step_avg:60.59ms
step:2093/2160 train_time:126836ms step_avg:60.60ms
step:2094/2160 train_time:126925ms step_avg:60.61ms
step:2095/2160 train_time:127015ms step_avg:60.63ms
step:2096/2160 train_time:127103ms step_avg:60.64ms
step:2097/2160 train_time:127192ms step_avg:60.65ms
step:2098/2160 train_time:127279ms step_avg:60.67ms
step:2099/2160 train_time:127370ms step_avg:60.68ms
step:2100/2160 train_time:127456ms step_avg:60.69ms
step:2101/2160 train_time:127545ms step_avg:60.71ms
step:2102/2160 train_time:127633ms step_avg:60.72ms
step:2103/2160 train_time:127722ms step_avg:60.73ms
step:2104/2160 train_time:127811ms step_avg:60.75ms
step:2105/2160 train_time:127899ms step_avg:60.76ms
step:2106/2160 train_time:127987ms step_avg:60.77ms
step:2107/2160 train_time:128076ms step_avg:60.79ms
step:2108/2160 train_time:128164ms step_avg:60.80ms
step:2109/2160 train_time:128253ms step_avg:60.81ms
step:2110/2160 train_time:128340ms step_avg:60.82ms
step:2111/2160 train_time:128430ms step_avg:60.84ms
step:2112/2160 train_time:128517ms step_avg:60.85ms
step:2113/2160 train_time:128607ms step_avg:60.86ms
step:2114/2160 train_time:128694ms step_avg:60.88ms
step:2115/2160 train_time:128784ms step_avg:60.89ms
step:2116/2160 train_time:128872ms step_avg:60.90ms
step:2117/2160 train_time:128961ms step_avg:60.92ms
step:2118/2160 train_time:129049ms step_avg:60.93ms
step:2119/2160 train_time:129138ms step_avg:60.94ms
step:2120/2160 train_time:129226ms step_avg:60.96ms
step:2121/2160 train_time:129315ms step_avg:60.97ms
step:2122/2160 train_time:129404ms step_avg:60.98ms
step:2123/2160 train_time:129495ms step_avg:61.00ms
step:2124/2160 train_time:129583ms step_avg:61.01ms
step:2125/2160 train_time:129673ms step_avg:61.02ms
step:2126/2160 train_time:129760ms step_avg:61.03ms
step:2127/2160 train_time:129850ms step_avg:61.05ms
step:2128/2160 train_time:129938ms step_avg:61.06ms
step:2129/2160 train_time:130028ms step_avg:61.07ms
step:2130/2160 train_time:130115ms step_avg:61.09ms
step:2131/2160 train_time:130204ms step_avg:61.10ms
step:2132/2160 train_time:130291ms step_avg:61.11ms
step:2133/2160 train_time:130380ms step_avg:61.13ms
step:2134/2160 train_time:130468ms step_avg:61.14ms
step:2135/2160 train_time:130558ms step_avg:61.15ms
step:2136/2160 train_time:130646ms step_avg:61.16ms
step:2137/2160 train_time:130735ms step_avg:61.18ms
step:2138/2160 train_time:130823ms step_avg:61.19ms
step:2139/2160 train_time:130913ms step_avg:61.20ms
step:2140/2160 train_time:131002ms step_avg:61.22ms
step:2141/2160 train_time:131092ms step_avg:61.23ms
step:2142/2160 train_time:131179ms step_avg:61.24ms
step:2143/2160 train_time:131270ms step_avg:61.26ms
step:2144/2160 train_time:131358ms step_avg:61.27ms
step:2145/2160 train_time:131447ms step_avg:61.28ms
step:2146/2160 train_time:131535ms step_avg:61.29ms
step:2147/2160 train_time:131625ms step_avg:61.31ms
step:2148/2160 train_time:131713ms step_avg:61.32ms
step:2149/2160 train_time:131803ms step_avg:61.33ms
step:2150/2160 train_time:131892ms step_avg:61.34ms
step:2151/2160 train_time:131981ms step_avg:61.36ms
step:2152/2160 train_time:132069ms step_avg:61.37ms
step:2153/2160 train_time:132159ms step_avg:61.38ms
step:2154/2160 train_time:132247ms step_avg:61.40ms
step:2155/2160 train_time:132337ms step_avg:61.41ms
step:2156/2160 train_time:132424ms step_avg:61.42ms
step:2157/2160 train_time:132515ms step_avg:61.43ms
step:2158/2160 train_time:132602ms step_avg:61.45ms
step:2159/2160 train_time:132691ms step_avg:61.46ms
step:2160/2160 train_time:132779ms step_avg:61.47ms
step:2160/2160 val_loss:3.2772 train_time:132869ms step_avg:61.51ms
peak memory allocated: 29896 MiB reserved: 61784 MiB

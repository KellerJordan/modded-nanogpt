import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = (16 / 8) * 0.8
    if x > 0.66:
        lr_max = (24 / 8) * 0.8
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sun Nov 30 02:05:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   31C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2160 train_time:97ms step_avg:97.14ms
step:2/2160 train_time:137ms step_avg:68.37ms
step:3/2160 train_time:156ms step_avg:52.16ms
step:4/2160 train_time:177ms step_avg:44.24ms
step:5/2160 train_time:205ms step_avg:41.00ms
step:6/2160 train_time:294ms step_avg:49.05ms
step:7/2160 train_time:335ms step_avg:47.86ms
step:8/2160 train_time:368ms step_avg:45.97ms
step:9/2160 train_time:401ms step_avg:44.60ms
step:10/2160 train_time:434ms step_avg:43.42ms
step:11/2160 train_time:468ms step_avg:42.58ms
step:12/2160 train_time:501ms step_avg:41.78ms
step:13/2160 train_time:535ms step_avg:41.17ms
step:14/2160 train_time:568ms step_avg:40.58ms
step:15/2160 train_time:602ms step_avg:40.15ms
step:16/2160 train_time:635ms step_avg:39.70ms
step:17/2160 train_time:669ms step_avg:39.38ms
step:18/2160 train_time:702ms step_avg:39.02ms
step:19/2160 train_time:736ms step_avg:38.76ms
step:20/2160 train_time:769ms step_avg:38.47ms
step:21/2160 train_time:804ms step_avg:38.27ms
step:22/2160 train_time:836ms step_avg:38.02ms
step:23/2160 train_time:871ms step_avg:37.86ms
step:24/2160 train_time:904ms step_avg:37.66ms
step:25/2160 train_time:938ms step_avg:37.52ms
step:26/2160 train_time:971ms step_avg:37.34ms
step:27/2160 train_time:1005ms step_avg:37.22ms
step:28/2160 train_time:1038ms step_avg:37.07ms
step:29/2160 train_time:1072ms step_avg:36.97ms
step:30/2160 train_time:1105ms step_avg:36.84ms
step:31/2160 train_time:1139ms step_avg:36.74ms
step:32/2160 train_time:1172ms step_avg:36.63ms
step:33/2160 train_time:1207ms step_avg:36.57ms
step:34/2160 train_time:1240ms step_avg:36.46ms
step:35/2160 train_time:1276ms step_avg:36.45ms
step:36/2160 train_time:1309ms step_avg:36.36ms
step:37/2160 train_time:1344ms step_avg:36.32ms
step:38/2160 train_time:1377ms step_avg:36.23ms
step:39/2160 train_time:1411ms step_avg:36.19ms
step:40/2160 train_time:1444ms step_avg:36.11ms
step:41/2160 train_time:1479ms step_avg:36.06ms
step:42/2160 train_time:1512ms step_avg:35.99ms
step:43/2160 train_time:1546ms step_avg:35.95ms
step:44/2160 train_time:1579ms step_avg:35.88ms
step:45/2160 train_time:1614ms step_avg:35.86ms
step:46/2160 train_time:1646ms step_avg:35.79ms
step:47/2160 train_time:1681ms step_avg:35.76ms
step:48/2160 train_time:1714ms step_avg:35.70ms
step:49/2160 train_time:1748ms step_avg:35.68ms
step:50/2160 train_time:1781ms step_avg:35.61ms
step:51/2160 train_time:1816ms step_avg:35.60ms
step:52/2160 train_time:1848ms step_avg:35.55ms
step:53/2160 train_time:1883ms step_avg:35.52ms
step:54/2160 train_time:1916ms step_avg:35.48ms
step:55/2160 train_time:1950ms step_avg:35.45ms
step:56/2160 train_time:1983ms step_avg:35.41ms
step:57/2160 train_time:2017ms step_avg:35.38ms
step:58/2160 train_time:2051ms step_avg:35.35ms
step:59/2160 train_time:2084ms step_avg:35.33ms
step:60/2160 train_time:2117ms step_avg:35.29ms
step:61/2160 train_time:2152ms step_avg:35.28ms
step:62/2160 train_time:2185ms step_avg:35.25ms
step:63/2160 train_time:2219ms step_avg:35.23ms
step:64/2160 train_time:2253ms step_avg:35.20ms
step:65/2160 train_time:2287ms step_avg:35.18ms
step:66/2160 train_time:2319ms step_avg:35.14ms
step:67/2160 train_time:2354ms step_avg:35.14ms
step:68/2160 train_time:2388ms step_avg:35.11ms
step:69/2160 train_time:2421ms step_avg:35.09ms
step:70/2160 train_time:2454ms step_avg:35.06ms
step:71/2160 train_time:2489ms step_avg:35.06ms
step:72/2160 train_time:2522ms step_avg:35.03ms
step:73/2160 train_time:2557ms step_avg:35.02ms
step:74/2160 train_time:2590ms step_avg:34.99ms
step:75/2160 train_time:2624ms step_avg:34.98ms
step:76/2160 train_time:2657ms step_avg:34.96ms
step:77/2160 train_time:2691ms step_avg:34.95ms
step:78/2160 train_time:2724ms step_avg:34.92ms
step:79/2160 train_time:2758ms step_avg:34.92ms
step:80/2160 train_time:2791ms step_avg:34.89ms
step:81/2160 train_time:2825ms step_avg:34.88ms
step:82/2160 train_time:2858ms step_avg:34.86ms
step:83/2160 train_time:2892ms step_avg:34.85ms
step:84/2160 train_time:2926ms step_avg:34.83ms
step:85/2160 train_time:2960ms step_avg:34.82ms
step:86/2160 train_time:2993ms step_avg:34.80ms
step:87/2160 train_time:3027ms step_avg:34.79ms
step:88/2160 train_time:3060ms step_avg:34.77ms
step:89/2160 train_time:3094ms step_avg:34.77ms
step:90/2160 train_time:3127ms step_avg:34.75ms
step:91/2160 train_time:3161ms step_avg:34.74ms
step:92/2160 train_time:3194ms step_avg:34.72ms
step:93/2160 train_time:3229ms step_avg:34.72ms
step:94/2160 train_time:3262ms step_avg:34.70ms
step:95/2160 train_time:3296ms step_avg:34.69ms
step:96/2160 train_time:3329ms step_avg:34.68ms
step:97/2160 train_time:3363ms step_avg:34.67ms
step:98/2160 train_time:3396ms step_avg:34.65ms
step:99/2160 train_time:3430ms step_avg:34.65ms
step:100/2160 train_time:3463ms step_avg:34.63ms
step:101/2160 train_time:3497ms step_avg:34.63ms
step:102/2160 train_time:3530ms step_avg:34.61ms
step:103/2160 train_time:3564ms step_avg:34.60ms
step:104/2160 train_time:3597ms step_avg:34.59ms
step:105/2160 train_time:3631ms step_avg:34.58ms
step:106/2160 train_time:3664ms step_avg:34.57ms
step:107/2160 train_time:3698ms step_avg:34.56ms
step:108/2160 train_time:3731ms step_avg:34.55ms
step:109/2160 train_time:3765ms step_avg:34.54ms
step:110/2160 train_time:3798ms step_avg:34.53ms
step:111/2160 train_time:3833ms step_avg:34.53ms
step:112/2160 train_time:3866ms step_avg:34.52ms
step:113/2160 train_time:3900ms step_avg:34.51ms
step:114/2160 train_time:3933ms step_avg:34.50ms
step:115/2160 train_time:3967ms step_avg:34.50ms
step:116/2160 train_time:4000ms step_avg:34.48ms
step:117/2160 train_time:4034ms step_avg:34.48ms
step:118/2160 train_time:4067ms step_avg:34.47ms
step:119/2160 train_time:4102ms step_avg:34.47ms
step:120/2160 train_time:4134ms step_avg:34.45ms
step:121/2160 train_time:4169ms step_avg:34.45ms
step:122/2160 train_time:4201ms step_avg:34.44ms
step:123/2160 train_time:4236ms step_avg:34.44ms
step:124/2160 train_time:4269ms step_avg:34.43ms
step:125/2160 train_time:4303ms step_avg:34.43ms
step:126/2160 train_time:4336ms step_avg:34.42ms
step:127/2160 train_time:4371ms step_avg:34.41ms
step:128/2160 train_time:4404ms step_avg:34.40ms
step:129/2160 train_time:4438ms step_avg:34.40ms
step:130/2160 train_time:4471ms step_avg:34.39ms
step:131/2160 train_time:4505ms step_avg:34.39ms
step:132/2160 train_time:4537ms step_avg:34.37ms
step:133/2160 train_time:4572ms step_avg:34.38ms
step:134/2160 train_time:4605ms step_avg:34.36ms
step:135/2160 train_time:4639ms step_avg:34.36ms
step:136/2160 train_time:4672ms step_avg:34.35ms
step:137/2160 train_time:4706ms step_avg:34.35ms
step:138/2160 train_time:4739ms step_avg:34.34ms
step:139/2160 train_time:4773ms step_avg:34.34ms
step:140/2160 train_time:4806ms step_avg:34.33ms
step:141/2160 train_time:4839ms step_avg:34.32ms
step:142/2160 train_time:4872ms step_avg:34.31ms
step:143/2160 train_time:4906ms step_avg:34.31ms
step:144/2160 train_time:4939ms step_avg:34.30ms
step:145/2160 train_time:4973ms step_avg:34.30ms
step:146/2160 train_time:5007ms step_avg:34.29ms
step:147/2160 train_time:5040ms step_avg:34.29ms
step:148/2160 train_time:5074ms step_avg:34.28ms
step:149/2160 train_time:5108ms step_avg:34.28ms
step:150/2160 train_time:5141ms step_avg:34.27ms
step:151/2160 train_time:5175ms step_avg:34.27ms
step:152/2160 train_time:5208ms step_avg:34.27ms
step:153/2160 train_time:5242ms step_avg:34.26ms
step:154/2160 train_time:5276ms step_avg:34.26ms
step:155/2160 train_time:5310ms step_avg:34.26ms
step:156/2160 train_time:5343ms step_avg:34.25ms
step:157/2160 train_time:5377ms step_avg:34.25ms
step:158/2160 train_time:5410ms step_avg:34.24ms
step:159/2160 train_time:5444ms step_avg:34.24ms
step:160/2160 train_time:5477ms step_avg:34.23ms
step:161/2160 train_time:5511ms step_avg:34.23ms
step:162/2160 train_time:5544ms step_avg:34.22ms
step:163/2160 train_time:5578ms step_avg:34.22ms
step:164/2160 train_time:5611ms step_avg:34.21ms
step:165/2160 train_time:5645ms step_avg:34.21ms
step:166/2160 train_time:5678ms step_avg:34.21ms
step:167/2160 train_time:5713ms step_avg:34.21ms
step:168/2160 train_time:5745ms step_avg:34.20ms
step:169/2160 train_time:5780ms step_avg:34.20ms
step:170/2160 train_time:5813ms step_avg:34.19ms
step:171/2160 train_time:5846ms step_avg:34.19ms
step:172/2160 train_time:5879ms step_avg:34.18ms
step:173/2160 train_time:5914ms step_avg:34.18ms
step:174/2160 train_time:5947ms step_avg:34.18ms
step:175/2160 train_time:5981ms step_avg:34.18ms
step:176/2160 train_time:6014ms step_avg:34.17ms
step:177/2160 train_time:6048ms step_avg:34.17ms
step:178/2160 train_time:6081ms step_avg:34.16ms
step:179/2160 train_time:6115ms step_avg:34.16ms
step:180/2160 train_time:6148ms step_avg:34.16ms
step:181/2160 train_time:6182ms step_avg:34.16ms
step:182/2160 train_time:6215ms step_avg:34.15ms
step:183/2160 train_time:6249ms step_avg:34.15ms
step:184/2160 train_time:6282ms step_avg:34.14ms
step:185/2160 train_time:6316ms step_avg:34.14ms
step:186/2160 train_time:6349ms step_avg:34.13ms
step:187/2160 train_time:6383ms step_avg:34.13ms
step:188/2160 train_time:6416ms step_avg:34.13ms
step:189/2160 train_time:6450ms step_avg:34.13ms
step:190/2160 train_time:6483ms step_avg:34.12ms
step:191/2160 train_time:6517ms step_avg:34.12ms
step:192/2160 train_time:6550ms step_avg:34.11ms
step:193/2160 train_time:6584ms step_avg:34.12ms
step:194/2160 train_time:6617ms step_avg:34.11ms
step:195/2160 train_time:6651ms step_avg:34.11ms
step:196/2160 train_time:6684ms step_avg:34.10ms
step:197/2160 train_time:6718ms step_avg:34.10ms
step:198/2160 train_time:6751ms step_avg:34.10ms
step:199/2160 train_time:6785ms step_avg:34.10ms
step:200/2160 train_time:6818ms step_avg:34.09ms
step:201/2160 train_time:6852ms step_avg:34.09ms
step:202/2160 train_time:6885ms step_avg:34.08ms
step:203/2160 train_time:6919ms step_avg:34.08ms
step:204/2160 train_time:6952ms step_avg:34.08ms
step:205/2160 train_time:6986ms step_avg:34.08ms
step:206/2160 train_time:7018ms step_avg:34.07ms
step:207/2160 train_time:7053ms step_avg:34.07ms
step:208/2160 train_time:7086ms step_avg:34.07ms
step:209/2160 train_time:7120ms step_avg:34.07ms
step:210/2160 train_time:7153ms step_avg:34.06ms
step:211/2160 train_time:7188ms step_avg:34.06ms
step:212/2160 train_time:7220ms step_avg:34.06ms
step:213/2160 train_time:7255ms step_avg:34.06ms
step:214/2160 train_time:7288ms step_avg:34.05ms
step:215/2160 train_time:7322ms step_avg:34.06ms
step:216/2160 train_time:7355ms step_avg:34.05ms
step:217/2160 train_time:7389ms step_avg:34.05ms
step:218/2160 train_time:7422ms step_avg:34.05ms
step:219/2160 train_time:7456ms step_avg:34.05ms
step:220/2160 train_time:7489ms step_avg:34.04ms
step:221/2160 train_time:7523ms step_avg:34.04ms
step:222/2160 train_time:7556ms step_avg:34.04ms
step:223/2160 train_time:7590ms step_avg:34.04ms
step:224/2160 train_time:7623ms step_avg:34.03ms
step:225/2160 train_time:7657ms step_avg:34.03ms
step:226/2160 train_time:7690ms step_avg:34.03ms
step:227/2160 train_time:7725ms step_avg:34.03ms
step:228/2160 train_time:7757ms step_avg:34.02ms
step:229/2160 train_time:7791ms step_avg:34.02ms
step:230/2160 train_time:7824ms step_avg:34.02ms
step:231/2160 train_time:7858ms step_avg:34.02ms
step:232/2160 train_time:7891ms step_avg:34.01ms
step:233/2160 train_time:7925ms step_avg:34.01ms
step:234/2160 train_time:7958ms step_avg:34.01ms
step:235/2160 train_time:7992ms step_avg:34.01ms
step:236/2160 train_time:8025ms step_avg:34.00ms
step:237/2160 train_time:8059ms step_avg:34.01ms
step:238/2160 train_time:8092ms step_avg:34.00ms
step:239/2160 train_time:8126ms step_avg:34.00ms
step:240/2160 train_time:8159ms step_avg:34.00ms
step:241/2160 train_time:8194ms step_avg:34.00ms
step:242/2160 train_time:8226ms step_avg:33.99ms
step:243/2160 train_time:8261ms step_avg:34.00ms
step:244/2160 train_time:8294ms step_avg:33.99ms
step:245/2160 train_time:8328ms step_avg:33.99ms
step:246/2160 train_time:8361ms step_avg:33.99ms
step:247/2160 train_time:8395ms step_avg:33.99ms
step:248/2160 train_time:8428ms step_avg:33.98ms
step:249/2160 train_time:8462ms step_avg:33.99ms
step:250/2160 train_time:8496ms step_avg:33.98ms
step:250/2160 val_loss:4.3006 train_time:8531ms step_avg:34.12ms
step:251/2160 train_time:8552ms step_avg:34.07ms
step:252/2160 train_time:8570ms step_avg:34.01ms
step:253/2160 train_time:8600ms step_avg:33.99ms
step:254/2160 train_time:8635ms step_avg:33.99ms
step:255/2160 train_time:8673ms step_avg:34.01ms
step:256/2160 train_time:8707ms step_avg:34.01ms
step:257/2160 train_time:8742ms step_avg:34.02ms
step:258/2160 train_time:8775ms step_avg:34.01ms
step:259/2160 train_time:8810ms step_avg:34.01ms
step:260/2160 train_time:8843ms step_avg:34.01ms
step:261/2160 train_time:8877ms step_avg:34.01ms
step:262/2160 train_time:8910ms step_avg:34.01ms
step:263/2160 train_time:8944ms step_avg:34.01ms
step:264/2160 train_time:8976ms step_avg:34.00ms
step:265/2160 train_time:9011ms step_avg:34.00ms
step:266/2160 train_time:9043ms step_avg:34.00ms
step:267/2160 train_time:9077ms step_avg:34.00ms
step:268/2160 train_time:9110ms step_avg:33.99ms
step:269/2160 train_time:9144ms step_avg:33.99ms
step:270/2160 train_time:9177ms step_avg:33.99ms
step:271/2160 train_time:9211ms step_avg:33.99ms
step:272/2160 train_time:9243ms step_avg:33.98ms
step:273/2160 train_time:9277ms step_avg:33.98ms
step:274/2160 train_time:9310ms step_avg:33.98ms
step:275/2160 train_time:9344ms step_avg:33.98ms
step:276/2160 train_time:9377ms step_avg:33.97ms
step:277/2160 train_time:9411ms step_avg:33.97ms
step:278/2160 train_time:9444ms step_avg:33.97ms
step:279/2160 train_time:9478ms step_avg:33.97ms
step:280/2160 train_time:9511ms step_avg:33.97ms
step:281/2160 train_time:9545ms step_avg:33.97ms
step:282/2160 train_time:9578ms step_avg:33.96ms
step:283/2160 train_time:9612ms step_avg:33.97ms
step:284/2160 train_time:9646ms step_avg:33.96ms
step:285/2160 train_time:9680ms step_avg:33.96ms
step:286/2160 train_time:9713ms step_avg:33.96ms
step:287/2160 train_time:9747ms step_avg:33.96ms
step:288/2160 train_time:9780ms step_avg:33.96ms
step:289/2160 train_time:9815ms step_avg:33.96ms
step:290/2160 train_time:9848ms step_avg:33.96ms
step:291/2160 train_time:9882ms step_avg:33.96ms
step:292/2160 train_time:9915ms step_avg:33.96ms
step:293/2160 train_time:9950ms step_avg:33.96ms
step:294/2160 train_time:9982ms step_avg:33.95ms
step:295/2160 train_time:10017ms step_avg:33.95ms
step:296/2160 train_time:10050ms step_avg:33.95ms
step:297/2160 train_time:10084ms step_avg:33.95ms
step:298/2160 train_time:10117ms step_avg:33.95ms
step:299/2160 train_time:10151ms step_avg:33.95ms
step:300/2160 train_time:10183ms step_avg:33.94ms
step:301/2160 train_time:10218ms step_avg:33.95ms
step:302/2160 train_time:10251ms step_avg:33.94ms
step:303/2160 train_time:10285ms step_avg:33.94ms
step:304/2160 train_time:10318ms step_avg:33.94ms
step:305/2160 train_time:10351ms step_avg:33.94ms
step:306/2160 train_time:10384ms step_avg:33.93ms
step:307/2160 train_time:10418ms step_avg:33.94ms
step:308/2160 train_time:10451ms step_avg:33.93ms
step:309/2160 train_time:10485ms step_avg:33.93ms
step:310/2160 train_time:10518ms step_avg:33.93ms
step:311/2160 train_time:10552ms step_avg:33.93ms
step:312/2160 train_time:10585ms step_avg:33.93ms
step:313/2160 train_time:10619ms step_avg:33.93ms
step:314/2160 train_time:10652ms step_avg:33.92ms
step:315/2160 train_time:10686ms step_avg:33.93ms
step:316/2160 train_time:10719ms step_avg:33.92ms
step:317/2160 train_time:10753ms step_avg:33.92ms
step:318/2160 train_time:10786ms step_avg:33.92ms
step:319/2160 train_time:10820ms step_avg:33.92ms
step:320/2160 train_time:10854ms step_avg:33.92ms
step:321/2160 train_time:10888ms step_avg:33.92ms
step:322/2160 train_time:10921ms step_avg:33.92ms
step:323/2160 train_time:10955ms step_avg:33.92ms
step:324/2160 train_time:10988ms step_avg:33.91ms
step:325/2160 train_time:11022ms step_avg:33.91ms
step:326/2160 train_time:11055ms step_avg:33.91ms
step:327/2160 train_time:11089ms step_avg:33.91ms
step:328/2160 train_time:11122ms step_avg:33.91ms
step:329/2160 train_time:11156ms step_avg:33.91ms
step:330/2160 train_time:11189ms step_avg:33.91ms
step:331/2160 train_time:11223ms step_avg:33.91ms
step:332/2160 train_time:11256ms step_avg:33.91ms
step:333/2160 train_time:11290ms step_avg:33.90ms
step:334/2160 train_time:11323ms step_avg:33.90ms
step:335/2160 train_time:11357ms step_avg:33.90ms
step:336/2160 train_time:11390ms step_avg:33.90ms
step:337/2160 train_time:11424ms step_avg:33.90ms
step:338/2160 train_time:11457ms step_avg:33.90ms
step:339/2160 train_time:11491ms step_avg:33.90ms
step:340/2160 train_time:11523ms step_avg:33.89ms
step:341/2160 train_time:11558ms step_avg:33.89ms
step:342/2160 train_time:11590ms step_avg:33.89ms
step:343/2160 train_time:11624ms step_avg:33.89ms
step:344/2160 train_time:11657ms step_avg:33.89ms
step:345/2160 train_time:11691ms step_avg:33.89ms
step:346/2160 train_time:11724ms step_avg:33.88ms
step:347/2160 train_time:11758ms step_avg:33.89ms
step:348/2160 train_time:11792ms step_avg:33.88ms
step:349/2160 train_time:11826ms step_avg:33.88ms
step:350/2160 train_time:11859ms step_avg:33.88ms
step:351/2160 train_time:11893ms step_avg:33.88ms
step:352/2160 train_time:11925ms step_avg:33.88ms
step:353/2160 train_time:11960ms step_avg:33.88ms
step:354/2160 train_time:11992ms step_avg:33.88ms
step:355/2160 train_time:12027ms step_avg:33.88ms
step:356/2160 train_time:12060ms step_avg:33.88ms
step:357/2160 train_time:12094ms step_avg:33.88ms
step:358/2160 train_time:12127ms step_avg:33.87ms
step:359/2160 train_time:12161ms step_avg:33.88ms
step:360/2160 train_time:12194ms step_avg:33.87ms
step:361/2160 train_time:12228ms step_avg:33.87ms
step:362/2160 train_time:12261ms step_avg:33.87ms
step:363/2160 train_time:12295ms step_avg:33.87ms
step:364/2160 train_time:12328ms step_avg:33.87ms
step:365/2160 train_time:12362ms step_avg:33.87ms
step:366/2160 train_time:12395ms step_avg:33.87ms
step:367/2160 train_time:12429ms step_avg:33.87ms
step:368/2160 train_time:12462ms step_avg:33.86ms
step:369/2160 train_time:12497ms step_avg:33.87ms
step:370/2160 train_time:12529ms step_avg:33.86ms
step:371/2160 train_time:12563ms step_avg:33.86ms
step:372/2160 train_time:12596ms step_avg:33.86ms
step:373/2160 train_time:12630ms step_avg:33.86ms
step:374/2160 train_time:12663ms step_avg:33.86ms
step:375/2160 train_time:12697ms step_avg:33.86ms
step:376/2160 train_time:12730ms step_avg:33.86ms
step:377/2160 train_time:12764ms step_avg:33.86ms
step:378/2160 train_time:12797ms step_avg:33.86ms
step:379/2160 train_time:12831ms step_avg:33.86ms
step:380/2160 train_time:12864ms step_avg:33.85ms
step:381/2160 train_time:12899ms step_avg:33.86ms
step:382/2160 train_time:12932ms step_avg:33.85ms
step:383/2160 train_time:12966ms step_avg:33.85ms
step:384/2160 train_time:12999ms step_avg:33.85ms
step:385/2160 train_time:13033ms step_avg:33.85ms
step:386/2160 train_time:13066ms step_avg:33.85ms
step:387/2160 train_time:13100ms step_avg:33.85ms
step:388/2160 train_time:13133ms step_avg:33.85ms
step:389/2160 train_time:13167ms step_avg:33.85ms
step:390/2160 train_time:13200ms step_avg:33.85ms
step:391/2160 train_time:13235ms step_avg:33.85ms
step:392/2160 train_time:13268ms step_avg:33.85ms
step:393/2160 train_time:13302ms step_avg:33.85ms
step:394/2160 train_time:13335ms step_avg:33.85ms
step:395/2160 train_time:13369ms step_avg:33.85ms
step:396/2160 train_time:13402ms step_avg:33.84ms
step:397/2160 train_time:13436ms step_avg:33.84ms
step:398/2160 train_time:13469ms step_avg:33.84ms
step:399/2160 train_time:13503ms step_avg:33.84ms
step:400/2160 train_time:13536ms step_avg:33.84ms
step:401/2160 train_time:13570ms step_avg:33.84ms
step:402/2160 train_time:13603ms step_avg:33.84ms
step:403/2160 train_time:13637ms step_avg:33.84ms
step:404/2160 train_time:13670ms step_avg:33.84ms
step:405/2160 train_time:13704ms step_avg:33.84ms
step:406/2160 train_time:13737ms step_avg:33.84ms
step:407/2160 train_time:13771ms step_avg:33.84ms
step:408/2160 train_time:13804ms step_avg:33.83ms
step:409/2160 train_time:13839ms step_avg:33.83ms
step:410/2160 train_time:13871ms step_avg:33.83ms
step:411/2160 train_time:13906ms step_avg:33.83ms
step:412/2160 train_time:13938ms step_avg:33.83ms
step:413/2160 train_time:13972ms step_avg:33.83ms
step:414/2160 train_time:14005ms step_avg:33.83ms
step:415/2160 train_time:14039ms step_avg:33.83ms
step:416/2160 train_time:14072ms step_avg:33.83ms
step:417/2160 train_time:14106ms step_avg:33.83ms
step:418/2160 train_time:14139ms step_avg:33.83ms
step:419/2160 train_time:14173ms step_avg:33.83ms
step:420/2160 train_time:14207ms step_avg:33.83ms
step:421/2160 train_time:14241ms step_avg:33.83ms
step:422/2160 train_time:14274ms step_avg:33.82ms
step:423/2160 train_time:14308ms step_avg:33.83ms
step:424/2160 train_time:14341ms step_avg:33.82ms
step:425/2160 train_time:14375ms step_avg:33.82ms
step:426/2160 train_time:14409ms step_avg:33.82ms
step:427/2160 train_time:14442ms step_avg:33.82ms
step:428/2160 train_time:14475ms step_avg:33.82ms
step:429/2160 train_time:14509ms step_avg:33.82ms
step:430/2160 train_time:14542ms step_avg:33.82ms
step:431/2160 train_time:14576ms step_avg:33.82ms
step:432/2160 train_time:14609ms step_avg:33.82ms
step:433/2160 train_time:14643ms step_avg:33.82ms
step:434/2160 train_time:14676ms step_avg:33.82ms
step:435/2160 train_time:14710ms step_avg:33.82ms
step:436/2160 train_time:14743ms step_avg:33.81ms
step:437/2160 train_time:14777ms step_avg:33.81ms
step:438/2160 train_time:14810ms step_avg:33.81ms
step:439/2160 train_time:14844ms step_avg:33.81ms
step:440/2160 train_time:14877ms step_avg:33.81ms
step:441/2160 train_time:14911ms step_avg:33.81ms
step:442/2160 train_time:14944ms step_avg:33.81ms
step:443/2160 train_time:14978ms step_avg:33.81ms
step:444/2160 train_time:15011ms step_avg:33.81ms
step:445/2160 train_time:15045ms step_avg:33.81ms
step:446/2160 train_time:15078ms step_avg:33.81ms
step:447/2160 train_time:15112ms step_avg:33.81ms
step:448/2160 train_time:15145ms step_avg:33.81ms
step:449/2160 train_time:15180ms step_avg:33.81ms
step:450/2160 train_time:15213ms step_avg:33.81ms
step:451/2160 train_time:15247ms step_avg:33.81ms
step:452/2160 train_time:15280ms step_avg:33.81ms
step:453/2160 train_time:15315ms step_avg:33.81ms
step:454/2160 train_time:15348ms step_avg:33.81ms
step:455/2160 train_time:15382ms step_avg:33.81ms
step:456/2160 train_time:15415ms step_avg:33.80ms
step:457/2160 train_time:15449ms step_avg:33.81ms
step:458/2160 train_time:15482ms step_avg:33.80ms
step:459/2160 train_time:15516ms step_avg:33.80ms
step:460/2160 train_time:15549ms step_avg:33.80ms
step:461/2160 train_time:15583ms step_avg:33.80ms
step:462/2160 train_time:15616ms step_avg:33.80ms
step:463/2160 train_time:15650ms step_avg:33.80ms
step:464/2160 train_time:15683ms step_avg:33.80ms
step:465/2160 train_time:15717ms step_avg:33.80ms
step:466/2160 train_time:15751ms step_avg:33.80ms
step:467/2160 train_time:15784ms step_avg:33.80ms
step:468/2160 train_time:15817ms step_avg:33.80ms
step:469/2160 train_time:15851ms step_avg:33.80ms
step:470/2160 train_time:15884ms step_avg:33.80ms
step:471/2160 train_time:15919ms step_avg:33.80ms
step:472/2160 train_time:15952ms step_avg:33.80ms
step:473/2160 train_time:15986ms step_avg:33.80ms
step:474/2160 train_time:16019ms step_avg:33.79ms
step:475/2160 train_time:16053ms step_avg:33.80ms
step:476/2160 train_time:16086ms step_avg:33.79ms
step:477/2160 train_time:16120ms step_avg:33.79ms
step:478/2160 train_time:16153ms step_avg:33.79ms
step:479/2160 train_time:16187ms step_avg:33.79ms
step:480/2160 train_time:16220ms step_avg:33.79ms
step:481/2160 train_time:16254ms step_avg:33.79ms
step:482/2160 train_time:16287ms step_avg:33.79ms
step:483/2160 train_time:16321ms step_avg:33.79ms
step:484/2160 train_time:16354ms step_avg:33.79ms
step:485/2160 train_time:16389ms step_avg:33.79ms
step:486/2160 train_time:16422ms step_avg:33.79ms
step:487/2160 train_time:16456ms step_avg:33.79ms
step:488/2160 train_time:16489ms step_avg:33.79ms
step:489/2160 train_time:16523ms step_avg:33.79ms
step:490/2160 train_time:16556ms step_avg:33.79ms
step:491/2160 train_time:16591ms step_avg:33.79ms
step:492/2160 train_time:16623ms step_avg:33.79ms
step:493/2160 train_time:16658ms step_avg:33.79ms
step:494/2160 train_time:16691ms step_avg:33.79ms
step:495/2160 train_time:16725ms step_avg:33.79ms
step:496/2160 train_time:16758ms step_avg:33.79ms
step:497/2160 train_time:16792ms step_avg:33.79ms
step:498/2160 train_time:16825ms step_avg:33.78ms
step:499/2160 train_time:16859ms step_avg:33.79ms
step:500/2160 train_time:16892ms step_avg:33.78ms
step:500/2160 val_loss:4.0136 train_time:16928ms step_avg:33.86ms
step:501/2160 train_time:16948ms step_avg:33.83ms
step:502/2160 train_time:16967ms step_avg:33.80ms
step:503/2160 train_time:16997ms step_avg:33.79ms
step:504/2160 train_time:17030ms step_avg:33.79ms
step:505/2160 train_time:17066ms step_avg:33.79ms
step:506/2160 train_time:17100ms step_avg:33.79ms
step:507/2160 train_time:17134ms step_avg:33.79ms
step:508/2160 train_time:17167ms step_avg:33.79ms
step:509/2160 train_time:17202ms step_avg:33.80ms
step:510/2160 train_time:17235ms step_avg:33.79ms
step:511/2160 train_time:17269ms step_avg:33.79ms
step:512/2160 train_time:17302ms step_avg:33.79ms
step:513/2160 train_time:17336ms step_avg:33.79ms
step:514/2160 train_time:17369ms step_avg:33.79ms
step:515/2160 train_time:17403ms step_avg:33.79ms
step:516/2160 train_time:17436ms step_avg:33.79ms
step:517/2160 train_time:17470ms step_avg:33.79ms
step:518/2160 train_time:17503ms step_avg:33.79ms
step:519/2160 train_time:17536ms step_avg:33.79ms
step:520/2160 train_time:17569ms step_avg:33.79ms
step:521/2160 train_time:17603ms step_avg:33.79ms
step:522/2160 train_time:17636ms step_avg:33.79ms
step:523/2160 train_time:17670ms step_avg:33.79ms
step:524/2160 train_time:17703ms step_avg:33.78ms
step:525/2160 train_time:17737ms step_avg:33.78ms
step:526/2160 train_time:17770ms step_avg:33.78ms
step:527/2160 train_time:17804ms step_avg:33.78ms
step:528/2160 train_time:17836ms step_avg:33.78ms
step:529/2160 train_time:17871ms step_avg:33.78ms
step:530/2160 train_time:17904ms step_avg:33.78ms
step:531/2160 train_time:17939ms step_avg:33.78ms
step:532/2160 train_time:17972ms step_avg:33.78ms
step:533/2160 train_time:18007ms step_avg:33.78ms
step:534/2160 train_time:18040ms step_avg:33.78ms
step:535/2160 train_time:18074ms step_avg:33.78ms
step:536/2160 train_time:18107ms step_avg:33.78ms
step:537/2160 train_time:18142ms step_avg:33.78ms
step:538/2160 train_time:18175ms step_avg:33.78ms
step:539/2160 train_time:18209ms step_avg:33.78ms
step:540/2160 train_time:18243ms step_avg:33.78ms
step:541/2160 train_time:18276ms step_avg:33.78ms
step:542/2160 train_time:18309ms step_avg:33.78ms
step:543/2160 train_time:18343ms step_avg:33.78ms
step:544/2160 train_time:18376ms step_avg:33.78ms
step:545/2160 train_time:18410ms step_avg:33.78ms
step:546/2160 train_time:18443ms step_avg:33.78ms
step:547/2160 train_time:18477ms step_avg:33.78ms
step:548/2160 train_time:18510ms step_avg:33.78ms
step:549/2160 train_time:18544ms step_avg:33.78ms
step:550/2160 train_time:18577ms step_avg:33.78ms
step:551/2160 train_time:18611ms step_avg:33.78ms
step:552/2160 train_time:18644ms step_avg:33.77ms
step:553/2160 train_time:18678ms step_avg:33.77ms
step:554/2160 train_time:18710ms step_avg:33.77ms
step:555/2160 train_time:18744ms step_avg:33.77ms
step:556/2160 train_time:18777ms step_avg:33.77ms
step:557/2160 train_time:18811ms step_avg:33.77ms
step:558/2160 train_time:18844ms step_avg:33.77ms
step:559/2160 train_time:18878ms step_avg:33.77ms
step:560/2160 train_time:18911ms step_avg:33.77ms
step:561/2160 train_time:18946ms step_avg:33.77ms
step:562/2160 train_time:18979ms step_avg:33.77ms
step:563/2160 train_time:19013ms step_avg:33.77ms
step:564/2160 train_time:19047ms step_avg:33.77ms
step:565/2160 train_time:19081ms step_avg:33.77ms
step:566/2160 train_time:19114ms step_avg:33.77ms
step:567/2160 train_time:19149ms step_avg:33.77ms
step:568/2160 train_time:19182ms step_avg:33.77ms
step:569/2160 train_time:19216ms step_avg:33.77ms
step:570/2160 train_time:19249ms step_avg:33.77ms
step:571/2160 train_time:19283ms step_avg:33.77ms
step:572/2160 train_time:19316ms step_avg:33.77ms
step:573/2160 train_time:19350ms step_avg:33.77ms
step:574/2160 train_time:19383ms step_avg:33.77ms
step:575/2160 train_time:19417ms step_avg:33.77ms
step:576/2160 train_time:19450ms step_avg:33.77ms
step:577/2160 train_time:19485ms step_avg:33.77ms
step:578/2160 train_time:19518ms step_avg:33.77ms
step:579/2160 train_time:19552ms step_avg:33.77ms
step:580/2160 train_time:19585ms step_avg:33.77ms
step:581/2160 train_time:19619ms step_avg:33.77ms
step:582/2160 train_time:19652ms step_avg:33.77ms
step:583/2160 train_time:19686ms step_avg:33.77ms
step:584/2160 train_time:19720ms step_avg:33.77ms
step:585/2160 train_time:19753ms step_avg:33.77ms
step:586/2160 train_time:19786ms step_avg:33.76ms
step:587/2160 train_time:19821ms step_avg:33.77ms
step:588/2160 train_time:19854ms step_avg:33.76ms
step:589/2160 train_time:19888ms step_avg:33.77ms
step:590/2160 train_time:19921ms step_avg:33.76ms
step:591/2160 train_time:19955ms step_avg:33.76ms
step:592/2160 train_time:19988ms step_avg:33.76ms
step:593/2160 train_time:20022ms step_avg:33.76ms
step:594/2160 train_time:20055ms step_avg:33.76ms
step:595/2160 train_time:20089ms step_avg:33.76ms
step:596/2160 train_time:20122ms step_avg:33.76ms
step:597/2160 train_time:20157ms step_avg:33.76ms
step:598/2160 train_time:20190ms step_avg:33.76ms
step:599/2160 train_time:20224ms step_avg:33.76ms
step:600/2160 train_time:20257ms step_avg:33.76ms
step:601/2160 train_time:20291ms step_avg:33.76ms
step:602/2160 train_time:20324ms step_avg:33.76ms
step:603/2160 train_time:20359ms step_avg:33.76ms
step:604/2160 train_time:20391ms step_avg:33.76ms
step:605/2160 train_time:20426ms step_avg:33.76ms
step:606/2160 train_time:20459ms step_avg:33.76ms
step:607/2160 train_time:20493ms step_avg:33.76ms
step:608/2160 train_time:20526ms step_avg:33.76ms
step:609/2160 train_time:20560ms step_avg:33.76ms
step:610/2160 train_time:20593ms step_avg:33.76ms
step:611/2160 train_time:20627ms step_avg:33.76ms
step:612/2160 train_time:20661ms step_avg:33.76ms
step:613/2160 train_time:20695ms step_avg:33.76ms
step:614/2160 train_time:20728ms step_avg:33.76ms
step:615/2160 train_time:20762ms step_avg:33.76ms
step:616/2160 train_time:20795ms step_avg:33.76ms
step:617/2160 train_time:20829ms step_avg:33.76ms
step:618/2160 train_time:20862ms step_avg:33.76ms
step:619/2160 train_time:20897ms step_avg:33.76ms
step:620/2160 train_time:20930ms step_avg:33.76ms
step:621/2160 train_time:20964ms step_avg:33.76ms
step:622/2160 train_time:20997ms step_avg:33.76ms
step:623/2160 train_time:21032ms step_avg:33.76ms
step:624/2160 train_time:21065ms step_avg:33.76ms
step:625/2160 train_time:21099ms step_avg:33.76ms
step:626/2160 train_time:21132ms step_avg:33.76ms
step:627/2160 train_time:21166ms step_avg:33.76ms
step:628/2160 train_time:21199ms step_avg:33.76ms
step:629/2160 train_time:21233ms step_avg:33.76ms
step:630/2160 train_time:21266ms step_avg:33.76ms
step:631/2160 train_time:21301ms step_avg:33.76ms
step:632/2160 train_time:21334ms step_avg:33.76ms
step:633/2160 train_time:21368ms step_avg:33.76ms
step:634/2160 train_time:21401ms step_avg:33.76ms
step:635/2160 train_time:21435ms step_avg:33.76ms
step:636/2160 train_time:21468ms step_avg:33.75ms
step:637/2160 train_time:21502ms step_avg:33.76ms
step:638/2160 train_time:21535ms step_avg:33.75ms
step:639/2160 train_time:21569ms step_avg:33.76ms
step:640/2160 train_time:21603ms step_avg:33.75ms
step:641/2160 train_time:21637ms step_avg:33.75ms
step:642/2160 train_time:21670ms step_avg:33.75ms
step:643/2160 train_time:21704ms step_avg:33.75ms
step:644/2160 train_time:21737ms step_avg:33.75ms
step:645/2160 train_time:21771ms step_avg:33.75ms
step:646/2160 train_time:21804ms step_avg:33.75ms
step:647/2160 train_time:21838ms step_avg:33.75ms
step:648/2160 train_time:21871ms step_avg:33.75ms
step:649/2160 train_time:21905ms step_avg:33.75ms
step:650/2160 train_time:21938ms step_avg:33.75ms
step:651/2160 train_time:21972ms step_avg:33.75ms
step:652/2160 train_time:22005ms step_avg:33.75ms
step:653/2160 train_time:22040ms step_avg:33.75ms
step:654/2160 train_time:22073ms step_avg:33.75ms
step:655/2160 train_time:22107ms step_avg:33.75ms
step:656/2160 train_time:22140ms step_avg:33.75ms
step:657/2160 train_time:22174ms step_avg:33.75ms
step:658/2160 train_time:22207ms step_avg:33.75ms
step:659/2160 train_time:22241ms step_avg:33.75ms
step:660/2160 train_time:22274ms step_avg:33.75ms
step:661/2160 train_time:22308ms step_avg:33.75ms
step:662/2160 train_time:22341ms step_avg:33.75ms
step:663/2160 train_time:22375ms step_avg:33.75ms
step:664/2160 train_time:22408ms step_avg:33.75ms
step:665/2160 train_time:22443ms step_avg:33.75ms
step:666/2160 train_time:22476ms step_avg:33.75ms
step:667/2160 train_time:22510ms step_avg:33.75ms
step:668/2160 train_time:22543ms step_avg:33.75ms
step:669/2160 train_time:22577ms step_avg:33.75ms
step:670/2160 train_time:22610ms step_avg:33.75ms
step:671/2160 train_time:22644ms step_avg:33.75ms
step:672/2160 train_time:22678ms step_avg:33.75ms
step:673/2160 train_time:22711ms step_avg:33.75ms
step:674/2160 train_time:22745ms step_avg:33.75ms
step:675/2160 train_time:22778ms step_avg:33.75ms
step:676/2160 train_time:22811ms step_avg:33.74ms
step:677/2160 train_time:22846ms step_avg:33.75ms
step:678/2160 train_time:22879ms step_avg:33.74ms
step:679/2160 train_time:22913ms step_avg:33.75ms
step:680/2160 train_time:22946ms step_avg:33.74ms
step:681/2160 train_time:22980ms step_avg:33.74ms
step:682/2160 train_time:23013ms step_avg:33.74ms
step:683/2160 train_time:23047ms step_avg:33.74ms
step:684/2160 train_time:23080ms step_avg:33.74ms
step:685/2160 train_time:23115ms step_avg:33.74ms
step:686/2160 train_time:23148ms step_avg:33.74ms
step:687/2160 train_time:23182ms step_avg:33.74ms
step:688/2160 train_time:23215ms step_avg:33.74ms
step:689/2160 train_time:23249ms step_avg:33.74ms
step:690/2160 train_time:23283ms step_avg:33.74ms
step:691/2160 train_time:23317ms step_avg:33.74ms
step:692/2160 train_time:23349ms step_avg:33.74ms
step:693/2160 train_time:23384ms step_avg:33.74ms
step:694/2160 train_time:23417ms step_avg:33.74ms
step:695/2160 train_time:23451ms step_avg:33.74ms
step:696/2160 train_time:23484ms step_avg:33.74ms
step:697/2160 train_time:23518ms step_avg:33.74ms
step:698/2160 train_time:23551ms step_avg:33.74ms
step:699/2160 train_time:23585ms step_avg:33.74ms
step:700/2160 train_time:23619ms step_avg:33.74ms
step:701/2160 train_time:23653ms step_avg:33.74ms
step:702/2160 train_time:23686ms step_avg:33.74ms
step:703/2160 train_time:23720ms step_avg:33.74ms
step:704/2160 train_time:23753ms step_avg:33.74ms
step:705/2160 train_time:23787ms step_avg:33.74ms
step:706/2160 train_time:23820ms step_avg:33.74ms
step:707/2160 train_time:23854ms step_avg:33.74ms
step:708/2160 train_time:23888ms step_avg:33.74ms
step:709/2160 train_time:23948ms step_avg:33.78ms
step:710/2160 train_time:24007ms step_avg:33.81ms
step:711/2160 train_time:24068ms step_avg:33.85ms
step:712/2160 train_time:24128ms step_avg:33.89ms
step:713/2160 train_time:24190ms step_avg:33.93ms
step:714/2160 train_time:24250ms step_avg:33.96ms
step:715/2160 train_time:24312ms step_avg:34.00ms
step:716/2160 train_time:24372ms step_avg:34.04ms
step:717/2160 train_time:24434ms step_avg:34.08ms
step:718/2160 train_time:24494ms step_avg:34.11ms
step:719/2160 train_time:24557ms step_avg:34.15ms
step:720/2160 train_time:24616ms step_avg:34.19ms
step:721/2160 train_time:24678ms step_avg:34.23ms
step:722/2160 train_time:24737ms step_avg:34.26ms
step:723/2160 train_time:24799ms step_avg:34.30ms
step:724/2160 train_time:24858ms step_avg:34.33ms
step:725/2160 train_time:24919ms step_avg:34.37ms
step:726/2160 train_time:24978ms step_avg:34.40ms
step:727/2160 train_time:25039ms step_avg:34.44ms
step:728/2160 train_time:25098ms step_avg:34.48ms
step:729/2160 train_time:25160ms step_avg:34.51ms
step:730/2160 train_time:25219ms step_avg:34.55ms
step:731/2160 train_time:25281ms step_avg:34.58ms
step:732/2160 train_time:25340ms step_avg:34.62ms
step:733/2160 train_time:25402ms step_avg:34.66ms
step:734/2160 train_time:25462ms step_avg:34.69ms
step:735/2160 train_time:25523ms step_avg:34.73ms
step:736/2160 train_time:25584ms step_avg:34.76ms
step:737/2160 train_time:25646ms step_avg:34.80ms
step:738/2160 train_time:25705ms step_avg:34.83ms
step:739/2160 train_time:25766ms step_avg:34.87ms
step:740/2160 train_time:25826ms step_avg:34.90ms
step:741/2160 train_time:25886ms step_avg:34.93ms
step:742/2160 train_time:25946ms step_avg:34.97ms
step:743/2160 train_time:26007ms step_avg:35.00ms
step:744/2160 train_time:26066ms step_avg:35.04ms
step:745/2160 train_time:26127ms step_avg:35.07ms
step:746/2160 train_time:26187ms step_avg:35.10ms
step:747/2160 train_time:26248ms step_avg:35.14ms
step:748/2160 train_time:26307ms step_avg:35.17ms
step:749/2160 train_time:26369ms step_avg:35.21ms
step:750/2160 train_time:26428ms step_avg:35.24ms
step:750/2160 val_loss:3.8616 train_time:26491ms step_avg:35.32ms
step:751/2160 train_time:26510ms step_avg:35.30ms
step:752/2160 train_time:26551ms step_avg:35.31ms
step:753/2160 train_time:26613ms step_avg:35.34ms
step:754/2160 train_time:26673ms step_avg:35.38ms
step:755/2160 train_time:26735ms step_avg:35.41ms
step:756/2160 train_time:26795ms step_avg:35.44ms
step:757/2160 train_time:26856ms step_avg:35.48ms
step:758/2160 train_time:26915ms step_avg:35.51ms
step:759/2160 train_time:26976ms step_avg:35.54ms
step:760/2160 train_time:27035ms step_avg:35.57ms
step:761/2160 train_time:27095ms step_avg:35.60ms
step:762/2160 train_time:27154ms step_avg:35.63ms
step:763/2160 train_time:27214ms step_avg:35.67ms
step:764/2160 train_time:27274ms step_avg:35.70ms
step:765/2160 train_time:27336ms step_avg:35.73ms
step:766/2160 train_time:27400ms step_avg:35.77ms
step:767/2160 train_time:27466ms step_avg:35.81ms
step:768/2160 train_time:27526ms step_avg:35.84ms
step:769/2160 train_time:27587ms step_avg:35.87ms
step:770/2160 train_time:27647ms step_avg:35.91ms
step:771/2160 train_time:27709ms step_avg:35.94ms
step:772/2160 train_time:27769ms step_avg:35.97ms
step:773/2160 train_time:27830ms step_avg:36.00ms
step:774/2160 train_time:27889ms step_avg:36.03ms
step:775/2160 train_time:27950ms step_avg:36.06ms
step:776/2160 train_time:28009ms step_avg:36.09ms
step:777/2160 train_time:28070ms step_avg:36.13ms
step:778/2160 train_time:28129ms step_avg:36.16ms
step:779/2160 train_time:28190ms step_avg:36.19ms
step:780/2160 train_time:28250ms step_avg:36.22ms
step:781/2160 train_time:28312ms step_avg:36.25ms
step:782/2160 train_time:28373ms step_avg:36.28ms
step:783/2160 train_time:28437ms step_avg:36.32ms
step:784/2160 train_time:28498ms step_avg:36.35ms
step:785/2160 train_time:28559ms step_avg:36.38ms
step:786/2160 train_time:28619ms step_avg:36.41ms
step:787/2160 train_time:28680ms step_avg:36.44ms
step:788/2160 train_time:28740ms step_avg:36.47ms
step:789/2160 train_time:28801ms step_avg:36.50ms
step:790/2160 train_time:28860ms step_avg:36.53ms
step:791/2160 train_time:28921ms step_avg:36.56ms
step:792/2160 train_time:28980ms step_avg:36.59ms
step:793/2160 train_time:29041ms step_avg:36.62ms
step:794/2160 train_time:29099ms step_avg:36.65ms
step:795/2160 train_time:29161ms step_avg:36.68ms
step:796/2160 train_time:29221ms step_avg:36.71ms
step:797/2160 train_time:29283ms step_avg:36.74ms
step:798/2160 train_time:29343ms step_avg:36.77ms
step:799/2160 train_time:29404ms step_avg:36.80ms
step:800/2160 train_time:29464ms step_avg:36.83ms
step:801/2160 train_time:29525ms step_avg:36.86ms
step:802/2160 train_time:29584ms step_avg:36.89ms
step:803/2160 train_time:29646ms step_avg:36.92ms
step:804/2160 train_time:29706ms step_avg:36.95ms
step:805/2160 train_time:29766ms step_avg:36.98ms
step:806/2160 train_time:29826ms step_avg:37.00ms
step:807/2160 train_time:29886ms step_avg:37.03ms
step:808/2160 train_time:29946ms step_avg:37.06ms
step:809/2160 train_time:30007ms step_avg:37.09ms
step:810/2160 train_time:30066ms step_avg:37.12ms
step:811/2160 train_time:30127ms step_avg:37.15ms
step:812/2160 train_time:30186ms step_avg:37.17ms
step:813/2160 train_time:30247ms step_avg:37.20ms
step:814/2160 train_time:30308ms step_avg:37.23ms
step:815/2160 train_time:30370ms step_avg:37.26ms
step:816/2160 train_time:30430ms step_avg:37.29ms
step:817/2160 train_time:30492ms step_avg:37.32ms
step:818/2160 train_time:30552ms step_avg:37.35ms
step:819/2160 train_time:30614ms step_avg:37.38ms
step:820/2160 train_time:30674ms step_avg:37.41ms
step:821/2160 train_time:30735ms step_avg:37.44ms
step:822/2160 train_time:30795ms step_avg:37.46ms
step:823/2160 train_time:30857ms step_avg:37.49ms
step:824/2160 train_time:30916ms step_avg:37.52ms
step:825/2160 train_time:30978ms step_avg:37.55ms
step:826/2160 train_time:31038ms step_avg:37.58ms
step:827/2160 train_time:31099ms step_avg:37.60ms
step:828/2160 train_time:31158ms step_avg:37.63ms
step:829/2160 train_time:31219ms step_avg:37.66ms
step:830/2160 train_time:31278ms step_avg:37.68ms
step:831/2160 train_time:31340ms step_avg:37.71ms
step:832/2160 train_time:31400ms step_avg:37.74ms
step:833/2160 train_time:31461ms step_avg:37.77ms
step:834/2160 train_time:31521ms step_avg:37.80ms
step:835/2160 train_time:31583ms step_avg:37.82ms
step:836/2160 train_time:31643ms step_avg:37.85ms
step:837/2160 train_time:31704ms step_avg:37.88ms
step:838/2160 train_time:31764ms step_avg:37.90ms
step:839/2160 train_time:31825ms step_avg:37.93ms
step:840/2160 train_time:31884ms step_avg:37.96ms
step:841/2160 train_time:31945ms step_avg:37.98ms
step:842/2160 train_time:32005ms step_avg:38.01ms
step:843/2160 train_time:32066ms step_avg:38.04ms
step:844/2160 train_time:32126ms step_avg:38.06ms
step:845/2160 train_time:32187ms step_avg:38.09ms
step:846/2160 train_time:32246ms step_avg:38.12ms
step:847/2160 train_time:32307ms step_avg:38.14ms
step:848/2160 train_time:32368ms step_avg:38.17ms
step:849/2160 train_time:32429ms step_avg:38.20ms
step:850/2160 train_time:32488ms step_avg:38.22ms
step:851/2160 train_time:32549ms step_avg:38.25ms
step:852/2160 train_time:32609ms step_avg:38.27ms
step:853/2160 train_time:32671ms step_avg:38.30ms
step:854/2160 train_time:32730ms step_avg:38.33ms
step:855/2160 train_time:32792ms step_avg:38.35ms
step:856/2160 train_time:32852ms step_avg:38.38ms
step:857/2160 train_time:32913ms step_avg:38.40ms
step:858/2160 train_time:32973ms step_avg:38.43ms
step:859/2160 train_time:33035ms step_avg:38.46ms
step:860/2160 train_time:33094ms step_avg:38.48ms
step:861/2160 train_time:33156ms step_avg:38.51ms
step:862/2160 train_time:33216ms step_avg:38.53ms
step:863/2160 train_time:33278ms step_avg:38.56ms
step:864/2160 train_time:33337ms step_avg:38.58ms
step:865/2160 train_time:33399ms step_avg:38.61ms
step:866/2160 train_time:33458ms step_avg:38.64ms
step:867/2160 train_time:33519ms step_avg:38.66ms
step:868/2160 train_time:33578ms step_avg:38.68ms
step:869/2160 train_time:33639ms step_avg:38.71ms
step:870/2160 train_time:33699ms step_avg:38.73ms
step:871/2160 train_time:33760ms step_avg:38.76ms
step:872/2160 train_time:33820ms step_avg:38.78ms
step:873/2160 train_time:33881ms step_avg:38.81ms
step:874/2160 train_time:33941ms step_avg:38.83ms
step:875/2160 train_time:34001ms step_avg:38.86ms
step:876/2160 train_time:34060ms step_avg:38.88ms
step:877/2160 train_time:34122ms step_avg:38.91ms
step:878/2160 train_time:34181ms step_avg:38.93ms
step:879/2160 train_time:34242ms step_avg:38.96ms
step:880/2160 train_time:34302ms step_avg:38.98ms
step:881/2160 train_time:34363ms step_avg:39.00ms
step:882/2160 train_time:34423ms step_avg:39.03ms
step:883/2160 train_time:34484ms step_avg:39.05ms
step:884/2160 train_time:34544ms step_avg:39.08ms
step:885/2160 train_time:34605ms step_avg:39.10ms
step:886/2160 train_time:34665ms step_avg:39.13ms
step:887/2160 train_time:34726ms step_avg:39.15ms
step:888/2160 train_time:34787ms step_avg:39.17ms
step:889/2160 train_time:34848ms step_avg:39.20ms
step:890/2160 train_time:34907ms step_avg:39.22ms
step:891/2160 train_time:34968ms step_avg:39.25ms
step:892/2160 train_time:35028ms step_avg:39.27ms
step:893/2160 train_time:35089ms step_avg:39.29ms
step:894/2160 train_time:35149ms step_avg:39.32ms
step:895/2160 train_time:35211ms step_avg:39.34ms
step:896/2160 train_time:35272ms step_avg:39.37ms
step:897/2160 train_time:35334ms step_avg:39.39ms
step:898/2160 train_time:35394ms step_avg:39.41ms
step:899/2160 train_time:35455ms step_avg:39.44ms
step:900/2160 train_time:35515ms step_avg:39.46ms
step:901/2160 train_time:35577ms step_avg:39.49ms
step:902/2160 train_time:35637ms step_avg:39.51ms
step:903/2160 train_time:35698ms step_avg:39.53ms
step:904/2160 train_time:35758ms step_avg:39.55ms
step:905/2160 train_time:35819ms step_avg:39.58ms
step:906/2160 train_time:35878ms step_avg:39.60ms
step:907/2160 train_time:35940ms step_avg:39.62ms
step:908/2160 train_time:35999ms step_avg:39.65ms
step:909/2160 train_time:36060ms step_avg:39.67ms
step:910/2160 train_time:36120ms step_avg:39.69ms
step:911/2160 train_time:36181ms step_avg:39.72ms
step:912/2160 train_time:36241ms step_avg:39.74ms
step:913/2160 train_time:36302ms step_avg:39.76ms
step:914/2160 train_time:36361ms step_avg:39.78ms
step:915/2160 train_time:36423ms step_avg:39.81ms
step:916/2160 train_time:36482ms step_avg:39.83ms
step:917/2160 train_time:36544ms step_avg:39.85ms
step:918/2160 train_time:36604ms step_avg:39.87ms
step:919/2160 train_time:36664ms step_avg:39.90ms
step:920/2160 train_time:36724ms step_avg:39.92ms
step:921/2160 train_time:36785ms step_avg:39.94ms
step:922/2160 train_time:36844ms step_avg:39.96ms
step:923/2160 train_time:36905ms step_avg:39.98ms
step:924/2160 train_time:36964ms step_avg:40.00ms
step:925/2160 train_time:37025ms step_avg:40.03ms
step:926/2160 train_time:37084ms step_avg:40.05ms
step:927/2160 train_time:37146ms step_avg:40.07ms
step:928/2160 train_time:37206ms step_avg:40.09ms
step:929/2160 train_time:37268ms step_avg:40.12ms
step:930/2160 train_time:37328ms step_avg:40.14ms
step:931/2160 train_time:37389ms step_avg:40.16ms
step:932/2160 train_time:37449ms step_avg:40.18ms
step:933/2160 train_time:37510ms step_avg:40.20ms
step:934/2160 train_time:37569ms step_avg:40.22ms
step:935/2160 train_time:37631ms step_avg:40.25ms
step:936/2160 train_time:37691ms step_avg:40.27ms
step:937/2160 train_time:37753ms step_avg:40.29ms
step:938/2160 train_time:37812ms step_avg:40.31ms
step:939/2160 train_time:37873ms step_avg:40.33ms
step:940/2160 train_time:37933ms step_avg:40.35ms
step:941/2160 train_time:37994ms step_avg:40.38ms
step:942/2160 train_time:38054ms step_avg:40.40ms
step:943/2160 train_time:38115ms step_avg:40.42ms
step:944/2160 train_time:38175ms step_avg:40.44ms
step:945/2160 train_time:38237ms step_avg:40.46ms
step:946/2160 train_time:38296ms step_avg:40.48ms
step:947/2160 train_time:38357ms step_avg:40.50ms
step:948/2160 train_time:38417ms step_avg:40.52ms
step:949/2160 train_time:38478ms step_avg:40.55ms
step:950/2160 train_time:38537ms step_avg:40.57ms
step:951/2160 train_time:38599ms step_avg:40.59ms
step:952/2160 train_time:38659ms step_avg:40.61ms
step:953/2160 train_time:38720ms step_avg:40.63ms
step:954/2160 train_time:38779ms step_avg:40.65ms
step:955/2160 train_time:38840ms step_avg:40.67ms
step:956/2160 train_time:38899ms step_avg:40.69ms
step:957/2160 train_time:38960ms step_avg:40.71ms
step:958/2160 train_time:39020ms step_avg:40.73ms
step:959/2160 train_time:39080ms step_avg:40.75ms
step:960/2160 train_time:39140ms step_avg:40.77ms
step:961/2160 train_time:39202ms step_avg:40.79ms
step:962/2160 train_time:39261ms step_avg:40.81ms
step:963/2160 train_time:39322ms step_avg:40.83ms
step:964/2160 train_time:39381ms step_avg:40.85ms
step:965/2160 train_time:39442ms step_avg:40.87ms
step:966/2160 train_time:39501ms step_avg:40.89ms
step:967/2160 train_time:39562ms step_avg:40.91ms
step:968/2160 train_time:39621ms step_avg:40.93ms
step:969/2160 train_time:39683ms step_avg:40.95ms
step:970/2160 train_time:39742ms step_avg:40.97ms
step:971/2160 train_time:39802ms step_avg:40.99ms
step:972/2160 train_time:39861ms step_avg:41.01ms
step:973/2160 train_time:39923ms step_avg:41.03ms
step:974/2160 train_time:39982ms step_avg:41.05ms
step:975/2160 train_time:40043ms step_avg:41.07ms
step:976/2160 train_time:40103ms step_avg:41.09ms
step:977/2160 train_time:40164ms step_avg:41.11ms
step:978/2160 train_time:40224ms step_avg:41.13ms
step:979/2160 train_time:40285ms step_avg:41.15ms
step:980/2160 train_time:40344ms step_avg:41.17ms
step:981/2160 train_time:40405ms step_avg:41.19ms
step:982/2160 train_time:40465ms step_avg:41.21ms
step:983/2160 train_time:40526ms step_avg:41.23ms
step:984/2160 train_time:40585ms step_avg:41.24ms
step:985/2160 train_time:40645ms step_avg:41.26ms
step:986/2160 train_time:40706ms step_avg:41.28ms
step:987/2160 train_time:40767ms step_avg:41.30ms
step:988/2160 train_time:40826ms step_avg:41.32ms
step:989/2160 train_time:40887ms step_avg:41.34ms
step:990/2160 train_time:40946ms step_avg:41.36ms
step:991/2160 train_time:41008ms step_avg:41.38ms
step:992/2160 train_time:41068ms step_avg:41.40ms
step:993/2160 train_time:41129ms step_avg:41.42ms
step:994/2160 train_time:41188ms step_avg:41.44ms
step:995/2160 train_time:41250ms step_avg:41.46ms
step:996/2160 train_time:41310ms step_avg:41.48ms
step:997/2160 train_time:41371ms step_avg:41.50ms
step:998/2160 train_time:41430ms step_avg:41.51ms
step:999/2160 train_time:41492ms step_avg:41.53ms
step:1000/2160 train_time:41551ms step_avg:41.55ms
step:1000/2160 val_loss:3.6954 train_time:41613ms step_avg:41.61ms
step:1001/2160 train_time:41634ms step_avg:41.59ms
step:1002/2160 train_time:41675ms step_avg:41.59ms
step:1003/2160 train_time:41738ms step_avg:41.61ms
step:1004/2160 train_time:41800ms step_avg:41.63ms
step:1005/2160 train_time:41861ms step_avg:41.65ms
step:1006/2160 train_time:41921ms step_avg:41.67ms
step:1007/2160 train_time:41981ms step_avg:41.69ms
step:1008/2160 train_time:42040ms step_avg:41.71ms
step:1009/2160 train_time:42101ms step_avg:41.73ms
step:1010/2160 train_time:42160ms step_avg:41.74ms
step:1011/2160 train_time:42221ms step_avg:41.76ms
step:1012/2160 train_time:42280ms step_avg:41.78ms
step:1013/2160 train_time:42341ms step_avg:41.80ms
step:1014/2160 train_time:42401ms step_avg:41.82ms
step:1015/2160 train_time:42461ms step_avg:41.83ms
step:1016/2160 train_time:42521ms step_avg:41.85ms
step:1017/2160 train_time:42583ms step_avg:41.87ms
step:1018/2160 train_time:42644ms step_avg:41.89ms
step:1019/2160 train_time:42707ms step_avg:41.91ms
step:1020/2160 train_time:42768ms step_avg:41.93ms
step:1021/2160 train_time:42829ms step_avg:41.95ms
step:1022/2160 train_time:42889ms step_avg:41.97ms
step:1023/2160 train_time:42950ms step_avg:41.98ms
step:1024/2160 train_time:43010ms step_avg:42.00ms
step:1025/2160 train_time:43071ms step_avg:42.02ms
step:1026/2160 train_time:43130ms step_avg:42.04ms
step:1027/2160 train_time:43191ms step_avg:42.06ms
step:1028/2160 train_time:43250ms step_avg:42.07ms
step:1029/2160 train_time:43311ms step_avg:42.09ms
step:1030/2160 train_time:43370ms step_avg:42.11ms
step:1031/2160 train_time:43431ms step_avg:42.13ms
step:1032/2160 train_time:43491ms step_avg:42.14ms
step:1033/2160 train_time:43551ms step_avg:42.16ms
step:1034/2160 train_time:43611ms step_avg:42.18ms
step:1035/2160 train_time:43673ms step_avg:42.20ms
step:1036/2160 train_time:43733ms step_avg:42.21ms
step:1037/2160 train_time:43795ms step_avg:42.23ms
step:1038/2160 train_time:43855ms step_avg:42.25ms
step:1039/2160 train_time:43917ms step_avg:42.27ms
step:1040/2160 train_time:43976ms step_avg:42.28ms
step:1041/2160 train_time:44038ms step_avg:42.30ms
step:1042/2160 train_time:44097ms step_avg:42.32ms
step:1043/2160 train_time:44159ms step_avg:42.34ms
step:1044/2160 train_time:44218ms step_avg:42.35ms
step:1045/2160 train_time:44279ms step_avg:42.37ms
step:1046/2160 train_time:44338ms step_avg:42.39ms
step:1047/2160 train_time:44399ms step_avg:42.41ms
step:1048/2160 train_time:44459ms step_avg:42.42ms
step:1049/2160 train_time:44521ms step_avg:42.44ms
step:1050/2160 train_time:44581ms step_avg:42.46ms
step:1051/2160 train_time:44643ms step_avg:42.48ms
step:1052/2160 train_time:44702ms step_avg:42.49ms
step:1053/2160 train_time:44764ms step_avg:42.51ms
step:1054/2160 train_time:44823ms step_avg:42.53ms
step:1055/2160 train_time:44884ms step_avg:42.54ms
step:1056/2160 train_time:44943ms step_avg:42.56ms
step:1057/2160 train_time:45004ms step_avg:42.58ms
step:1058/2160 train_time:45063ms step_avg:42.59ms
step:1059/2160 train_time:45124ms step_avg:42.61ms
step:1060/2160 train_time:45184ms step_avg:42.63ms
step:1061/2160 train_time:45245ms step_avg:42.64ms
step:1062/2160 train_time:45304ms step_avg:42.66ms
step:1063/2160 train_time:45365ms step_avg:42.68ms
step:1064/2160 train_time:45425ms step_avg:42.69ms
step:1065/2160 train_time:45486ms step_avg:42.71ms
step:1066/2160 train_time:45546ms step_avg:42.73ms
step:1067/2160 train_time:45607ms step_avg:42.74ms
step:1068/2160 train_time:45666ms step_avg:42.76ms
step:1069/2160 train_time:45727ms step_avg:42.78ms
step:1070/2160 train_time:45786ms step_avg:42.79ms
step:1071/2160 train_time:45848ms step_avg:42.81ms
step:1072/2160 train_time:45907ms step_avg:42.82ms
step:1073/2160 train_time:45968ms step_avg:42.84ms
step:1074/2160 train_time:46028ms step_avg:42.86ms
step:1075/2160 train_time:46089ms step_avg:42.87ms
step:1076/2160 train_time:46149ms step_avg:42.89ms
step:1077/2160 train_time:46210ms step_avg:42.91ms
step:1078/2160 train_time:46269ms step_avg:42.92ms
step:1079/2160 train_time:46330ms step_avg:42.94ms
step:1080/2160 train_time:46389ms step_avg:42.95ms
step:1081/2160 train_time:46451ms step_avg:42.97ms
step:1082/2160 train_time:46511ms step_avg:42.99ms
step:1083/2160 train_time:46572ms step_avg:43.00ms
step:1084/2160 train_time:46631ms step_avg:43.02ms
step:1085/2160 train_time:46692ms step_avg:43.03ms
step:1086/2160 train_time:46752ms step_avg:43.05ms
step:1087/2160 train_time:46813ms step_avg:43.07ms
step:1088/2160 train_time:46873ms step_avg:43.08ms
step:1089/2160 train_time:46934ms step_avg:43.10ms
step:1090/2160 train_time:46993ms step_avg:43.11ms
step:1091/2160 train_time:47055ms step_avg:43.13ms
step:1092/2160 train_time:47115ms step_avg:43.15ms
step:1093/2160 train_time:47176ms step_avg:43.16ms
step:1094/2160 train_time:47236ms step_avg:43.18ms
step:1095/2160 train_time:47298ms step_avg:43.19ms
step:1096/2160 train_time:47357ms step_avg:43.21ms
step:1097/2160 train_time:47419ms step_avg:43.23ms
step:1098/2160 train_time:47479ms step_avg:43.24ms
step:1099/2160 train_time:47540ms step_avg:43.26ms
step:1100/2160 train_time:47600ms step_avg:43.27ms
step:1101/2160 train_time:47662ms step_avg:43.29ms
step:1102/2160 train_time:47721ms step_avg:43.30ms
step:1103/2160 train_time:47782ms step_avg:43.32ms
step:1104/2160 train_time:47842ms step_avg:43.34ms
step:1105/2160 train_time:47904ms step_avg:43.35ms
step:1106/2160 train_time:47963ms step_avg:43.37ms
step:1107/2160 train_time:48024ms step_avg:43.38ms
step:1108/2160 train_time:48084ms step_avg:43.40ms
step:1109/2160 train_time:48145ms step_avg:43.41ms
step:1110/2160 train_time:48204ms step_avg:43.43ms
step:1111/2160 train_time:48265ms step_avg:43.44ms
step:1112/2160 train_time:48325ms step_avg:43.46ms
step:1113/2160 train_time:48386ms step_avg:43.47ms
step:1114/2160 train_time:48445ms step_avg:43.49ms
step:1115/2160 train_time:48507ms step_avg:43.50ms
step:1116/2160 train_time:48567ms step_avg:43.52ms
step:1117/2160 train_time:48628ms step_avg:43.53ms
step:1118/2160 train_time:48688ms step_avg:43.55ms
step:1119/2160 train_time:48749ms step_avg:43.56ms
step:1120/2160 train_time:48809ms step_avg:43.58ms
step:1121/2160 train_time:48870ms step_avg:43.59ms
step:1122/2160 train_time:48929ms step_avg:43.61ms
step:1123/2160 train_time:48990ms step_avg:43.62ms
step:1124/2160 train_time:49051ms step_avg:43.64ms
step:1125/2160 train_time:49112ms step_avg:43.65ms
step:1126/2160 train_time:49171ms step_avg:43.67ms
step:1127/2160 train_time:49232ms step_avg:43.68ms
step:1128/2160 train_time:49292ms step_avg:43.70ms
step:1129/2160 train_time:49353ms step_avg:43.71ms
step:1130/2160 train_time:49412ms step_avg:43.73ms
step:1131/2160 train_time:49474ms step_avg:43.74ms
step:1132/2160 train_time:49533ms step_avg:43.76ms
step:1133/2160 train_time:49595ms step_avg:43.77ms
step:1134/2160 train_time:49654ms step_avg:43.79ms
step:1135/2160 train_time:49716ms step_avg:43.80ms
step:1136/2160 train_time:49776ms step_avg:43.82ms
step:1137/2160 train_time:49838ms step_avg:43.83ms
step:1138/2160 train_time:49897ms step_avg:43.85ms
step:1139/2160 train_time:49959ms step_avg:43.86ms
step:1140/2160 train_time:50019ms step_avg:43.88ms
step:1141/2160 train_time:50081ms step_avg:43.89ms
step:1142/2160 train_time:50140ms step_avg:43.91ms
step:1143/2160 train_time:50202ms step_avg:43.92ms
step:1144/2160 train_time:50261ms step_avg:43.93ms
step:1145/2160 train_time:50322ms step_avg:43.95ms
step:1146/2160 train_time:50382ms step_avg:43.96ms
step:1147/2160 train_time:50444ms step_avg:43.98ms
step:1148/2160 train_time:50503ms step_avg:43.99ms
step:1149/2160 train_time:50564ms step_avg:44.01ms
step:1150/2160 train_time:50623ms step_avg:44.02ms
step:1151/2160 train_time:50684ms step_avg:44.03ms
step:1152/2160 train_time:50744ms step_avg:44.05ms
step:1153/2160 train_time:50805ms step_avg:44.06ms
step:1154/2160 train_time:50864ms step_avg:44.08ms
step:1155/2160 train_time:50926ms step_avg:44.09ms
step:1156/2160 train_time:50986ms step_avg:44.11ms
step:1157/2160 train_time:51047ms step_avg:44.12ms
step:1158/2160 train_time:51106ms step_avg:44.13ms
step:1159/2160 train_time:51167ms step_avg:44.15ms
step:1160/2160 train_time:51227ms step_avg:44.16ms
step:1161/2160 train_time:51288ms step_avg:44.18ms
step:1162/2160 train_time:51347ms step_avg:44.19ms
step:1163/2160 train_time:51408ms step_avg:44.20ms
step:1164/2160 train_time:51468ms step_avg:44.22ms
step:1165/2160 train_time:51528ms step_avg:44.23ms
step:1166/2160 train_time:51588ms step_avg:44.24ms
step:1167/2160 train_time:51649ms step_avg:44.26ms
step:1168/2160 train_time:51708ms step_avg:44.27ms
step:1169/2160 train_time:51770ms step_avg:44.29ms
step:1170/2160 train_time:51829ms step_avg:44.30ms
step:1171/2160 train_time:51890ms step_avg:44.31ms
step:1172/2160 train_time:51951ms step_avg:44.33ms
step:1173/2160 train_time:52012ms step_avg:44.34ms
step:1174/2160 train_time:52072ms step_avg:44.35ms
step:1175/2160 train_time:52133ms step_avg:44.37ms
step:1176/2160 train_time:52192ms step_avg:44.38ms
step:1177/2160 train_time:52254ms step_avg:44.40ms
step:1178/2160 train_time:52314ms step_avg:44.41ms
step:1179/2160 train_time:52375ms step_avg:44.42ms
step:1180/2160 train_time:52435ms step_avg:44.44ms
step:1181/2160 train_time:52497ms step_avg:44.45ms
step:1182/2160 train_time:52557ms step_avg:44.46ms
step:1183/2160 train_time:52618ms step_avg:44.48ms
step:1184/2160 train_time:52678ms step_avg:44.49ms
step:1185/2160 train_time:52740ms step_avg:44.51ms
step:1186/2160 train_time:52799ms step_avg:44.52ms
step:1187/2160 train_time:52861ms step_avg:44.53ms
step:1188/2160 train_time:52921ms step_avg:44.55ms
step:1189/2160 train_time:52982ms step_avg:44.56ms
step:1190/2160 train_time:53042ms step_avg:44.57ms
step:1191/2160 train_time:53104ms step_avg:44.59ms
step:1192/2160 train_time:53164ms step_avg:44.60ms
step:1193/2160 train_time:53225ms step_avg:44.61ms
step:1194/2160 train_time:53284ms step_avg:44.63ms
step:1195/2160 train_time:53345ms step_avg:44.64ms
step:1196/2160 train_time:53404ms step_avg:44.65ms
step:1197/2160 train_time:53465ms step_avg:44.67ms
step:1198/2160 train_time:53525ms step_avg:44.68ms
step:1199/2160 train_time:53586ms step_avg:44.69ms
step:1200/2160 train_time:53646ms step_avg:44.70ms
step:1201/2160 train_time:53707ms step_avg:44.72ms
step:1202/2160 train_time:53768ms step_avg:44.73ms
step:1203/2160 train_time:53829ms step_avg:44.75ms
step:1204/2160 train_time:53888ms step_avg:44.76ms
step:1205/2160 train_time:53950ms step_avg:44.77ms
step:1206/2160 train_time:54009ms step_avg:44.78ms
step:1207/2160 train_time:54070ms step_avg:44.80ms
step:1208/2160 train_time:54130ms step_avg:44.81ms
step:1209/2160 train_time:54190ms step_avg:44.82ms
step:1210/2160 train_time:54250ms step_avg:44.83ms
step:1211/2160 train_time:54311ms step_avg:44.85ms
step:1212/2160 train_time:54370ms step_avg:44.86ms
step:1213/2160 train_time:54431ms step_avg:44.87ms
step:1214/2160 train_time:54491ms step_avg:44.89ms
step:1215/2160 train_time:54552ms step_avg:44.90ms
step:1216/2160 train_time:54612ms step_avg:44.91ms
step:1217/2160 train_time:54673ms step_avg:44.92ms
step:1218/2160 train_time:54733ms step_avg:44.94ms
step:1219/2160 train_time:54795ms step_avg:44.95ms
step:1220/2160 train_time:54855ms step_avg:44.96ms
step:1221/2160 train_time:54917ms step_avg:44.98ms
step:1222/2160 train_time:54976ms step_avg:44.99ms
step:1223/2160 train_time:55038ms step_avg:45.00ms
step:1224/2160 train_time:55098ms step_avg:45.01ms
step:1225/2160 train_time:55159ms step_avg:45.03ms
step:1226/2160 train_time:55219ms step_avg:45.04ms
step:1227/2160 train_time:55281ms step_avg:45.05ms
step:1228/2160 train_time:55341ms step_avg:45.07ms
step:1229/2160 train_time:55403ms step_avg:45.08ms
step:1230/2160 train_time:55463ms step_avg:45.09ms
step:1231/2160 train_time:55524ms step_avg:45.10ms
step:1232/2160 train_time:55583ms step_avg:45.12ms
step:1233/2160 train_time:55645ms step_avg:45.13ms
step:1234/2160 train_time:55704ms step_avg:45.14ms
step:1235/2160 train_time:55765ms step_avg:45.15ms
step:1236/2160 train_time:55824ms step_avg:45.17ms
step:1237/2160 train_time:55885ms step_avg:45.18ms
step:1238/2160 train_time:55945ms step_avg:45.19ms
step:1239/2160 train_time:56006ms step_avg:45.20ms
step:1240/2160 train_time:56065ms step_avg:45.21ms
step:1241/2160 train_time:56127ms step_avg:45.23ms
step:1242/2160 train_time:56186ms step_avg:45.24ms
step:1243/2160 train_time:56248ms step_avg:45.25ms
step:1244/2160 train_time:56307ms step_avg:45.26ms
step:1245/2160 train_time:56368ms step_avg:45.28ms
step:1246/2160 train_time:56428ms step_avg:45.29ms
step:1247/2160 train_time:56489ms step_avg:45.30ms
step:1248/2160 train_time:56548ms step_avg:45.31ms
step:1249/2160 train_time:56609ms step_avg:45.32ms
step:1250/2160 train_time:56669ms step_avg:45.33ms
step:1250/2160 val_loss:3.5809 train_time:56731ms step_avg:45.38ms
step:1251/2160 train_time:56750ms step_avg:45.36ms
step:1252/2160 train_time:56792ms step_avg:45.36ms
step:1253/2160 train_time:56856ms step_avg:45.38ms
step:1254/2160 train_time:56917ms step_avg:45.39ms
step:1255/2160 train_time:56978ms step_avg:45.40ms
step:1256/2160 train_time:57038ms step_avg:45.41ms
step:1257/2160 train_time:57098ms step_avg:45.42ms
step:1258/2160 train_time:57158ms step_avg:45.44ms
step:1259/2160 train_time:57218ms step_avg:45.45ms
step:1260/2160 train_time:57277ms step_avg:45.46ms
step:1261/2160 train_time:57338ms step_avg:45.47ms
step:1262/2160 train_time:57398ms step_avg:45.48ms
step:1263/2160 train_time:57459ms step_avg:45.49ms
step:1264/2160 train_time:57518ms step_avg:45.50ms
step:1265/2160 train_time:57579ms step_avg:45.52ms
step:1266/2160 train_time:57639ms step_avg:45.53ms
step:1267/2160 train_time:57703ms step_avg:45.54ms
step:1268/2160 train_time:57765ms step_avg:45.56ms
step:1269/2160 train_time:57827ms step_avg:45.57ms
step:1270/2160 train_time:57887ms step_avg:45.58ms
step:1271/2160 train_time:57948ms step_avg:45.59ms
step:1272/2160 train_time:58008ms step_avg:45.60ms
step:1273/2160 train_time:58069ms step_avg:45.62ms
step:1274/2160 train_time:58128ms step_avg:45.63ms
step:1275/2160 train_time:58189ms step_avg:45.64ms
step:1276/2160 train_time:58248ms step_avg:45.65ms
step:1277/2160 train_time:58309ms step_avg:45.66ms
step:1278/2160 train_time:58369ms step_avg:45.67ms
step:1279/2160 train_time:58429ms step_avg:45.68ms
step:1280/2160 train_time:58488ms step_avg:45.69ms
step:1281/2160 train_time:58549ms step_avg:45.71ms
step:1282/2160 train_time:58609ms step_avg:45.72ms
step:1283/2160 train_time:58672ms step_avg:45.73ms
step:1284/2160 train_time:58732ms step_avg:45.74ms
step:1285/2160 train_time:58793ms step_avg:45.75ms
step:1286/2160 train_time:58853ms step_avg:45.76ms
step:1287/2160 train_time:58914ms step_avg:45.78ms
step:1288/2160 train_time:58974ms step_avg:45.79ms
step:1289/2160 train_time:59036ms step_avg:45.80ms
step:1290/2160 train_time:59096ms step_avg:45.81ms
step:1291/2160 train_time:59158ms step_avg:45.82ms
step:1292/2160 train_time:59217ms step_avg:45.83ms
step:1293/2160 train_time:59279ms step_avg:45.85ms
step:1294/2160 train_time:59338ms step_avg:45.86ms
step:1295/2160 train_time:59399ms step_avg:45.87ms
step:1296/2160 train_time:59459ms step_avg:45.88ms
step:1297/2160 train_time:59520ms step_avg:45.89ms
step:1298/2160 train_time:59580ms step_avg:45.90ms
step:1299/2160 train_time:59642ms step_avg:45.91ms
step:1300/2160 train_time:59702ms step_avg:45.92ms
step:1301/2160 train_time:59764ms step_avg:45.94ms
step:1302/2160 train_time:59824ms step_avg:45.95ms
step:1303/2160 train_time:59887ms step_avg:45.96ms
step:1304/2160 train_time:59946ms step_avg:45.97ms
step:1305/2160 train_time:60007ms step_avg:45.98ms
step:1306/2160 train_time:60066ms step_avg:45.99ms
step:1307/2160 train_time:60127ms step_avg:46.00ms
step:1308/2160 train_time:60187ms step_avg:46.01ms
step:1309/2160 train_time:60248ms step_avg:46.03ms
step:1310/2160 train_time:60308ms step_avg:46.04ms
step:1311/2160 train_time:60369ms step_avg:46.05ms
step:1312/2160 train_time:60429ms step_avg:46.06ms
step:1313/2160 train_time:60490ms step_avg:46.07ms
step:1314/2160 train_time:60550ms step_avg:46.08ms
step:1315/2160 train_time:60611ms step_avg:46.09ms
step:1316/2160 train_time:60671ms step_avg:46.10ms
step:1317/2160 train_time:60731ms step_avg:46.11ms
step:1318/2160 train_time:60791ms step_avg:46.12ms
step:1319/2160 train_time:60853ms step_avg:46.14ms
step:1320/2160 train_time:60912ms step_avg:46.15ms
step:1321/2160 train_time:60973ms step_avg:46.16ms
step:1322/2160 train_time:61033ms step_avg:46.17ms
step:1323/2160 train_time:61094ms step_avg:46.18ms
step:1324/2160 train_time:61154ms step_avg:46.19ms
step:1325/2160 train_time:61215ms step_avg:46.20ms
step:1326/2160 train_time:61274ms step_avg:46.21ms
step:1327/2160 train_time:61336ms step_avg:46.22ms
step:1328/2160 train_time:61397ms step_avg:46.23ms
step:1329/2160 train_time:61459ms step_avg:46.24ms
step:1330/2160 train_time:61519ms step_avg:46.25ms
step:1331/2160 train_time:61581ms step_avg:46.27ms
step:1332/2160 train_time:61641ms step_avg:46.28ms
step:1333/2160 train_time:61703ms step_avg:46.29ms
step:1334/2160 train_time:61762ms step_avg:46.30ms
step:1335/2160 train_time:61824ms step_avg:46.31ms
step:1336/2160 train_time:61884ms step_avg:46.32ms
step:1337/2160 train_time:61946ms step_avg:46.33ms
step:1338/2160 train_time:62005ms step_avg:46.34ms
step:1339/2160 train_time:62066ms step_avg:46.35ms
step:1340/2160 train_time:62126ms step_avg:46.36ms
step:1341/2160 train_time:62187ms step_avg:46.37ms
step:1342/2160 train_time:62246ms step_avg:46.38ms
step:1343/2160 train_time:62307ms step_avg:46.39ms
step:1344/2160 train_time:62367ms step_avg:46.40ms
step:1345/2160 train_time:62429ms step_avg:46.42ms
step:1346/2160 train_time:62489ms step_avg:46.43ms
step:1347/2160 train_time:62551ms step_avg:46.44ms
step:1348/2160 train_time:62611ms step_avg:46.45ms
step:1349/2160 train_time:62672ms step_avg:46.46ms
step:1350/2160 train_time:62732ms step_avg:46.47ms
step:1351/2160 train_time:62793ms step_avg:46.48ms
step:1352/2160 train_time:62853ms step_avg:46.49ms
step:1353/2160 train_time:62914ms step_avg:46.50ms
step:1354/2160 train_time:62973ms step_avg:46.51ms
step:1355/2160 train_time:63034ms step_avg:46.52ms
step:1356/2160 train_time:63094ms step_avg:46.53ms
step:1357/2160 train_time:63155ms step_avg:46.54ms
step:1358/2160 train_time:63215ms step_avg:46.55ms
step:1359/2160 train_time:63276ms step_avg:46.56ms
step:1360/2160 train_time:63337ms step_avg:46.57ms
step:1361/2160 train_time:63399ms step_avg:46.58ms
step:1362/2160 train_time:63459ms step_avg:46.59ms
step:1363/2160 train_time:63521ms step_avg:46.60ms
step:1364/2160 train_time:63581ms step_avg:46.61ms
step:1365/2160 train_time:63643ms step_avg:46.62ms
step:1366/2160 train_time:63702ms step_avg:46.63ms
step:1367/2160 train_time:63763ms step_avg:46.64ms
step:1368/2160 train_time:63823ms step_avg:46.65ms
step:1369/2160 train_time:63884ms step_avg:46.66ms
step:1370/2160 train_time:63943ms step_avg:46.67ms
step:1371/2160 train_time:64004ms step_avg:46.68ms
step:1372/2160 train_time:64064ms step_avg:46.69ms
step:1373/2160 train_time:64126ms step_avg:46.70ms
step:1374/2160 train_time:64185ms step_avg:46.71ms
step:1375/2160 train_time:64246ms step_avg:46.72ms
step:1376/2160 train_time:64306ms step_avg:46.73ms
step:1377/2160 train_time:64367ms step_avg:46.74ms
step:1378/2160 train_time:64427ms step_avg:46.75ms
step:1379/2160 train_time:64490ms step_avg:46.77ms
step:1380/2160 train_time:64550ms step_avg:46.78ms
step:1381/2160 train_time:64611ms step_avg:46.79ms
step:1382/2160 train_time:64670ms step_avg:46.79ms
step:1383/2160 train_time:64731ms step_avg:46.81ms
step:1384/2160 train_time:64792ms step_avg:46.81ms
step:1385/2160 train_time:64853ms step_avg:46.83ms
step:1386/2160 train_time:64913ms step_avg:46.83ms
step:1387/2160 train_time:64973ms step_avg:46.84ms
step:1388/2160 train_time:65033ms step_avg:46.85ms
step:1389/2160 train_time:65094ms step_avg:46.86ms
step:1390/2160 train_time:65154ms step_avg:46.87ms
step:1391/2160 train_time:65216ms step_avg:46.88ms
step:1392/2160 train_time:65275ms step_avg:46.89ms
step:1393/2160 train_time:65337ms step_avg:46.90ms
step:1394/2160 train_time:65398ms step_avg:46.91ms
step:1395/2160 train_time:65459ms step_avg:46.92ms
step:1396/2160 train_time:65519ms step_avg:46.93ms
step:1397/2160 train_time:65581ms step_avg:46.94ms
step:1398/2160 train_time:65640ms step_avg:46.95ms
step:1399/2160 train_time:65702ms step_avg:46.96ms
step:1400/2160 train_time:65762ms step_avg:46.97ms
step:1401/2160 train_time:65823ms step_avg:46.98ms
step:1402/2160 train_time:65883ms step_avg:46.99ms
step:1403/2160 train_time:65944ms step_avg:47.00ms
step:1404/2160 train_time:66003ms step_avg:47.01ms
step:1405/2160 train_time:66064ms step_avg:47.02ms
step:1406/2160 train_time:66123ms step_avg:47.03ms
step:1407/2160 train_time:66184ms step_avg:47.04ms
step:1408/2160 train_time:66243ms step_avg:47.05ms
step:1409/2160 train_time:66304ms step_avg:47.06ms
step:1410/2160 train_time:66364ms step_avg:47.07ms
step:1411/2160 train_time:66425ms step_avg:47.08ms
step:1412/2160 train_time:66485ms step_avg:47.09ms
step:1413/2160 train_time:66546ms step_avg:47.10ms
step:1414/2160 train_time:66605ms step_avg:47.10ms
step:1415/2160 train_time:66666ms step_avg:47.11ms
step:1416/2160 train_time:66754ms step_avg:47.14ms
step:1417/2160 train_time:66843ms step_avg:47.17ms
step:1418/2160 train_time:66931ms step_avg:47.20ms
step:1419/2160 train_time:67020ms step_avg:47.23ms
step:1420/2160 train_time:67106ms step_avg:47.26ms
step:1421/2160 train_time:67196ms step_avg:47.29ms
step:1422/2160 train_time:67283ms step_avg:47.32ms
step:1423/2160 train_time:67374ms step_avg:47.35ms
step:1424/2160 train_time:67462ms step_avg:47.38ms
step:1425/2160 train_time:67551ms step_avg:47.40ms
step:1426/2160 train_time:67638ms step_avg:47.43ms
step:1427/2160 train_time:67728ms step_avg:47.46ms
step:1428/2160 train_time:67816ms step_avg:47.49ms
step:1429/2160 train_time:67904ms step_avg:47.52ms
step:1430/2160 train_time:67993ms step_avg:47.55ms
step:1431/2160 train_time:68082ms step_avg:47.58ms
step:1432/2160 train_time:68170ms step_avg:47.60ms
step:1433/2160 train_time:68259ms step_avg:47.63ms
step:1434/2160 train_time:68347ms step_avg:47.66ms
step:1435/2160 train_time:68436ms step_avg:47.69ms
step:1436/2160 train_time:68524ms step_avg:47.72ms
step:1437/2160 train_time:68613ms step_avg:47.75ms
step:1438/2160 train_time:68699ms step_avg:47.77ms
step:1439/2160 train_time:68789ms step_avg:47.80ms
step:1440/2160 train_time:68877ms step_avg:47.83ms
step:1441/2160 train_time:68966ms step_avg:47.86ms
step:1442/2160 train_time:69053ms step_avg:47.89ms
step:1443/2160 train_time:69142ms step_avg:47.92ms
step:1444/2160 train_time:69230ms step_avg:47.94ms
step:1445/2160 train_time:69319ms step_avg:47.97ms
step:1446/2160 train_time:69407ms step_avg:48.00ms
step:1447/2160 train_time:69496ms step_avg:48.03ms
step:1448/2160 train_time:69583ms step_avg:48.05ms
step:1449/2160 train_time:69672ms step_avg:48.08ms
step:1450/2160 train_time:69760ms step_avg:48.11ms
step:1451/2160 train_time:69849ms step_avg:48.14ms
step:1452/2160 train_time:69936ms step_avg:48.17ms
step:1453/2160 train_time:70026ms step_avg:48.19ms
step:1454/2160 train_time:70113ms step_avg:48.22ms
step:1455/2160 train_time:70203ms step_avg:48.25ms
step:1456/2160 train_time:70291ms step_avg:48.28ms
step:1457/2160 train_time:70380ms step_avg:48.30ms
step:1458/2160 train_time:70468ms step_avg:48.33ms
step:1459/2160 train_time:70557ms step_avg:48.36ms
step:1460/2160 train_time:70644ms step_avg:48.39ms
step:1461/2160 train_time:70734ms step_avg:48.41ms
step:1462/2160 train_time:70821ms step_avg:48.44ms
step:1463/2160 train_time:70910ms step_avg:48.47ms
step:1464/2160 train_time:70997ms step_avg:48.50ms
step:1465/2160 train_time:71086ms step_avg:48.52ms
step:1466/2160 train_time:71174ms step_avg:48.55ms
step:1467/2160 train_time:71263ms step_avg:48.58ms
step:1468/2160 train_time:71351ms step_avg:48.60ms
step:1469/2160 train_time:71441ms step_avg:48.63ms
step:1470/2160 train_time:71529ms step_avg:48.66ms
step:1471/2160 train_time:71618ms step_avg:48.69ms
step:1472/2160 train_time:71706ms step_avg:48.71ms
step:1473/2160 train_time:71795ms step_avg:48.74ms
step:1474/2160 train_time:71883ms step_avg:48.77ms
step:1475/2160 train_time:71972ms step_avg:48.79ms
step:1476/2160 train_time:72059ms step_avg:48.82ms
step:1477/2160 train_time:72148ms step_avg:48.85ms
step:1478/2160 train_time:72235ms step_avg:48.87ms
step:1479/2160 train_time:72325ms step_avg:48.90ms
step:1480/2160 train_time:72413ms step_avg:48.93ms
step:1481/2160 train_time:72502ms step_avg:48.96ms
step:1482/2160 train_time:72590ms step_avg:48.98ms
step:1483/2160 train_time:72679ms step_avg:49.01ms
step:1484/2160 train_time:72768ms step_avg:49.03ms
step:1485/2160 train_time:72857ms step_avg:49.06ms
step:1486/2160 train_time:72944ms step_avg:49.09ms
step:1487/2160 train_time:73033ms step_avg:49.11ms
step:1488/2160 train_time:73121ms step_avg:49.14ms
step:1489/2160 train_time:73210ms step_avg:49.17ms
step:1490/2160 train_time:73297ms step_avg:49.19ms
step:1491/2160 train_time:73386ms step_avg:49.22ms
step:1492/2160 train_time:73473ms step_avg:49.24ms
step:1493/2160 train_time:73562ms step_avg:49.27ms
step:1494/2160 train_time:73649ms step_avg:49.30ms
step:1495/2160 train_time:73739ms step_avg:49.32ms
step:1496/2160 train_time:73827ms step_avg:49.35ms
step:1497/2160 train_time:73917ms step_avg:49.38ms
step:1498/2160 train_time:74004ms step_avg:49.40ms
step:1499/2160 train_time:74093ms step_avg:49.43ms
step:1500/2160 train_time:74180ms step_avg:49.45ms
step:1500/2160 val_loss:3.4984 train_time:74271ms step_avg:49.51ms
step:1501/2160 train_time:74291ms step_avg:49.49ms
step:1502/2160 train_time:74362ms step_avg:49.51ms
step:1503/2160 train_time:74452ms step_avg:49.54ms
step:1504/2160 train_time:74542ms step_avg:49.56ms
step:1505/2160 train_time:74631ms step_avg:49.59ms
step:1506/2160 train_time:74717ms step_avg:49.61ms
step:1507/2160 train_time:74805ms step_avg:49.64ms
step:1508/2160 train_time:74891ms step_avg:49.66ms
step:1509/2160 train_time:74979ms step_avg:49.69ms
step:1510/2160 train_time:75066ms step_avg:49.71ms
step:1511/2160 train_time:75156ms step_avg:49.74ms
step:1512/2160 train_time:75248ms step_avg:49.77ms
step:1513/2160 train_time:75338ms step_avg:49.79ms
step:1514/2160 train_time:75425ms step_avg:49.82ms
step:1515/2160 train_time:75515ms step_avg:49.85ms
step:1516/2160 train_time:75603ms step_avg:49.87ms
step:1517/2160 train_time:75692ms step_avg:49.90ms
step:1518/2160 train_time:75778ms step_avg:49.92ms
step:1519/2160 train_time:75865ms step_avg:49.94ms
step:1520/2160 train_time:75951ms step_avg:49.97ms
step:1521/2160 train_time:76039ms step_avg:49.99ms
step:1522/2160 train_time:76127ms step_avg:50.02ms
step:1523/2160 train_time:76217ms step_avg:50.04ms
step:1524/2160 train_time:76306ms step_avg:50.07ms
step:1525/2160 train_time:76396ms step_avg:50.10ms
step:1526/2160 train_time:76484ms step_avg:50.12ms
step:1527/2160 train_time:76574ms step_avg:50.15ms
step:1528/2160 train_time:76661ms step_avg:50.17ms
step:1529/2160 train_time:76749ms step_avg:50.20ms
step:1530/2160 train_time:76837ms step_avg:50.22ms
step:1531/2160 train_time:76925ms step_avg:50.24ms
step:1532/2160 train_time:77011ms step_avg:50.27ms
step:1533/2160 train_time:77100ms step_avg:50.29ms
step:1534/2160 train_time:77189ms step_avg:50.32ms
step:1535/2160 train_time:77278ms step_avg:50.34ms
step:1536/2160 train_time:77367ms step_avg:50.37ms
step:1537/2160 train_time:77456ms step_avg:50.39ms
step:1538/2160 train_time:77544ms step_avg:50.42ms
step:1539/2160 train_time:77634ms step_avg:50.44ms
step:1540/2160 train_time:77721ms step_avg:50.47ms
step:1541/2160 train_time:77809ms step_avg:50.49ms
step:1542/2160 train_time:77896ms step_avg:50.52ms
step:1543/2160 train_time:77985ms step_avg:50.54ms
step:1544/2160 train_time:78073ms step_avg:50.57ms
step:1545/2160 train_time:78162ms step_avg:50.59ms
step:1546/2160 train_time:78249ms step_avg:50.61ms
step:1547/2160 train_time:78339ms step_avg:50.64ms
step:1548/2160 train_time:78426ms step_avg:50.66ms
step:1549/2160 train_time:78516ms step_avg:50.69ms
step:1550/2160 train_time:78603ms step_avg:50.71ms
step:1551/2160 train_time:78692ms step_avg:50.74ms
step:1552/2160 train_time:78779ms step_avg:50.76ms
step:1553/2160 train_time:78867ms step_avg:50.78ms
step:1554/2160 train_time:78955ms step_avg:50.81ms
step:1555/2160 train_time:79043ms step_avg:50.83ms
step:1556/2160 train_time:79131ms step_avg:50.86ms
step:1557/2160 train_time:79221ms step_avg:50.88ms
step:1558/2160 train_time:79309ms step_avg:50.90ms
step:1559/2160 train_time:79398ms step_avg:50.93ms
step:1560/2160 train_time:79486ms step_avg:50.95ms
step:1561/2160 train_time:79575ms step_avg:50.98ms
step:1562/2160 train_time:79662ms step_avg:51.00ms
step:1563/2160 train_time:79753ms step_avg:51.03ms
step:1564/2160 train_time:79840ms step_avg:51.05ms
step:1565/2160 train_time:79928ms step_avg:51.07ms
step:1566/2160 train_time:80016ms step_avg:51.10ms
step:1567/2160 train_time:80105ms step_avg:51.12ms
step:1568/2160 train_time:80192ms step_avg:51.14ms
step:1569/2160 train_time:80282ms step_avg:51.17ms
step:1570/2160 train_time:80369ms step_avg:51.19ms
step:1571/2160 train_time:80458ms step_avg:51.21ms
step:1572/2160 train_time:80545ms step_avg:51.24ms
step:1573/2160 train_time:80634ms step_avg:51.26ms
step:1574/2160 train_time:80722ms step_avg:51.28ms
step:1575/2160 train_time:80811ms step_avg:51.31ms
step:1576/2160 train_time:80899ms step_avg:51.33ms
step:1577/2160 train_time:80987ms step_avg:51.36ms
step:1578/2160 train_time:81075ms step_avg:51.38ms
step:1579/2160 train_time:81164ms step_avg:51.40ms
step:1580/2160 train_time:81251ms step_avg:51.42ms
step:1581/2160 train_time:81340ms step_avg:51.45ms
step:1582/2160 train_time:81427ms step_avg:51.47ms
step:1583/2160 train_time:81518ms step_avg:51.50ms
step:1584/2160 train_time:81605ms step_avg:51.52ms
step:1585/2160 train_time:81695ms step_avg:51.54ms
step:1586/2160 train_time:81782ms step_avg:51.57ms
step:1587/2160 train_time:81872ms step_avg:51.59ms
step:1588/2160 train_time:81959ms step_avg:51.61ms
step:1589/2160 train_time:82047ms step_avg:51.63ms
step:1590/2160 train_time:82135ms step_avg:51.66ms
step:1591/2160 train_time:82224ms step_avg:51.68ms
step:1592/2160 train_time:82312ms step_avg:51.70ms
step:1593/2160 train_time:82400ms step_avg:51.73ms
step:1594/2160 train_time:82488ms step_avg:51.75ms
step:1595/2160 train_time:82578ms step_avg:51.77ms
step:1596/2160 train_time:82665ms step_avg:51.79ms
step:1597/2160 train_time:82753ms step_avg:51.82ms
step:1598/2160 train_time:82841ms step_avg:51.84ms
step:1599/2160 train_time:82930ms step_avg:51.86ms
step:1600/2160 train_time:83017ms step_avg:51.89ms
step:1601/2160 train_time:83106ms step_avg:51.91ms
step:1602/2160 train_time:83194ms step_avg:51.93ms
step:1603/2160 train_time:83282ms step_avg:51.95ms
step:1604/2160 train_time:83370ms step_avg:51.98ms
step:1605/2160 train_time:83459ms step_avg:52.00ms
step:1606/2160 train_time:83546ms step_avg:52.02ms
step:1607/2160 train_time:83635ms step_avg:52.04ms
step:1608/2160 train_time:83722ms step_avg:52.07ms
step:1609/2160 train_time:83811ms step_avg:52.09ms
step:1610/2160 train_time:83899ms step_avg:52.11ms
step:1611/2160 train_time:83988ms step_avg:52.13ms
step:1612/2160 train_time:84076ms step_avg:52.16ms
step:1613/2160 train_time:84165ms step_avg:52.18ms
step:1614/2160 train_time:84252ms step_avg:52.20ms
step:1615/2160 train_time:84341ms step_avg:52.22ms
step:1616/2160 train_time:84429ms step_avg:52.25ms
step:1617/2160 train_time:84518ms step_avg:52.27ms
step:1618/2160 train_time:84605ms step_avg:52.29ms
step:1619/2160 train_time:84695ms step_avg:52.31ms
step:1620/2160 train_time:84783ms step_avg:52.33ms
step:1621/2160 train_time:84871ms step_avg:52.36ms
step:1622/2160 train_time:84959ms step_avg:52.38ms
step:1623/2160 train_time:85048ms step_avg:52.40ms
step:1624/2160 train_time:85135ms step_avg:52.42ms
step:1625/2160 train_time:85223ms step_avg:52.44ms
step:1626/2160 train_time:85311ms step_avg:52.47ms
step:1627/2160 train_time:85400ms step_avg:52.49ms
step:1628/2160 train_time:85489ms step_avg:52.51ms
step:1629/2160 train_time:85578ms step_avg:52.53ms
step:1630/2160 train_time:85664ms step_avg:52.55ms
step:1631/2160 train_time:85754ms step_avg:52.58ms
step:1632/2160 train_time:85841ms step_avg:52.60ms
step:1633/2160 train_time:85931ms step_avg:52.62ms
step:1634/2160 train_time:86018ms step_avg:52.64ms
step:1635/2160 train_time:86107ms step_avg:52.66ms
step:1636/2160 train_time:86193ms step_avg:52.69ms
step:1637/2160 train_time:86282ms step_avg:52.71ms
step:1638/2160 train_time:86370ms step_avg:52.73ms
step:1639/2160 train_time:86459ms step_avg:52.75ms
step:1640/2160 train_time:86547ms step_avg:52.77ms
step:1641/2160 train_time:86636ms step_avg:52.79ms
step:1642/2160 train_time:86723ms step_avg:52.82ms
step:1643/2160 train_time:86813ms step_avg:52.84ms
step:1644/2160 train_time:86900ms step_avg:52.86ms
step:1645/2160 train_time:86989ms step_avg:52.88ms
step:1646/2160 train_time:87077ms step_avg:52.90ms
step:1647/2160 train_time:87166ms step_avg:52.92ms
step:1648/2160 train_time:87253ms step_avg:52.94ms
step:1649/2160 train_time:87342ms step_avg:52.97ms
step:1650/2160 train_time:87430ms step_avg:52.99ms
step:1651/2160 train_time:87520ms step_avg:53.01ms
step:1652/2160 train_time:87607ms step_avg:53.03ms
step:1653/2160 train_time:87697ms step_avg:53.05ms
step:1654/2160 train_time:87784ms step_avg:53.07ms
step:1655/2160 train_time:87874ms step_avg:53.10ms
step:1656/2160 train_time:87961ms step_avg:53.12ms
step:1657/2160 train_time:88050ms step_avg:53.14ms
step:1658/2160 train_time:88137ms step_avg:53.16ms
step:1659/2160 train_time:88225ms step_avg:53.18ms
step:1660/2160 train_time:88313ms step_avg:53.20ms
step:1661/2160 train_time:88402ms step_avg:53.22ms
step:1662/2160 train_time:88489ms step_avg:53.24ms
step:1663/2160 train_time:88579ms step_avg:53.26ms
step:1664/2160 train_time:88666ms step_avg:53.28ms
step:1665/2160 train_time:88756ms step_avg:53.31ms
step:1666/2160 train_time:88843ms step_avg:53.33ms
step:1667/2160 train_time:88933ms step_avg:53.35ms
step:1668/2160 train_time:89020ms step_avg:53.37ms
step:1669/2160 train_time:89108ms step_avg:53.39ms
step:1670/2160 train_time:89196ms step_avg:53.41ms
step:1671/2160 train_time:89285ms step_avg:53.43ms
step:1672/2160 train_time:89372ms step_avg:53.45ms
step:1673/2160 train_time:89462ms step_avg:53.47ms
step:1674/2160 train_time:89549ms step_avg:53.49ms
step:1675/2160 train_time:89638ms step_avg:53.52ms
step:1676/2160 train_time:89725ms step_avg:53.54ms
step:1677/2160 train_time:89814ms step_avg:53.56ms
step:1678/2160 train_time:89902ms step_avg:53.58ms
step:1679/2160 train_time:89991ms step_avg:53.60ms
step:1680/2160 train_time:90078ms step_avg:53.62ms
step:1681/2160 train_time:90168ms step_avg:53.64ms
step:1682/2160 train_time:90254ms step_avg:53.66ms
step:1683/2160 train_time:90343ms step_avg:53.68ms
step:1684/2160 train_time:90431ms step_avg:53.70ms
step:1685/2160 train_time:90520ms step_avg:53.72ms
step:1686/2160 train_time:90608ms step_avg:53.74ms
step:1687/2160 train_time:90697ms step_avg:53.76ms
step:1688/2160 train_time:90785ms step_avg:53.78ms
step:1689/2160 train_time:90875ms step_avg:53.80ms
step:1690/2160 train_time:90962ms step_avg:53.82ms
step:1691/2160 train_time:91051ms step_avg:53.84ms
step:1692/2160 train_time:91138ms step_avg:53.86ms
step:1693/2160 train_time:91228ms step_avg:53.89ms
step:1694/2160 train_time:91315ms step_avg:53.90ms
step:1695/2160 train_time:91404ms step_avg:53.93ms
step:1696/2160 train_time:91491ms step_avg:53.95ms
step:1697/2160 train_time:91580ms step_avg:53.97ms
step:1698/2160 train_time:91667ms step_avg:53.99ms
step:1699/2160 train_time:91757ms step_avg:54.01ms
step:1700/2160 train_time:91845ms step_avg:54.03ms
step:1701/2160 train_time:91934ms step_avg:54.05ms
step:1702/2160 train_time:92020ms step_avg:54.07ms
step:1703/2160 train_time:92110ms step_avg:54.09ms
step:1704/2160 train_time:92196ms step_avg:54.11ms
step:1705/2160 train_time:92285ms step_avg:54.13ms
step:1706/2160 train_time:92373ms step_avg:54.15ms
step:1707/2160 train_time:92461ms step_avg:54.17ms
step:1708/2160 train_time:92548ms step_avg:54.19ms
step:1709/2160 train_time:92638ms step_avg:54.21ms
step:1710/2160 train_time:92725ms step_avg:54.23ms
step:1711/2160 train_time:92814ms step_avg:54.25ms
step:1712/2160 train_time:92902ms step_avg:54.26ms
step:1713/2160 train_time:92990ms step_avg:54.28ms
step:1714/2160 train_time:93078ms step_avg:54.30ms
step:1715/2160 train_time:93167ms step_avg:54.32ms
step:1716/2160 train_time:93254ms step_avg:54.34ms
step:1717/2160 train_time:93344ms step_avg:54.36ms
step:1718/2160 train_time:93432ms step_avg:54.38ms
step:1719/2160 train_time:93522ms step_avg:54.40ms
step:1720/2160 train_time:93610ms step_avg:54.42ms
step:1721/2160 train_time:93699ms step_avg:54.44ms
step:1722/2160 train_time:93787ms step_avg:54.46ms
step:1723/2160 train_time:93877ms step_avg:54.48ms
step:1724/2160 train_time:93964ms step_avg:54.50ms
step:1725/2160 train_time:94053ms step_avg:54.52ms
step:1726/2160 train_time:94141ms step_avg:54.54ms
step:1727/2160 train_time:94230ms step_avg:54.56ms
step:1728/2160 train_time:94318ms step_avg:54.58ms
step:1729/2160 train_time:94407ms step_avg:54.60ms
step:1730/2160 train_time:94494ms step_avg:54.62ms
step:1731/2160 train_time:94583ms step_avg:54.64ms
step:1732/2160 train_time:94670ms step_avg:54.66ms
step:1733/2160 train_time:94760ms step_avg:54.68ms
step:1734/2160 train_time:94848ms step_avg:54.70ms
step:1735/2160 train_time:94938ms step_avg:54.72ms
step:1736/2160 train_time:95025ms step_avg:54.74ms
step:1737/2160 train_time:95113ms step_avg:54.76ms
step:1738/2160 train_time:95201ms step_avg:54.78ms
step:1739/2160 train_time:95290ms step_avg:54.80ms
step:1740/2160 train_time:95377ms step_avg:54.81ms
step:1741/2160 train_time:95466ms step_avg:54.83ms
step:1742/2160 train_time:95553ms step_avg:54.85ms
step:1743/2160 train_time:95643ms step_avg:54.87ms
step:1744/2160 train_time:95731ms step_avg:54.89ms
step:1745/2160 train_time:95820ms step_avg:54.91ms
step:1746/2160 train_time:95909ms step_avg:54.93ms
step:1747/2160 train_time:95999ms step_avg:54.95ms
step:1748/2160 train_time:96086ms step_avg:54.97ms
step:1749/2160 train_time:96176ms step_avg:54.99ms
step:1750/2160 train_time:96263ms step_avg:55.01ms
step:1750/2160 val_loss:3.3963 train_time:96353ms step_avg:55.06ms
step:1751/2160 train_time:96373ms step_avg:55.04ms
step:1752/2160 train_time:96444ms step_avg:55.05ms
step:1753/2160 train_time:96539ms step_avg:55.07ms
step:1754/2160 train_time:96626ms step_avg:55.09ms
step:1755/2160 train_time:96715ms step_avg:55.11ms
step:1756/2160 train_time:96801ms step_avg:55.13ms
step:1757/2160 train_time:96889ms step_avg:55.14ms
step:1758/2160 train_time:96976ms step_avg:55.16ms
step:1759/2160 train_time:97065ms step_avg:55.18ms
step:1760/2160 train_time:97151ms step_avg:55.20ms
step:1761/2160 train_time:97240ms step_avg:55.22ms
step:1762/2160 train_time:97330ms step_avg:55.24ms
step:1763/2160 train_time:97421ms step_avg:55.26ms
step:1764/2160 train_time:97511ms step_avg:55.28ms
step:1765/2160 train_time:97601ms step_avg:55.30ms
step:1766/2160 train_time:97688ms step_avg:55.32ms
step:1767/2160 train_time:97776ms step_avg:55.33ms
step:1768/2160 train_time:97863ms step_avg:55.35ms
step:1769/2160 train_time:97952ms step_avg:55.37ms
step:1770/2160 train_time:98039ms step_avg:55.39ms
step:1771/2160 train_time:98129ms step_avg:55.41ms
step:1772/2160 train_time:98216ms step_avg:55.43ms
step:1773/2160 train_time:98306ms step_avg:55.45ms
step:1774/2160 train_time:98395ms step_avg:55.46ms
step:1775/2160 train_time:98485ms step_avg:55.48ms
step:1776/2160 train_time:98573ms step_avg:55.50ms
step:1777/2160 train_time:98662ms step_avg:55.52ms
step:1778/2160 train_time:98749ms step_avg:55.54ms
step:1779/2160 train_time:98837ms step_avg:55.56ms
step:1780/2160 train_time:98924ms step_avg:55.58ms
step:1781/2160 train_time:99013ms step_avg:55.59ms
step:1782/2160 train_time:99099ms step_avg:55.61ms
step:1783/2160 train_time:99189ms step_avg:55.63ms
step:1784/2160 train_time:99276ms step_avg:55.65ms
step:1785/2160 train_time:99366ms step_avg:55.67ms
step:1786/2160 train_time:99454ms step_avg:55.69ms
step:1787/2160 train_time:99544ms step_avg:55.70ms
step:1788/2160 train_time:99632ms step_avg:55.72ms
step:1789/2160 train_time:99721ms step_avg:55.74ms
step:1790/2160 train_time:99809ms step_avg:55.76ms
step:1791/2160 train_time:99898ms step_avg:55.78ms
step:1792/2160 train_time:99984ms step_avg:55.79ms
step:1793/2160 train_time:100074ms step_avg:55.81ms
step:1794/2160 train_time:100160ms step_avg:55.83ms
step:1795/2160 train_time:100250ms step_avg:55.85ms
step:1796/2160 train_time:100338ms step_avg:55.87ms
step:1797/2160 train_time:100428ms step_avg:55.89ms
step:1798/2160 train_time:100517ms step_avg:55.90ms
step:1799/2160 train_time:100607ms step_avg:55.92ms
step:1800/2160 train_time:100695ms step_avg:55.94ms
step:1801/2160 train_time:100785ms step_avg:55.96ms
step:1802/2160 train_time:100872ms step_avg:55.98ms
step:1803/2160 train_time:100961ms step_avg:56.00ms
step:1804/2160 train_time:101048ms step_avg:56.01ms
step:1805/2160 train_time:101137ms step_avg:56.03ms
step:1806/2160 train_time:101224ms step_avg:56.05ms
step:1807/2160 train_time:101314ms step_avg:56.07ms
step:1808/2160 train_time:101402ms step_avg:56.09ms
step:1809/2160 train_time:101491ms step_avg:56.10ms
step:1810/2160 train_time:101579ms step_avg:56.12ms
step:1811/2160 train_time:101669ms step_avg:56.14ms
step:1812/2160 train_time:101757ms step_avg:56.16ms
step:1813/2160 train_time:101846ms step_avg:56.18ms
step:1814/2160 train_time:101934ms step_avg:56.19ms
step:1815/2160 train_time:102022ms step_avg:56.21ms
step:1816/2160 train_time:102109ms step_avg:56.23ms
step:1817/2160 train_time:102198ms step_avg:56.25ms
step:1818/2160 train_time:102285ms step_avg:56.26ms
step:1819/2160 train_time:102375ms step_avg:56.28ms
step:1820/2160 train_time:102462ms step_avg:56.30ms
step:1821/2160 train_time:102552ms step_avg:56.32ms
step:1822/2160 train_time:102639ms step_avg:56.33ms
step:1823/2160 train_time:102728ms step_avg:56.35ms
step:1824/2160 train_time:102816ms step_avg:56.37ms
step:1825/2160 train_time:102905ms step_avg:56.39ms
step:1826/2160 train_time:102993ms step_avg:56.40ms
step:1827/2160 train_time:103083ms step_avg:56.42ms
step:1828/2160 train_time:103169ms step_avg:56.44ms
step:1829/2160 train_time:103258ms step_avg:56.46ms
step:1830/2160 train_time:103346ms step_avg:56.47ms
step:1831/2160 train_time:103435ms step_avg:56.49ms
step:1832/2160 train_time:103522ms step_avg:56.51ms
step:1833/2160 train_time:103611ms step_avg:56.53ms
step:1834/2160 train_time:103698ms step_avg:56.54ms
step:1835/2160 train_time:103789ms step_avg:56.56ms
step:1836/2160 train_time:103875ms step_avg:56.58ms
step:1837/2160 train_time:103964ms step_avg:56.59ms
step:1838/2160 train_time:104052ms step_avg:56.61ms
step:1839/2160 train_time:104142ms step_avg:56.63ms
step:1840/2160 train_time:104228ms step_avg:56.65ms
step:1841/2160 train_time:104317ms step_avg:56.66ms
step:1842/2160 train_time:104405ms step_avg:56.68ms
step:1843/2160 train_time:104495ms step_avg:56.70ms
step:1844/2160 train_time:104582ms step_avg:56.72ms
step:1845/2160 train_time:104672ms step_avg:56.73ms
step:1846/2160 train_time:104759ms step_avg:56.75ms
step:1847/2160 train_time:104848ms step_avg:56.77ms
step:1848/2160 train_time:104936ms step_avg:56.78ms
step:1849/2160 train_time:105025ms step_avg:56.80ms
step:1850/2160 train_time:105112ms step_avg:56.82ms
step:1851/2160 train_time:105201ms step_avg:56.83ms
step:1852/2160 train_time:105288ms step_avg:56.85ms
step:1853/2160 train_time:105378ms step_avg:56.87ms
step:1854/2160 train_time:105464ms step_avg:56.88ms
step:1855/2160 train_time:105554ms step_avg:56.90ms
step:1856/2160 train_time:105641ms step_avg:56.92ms
step:1857/2160 train_time:105731ms step_avg:56.94ms
step:1858/2160 train_time:105817ms step_avg:56.95ms
step:1859/2160 train_time:105907ms step_avg:56.97ms
step:1860/2160 train_time:105995ms step_avg:56.99ms
step:1861/2160 train_time:106083ms step_avg:57.00ms
step:1862/2160 train_time:106170ms step_avg:57.02ms
step:1863/2160 train_time:106259ms step_avg:57.04ms
step:1864/2160 train_time:106347ms step_avg:57.05ms
step:1865/2160 train_time:106437ms step_avg:57.07ms
step:1866/2160 train_time:106524ms step_avg:57.09ms
step:1867/2160 train_time:106613ms step_avg:57.10ms
step:1868/2160 train_time:106701ms step_avg:57.12ms
step:1869/2160 train_time:106791ms step_avg:57.14ms
step:1870/2160 train_time:106878ms step_avg:57.15ms
step:1871/2160 train_time:106967ms step_avg:57.17ms
step:1872/2160 train_time:107054ms step_avg:57.19ms
step:1873/2160 train_time:107143ms step_avg:57.20ms
step:1874/2160 train_time:107230ms step_avg:57.22ms
step:1875/2160 train_time:107319ms step_avg:57.24ms
step:1876/2160 train_time:107408ms step_avg:57.25ms
step:1877/2160 train_time:107496ms step_avg:57.27ms
step:1878/2160 train_time:107584ms step_avg:57.29ms
step:1879/2160 train_time:107674ms step_avg:57.30ms
step:1880/2160 train_time:107760ms step_avg:57.32ms
step:1881/2160 train_time:107850ms step_avg:57.34ms
step:1882/2160 train_time:107937ms step_avg:57.35ms
step:1883/2160 train_time:108027ms step_avg:57.37ms
step:1884/2160 train_time:108114ms step_avg:57.39ms
step:1885/2160 train_time:108203ms step_avg:57.40ms
step:1886/2160 train_time:108291ms step_avg:57.42ms
step:1887/2160 train_time:108380ms step_avg:57.44ms
step:1888/2160 train_time:108468ms step_avg:57.45ms
step:1889/2160 train_time:108558ms step_avg:57.47ms
step:1890/2160 train_time:108645ms step_avg:57.48ms
step:1891/2160 train_time:108734ms step_avg:57.50ms
step:1892/2160 train_time:108822ms step_avg:57.52ms
step:1893/2160 train_time:108912ms step_avg:57.53ms
step:1894/2160 train_time:108999ms step_avg:57.55ms
step:1895/2160 train_time:109088ms step_avg:57.57ms
step:1896/2160 train_time:109174ms step_avg:57.58ms
step:1897/2160 train_time:109263ms step_avg:57.60ms
step:1898/2160 train_time:109351ms step_avg:57.61ms
step:1899/2160 train_time:109439ms step_avg:57.63ms
step:1900/2160 train_time:109527ms step_avg:57.65ms
step:1901/2160 train_time:109617ms step_avg:57.66ms
step:1902/2160 train_time:109705ms step_avg:57.68ms
step:1903/2160 train_time:109794ms step_avg:57.70ms
step:1904/2160 train_time:109882ms step_avg:57.71ms
step:1905/2160 train_time:109971ms step_avg:57.73ms
step:1906/2160 train_time:110058ms step_avg:57.74ms
step:1907/2160 train_time:110148ms step_avg:57.76ms
step:1908/2160 train_time:110235ms step_avg:57.78ms
step:1909/2160 train_time:110325ms step_avg:57.79ms
step:1910/2160 train_time:110413ms step_avg:57.81ms
step:1911/2160 train_time:110502ms step_avg:57.82ms
step:1912/2160 train_time:110590ms step_avg:57.84ms
step:1913/2160 train_time:110679ms step_avg:57.86ms
step:1914/2160 train_time:110766ms step_avg:57.87ms
step:1915/2160 train_time:110856ms step_avg:57.89ms
step:1916/2160 train_time:110944ms step_avg:57.90ms
step:1917/2160 train_time:111033ms step_avg:57.92ms
step:1918/2160 train_time:111119ms step_avg:57.94ms
step:1919/2160 train_time:111209ms step_avg:57.95ms
step:1920/2160 train_time:111296ms step_avg:57.97ms
step:1921/2160 train_time:111386ms step_avg:57.98ms
step:1922/2160 train_time:111473ms step_avg:58.00ms
step:1923/2160 train_time:111563ms step_avg:58.01ms
step:1924/2160 train_time:111650ms step_avg:58.03ms
step:1925/2160 train_time:111739ms step_avg:58.05ms
step:1926/2160 train_time:111826ms step_avg:58.06ms
step:1927/2160 train_time:111916ms step_avg:58.08ms
step:1928/2160 train_time:112004ms step_avg:58.09ms
step:1929/2160 train_time:112094ms step_avg:58.11ms
step:1930/2160 train_time:112181ms step_avg:58.12ms
step:1931/2160 train_time:112270ms step_avg:58.14ms
step:1932/2160 train_time:112357ms step_avg:58.16ms
step:1933/2160 train_time:112447ms step_avg:58.17ms
step:1934/2160 train_time:112535ms step_avg:58.19ms
step:1935/2160 train_time:112624ms step_avg:58.20ms
step:1936/2160 train_time:112711ms step_avg:58.22ms
step:1937/2160 train_time:112801ms step_avg:58.23ms
step:1938/2160 train_time:112888ms step_avg:58.25ms
step:1939/2160 train_time:112978ms step_avg:58.27ms
step:1940/2160 train_time:113066ms step_avg:58.28ms
step:1941/2160 train_time:113156ms step_avg:58.30ms
step:1942/2160 train_time:113243ms step_avg:58.31ms
step:1943/2160 train_time:113332ms step_avg:58.33ms
step:1944/2160 train_time:113420ms step_avg:58.34ms
step:1945/2160 train_time:113509ms step_avg:58.36ms
step:1946/2160 train_time:113596ms step_avg:58.37ms
step:1947/2160 train_time:113686ms step_avg:58.39ms
step:1948/2160 train_time:113773ms step_avg:58.41ms
step:1949/2160 train_time:113862ms step_avg:58.42ms
step:1950/2160 train_time:113950ms step_avg:58.44ms
step:1951/2160 train_time:114040ms step_avg:58.45ms
step:1952/2160 train_time:114128ms step_avg:58.47ms
step:1953/2160 train_time:114218ms step_avg:58.48ms
step:1954/2160 train_time:114305ms step_avg:58.50ms
step:1955/2160 train_time:114394ms step_avg:58.51ms
step:1956/2160 train_time:114481ms step_avg:58.53ms
step:1957/2160 train_time:114569ms step_avg:58.54ms
step:1958/2160 train_time:114657ms step_avg:58.56ms
step:1959/2160 train_time:114746ms step_avg:58.57ms
step:1960/2160 train_time:114833ms step_avg:58.59ms
step:1961/2160 train_time:114924ms step_avg:58.60ms
step:1962/2160 train_time:115011ms step_avg:58.62ms
step:1963/2160 train_time:115100ms step_avg:58.63ms
step:1964/2160 train_time:115188ms step_avg:58.65ms
step:1965/2160 train_time:115277ms step_avg:58.67ms
step:1966/2160 train_time:115365ms step_avg:58.68ms
step:1967/2160 train_time:115454ms step_avg:58.70ms
step:1968/2160 train_time:115541ms step_avg:58.71ms
step:1969/2160 train_time:115630ms step_avg:58.73ms
step:1970/2160 train_time:115717ms step_avg:58.74ms
step:1971/2160 train_time:115807ms step_avg:58.76ms
step:1972/2160 train_time:115896ms step_avg:58.77ms
step:1973/2160 train_time:115985ms step_avg:58.79ms
step:1974/2160 train_time:116073ms step_avg:58.80ms
step:1975/2160 train_time:116162ms step_avg:58.82ms
step:1976/2160 train_time:116249ms step_avg:58.83ms
step:1977/2160 train_time:116338ms step_avg:58.85ms
step:1978/2160 train_time:116425ms step_avg:58.86ms
step:1979/2160 train_time:116515ms step_avg:58.88ms
step:1980/2160 train_time:116603ms step_avg:58.89ms
step:1981/2160 train_time:116693ms step_avg:58.91ms
step:1982/2160 train_time:116780ms step_avg:58.92ms
step:1983/2160 train_time:116870ms step_avg:58.94ms
step:1984/2160 train_time:116959ms step_avg:58.95ms
step:1985/2160 train_time:117048ms step_avg:58.97ms
step:1986/2160 train_time:117135ms step_avg:58.98ms
step:1987/2160 train_time:117224ms step_avg:59.00ms
step:1988/2160 train_time:117312ms step_avg:59.01ms
step:1989/2160 train_time:117402ms step_avg:59.03ms
step:1990/2160 train_time:117490ms step_avg:59.04ms
step:1991/2160 train_time:117579ms step_avg:59.06ms
step:1992/2160 train_time:117666ms step_avg:59.07ms
step:1993/2160 train_time:117756ms step_avg:59.08ms
step:1994/2160 train_time:117843ms step_avg:59.10ms
step:1995/2160 train_time:117933ms step_avg:59.11ms
step:1996/2160 train_time:118021ms step_avg:59.13ms
step:1997/2160 train_time:118110ms step_avg:59.14ms
step:1998/2160 train_time:118198ms step_avg:59.16ms
step:1999/2160 train_time:118286ms step_avg:59.17ms
step:2000/2160 train_time:118373ms step_avg:59.19ms
step:2000/2160 val_loss:3.3168 train_time:118463ms step_avg:59.23ms
step:2001/2160 train_time:118482ms step_avg:59.21ms
step:2002/2160 train_time:118555ms step_avg:59.22ms
step:2003/2160 train_time:118648ms step_avg:59.24ms
step:2004/2160 train_time:118735ms step_avg:59.25ms
step:2005/2160 train_time:118824ms step_avg:59.26ms
step:2006/2160 train_time:118911ms step_avg:59.28ms
step:2007/2160 train_time:118999ms step_avg:59.29ms
step:2008/2160 train_time:119086ms step_avg:59.31ms
step:2009/2160 train_time:119175ms step_avg:59.32ms
step:2010/2160 train_time:119263ms step_avg:59.33ms
step:2011/2160 train_time:119351ms step_avg:59.35ms
step:2012/2160 train_time:119442ms step_avg:59.36ms
step:2013/2160 train_time:119533ms step_avg:59.38ms
step:2014/2160 train_time:119622ms step_avg:59.40ms
step:2015/2160 train_time:119713ms step_avg:59.41ms
step:2016/2160 train_time:119801ms step_avg:59.42ms
step:2017/2160 train_time:119892ms step_avg:59.44ms
step:2018/2160 train_time:119977ms step_avg:59.45ms
step:2019/2160 train_time:120066ms step_avg:59.47ms
step:2020/2160 train_time:120153ms step_avg:59.48ms
step:2021/2160 train_time:120242ms step_avg:59.50ms
step:2022/2160 train_time:120329ms step_avg:59.51ms
step:2023/2160 train_time:120419ms step_avg:59.53ms
step:2024/2160 train_time:120508ms step_avg:59.54ms
step:2025/2160 train_time:120598ms step_avg:59.55ms
step:2026/2160 train_time:120686ms step_avg:59.57ms
step:2027/2160 train_time:120776ms step_avg:59.58ms
step:2028/2160 train_time:120863ms step_avg:59.60ms
step:2029/2160 train_time:120952ms step_avg:59.61ms
step:2030/2160 train_time:121039ms step_avg:59.63ms
step:2031/2160 train_time:121128ms step_avg:59.64ms
step:2032/2160 train_time:121214ms step_avg:59.65ms
step:2033/2160 train_time:121303ms step_avg:59.67ms
step:2034/2160 train_time:121391ms step_avg:59.68ms
step:2035/2160 train_time:121480ms step_avg:59.70ms
step:2036/2160 train_time:121568ms step_avg:59.71ms
step:2037/2160 train_time:121658ms step_avg:59.72ms
step:2038/2160 train_time:121747ms step_avg:59.74ms
step:2039/2160 train_time:121837ms step_avg:59.75ms
step:2040/2160 train_time:121924ms step_avg:59.77ms
step:2041/2160 train_time:122013ms step_avg:59.78ms
step:2042/2160 train_time:122099ms step_avg:59.79ms
step:2043/2160 train_time:122188ms step_avg:59.81ms
step:2044/2160 train_time:122276ms step_avg:59.82ms
step:2045/2160 train_time:122364ms step_avg:59.84ms
step:2046/2160 train_time:122452ms step_avg:59.85ms
step:2047/2160 train_time:122543ms step_avg:59.86ms
step:2048/2160 train_time:122631ms step_avg:59.88ms
step:2049/2160 train_time:122722ms step_avg:59.89ms
step:2050/2160 train_time:122810ms step_avg:59.91ms
step:2051/2160 train_time:122899ms step_avg:59.92ms
step:2052/2160 train_time:122986ms step_avg:59.93ms
step:2053/2160 train_time:123075ms step_avg:59.95ms
step:2054/2160 train_time:123162ms step_avg:59.96ms
step:2055/2160 train_time:123252ms step_avg:59.98ms
step:2056/2160 train_time:123338ms step_avg:59.99ms
step:2057/2160 train_time:123427ms step_avg:60.00ms
step:2058/2160 train_time:123514ms step_avg:60.02ms
step:2059/2160 train_time:123604ms step_avg:60.03ms
step:2060/2160 train_time:123691ms step_avg:60.04ms
step:2061/2160 train_time:123781ms step_avg:60.06ms
step:2062/2160 train_time:123869ms step_avg:60.07ms
step:2063/2160 train_time:123957ms step_avg:60.09ms
step:2064/2160 train_time:124045ms step_avg:60.10ms
step:2065/2160 train_time:124134ms step_avg:60.11ms
step:2066/2160 train_time:124221ms step_avg:60.13ms
step:2067/2160 train_time:124310ms step_avg:60.14ms
step:2068/2160 train_time:124397ms step_avg:60.15ms
step:2069/2160 train_time:124487ms step_avg:60.17ms
step:2070/2160 train_time:124574ms step_avg:60.18ms
step:2071/2160 train_time:124665ms step_avg:60.20ms
step:2072/2160 train_time:124753ms step_avg:60.21ms
step:2073/2160 train_time:124844ms step_avg:60.22ms
step:2074/2160 train_time:124932ms step_avg:60.24ms
step:2075/2160 train_time:125020ms step_avg:60.25ms
step:2076/2160 train_time:125107ms step_avg:60.26ms
step:2077/2160 train_time:125196ms step_avg:60.28ms
step:2078/2160 train_time:125284ms step_avg:60.29ms
step:2079/2160 train_time:125373ms step_avg:60.30ms
step:2080/2160 train_time:125461ms step_avg:60.32ms
step:2081/2160 train_time:125550ms step_avg:60.33ms
step:2082/2160 train_time:125638ms step_avg:60.34ms
step:2083/2160 train_time:125728ms step_avg:60.36ms
step:2084/2160 train_time:125817ms step_avg:60.37ms
step:2085/2160 train_time:125907ms step_avg:60.39ms
step:2086/2160 train_time:125994ms step_avg:60.40ms
step:2087/2160 train_time:126084ms step_avg:60.41ms
step:2088/2160 train_time:126171ms step_avg:60.43ms
step:2089/2160 train_time:126261ms step_avg:60.44ms
step:2090/2160 train_time:126349ms step_avg:60.45ms
step:2091/2160 train_time:126438ms step_avg:60.47ms
step:2092/2160 train_time:126525ms step_avg:60.48ms
step:2093/2160 train_time:126614ms step_avg:60.49ms
step:2094/2160 train_time:126702ms step_avg:60.51ms
step:2095/2160 train_time:126792ms step_avg:60.52ms
step:2096/2160 train_time:126879ms step_avg:60.53ms
step:2097/2160 train_time:126969ms step_avg:60.55ms
step:2098/2160 train_time:127056ms step_avg:60.56ms
step:2099/2160 train_time:127145ms step_avg:60.57ms
step:2100/2160 train_time:127232ms step_avg:60.59ms
step:2101/2160 train_time:127321ms step_avg:60.60ms
step:2102/2160 train_time:127408ms step_avg:60.61ms
step:2103/2160 train_time:127498ms step_avg:60.63ms
step:2104/2160 train_time:127585ms step_avg:60.64ms
step:2105/2160 train_time:127675ms step_avg:60.65ms
step:2106/2160 train_time:127764ms step_avg:60.67ms
step:2107/2160 train_time:127854ms step_avg:60.68ms
step:2108/2160 train_time:127942ms step_avg:60.69ms
step:2109/2160 train_time:128031ms step_avg:60.71ms
step:2110/2160 train_time:128118ms step_avg:60.72ms
step:2111/2160 train_time:128207ms step_avg:60.73ms
step:2112/2160 train_time:128295ms step_avg:60.75ms
step:2113/2160 train_time:128386ms step_avg:60.76ms
step:2114/2160 train_time:128475ms step_avg:60.77ms
step:2115/2160 train_time:128564ms step_avg:60.79ms
step:2116/2160 train_time:128651ms step_avg:60.80ms
step:2117/2160 train_time:128739ms step_avg:60.81ms
step:2118/2160 train_time:128827ms step_avg:60.82ms
step:2119/2160 train_time:128915ms step_avg:60.84ms
step:2120/2160 train_time:129002ms step_avg:60.85ms
step:2121/2160 train_time:129092ms step_avg:60.86ms
step:2122/2160 train_time:129180ms step_avg:60.88ms
step:2123/2160 train_time:129270ms step_avg:60.89ms
step:2124/2160 train_time:129357ms step_avg:60.90ms
step:2125/2160 train_time:129447ms step_avg:60.92ms
step:2126/2160 train_time:129534ms step_avg:60.93ms
step:2127/2160 train_time:129625ms step_avg:60.94ms
step:2128/2160 train_time:129713ms step_avg:60.96ms
step:2129/2160 train_time:129803ms step_avg:60.97ms
step:2130/2160 train_time:129890ms step_avg:60.98ms
step:2131/2160 train_time:129979ms step_avg:60.99ms
step:2132/2160 train_time:130067ms step_avg:61.01ms
step:2133/2160 train_time:130157ms step_avg:61.02ms
step:2134/2160 train_time:130244ms step_avg:61.03ms
step:2135/2160 train_time:130334ms step_avg:61.05ms
step:2136/2160 train_time:130421ms step_avg:61.06ms
step:2137/2160 train_time:130510ms step_avg:61.07ms
step:2138/2160 train_time:130598ms step_avg:61.08ms
step:2139/2160 train_time:130688ms step_avg:61.10ms
step:2140/2160 train_time:130777ms step_avg:61.11ms
step:2141/2160 train_time:130867ms step_avg:61.12ms
step:2142/2160 train_time:130953ms step_avg:61.14ms
step:2143/2160 train_time:131043ms step_avg:61.15ms
step:2144/2160 train_time:131131ms step_avg:61.16ms
step:2145/2160 train_time:131221ms step_avg:61.18ms
step:2146/2160 train_time:131310ms step_avg:61.19ms
step:2147/2160 train_time:131399ms step_avg:61.20ms
step:2148/2160 train_time:131486ms step_avg:61.21ms
step:2149/2160 train_time:131575ms step_avg:61.23ms
step:2150/2160 train_time:131663ms step_avg:61.24ms
step:2151/2160 train_time:131754ms step_avg:61.25ms
step:2152/2160 train_time:131842ms step_avg:61.26ms
step:2153/2160 train_time:131931ms step_avg:61.28ms
step:2154/2160 train_time:132019ms step_avg:61.29ms
step:2155/2160 train_time:132108ms step_avg:61.30ms
step:2156/2160 train_time:132196ms step_avg:61.32ms
step:2157/2160 train_time:132285ms step_avg:61.33ms
step:2158/2160 train_time:132373ms step_avg:61.34ms
step:2159/2160 train_time:132462ms step_avg:61.35ms
step:2160/2160 train_time:132551ms step_avg:61.37ms
step:2160/2160 val_loss:3.2808 train_time:132642ms step_avg:61.41ms
peak memory allocated: 29740 MiB reserved: 61160 MiB

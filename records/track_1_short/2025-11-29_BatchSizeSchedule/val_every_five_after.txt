import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 5  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec  5 21:19:14 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   32C    P0            120W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   29C    P0            117W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   29C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   32C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   34C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   28C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          186958      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    0   N/A  N/A          186959      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          186960      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          186961      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          186962      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          186963      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          186964      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          186965      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    1   N/A  N/A          186959      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    2   N/A  N/A          186960      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    3   N/A  N/A          186961      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    4   N/A  N/A          186962      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    5   N/A  N/A          186963      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    6   N/A  N/A          186964      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    7   N/A  N/A          186965      C   /home/ubuntu/.venv/bin/python3         1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:109ms step_avg:109.17ms
step:2/2160 train_time:153ms step_avg:76.28ms
step:3/2160 train_time:174ms step_avg:58.00ms
step:4/2160 train_time:197ms step_avg:49.15ms
step:5/2160 train_time:219ms step_avg:43.79ms
step:5/2160 val_loss:7.9620 train_time:252ms step_avg:50.39ms
step:6/2160 train_time:278ms step_avg:46.29ms
step:7/2160 train_time:298ms step_avg:42.64ms
step:8/2160 train_time:321ms step_avg:40.14ms
step:9/2160 train_time:356ms step_avg:39.52ms
step:10/2160 train_time:392ms step_avg:39.18ms
step:10/2160 val_loss:7.0051 train_time:431ms step_avg:43.08ms
step:11/2160 train_time:453ms step_avg:41.21ms
step:12/2160 train_time:476ms step_avg:39.63ms
step:13/2160 train_time:500ms step_avg:38.47ms
step:14/2160 train_time:534ms step_avg:38.12ms
step:15/2160 train_time:572ms step_avg:38.15ms
step:15/2160 val_loss:6.5731 train_time:608ms step_avg:40.55ms
step:16/2160 train_time:632ms step_avg:39.51ms
step:17/2160 train_time:653ms step_avg:38.41ms
step:18/2160 train_time:680ms step_avg:37.77ms
step:19/2160 train_time:715ms step_avg:37.64ms
step:20/2160 train_time:751ms step_avg:37.53ms
step:20/2160 val_loss:6.3531 train_time:789ms step_avg:39.43ms
step:21/2160 train_time:812ms step_avg:38.65ms
step:22/2160 train_time:834ms step_avg:37.91ms
step:23/2160 train_time:858ms step_avg:37.30ms
step:24/2160 train_time:891ms step_avg:37.14ms
step:25/2160 train_time:929ms step_avg:37.18ms
step:25/2160 val_loss:6.1936 train_time:966ms step_avg:38.64ms
step:26/2160 train_time:990ms step_avg:38.07ms
step:27/2160 train_time:1010ms step_avg:37.42ms
step:28/2160 train_time:1036ms step_avg:37.02ms
step:29/2160 train_time:1073ms step_avg:36.99ms
step:30/2160 train_time:1108ms step_avg:36.93ms
step:30/2160 val_loss:6.0405 train_time:1146ms step_avg:38.19ms
step:31/2160 train_time:1168ms step_avg:37.67ms
step:32/2160 train_time:1190ms step_avg:37.19ms
step:33/2160 train_time:1216ms step_avg:36.84ms
step:34/2160 train_time:1249ms step_avg:36.74ms
step:35/2160 train_time:1287ms step_avg:36.76ms
step:35/2160 val_loss:5.9063 train_time:1322ms step_avg:37.77ms
step:36/2160 train_time:1346ms step_avg:37.38ms
step:37/2160 train_time:1366ms step_avg:36.93ms
step:38/2160 train_time:1392ms step_avg:36.62ms
step:39/2160 train_time:1427ms step_avg:36.59ms
step:40/2160 train_time:1462ms step_avg:36.56ms
step:40/2160 val_loss:5.7921 train_time:1501ms step_avg:37.52ms
step:41/2160 train_time:1523ms step_avg:37.15ms
step:42/2160 train_time:1545ms step_avg:36.79ms
step:43/2160 train_time:1571ms step_avg:36.53ms
step:44/2160 train_time:1605ms step_avg:36.47ms
step:45/2160 train_time:1642ms step_avg:36.49ms
step:45/2160 val_loss:5.6811 train_time:1679ms step_avg:37.31ms
step:46/2160 train_time:1702ms step_avg:37.01ms
step:47/2160 train_time:1723ms step_avg:36.66ms
step:48/2160 train_time:1749ms step_avg:36.43ms
step:49/2160 train_time:1784ms step_avg:36.41ms
step:50/2160 train_time:1819ms step_avg:36.38ms
step:50/2160 val_loss:5.6041 train_time:1857ms step_avg:37.14ms
step:51/2160 train_time:1879ms step_avg:36.84ms
step:52/2160 train_time:1901ms step_avg:36.56ms
step:53/2160 train_time:1927ms step_avg:36.35ms
step:54/2160 train_time:1961ms step_avg:36.31ms
step:55/2160 train_time:1997ms step_avg:36.32ms
step:55/2160 val_loss:5.5145 train_time:2033ms step_avg:36.96ms
step:56/2160 train_time:2057ms step_avg:36.73ms
step:57/2160 train_time:2077ms step_avg:36.44ms
step:58/2160 train_time:2105ms step_avg:36.30ms
step:59/2160 train_time:2140ms step_avg:36.28ms
step:60/2160 train_time:2174ms step_avg:36.24ms
step:60/2160 val_loss:5.4729 train_time:2211ms step_avg:36.85ms
step:61/2160 train_time:2233ms step_avg:36.61ms
step:62/2160 train_time:2255ms step_avg:36.37ms
step:63/2160 train_time:2281ms step_avg:36.20ms
step:64/2160 train_time:2316ms step_avg:36.18ms
step:65/2160 train_time:2353ms step_avg:36.20ms
step:65/2160 val_loss:5.3883 train_time:2388ms step_avg:36.73ms
step:66/2160 train_time:2411ms step_avg:36.53ms
step:67/2160 train_time:2432ms step_avg:36.29ms
step:68/2160 train_time:2459ms step_avg:36.16ms
step:69/2160 train_time:2494ms step_avg:36.15ms
step:70/2160 train_time:2529ms step_avg:36.12ms
step:70/2160 val_loss:5.3414 train_time:2566ms step_avg:36.65ms
step:71/2160 train_time:2588ms step_avg:36.45ms
step:72/2160 train_time:2610ms step_avg:36.25ms
step:73/2160 train_time:2635ms step_avg:36.10ms
step:74/2160 train_time:2670ms step_avg:36.08ms
step:75/2160 train_time:2706ms step_avg:36.08ms
step:75/2160 val_loss:5.2780 train_time:2742ms step_avg:36.56ms
step:76/2160 train_time:2765ms step_avg:36.38ms
step:77/2160 train_time:2786ms step_avg:36.18ms
step:78/2160 train_time:2811ms step_avg:36.04ms
step:79/2160 train_time:2847ms step_avg:36.04ms
step:80/2160 train_time:2882ms step_avg:36.02ms
step:80/2160 val_loss:5.2317 train_time:2920ms step_avg:36.50ms
step:81/2160 train_time:2942ms step_avg:36.33ms
step:82/2160 train_time:2964ms step_avg:36.15ms
step:83/2160 train_time:2991ms step_avg:36.04ms
step:84/2160 train_time:3026ms step_avg:36.02ms
step:85/2160 train_time:3063ms step_avg:36.03ms
step:85/2160 val_loss:5.1782 train_time:3098ms step_avg:36.45ms
step:86/2160 train_time:3122ms step_avg:36.30ms
step:87/2160 train_time:3143ms step_avg:36.12ms
step:88/2160 train_time:3169ms step_avg:36.01ms
step:89/2160 train_time:3206ms step_avg:36.02ms
step:90/2160 train_time:3241ms step_avg:36.01ms
step:90/2160 val_loss:5.1462 train_time:3278ms step_avg:36.42ms
step:91/2160 train_time:3301ms step_avg:36.27ms
step:92/2160 train_time:3323ms step_avg:36.12ms
step:93/2160 train_time:3348ms step_avg:36.00ms
step:94/2160 train_time:3383ms step_avg:35.99ms
step:95/2160 train_time:3420ms step_avg:36.00ms
step:95/2160 val_loss:5.0848 train_time:3455ms step_avg:36.37ms
step:96/2160 train_time:3478ms step_avg:36.23ms
step:97/2160 train_time:3499ms step_avg:36.07ms
step:98/2160 train_time:3527ms step_avg:35.99ms
step:99/2160 train_time:3562ms step_avg:35.98ms
step:100/2160 train_time:3597ms step_avg:35.97ms
step:100/2160 val_loss:5.0495 train_time:3634ms step_avg:36.34ms
step:101/2160 train_time:3657ms step_avg:36.20ms
step:102/2160 train_time:3679ms step_avg:36.06ms
step:103/2160 train_time:3704ms step_avg:35.96ms
step:104/2160 train_time:3738ms step_avg:35.95ms
step:105/2160 train_time:3775ms step_avg:35.95ms
step:105/2160 val_loss:5.0013 train_time:3810ms step_avg:36.29ms
step:106/2160 train_time:3835ms step_avg:36.18ms
step:107/2160 train_time:3855ms step_avg:36.03ms
step:108/2160 train_time:3880ms step_avg:35.93ms
step:109/2160 train_time:3916ms step_avg:35.92ms
step:110/2160 train_time:3950ms step_avg:35.91ms
step:110/2160 val_loss:4.9678 train_time:3987ms step_avg:36.25ms
step:111/2160 train_time:4009ms step_avg:36.12ms
step:112/2160 train_time:4031ms step_avg:35.99ms
step:113/2160 train_time:4057ms step_avg:35.90ms
step:114/2160 train_time:4092ms step_avg:35.90ms
step:115/2160 train_time:4130ms step_avg:35.91ms
step:115/2160 val_loss:4.9142 train_time:4166ms step_avg:36.22ms
step:116/2160 train_time:4189ms step_avg:36.11ms
step:117/2160 train_time:4209ms step_avg:35.98ms
step:118/2160 train_time:4237ms step_avg:35.91ms
step:119/2160 train_time:4272ms step_avg:35.90ms
step:120/2160 train_time:4307ms step_avg:35.89ms
step:120/2160 val_loss:4.8811 train_time:4344ms step_avg:36.20ms
step:121/2160 train_time:4366ms step_avg:36.08ms
step:122/2160 train_time:4388ms step_avg:35.97ms
step:123/2160 train_time:4416ms step_avg:35.91ms
step:124/2160 train_time:4450ms step_avg:35.89ms
step:125/2160 train_time:4486ms step_avg:35.89ms
step:125/2160 val_loss:4.8281 train_time:4521ms step_avg:36.17ms
step:126/2160 train_time:4546ms step_avg:36.08ms
step:127/2160 train_time:4567ms step_avg:35.96ms
step:128/2160 train_time:4591ms step_avg:35.87ms
step:129/2160 train_time:4627ms step_avg:35.87ms
step:130/2160 train_time:4661ms step_avg:35.85ms
step:130/2160 val_loss:4.8005 train_time:4700ms step_avg:36.15ms
step:131/2160 train_time:4722ms step_avg:36.05ms
step:132/2160 train_time:4744ms step_avg:35.94ms
step:133/2160 train_time:4770ms step_avg:35.86ms
step:134/2160 train_time:4803ms step_avg:35.85ms
step:135/2160 train_time:4839ms step_avg:35.84ms
step:135/2160 val_loss:4.7626 train_time:4874ms step_avg:36.10ms
step:136/2160 train_time:4897ms step_avg:36.01ms
step:137/2160 train_time:4918ms step_avg:35.90ms
step:138/2160 train_time:4944ms step_avg:35.83ms
step:139/2160 train_time:4979ms step_avg:35.82ms
step:140/2160 train_time:5014ms step_avg:35.81ms
step:140/2160 val_loss:4.7426 train_time:5052ms step_avg:36.08ms
step:141/2160 train_time:5075ms step_avg:36.00ms
step:142/2160 train_time:5098ms step_avg:35.90ms
step:143/2160 train_time:5121ms step_avg:35.81ms
step:144/2160 train_time:5155ms step_avg:35.80ms
step:145/2160 train_time:5193ms step_avg:35.81ms
step:145/2160 val_loss:4.6888 train_time:5229ms step_avg:36.06ms
step:146/2160 train_time:5252ms step_avg:35.97ms
step:147/2160 train_time:5273ms step_avg:35.87ms
step:148/2160 train_time:5298ms step_avg:35.80ms
step:149/2160 train_time:5334ms step_avg:35.80ms
step:150/2160 train_time:5369ms step_avg:35.79ms
step:150/2160 val_loss:4.6553 train_time:5406ms step_avg:36.04ms
step:151/2160 train_time:5428ms step_avg:35.95ms
step:152/2160 train_time:5450ms step_avg:35.85ms
step:153/2160 train_time:5476ms step_avg:35.79ms
step:154/2160 train_time:5511ms step_avg:35.79ms
step:155/2160 train_time:5548ms step_avg:35.80ms
step:155/2160 val_loss:4.6220 train_time:5584ms step_avg:36.03ms
step:156/2160 train_time:5608ms step_avg:35.95ms
step:157/2160 train_time:5629ms step_avg:35.85ms
step:158/2160 train_time:5654ms step_avg:35.78ms
step:159/2160 train_time:5690ms step_avg:35.78ms
step:160/2160 train_time:5725ms step_avg:35.78ms
step:160/2160 val_loss:4.5945 train_time:5761ms step_avg:36.01ms
step:161/2160 train_time:5784ms step_avg:35.92ms
step:162/2160 train_time:5806ms step_avg:35.84ms
step:163/2160 train_time:5834ms step_avg:35.79ms
step:164/2160 train_time:5867ms step_avg:35.78ms
step:165/2160 train_time:5905ms step_avg:35.79ms
step:165/2160 val_loss:4.5654 train_time:5940ms step_avg:36.00ms
step:166/2160 train_time:5964ms step_avg:35.93ms
step:167/2160 train_time:5984ms step_avg:35.83ms
step:168/2160 train_time:6010ms step_avg:35.78ms
step:169/2160 train_time:6046ms step_avg:35.77ms
step:170/2160 train_time:6081ms step_avg:35.77ms
step:170/2160 val_loss:4.5508 train_time:6119ms step_avg:36.00ms
step:171/2160 train_time:6141ms step_avg:35.91ms
step:172/2160 train_time:6163ms step_avg:35.83ms
step:173/2160 train_time:6188ms step_avg:35.77ms
step:174/2160 train_time:6222ms step_avg:35.76ms
step:175/2160 train_time:6260ms step_avg:35.77ms
step:175/2160 val_loss:4.5189 train_time:6295ms step_avg:35.97ms
step:176/2160 train_time:6321ms step_avg:35.91ms
step:177/2160 train_time:6341ms step_avg:35.83ms
step:178/2160 train_time:6366ms step_avg:35.76ms
step:179/2160 train_time:6401ms step_avg:35.76ms
step:180/2160 train_time:6435ms step_avg:35.75ms
step:180/2160 val_loss:4.5011 train_time:6473ms step_avg:35.96ms
step:181/2160 train_time:6495ms step_avg:35.88ms
step:182/2160 train_time:6517ms step_avg:35.81ms
step:183/2160 train_time:6542ms step_avg:35.75ms
step:184/2160 train_time:6576ms step_avg:35.74ms
step:185/2160 train_time:6614ms step_avg:35.75ms
step:185/2160 val_loss:4.4753 train_time:6649ms step_avg:35.94ms
step:186/2160 train_time:6672ms step_avg:35.87ms
step:187/2160 train_time:6693ms step_avg:35.79ms
step:188/2160 train_time:6721ms step_avg:35.75ms
step:189/2160 train_time:6756ms step_avg:35.74ms
step:190/2160 train_time:6791ms step_avg:35.74ms
step:190/2160 val_loss:4.4705 train_time:6828ms step_avg:35.94ms
step:191/2160 train_time:6850ms step_avg:35.86ms
step:192/2160 train_time:6872ms step_avg:35.79ms
step:193/2160 train_time:6898ms step_avg:35.74ms
step:194/2160 train_time:6932ms step_avg:35.73ms
step:195/2160 train_time:6968ms step_avg:35.73ms
step:195/2160 val_loss:4.4664 train_time:7002ms step_avg:35.91ms
step:196/2160 train_time:7026ms step_avg:35.85ms
step:197/2160 train_time:7047ms step_avg:35.77ms
step:198/2160 train_time:7072ms step_avg:35.72ms
step:199/2160 train_time:7106ms step_avg:35.71ms
step:200/2160 train_time:7141ms step_avg:35.70ms
step:200/2160 val_loss:4.4572 train_time:7179ms step_avg:35.89ms
step:201/2160 train_time:7201ms step_avg:35.83ms
step:202/2160 train_time:7223ms step_avg:35.76ms
step:203/2160 train_time:7248ms step_avg:35.70ms
step:204/2160 train_time:7282ms step_avg:35.69ms
step:205/2160 train_time:7319ms step_avg:35.70ms
step:205/2160 val_loss:4.4260 train_time:7354ms step_avg:35.87ms
step:206/2160 train_time:7377ms step_avg:35.81ms
step:207/2160 train_time:7398ms step_avg:35.74ms
step:208/2160 train_time:7424ms step_avg:35.69ms
step:209/2160 train_time:7459ms step_avg:35.69ms
step:210/2160 train_time:7496ms step_avg:35.69ms
step:210/2160 val_loss:4.4096 train_time:7534ms step_avg:35.88ms
step:211/2160 train_time:7556ms step_avg:35.81ms
step:212/2160 train_time:7578ms step_avg:35.74ms
step:213/2160 train_time:7605ms step_avg:35.70ms
step:214/2160 train_time:7639ms step_avg:35.70ms
step:215/2160 train_time:7675ms step_avg:35.70ms
step:215/2160 val_loss:4.3879 train_time:7709ms step_avg:35.86ms
step:216/2160 train_time:7733ms step_avg:35.80ms
step:217/2160 train_time:7753ms step_avg:35.73ms
step:218/2160 train_time:7779ms step_avg:35.68ms
step:219/2160 train_time:7816ms step_avg:35.69ms
step:220/2160 train_time:7850ms step_avg:35.68ms
step:220/2160 val_loss:4.3781 train_time:7889ms step_avg:35.86ms
step:221/2160 train_time:7911ms step_avg:35.79ms
step:222/2160 train_time:7933ms step_avg:35.73ms
step:223/2160 train_time:7958ms step_avg:35.69ms
step:224/2160 train_time:7992ms step_avg:35.68ms
step:225/2160 train_time:8028ms step_avg:35.68ms
step:225/2160 val_loss:4.3634 train_time:8063ms step_avg:35.84ms
step:226/2160 train_time:8087ms step_avg:35.78ms
step:227/2160 train_time:8107ms step_avg:35.71ms
step:228/2160 train_time:8133ms step_avg:35.67ms
step:229/2160 train_time:8170ms step_avg:35.68ms
step:230/2160 train_time:8205ms step_avg:35.67ms
step:230/2160 val_loss:4.3568 train_time:8242ms step_avg:35.84ms
step:231/2160 train_time:8265ms step_avg:35.78ms
step:232/2160 train_time:8287ms step_avg:35.72ms
step:233/2160 train_time:8313ms step_avg:35.68ms
step:234/2160 train_time:8348ms step_avg:35.67ms
step:235/2160 train_time:8384ms step_avg:35.68ms
step:235/2160 val_loss:4.3367 train_time:8419ms step_avg:35.83ms
step:236/2160 train_time:8443ms step_avg:35.78ms
step:237/2160 train_time:8463ms step_avg:35.71ms
step:238/2160 train_time:8489ms step_avg:35.67ms
step:239/2160 train_time:8525ms step_avg:35.67ms
step:240/2160 train_time:8560ms step_avg:35.67ms
step:240/2160 val_loss:4.3297 train_time:8598ms step_avg:35.82ms
step:241/2160 train_time:8620ms step_avg:35.77ms
step:242/2160 train_time:8642ms step_avg:35.71ms
step:243/2160 train_time:8671ms step_avg:35.68ms
step:244/2160 train_time:8705ms step_avg:35.68ms
step:245/2160 train_time:8743ms step_avg:35.68ms
step:245/2160 val_loss:4.3180 train_time:8778ms step_avg:35.83ms
step:246/2160 train_time:8802ms step_avg:35.78ms
step:247/2160 train_time:8822ms step_avg:35.72ms
step:248/2160 train_time:8849ms step_avg:35.68ms
step:249/2160 train_time:8884ms step_avg:35.68ms
step:250/2160 train_time:8919ms step_avg:35.67ms
step:250/2160 val_loss:4.3139 train_time:8955ms step_avg:35.82ms
step:251/2160 train_time:8977ms step_avg:35.77ms
step:252/2160 train_time:8999ms step_avg:35.71ms
step:253/2160 train_time:9025ms step_avg:35.67ms
step:254/2160 train_time:9058ms step_avg:35.66ms
step:255/2160 train_time:9095ms step_avg:35.67ms
step:255/2160 val_loss:4.2991 train_time:9129ms step_avg:35.80ms
step:256/2160 train_time:9153ms step_avg:35.75ms
step:257/2160 train_time:9173ms step_avg:35.69ms
step:258/2160 train_time:9199ms step_avg:35.66ms
step:259/2160 train_time:9235ms step_avg:35.66ms
step:260/2160 train_time:9271ms step_avg:35.66ms
step:260/2160 val_loss:4.2918 train_time:9308ms step_avg:35.80ms
step:261/2160 train_time:9330ms step_avg:35.75ms
step:262/2160 train_time:9352ms step_avg:35.69ms
step:263/2160 train_time:9377ms step_avg:35.65ms
step:264/2160 train_time:9411ms step_avg:35.65ms
step:265/2160 train_time:9449ms step_avg:35.66ms
step:265/2160 val_loss:4.2796 train_time:9485ms step_avg:35.79ms
step:266/2160 train_time:9509ms step_avg:35.75ms
step:267/2160 train_time:9529ms step_avg:35.69ms
step:268/2160 train_time:9555ms step_avg:35.65ms
step:269/2160 train_time:9590ms step_avg:35.65ms
step:270/2160 train_time:9625ms step_avg:35.65ms
step:270/2160 val_loss:4.2790 train_time:9662ms step_avg:35.79ms
step:271/2160 train_time:9685ms step_avg:35.74ms
step:272/2160 train_time:9707ms step_avg:35.69ms
step:273/2160 train_time:9735ms step_avg:35.66ms
step:274/2160 train_time:9768ms step_avg:35.65ms
step:275/2160 train_time:9805ms step_avg:35.66ms
step:275/2160 val_loss:4.2663 train_time:9841ms step_avg:35.78ms
step:276/2160 train_time:9864ms step_avg:35.74ms
step:277/2160 train_time:9885ms step_avg:35.68ms
step:278/2160 train_time:9910ms step_avg:35.65ms
step:279/2160 train_time:9946ms step_avg:35.65ms
step:280/2160 train_time:9980ms step_avg:35.64ms
step:280/2160 val_loss:4.2604 train_time:10019ms step_avg:35.78ms
step:281/2160 train_time:10041ms step_avg:35.73ms
step:282/2160 train_time:10063ms step_avg:35.69ms
step:283/2160 train_time:10089ms step_avg:35.65ms
step:284/2160 train_time:10123ms step_avg:35.64ms
step:285/2160 train_time:10159ms step_avg:35.65ms
step:285/2160 val_loss:4.2481 train_time:10193ms step_avg:35.77ms
step:286/2160 train_time:10217ms step_avg:35.72ms
step:287/2160 train_time:10237ms step_avg:35.67ms
step:288/2160 train_time:10264ms step_avg:35.64ms
step:289/2160 train_time:10301ms step_avg:35.64ms
step:290/2160 train_time:10335ms step_avg:35.64ms
step:290/2160 val_loss:4.2452 train_time:10372ms step_avg:35.76ms
step:291/2160 train_time:10394ms step_avg:35.72ms
step:292/2160 train_time:10416ms step_avg:35.67ms
step:293/2160 train_time:10443ms step_avg:35.64ms
step:294/2160 train_time:10476ms step_avg:35.63ms
step:295/2160 train_time:10512ms step_avg:35.63ms
step:295/2160 val_loss:4.2350 train_time:10548ms step_avg:35.75ms
step:296/2160 train_time:10571ms step_avg:35.71ms
step:297/2160 train_time:10592ms step_avg:35.66ms
step:298/2160 train_time:10618ms step_avg:35.63ms
step:299/2160 train_time:10654ms step_avg:35.63ms
step:300/2160 train_time:10687ms step_avg:35.62ms
step:300/2160 val_loss:4.2338 train_time:10724ms step_avg:35.75ms
step:301/2160 train_time:10747ms step_avg:35.70ms
step:302/2160 train_time:10768ms step_avg:35.66ms
step:303/2160 train_time:10794ms step_avg:35.62ms
step:304/2160 train_time:10828ms step_avg:35.62ms
step:305/2160 train_time:10864ms step_avg:35.62ms
step:305/2160 val_loss:4.2210 train_time:10898ms step_avg:35.73ms
step:306/2160 train_time:10922ms step_avg:35.69ms
step:307/2160 train_time:10943ms step_avg:35.64ms
step:308/2160 train_time:10969ms step_avg:35.61ms
step:309/2160 train_time:11005ms step_avg:35.61ms
step:310/2160 train_time:11039ms step_avg:35.61ms
step:310/2160 val_loss:4.2220 train_time:11075ms step_avg:35.73ms
step:311/2160 train_time:11097ms step_avg:35.68ms
step:312/2160 train_time:11119ms step_avg:35.64ms
step:313/2160 train_time:11146ms step_avg:35.61ms
step:314/2160 train_time:11179ms step_avg:35.60ms
step:315/2160 train_time:11216ms step_avg:35.61ms
step:315/2160 val_loss:4.2209 train_time:11251ms step_avg:35.72ms
step:316/2160 train_time:11274ms step_avg:35.68ms
step:317/2160 train_time:11295ms step_avg:35.63ms
step:318/2160 train_time:11321ms step_avg:35.60ms
step:319/2160 train_time:11355ms step_avg:35.60ms
step:320/2160 train_time:11390ms step_avg:35.59ms
step:320/2160 val_loss:4.2126 train_time:11426ms step_avg:35.71ms
step:321/2160 train_time:11448ms step_avg:35.66ms
step:322/2160 train_time:11470ms step_avg:35.62ms
step:323/2160 train_time:11496ms step_avg:35.59ms
step:324/2160 train_time:11530ms step_avg:35.59ms
step:325/2160 train_time:11567ms step_avg:35.59ms
step:325/2160 val_loss:4.1965 train_time:11602ms step_avg:35.70ms
step:326/2160 train_time:11625ms step_avg:35.66ms
step:327/2160 train_time:11645ms step_avg:35.61ms
step:328/2160 train_time:11672ms step_avg:35.58ms
step:329/2160 train_time:11707ms step_avg:35.58ms
step:330/2160 train_time:11742ms step_avg:35.58ms
step:330/2160 val_loss:4.1922 train_time:11779ms step_avg:35.69ms
step:331/2160 train_time:11801ms step_avg:35.65ms
step:332/2160 train_time:11822ms step_avg:35.61ms
step:333/2160 train_time:11848ms step_avg:35.58ms
step:334/2160 train_time:11883ms step_avg:35.58ms
step:335/2160 train_time:11920ms step_avg:35.58ms
step:335/2160 val_loss:4.1805 train_time:11955ms step_avg:35.69ms
step:336/2160 train_time:11979ms step_avg:35.65ms
step:337/2160 train_time:12000ms step_avg:35.61ms
step:338/2160 train_time:12025ms step_avg:35.58ms
step:339/2160 train_time:12061ms step_avg:35.58ms
step:340/2160 train_time:12095ms step_avg:35.57ms
step:340/2160 val_loss:4.1772 train_time:12131ms step_avg:35.68ms
step:341/2160 train_time:12153ms step_avg:35.64ms
step:342/2160 train_time:12175ms step_avg:35.60ms
step:343/2160 train_time:12202ms step_avg:35.57ms
step:344/2160 train_time:12235ms step_avg:35.57ms
step:345/2160 train_time:12273ms step_avg:35.57ms
step:345/2160 val_loss:4.1655 train_time:12308ms step_avg:35.68ms
step:346/2160 train_time:12332ms step_avg:35.64ms
step:347/2160 train_time:12352ms step_avg:35.60ms
step:348/2160 train_time:12378ms step_avg:35.57ms
step:349/2160 train_time:12414ms step_avg:35.57ms
step:350/2160 train_time:12448ms step_avg:35.57ms
step:350/2160 val_loss:4.1604 train_time:12486ms step_avg:35.67ms
step:351/2160 train_time:12507ms step_avg:35.63ms
step:352/2160 train_time:12529ms step_avg:35.59ms
step:353/2160 train_time:12556ms step_avg:35.57ms
step:354/2160 train_time:12589ms step_avg:35.56ms
step:355/2160 train_time:12626ms step_avg:35.57ms
step:355/2160 val_loss:4.1537 train_time:12661ms step_avg:35.66ms
step:356/2160 train_time:12686ms step_avg:35.64ms
step:357/2160 train_time:12707ms step_avg:35.59ms
step:358/2160 train_time:12731ms step_avg:35.56ms
step:359/2160 train_time:12766ms step_avg:35.56ms
step:360/2160 train_time:12800ms step_avg:35.55ms
step:360/2160 val_loss:4.1496 train_time:12836ms step_avg:35.66ms
step:361/2160 train_time:12858ms step_avg:35.62ms
step:362/2160 train_time:12880ms step_avg:35.58ms
step:363/2160 train_time:12907ms step_avg:35.56ms
step:364/2160 train_time:12941ms step_avg:35.55ms
step:365/2160 train_time:12978ms step_avg:35.56ms
step:365/2160 val_loss:4.1378 train_time:13013ms step_avg:35.65ms
step:366/2160 train_time:13037ms step_avg:35.62ms
step:367/2160 train_time:13057ms step_avg:35.58ms
step:368/2160 train_time:13083ms step_avg:35.55ms
step:369/2160 train_time:13119ms step_avg:35.55ms
step:370/2160 train_time:13153ms step_avg:35.55ms
step:370/2160 val_loss:4.1332 train_time:13190ms step_avg:35.65ms
step:371/2160 train_time:13214ms step_avg:35.62ms
step:372/2160 train_time:13236ms step_avg:35.58ms
step:373/2160 train_time:13259ms step_avg:35.55ms
step:374/2160 train_time:13293ms step_avg:35.54ms
step:375/2160 train_time:13330ms step_avg:35.55ms
step:375/2160 val_loss:4.1268 train_time:13365ms step_avg:35.64ms
step:376/2160 train_time:13389ms step_avg:35.61ms
step:377/2160 train_time:13409ms step_avg:35.57ms
step:378/2160 train_time:13436ms step_avg:35.54ms
step:379/2160 train_time:13472ms step_avg:35.55ms
step:380/2160 train_time:13507ms step_avg:35.55ms
step:380/2160 val_loss:4.1209 train_time:13545ms step_avg:35.64ms
step:381/2160 train_time:13567ms step_avg:35.61ms
step:382/2160 train_time:13589ms step_avg:35.57ms
step:383/2160 train_time:13615ms step_avg:35.55ms
step:384/2160 train_time:13648ms step_avg:35.54ms
step:385/2160 train_time:13685ms step_avg:35.54ms
step:385/2160 val_loss:4.1161 train_time:13720ms step_avg:35.64ms
step:386/2160 train_time:13743ms step_avg:35.60ms
step:387/2160 train_time:13763ms step_avg:35.56ms
step:388/2160 train_time:13792ms step_avg:35.55ms
step:389/2160 train_time:13827ms step_avg:35.54ms
step:390/2160 train_time:13861ms step_avg:35.54ms
step:390/2160 val_loss:4.1141 train_time:13898ms step_avg:35.63ms
step:391/2160 train_time:13920ms step_avg:35.60ms
step:392/2160 train_time:13942ms step_avg:35.57ms
step:393/2160 train_time:13967ms step_avg:35.54ms
step:394/2160 train_time:14001ms step_avg:35.54ms
step:395/2160 train_time:14039ms step_avg:35.54ms
step:395/2160 val_loss:4.1039 train_time:14074ms step_avg:35.63ms
step:396/2160 train_time:14098ms step_avg:35.60ms
step:397/2160 train_time:14118ms step_avg:35.56ms
step:398/2160 train_time:14146ms step_avg:35.54ms
step:399/2160 train_time:14182ms step_avg:35.54ms
step:400/2160 train_time:14217ms step_avg:35.54ms
step:400/2160 val_loss:4.1031 train_time:14255ms step_avg:35.64ms
step:401/2160 train_time:14277ms step_avg:35.60ms
step:402/2160 train_time:14299ms step_avg:35.57ms
step:403/2160 train_time:14325ms step_avg:35.55ms
step:404/2160 train_time:14359ms step_avg:35.54ms
step:405/2160 train_time:14395ms step_avg:35.54ms
step:405/2160 val_loss:4.0963 train_time:14431ms step_avg:35.63ms
step:406/2160 train_time:14455ms step_avg:35.60ms
step:407/2160 train_time:14475ms step_avg:35.57ms
step:408/2160 train_time:14502ms step_avg:35.54ms
step:409/2160 train_time:14537ms step_avg:35.54ms
step:410/2160 train_time:14571ms step_avg:35.54ms
step:410/2160 val_loss:4.0926 train_time:14607ms step_avg:35.63ms
step:411/2160 train_time:14630ms step_avg:35.60ms
step:412/2160 train_time:14652ms step_avg:35.56ms
step:413/2160 train_time:14677ms step_avg:35.54ms
step:414/2160 train_time:14711ms step_avg:35.53ms
step:415/2160 train_time:14748ms step_avg:35.54ms
step:415/2160 val_loss:4.0850 train_time:14783ms step_avg:35.62ms
step:416/2160 train_time:14806ms step_avg:35.59ms
step:417/2160 train_time:14827ms step_avg:35.56ms
step:418/2160 train_time:14853ms step_avg:35.53ms
step:419/2160 train_time:14888ms step_avg:35.53ms
step:420/2160 train_time:14922ms step_avg:35.53ms
step:420/2160 val_loss:4.0849 train_time:14960ms step_avg:35.62ms
step:421/2160 train_time:14982ms step_avg:35.59ms
step:422/2160 train_time:15004ms step_avg:35.55ms
step:423/2160 train_time:15030ms step_avg:35.53ms
step:424/2160 train_time:15064ms step_avg:35.53ms
step:425/2160 train_time:15102ms step_avg:35.53ms
step:425/2160 val_loss:4.0752 train_time:15135ms step_avg:35.61ms
step:426/2160 train_time:15159ms step_avg:35.58ms
step:427/2160 train_time:15179ms step_avg:35.55ms
step:428/2160 train_time:15205ms step_avg:35.53ms
step:429/2160 train_time:15240ms step_avg:35.52ms
step:430/2160 train_time:15274ms step_avg:35.52ms
step:430/2160 val_loss:4.0748 train_time:15311ms step_avg:35.61ms
step:431/2160 train_time:15333ms step_avg:35.58ms
step:432/2160 train_time:15355ms step_avg:35.54ms
step:433/2160 train_time:15382ms step_avg:35.52ms
step:434/2160 train_time:15415ms step_avg:35.52ms
step:435/2160 train_time:15453ms step_avg:35.53ms
step:435/2160 val_loss:4.0673 train_time:15489ms step_avg:35.61ms
step:436/2160 train_time:15512ms step_avg:35.58ms
step:437/2160 train_time:15533ms step_avg:35.54ms
step:438/2160 train_time:15559ms step_avg:35.52ms
step:439/2160 train_time:15595ms step_avg:35.52ms
step:440/2160 train_time:15629ms step_avg:35.52ms
step:440/2160 val_loss:4.0851 train_time:15666ms step_avg:35.61ms
step:441/2160 train_time:15688ms step_avg:35.57ms
step:442/2160 train_time:15710ms step_avg:35.54ms
step:443/2160 train_time:15737ms step_avg:35.52ms
step:444/2160 train_time:15771ms step_avg:35.52ms
step:445/2160 train_time:15808ms step_avg:35.52ms
step:445/2160 val_loss:4.0581 train_time:15842ms step_avg:35.60ms
step:446/2160 train_time:15865ms step_avg:35.57ms
step:447/2160 train_time:15886ms step_avg:35.54ms
step:448/2160 train_time:15914ms step_avg:35.52ms
step:449/2160 train_time:15950ms step_avg:35.52ms
step:450/2160 train_time:15984ms step_avg:35.52ms
step:450/2160 val_loss:4.0539 train_time:16022ms step_avg:35.60ms
step:451/2160 train_time:16044ms step_avg:35.57ms
step:452/2160 train_time:16066ms step_avg:35.54ms
step:453/2160 train_time:16093ms step_avg:35.52ms
step:454/2160 train_time:16126ms step_avg:35.52ms
step:455/2160 train_time:16162ms step_avg:35.52ms
step:455/2160 val_loss:4.0474 train_time:16197ms step_avg:35.60ms
step:456/2160 train_time:16221ms step_avg:35.57ms
step:457/2160 train_time:16242ms step_avg:35.54ms
step:458/2160 train_time:16268ms step_avg:35.52ms
step:459/2160 train_time:16304ms step_avg:35.52ms
step:460/2160 train_time:16337ms step_avg:35.52ms
step:460/2160 val_loss:4.0446 train_time:16374ms step_avg:35.60ms
step:461/2160 train_time:16396ms step_avg:35.57ms
step:462/2160 train_time:16418ms step_avg:35.54ms
step:463/2160 train_time:16443ms step_avg:35.51ms
step:464/2160 train_time:16477ms step_avg:35.51ms
step:465/2160 train_time:16514ms step_avg:35.51ms
step:465/2160 val_loss:4.0378 train_time:16549ms step_avg:35.59ms
step:466/2160 train_time:16572ms step_avg:35.56ms
step:467/2160 train_time:16593ms step_avg:35.53ms
step:468/2160 train_time:16619ms step_avg:35.51ms
step:469/2160 train_time:16654ms step_avg:35.51ms
step:470/2160 train_time:16690ms step_avg:35.51ms
step:470/2160 val_loss:4.0370 train_time:16728ms step_avg:35.59ms
step:471/2160 train_time:16750ms step_avg:35.56ms
step:472/2160 train_time:16772ms step_avg:35.53ms
step:473/2160 train_time:16798ms step_avg:35.51ms
step:474/2160 train_time:16831ms step_avg:35.51ms
step:475/2160 train_time:16868ms step_avg:35.51ms
step:475/2160 val_loss:4.0308 train_time:16903ms step_avg:35.58ms
step:476/2160 train_time:16927ms step_avg:35.56ms
step:477/2160 train_time:16947ms step_avg:35.53ms
step:478/2160 train_time:16973ms step_avg:35.51ms
step:479/2160 train_time:17008ms step_avg:35.51ms
step:480/2160 train_time:17043ms step_avg:35.51ms
step:480/2160 val_loss:4.0249 train_time:17080ms step_avg:35.58ms
step:481/2160 train_time:17102ms step_avg:35.56ms
step:482/2160 train_time:17124ms step_avg:35.53ms
step:483/2160 train_time:17150ms step_avg:35.51ms
step:484/2160 train_time:17184ms step_avg:35.50ms
step:485/2160 train_time:17220ms step_avg:35.51ms
step:485/2160 val_loss:4.0226 train_time:17255ms step_avg:35.58ms
step:486/2160 train_time:17279ms step_avg:35.55ms
step:487/2160 train_time:17299ms step_avg:35.52ms
step:488/2160 train_time:17325ms step_avg:35.50ms
step:489/2160 train_time:17361ms step_avg:35.50ms
step:490/2160 train_time:17397ms step_avg:35.50ms
step:490/2160 val_loss:4.0186 train_time:17434ms step_avg:35.58ms
step:491/2160 train_time:17456ms step_avg:35.55ms
step:492/2160 train_time:17478ms step_avg:35.52ms
step:493/2160 train_time:17503ms step_avg:35.50ms
step:494/2160 train_time:17538ms step_avg:35.50ms
step:495/2160 train_time:17575ms step_avg:35.50ms
step:495/2160 val_loss:4.0157 train_time:17609ms step_avg:35.57ms
step:496/2160 train_time:17633ms step_avg:35.55ms
step:497/2160 train_time:17653ms step_avg:35.52ms
step:498/2160 train_time:17680ms step_avg:35.50ms
step:499/2160 train_time:17716ms step_avg:35.50ms
step:500/2160 train_time:17749ms step_avg:35.50ms
step:500/2160 val_loss:4.0120 train_time:17785ms step_avg:35.57ms
step:501/2160 train_time:17807ms step_avg:35.54ms
step:502/2160 train_time:17829ms step_avg:35.52ms
step:503/2160 train_time:17856ms step_avg:35.50ms
step:504/2160 train_time:17889ms step_avg:35.49ms
step:505/2160 train_time:17926ms step_avg:35.50ms
step:505/2160 val_loss:4.0057 train_time:17961ms step_avg:35.57ms
step:506/2160 train_time:17985ms step_avg:35.54ms
step:507/2160 train_time:18005ms step_avg:35.51ms
step:508/2160 train_time:18030ms step_avg:35.49ms
step:509/2160 train_time:18066ms step_avg:35.49ms
step:510/2160 train_time:18101ms step_avg:35.49ms
step:510/2160 val_loss:4.0054 train_time:18139ms step_avg:35.57ms
step:511/2160 train_time:18161ms step_avg:35.54ms
step:512/2160 train_time:18183ms step_avg:35.51ms
step:513/2160 train_time:18210ms step_avg:35.50ms
step:514/2160 train_time:18244ms step_avg:35.49ms
step:515/2160 train_time:18281ms step_avg:35.50ms
step:515/2160 val_loss:4.0017 train_time:18316ms step_avg:35.57ms
step:516/2160 train_time:18340ms step_avg:35.54ms
step:517/2160 train_time:18360ms step_avg:35.51ms
step:518/2160 train_time:18387ms step_avg:35.50ms
step:519/2160 train_time:18423ms step_avg:35.50ms
step:520/2160 train_time:18458ms step_avg:35.50ms
step:520/2160 val_loss:4.0108 train_time:18495ms step_avg:35.57ms
step:521/2160 train_time:18517ms step_avg:35.54ms
step:522/2160 train_time:18539ms step_avg:35.52ms
step:523/2160 train_time:18565ms step_avg:35.50ms
step:524/2160 train_time:18600ms step_avg:35.50ms
step:525/2160 train_time:18638ms step_avg:35.50ms
step:525/2160 val_loss:3.9921 train_time:18673ms step_avg:35.57ms
step:526/2160 train_time:18696ms step_avg:35.54ms
step:527/2160 train_time:18717ms step_avg:35.52ms
step:528/2160 train_time:18746ms step_avg:35.50ms
step:529/2160 train_time:18781ms step_avg:35.50ms
step:530/2160 train_time:18815ms step_avg:35.50ms
step:530/2160 val_loss:3.9910 train_time:18853ms step_avg:35.57ms
step:531/2160 train_time:18875ms step_avg:35.55ms
step:532/2160 train_time:18897ms step_avg:35.52ms
step:533/2160 train_time:18922ms step_avg:35.50ms
step:534/2160 train_time:18957ms step_avg:35.50ms
step:535/2160 train_time:18994ms step_avg:35.50ms
step:535/2160 val_loss:3.9850 train_time:19030ms step_avg:35.57ms
step:536/2160 train_time:19053ms step_avg:35.55ms
step:537/2160 train_time:19073ms step_avg:35.52ms
step:538/2160 train_time:19100ms step_avg:35.50ms
step:539/2160 train_time:19135ms step_avg:35.50ms
step:540/2160 train_time:19169ms step_avg:35.50ms
step:540/2160 val_loss:3.9844 train_time:19208ms step_avg:35.57ms
step:541/2160 train_time:19230ms step_avg:35.55ms
step:542/2160 train_time:19252ms step_avg:35.52ms
step:543/2160 train_time:19277ms step_avg:35.50ms
step:544/2160 train_time:19313ms step_avg:35.50ms
step:545/2160 train_time:19350ms step_avg:35.50ms
step:545/2160 val_loss:3.9787 train_time:19384ms step_avg:35.57ms
step:546/2160 train_time:19408ms step_avg:35.55ms
step:547/2160 train_time:19428ms step_avg:35.52ms
step:548/2160 train_time:19454ms step_avg:35.50ms
step:549/2160 train_time:19489ms step_avg:35.50ms
step:550/2160 train_time:19523ms step_avg:35.50ms
step:550/2160 val_loss:3.9749 train_time:19560ms step_avg:35.56ms
step:551/2160 train_time:19582ms step_avg:35.54ms
step:552/2160 train_time:19604ms step_avg:35.51ms
step:553/2160 train_time:19630ms step_avg:35.50ms
step:554/2160 train_time:19663ms step_avg:35.49ms
step:555/2160 train_time:19701ms step_avg:35.50ms
step:555/2160 val_loss:3.9741 train_time:19736ms step_avg:35.56ms
step:556/2160 train_time:19760ms step_avg:35.54ms
step:557/2160 train_time:19781ms step_avg:35.51ms
step:558/2160 train_time:19808ms step_avg:35.50ms
step:559/2160 train_time:19844ms step_avg:35.50ms
step:560/2160 train_time:19879ms step_avg:35.50ms
step:560/2160 val_loss:3.9715 train_time:19916ms step_avg:35.56ms
step:561/2160 train_time:19938ms step_avg:35.54ms
step:562/2160 train_time:19959ms step_avg:35.52ms
step:563/2160 train_time:19986ms step_avg:35.50ms
step:564/2160 train_time:20020ms step_avg:35.50ms
step:565/2160 train_time:20057ms step_avg:35.50ms
step:565/2160 val_loss:3.9669 train_time:20091ms step_avg:35.56ms
step:566/2160 train_time:20116ms step_avg:35.54ms
step:567/2160 train_time:20136ms step_avg:35.51ms
step:568/2160 train_time:20161ms step_avg:35.49ms
step:569/2160 train_time:20197ms step_avg:35.50ms
step:570/2160 train_time:20233ms step_avg:35.50ms
step:570/2160 val_loss:3.9651 train_time:20270ms step_avg:35.56ms
step:571/2160 train_time:20292ms step_avg:35.54ms
step:572/2160 train_time:20314ms step_avg:35.51ms
step:573/2160 train_time:20342ms step_avg:35.50ms
step:574/2160 train_time:20376ms step_avg:35.50ms
step:575/2160 train_time:20413ms step_avg:35.50ms
step:575/2160 val_loss:3.9588 train_time:20448ms step_avg:35.56ms
step:576/2160 train_time:20472ms step_avg:35.54ms
step:577/2160 train_time:20492ms step_avg:35.51ms
step:578/2160 train_time:20519ms step_avg:35.50ms
step:579/2160 train_time:20554ms step_avg:35.50ms
step:580/2160 train_time:20588ms step_avg:35.50ms
step:580/2160 val_loss:3.9561 train_time:20626ms step_avg:35.56ms
step:581/2160 train_time:20649ms step_avg:35.54ms
step:582/2160 train_time:20671ms step_avg:35.52ms
step:583/2160 train_time:20696ms step_avg:35.50ms
step:584/2160 train_time:20730ms step_avg:35.50ms
step:585/2160 train_time:20767ms step_avg:35.50ms
step:585/2160 val_loss:3.9531 train_time:20802ms step_avg:35.56ms
step:586/2160 train_time:20826ms step_avg:35.54ms
step:587/2160 train_time:20846ms step_avg:35.51ms
step:588/2160 train_time:20874ms step_avg:35.50ms
step:589/2160 train_time:20910ms step_avg:35.50ms
step:590/2160 train_time:20945ms step_avg:35.50ms
step:590/2160 val_loss:3.9727 train_time:20982ms step_avg:35.56ms
step:591/2160 train_time:21004ms step_avg:35.54ms
step:592/2160 train_time:21026ms step_avg:35.52ms
step:593/2160 train_time:21052ms step_avg:35.50ms
step:594/2160 train_time:21085ms step_avg:35.50ms
step:595/2160 train_time:21122ms step_avg:35.50ms
step:595/2160 val_loss:3.9483 train_time:21158ms step_avg:35.56ms
step:596/2160 train_time:21181ms step_avg:35.54ms
step:597/2160 train_time:21202ms step_avg:35.51ms
step:598/2160 train_time:21227ms step_avg:35.50ms
step:599/2160 train_time:21262ms step_avg:35.50ms
step:600/2160 train_time:21297ms step_avg:35.49ms
step:600/2160 val_loss:3.9468 train_time:21333ms step_avg:35.56ms
step:601/2160 train_time:21356ms step_avg:35.53ms
step:602/2160 train_time:21377ms step_avg:35.51ms
step:603/2160 train_time:21403ms step_avg:35.49ms
step:604/2160 train_time:21437ms step_avg:35.49ms
step:605/2160 train_time:21474ms step_avg:35.49ms
step:605/2160 val_loss:3.9420 train_time:21509ms step_avg:35.55ms
step:606/2160 train_time:21533ms step_avg:35.53ms
step:607/2160 train_time:21553ms step_avg:35.51ms
step:608/2160 train_time:21580ms step_avg:35.49ms
step:609/2160 train_time:21614ms step_avg:35.49ms
step:610/2160 train_time:21650ms step_avg:35.49ms
step:610/2160 val_loss:3.9413 train_time:21687ms step_avg:35.55ms
step:611/2160 train_time:21709ms step_avg:35.53ms
step:612/2160 train_time:21731ms step_avg:35.51ms
step:613/2160 train_time:21756ms step_avg:35.49ms
step:614/2160 train_time:21789ms step_avg:35.49ms
step:615/2160 train_time:21826ms step_avg:35.49ms
step:615/2160 val_loss:3.9375 train_time:21861ms step_avg:35.55ms
step:616/2160 train_time:21884ms step_avg:35.53ms
step:617/2160 train_time:21905ms step_avg:35.50ms
step:618/2160 train_time:21930ms step_avg:35.49ms
step:619/2160 train_time:21967ms step_avg:35.49ms
step:620/2160 train_time:22001ms step_avg:35.49ms
step:620/2160 val_loss:3.9335 train_time:22037ms step_avg:35.54ms
step:621/2160 train_time:22060ms step_avg:35.52ms
step:622/2160 train_time:22081ms step_avg:35.50ms
step:623/2160 train_time:22107ms step_avg:35.48ms
step:624/2160 train_time:22141ms step_avg:35.48ms
step:625/2160 train_time:22178ms step_avg:35.48ms
step:625/2160 val_loss:3.9290 train_time:22213ms step_avg:35.54ms
step:626/2160 train_time:22237ms step_avg:35.52ms
step:627/2160 train_time:22257ms step_avg:35.50ms
step:628/2160 train_time:22286ms step_avg:35.49ms
step:629/2160 train_time:22320ms step_avg:35.49ms
step:630/2160 train_time:22355ms step_avg:35.48ms
step:630/2160 val_loss:3.9308 train_time:22392ms step_avg:35.54ms
step:631/2160 train_time:22414ms step_avg:35.52ms
step:632/2160 train_time:22436ms step_avg:35.50ms
step:633/2160 train_time:22464ms step_avg:35.49ms
step:634/2160 train_time:22497ms step_avg:35.48ms
step:635/2160 train_time:22534ms step_avg:35.49ms
step:635/2160 val_loss:3.9238 train_time:22569ms step_avg:35.54ms
step:636/2160 train_time:22593ms step_avg:35.52ms
step:637/2160 train_time:22613ms step_avg:35.50ms
step:638/2160 train_time:22640ms step_avg:35.49ms
step:639/2160 train_time:22676ms step_avg:35.49ms
step:640/2160 train_time:22711ms step_avg:35.49ms
step:640/2160 val_loss:3.9218 train_time:22749ms step_avg:35.54ms
step:641/2160 train_time:22771ms step_avg:35.52ms
step:642/2160 train_time:22793ms step_avg:35.50ms
step:643/2160 train_time:22818ms step_avg:35.49ms
step:644/2160 train_time:22851ms step_avg:35.48ms
step:645/2160 train_time:22888ms step_avg:35.49ms
step:645/2160 val_loss:3.9180 train_time:22924ms step_avg:35.54ms
step:646/2160 train_time:22948ms step_avg:35.52ms
step:647/2160 train_time:22968ms step_avg:35.50ms
step:648/2160 train_time:22996ms step_avg:35.49ms
step:649/2160 train_time:23031ms step_avg:35.49ms
step:650/2160 train_time:23064ms step_avg:35.48ms
step:650/2160 val_loss:3.9164 train_time:23101ms step_avg:35.54ms
step:651/2160 train_time:23124ms step_avg:35.52ms
step:652/2160 train_time:23146ms step_avg:35.50ms
step:653/2160 train_time:23172ms step_avg:35.48ms
step:654/2160 train_time:23206ms step_avg:35.48ms
step:655/2160 train_time:23243ms step_avg:35.49ms
step:655/2160 val_loss:3.9141 train_time:23279ms step_avg:35.54ms
step:656/2160 train_time:23302ms step_avg:35.52ms
step:657/2160 train_time:23323ms step_avg:35.50ms
step:658/2160 train_time:23349ms step_avg:35.48ms
step:659/2160 train_time:23385ms step_avg:35.49ms
step:660/2160 train_time:23420ms step_avg:35.49ms
step:660/2160 val_loss:3.9132 train_time:23457ms step_avg:35.54ms
step:661/2160 train_time:23479ms step_avg:35.52ms
step:662/2160 train_time:23501ms step_avg:35.50ms
step:663/2160 train_time:23527ms step_avg:35.49ms
step:664/2160 train_time:23561ms step_avg:35.48ms
step:665/2160 train_time:23598ms step_avg:35.49ms
step:665/2160 val_loss:3.9094 train_time:23632ms step_avg:35.54ms
step:666/2160 train_time:23656ms step_avg:35.52ms
step:667/2160 train_time:23676ms step_avg:35.50ms
step:668/2160 train_time:23702ms step_avg:35.48ms
step:669/2160 train_time:23739ms step_avg:35.48ms
step:670/2160 train_time:23773ms step_avg:35.48ms
step:670/2160 val_loss:3.9186 train_time:23812ms step_avg:35.54ms
step:671/2160 train_time:23834ms step_avg:35.52ms
step:672/2160 train_time:23856ms step_avg:35.50ms
step:673/2160 train_time:23881ms step_avg:35.49ms
step:674/2160 train_time:23915ms step_avg:35.48ms
step:675/2160 train_time:23953ms step_avg:35.49ms
step:675/2160 val_loss:3.9024 train_time:23988ms step_avg:35.54ms
step:676/2160 train_time:24012ms step_avg:35.52ms
step:677/2160 train_time:24032ms step_avg:35.50ms
step:678/2160 train_time:24058ms step_avg:35.48ms
step:679/2160 train_time:24093ms step_avg:35.48ms
step:680/2160 train_time:24127ms step_avg:35.48ms
step:680/2160 val_loss:3.9022 train_time:24166ms step_avg:35.54ms
step:681/2160 train_time:24188ms step_avg:35.52ms
step:682/2160 train_time:24210ms step_avg:35.50ms
step:683/2160 train_time:24236ms step_avg:35.48ms
step:684/2160 train_time:24271ms step_avg:35.48ms
step:685/2160 train_time:24308ms step_avg:35.49ms
step:685/2160 val_loss:3.8978 train_time:24344ms step_avg:35.54ms
step:686/2160 train_time:24370ms step_avg:35.52ms
step:687/2160 train_time:24390ms step_avg:35.50ms
step:688/2160 train_time:24413ms step_avg:35.48ms
step:689/2160 train_time:24448ms step_avg:35.48ms
step:690/2160 train_time:24483ms step_avg:35.48ms
step:690/2160 val_loss:3.8973 train_time:24520ms step_avg:35.54ms
step:691/2160 train_time:24542ms step_avg:35.52ms
step:692/2160 train_time:24564ms step_avg:35.50ms
step:693/2160 train_time:24590ms step_avg:35.48ms
step:694/2160 train_time:24623ms step_avg:35.48ms
step:695/2160 train_time:24661ms step_avg:35.48ms
step:695/2160 val_loss:3.8939 train_time:24696ms step_avg:35.53ms
step:696/2160 train_time:24719ms step_avg:35.52ms
step:697/2160 train_time:24740ms step_avg:35.49ms
step:698/2160 train_time:24768ms step_avg:35.48ms
step:699/2160 train_time:24804ms step_avg:35.49ms
step:700/2160 train_time:24839ms step_avg:35.48ms
step:700/2160 val_loss:3.8953 train_time:24877ms step_avg:35.54ms
step:701/2160 train_time:24900ms step_avg:35.52ms
step:702/2160 train_time:24922ms step_avg:35.50ms
step:703/2160 train_time:24947ms step_avg:35.49ms
step:704/2160 train_time:24981ms step_avg:35.48ms
step:705/2160 train_time:25016ms step_avg:35.48ms
step:705/2160 val_loss:3.9210 train_time:25052ms step_avg:35.53ms
step:706/2160 train_time:25076ms step_avg:35.52ms
step:707/2160 train_time:25096ms step_avg:35.50ms
step:708/2160 train_time:25124ms step_avg:35.49ms
step:709/2160 train_time:25187ms step_avg:35.52ms
step:710/2160 train_time:25248ms step_avg:35.56ms
step:710/2160 val_loss:3.9774 train_time:25311ms step_avg:35.65ms
step:711/2160 train_time:25333ms step_avg:35.63ms
step:712/2160 train_time:25370ms step_avg:35.63ms
step:713/2160 train_time:25435ms step_avg:35.67ms
step:714/2160 train_time:25502ms step_avg:35.72ms
step:715/2160 train_time:25563ms step_avg:35.75ms
step:715/2160 val_loss:3.9014 train_time:25621ms step_avg:35.83ms
step:716/2160 train_time:25646ms step_avg:35.82ms
step:717/2160 train_time:25684ms step_avg:35.82ms
step:718/2160 train_time:25744ms step_avg:35.86ms
step:719/2160 train_time:25808ms step_avg:35.89ms
step:720/2160 train_time:25868ms step_avg:35.93ms
step:720/2160 val_loss:3.8899 train_time:25930ms step_avg:36.01ms
step:721/2160 train_time:25953ms step_avg:36.00ms
step:722/2160 train_time:25991ms step_avg:36.00ms
step:723/2160 train_time:26055ms step_avg:36.04ms
step:724/2160 train_time:26120ms step_avg:36.08ms
step:725/2160 train_time:26184ms step_avg:36.12ms
step:725/2160 val_loss:3.8790 train_time:26244ms step_avg:36.20ms
step:726/2160 train_time:26268ms step_avg:36.18ms
step:727/2160 train_time:26307ms step_avg:36.19ms
step:728/2160 train_time:26369ms step_avg:36.22ms
step:729/2160 train_time:26434ms step_avg:36.26ms
step:730/2160 train_time:26495ms step_avg:36.29ms
step:730/2160 val_loss:3.8709 train_time:26557ms step_avg:36.38ms
step:731/2160 train_time:26580ms step_avg:36.36ms
step:732/2160 train_time:26620ms step_avg:36.37ms
step:733/2160 train_time:26686ms step_avg:36.41ms
step:734/2160 train_time:26748ms step_avg:36.44ms
step:735/2160 train_time:26811ms step_avg:36.48ms
step:735/2160 val_loss:3.8680 train_time:26871ms step_avg:36.56ms
step:736/2160 train_time:26895ms step_avg:36.54ms
step:737/2160 train_time:26935ms step_avg:36.55ms
step:738/2160 train_time:26998ms step_avg:36.58ms
step:739/2160 train_time:27064ms step_avg:36.62ms
step:740/2160 train_time:27126ms step_avg:36.66ms
step:740/2160 val_loss:3.8674 train_time:27187ms step_avg:36.74ms
step:741/2160 train_time:27210ms step_avg:36.72ms
step:742/2160 train_time:27247ms step_avg:36.72ms
step:743/2160 train_time:27311ms step_avg:36.76ms
step:744/2160 train_time:27374ms step_avg:36.79ms
step:745/2160 train_time:27437ms step_avg:36.83ms
step:745/2160 val_loss:3.8562 train_time:27496ms step_avg:36.91ms
step:746/2160 train_time:27522ms step_avg:36.89ms
step:747/2160 train_time:27560ms step_avg:36.89ms
step:748/2160 train_time:27622ms step_avg:36.93ms
step:749/2160 train_time:27688ms step_avg:36.97ms
step:750/2160 train_time:27748ms step_avg:37.00ms
step:750/2160 val_loss:3.8463 train_time:27809ms step_avg:37.08ms
step:751/2160 train_time:27832ms step_avg:37.06ms
step:752/2160 train_time:27872ms step_avg:37.06ms
step:753/2160 train_time:27936ms step_avg:37.10ms
step:754/2160 train_time:27997ms step_avg:37.13ms
step:755/2160 train_time:28059ms step_avg:37.16ms
step:755/2160 val_loss:3.8403 train_time:28119ms step_avg:37.24ms
step:756/2160 train_time:28143ms step_avg:37.23ms
step:757/2160 train_time:28182ms step_avg:37.23ms
step:758/2160 train_time:28245ms step_avg:37.26ms
step:759/2160 train_time:28310ms step_avg:37.30ms
step:760/2160 train_time:28371ms step_avg:37.33ms
step:760/2160 val_loss:3.8368 train_time:28433ms step_avg:37.41ms
step:761/2160 train_time:28456ms step_avg:37.39ms
step:762/2160 train_time:28494ms step_avg:37.39ms
step:763/2160 train_time:28560ms step_avg:37.43ms
step:764/2160 train_time:28622ms step_avg:37.46ms
step:765/2160 train_time:28685ms step_avg:37.50ms
step:765/2160 val_loss:3.8292 train_time:28744ms step_avg:37.57ms
step:766/2160 train_time:28768ms step_avg:37.56ms
step:767/2160 train_time:28809ms step_avg:37.56ms
step:768/2160 train_time:28872ms step_avg:37.59ms
step:769/2160 train_time:28935ms step_avg:37.63ms
step:770/2160 train_time:28995ms step_avg:37.66ms
step:770/2160 val_loss:3.8321 train_time:29058ms step_avg:37.74ms
step:771/2160 train_time:29080ms step_avg:37.72ms
step:772/2160 train_time:29120ms step_avg:37.72ms
step:773/2160 train_time:29187ms step_avg:37.76ms
step:774/2160 train_time:29251ms step_avg:37.79ms
step:775/2160 train_time:29313ms step_avg:37.82ms
step:775/2160 val_loss:3.8211 train_time:29373ms step_avg:37.90ms
step:776/2160 train_time:29397ms step_avg:37.88ms
step:777/2160 train_time:29436ms step_avg:37.88ms
step:778/2160 train_time:29497ms step_avg:37.91ms
step:779/2160 train_time:29561ms step_avg:37.95ms
step:780/2160 train_time:29620ms step_avg:37.97ms
step:780/2160 val_loss:3.8225 train_time:29682ms step_avg:38.05ms
step:781/2160 train_time:29705ms step_avg:38.03ms
step:782/2160 train_time:29746ms step_avg:38.04ms
step:783/2160 train_time:29809ms step_avg:38.07ms
step:784/2160 train_time:29873ms step_avg:38.10ms
step:785/2160 train_time:29936ms step_avg:38.13ms
step:785/2160 val_loss:3.8108 train_time:29996ms step_avg:38.21ms
step:786/2160 train_time:30020ms step_avg:38.19ms
step:787/2160 train_time:30061ms step_avg:38.20ms
step:788/2160 train_time:30122ms step_avg:38.23ms
step:789/2160 train_time:30186ms step_avg:38.26ms
step:790/2160 train_time:30247ms step_avg:38.29ms
step:790/2160 val_loss:3.8095 train_time:30309ms step_avg:38.37ms
step:791/2160 train_time:30332ms step_avg:38.35ms
step:792/2160 train_time:30371ms step_avg:38.35ms
step:793/2160 train_time:30436ms step_avg:38.38ms
step:794/2160 train_time:30497ms step_avg:38.41ms
step:795/2160 train_time:30559ms step_avg:38.44ms
step:795/2160 val_loss:3.8023 train_time:30618ms step_avg:38.51ms
step:796/2160 train_time:30643ms step_avg:38.50ms
step:797/2160 train_time:30683ms step_avg:38.50ms
step:798/2160 train_time:30745ms step_avg:38.53ms
step:799/2160 train_time:30810ms step_avg:38.56ms
step:800/2160 train_time:30870ms step_avg:38.59ms
step:800/2160 val_loss:3.8062 train_time:30932ms step_avg:38.67ms
step:801/2160 train_time:30955ms step_avg:38.65ms
step:802/2160 train_time:30993ms step_avg:38.64ms
step:803/2160 train_time:31056ms step_avg:38.68ms
step:804/2160 train_time:31119ms step_avg:38.70ms
step:805/2160 train_time:31181ms step_avg:38.73ms
step:805/2160 val_loss:3.7960 train_time:31240ms step_avg:38.81ms
step:806/2160 train_time:31264ms step_avg:38.79ms
step:807/2160 train_time:31304ms step_avg:38.79ms
step:808/2160 train_time:31365ms step_avg:38.82ms
step:809/2160 train_time:31430ms step_avg:38.85ms
step:810/2160 train_time:31490ms step_avg:38.88ms
step:810/2160 val_loss:3.7938 train_time:31551ms step_avg:38.95ms
step:811/2160 train_time:31574ms step_avg:38.93ms
step:812/2160 train_time:31612ms step_avg:38.93ms
step:813/2160 train_time:31679ms step_avg:38.97ms
step:814/2160 train_time:31742ms step_avg:39.00ms
step:815/2160 train_time:31805ms step_avg:39.02ms
step:815/2160 val_loss:3.7878 train_time:31865ms step_avg:39.10ms
step:816/2160 train_time:31889ms step_avg:39.08ms
step:817/2160 train_time:31930ms step_avg:39.08ms
step:818/2160 train_time:31993ms step_avg:39.11ms
step:819/2160 train_time:32059ms step_avg:39.14ms
step:820/2160 train_time:32118ms step_avg:39.17ms
step:820/2160 val_loss:3.7839 train_time:32180ms step_avg:39.24ms
step:821/2160 train_time:32205ms step_avg:39.23ms
step:822/2160 train_time:32244ms step_avg:39.23ms
step:823/2160 train_time:32306ms step_avg:39.25ms
step:824/2160 train_time:32371ms step_avg:39.28ms
step:825/2160 train_time:32434ms step_avg:39.31ms
step:825/2160 val_loss:3.7790 train_time:32494ms step_avg:39.39ms
step:826/2160 train_time:32519ms step_avg:39.37ms
step:827/2160 train_time:32557ms step_avg:39.37ms
step:828/2160 train_time:32619ms step_avg:39.39ms
step:829/2160 train_time:32685ms step_avg:39.43ms
step:830/2160 train_time:32746ms step_avg:39.45ms
step:830/2160 val_loss:3.8042 train_time:32808ms step_avg:39.53ms
step:831/2160 train_time:32831ms step_avg:39.51ms
step:832/2160 train_time:32869ms step_avg:39.51ms
step:833/2160 train_time:32933ms step_avg:39.54ms
step:834/2160 train_time:32995ms step_avg:39.56ms
step:835/2160 train_time:33057ms step_avg:39.59ms
step:835/2160 val_loss:3.7766 train_time:33117ms step_avg:39.66ms
step:836/2160 train_time:33141ms step_avg:39.64ms
step:837/2160 train_time:33181ms step_avg:39.64ms
step:838/2160 train_time:33244ms step_avg:39.67ms
step:839/2160 train_time:33310ms step_avg:39.70ms
step:840/2160 train_time:33371ms step_avg:39.73ms
step:840/2160 val_loss:3.7731 train_time:33434ms step_avg:39.80ms
step:841/2160 train_time:33457ms step_avg:39.78ms
step:842/2160 train_time:33495ms step_avg:39.78ms
step:843/2160 train_time:33559ms step_avg:39.81ms
step:844/2160 train_time:33622ms step_avg:39.84ms
step:845/2160 train_time:33685ms step_avg:39.86ms
step:845/2160 val_loss:3.7690 train_time:33744ms step_avg:39.93ms
step:846/2160 train_time:33768ms step_avg:39.92ms
step:847/2160 train_time:33809ms step_avg:39.92ms
step:848/2160 train_time:33872ms step_avg:39.94ms
step:849/2160 train_time:33938ms step_avg:39.97ms
step:850/2160 train_time:33999ms step_avg:40.00ms
step:850/2160 val_loss:3.7692 train_time:34061ms step_avg:40.07ms
step:851/2160 train_time:34083ms step_avg:40.05ms
step:852/2160 train_time:34123ms step_avg:40.05ms
step:853/2160 train_time:34186ms step_avg:40.08ms
step:854/2160 train_time:34250ms step_avg:40.11ms
step:855/2160 train_time:34313ms step_avg:40.13ms
step:855/2160 val_loss:3.7608 train_time:34372ms step_avg:40.20ms
step:856/2160 train_time:34396ms step_avg:40.18ms
step:857/2160 train_time:34437ms step_avg:40.18ms
step:858/2160 train_time:34499ms step_avg:40.21ms
step:859/2160 train_time:34563ms step_avg:40.24ms
step:860/2160 train_time:34622ms step_avg:40.26ms
step:860/2160 val_loss:3.7603 train_time:34684ms step_avg:40.33ms
step:861/2160 train_time:34709ms step_avg:40.31ms
step:862/2160 train_time:34745ms step_avg:40.31ms
step:863/2160 train_time:34809ms step_avg:40.33ms
step:864/2160 train_time:34872ms step_avg:40.36ms
step:865/2160 train_time:34934ms step_avg:40.39ms
step:865/2160 val_loss:3.7521 train_time:34994ms step_avg:40.45ms
step:866/2160 train_time:35018ms step_avg:40.44ms
step:867/2160 train_time:35057ms step_avg:40.44ms
step:868/2160 train_time:35119ms step_avg:40.46ms
step:869/2160 train_time:35185ms step_avg:40.49ms
step:870/2160 train_time:35245ms step_avg:40.51ms
step:870/2160 val_loss:3.7563 train_time:35308ms step_avg:40.58ms
step:871/2160 train_time:35330ms step_avg:40.56ms
step:872/2160 train_time:35369ms step_avg:40.56ms
step:873/2160 train_time:35433ms step_avg:40.59ms
step:874/2160 train_time:35495ms step_avg:40.61ms
step:875/2160 train_time:35556ms step_avg:40.64ms
step:875/2160 val_loss:3.7465 train_time:35615ms step_avg:40.70ms
step:876/2160 train_time:35639ms step_avg:40.68ms
step:877/2160 train_time:35679ms step_avg:40.68ms
step:878/2160 train_time:35741ms step_avg:40.71ms
step:879/2160 train_time:35806ms step_avg:40.74ms
step:880/2160 train_time:35867ms step_avg:40.76ms
step:880/2160 val_loss:3.7490 train_time:35929ms step_avg:40.83ms
step:881/2160 train_time:35951ms step_avg:40.81ms
step:882/2160 train_time:35991ms step_avg:40.81ms
step:883/2160 train_time:36055ms step_avg:40.83ms
step:884/2160 train_time:36119ms step_avg:40.86ms
step:885/2160 train_time:36182ms step_avg:40.88ms
step:885/2160 val_loss:3.7430 train_time:36242ms step_avg:40.95ms
step:886/2160 train_time:36266ms step_avg:40.93ms
step:887/2160 train_time:36307ms step_avg:40.93ms
step:888/2160 train_time:36371ms step_avg:40.96ms
step:889/2160 train_time:36436ms step_avg:40.98ms
step:890/2160 train_time:36495ms step_avg:41.01ms
step:890/2160 val_loss:3.7442 train_time:36557ms step_avg:41.08ms
step:891/2160 train_time:36581ms step_avg:41.06ms
step:892/2160 train_time:36619ms step_avg:41.05ms
step:893/2160 train_time:36684ms step_avg:41.08ms
step:894/2160 train_time:36747ms step_avg:41.10ms
step:895/2160 train_time:36809ms step_avg:41.13ms
step:895/2160 val_loss:3.7359 train_time:36869ms step_avg:41.19ms
step:896/2160 train_time:36893ms step_avg:41.17ms
step:897/2160 train_time:36932ms step_avg:41.17ms
step:898/2160 train_time:36994ms step_avg:41.20ms
step:899/2160 train_time:37058ms step_avg:41.22ms
step:900/2160 train_time:37118ms step_avg:41.24ms
step:900/2160 val_loss:3.7499 train_time:37180ms step_avg:41.31ms
step:901/2160 train_time:37203ms step_avg:41.29ms
step:902/2160 train_time:37241ms step_avg:41.29ms
step:903/2160 train_time:37306ms step_avg:41.31ms
step:904/2160 train_time:37368ms step_avg:41.34ms
step:905/2160 train_time:37430ms step_avg:41.36ms
step:905/2160 val_loss:3.7326 train_time:37489ms step_avg:41.42ms
step:906/2160 train_time:37514ms step_avg:41.41ms
step:907/2160 train_time:37553ms step_avg:41.40ms
step:908/2160 train_time:37618ms step_avg:41.43ms
step:909/2160 train_time:37683ms step_avg:41.45ms
step:910/2160 train_time:37744ms step_avg:41.48ms
step:910/2160 val_loss:3.7313 train_time:37806ms step_avg:41.55ms
step:911/2160 train_time:37829ms step_avg:41.52ms
step:912/2160 train_time:37869ms step_avg:41.52ms
step:913/2160 train_time:37936ms step_avg:41.55ms
step:914/2160 train_time:37999ms step_avg:41.57ms
step:915/2160 train_time:38061ms step_avg:41.60ms
step:915/2160 val_loss:3.7256 train_time:38121ms step_avg:41.66ms
step:916/2160 train_time:38145ms step_avg:41.64ms
step:917/2160 train_time:38185ms step_avg:41.64ms
step:918/2160 train_time:38246ms step_avg:41.66ms
step:919/2160 train_time:38310ms step_avg:41.69ms
step:920/2160 train_time:38371ms step_avg:41.71ms
step:920/2160 val_loss:3.7291 train_time:38433ms step_avg:41.77ms
step:921/2160 train_time:38455ms step_avg:41.75ms
step:922/2160 train_time:38494ms step_avg:41.75ms
step:923/2160 train_time:38557ms step_avg:41.77ms
step:924/2160 train_time:38618ms step_avg:41.79ms
step:925/2160 train_time:38680ms step_avg:41.82ms
step:925/2160 val_loss:3.7223 train_time:38740ms step_avg:41.88ms
step:926/2160 train_time:38764ms step_avg:41.86ms
step:927/2160 train_time:38803ms step_avg:41.86ms
step:928/2160 train_time:38865ms step_avg:41.88ms
step:929/2160 train_time:38929ms step_avg:41.90ms
step:930/2160 train_time:38990ms step_avg:41.92ms
step:930/2160 val_loss:3.7222 train_time:39052ms step_avg:41.99ms
step:931/2160 train_time:39075ms step_avg:41.97ms
step:932/2160 train_time:39113ms step_avg:41.97ms
step:933/2160 train_time:39178ms step_avg:41.99ms
step:934/2160 train_time:39240ms step_avg:42.01ms
step:935/2160 train_time:39302ms step_avg:42.03ms
step:935/2160 val_loss:3.7137 train_time:39362ms step_avg:42.10ms
step:936/2160 train_time:39386ms step_avg:42.08ms
step:937/2160 train_time:39425ms step_avg:42.08ms
step:938/2160 train_time:39487ms step_avg:42.10ms
step:939/2160 train_time:39553ms step_avg:42.12ms
step:940/2160 train_time:39612ms step_avg:42.14ms
step:940/2160 val_loss:3.7162 train_time:39674ms step_avg:42.21ms
step:941/2160 train_time:39697ms step_avg:42.19ms
step:942/2160 train_time:39738ms step_avg:42.19ms
step:943/2160 train_time:39802ms step_avg:42.21ms
step:944/2160 train_time:39865ms step_avg:42.23ms
step:945/2160 train_time:39928ms step_avg:42.25ms
step:945/2160 val_loss:3.7128 train_time:39988ms step_avg:42.32ms
step:946/2160 train_time:40012ms step_avg:42.30ms
step:947/2160 train_time:40051ms step_avg:42.29ms
step:948/2160 train_time:40114ms step_avg:42.31ms
step:949/2160 train_time:40178ms step_avg:42.34ms
step:950/2160 train_time:40238ms step_avg:42.36ms
step:950/2160 val_loss:3.7133 train_time:40300ms step_avg:42.42ms
step:951/2160 train_time:40322ms step_avg:42.40ms
step:952/2160 train_time:40361ms step_avg:42.40ms
step:953/2160 train_time:40427ms step_avg:42.42ms
step:954/2160 train_time:40491ms step_avg:42.44ms
step:955/2160 train_time:40552ms step_avg:42.46ms
step:955/2160 val_loss:3.7076 train_time:40613ms step_avg:42.53ms
step:956/2160 train_time:40636ms step_avg:42.51ms
step:957/2160 train_time:40677ms step_avg:42.50ms
step:958/2160 train_time:40739ms step_avg:42.53ms
step:959/2160 train_time:40802ms step_avg:42.55ms
step:960/2160 train_time:40863ms step_avg:42.57ms
step:960/2160 val_loss:3.7061 train_time:40924ms step_avg:42.63ms
step:961/2160 train_time:40947ms step_avg:42.61ms
step:962/2160 train_time:40986ms step_avg:42.61ms
step:963/2160 train_time:41053ms step_avg:42.63ms
step:964/2160 train_time:41117ms step_avg:42.65ms
step:965/2160 train_time:41178ms step_avg:42.67ms
step:965/2160 val_loss:3.7019 train_time:41238ms step_avg:42.73ms
step:966/2160 train_time:41262ms step_avg:42.71ms
step:967/2160 train_time:41304ms step_avg:42.71ms
step:968/2160 train_time:41367ms step_avg:42.73ms
step:969/2160 train_time:41432ms step_avg:42.76ms
step:970/2160 train_time:41492ms step_avg:42.77ms
step:970/2160 val_loss:3.7040 train_time:41553ms step_avg:42.84ms
step:971/2160 train_time:41576ms step_avg:42.82ms
step:972/2160 train_time:41614ms step_avg:42.81ms
step:973/2160 train_time:41679ms step_avg:42.84ms
step:974/2160 train_time:41746ms step_avg:42.86ms
step:975/2160 train_time:41808ms step_avg:42.88ms
step:975/2160 val_loss:3.6980 train_time:41868ms step_avg:42.94ms
step:976/2160 train_time:41892ms step_avg:42.92ms
step:977/2160 train_time:41932ms step_avg:42.92ms
step:978/2160 train_time:41994ms step_avg:42.94ms
step:979/2160 train_time:42060ms step_avg:42.96ms
step:980/2160 train_time:42121ms step_avg:42.98ms
step:980/2160 val_loss:3.7021 train_time:42182ms step_avg:43.04ms
step:981/2160 train_time:42207ms step_avg:43.02ms
step:982/2160 train_time:42247ms step_avg:43.02ms
step:983/2160 train_time:42311ms step_avg:43.04ms
step:984/2160 train_time:42374ms step_avg:43.06ms
step:985/2160 train_time:42436ms step_avg:43.08ms
step:985/2160 val_loss:3.6896 train_time:42497ms step_avg:43.14ms
step:986/2160 train_time:42521ms step_avg:43.12ms
step:987/2160 train_time:42562ms step_avg:43.12ms
step:988/2160 train_time:42625ms step_avg:43.14ms
step:989/2160 train_time:42691ms step_avg:43.17ms
step:990/2160 train_time:42753ms step_avg:43.19ms
step:990/2160 val_loss:3.6911 train_time:42816ms step_avg:43.25ms
step:991/2160 train_time:42839ms step_avg:43.23ms
step:992/2160 train_time:42879ms step_avg:43.22ms
step:993/2160 train_time:42945ms step_avg:43.25ms
step:994/2160 train_time:43008ms step_avg:43.27ms
step:995/2160 train_time:43069ms step_avg:43.29ms
step:995/2160 val_loss:3.6860 train_time:43128ms step_avg:43.34ms
step:996/2160 train_time:43152ms step_avg:43.33ms
step:997/2160 train_time:43191ms step_avg:43.32ms
step:998/2160 train_time:43251ms step_avg:43.34ms
step:999/2160 train_time:43316ms step_avg:43.36ms
step:1000/2160 train_time:43376ms step_avg:43.38ms
step:1000/2160 val_loss:3.6838 train_time:43438ms step_avg:43.44ms
step:1001/2160 train_time:43461ms step_avg:43.42ms
step:1002/2160 train_time:43502ms step_avg:43.42ms
step:1003/2160 train_time:43566ms step_avg:43.44ms
step:1004/2160 train_time:43630ms step_avg:43.46ms
step:1005/2160 train_time:43693ms step_avg:43.48ms
step:1005/2160 val_loss:3.6828 train_time:43753ms step_avg:43.54ms
step:1006/2160 train_time:43777ms step_avg:43.52ms
step:1007/2160 train_time:43817ms step_avg:43.51ms
step:1008/2160 train_time:43881ms step_avg:43.53ms
step:1009/2160 train_time:43947ms step_avg:43.56ms
step:1010/2160 train_time:44007ms step_avg:43.57ms
step:1010/2160 val_loss:3.6793 train_time:44068ms step_avg:43.63ms
step:1011/2160 train_time:44091ms step_avg:43.61ms
step:1012/2160 train_time:44132ms step_avg:43.61ms
step:1013/2160 train_time:44198ms step_avg:43.63ms
step:1014/2160 train_time:44260ms step_avg:43.65ms
step:1015/2160 train_time:44322ms step_avg:43.67ms
step:1015/2160 val_loss:3.6751 train_time:44382ms step_avg:43.73ms
step:1016/2160 train_time:44406ms step_avg:43.71ms
step:1017/2160 train_time:44446ms step_avg:43.70ms
step:1018/2160 train_time:44508ms step_avg:43.72ms
step:1019/2160 train_time:44572ms step_avg:43.74ms
step:1020/2160 train_time:44633ms step_avg:43.76ms
step:1020/2160 val_loss:3.6762 train_time:44695ms step_avg:43.82ms
step:1021/2160 train_time:44717ms step_avg:43.80ms
step:1022/2160 train_time:44756ms step_avg:43.79ms
step:1023/2160 train_time:44822ms step_avg:43.81ms
step:1024/2160 train_time:44885ms step_avg:43.83ms
step:1025/2160 train_time:44948ms step_avg:43.85ms
step:1025/2160 val_loss:3.6699 train_time:45007ms step_avg:43.91ms
step:1026/2160 train_time:45031ms step_avg:43.89ms
step:1027/2160 train_time:45073ms step_avg:43.89ms
step:1028/2160 train_time:45136ms step_avg:43.91ms
step:1029/2160 train_time:45201ms step_avg:43.93ms
step:1030/2160 train_time:45260ms step_avg:43.94ms
step:1030/2160 val_loss:3.6677 train_time:45322ms step_avg:44.00ms
step:1031/2160 train_time:45345ms step_avg:43.98ms
step:1032/2160 train_time:45385ms step_avg:43.98ms
step:1033/2160 train_time:45449ms step_avg:44.00ms
step:1034/2160 train_time:45512ms step_avg:44.02ms
step:1035/2160 train_time:45574ms step_avg:44.03ms
step:1035/2160 val_loss:3.6658 train_time:45634ms step_avg:44.09ms
step:1036/2160 train_time:45658ms step_avg:44.07ms
step:1037/2160 train_time:45697ms step_avg:44.07ms
step:1038/2160 train_time:45759ms step_avg:44.08ms
step:1039/2160 train_time:45826ms step_avg:44.11ms
step:1040/2160 train_time:45885ms step_avg:44.12ms
step:1040/2160 val_loss:3.6664 train_time:45946ms step_avg:44.18ms
step:1041/2160 train_time:45969ms step_avg:44.16ms
step:1042/2160 train_time:46007ms step_avg:44.15ms
step:1043/2160 train_time:46070ms step_avg:44.17ms
step:1044/2160 train_time:46133ms step_avg:44.19ms
step:1045/2160 train_time:46198ms step_avg:44.21ms
step:1045/2160 val_loss:3.6588 train_time:46257ms step_avg:44.27ms
step:1046/2160 train_time:46281ms step_avg:44.25ms
step:1047/2160 train_time:46323ms step_avg:44.24ms
step:1048/2160 train_time:46388ms step_avg:44.26ms
step:1049/2160 train_time:46453ms step_avg:44.28ms
step:1050/2160 train_time:46515ms step_avg:44.30ms
step:1050/2160 val_loss:3.6585 train_time:46577ms step_avg:44.36ms
step:1051/2160 train_time:46599ms step_avg:44.34ms
step:1052/2160 train_time:46638ms step_avg:44.33ms
step:1053/2160 train_time:46703ms step_avg:44.35ms
step:1054/2160 train_time:46766ms step_avg:44.37ms
step:1055/2160 train_time:46827ms step_avg:44.39ms
step:1055/2160 val_loss:3.6551 train_time:46887ms step_avg:44.44ms
step:1056/2160 train_time:46912ms step_avg:44.42ms
step:1057/2160 train_time:46951ms step_avg:44.42ms
step:1058/2160 train_time:47014ms step_avg:44.44ms
step:1059/2160 train_time:47077ms step_avg:44.45ms
step:1060/2160 train_time:47138ms step_avg:44.47ms
step:1060/2160 val_loss:3.6672 train_time:47199ms step_avg:44.53ms
step:1061/2160 train_time:47222ms step_avg:44.51ms
step:1062/2160 train_time:47260ms step_avg:44.50ms
step:1063/2160 train_time:47325ms step_avg:44.52ms
step:1064/2160 train_time:47386ms step_avg:44.54ms
step:1065/2160 train_time:47447ms step_avg:44.55ms
step:1065/2160 val_loss:3.6517 train_time:47507ms step_avg:44.61ms
step:1066/2160 train_time:47531ms step_avg:44.59ms
step:1067/2160 train_time:47570ms step_avg:44.58ms
step:1068/2160 train_time:47633ms step_avg:44.60ms
step:1069/2160 train_time:47699ms step_avg:44.62ms
step:1070/2160 train_time:47759ms step_avg:44.63ms
step:1070/2160 val_loss:3.6491 train_time:47821ms step_avg:44.69ms
step:1071/2160 train_time:47844ms step_avg:44.67ms
step:1072/2160 train_time:47882ms step_avg:44.67ms
step:1073/2160 train_time:47947ms step_avg:44.68ms
step:1074/2160 train_time:48010ms step_avg:44.70ms
step:1075/2160 train_time:48072ms step_avg:44.72ms
step:1075/2160 val_loss:3.6449 train_time:48132ms step_avg:44.77ms
step:1076/2160 train_time:48156ms step_avg:44.75ms
step:1077/2160 train_time:48196ms step_avg:44.75ms
step:1078/2160 train_time:48258ms step_avg:44.77ms
step:1079/2160 train_time:48323ms step_avg:44.78ms
step:1080/2160 train_time:48384ms step_avg:44.80ms
step:1080/2160 val_loss:3.6444 train_time:48446ms step_avg:44.86ms
step:1081/2160 train_time:48469ms step_avg:44.84ms
step:1082/2160 train_time:48508ms step_avg:44.83ms
step:1083/2160 train_time:48572ms step_avg:44.85ms
step:1084/2160 train_time:48634ms step_avg:44.87ms
step:1085/2160 train_time:48695ms step_avg:44.88ms
step:1085/2160 val_loss:3.6399 train_time:48755ms step_avg:44.94ms
step:1086/2160 train_time:48779ms step_avg:44.92ms
step:1087/2160 train_time:48819ms step_avg:44.91ms
step:1088/2160 train_time:48881ms step_avg:44.93ms
step:1089/2160 train_time:48945ms step_avg:44.95ms
step:1090/2160 train_time:49005ms step_avg:44.96ms
step:1090/2160 val_loss:3.6395 train_time:49067ms step_avg:45.02ms
step:1091/2160 train_time:49089ms step_avg:44.99ms
step:1092/2160 train_time:49128ms step_avg:44.99ms
step:1093/2160 train_time:49193ms step_avg:45.01ms
step:1094/2160 train_time:49255ms step_avg:45.02ms
step:1095/2160 train_time:49317ms step_avg:45.04ms
step:1095/2160 val_loss:3.6357 train_time:49377ms step_avg:45.09ms
step:1096/2160 train_time:49401ms step_avg:45.07ms
step:1097/2160 train_time:49443ms step_avg:45.07ms
step:1098/2160 train_time:49502ms step_avg:45.08ms
step:1099/2160 train_time:49567ms step_avg:45.10ms
step:1100/2160 train_time:49627ms step_avg:45.12ms
step:1100/2160 val_loss:3.6327 train_time:49689ms step_avg:45.17ms
step:1101/2160 train_time:49712ms step_avg:45.15ms
step:1102/2160 train_time:49751ms step_avg:45.15ms
step:1103/2160 train_time:49815ms step_avg:45.16ms
step:1104/2160 train_time:49880ms step_avg:45.18ms
step:1105/2160 train_time:49944ms step_avg:45.20ms
step:1105/2160 val_loss:3.6298 train_time:50003ms step_avg:45.25ms
step:1106/2160 train_time:50027ms step_avg:45.23ms
step:1107/2160 train_time:50066ms step_avg:45.23ms
step:1108/2160 train_time:50129ms step_avg:45.24ms
step:1109/2160 train_time:50195ms step_avg:45.26ms
step:1110/2160 train_time:50255ms step_avg:45.28ms
step:1110/2160 val_loss:3.6348 train_time:50317ms step_avg:45.33ms
step:1111/2160 train_time:50340ms step_avg:45.31ms
step:1112/2160 train_time:50379ms step_avg:45.31ms
step:1113/2160 train_time:50446ms step_avg:45.32ms
step:1114/2160 train_time:50509ms step_avg:45.34ms
step:1115/2160 train_time:50571ms step_avg:45.36ms
step:1115/2160 val_loss:3.6250 train_time:50631ms step_avg:45.41ms
step:1116/2160 train_time:50655ms step_avg:45.39ms
step:1117/2160 train_time:50696ms step_avg:45.39ms
step:1118/2160 train_time:50759ms step_avg:45.40ms
step:1119/2160 train_time:50824ms step_avg:45.42ms
step:1120/2160 train_time:50884ms step_avg:45.43ms
step:1120/2160 val_loss:3.6249 train_time:50945ms step_avg:45.49ms
step:1121/2160 train_time:50968ms step_avg:45.47ms
step:1122/2160 train_time:51007ms step_avg:45.46ms
step:1123/2160 train_time:51071ms step_avg:45.48ms
step:1124/2160 train_time:51134ms step_avg:45.49ms
step:1125/2160 train_time:51197ms step_avg:45.51ms
step:1125/2160 val_loss:3.6202 train_time:51257ms step_avg:45.56ms
step:1126/2160 train_time:51281ms step_avg:45.54ms
step:1127/2160 train_time:51319ms step_avg:45.54ms
step:1128/2160 train_time:51383ms step_avg:45.55ms
step:1129/2160 train_time:51448ms step_avg:45.57ms
step:1130/2160 train_time:51508ms step_avg:45.58ms
step:1130/2160 val_loss:3.6204 train_time:51570ms step_avg:45.64ms
step:1131/2160 train_time:51592ms step_avg:45.62ms
step:1132/2160 train_time:51631ms step_avg:45.61ms
step:1133/2160 train_time:51697ms step_avg:45.63ms
step:1134/2160 train_time:51760ms step_avg:45.64ms
step:1135/2160 train_time:51822ms step_avg:45.66ms
step:1135/2160 val_loss:3.6169 train_time:51882ms step_avg:45.71ms
step:1136/2160 train_time:51907ms step_avg:45.69ms
step:1137/2160 train_time:51945ms step_avg:45.69ms
step:1138/2160 train_time:52011ms step_avg:45.70ms
step:1139/2160 train_time:52075ms step_avg:45.72ms
step:1140/2160 train_time:52134ms step_avg:45.73ms
step:1140/2160 val_loss:3.6172 train_time:52196ms step_avg:45.79ms
step:1141/2160 train_time:52218ms step_avg:45.77ms
step:1142/2160 train_time:52259ms step_avg:45.76ms
step:1143/2160 train_time:52324ms step_avg:45.78ms
step:1144/2160 train_time:52388ms step_avg:45.79ms
step:1145/2160 train_time:52450ms step_avg:45.81ms
step:1145/2160 val_loss:3.6115 train_time:52510ms step_avg:45.86ms
step:1146/2160 train_time:52534ms step_avg:45.84ms
step:1147/2160 train_time:52574ms step_avg:45.84ms
step:1148/2160 train_time:52635ms step_avg:45.85ms
step:1149/2160 train_time:52702ms step_avg:45.87ms
step:1150/2160 train_time:52762ms step_avg:45.88ms
step:1150/2160 val_loss:3.6144 train_time:52825ms step_avg:45.93ms
step:1151/2160 train_time:52849ms step_avg:45.92ms
step:1152/2160 train_time:52886ms step_avg:45.91ms
step:1153/2160 train_time:52950ms step_avg:45.92ms
step:1154/2160 train_time:53013ms step_avg:45.94ms
step:1155/2160 train_time:53075ms step_avg:45.95ms
step:1155/2160 val_loss:3.6095 train_time:53135ms step_avg:46.00ms
step:1156/2160 train_time:53159ms step_avg:45.99ms
step:1157/2160 train_time:53199ms step_avg:45.98ms
step:1158/2160 train_time:53261ms step_avg:45.99ms
step:1159/2160 train_time:53326ms step_avg:46.01ms
step:1160/2160 train_time:53387ms step_avg:46.02ms
step:1160/2160 val_loss:3.6063 train_time:53449ms step_avg:46.08ms
step:1161/2160 train_time:53472ms step_avg:46.06ms
step:1162/2160 train_time:53510ms step_avg:46.05ms
step:1163/2160 train_time:53576ms step_avg:46.07ms
step:1164/2160 train_time:53637ms step_avg:46.08ms
step:1165/2160 train_time:53698ms step_avg:46.09ms
step:1165/2160 val_loss:3.6032 train_time:53757ms step_avg:46.14ms
step:1166/2160 train_time:53784ms step_avg:46.13ms
step:1167/2160 train_time:53820ms step_avg:46.12ms
step:1168/2160 train_time:53883ms step_avg:46.13ms
step:1169/2160 train_time:53949ms step_avg:46.15ms
step:1170/2160 train_time:54008ms step_avg:46.16ms
step:1170/2160 val_loss:3.6020 train_time:54071ms step_avg:46.21ms
step:1171/2160 train_time:54093ms step_avg:46.19ms
step:1172/2160 train_time:54132ms step_avg:46.19ms
step:1173/2160 train_time:54197ms step_avg:46.20ms
step:1174/2160 train_time:54261ms step_avg:46.22ms
step:1175/2160 train_time:54322ms step_avg:46.23ms
step:1175/2160 val_loss:3.5992 train_time:54382ms step_avg:46.28ms
step:1176/2160 train_time:54405ms step_avg:46.26ms
step:1177/2160 train_time:54446ms step_avg:46.26ms
step:1178/2160 train_time:54508ms step_avg:46.27ms
step:1179/2160 train_time:54573ms step_avg:46.29ms
step:1180/2160 train_time:54635ms step_avg:46.30ms
step:1180/2160 val_loss:3.5989 train_time:54698ms step_avg:46.35ms
step:1181/2160 train_time:54720ms step_avg:46.33ms
step:1182/2160 train_time:54760ms step_avg:46.33ms
step:1183/2160 train_time:54826ms step_avg:46.34ms
step:1184/2160 train_time:54890ms step_avg:46.36ms
step:1185/2160 train_time:54952ms step_avg:46.37ms
step:1185/2160 val_loss:3.5946 train_time:55012ms step_avg:46.42ms
step:1186/2160 train_time:55037ms step_avg:46.41ms
step:1187/2160 train_time:55077ms step_avg:46.40ms
step:1188/2160 train_time:55138ms step_avg:46.41ms
step:1189/2160 train_time:55203ms step_avg:46.43ms
step:1190/2160 train_time:55263ms step_avg:46.44ms
step:1190/2160 val_loss:3.5957 train_time:55325ms step_avg:46.49ms
step:1191/2160 train_time:55348ms step_avg:46.47ms
step:1192/2160 train_time:55386ms step_avg:46.46ms
step:1193/2160 train_time:55452ms step_avg:46.48ms
step:1194/2160 train_time:55513ms step_avg:46.49ms
step:1195/2160 train_time:55575ms step_avg:46.51ms
step:1195/2160 val_loss:3.5925 train_time:55634ms step_avg:46.56ms
step:1196/2160 train_time:55658ms step_avg:46.54ms
step:1197/2160 train_time:55698ms step_avg:46.53ms
step:1198/2160 train_time:55762ms step_avg:46.55ms
step:1199/2160 train_time:55828ms step_avg:46.56ms
step:1200/2160 train_time:55890ms step_avg:46.58ms
step:1200/2160 val_loss:3.5903 train_time:55952ms step_avg:46.63ms
step:1201/2160 train_time:55974ms step_avg:46.61ms
step:1202/2160 train_time:56014ms step_avg:46.60ms
step:1203/2160 train_time:56079ms step_avg:46.62ms
step:1204/2160 train_time:56142ms step_avg:46.63ms
step:1205/2160 train_time:56206ms step_avg:46.64ms
step:1205/2160 val_loss:3.5858 train_time:56266ms step_avg:46.69ms
step:1206/2160 train_time:56290ms step_avg:46.67ms
step:1207/2160 train_time:56329ms step_avg:46.67ms
step:1208/2160 train_time:56391ms step_avg:46.68ms
step:1209/2160 train_time:56456ms step_avg:46.70ms
step:1210/2160 train_time:56515ms step_avg:46.71ms
step:1210/2160 val_loss:3.5867 train_time:56576ms step_avg:46.76ms
step:1211/2160 train_time:56599ms step_avg:46.74ms
step:1212/2160 train_time:56637ms step_avg:46.73ms
step:1213/2160 train_time:56701ms step_avg:46.74ms
step:1214/2160 train_time:56763ms step_avg:46.76ms
step:1215/2160 train_time:56824ms step_avg:46.77ms
step:1215/2160 val_loss:3.5841 train_time:56884ms step_avg:46.82ms
step:1216/2160 train_time:56908ms step_avg:46.80ms
step:1217/2160 train_time:56947ms step_avg:46.79ms
step:1218/2160 train_time:57011ms step_avg:46.81ms
step:1219/2160 train_time:57074ms step_avg:46.82ms
step:1220/2160 train_time:57134ms step_avg:46.83ms
step:1220/2160 val_loss:3.5845 train_time:57196ms step_avg:46.88ms
step:1221/2160 train_time:57219ms step_avg:46.86ms
step:1222/2160 train_time:57258ms step_avg:46.86ms
step:1223/2160 train_time:57324ms step_avg:46.87ms
step:1224/2160 train_time:57386ms step_avg:46.88ms
step:1225/2160 train_time:57448ms step_avg:46.90ms
step:1225/2160 val_loss:3.5792 train_time:57508ms step_avg:46.95ms
step:1226/2160 train_time:57532ms step_avg:46.93ms
step:1227/2160 train_time:57571ms step_avg:46.92ms
step:1228/2160 train_time:57635ms step_avg:46.93ms
step:1229/2160 train_time:57699ms step_avg:46.95ms
step:1230/2160 train_time:57760ms step_avg:46.96ms
step:1230/2160 val_loss:3.5770 train_time:57822ms step_avg:47.01ms
step:1231/2160 train_time:57844ms step_avg:46.99ms
step:1232/2160 train_time:57883ms step_avg:46.98ms
step:1233/2160 train_time:57948ms step_avg:47.00ms
step:1234/2160 train_time:58012ms step_avg:47.01ms
step:1235/2160 train_time:58074ms step_avg:47.02ms
step:1235/2160 val_loss:3.5742 train_time:58133ms step_avg:47.07ms
step:1236/2160 train_time:58157ms step_avg:47.05ms
step:1237/2160 train_time:58197ms step_avg:47.05ms
step:1238/2160 train_time:58258ms step_avg:47.06ms
step:1239/2160 train_time:58323ms step_avg:47.07ms
step:1240/2160 train_time:58384ms step_avg:47.08ms
step:1240/2160 val_loss:3.5727 train_time:58447ms step_avg:47.13ms
step:1241/2160 train_time:58469ms step_avg:47.11ms
step:1242/2160 train_time:58508ms step_avg:47.11ms
step:1243/2160 train_time:58573ms step_avg:47.12ms
step:1244/2160 train_time:58636ms step_avg:47.14ms
step:1245/2160 train_time:58699ms step_avg:47.15ms
step:1245/2160 val_loss:3.5693 train_time:58759ms step_avg:47.20ms
step:1246/2160 train_time:58784ms step_avg:47.18ms
step:1247/2160 train_time:58824ms step_avg:47.17ms
step:1248/2160 train_time:58887ms step_avg:47.19ms
step:1249/2160 train_time:58952ms step_avg:47.20ms
step:1250/2160 train_time:59012ms step_avg:47.21ms
step:1250/2160 val_loss:3.5687 train_time:59074ms step_avg:47.26ms
step:1251/2160 train_time:59096ms step_avg:47.24ms
step:1252/2160 train_time:59134ms step_avg:47.23ms
step:1253/2160 train_time:59198ms step_avg:47.25ms
step:1254/2160 train_time:59261ms step_avg:47.26ms
step:1255/2160 train_time:59323ms step_avg:47.27ms
step:1255/2160 val_loss:3.5672 train_time:59382ms step_avg:47.32ms
step:1256/2160 train_time:59406ms step_avg:47.30ms
step:1257/2160 train_time:59447ms step_avg:47.29ms
step:1258/2160 train_time:59508ms step_avg:47.30ms
step:1259/2160 train_time:59572ms step_avg:47.32ms
step:1260/2160 train_time:59633ms step_avg:47.33ms
step:1260/2160 val_loss:3.5654 train_time:59695ms step_avg:47.38ms
step:1261/2160 train_time:59718ms step_avg:47.36ms
step:1262/2160 train_time:59757ms step_avg:47.35ms
step:1263/2160 train_time:59823ms step_avg:47.37ms
step:1264/2160 train_time:59885ms step_avg:47.38ms
step:1265/2160 train_time:59946ms step_avg:47.39ms
step:1265/2160 val_loss:3.5622 train_time:60006ms step_avg:47.44ms
step:1266/2160 train_time:60030ms step_avg:47.42ms
step:1267/2160 train_time:60069ms step_avg:47.41ms
step:1268/2160 train_time:60133ms step_avg:47.42ms
step:1269/2160 train_time:60198ms step_avg:47.44ms
step:1270/2160 train_time:60257ms step_avg:47.45ms
step:1270/2160 val_loss:3.5596 train_time:60320ms step_avg:47.50ms
step:1271/2160 train_time:60343ms step_avg:47.48ms
step:1272/2160 train_time:60383ms step_avg:47.47ms
step:1273/2160 train_time:60447ms step_avg:47.48ms
step:1274/2160 train_time:60510ms step_avg:47.50ms
step:1275/2160 train_time:60572ms step_avg:47.51ms
step:1275/2160 val_loss:3.5583 train_time:60632ms step_avg:47.55ms
step:1276/2160 train_time:60657ms step_avg:47.54ms
step:1277/2160 train_time:60697ms step_avg:47.53ms
step:1278/2160 train_time:60758ms step_avg:47.54ms
step:1279/2160 train_time:60822ms step_avg:47.55ms
step:1280/2160 train_time:60881ms step_avg:47.56ms
step:1280/2160 val_loss:3.5582 train_time:60943ms step_avg:47.61ms
step:1281/2160 train_time:60966ms step_avg:47.59ms
step:1282/2160 train_time:61003ms step_avg:47.58ms
step:1283/2160 train_time:61068ms step_avg:47.60ms
step:1284/2160 train_time:61132ms step_avg:47.61ms
step:1285/2160 train_time:61193ms step_avg:47.62ms
step:1285/2160 val_loss:3.5554 train_time:61254ms step_avg:47.67ms
step:1286/2160 train_time:61278ms step_avg:47.65ms
step:1287/2160 train_time:61318ms step_avg:47.64ms
step:1288/2160 train_time:61381ms step_avg:47.66ms
step:1289/2160 train_time:61445ms step_avg:47.67ms
step:1290/2160 train_time:61504ms step_avg:47.68ms
step:1290/2160 val_loss:3.5533 train_time:61567ms step_avg:47.73ms
step:1291/2160 train_time:61589ms step_avg:47.71ms
step:1292/2160 train_time:61630ms step_avg:47.70ms
step:1293/2160 train_time:61696ms step_avg:47.72ms
step:1294/2160 train_time:61758ms step_avg:47.73ms
step:1295/2160 train_time:61820ms step_avg:47.74ms
step:1295/2160 val_loss:3.5510 train_time:61879ms step_avg:47.78ms
step:1296/2160 train_time:61903ms step_avg:47.76ms
step:1297/2160 train_time:61942ms step_avg:47.76ms
step:1298/2160 train_time:62004ms step_avg:47.77ms
step:1299/2160 train_time:62070ms step_avg:47.78ms
step:1300/2160 train_time:62131ms step_avg:47.79ms
step:1300/2160 val_loss:3.5487 train_time:62192ms step_avg:47.84ms
step:1301/2160 train_time:62215ms step_avg:47.82ms
step:1302/2160 train_time:62253ms step_avg:47.81ms
step:1303/2160 train_time:62318ms step_avg:47.83ms
step:1304/2160 train_time:62381ms step_avg:47.84ms
step:1305/2160 train_time:62444ms step_avg:47.85ms
step:1305/2160 val_loss:3.5470 train_time:62503ms step_avg:47.90ms
step:1306/2160 train_time:62528ms step_avg:47.88ms
step:1307/2160 train_time:62567ms step_avg:47.87ms
step:1308/2160 train_time:62631ms step_avg:47.88ms
step:1309/2160 train_time:62695ms step_avg:47.90ms
step:1310/2160 train_time:62755ms step_avg:47.90ms
step:1310/2160 val_loss:3.5458 train_time:62817ms step_avg:47.95ms
step:1311/2160 train_time:62840ms step_avg:47.93ms
step:1312/2160 train_time:62879ms step_avg:47.93ms
step:1313/2160 train_time:62945ms step_avg:47.94ms
step:1314/2160 train_time:63008ms step_avg:47.95ms
step:1315/2160 train_time:63070ms step_avg:47.96ms
step:1315/2160 val_loss:3.5435 train_time:63129ms step_avg:48.01ms
step:1316/2160 train_time:63153ms step_avg:47.99ms
step:1317/2160 train_time:63193ms step_avg:47.98ms
step:1318/2160 train_time:63256ms step_avg:47.99ms
step:1319/2160 train_time:63320ms step_avg:48.01ms
step:1320/2160 train_time:63380ms step_avg:48.02ms
step:1320/2160 val_loss:3.5437 train_time:63442ms step_avg:48.06ms
step:1321/2160 train_time:63464ms step_avg:48.04ms
step:1322/2160 train_time:63505ms step_avg:48.04ms
step:1323/2160 train_time:63570ms step_avg:48.05ms
step:1324/2160 train_time:63632ms step_avg:48.06ms
step:1325/2160 train_time:63695ms step_avg:48.07ms
step:1325/2160 val_loss:3.5390 train_time:63754ms step_avg:48.12ms
step:1326/2160 train_time:63778ms step_avg:48.10ms
step:1327/2160 train_time:63821ms step_avg:48.09ms
step:1328/2160 train_time:63882ms step_avg:48.10ms
step:1329/2160 train_time:63947ms step_avg:48.12ms
step:1330/2160 train_time:64007ms step_avg:48.13ms
step:1330/2160 val_loss:3.5404 train_time:64069ms step_avg:48.17ms
step:1331/2160 train_time:64091ms step_avg:48.15ms
step:1332/2160 train_time:64131ms step_avg:48.15ms
step:1333/2160 train_time:64198ms step_avg:48.16ms
step:1334/2160 train_time:64260ms step_avg:48.17ms
step:1335/2160 train_time:64322ms step_avg:48.18ms
step:1335/2160 val_loss:3.5378 train_time:64381ms step_avg:48.23ms
step:1336/2160 train_time:64405ms step_avg:48.21ms
step:1337/2160 train_time:64445ms step_avg:48.20ms
step:1338/2160 train_time:64507ms step_avg:48.21ms
step:1339/2160 train_time:64572ms step_avg:48.22ms
step:1340/2160 train_time:64634ms step_avg:48.23ms
step:1340/2160 val_loss:3.5358 train_time:64696ms step_avg:48.28ms
step:1341/2160 train_time:64718ms step_avg:48.26ms
step:1342/2160 train_time:64758ms step_avg:48.25ms
step:1343/2160 train_time:64822ms step_avg:48.27ms
step:1344/2160 train_time:64887ms step_avg:48.28ms
step:1345/2160 train_time:64949ms step_avg:48.29ms
step:1345/2160 val_loss:3.5320 train_time:65009ms step_avg:48.33ms
step:1346/2160 train_time:65033ms step_avg:48.32ms
step:1347/2160 train_time:65072ms step_avg:48.31ms
step:1348/2160 train_time:65133ms step_avg:48.32ms
step:1349/2160 train_time:65198ms step_avg:48.33ms
step:1350/2160 train_time:65260ms step_avg:48.34ms
step:1350/2160 val_loss:3.5334 train_time:65322ms step_avg:48.39ms
step:1351/2160 train_time:65345ms step_avg:48.37ms
step:1352/2160 train_time:65386ms step_avg:48.36ms
step:1353/2160 train_time:65451ms step_avg:48.37ms
step:1354/2160 train_time:65514ms step_avg:48.39ms
step:1355/2160 train_time:65577ms step_avg:48.40ms
step:1355/2160 val_loss:3.5292 train_time:65637ms step_avg:48.44ms
step:1356/2160 train_time:65661ms step_avg:48.42ms
step:1357/2160 train_time:65703ms step_avg:48.42ms
step:1358/2160 train_time:65765ms step_avg:48.43ms
step:1359/2160 train_time:65831ms step_avg:48.44ms
step:1360/2160 train_time:65891ms step_avg:48.45ms
step:1360/2160 val_loss:3.5325 train_time:65953ms step_avg:48.50ms
step:1361/2160 train_time:65976ms step_avg:48.48ms
step:1362/2160 train_time:66016ms step_avg:48.47ms
step:1363/2160 train_time:66080ms step_avg:48.48ms
step:1364/2160 train_time:66144ms step_avg:48.49ms
step:1365/2160 train_time:66206ms step_avg:48.50ms
step:1365/2160 val_loss:3.5290 train_time:66266ms step_avg:48.55ms
step:1366/2160 train_time:66290ms step_avg:48.53ms
step:1367/2160 train_time:66331ms step_avg:48.52ms
step:1368/2160 train_time:66392ms step_avg:48.53ms
step:1369/2160 train_time:66457ms step_avg:48.54ms
step:1370/2160 train_time:66517ms step_avg:48.55ms
step:1370/2160 val_loss:3.5255 train_time:66579ms step_avg:48.60ms
step:1371/2160 train_time:66601ms step_avg:48.58ms
step:1372/2160 train_time:66640ms step_avg:48.57ms
step:1373/2160 train_time:66705ms step_avg:48.58ms
step:1374/2160 train_time:66765ms step_avg:48.59ms
step:1375/2160 train_time:66826ms step_avg:48.60ms
step:1375/2160 val_loss:3.5227 train_time:66885ms step_avg:48.64ms
step:1376/2160 train_time:66911ms step_avg:48.63ms
step:1377/2160 train_time:66949ms step_avg:48.62ms
step:1378/2160 train_time:67013ms step_avg:48.63ms
step:1379/2160 train_time:67078ms step_avg:48.64ms
step:1380/2160 train_time:67139ms step_avg:48.65ms
step:1380/2160 val_loss:3.5208 train_time:67201ms step_avg:48.70ms
step:1381/2160 train_time:67223ms step_avg:48.68ms
step:1382/2160 train_time:67262ms step_avg:48.67ms
step:1383/2160 train_time:67327ms step_avg:48.68ms
step:1384/2160 train_time:67391ms step_avg:48.69ms
step:1385/2160 train_time:67454ms step_avg:48.70ms
step:1385/2160 val_loss:3.5191 train_time:67513ms step_avg:48.75ms
step:1386/2160 train_time:67537ms step_avg:48.73ms
step:1387/2160 train_time:67578ms step_avg:48.72ms
step:1388/2160 train_time:67642ms step_avg:48.73ms
step:1389/2160 train_time:67707ms step_avg:48.75ms
step:1390/2160 train_time:67769ms step_avg:48.75ms
step:1390/2160 val_loss:3.5175 train_time:67832ms step_avg:48.80ms
step:1391/2160 train_time:67855ms step_avg:48.78ms
step:1392/2160 train_time:67893ms step_avg:48.77ms
step:1393/2160 train_time:67958ms step_avg:48.79ms
step:1394/2160 train_time:68022ms step_avg:48.80ms
step:1395/2160 train_time:68085ms step_avg:48.81ms
step:1395/2160 val_loss:3.5155 train_time:68144ms step_avg:48.85ms
step:1396/2160 train_time:68168ms step_avg:48.83ms
step:1397/2160 train_time:68209ms step_avg:48.83ms
step:1398/2160 train_time:68270ms step_avg:48.83ms
step:1399/2160 train_time:68335ms step_avg:48.85ms
step:1400/2160 train_time:68397ms step_avg:48.85ms
step:1400/2160 val_loss:3.5131 train_time:68459ms step_avg:48.90ms
step:1401/2160 train_time:68481ms step_avg:48.88ms
step:1402/2160 train_time:68520ms step_avg:48.87ms
step:1403/2160 train_time:68586ms step_avg:48.89ms
step:1404/2160 train_time:68649ms step_avg:48.90ms
step:1405/2160 train_time:68712ms step_avg:48.91ms
step:1405/2160 val_loss:3.5195 train_time:68772ms step_avg:48.95ms
step:1406/2160 train_time:68796ms step_avg:48.93ms
step:1407/2160 train_time:68837ms step_avg:48.92ms
step:1408/2160 train_time:68900ms step_avg:48.93ms
step:1409/2160 train_time:68964ms step_avg:48.95ms
step:1410/2160 train_time:69024ms step_avg:48.95ms
step:1410/2160 val_loss:3.5233 train_time:69086ms step_avg:49.00ms
step:1411/2160 train_time:69109ms step_avg:48.98ms
step:1412/2160 train_time:69148ms step_avg:48.97ms
step:1413/2160 train_time:69215ms step_avg:48.98ms
step:1414/2160 train_time:69278ms step_avg:48.99ms
step:1415/2160 train_time:69339ms step_avg:49.00ms
step:1415/2160 val_loss:3.5175 train_time:69426ms step_avg:49.06ms
step:1416/2160 train_time:69452ms step_avg:49.05ms
step:1417/2160 train_time:69518ms step_avg:49.06ms
step:1418/2160 train_time:69614ms step_avg:49.09ms
step:1419/2160 train_time:69704ms step_avg:49.12ms
step:1420/2160 train_time:69790ms step_avg:49.15ms
step:1420/2160 val_loss:3.5310 train_time:69878ms step_avg:49.21ms
step:1421/2160 train_time:69901ms step_avg:49.19ms
step:1422/2160 train_time:69971ms step_avg:49.21ms
step:1423/2160 train_time:70067ms step_avg:49.24ms
step:1424/2160 train_time:70154ms step_avg:49.27ms
step:1425/2160 train_time:70243ms step_avg:49.29ms
step:1425/2160 val_loss:3.5061 train_time:70328ms step_avg:49.35ms
step:1426/2160 train_time:70353ms step_avg:49.34ms
step:1427/2160 train_time:70422ms step_avg:49.35ms
step:1428/2160 train_time:70516ms step_avg:49.38ms
step:1429/2160 train_time:70607ms step_avg:49.41ms
step:1430/2160 train_time:70694ms step_avg:49.44ms
step:1430/2160 val_loss:3.5030 train_time:70782ms step_avg:49.50ms
step:1431/2160 train_time:70805ms step_avg:49.48ms
step:1432/2160 train_time:70872ms step_avg:49.49ms
step:1433/2160 train_time:70964ms step_avg:49.52ms
step:1434/2160 train_time:71052ms step_avg:49.55ms
step:1435/2160 train_time:71140ms step_avg:49.58ms
step:1435/2160 val_loss:3.4990 train_time:71226ms step_avg:49.64ms
step:1436/2160 train_time:71251ms step_avg:49.62ms
step:1437/2160 train_time:71320ms step_avg:49.63ms
step:1438/2160 train_time:71412ms step_avg:49.66ms
step:1439/2160 train_time:71500ms step_avg:49.69ms
step:1440/2160 train_time:71589ms step_avg:49.71ms
step:1440/2160 val_loss:3.4974 train_time:71677ms step_avg:49.78ms
step:1441/2160 train_time:71700ms step_avg:49.76ms
step:1442/2160 train_time:71768ms step_avg:49.77ms
step:1443/2160 train_time:71863ms step_avg:49.80ms
step:1444/2160 train_time:71951ms step_avg:49.83ms
step:1445/2160 train_time:72040ms step_avg:49.85ms
step:1445/2160 val_loss:3.4929 train_time:72126ms step_avg:49.91ms
step:1446/2160 train_time:72153ms step_avg:49.90ms
step:1447/2160 train_time:72218ms step_avg:49.91ms
step:1448/2160 train_time:72311ms step_avg:49.94ms
step:1449/2160 train_time:72400ms step_avg:49.97ms
step:1450/2160 train_time:72486ms step_avg:49.99ms
step:1450/2160 val_loss:3.4912 train_time:72574ms step_avg:50.05ms
step:1451/2160 train_time:72597ms step_avg:50.03ms
step:1452/2160 train_time:72666ms step_avg:50.05ms
step:1453/2160 train_time:72761ms step_avg:50.08ms
step:1454/2160 train_time:72851ms step_avg:50.10ms
step:1455/2160 train_time:72939ms step_avg:50.13ms
step:1455/2160 val_loss:3.4876 train_time:73025ms step_avg:50.19ms
step:1456/2160 train_time:73050ms step_avg:50.17ms
step:1457/2160 train_time:73118ms step_avg:50.18ms
step:1458/2160 train_time:73209ms step_avg:50.21ms
step:1459/2160 train_time:73299ms step_avg:50.24ms
step:1460/2160 train_time:73385ms step_avg:50.26ms
step:1460/2160 val_loss:3.4865 train_time:73473ms step_avg:50.32ms
step:1461/2160 train_time:73496ms step_avg:50.31ms
step:1462/2160 train_time:73563ms step_avg:50.32ms
step:1463/2160 train_time:73657ms step_avg:50.35ms
step:1464/2160 train_time:73744ms step_avg:50.37ms
step:1465/2160 train_time:73832ms step_avg:50.40ms
step:1465/2160 val_loss:3.4828 train_time:73918ms step_avg:50.46ms
step:1466/2160 train_time:73943ms step_avg:50.44ms
step:1467/2160 train_time:74013ms step_avg:50.45ms
step:1468/2160 train_time:74109ms step_avg:50.48ms
step:1469/2160 train_time:74200ms step_avg:50.51ms
step:1470/2160 train_time:74287ms step_avg:50.54ms
step:1470/2160 val_loss:3.4822 train_time:74375ms step_avg:50.60ms
step:1471/2160 train_time:74398ms step_avg:50.58ms
step:1472/2160 train_time:74467ms step_avg:50.59ms
step:1473/2160 train_time:74564ms step_avg:50.62ms
step:1474/2160 train_time:74652ms step_avg:50.65ms
step:1475/2160 train_time:74741ms step_avg:50.67ms
step:1475/2160 val_loss:3.4789 train_time:74827ms step_avg:50.73ms
step:1476/2160 train_time:74852ms step_avg:50.71ms
step:1477/2160 train_time:74923ms step_avg:50.73ms
step:1478/2160 train_time:75014ms step_avg:50.75ms
step:1479/2160 train_time:75105ms step_avg:50.78ms
step:1480/2160 train_time:75192ms step_avg:50.81ms
step:1480/2160 val_loss:3.4777 train_time:75280ms step_avg:50.86ms
step:1481/2160 train_time:75304ms step_avg:50.85ms
step:1482/2160 train_time:75369ms step_avg:50.86ms
step:1483/2160 train_time:75462ms step_avg:50.88ms
step:1484/2160 train_time:75550ms step_avg:50.91ms
step:1485/2160 train_time:75638ms step_avg:50.93ms
step:1485/2160 val_loss:3.4732 train_time:75724ms step_avg:50.99ms
step:1486/2160 train_time:75749ms step_avg:50.97ms
step:1487/2160 train_time:75816ms step_avg:50.99ms
step:1488/2160 train_time:75909ms step_avg:51.01ms
step:1489/2160 train_time:75998ms step_avg:51.04ms
step:1490/2160 train_time:76084ms step_avg:51.06ms
step:1490/2160 val_loss:3.4736 train_time:76172ms step_avg:51.12ms
step:1491/2160 train_time:76195ms step_avg:51.10ms
step:1492/2160 train_time:76263ms step_avg:51.11ms
step:1493/2160 train_time:76361ms step_avg:51.15ms
step:1494/2160 train_time:76450ms step_avg:51.17ms
step:1495/2160 train_time:76539ms step_avg:51.20ms
step:1495/2160 val_loss:3.4685 train_time:76625ms step_avg:51.25ms
step:1496/2160 train_time:76650ms step_avg:51.24ms
step:1497/2160 train_time:76718ms step_avg:51.25ms
step:1498/2160 train_time:76809ms step_avg:51.27ms
step:1499/2160 train_time:76899ms step_avg:51.30ms
step:1500/2160 train_time:76985ms step_avg:51.32ms
step:1500/2160 val_loss:3.4697 train_time:77074ms step_avg:51.38ms
step:1501/2160 train_time:77097ms step_avg:51.36ms
step:1502/2160 train_time:77164ms step_avg:51.37ms
step:1503/2160 train_time:77260ms step_avg:51.40ms
step:1504/2160 train_time:77348ms step_avg:51.43ms
step:1505/2160 train_time:77437ms step_avg:51.45ms
step:1505/2160 val_loss:3.4645 train_time:77523ms step_avg:51.51ms
step:1506/2160 train_time:77547ms step_avg:51.49ms
step:1507/2160 train_time:77615ms step_avg:51.50ms
step:1508/2160 train_time:77710ms step_avg:51.53ms
step:1509/2160 train_time:77801ms step_avg:51.56ms
step:1510/2160 train_time:77888ms step_avg:51.58ms
step:1510/2160 val_loss:3.4642 train_time:77976ms step_avg:51.64ms
step:1511/2160 train_time:78000ms step_avg:51.62ms
step:1512/2160 train_time:78070ms step_avg:51.63ms
step:1513/2160 train_time:78164ms step_avg:51.66ms
step:1514/2160 train_time:78251ms step_avg:51.69ms
step:1515/2160 train_time:78340ms step_avg:51.71ms
step:1515/2160 val_loss:3.4608 train_time:78426ms step_avg:51.77ms
step:1516/2160 train_time:78451ms step_avg:51.75ms
step:1517/2160 train_time:78523ms step_avg:51.76ms
step:1518/2160 train_time:78615ms step_avg:51.79ms
step:1519/2160 train_time:78704ms step_avg:51.81ms
step:1520/2160 train_time:78791ms step_avg:51.84ms
step:1520/2160 val_loss:3.4586 train_time:78880ms step_avg:51.89ms
step:1521/2160 train_time:78903ms step_avg:51.88ms
step:1522/2160 train_time:78971ms step_avg:51.89ms
step:1523/2160 train_time:79066ms step_avg:51.91ms
step:1524/2160 train_time:79153ms step_avg:51.94ms
step:1525/2160 train_time:79242ms step_avg:51.96ms
step:1525/2160 val_loss:3.4577 train_time:79328ms step_avg:52.02ms
step:1526/2160 train_time:79353ms step_avg:52.00ms
step:1527/2160 train_time:79419ms step_avg:52.01ms
step:1528/2160 train_time:79510ms step_avg:52.04ms
step:1529/2160 train_time:79599ms step_avg:52.06ms
step:1530/2160 train_time:79685ms step_avg:52.08ms
step:1530/2160 val_loss:3.4560 train_time:79774ms step_avg:52.14ms
step:1531/2160 train_time:79797ms step_avg:52.12ms
step:1532/2160 train_time:79864ms step_avg:52.13ms
step:1533/2160 train_time:79959ms step_avg:52.16ms
step:1534/2160 train_time:80047ms step_avg:52.18ms
step:1535/2160 train_time:80135ms step_avg:52.21ms
step:1535/2160 val_loss:3.4525 train_time:80221ms step_avg:52.26ms
step:1536/2160 train_time:80246ms step_avg:52.24ms
step:1537/2160 train_time:80314ms step_avg:52.25ms
step:1538/2160 train_time:80408ms step_avg:52.28ms
step:1539/2160 train_time:80498ms step_avg:52.31ms
step:1540/2160 train_time:80585ms step_avg:52.33ms
step:1540/2160 val_loss:3.4525 train_time:80673ms step_avg:52.39ms
step:1541/2160 train_time:80697ms step_avg:52.37ms
step:1542/2160 train_time:80763ms step_avg:52.38ms
step:1543/2160 train_time:80858ms step_avg:52.40ms
step:1544/2160 train_time:80946ms step_avg:52.43ms
step:1545/2160 train_time:81035ms step_avg:52.45ms
step:1545/2160 val_loss:3.4492 train_time:81121ms step_avg:52.51ms
step:1546/2160 train_time:81145ms step_avg:52.49ms
step:1547/2160 train_time:81218ms step_avg:52.50ms
step:1548/2160 train_time:81311ms step_avg:52.53ms
step:1549/2160 train_time:81399ms step_avg:52.55ms
step:1550/2160 train_time:81486ms step_avg:52.57ms
step:1550/2160 val_loss:3.4482 train_time:81574ms step_avg:52.63ms
step:1551/2160 train_time:81598ms step_avg:52.61ms
step:1552/2160 train_time:81667ms step_avg:52.62ms
step:1553/2160 train_time:81762ms step_avg:52.65ms
step:1554/2160 train_time:81849ms step_avg:52.67ms
step:1555/2160 train_time:81938ms step_avg:52.69ms
step:1555/2160 val_loss:3.4469 train_time:82024ms step_avg:52.75ms
step:1556/2160 train_time:82050ms step_avg:52.73ms
step:1557/2160 train_time:82117ms step_avg:52.74ms
step:1558/2160 train_time:82210ms step_avg:52.77ms
step:1559/2160 train_time:82300ms step_avg:52.79ms
step:1560/2160 train_time:82387ms step_avg:52.81ms
step:1560/2160 val_loss:3.4450 train_time:82475ms step_avg:52.87ms
step:1561/2160 train_time:82498ms step_avg:52.85ms
step:1562/2160 train_time:82565ms step_avg:52.86ms
step:1563/2160 train_time:82660ms step_avg:52.89ms
step:1564/2160 train_time:82747ms step_avg:52.91ms
step:1565/2160 train_time:82836ms step_avg:52.93ms
step:1565/2160 val_loss:3.4426 train_time:82921ms step_avg:52.98ms
step:1566/2160 train_time:82947ms step_avg:52.97ms
step:1567/2160 train_time:83015ms step_avg:52.98ms
step:1568/2160 train_time:83106ms step_avg:53.00ms
step:1569/2160 train_time:83197ms step_avg:53.03ms
step:1570/2160 train_time:83284ms step_avg:53.05ms
step:1570/2160 val_loss:3.4408 train_time:83373ms step_avg:53.10ms
step:1571/2160 train_time:83396ms step_avg:53.08ms
step:1572/2160 train_time:83464ms step_avg:53.09ms
step:1573/2160 train_time:83556ms step_avg:53.12ms
step:1574/2160 train_time:83645ms step_avg:53.14ms
step:1575/2160 train_time:83732ms step_avg:53.16ms
step:1575/2160 val_loss:3.4380 train_time:83818ms step_avg:53.22ms
step:1576/2160 train_time:83843ms step_avg:53.20ms
step:1577/2160 train_time:83912ms step_avg:53.21ms
step:1578/2160 train_time:84003ms step_avg:53.23ms
step:1579/2160 train_time:84092ms step_avg:53.26ms
step:1580/2160 train_time:84180ms step_avg:53.28ms
step:1580/2160 val_loss:3.4388 train_time:84268ms step_avg:53.33ms
step:1581/2160 train_time:84291ms step_avg:53.31ms
step:1582/2160 train_time:84361ms step_avg:53.33ms
step:1583/2160 train_time:84455ms step_avg:53.35ms
step:1584/2160 train_time:84543ms step_avg:53.37ms
step:1585/2160 train_time:84633ms step_avg:53.40ms
step:1585/2160 val_loss:3.4338 train_time:84719ms step_avg:53.45ms
step:1586/2160 train_time:84744ms step_avg:53.43ms
step:1587/2160 train_time:84813ms step_avg:53.44ms
step:1588/2160 train_time:84904ms step_avg:53.47ms
step:1589/2160 train_time:84994ms step_avg:53.49ms
step:1590/2160 train_time:85080ms step_avg:53.51ms
step:1590/2160 val_loss:3.4335 train_time:85169ms step_avg:53.57ms
step:1591/2160 train_time:85192ms step_avg:53.55ms
step:1592/2160 train_time:85263ms step_avg:53.56ms
step:1593/2160 train_time:85357ms step_avg:53.58ms
step:1594/2160 train_time:85446ms step_avg:53.60ms
step:1595/2160 train_time:85534ms step_avg:53.63ms
step:1595/2160 val_loss:3.4298 train_time:85621ms step_avg:53.68ms
step:1596/2160 train_time:85645ms step_avg:53.66ms
step:1597/2160 train_time:85716ms step_avg:53.67ms
step:1598/2160 train_time:85809ms step_avg:53.70ms
step:1599/2160 train_time:85900ms step_avg:53.72ms
step:1600/2160 train_time:85987ms step_avg:53.74ms
step:1600/2160 val_loss:3.4290 train_time:86075ms step_avg:53.80ms
step:1601/2160 train_time:86098ms step_avg:53.78ms
step:1602/2160 train_time:86167ms step_avg:53.79ms
step:1603/2160 train_time:86263ms step_avg:53.81ms
step:1604/2160 train_time:86350ms step_avg:53.83ms
step:1605/2160 train_time:86439ms step_avg:53.86ms
step:1605/2160 val_loss:3.4260 train_time:86525ms step_avg:53.91ms
step:1606/2160 train_time:86550ms step_avg:53.89ms
step:1607/2160 train_time:86617ms step_avg:53.90ms
step:1608/2160 train_time:86710ms step_avg:53.92ms
step:1609/2160 train_time:86799ms step_avg:53.95ms
step:1610/2160 train_time:86886ms step_avg:53.97ms
step:1610/2160 val_loss:3.4263 train_time:86974ms step_avg:54.02ms
step:1611/2160 train_time:86997ms step_avg:54.00ms
step:1612/2160 train_time:87064ms step_avg:54.01ms
step:1613/2160 train_time:87158ms step_avg:54.04ms
step:1614/2160 train_time:87246ms step_avg:54.06ms
step:1615/2160 train_time:87335ms step_avg:54.08ms
step:1615/2160 val_loss:3.4233 train_time:87421ms step_avg:54.13ms
step:1616/2160 train_time:87445ms step_avg:54.11ms
step:1617/2160 train_time:87512ms step_avg:54.12ms
step:1618/2160 train_time:87606ms step_avg:54.14ms
step:1619/2160 train_time:87696ms step_avg:54.17ms
step:1620/2160 train_time:87783ms step_avg:54.19ms
step:1620/2160 val_loss:3.4214 train_time:87870ms step_avg:54.24ms
step:1621/2160 train_time:87893ms step_avg:54.22ms
step:1622/2160 train_time:87961ms step_avg:54.23ms
step:1623/2160 train_time:88051ms step_avg:54.25ms
step:1624/2160 train_time:88138ms step_avg:54.27ms
step:1625/2160 train_time:88227ms step_avg:54.29ms
step:1625/2160 val_loss:3.4191 train_time:88313ms step_avg:54.35ms
step:1626/2160 train_time:88338ms step_avg:54.33ms
step:1627/2160 train_time:88408ms step_avg:54.34ms
step:1628/2160 train_time:88498ms step_avg:54.36ms
step:1629/2160 train_time:88587ms step_avg:54.38ms
step:1630/2160 train_time:88674ms step_avg:54.40ms
step:1630/2160 val_loss:3.4183 train_time:88761ms step_avg:54.45ms
step:1631/2160 train_time:88787ms step_avg:54.44ms
step:1632/2160 train_time:88852ms step_avg:54.44ms
step:1633/2160 train_time:88947ms step_avg:54.47ms
step:1634/2160 train_time:89036ms step_avg:54.49ms
step:1635/2160 train_time:89125ms step_avg:54.51ms
step:1635/2160 val_loss:3.4152 train_time:89210ms step_avg:54.56ms
step:1636/2160 train_time:89235ms step_avg:54.54ms
step:1637/2160 train_time:89304ms step_avg:54.55ms
step:1638/2160 train_time:89396ms step_avg:54.58ms
step:1639/2160 train_time:89486ms step_avg:54.60ms
step:1640/2160 train_time:89572ms step_avg:54.62ms
step:1640/2160 val_loss:3.4140 train_time:89661ms step_avg:54.67ms
step:1641/2160 train_time:89684ms step_avg:54.65ms
step:1642/2160 train_time:89753ms step_avg:54.66ms
step:1643/2160 train_time:89847ms step_avg:54.68ms
step:1644/2160 train_time:89934ms step_avg:54.70ms
step:1645/2160 train_time:90023ms step_avg:54.73ms
step:1645/2160 val_loss:3.4117 train_time:90110ms step_avg:54.78ms
step:1646/2160 train_time:90136ms step_avg:54.76ms
step:1647/2160 train_time:90204ms step_avg:54.77ms
step:1648/2160 train_time:90295ms step_avg:54.79ms
step:1649/2160 train_time:90384ms step_avg:54.81ms
step:1650/2160 train_time:90471ms step_avg:54.83ms
step:1650/2160 val_loss:3.4111 train_time:90558ms step_avg:54.88ms
step:1651/2160 train_time:90582ms step_avg:54.86ms
step:1652/2160 train_time:90649ms step_avg:54.87ms
step:1653/2160 train_time:90745ms step_avg:54.90ms
step:1654/2160 train_time:90833ms step_avg:54.92ms
step:1655/2160 train_time:90921ms step_avg:54.94ms
step:1655/2160 val_loss:3.4090 train_time:91008ms step_avg:54.99ms
step:1656/2160 train_time:91033ms step_avg:54.97ms
step:1657/2160 train_time:91100ms step_avg:54.98ms
step:1658/2160 train_time:91193ms step_avg:55.00ms
step:1659/2160 train_time:91283ms step_avg:55.02ms
step:1660/2160 train_time:91369ms step_avg:55.04ms
step:1660/2160 val_loss:3.4082 train_time:91459ms step_avg:55.10ms
step:1661/2160 train_time:91483ms step_avg:55.08ms
step:1662/2160 train_time:91550ms step_avg:55.08ms
step:1663/2160 train_time:91644ms step_avg:55.11ms
step:1664/2160 train_time:91732ms step_avg:55.13ms
step:1665/2160 train_time:91821ms step_avg:55.15ms
step:1665/2160 val_loss:3.4047 train_time:91907ms step_avg:55.20ms
step:1666/2160 train_time:91932ms step_avg:55.18ms
step:1667/2160 train_time:92002ms step_avg:55.19ms
step:1668/2160 train_time:92093ms step_avg:55.21ms
step:1669/2160 train_time:92184ms step_avg:55.23ms
step:1670/2160 train_time:92271ms step_avg:55.25ms
step:1670/2160 val_loss:3.4043 train_time:92361ms step_avg:55.31ms
step:1671/2160 train_time:92384ms step_avg:55.29ms
step:1672/2160 train_time:92453ms step_avg:55.30ms
step:1673/2160 train_time:92551ms step_avg:55.32ms
step:1674/2160 train_time:92641ms step_avg:55.34ms
step:1675/2160 train_time:92731ms step_avg:55.36ms
step:1675/2160 val_loss:3.4019 train_time:92817ms step_avg:55.41ms
step:1676/2160 train_time:92842ms step_avg:55.39ms
step:1677/2160 train_time:92909ms step_avg:55.40ms
step:1678/2160 train_time:93002ms step_avg:55.42ms
step:1679/2160 train_time:93093ms step_avg:55.45ms
step:1680/2160 train_time:93180ms step_avg:55.46ms
step:1680/2160 val_loss:3.4015 train_time:93269ms step_avg:55.52ms
step:1681/2160 train_time:93292ms step_avg:55.50ms
step:1682/2160 train_time:93362ms step_avg:55.51ms
step:1683/2160 train_time:93457ms step_avg:55.53ms
step:1684/2160 train_time:93544ms step_avg:55.55ms
step:1685/2160 train_time:93632ms step_avg:55.57ms
step:1685/2160 val_loss:3.3987 train_time:93719ms step_avg:55.62ms
step:1686/2160 train_time:93744ms step_avg:55.60ms
step:1687/2160 train_time:93813ms step_avg:55.61ms
step:1688/2160 train_time:93906ms step_avg:55.63ms
step:1689/2160 train_time:93995ms step_avg:55.65ms
step:1690/2160 train_time:94081ms step_avg:55.67ms
step:1690/2160 val_loss:3.3974 train_time:94171ms step_avg:55.72ms
step:1691/2160 train_time:94194ms step_avg:55.70ms
step:1692/2160 train_time:94263ms step_avg:55.71ms
step:1693/2160 train_time:94357ms step_avg:55.73ms
step:1694/2160 train_time:94446ms step_avg:55.75ms
step:1695/2160 train_time:94534ms step_avg:55.77ms
step:1695/2160 val_loss:3.3950 train_time:94620ms step_avg:55.82ms
step:1696/2160 train_time:94646ms step_avg:55.81ms
step:1697/2160 train_time:94715ms step_avg:55.81ms
step:1698/2160 train_time:94807ms step_avg:55.83ms
step:1699/2160 train_time:94897ms step_avg:55.85ms
step:1700/2160 train_time:94983ms step_avg:55.87ms
step:1700/2160 val_loss:3.3936 train_time:95071ms step_avg:55.92ms
step:1701/2160 train_time:95094ms step_avg:55.90ms
step:1702/2160 train_time:95161ms step_avg:55.91ms
step:1703/2160 train_time:95253ms step_avg:55.93ms
step:1704/2160 train_time:95340ms step_avg:55.95ms
step:1705/2160 train_time:95429ms step_avg:55.97ms
step:1705/2160 val_loss:3.3919 train_time:95515ms step_avg:56.02ms
step:1706/2160 train_time:95540ms step_avg:56.00ms
step:1707/2160 train_time:95607ms step_avg:56.01ms
step:1708/2160 train_time:95702ms step_avg:56.03ms
step:1709/2160 train_time:95792ms step_avg:56.05ms
step:1710/2160 train_time:95878ms step_avg:56.07ms
step:1710/2160 val_loss:3.3903 train_time:95966ms step_avg:56.12ms
step:1711/2160 train_time:95990ms step_avg:56.10ms
step:1712/2160 train_time:96060ms step_avg:56.11ms
step:1713/2160 train_time:96153ms step_avg:56.13ms
step:1714/2160 train_time:96242ms step_avg:56.15ms
step:1715/2160 train_time:96330ms step_avg:56.17ms
step:1715/2160 val_loss:3.3891 train_time:96417ms step_avg:56.22ms
step:1716/2160 train_time:96442ms step_avg:56.20ms
step:1717/2160 train_time:96511ms step_avg:56.21ms
step:1718/2160 train_time:96605ms step_avg:56.23ms
step:1719/2160 train_time:96695ms step_avg:56.25ms
step:1720/2160 train_time:96781ms step_avg:56.27ms
step:1720/2160 val_loss:3.3880 train_time:96870ms step_avg:56.32ms
step:1721/2160 train_time:96893ms step_avg:56.30ms
step:1722/2160 train_time:96962ms step_avg:56.31ms
step:1723/2160 train_time:97056ms step_avg:56.33ms
step:1724/2160 train_time:97145ms step_avg:56.35ms
step:1725/2160 train_time:97234ms step_avg:56.37ms
step:1725/2160 val_loss:3.3860 train_time:97319ms step_avg:56.42ms
step:1726/2160 train_time:97344ms step_avg:56.40ms
step:1727/2160 train_time:97413ms step_avg:56.41ms
step:1728/2160 train_time:97504ms step_avg:56.43ms
step:1729/2160 train_time:97594ms step_avg:56.45ms
step:1730/2160 train_time:97681ms step_avg:56.46ms
step:1730/2160 val_loss:3.3838 train_time:97769ms step_avg:56.51ms
step:1731/2160 train_time:97792ms step_avg:56.49ms
step:1732/2160 train_time:97861ms step_avg:56.50ms
step:1733/2160 train_time:97957ms step_avg:56.52ms
step:1734/2160 train_time:98043ms step_avg:56.54ms
step:1735/2160 train_time:98132ms step_avg:56.56ms
step:1735/2160 val_loss:3.3817 train_time:98218ms step_avg:56.61ms
step:1736/2160 train_time:98243ms step_avg:56.59ms
step:1737/2160 train_time:98315ms step_avg:56.60ms
step:1738/2160 train_time:98407ms step_avg:56.62ms
step:1739/2160 train_time:98496ms step_avg:56.64ms
step:1740/2160 train_time:98582ms step_avg:56.66ms
step:1740/2160 val_loss:3.3801 train_time:98670ms step_avg:56.71ms
step:1741/2160 train_time:98693ms step_avg:56.69ms
step:1742/2160 train_time:98764ms step_avg:56.70ms
step:1743/2160 train_time:98858ms step_avg:56.72ms
step:1744/2160 train_time:98945ms step_avg:56.73ms
step:1745/2160 train_time:99035ms step_avg:56.75ms
step:1745/2160 val_loss:3.3789 train_time:99120ms step_avg:56.80ms
step:1746/2160 train_time:99145ms step_avg:56.78ms
step:1747/2160 train_time:99211ms step_avg:56.79ms
step:1748/2160 train_time:99300ms step_avg:56.81ms
step:1749/2160 train_time:99389ms step_avg:56.83ms
step:1750/2160 train_time:99476ms step_avg:56.84ms
step:1750/2160 val_loss:3.3772 train_time:99564ms step_avg:56.89ms
step:1751/2160 train_time:99587ms step_avg:56.87ms
step:1752/2160 train_time:99655ms step_avg:56.88ms
step:1753/2160 train_time:99750ms step_avg:56.90ms
step:1754/2160 train_time:99838ms step_avg:56.92ms
step:1755/2160 train_time:99927ms step_avg:56.94ms
step:1755/2160 val_loss:3.3753 train_time:100014ms step_avg:56.99ms
step:1756/2160 train_time:100039ms step_avg:56.97ms
step:1757/2160 train_time:100108ms step_avg:56.98ms
step:1758/2160 train_time:100201ms step_avg:57.00ms
step:1759/2160 train_time:100289ms step_avg:57.02ms
step:1760/2160 train_time:100376ms step_avg:57.03ms
step:1760/2160 val_loss:3.3743 train_time:100464ms step_avg:57.08ms
step:1761/2160 train_time:100489ms step_avg:57.06ms
step:1762/2160 train_time:100556ms step_avg:57.07ms
step:1763/2160 train_time:100649ms step_avg:57.09ms
step:1764/2160 train_time:100737ms step_avg:57.11ms
step:1765/2160 train_time:100825ms step_avg:57.12ms
step:1765/2160 val_loss:3.3724 train_time:100911ms step_avg:57.17ms
step:1766/2160 train_time:100936ms step_avg:57.16ms
step:1767/2160 train_time:101004ms step_avg:57.16ms
step:1768/2160 train_time:101096ms step_avg:57.18ms
step:1769/2160 train_time:101186ms step_avg:57.20ms
step:1770/2160 train_time:101272ms step_avg:57.22ms
step:1770/2160 val_loss:3.3714 train_time:101360ms step_avg:57.27ms
step:1771/2160 train_time:101385ms step_avg:57.25ms
step:1772/2160 train_time:101451ms step_avg:57.25ms
step:1773/2160 train_time:101546ms step_avg:57.27ms
step:1774/2160 train_time:101635ms step_avg:57.29ms
step:1775/2160 train_time:101724ms step_avg:57.31ms
step:1775/2160 val_loss:3.3701 train_time:101810ms step_avg:57.36ms
step:1776/2160 train_time:101835ms step_avg:57.34ms
step:1777/2160 train_time:101903ms step_avg:57.35ms
step:1778/2160 train_time:101995ms step_avg:57.37ms
step:1779/2160 train_time:102085ms step_avg:57.38ms
step:1780/2160 train_time:102173ms step_avg:57.40ms
step:1780/2160 val_loss:3.3681 train_time:102260ms step_avg:57.45ms
step:1781/2160 train_time:102284ms step_avg:57.43ms
step:1782/2160 train_time:102355ms step_avg:57.44ms
step:1783/2160 train_time:102447ms step_avg:57.46ms
step:1784/2160 train_time:102534ms step_avg:57.47ms
step:1785/2160 train_time:102623ms step_avg:57.49ms
step:1785/2160 val_loss:3.3666 train_time:102709ms step_avg:57.54ms
step:1786/2160 train_time:102734ms step_avg:57.52ms
step:1787/2160 train_time:102804ms step_avg:57.53ms
step:1788/2160 train_time:102896ms step_avg:57.55ms
step:1789/2160 train_time:102986ms step_avg:57.57ms
step:1790/2160 train_time:103073ms step_avg:57.58ms
step:1790/2160 val_loss:3.3658 train_time:103161ms step_avg:57.63ms
step:1791/2160 train_time:103184ms step_avg:57.61ms
step:1792/2160 train_time:103251ms step_avg:57.62ms
step:1793/2160 train_time:103342ms step_avg:57.64ms
step:1794/2160 train_time:103430ms step_avg:57.65ms
step:1795/2160 train_time:103519ms step_avg:57.67ms
step:1795/2160 val_loss:3.3636 train_time:103605ms step_avg:57.72ms
step:1796/2160 train_time:103630ms step_avg:57.70ms
step:1797/2160 train_time:103697ms step_avg:57.71ms
step:1798/2160 train_time:103791ms step_avg:57.73ms
step:1799/2160 train_time:103883ms step_avg:57.75ms
step:1800/2160 train_time:103970ms step_avg:57.76ms
step:1800/2160 val_loss:3.3634 train_time:104058ms step_avg:57.81ms
step:1801/2160 train_time:104082ms step_avg:57.79ms
step:1802/2160 train_time:104150ms step_avg:57.80ms
step:1803/2160 train_time:104247ms step_avg:57.82ms
step:1804/2160 train_time:104334ms step_avg:57.84ms
step:1805/2160 train_time:104423ms step_avg:57.85ms
step:1805/2160 val_loss:3.3610 train_time:104508ms step_avg:57.90ms
step:1806/2160 train_time:104533ms step_avg:57.88ms
step:1807/2160 train_time:104600ms step_avg:57.89ms
step:1808/2160 train_time:104692ms step_avg:57.90ms
step:1809/2160 train_time:104782ms step_avg:57.92ms
step:1810/2160 train_time:104869ms step_avg:57.94ms
step:1810/2160 val_loss:3.3602 train_time:104958ms step_avg:57.99ms
step:1811/2160 train_time:104981ms step_avg:57.97ms
step:1812/2160 train_time:105051ms step_avg:57.98ms
step:1813/2160 train_time:105146ms step_avg:58.00ms
step:1814/2160 train_time:105233ms step_avg:58.01ms
step:1815/2160 train_time:105321ms step_avg:58.03ms
step:1815/2160 val_loss:3.3582 train_time:105408ms step_avg:58.08ms
step:1816/2160 train_time:105433ms step_avg:58.06ms
step:1817/2160 train_time:105500ms step_avg:58.06ms
step:1818/2160 train_time:105589ms step_avg:58.08ms
step:1819/2160 train_time:105678ms step_avg:58.10ms
step:1820/2160 train_time:105764ms step_avg:58.11ms
step:1820/2160 val_loss:3.3561 train_time:105853ms step_avg:58.16ms
step:1821/2160 train_time:105876ms step_avg:58.14ms
step:1822/2160 train_time:105944ms step_avg:58.15ms
step:1823/2160 train_time:106037ms step_avg:58.17ms
step:1824/2160 train_time:106125ms step_avg:58.18ms
step:1825/2160 train_time:106214ms step_avg:58.20ms
step:1825/2160 val_loss:3.3546 train_time:106300ms step_avg:58.25ms
step:1826/2160 train_time:106324ms step_avg:58.23ms
step:1827/2160 train_time:106392ms step_avg:58.23ms
step:1828/2160 train_time:106482ms step_avg:58.25ms
step:1829/2160 train_time:106572ms step_avg:58.27ms
step:1830/2160 train_time:106658ms step_avg:58.28ms
step:1830/2160 val_loss:3.3541 train_time:106747ms step_avg:58.33ms
step:1831/2160 train_time:106770ms step_avg:58.31ms
step:1832/2160 train_time:106841ms step_avg:58.32ms
step:1833/2160 train_time:106935ms step_avg:58.34ms
step:1834/2160 train_time:107022ms step_avg:58.35ms
step:1835/2160 train_time:107111ms step_avg:58.37ms
step:1835/2160 val_loss:3.3520 train_time:107197ms step_avg:58.42ms
step:1836/2160 train_time:107222ms step_avg:58.40ms
step:1837/2160 train_time:107293ms step_avg:58.41ms
step:1838/2160 train_time:107384ms step_avg:58.42ms
step:1839/2160 train_time:107473ms step_avg:58.44ms
step:1840/2160 train_time:107560ms step_avg:58.46ms
step:1840/2160 val_loss:3.3522 train_time:107649ms step_avg:58.50ms
step:1841/2160 train_time:107672ms step_avg:58.49ms
step:1842/2160 train_time:107742ms step_avg:58.49ms
step:1843/2160 train_time:107837ms step_avg:58.51ms
step:1844/2160 train_time:107925ms step_avg:58.53ms
step:1845/2160 train_time:108013ms step_avg:58.54ms
step:1845/2160 val_loss:3.3496 train_time:108099ms step_avg:58.59ms
step:1846/2160 train_time:108126ms step_avg:58.57ms
step:1847/2160 train_time:108191ms step_avg:58.58ms
step:1848/2160 train_time:108282ms step_avg:58.59ms
step:1849/2160 train_time:108371ms step_avg:58.61ms
step:1850/2160 train_time:108458ms step_avg:58.63ms
step:1850/2160 val_loss:3.3476 train_time:108545ms step_avg:58.67ms
step:1851/2160 train_time:108569ms step_avg:58.65ms
step:1852/2160 train_time:108636ms step_avg:58.66ms
step:1853/2160 train_time:108727ms step_avg:58.68ms
step:1854/2160 train_time:108815ms step_avg:58.69ms
step:1855/2160 train_time:108903ms step_avg:58.71ms
step:1855/2160 val_loss:3.3464 train_time:108989ms step_avg:58.75ms
step:1856/2160 train_time:109014ms step_avg:58.74ms
step:1857/2160 train_time:109082ms step_avg:58.74ms
step:1858/2160 train_time:109174ms step_avg:58.76ms
step:1859/2160 train_time:109264ms step_avg:58.78ms
step:1860/2160 train_time:109351ms step_avg:58.79ms
step:1860/2160 val_loss:3.3452 train_time:109439ms step_avg:58.84ms
step:1861/2160 train_time:109463ms step_avg:58.82ms
step:1862/2160 train_time:109530ms step_avg:58.82ms
step:1863/2160 train_time:109622ms step_avg:58.84ms
step:1864/2160 train_time:109709ms step_avg:58.86ms
step:1865/2160 train_time:109798ms step_avg:58.87ms
step:1865/2160 val_loss:3.3432 train_time:109883ms step_avg:58.92ms
step:1866/2160 train_time:109908ms step_avg:58.90ms
step:1867/2160 train_time:109978ms step_avg:58.91ms
step:1868/2160 train_time:110069ms step_avg:58.92ms
step:1869/2160 train_time:110158ms step_avg:58.94ms
step:1870/2160 train_time:110246ms step_avg:58.96ms
step:1870/2160 val_loss:3.3422 train_time:110334ms step_avg:59.00ms
step:1871/2160 train_time:110357ms step_avg:58.98ms
step:1872/2160 train_time:110425ms step_avg:58.99ms
step:1873/2160 train_time:110519ms step_avg:59.01ms
step:1874/2160 train_time:110607ms step_avg:59.02ms
step:1875/2160 train_time:110696ms step_avg:59.04ms
step:1875/2160 val_loss:3.3411 train_time:110782ms step_avg:59.08ms
step:1876/2160 train_time:110809ms step_avg:59.07ms
step:1877/2160 train_time:110874ms step_avg:59.07ms
step:1878/2160 train_time:110967ms step_avg:59.09ms
step:1879/2160 train_time:111057ms step_avg:59.10ms
step:1880/2160 train_time:111144ms step_avg:59.12ms
step:1880/2160 val_loss:3.3395 train_time:111232ms step_avg:59.17ms
step:1881/2160 train_time:111256ms step_avg:59.15ms
step:1882/2160 train_time:111325ms step_avg:59.15ms
step:1883/2160 train_time:111420ms step_avg:59.17ms
step:1884/2160 train_time:111508ms step_avg:59.19ms
step:1885/2160 train_time:111596ms step_avg:59.20ms
step:1885/2160 val_loss:3.3384 train_time:111681ms step_avg:59.25ms
step:1886/2160 train_time:111707ms step_avg:59.23ms
step:1887/2160 train_time:111773ms step_avg:59.23ms
step:1888/2160 train_time:111866ms step_avg:59.25ms
step:1889/2160 train_time:111957ms step_avg:59.27ms
step:1890/2160 train_time:112043ms step_avg:59.28ms
step:1890/2160 val_loss:3.3367 train_time:112131ms step_avg:59.33ms
step:1891/2160 train_time:112154ms step_avg:59.31ms
step:1892/2160 train_time:112223ms step_avg:59.31ms
step:1893/2160 train_time:112315ms step_avg:59.33ms
step:1894/2160 train_time:112403ms step_avg:59.35ms
step:1895/2160 train_time:112491ms step_avg:59.36ms
step:1895/2160 val_loss:3.3351 train_time:112577ms step_avg:59.41ms
step:1896/2160 train_time:112602ms step_avg:59.39ms
step:1897/2160 train_time:112669ms step_avg:59.39ms
step:1898/2160 train_time:112761ms step_avg:59.41ms
step:1899/2160 train_time:112851ms step_avg:59.43ms
step:1900/2160 train_time:112937ms step_avg:59.44ms
step:1900/2160 val_loss:3.3340 train_time:113025ms step_avg:59.49ms
step:1901/2160 train_time:113048ms step_avg:59.47ms
step:1902/2160 train_time:113115ms step_avg:59.47ms
step:1903/2160 train_time:113209ms step_avg:59.49ms
step:1904/2160 train_time:113297ms step_avg:59.50ms
step:1905/2160 train_time:113386ms step_avg:59.52ms
step:1905/2160 val_loss:3.3328 train_time:113472ms step_avg:59.57ms
step:1906/2160 train_time:113498ms step_avg:59.55ms
step:1907/2160 train_time:113566ms step_avg:59.55ms
step:1908/2160 train_time:113656ms step_avg:59.57ms
step:1909/2160 train_time:113746ms step_avg:59.58ms
step:1910/2160 train_time:113832ms step_avg:59.60ms
step:1910/2160 val_loss:3.3312 train_time:113920ms step_avg:59.64ms
step:1911/2160 train_time:113943ms step_avg:59.62ms
step:1912/2160 train_time:114013ms step_avg:59.63ms
step:1913/2160 train_time:114107ms step_avg:59.65ms
step:1914/2160 train_time:114195ms step_avg:59.66ms
step:1915/2160 train_time:114284ms step_avg:59.68ms
step:1915/2160 val_loss:3.3305 train_time:114370ms step_avg:59.72ms
step:1916/2160 train_time:114395ms step_avg:59.71ms
step:1917/2160 train_time:114463ms step_avg:59.71ms
step:1918/2160 train_time:114556ms step_avg:59.73ms
step:1919/2160 train_time:114646ms step_avg:59.74ms
step:1920/2160 train_time:114733ms step_avg:59.76ms
step:1920/2160 val_loss:3.3287 train_time:114822ms step_avg:59.80ms
step:1921/2160 train_time:114845ms step_avg:59.78ms
step:1922/2160 train_time:114911ms step_avg:59.79ms
step:1923/2160 train_time:115005ms step_avg:59.80ms
step:1924/2160 train_time:115092ms step_avg:59.82ms
step:1925/2160 train_time:115179ms step_avg:59.83ms
step:1925/2160 val_loss:3.3278 train_time:115265ms step_avg:59.88ms
step:1926/2160 train_time:115290ms step_avg:59.86ms
step:1927/2160 train_time:115357ms step_avg:59.86ms
step:1928/2160 train_time:115450ms step_avg:59.88ms
step:1929/2160 train_time:115541ms step_avg:59.90ms
step:1930/2160 train_time:115628ms step_avg:59.91ms
step:1930/2160 val_loss:3.3263 train_time:115716ms step_avg:59.96ms
step:1931/2160 train_time:115739ms step_avg:59.94ms
step:1932/2160 train_time:115805ms step_avg:59.94ms
step:1933/2160 train_time:115899ms step_avg:59.96ms
step:1934/2160 train_time:115987ms step_avg:59.97ms
step:1935/2160 train_time:116076ms step_avg:59.99ms
step:1935/2160 val_loss:3.3248 train_time:116162ms step_avg:60.03ms
step:1936/2160 train_time:116187ms step_avg:60.01ms
step:1937/2160 train_time:116255ms step_avg:60.02ms
step:1938/2160 train_time:116347ms step_avg:60.03ms
step:1939/2160 train_time:116438ms step_avg:60.05ms
step:1940/2160 train_time:116524ms step_avg:60.06ms
step:1940/2160 val_loss:3.3241 train_time:116612ms step_avg:60.11ms
step:1941/2160 train_time:116635ms step_avg:60.09ms
step:1942/2160 train_time:116704ms step_avg:60.09ms
step:1943/2160 train_time:116799ms step_avg:60.11ms
step:1944/2160 train_time:116887ms step_avg:60.13ms
step:1945/2160 train_time:116977ms step_avg:60.14ms
step:1945/2160 val_loss:3.3225 train_time:117062ms step_avg:60.19ms
step:1946/2160 train_time:117087ms step_avg:60.17ms
step:1947/2160 train_time:117155ms step_avg:60.17ms
step:1948/2160 train_time:117249ms step_avg:60.19ms
step:1949/2160 train_time:117341ms step_avg:60.21ms
step:1950/2160 train_time:117428ms step_avg:60.22ms
step:1950/2160 val_loss:3.3215 train_time:117516ms step_avg:60.26ms
step:1951/2160 train_time:117540ms step_avg:60.25ms
step:1952/2160 train_time:117608ms step_avg:60.25ms
step:1953/2160 train_time:117703ms step_avg:60.27ms
step:1954/2160 train_time:117792ms step_avg:60.28ms
step:1955/2160 train_time:117882ms step_avg:60.30ms
step:1955/2160 val_loss:3.3201 train_time:117969ms step_avg:60.34ms
step:1956/2160 train_time:117993ms step_avg:60.32ms
step:1957/2160 train_time:118063ms step_avg:60.33ms
step:1958/2160 train_time:118151ms step_avg:60.34ms
step:1959/2160 train_time:118240ms step_avg:60.36ms
step:1960/2160 train_time:118326ms step_avg:60.37ms
step:1960/2160 val_loss:3.3190 train_time:118414ms step_avg:60.42ms
step:1961/2160 train_time:118437ms step_avg:60.40ms
step:1962/2160 train_time:118504ms step_avg:60.40ms
step:1963/2160 train_time:118600ms step_avg:60.42ms
step:1964/2160 train_time:118688ms step_avg:60.43ms
step:1965/2160 train_time:118778ms step_avg:60.45ms
step:1965/2160 val_loss:3.3180 train_time:118864ms step_avg:60.49ms
step:1966/2160 train_time:118889ms step_avg:60.47ms
step:1967/2160 train_time:118956ms step_avg:60.48ms
step:1968/2160 train_time:119046ms step_avg:60.49ms
step:1969/2160 train_time:119135ms step_avg:60.51ms
step:1970/2160 train_time:119222ms step_avg:60.52ms
step:1970/2160 val_loss:3.3161 train_time:119310ms step_avg:60.56ms
step:1971/2160 train_time:119333ms step_avg:60.54ms
step:1972/2160 train_time:119401ms step_avg:60.55ms
step:1973/2160 train_time:119498ms step_avg:60.57ms
step:1974/2160 train_time:119587ms step_avg:60.58ms
step:1975/2160 train_time:119676ms step_avg:60.60ms
step:1975/2160 val_loss:3.3148 train_time:119762ms step_avg:60.64ms
step:1976/2160 train_time:119787ms step_avg:60.62ms
step:1977/2160 train_time:119855ms step_avg:60.62ms
step:1978/2160 train_time:119947ms step_avg:60.64ms
step:1979/2160 train_time:120037ms step_avg:60.66ms
step:1980/2160 train_time:120123ms step_avg:60.67ms
step:1980/2160 val_loss:3.3134 train_time:120211ms step_avg:60.71ms
step:1981/2160 train_time:120234ms step_avg:60.69ms
step:1982/2160 train_time:120301ms step_avg:60.70ms
step:1983/2160 train_time:120398ms step_avg:60.71ms
step:1984/2160 train_time:120486ms step_avg:60.73ms
step:1985/2160 train_time:120575ms step_avg:60.74ms
step:1985/2160 val_loss:3.3127 train_time:120661ms step_avg:60.79ms
step:1986/2160 train_time:120686ms step_avg:60.77ms
step:1987/2160 train_time:120755ms step_avg:60.77ms
step:1988/2160 train_time:120847ms step_avg:60.79ms
step:1989/2160 train_time:120936ms step_avg:60.80ms
step:1990/2160 train_time:121022ms step_avg:60.82ms
step:1990/2160 val_loss:3.3113 train_time:121111ms step_avg:60.86ms
step:1991/2160 train_time:121134ms step_avg:60.84ms
step:1992/2160 train_time:121201ms step_avg:60.84ms
step:1993/2160 train_time:121295ms step_avg:60.86ms
step:1994/2160 train_time:121384ms step_avg:60.87ms
step:1995/2160 train_time:121472ms step_avg:60.89ms
step:1995/2160 val_loss:3.3102 train_time:121558ms step_avg:60.93ms
step:1996/2160 train_time:121583ms step_avg:60.91ms
step:1997/2160 train_time:121653ms step_avg:60.92ms
step:1998/2160 train_time:121744ms step_avg:60.93ms
step:1999/2160 train_time:121835ms step_avg:60.95ms
step:2000/2160 train_time:121922ms step_avg:60.96ms
step:2000/2160 val_loss:3.3090 train_time:122010ms step_avg:61.01ms
step:2001/2160 train_time:122034ms step_avg:60.99ms
step:2002/2160 train_time:122102ms step_avg:60.99ms
step:2003/2160 train_time:122199ms step_avg:61.01ms
step:2004/2160 train_time:122287ms step_avg:61.02ms
step:2005/2160 train_time:122376ms step_avg:61.04ms
step:2005/2160 val_loss:3.3081 train_time:122462ms step_avg:61.08ms
step:2006/2160 train_time:122486ms step_avg:61.06ms
step:2007/2160 train_time:122556ms step_avg:61.06ms
step:2008/2160 train_time:122648ms step_avg:61.08ms
step:2009/2160 train_time:122738ms step_avg:61.09ms
step:2010/2160 train_time:122824ms step_avg:61.11ms
step:2010/2160 val_loss:3.3069 train_time:122912ms step_avg:61.15ms
step:2011/2160 train_time:122935ms step_avg:61.13ms
step:2012/2160 train_time:123003ms step_avg:61.13ms
step:2013/2160 train_time:123098ms step_avg:61.15ms
step:2014/2160 train_time:123186ms step_avg:61.16ms
step:2015/2160 train_time:123275ms step_avg:61.18ms
step:2015/2160 val_loss:3.3056 train_time:123361ms step_avg:61.22ms
step:2016/2160 train_time:123386ms step_avg:61.20ms
step:2017/2160 train_time:123455ms step_avg:61.21ms
step:2018/2160 train_time:123547ms step_avg:61.22ms
step:2019/2160 train_time:123636ms step_avg:61.24ms
step:2020/2160 train_time:123724ms step_avg:61.25ms
step:2020/2160 val_loss:3.3052 train_time:123813ms step_avg:61.29ms
step:2021/2160 train_time:123836ms step_avg:61.27ms
step:2022/2160 train_time:123903ms step_avg:61.28ms
step:2023/2160 train_time:123999ms step_avg:61.29ms
step:2024/2160 train_time:124087ms step_avg:61.31ms
step:2025/2160 train_time:124178ms step_avg:61.32ms
step:2025/2160 val_loss:3.3040 train_time:124264ms step_avg:61.36ms
step:2026/2160 train_time:124288ms step_avg:61.35ms
step:2027/2160 train_time:124357ms step_avg:61.35ms
step:2028/2160 train_time:124448ms step_avg:61.36ms
step:2029/2160 train_time:124537ms step_avg:61.38ms
step:2030/2160 train_time:124623ms step_avg:61.39ms
step:2030/2160 val_loss:3.3025 train_time:124711ms step_avg:61.43ms
step:2031/2160 train_time:124737ms step_avg:61.42ms
step:2032/2160 train_time:124801ms step_avg:61.42ms
step:2033/2160 train_time:124896ms step_avg:61.43ms
step:2034/2160 train_time:124985ms step_avg:61.45ms
step:2035/2160 train_time:125073ms step_avg:61.46ms
step:2035/2160 val_loss:3.3015 train_time:125160ms step_avg:61.50ms
step:2036/2160 train_time:125184ms step_avg:61.49ms
step:2037/2160 train_time:125254ms step_avg:61.49ms
step:2038/2160 train_time:125346ms step_avg:61.50ms
step:2039/2160 train_time:125438ms step_avg:61.52ms
step:2040/2160 train_time:125525ms step_avg:61.53ms
step:2040/2160 val_loss:3.3004 train_time:125615ms step_avg:61.58ms
step:2041/2160 train_time:125638ms step_avg:61.56ms
step:2042/2160 train_time:125707ms step_avg:61.56ms
step:2043/2160 train_time:125802ms step_avg:61.58ms
step:2044/2160 train_time:125890ms step_avg:61.59ms
step:2045/2160 train_time:125979ms step_avg:61.60ms
step:2045/2160 val_loss:3.2995 train_time:126065ms step_avg:61.65ms
step:2046/2160 train_time:126091ms step_avg:61.63ms
step:2047/2160 train_time:126160ms step_avg:61.63ms
step:2048/2160 train_time:126252ms step_avg:61.65ms
step:2049/2160 train_time:126342ms step_avg:61.66ms
step:2050/2160 train_time:126429ms step_avg:61.67ms
step:2050/2160 val_loss:3.2980 train_time:126517ms step_avg:61.72ms
step:2051/2160 train_time:126540ms step_avg:61.70ms
step:2052/2160 train_time:126608ms step_avg:61.70ms
step:2053/2160 train_time:126704ms step_avg:61.72ms
step:2054/2160 train_time:126793ms step_avg:61.73ms
step:2055/2160 train_time:126882ms step_avg:61.74ms
step:2055/2160 val_loss:3.2976 train_time:126967ms step_avg:61.78ms
step:2056/2160 train_time:126992ms step_avg:61.77ms
step:2057/2160 train_time:127059ms step_avg:61.77ms
step:2058/2160 train_time:127150ms step_avg:61.78ms
step:2059/2160 train_time:127241ms step_avg:61.80ms
step:2060/2160 train_time:127327ms step_avg:61.81ms
step:2060/2160 val_loss:3.2965 train_time:127416ms step_avg:61.85ms
step:2061/2160 train_time:127441ms step_avg:61.83ms
step:2062/2160 train_time:127509ms step_avg:61.84ms
step:2063/2160 train_time:127603ms step_avg:61.85ms
step:2064/2160 train_time:127692ms step_avg:61.87ms
step:2065/2160 train_time:127781ms step_avg:61.88ms
step:2065/2160 val_loss:3.2957 train_time:127868ms step_avg:61.92ms
step:2066/2160 train_time:127893ms step_avg:61.90ms
step:2067/2160 train_time:127961ms step_avg:61.91ms
step:2068/2160 train_time:128059ms step_avg:61.92ms
step:2069/2160 train_time:128149ms step_avg:61.94ms
step:2070/2160 train_time:128236ms step_avg:61.95ms
step:2070/2160 val_loss:3.2947 train_time:128324ms step_avg:61.99ms
step:2071/2160 train_time:128347ms step_avg:61.97ms
step:2072/2160 train_time:128416ms step_avg:61.98ms
step:2073/2160 train_time:128510ms step_avg:61.99ms
step:2074/2160 train_time:128599ms step_avg:62.01ms
step:2075/2160 train_time:128688ms step_avg:62.02ms
step:2075/2160 val_loss:3.2937 train_time:128774ms step_avg:62.06ms
step:2076/2160 train_time:128798ms step_avg:62.04ms
step:2077/2160 train_time:128865ms step_avg:62.04ms
step:2078/2160 train_time:128956ms step_avg:62.06ms
step:2079/2160 train_time:129046ms step_avg:62.07ms
step:2080/2160 train_time:129132ms step_avg:62.08ms
step:2080/2160 val_loss:3.2927 train_time:129221ms step_avg:62.13ms
step:2081/2160 train_time:129244ms step_avg:62.11ms
step:2082/2160 train_time:129313ms step_avg:62.11ms
step:2083/2160 train_time:129410ms step_avg:62.13ms
step:2084/2160 train_time:129497ms step_avg:62.14ms
step:2085/2160 train_time:129587ms step_avg:62.15ms
step:2085/2160 val_loss:3.2921 train_time:129674ms step_avg:62.19ms
step:2086/2160 train_time:129699ms step_avg:62.18ms
step:2087/2160 train_time:129767ms step_avg:62.18ms
step:2088/2160 train_time:129858ms step_avg:62.19ms
step:2089/2160 train_time:129947ms step_avg:62.21ms
step:2090/2160 train_time:130034ms step_avg:62.22ms
step:2090/2160 val_loss:3.2913 train_time:130122ms step_avg:62.26ms
step:2091/2160 train_time:130147ms step_avg:62.24ms
step:2092/2160 train_time:130213ms step_avg:62.24ms
step:2093/2160 train_time:130310ms step_avg:62.26ms
step:2094/2160 train_time:130399ms step_avg:62.27ms
step:2095/2160 train_time:130489ms step_avg:62.29ms
step:2095/2160 val_loss:3.2903 train_time:130575ms step_avg:62.33ms
step:2096/2160 train_time:130599ms step_avg:62.31ms
step:2097/2160 train_time:130668ms step_avg:62.31ms
step:2098/2160 train_time:130760ms step_avg:62.33ms
step:2099/2160 train_time:130850ms step_avg:62.34ms
step:2100/2160 train_time:130936ms step_avg:62.35ms
step:2100/2160 val_loss:3.2894 train_time:131025ms step_avg:62.39ms
step:2101/2160 train_time:131048ms step_avg:62.37ms
step:2102/2160 train_time:131115ms step_avg:62.38ms
step:2103/2160 train_time:131212ms step_avg:62.39ms
step:2104/2160 train_time:131302ms step_avg:62.41ms
step:2105/2160 train_time:131390ms step_avg:62.42ms
step:2105/2160 val_loss:3.2887 train_time:131476ms step_avg:62.46ms
step:2106/2160 train_time:131501ms step_avg:62.44ms
step:2107/2160 train_time:131572ms step_avg:62.45ms
step:2108/2160 train_time:131662ms step_avg:62.46ms
step:2109/2160 train_time:131753ms step_avg:62.47ms
step:2110/2160 train_time:131840ms step_avg:62.48ms
step:2110/2160 val_loss:3.2879 train_time:131929ms step_avg:62.53ms
step:2111/2160 train_time:131952ms step_avg:62.51ms
step:2112/2160 train_time:132020ms step_avg:62.51ms
step:2113/2160 train_time:132116ms step_avg:62.53ms
step:2114/2160 train_time:132206ms step_avg:62.54ms
step:2115/2160 train_time:132294ms step_avg:62.55ms
step:2115/2160 val_loss:3.2871 train_time:132381ms step_avg:62.59ms
step:2116/2160 train_time:132405ms step_avg:62.57ms
step:2117/2160 train_time:132475ms step_avg:62.58ms
step:2118/2160 train_time:132569ms step_avg:62.59ms
step:2119/2160 train_time:132658ms step_avg:62.60ms
step:2120/2160 train_time:132746ms step_avg:62.62ms
step:2120/2160 val_loss:3.2848 train_time:132835ms step_avg:62.66ms
step:2121/2160 train_time:132860ms step_avg:62.64ms
step:2122/2160 train_time:132924ms step_avg:62.64ms
step:2123/2160 train_time:133020ms step_avg:62.66ms
step:2124/2160 train_time:133113ms step_avg:62.67ms
step:2125/2160 train_time:133201ms step_avg:62.68ms
step:2125/2160 val_loss:3.2836 train_time:133286ms step_avg:62.72ms
step:2126/2160 train_time:133311ms step_avg:62.71ms
step:2127/2160 train_time:133380ms step_avg:62.71ms
step:2128/2160 train_time:133470ms step_avg:62.72ms
step:2129/2160 train_time:133560ms step_avg:62.73ms
step:2130/2160 train_time:133648ms step_avg:62.75ms
step:2130/2160 val_loss:3.2828 train_time:133735ms step_avg:62.79ms
step:2131/2160 train_time:133759ms step_avg:62.77ms
step:2132/2160 train_time:133827ms step_avg:62.77ms
step:2133/2160 train_time:133917ms step_avg:62.78ms
step:2134/2160 train_time:134006ms step_avg:62.80ms
step:2135/2160 train_time:134094ms step_avg:62.81ms
step:2135/2160 val_loss:3.2821 train_time:134180ms step_avg:62.85ms
step:2136/2160 train_time:134205ms step_avg:62.83ms
step:2137/2160 train_time:134274ms step_avg:62.83ms
step:2138/2160 train_time:134365ms step_avg:62.85ms
step:2139/2160 train_time:134454ms step_avg:62.86ms
step:2140/2160 train_time:134541ms step_avg:62.87ms
step:2140/2160 val_loss:3.2815 train_time:134630ms step_avg:62.91ms
step:2141/2160 train_time:134653ms step_avg:62.89ms
step:2142/2160 train_time:134721ms step_avg:62.89ms
step:2143/2160 train_time:134815ms step_avg:62.91ms
step:2144/2160 train_time:134903ms step_avg:62.92ms
step:2145/2160 train_time:134992ms step_avg:62.93ms
step:2145/2160 val_loss:3.2809 train_time:135079ms step_avg:62.97ms
step:2146/2160 train_time:135103ms step_avg:62.96ms
step:2147/2160 train_time:135173ms step_avg:62.96ms
step:2148/2160 train_time:135264ms step_avg:62.97ms
step:2149/2160 train_time:135354ms step_avg:62.98ms
step:2150/2160 train_time:135442ms step_avg:63.00ms
step:2150/2160 val_loss:3.2805 train_time:135531ms step_avg:63.04ms
step:2151/2160 train_time:135554ms step_avg:63.02ms
step:2152/2160 train_time:135621ms step_avg:63.02ms
step:2153/2160 train_time:135716ms step_avg:63.04ms
step:2154/2160 train_time:135804ms step_avg:63.05ms
step:2155/2160 train_time:135892ms step_avg:63.06ms
step:2155/2160 val_loss:3.2799 train_time:135978ms step_avg:63.10ms
step:2156/2160 train_time:136003ms step_avg:63.08ms
step:2157/2160 train_time:136073ms step_avg:63.08ms
step:2158/2160 train_time:136162ms step_avg:63.10ms
step:2159/2160 train_time:136251ms step_avg:63.11ms
step:2160/2160 train_time:136337ms step_avg:63.12ms
step:2160/2160 val_loss:3.2770 train_time:136426ms step_avg:63.16ms
peak memory allocated: 30261 MiB reserved: 45216 MiB

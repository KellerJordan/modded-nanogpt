import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = 1.51  # (16/8)**0.6
    if x > 0.66:
        lr_max = 1.93  # (24/8)**0.6
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20251204+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Dec  5 20:38:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   30C    P0            115W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   36C    P0            126W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   31C    P0            118W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   31C    P0            119W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   38C    P0            125W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    1519MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A          168701      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    0   N/A  N/A          168702      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          168703      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          168704      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          168705      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          168706      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          168707      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    0   N/A  N/A          168708      C   /home/ubuntu/.venv/bin/python3          614MiB |
|    1   N/A  N/A          168702      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    2   N/A  N/A          168703      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    3   N/A  N/A          168704      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    4   N/A  N/A          168705      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    5   N/A  N/A          168706      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    6   N/A  N/A          168707      C   /home/ubuntu/.venv/bin/python3         1510MiB |
|    7   N/A  N/A          168708      C   /home/ubuntu/.venv/bin/python3         1510MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.03ms
step:1/2160 train_time:100ms step_avg:100.42ms
step:2/2160 train_time:144ms step_avg:72.10ms
step:3/2160 train_time:166ms step_avg:55.32ms
step:4/2160 train_time:189ms step_avg:47.20ms
step:5/2160 train_time:213ms step_avg:42.67ms
step:6/2160 train_time:379ms step_avg:63.14ms
step:7/2160 train_time:426ms step_avg:60.89ms
step:8/2160 train_time:460ms step_avg:57.44ms
step:9/2160 train_time:493ms step_avg:54.81ms
step:10/2160 train_time:526ms step_avg:52.63ms
step:11/2160 train_time:560ms step_avg:50.91ms
step:12/2160 train_time:593ms step_avg:49.43ms
step:13/2160 train_time:627ms step_avg:48.24ms
step:14/2160 train_time:660ms step_avg:47.17ms
step:15/2160 train_time:694ms step_avg:46.28ms
step:16/2160 train_time:728ms step_avg:45.48ms
step:17/2160 train_time:762ms step_avg:44.80ms
step:18/2160 train_time:795ms step_avg:44.17ms
step:19/2160 train_time:829ms step_avg:43.62ms
step:20/2160 train_time:862ms step_avg:43.10ms
step:21/2160 train_time:896ms step_avg:42.67ms
step:22/2160 train_time:929ms step_avg:42.24ms
step:23/2160 train_time:963ms step_avg:41.88ms
step:24/2160 train_time:997ms step_avg:41.53ms
step:25/2160 train_time:1030ms step_avg:41.22ms
step:26/2160 train_time:1064ms step_avg:40.93ms
step:27/2160 train_time:1098ms step_avg:40.65ms
step:28/2160 train_time:1131ms step_avg:40.39ms
step:29/2160 train_time:1165ms step_avg:40.18ms
step:30/2160 train_time:1198ms step_avg:39.95ms
step:31/2160 train_time:1232ms step_avg:39.74ms
step:32/2160 train_time:1265ms step_avg:39.55ms
step:33/2160 train_time:1299ms step_avg:39.38ms
step:34/2160 train_time:1333ms step_avg:39.21ms
step:35/2160 train_time:1368ms step_avg:39.10ms
step:36/2160 train_time:1402ms step_avg:38.94ms
step:37/2160 train_time:1436ms step_avg:38.81ms
step:38/2160 train_time:1469ms step_avg:38.67ms
step:39/2160 train_time:1504ms step_avg:38.57ms
step:40/2160 train_time:1538ms step_avg:38.44ms
step:41/2160 train_time:1572ms step_avg:38.34ms
step:42/2160 train_time:1605ms step_avg:38.22ms
step:43/2160 train_time:1639ms step_avg:38.12ms
step:44/2160 train_time:1673ms step_avg:38.02ms
step:45/2160 train_time:1707ms step_avg:37.93ms
step:46/2160 train_time:1740ms step_avg:37.83ms
step:47/2160 train_time:1774ms step_avg:37.74ms
step:48/2160 train_time:1807ms step_avg:37.65ms
step:49/2160 train_time:1841ms step_avg:37.57ms
step:50/2160 train_time:1875ms step_avg:37.49ms
step:51/2160 train_time:1909ms step_avg:37.42ms
step:52/2160 train_time:1942ms step_avg:37.34ms
step:53/2160 train_time:1976ms step_avg:37.28ms
step:54/2160 train_time:2009ms step_avg:37.21ms
step:55/2160 train_time:2043ms step_avg:37.15ms
step:56/2160 train_time:2076ms step_avg:37.08ms
step:57/2160 train_time:2110ms step_avg:37.02ms
step:58/2160 train_time:2144ms step_avg:36.96ms
step:59/2160 train_time:2178ms step_avg:36.91ms
step:60/2160 train_time:2211ms step_avg:36.85ms
step:61/2160 train_time:2245ms step_avg:36.80ms
step:62/2160 train_time:2278ms step_avg:36.75ms
step:63/2160 train_time:2313ms step_avg:36.71ms
step:64/2160 train_time:2346ms step_avg:36.65ms
step:65/2160 train_time:2380ms step_avg:36.61ms
step:66/2160 train_time:2413ms step_avg:36.56ms
step:67/2160 train_time:2448ms step_avg:36.53ms
step:68/2160 train_time:2481ms step_avg:36.48ms
step:69/2160 train_time:2515ms step_avg:36.45ms
step:70/2160 train_time:2548ms step_avg:36.40ms
step:71/2160 train_time:2582ms step_avg:36.37ms
step:72/2160 train_time:2616ms step_avg:36.33ms
step:73/2160 train_time:2650ms step_avg:36.30ms
step:74/2160 train_time:2683ms step_avg:36.26ms
step:75/2160 train_time:2717ms step_avg:36.23ms
step:76/2160 train_time:2750ms step_avg:36.19ms
step:77/2160 train_time:2785ms step_avg:36.17ms
step:78/2160 train_time:2818ms step_avg:36.13ms
step:79/2160 train_time:2853ms step_avg:36.11ms
step:80/2160 train_time:2886ms step_avg:36.07ms
step:81/2160 train_time:2920ms step_avg:36.05ms
step:82/2160 train_time:2953ms step_avg:36.01ms
step:83/2160 train_time:2987ms step_avg:35.99ms
step:84/2160 train_time:3021ms step_avg:35.96ms
step:85/2160 train_time:3055ms step_avg:35.94ms
step:86/2160 train_time:3088ms step_avg:35.91ms
step:87/2160 train_time:3122ms step_avg:35.88ms
step:88/2160 train_time:3155ms step_avg:35.85ms
step:89/2160 train_time:3189ms step_avg:35.83ms
step:90/2160 train_time:3222ms step_avg:35.80ms
step:91/2160 train_time:3256ms step_avg:35.78ms
step:92/2160 train_time:3289ms step_avg:35.75ms
step:93/2160 train_time:3324ms step_avg:35.74ms
step:94/2160 train_time:3357ms step_avg:35.71ms
step:95/2160 train_time:3391ms step_avg:35.69ms
step:96/2160 train_time:3424ms step_avg:35.67ms
step:97/2160 train_time:3458ms step_avg:35.65ms
step:98/2160 train_time:3492ms step_avg:35.63ms
step:99/2160 train_time:3526ms step_avg:35.61ms
step:100/2160 train_time:3559ms step_avg:35.59ms
step:101/2160 train_time:3593ms step_avg:35.57ms
step:102/2160 train_time:3626ms step_avg:35.55ms
step:103/2160 train_time:3660ms step_avg:35.53ms
step:104/2160 train_time:3693ms step_avg:35.51ms
step:105/2160 train_time:3727ms step_avg:35.50ms
step:106/2160 train_time:3761ms step_avg:35.48ms
step:107/2160 train_time:3795ms step_avg:35.46ms
step:108/2160 train_time:3828ms step_avg:35.44ms
step:109/2160 train_time:3862ms step_avg:35.43ms
step:110/2160 train_time:3895ms step_avg:35.41ms
step:111/2160 train_time:3929ms step_avg:35.40ms
step:112/2160 train_time:3962ms step_avg:35.38ms
step:113/2160 train_time:3996ms step_avg:35.37ms
step:114/2160 train_time:4030ms step_avg:35.35ms
step:115/2160 train_time:4064ms step_avg:35.34ms
step:116/2160 train_time:4097ms step_avg:35.32ms
step:117/2160 train_time:4131ms step_avg:35.31ms
step:118/2160 train_time:4164ms step_avg:35.29ms
step:119/2160 train_time:4198ms step_avg:35.28ms
step:120/2160 train_time:4232ms step_avg:35.26ms
step:121/2160 train_time:4266ms step_avg:35.26ms
step:122/2160 train_time:4299ms step_avg:35.24ms
step:123/2160 train_time:4333ms step_avg:35.23ms
step:124/2160 train_time:4366ms step_avg:35.21ms
step:125/2160 train_time:4401ms step_avg:35.21ms
step:126/2160 train_time:4434ms step_avg:35.19ms
step:127/2160 train_time:4468ms step_avg:35.18ms
step:128/2160 train_time:4501ms step_avg:35.16ms
step:129/2160 train_time:4535ms step_avg:35.15ms
step:130/2160 train_time:4568ms step_avg:35.14ms
step:131/2160 train_time:4603ms step_avg:35.14ms
step:132/2160 train_time:4636ms step_avg:35.12ms
step:133/2160 train_time:4670ms step_avg:35.11ms
step:134/2160 train_time:4703ms step_avg:35.10ms
step:135/2160 train_time:4737ms step_avg:35.09ms
step:136/2160 train_time:4770ms step_avg:35.07ms
step:137/2160 train_time:4804ms step_avg:35.07ms
step:138/2160 train_time:4837ms step_avg:35.05ms
step:139/2160 train_time:4871ms step_avg:35.05ms
step:140/2160 train_time:4905ms step_avg:35.03ms
step:141/2160 train_time:4938ms step_avg:35.02ms
step:142/2160 train_time:4972ms step_avg:35.01ms
step:143/2160 train_time:5006ms step_avg:35.01ms
step:144/2160 train_time:5039ms step_avg:35.00ms
step:145/2160 train_time:5073ms step_avg:34.99ms
step:146/2160 train_time:5106ms step_avg:34.98ms
step:147/2160 train_time:5140ms step_avg:34.97ms
step:148/2160 train_time:5173ms step_avg:34.96ms
step:149/2160 train_time:5208ms step_avg:34.95ms
step:150/2160 train_time:5241ms step_avg:34.94ms
step:151/2160 train_time:5275ms step_avg:34.93ms
step:152/2160 train_time:5308ms step_avg:34.92ms
step:153/2160 train_time:5342ms step_avg:34.91ms
step:154/2160 train_time:5375ms step_avg:34.90ms
step:155/2160 train_time:5409ms step_avg:34.90ms
step:156/2160 train_time:5443ms step_avg:34.89ms
step:157/2160 train_time:5476ms step_avg:34.88ms
step:158/2160 train_time:5510ms step_avg:34.87ms
step:159/2160 train_time:5544ms step_avg:34.87ms
step:160/2160 train_time:5577ms step_avg:34.86ms
step:161/2160 train_time:5611ms step_avg:34.85ms
step:162/2160 train_time:5644ms step_avg:34.84ms
step:163/2160 train_time:5678ms step_avg:34.83ms
step:164/2160 train_time:5711ms step_avg:34.82ms
step:165/2160 train_time:5745ms step_avg:34.82ms
step:166/2160 train_time:5778ms step_avg:34.81ms
step:167/2160 train_time:5812ms step_avg:34.80ms
step:168/2160 train_time:5846ms step_avg:34.80ms
step:169/2160 train_time:5880ms step_avg:34.79ms
step:170/2160 train_time:5913ms step_avg:34.78ms
step:171/2160 train_time:5947ms step_avg:34.78ms
step:172/2160 train_time:5980ms step_avg:34.77ms
step:173/2160 train_time:6014ms step_avg:34.76ms
step:174/2160 train_time:6047ms step_avg:34.75ms
step:175/2160 train_time:6081ms step_avg:34.75ms
step:176/2160 train_time:6114ms step_avg:34.74ms
step:177/2160 train_time:6148ms step_avg:34.73ms
step:178/2160 train_time:6181ms step_avg:34.72ms
step:179/2160 train_time:6215ms step_avg:34.72ms
step:180/2160 train_time:6248ms step_avg:34.71ms
step:181/2160 train_time:6282ms step_avg:34.71ms
step:182/2160 train_time:6315ms step_avg:34.70ms
step:183/2160 train_time:6349ms step_avg:34.69ms
step:184/2160 train_time:6382ms step_avg:34.69ms
step:185/2160 train_time:6415ms step_avg:34.68ms
step:186/2160 train_time:6449ms step_avg:34.67ms
step:187/2160 train_time:6483ms step_avg:34.67ms
step:188/2160 train_time:6516ms step_avg:34.66ms
step:189/2160 train_time:6550ms step_avg:34.66ms
step:190/2160 train_time:6583ms step_avg:34.65ms
step:191/2160 train_time:6617ms step_avg:34.64ms
step:192/2160 train_time:6650ms step_avg:34.64ms
step:193/2160 train_time:6684ms step_avg:34.63ms
step:194/2160 train_time:6717ms step_avg:34.63ms
step:195/2160 train_time:6751ms step_avg:34.62ms
step:196/2160 train_time:6785ms step_avg:34.62ms
step:197/2160 train_time:6818ms step_avg:34.61ms
step:198/2160 train_time:6852ms step_avg:34.60ms
step:199/2160 train_time:6886ms step_avg:34.60ms
step:200/2160 train_time:6919ms step_avg:34.59ms
step:201/2160 train_time:6953ms step_avg:34.59ms
step:202/2160 train_time:6987ms step_avg:34.59ms
step:203/2160 train_time:7020ms step_avg:34.58ms
step:204/2160 train_time:7053ms step_avg:34.57ms
step:205/2160 train_time:7087ms step_avg:34.57ms
step:206/2160 train_time:7120ms step_avg:34.56ms
step:207/2160 train_time:7154ms step_avg:34.56ms
step:208/2160 train_time:7187ms step_avg:34.56ms
step:209/2160 train_time:7221ms step_avg:34.55ms
step:210/2160 train_time:7254ms step_avg:34.54ms
step:211/2160 train_time:7288ms step_avg:34.54ms
step:212/2160 train_time:7321ms step_avg:34.53ms
step:213/2160 train_time:7355ms step_avg:34.53ms
step:214/2160 train_time:7388ms step_avg:34.53ms
step:215/2160 train_time:7422ms step_avg:34.52ms
step:216/2160 train_time:7455ms step_avg:34.52ms
step:217/2160 train_time:7490ms step_avg:34.51ms
step:218/2160 train_time:7523ms step_avg:34.51ms
step:219/2160 train_time:7556ms step_avg:34.50ms
step:220/2160 train_time:7590ms step_avg:34.50ms
step:221/2160 train_time:7624ms step_avg:34.50ms
step:222/2160 train_time:7657ms step_avg:34.49ms
step:223/2160 train_time:7691ms step_avg:34.49ms
step:224/2160 train_time:7724ms step_avg:34.48ms
step:225/2160 train_time:7758ms step_avg:34.48ms
step:226/2160 train_time:7791ms step_avg:34.47ms
step:227/2160 train_time:7825ms step_avg:34.47ms
step:228/2160 train_time:7858ms step_avg:34.47ms
step:229/2160 train_time:7892ms step_avg:34.46ms
step:230/2160 train_time:7925ms step_avg:34.46ms
step:231/2160 train_time:7960ms step_avg:34.46ms
step:232/2160 train_time:7993ms step_avg:34.45ms
step:233/2160 train_time:8027ms step_avg:34.45ms
step:234/2160 train_time:8060ms step_avg:34.45ms
step:235/2160 train_time:8094ms step_avg:34.44ms
step:236/2160 train_time:8127ms step_avg:34.44ms
step:237/2160 train_time:8161ms step_avg:34.43ms
step:238/2160 train_time:8194ms step_avg:34.43ms
step:239/2160 train_time:8228ms step_avg:34.43ms
step:240/2160 train_time:8261ms step_avg:34.42ms
step:241/2160 train_time:8295ms step_avg:34.42ms
step:242/2160 train_time:8328ms step_avg:34.41ms
step:243/2160 train_time:8362ms step_avg:34.41ms
step:244/2160 train_time:8395ms step_avg:34.40ms
step:245/2160 train_time:8429ms step_avg:34.40ms
step:246/2160 train_time:8462ms step_avg:34.40ms
step:247/2160 train_time:8496ms step_avg:34.40ms
step:248/2160 train_time:8529ms step_avg:34.39ms
step:249/2160 train_time:8563ms step_avg:34.39ms
step:250/2160 train_time:8596ms step_avg:34.38ms
step:250/2160 val_loss:4.3019 train_time:8631ms step_avg:34.52ms
step:251/2160 train_time:8653ms step_avg:34.48ms
step:252/2160 train_time:8675ms step_avg:34.43ms
step:253/2160 train_time:8700ms step_avg:34.39ms
step:254/2160 train_time:8735ms step_avg:34.39ms
step:255/2160 train_time:8771ms step_avg:34.40ms
step:256/2160 train_time:8805ms step_avg:34.39ms
step:257/2160 train_time:8840ms step_avg:34.40ms
step:258/2160 train_time:8873ms step_avg:34.39ms
step:259/2160 train_time:8907ms step_avg:34.39ms
step:260/2160 train_time:8941ms step_avg:34.39ms
step:261/2160 train_time:8975ms step_avg:34.39ms
step:262/2160 train_time:9008ms step_avg:34.38ms
step:263/2160 train_time:9041ms step_avg:34.38ms
step:264/2160 train_time:9075ms step_avg:34.37ms
step:265/2160 train_time:9109ms step_avg:34.37ms
step:266/2160 train_time:9142ms step_avg:34.37ms
step:267/2160 train_time:9175ms step_avg:34.37ms
step:268/2160 train_time:9209ms step_avg:34.36ms
step:269/2160 train_time:9242ms step_avg:34.36ms
step:270/2160 train_time:9275ms step_avg:34.35ms
step:271/2160 train_time:9309ms step_avg:34.35ms
step:272/2160 train_time:9342ms step_avg:34.35ms
step:273/2160 train_time:9376ms step_avg:34.35ms
step:274/2160 train_time:9409ms step_avg:34.34ms
step:275/2160 train_time:9443ms step_avg:34.34ms
step:276/2160 train_time:9476ms step_avg:34.33ms
step:277/2160 train_time:9510ms step_avg:34.33ms
step:278/2160 train_time:9543ms step_avg:34.33ms
step:279/2160 train_time:9577ms step_avg:34.33ms
step:280/2160 train_time:9610ms step_avg:34.32ms
step:281/2160 train_time:9644ms step_avg:34.32ms
step:282/2160 train_time:9677ms step_avg:34.32ms
step:283/2160 train_time:9712ms step_avg:34.32ms
step:284/2160 train_time:9745ms step_avg:34.31ms
step:285/2160 train_time:9780ms step_avg:34.31ms
step:286/2160 train_time:9813ms step_avg:34.31ms
step:287/2160 train_time:9847ms step_avg:34.31ms
step:288/2160 train_time:9880ms step_avg:34.31ms
step:289/2160 train_time:9915ms step_avg:34.31ms
step:290/2160 train_time:9948ms step_avg:34.30ms
step:291/2160 train_time:9982ms step_avg:34.30ms
step:292/2160 train_time:10015ms step_avg:34.30ms
step:293/2160 train_time:10049ms step_avg:34.30ms
step:294/2160 train_time:10082ms step_avg:34.29ms
step:295/2160 train_time:10117ms step_avg:34.29ms
step:296/2160 train_time:10150ms step_avg:34.29ms
step:297/2160 train_time:10184ms step_avg:34.29ms
step:298/2160 train_time:10217ms step_avg:34.28ms
step:299/2160 train_time:10251ms step_avg:34.28ms
step:300/2160 train_time:10284ms step_avg:34.28ms
step:301/2160 train_time:10317ms step_avg:34.28ms
step:302/2160 train_time:10351ms step_avg:34.27ms
step:303/2160 train_time:10384ms step_avg:34.27ms
step:304/2160 train_time:10417ms step_avg:34.27ms
step:305/2160 train_time:10451ms step_avg:34.27ms
step:306/2160 train_time:10484ms step_avg:34.26ms
step:307/2160 train_time:10518ms step_avg:34.26ms
step:308/2160 train_time:10551ms step_avg:34.26ms
step:309/2160 train_time:10585ms step_avg:34.25ms
step:310/2160 train_time:10618ms step_avg:34.25ms
step:311/2160 train_time:10653ms step_avg:34.25ms
step:312/2160 train_time:10686ms step_avg:34.25ms
step:313/2160 train_time:10720ms step_avg:34.25ms
step:314/2160 train_time:10753ms step_avg:34.24ms
step:315/2160 train_time:10787ms step_avg:34.24ms
step:316/2160 train_time:10820ms step_avg:34.24ms
step:317/2160 train_time:10854ms step_avg:34.24ms
step:318/2160 train_time:10887ms step_avg:34.24ms
step:319/2160 train_time:10921ms step_avg:34.24ms
step:320/2160 train_time:10955ms step_avg:34.23ms
step:321/2160 train_time:10989ms step_avg:34.23ms
step:322/2160 train_time:11022ms step_avg:34.23ms
step:323/2160 train_time:11056ms step_avg:34.23ms
step:324/2160 train_time:11090ms step_avg:34.23ms
step:325/2160 train_time:11123ms step_avg:34.23ms
step:326/2160 train_time:11157ms step_avg:34.22ms
step:327/2160 train_time:11191ms step_avg:34.22ms
step:328/2160 train_time:11224ms step_avg:34.22ms
step:329/2160 train_time:11258ms step_avg:34.22ms
step:330/2160 train_time:11292ms step_avg:34.22ms
step:331/2160 train_time:11326ms step_avg:34.22ms
step:332/2160 train_time:11359ms step_avg:34.21ms
step:333/2160 train_time:11393ms step_avg:34.21ms
step:334/2160 train_time:11426ms step_avg:34.21ms
step:335/2160 train_time:11460ms step_avg:34.21ms
step:336/2160 train_time:11493ms step_avg:34.20ms
step:337/2160 train_time:11527ms step_avg:34.20ms
step:338/2160 train_time:11560ms step_avg:34.20ms
step:339/2160 train_time:11594ms step_avg:34.20ms
step:340/2160 train_time:11627ms step_avg:34.20ms
step:341/2160 train_time:11661ms step_avg:34.20ms
step:342/2160 train_time:11694ms step_avg:34.19ms
step:343/2160 train_time:11728ms step_avg:34.19ms
step:344/2160 train_time:11761ms step_avg:34.19ms
step:345/2160 train_time:11795ms step_avg:34.19ms
step:346/2160 train_time:11829ms step_avg:34.19ms
step:347/2160 train_time:11862ms step_avg:34.18ms
step:348/2160 train_time:11895ms step_avg:34.18ms
step:349/2160 train_time:11929ms step_avg:34.18ms
step:350/2160 train_time:11962ms step_avg:34.18ms
step:351/2160 train_time:11996ms step_avg:34.18ms
step:352/2160 train_time:12030ms step_avg:34.18ms
step:353/2160 train_time:12063ms step_avg:34.17ms
step:354/2160 train_time:12096ms step_avg:34.17ms
step:355/2160 train_time:12131ms step_avg:34.17ms
step:356/2160 train_time:12163ms step_avg:34.17ms
step:357/2160 train_time:12198ms step_avg:34.17ms
step:358/2160 train_time:12231ms step_avg:34.17ms
step:359/2160 train_time:12265ms step_avg:34.16ms
step:360/2160 train_time:12298ms step_avg:34.16ms
step:361/2160 train_time:12332ms step_avg:34.16ms
step:362/2160 train_time:12366ms step_avg:34.16ms
step:363/2160 train_time:12399ms step_avg:34.16ms
step:364/2160 train_time:12433ms step_avg:34.16ms
step:365/2160 train_time:12466ms step_avg:34.15ms
step:366/2160 train_time:12499ms step_avg:34.15ms
step:367/2160 train_time:12533ms step_avg:34.15ms
step:368/2160 train_time:12566ms step_avg:34.15ms
step:369/2160 train_time:12600ms step_avg:34.15ms
step:370/2160 train_time:12633ms step_avg:34.14ms
step:371/2160 train_time:12667ms step_avg:34.14ms
step:372/2160 train_time:12700ms step_avg:34.14ms
step:373/2160 train_time:12734ms step_avg:34.14ms
step:374/2160 train_time:12768ms step_avg:34.14ms
step:375/2160 train_time:12801ms step_avg:34.14ms
step:376/2160 train_time:12835ms step_avg:34.13ms
step:377/2160 train_time:12868ms step_avg:34.13ms
step:378/2160 train_time:12901ms step_avg:34.13ms
step:379/2160 train_time:12935ms step_avg:34.13ms
step:380/2160 train_time:12969ms step_avg:34.13ms
step:381/2160 train_time:13002ms step_avg:34.13ms
step:382/2160 train_time:13036ms step_avg:34.13ms
step:383/2160 train_time:13070ms step_avg:34.12ms
step:384/2160 train_time:13103ms step_avg:34.12ms
step:385/2160 train_time:13137ms step_avg:34.12ms
step:386/2160 train_time:13170ms step_avg:34.12ms
step:387/2160 train_time:13204ms step_avg:34.12ms
step:388/2160 train_time:13237ms step_avg:34.12ms
step:389/2160 train_time:13271ms step_avg:34.12ms
step:390/2160 train_time:13305ms step_avg:34.11ms
step:391/2160 train_time:13339ms step_avg:34.11ms
step:392/2160 train_time:13372ms step_avg:34.11ms
step:393/2160 train_time:13406ms step_avg:34.11ms
step:394/2160 train_time:13439ms step_avg:34.11ms
step:395/2160 train_time:13474ms step_avg:34.11ms
step:396/2160 train_time:13507ms step_avg:34.11ms
step:397/2160 train_time:13540ms step_avg:34.11ms
step:398/2160 train_time:13573ms step_avg:34.10ms
step:399/2160 train_time:13607ms step_avg:34.10ms
step:400/2160 train_time:13640ms step_avg:34.10ms
step:401/2160 train_time:13675ms step_avg:34.10ms
step:402/2160 train_time:13708ms step_avg:34.10ms
step:403/2160 train_time:13742ms step_avg:34.10ms
step:404/2160 train_time:13775ms step_avg:34.10ms
step:405/2160 train_time:13808ms step_avg:34.09ms
step:406/2160 train_time:13842ms step_avg:34.09ms
step:407/2160 train_time:13876ms step_avg:34.09ms
step:408/2160 train_time:13909ms step_avg:34.09ms
step:409/2160 train_time:13942ms step_avg:34.09ms
step:410/2160 train_time:13976ms step_avg:34.09ms
step:411/2160 train_time:14009ms step_avg:34.09ms
step:412/2160 train_time:14042ms step_avg:34.08ms
step:413/2160 train_time:14076ms step_avg:34.08ms
step:414/2160 train_time:14110ms step_avg:34.08ms
step:415/2160 train_time:14143ms step_avg:34.08ms
step:416/2160 train_time:14176ms step_avg:34.08ms
step:417/2160 train_time:14210ms step_avg:34.08ms
step:418/2160 train_time:14244ms step_avg:34.08ms
step:419/2160 train_time:14277ms step_avg:34.07ms
step:420/2160 train_time:14311ms step_avg:34.07ms
step:421/2160 train_time:14344ms step_avg:34.07ms
step:422/2160 train_time:14378ms step_avg:34.07ms
step:423/2160 train_time:14412ms step_avg:34.07ms
step:424/2160 train_time:14445ms step_avg:34.07ms
step:425/2160 train_time:14479ms step_avg:34.07ms
step:426/2160 train_time:14512ms step_avg:34.07ms
step:427/2160 train_time:14546ms step_avg:34.07ms
step:428/2160 train_time:14579ms step_avg:34.06ms
step:429/2160 train_time:14614ms step_avg:34.07ms
step:430/2160 train_time:14647ms step_avg:34.06ms
step:431/2160 train_time:14681ms step_avg:34.06ms
step:432/2160 train_time:14714ms step_avg:34.06ms
step:433/2160 train_time:14748ms step_avg:34.06ms
step:434/2160 train_time:14781ms step_avg:34.06ms
step:435/2160 train_time:14815ms step_avg:34.06ms
step:436/2160 train_time:14849ms step_avg:34.06ms
step:437/2160 train_time:14882ms step_avg:34.06ms
step:438/2160 train_time:14915ms step_avg:34.05ms
step:439/2160 train_time:14949ms step_avg:34.05ms
step:440/2160 train_time:14982ms step_avg:34.05ms
step:441/2160 train_time:15016ms step_avg:34.05ms
step:442/2160 train_time:15050ms step_avg:34.05ms
step:443/2160 train_time:15083ms step_avg:34.05ms
step:444/2160 train_time:15116ms step_avg:34.05ms
step:445/2160 train_time:15150ms step_avg:34.05ms
step:446/2160 train_time:15184ms step_avg:34.04ms
step:447/2160 train_time:15217ms step_avg:34.04ms
step:448/2160 train_time:15251ms step_avg:34.04ms
step:449/2160 train_time:15284ms step_avg:34.04ms
step:450/2160 train_time:15317ms step_avg:34.04ms
step:451/2160 train_time:15351ms step_avg:34.04ms
step:452/2160 train_time:15384ms step_avg:34.04ms
step:453/2160 train_time:15419ms step_avg:34.04ms
step:454/2160 train_time:15452ms step_avg:34.03ms
step:455/2160 train_time:15485ms step_avg:34.03ms
step:456/2160 train_time:15518ms step_avg:34.03ms
step:457/2160 train_time:15553ms step_avg:34.03ms
step:458/2160 train_time:15586ms step_avg:34.03ms
step:459/2160 train_time:15620ms step_avg:34.03ms
step:460/2160 train_time:15653ms step_avg:34.03ms
step:461/2160 train_time:15687ms step_avg:34.03ms
step:462/2160 train_time:15720ms step_avg:34.03ms
step:463/2160 train_time:15754ms step_avg:34.03ms
step:464/2160 train_time:15787ms step_avg:34.02ms
step:465/2160 train_time:15821ms step_avg:34.02ms
step:466/2160 train_time:15854ms step_avg:34.02ms
step:467/2160 train_time:15888ms step_avg:34.02ms
step:468/2160 train_time:15922ms step_avg:34.02ms
step:469/2160 train_time:15956ms step_avg:34.02ms
step:470/2160 train_time:15989ms step_avg:34.02ms
step:471/2160 train_time:16023ms step_avg:34.02ms
step:472/2160 train_time:16056ms step_avg:34.02ms
step:473/2160 train_time:16090ms step_avg:34.02ms
step:474/2160 train_time:16123ms step_avg:34.01ms
step:475/2160 train_time:16157ms step_avg:34.01ms
step:476/2160 train_time:16190ms step_avg:34.01ms
step:477/2160 train_time:16224ms step_avg:34.01ms
step:478/2160 train_time:16257ms step_avg:34.01ms
step:479/2160 train_time:16291ms step_avg:34.01ms
step:480/2160 train_time:16324ms step_avg:34.01ms
step:481/2160 train_time:16358ms step_avg:34.01ms
step:482/2160 train_time:16391ms step_avg:34.01ms
step:483/2160 train_time:16425ms step_avg:34.01ms
step:484/2160 train_time:16458ms step_avg:34.00ms
step:485/2160 train_time:16492ms step_avg:34.00ms
step:486/2160 train_time:16525ms step_avg:34.00ms
step:487/2160 train_time:16559ms step_avg:34.00ms
step:488/2160 train_time:16592ms step_avg:34.00ms
step:489/2160 train_time:16626ms step_avg:34.00ms
step:490/2160 train_time:16659ms step_avg:34.00ms
step:491/2160 train_time:16693ms step_avg:34.00ms
step:492/2160 train_time:16727ms step_avg:34.00ms
step:493/2160 train_time:16761ms step_avg:34.00ms
step:494/2160 train_time:16794ms step_avg:34.00ms
step:495/2160 train_time:16827ms step_avg:33.99ms
step:496/2160 train_time:16860ms step_avg:33.99ms
step:497/2160 train_time:16895ms step_avg:33.99ms
step:498/2160 train_time:16928ms step_avg:33.99ms
step:499/2160 train_time:16962ms step_avg:33.99ms
step:500/2160 train_time:16995ms step_avg:33.99ms
step:500/2160 val_loss:4.0087 train_time:17030ms step_avg:34.06ms
step:501/2160 train_time:17052ms step_avg:34.04ms
step:502/2160 train_time:17075ms step_avg:34.01ms
step:503/2160 train_time:17100ms step_avg:34.00ms
step:504/2160 train_time:17134ms step_avg:34.00ms
step:505/2160 train_time:17169ms step_avg:34.00ms
step:506/2160 train_time:17203ms step_avg:34.00ms
step:507/2160 train_time:17239ms step_avg:34.00ms
step:508/2160 train_time:17272ms step_avg:34.00ms
step:509/2160 train_time:17306ms step_avg:34.00ms
step:510/2160 train_time:17339ms step_avg:34.00ms
step:511/2160 train_time:17373ms step_avg:34.00ms
step:512/2160 train_time:17406ms step_avg:34.00ms
step:513/2160 train_time:17440ms step_avg:34.00ms
step:514/2160 train_time:17473ms step_avg:33.99ms
step:515/2160 train_time:17507ms step_avg:33.99ms
step:516/2160 train_time:17540ms step_avg:33.99ms
step:517/2160 train_time:17574ms step_avg:33.99ms
step:518/2160 train_time:17607ms step_avg:33.99ms
step:519/2160 train_time:17640ms step_avg:33.99ms
step:520/2160 train_time:17673ms step_avg:33.99ms
step:521/2160 train_time:17707ms step_avg:33.99ms
step:522/2160 train_time:17740ms step_avg:33.99ms
step:523/2160 train_time:17774ms step_avg:33.98ms
step:524/2160 train_time:17807ms step_avg:33.98ms
step:525/2160 train_time:17841ms step_avg:33.98ms
step:526/2160 train_time:17874ms step_avg:33.98ms
step:527/2160 train_time:17907ms step_avg:33.98ms
step:528/2160 train_time:17940ms step_avg:33.98ms
step:529/2160 train_time:17974ms step_avg:33.98ms
step:530/2160 train_time:18007ms step_avg:33.98ms
step:531/2160 train_time:18041ms step_avg:33.97ms
step:532/2160 train_time:18074ms step_avg:33.97ms
step:533/2160 train_time:18108ms step_avg:33.97ms
step:534/2160 train_time:18141ms step_avg:33.97ms
step:535/2160 train_time:18175ms step_avg:33.97ms
step:536/2160 train_time:18209ms step_avg:33.97ms
step:537/2160 train_time:18244ms step_avg:33.97ms
step:538/2160 train_time:18277ms step_avg:33.97ms
step:539/2160 train_time:18311ms step_avg:33.97ms
step:540/2160 train_time:18344ms step_avg:33.97ms
step:541/2160 train_time:18378ms step_avg:33.97ms
step:542/2160 train_time:18411ms step_avg:33.97ms
step:543/2160 train_time:18445ms step_avg:33.97ms
step:544/2160 train_time:18478ms step_avg:33.97ms
step:545/2160 train_time:18512ms step_avg:33.97ms
step:546/2160 train_time:18545ms step_avg:33.97ms
step:547/2160 train_time:18579ms step_avg:33.97ms
step:548/2160 train_time:18612ms step_avg:33.96ms
step:549/2160 train_time:18646ms step_avg:33.96ms
step:550/2160 train_time:18679ms step_avg:33.96ms
step:551/2160 train_time:18713ms step_avg:33.96ms
step:552/2160 train_time:18746ms step_avg:33.96ms
step:553/2160 train_time:18780ms step_avg:33.96ms
step:554/2160 train_time:18813ms step_avg:33.96ms
step:555/2160 train_time:18846ms step_avg:33.96ms
step:556/2160 train_time:18879ms step_avg:33.96ms
step:557/2160 train_time:18914ms step_avg:33.96ms
step:558/2160 train_time:18947ms step_avg:33.95ms
step:559/2160 train_time:18981ms step_avg:33.96ms
step:560/2160 train_time:19014ms step_avg:33.95ms
step:561/2160 train_time:19048ms step_avg:33.95ms
step:562/2160 train_time:19081ms step_avg:33.95ms
step:563/2160 train_time:19115ms step_avg:33.95ms
step:564/2160 train_time:19148ms step_avg:33.95ms
step:565/2160 train_time:19182ms step_avg:33.95ms
step:566/2160 train_time:19215ms step_avg:33.95ms
step:567/2160 train_time:19250ms step_avg:33.95ms
step:568/2160 train_time:19282ms step_avg:33.95ms
step:569/2160 train_time:19317ms step_avg:33.95ms
step:570/2160 train_time:19350ms step_avg:33.95ms
step:571/2160 train_time:19384ms step_avg:33.95ms
step:572/2160 train_time:19417ms step_avg:33.95ms
step:573/2160 train_time:19452ms step_avg:33.95ms
step:574/2160 train_time:19485ms step_avg:33.95ms
step:575/2160 train_time:19519ms step_avg:33.95ms
step:576/2160 train_time:19552ms step_avg:33.94ms
step:577/2160 train_time:19586ms step_avg:33.94ms
step:578/2160 train_time:19619ms step_avg:33.94ms
step:579/2160 train_time:19653ms step_avg:33.94ms
step:580/2160 train_time:19686ms step_avg:33.94ms
step:581/2160 train_time:19720ms step_avg:33.94ms
step:582/2160 train_time:19754ms step_avg:33.94ms
step:583/2160 train_time:19787ms step_avg:33.94ms
step:584/2160 train_time:19820ms step_avg:33.94ms
step:585/2160 train_time:19854ms step_avg:33.94ms
step:586/2160 train_time:19887ms step_avg:33.94ms
step:587/2160 train_time:19921ms step_avg:33.94ms
step:588/2160 train_time:19955ms step_avg:33.94ms
step:589/2160 train_time:19989ms step_avg:33.94ms
step:590/2160 train_time:20022ms step_avg:33.93ms
step:591/2160 train_time:20056ms step_avg:33.93ms
step:592/2160 train_time:20089ms step_avg:33.93ms
step:593/2160 train_time:20123ms step_avg:33.93ms
step:594/2160 train_time:20156ms step_avg:33.93ms
step:595/2160 train_time:20190ms step_avg:33.93ms
step:596/2160 train_time:20223ms step_avg:33.93ms
step:597/2160 train_time:20257ms step_avg:33.93ms
step:598/2160 train_time:20290ms step_avg:33.93ms
step:599/2160 train_time:20324ms step_avg:33.93ms
step:600/2160 train_time:20357ms step_avg:33.93ms
step:601/2160 train_time:20391ms step_avg:33.93ms
step:602/2160 train_time:20425ms step_avg:33.93ms
step:603/2160 train_time:20459ms step_avg:33.93ms
step:604/2160 train_time:20492ms step_avg:33.93ms
step:605/2160 train_time:20526ms step_avg:33.93ms
step:606/2160 train_time:20559ms step_avg:33.93ms
step:607/2160 train_time:20593ms step_avg:33.93ms
step:608/2160 train_time:20626ms step_avg:33.92ms
step:609/2160 train_time:20660ms step_avg:33.93ms
step:610/2160 train_time:20694ms step_avg:33.92ms
step:611/2160 train_time:20728ms step_avg:33.92ms
step:612/2160 train_time:20761ms step_avg:33.92ms
step:613/2160 train_time:20795ms step_avg:33.92ms
step:614/2160 train_time:20828ms step_avg:33.92ms
step:615/2160 train_time:20862ms step_avg:33.92ms
step:616/2160 train_time:20895ms step_avg:33.92ms
step:617/2160 train_time:20929ms step_avg:33.92ms
step:618/2160 train_time:20962ms step_avg:33.92ms
step:619/2160 train_time:20996ms step_avg:33.92ms
step:620/2160 train_time:21029ms step_avg:33.92ms
step:621/2160 train_time:21063ms step_avg:33.92ms
step:622/2160 train_time:21096ms step_avg:33.92ms
step:623/2160 train_time:21130ms step_avg:33.92ms
step:624/2160 train_time:21164ms step_avg:33.92ms
step:625/2160 train_time:21197ms step_avg:33.92ms
step:626/2160 train_time:21231ms step_avg:33.91ms
step:627/2160 train_time:21264ms step_avg:33.91ms
step:628/2160 train_time:21298ms step_avg:33.91ms
step:629/2160 train_time:21331ms step_avg:33.91ms
step:630/2160 train_time:21365ms step_avg:33.91ms
step:631/2160 train_time:21399ms step_avg:33.91ms
step:632/2160 train_time:21432ms step_avg:33.91ms
step:633/2160 train_time:21466ms step_avg:33.91ms
step:634/2160 train_time:21499ms step_avg:33.91ms
step:635/2160 train_time:21533ms step_avg:33.91ms
step:636/2160 train_time:21566ms step_avg:33.91ms
step:637/2160 train_time:21601ms step_avg:33.91ms
step:638/2160 train_time:21634ms step_avg:33.91ms
step:639/2160 train_time:21668ms step_avg:33.91ms
step:640/2160 train_time:21701ms step_avg:33.91ms
step:641/2160 train_time:21735ms step_avg:33.91ms
step:642/2160 train_time:21769ms step_avg:33.91ms
step:643/2160 train_time:21802ms step_avg:33.91ms
step:644/2160 train_time:21836ms step_avg:33.91ms
step:645/2160 train_time:21869ms step_avg:33.91ms
step:646/2160 train_time:21902ms step_avg:33.90ms
step:647/2160 train_time:21936ms step_avg:33.90ms
step:648/2160 train_time:21969ms step_avg:33.90ms
step:649/2160 train_time:22003ms step_avg:33.90ms
step:650/2160 train_time:22037ms step_avg:33.90ms
step:651/2160 train_time:22070ms step_avg:33.90ms
step:652/2160 train_time:22103ms step_avg:33.90ms
step:653/2160 train_time:22137ms step_avg:33.90ms
step:654/2160 train_time:22171ms step_avg:33.90ms
step:655/2160 train_time:22205ms step_avg:33.90ms
step:656/2160 train_time:22238ms step_avg:33.90ms
step:657/2160 train_time:22272ms step_avg:33.90ms
step:658/2160 train_time:22305ms step_avg:33.90ms
step:659/2160 train_time:22339ms step_avg:33.90ms
step:660/2160 train_time:22373ms step_avg:33.90ms
step:661/2160 train_time:22406ms step_avg:33.90ms
step:662/2160 train_time:22439ms step_avg:33.90ms
step:663/2160 train_time:22473ms step_avg:33.90ms
step:664/2160 train_time:22507ms step_avg:33.90ms
step:665/2160 train_time:22541ms step_avg:33.90ms
step:666/2160 train_time:22575ms step_avg:33.90ms
step:667/2160 train_time:22608ms step_avg:33.90ms
step:668/2160 train_time:22642ms step_avg:33.89ms
step:669/2160 train_time:22676ms step_avg:33.90ms
step:670/2160 train_time:22709ms step_avg:33.89ms
step:671/2160 train_time:22744ms step_avg:33.90ms
step:672/2160 train_time:22777ms step_avg:33.89ms
step:673/2160 train_time:22810ms step_avg:33.89ms
step:674/2160 train_time:22844ms step_avg:33.89ms
step:675/2160 train_time:22877ms step_avg:33.89ms
step:676/2160 train_time:22911ms step_avg:33.89ms
step:677/2160 train_time:22945ms step_avg:33.89ms
step:678/2160 train_time:22978ms step_avg:33.89ms
step:679/2160 train_time:23012ms step_avg:33.89ms
step:680/2160 train_time:23045ms step_avg:33.89ms
step:681/2160 train_time:23079ms step_avg:33.89ms
step:682/2160 train_time:23112ms step_avg:33.89ms
step:683/2160 train_time:23146ms step_avg:33.89ms
step:684/2160 train_time:23179ms step_avg:33.89ms
step:685/2160 train_time:23213ms step_avg:33.89ms
step:686/2160 train_time:23246ms step_avg:33.89ms
step:687/2160 train_time:23281ms step_avg:33.89ms
step:688/2160 train_time:23314ms step_avg:33.89ms
step:689/2160 train_time:23347ms step_avg:33.89ms
step:690/2160 train_time:23381ms step_avg:33.88ms
step:691/2160 train_time:23415ms step_avg:33.89ms
step:692/2160 train_time:23448ms step_avg:33.88ms
step:693/2160 train_time:23482ms step_avg:33.88ms
step:694/2160 train_time:23516ms step_avg:33.88ms
step:695/2160 train_time:23549ms step_avg:33.88ms
step:696/2160 train_time:23582ms step_avg:33.88ms
step:697/2160 train_time:23617ms step_avg:33.88ms
step:698/2160 train_time:23650ms step_avg:33.88ms
step:699/2160 train_time:23684ms step_avg:33.88ms
step:700/2160 train_time:23717ms step_avg:33.88ms
step:701/2160 train_time:23751ms step_avg:33.88ms
step:702/2160 train_time:23784ms step_avg:33.88ms
step:703/2160 train_time:23819ms step_avg:33.88ms
step:704/2160 train_time:23852ms step_avg:33.88ms
step:705/2160 train_time:23886ms step_avg:33.88ms
step:706/2160 train_time:23919ms step_avg:33.88ms
step:707/2160 train_time:23953ms step_avg:33.88ms
step:708/2160 train_time:23987ms step_avg:33.88ms
step:709/2160 train_time:24047ms step_avg:33.92ms
step:710/2160 train_time:24106ms step_avg:33.95ms
step:711/2160 train_time:24168ms step_avg:33.99ms
step:712/2160 train_time:24228ms step_avg:34.03ms
step:713/2160 train_time:24289ms step_avg:34.07ms
step:714/2160 train_time:24349ms step_avg:34.10ms
step:715/2160 train_time:24411ms step_avg:34.14ms
step:716/2160 train_time:24471ms step_avg:34.18ms
step:717/2160 train_time:24533ms step_avg:34.22ms
step:718/2160 train_time:24592ms step_avg:34.25ms
step:719/2160 train_time:24654ms step_avg:34.29ms
step:720/2160 train_time:24713ms step_avg:34.32ms
step:721/2160 train_time:24775ms step_avg:34.36ms
step:722/2160 train_time:24835ms step_avg:34.40ms
step:723/2160 train_time:24896ms step_avg:34.43ms
step:724/2160 train_time:24955ms step_avg:34.47ms
step:725/2160 train_time:25016ms step_avg:34.50ms
step:726/2160 train_time:25075ms step_avg:34.54ms
step:727/2160 train_time:25136ms step_avg:34.57ms
step:728/2160 train_time:25196ms step_avg:34.61ms
step:729/2160 train_time:25256ms step_avg:34.65ms
step:730/2160 train_time:25316ms step_avg:34.68ms
step:731/2160 train_time:25378ms step_avg:34.72ms
step:732/2160 train_time:25437ms step_avg:34.75ms
step:733/2160 train_time:25499ms step_avg:34.79ms
step:734/2160 train_time:25558ms step_avg:34.82ms
step:735/2160 train_time:25619ms step_avg:34.86ms
step:736/2160 train_time:25679ms step_avg:34.89ms
step:737/2160 train_time:25739ms step_avg:34.92ms
step:738/2160 train_time:25798ms step_avg:34.96ms
step:739/2160 train_time:25859ms step_avg:34.99ms
step:740/2160 train_time:25918ms step_avg:35.02ms
step:741/2160 train_time:25978ms step_avg:35.06ms
step:742/2160 train_time:26038ms step_avg:35.09ms
step:743/2160 train_time:26098ms step_avg:35.13ms
step:744/2160 train_time:26157ms step_avg:35.16ms
step:745/2160 train_time:26219ms step_avg:35.19ms
step:746/2160 train_time:26279ms step_avg:35.23ms
step:747/2160 train_time:26340ms step_avg:35.26ms
step:748/2160 train_time:26400ms step_avg:35.29ms
step:749/2160 train_time:26460ms step_avg:35.33ms
step:750/2160 train_time:26519ms step_avg:35.36ms
step:750/2160 val_loss:3.8475 train_time:26581ms step_avg:35.44ms
step:751/2160 train_time:26604ms step_avg:35.42ms
step:752/2160 train_time:26641ms step_avg:35.43ms
step:753/2160 train_time:26705ms step_avg:35.46ms
step:754/2160 train_time:26769ms step_avg:35.50ms
step:755/2160 train_time:26833ms step_avg:35.54ms
step:756/2160 train_time:26892ms step_avg:35.57ms
step:757/2160 train_time:26953ms step_avg:35.60ms
step:758/2160 train_time:27011ms step_avg:35.63ms
step:759/2160 train_time:27072ms step_avg:35.67ms
step:760/2160 train_time:27130ms step_avg:35.70ms
step:761/2160 train_time:27192ms step_avg:35.73ms
step:762/2160 train_time:27252ms step_avg:35.76ms
step:763/2160 train_time:27312ms step_avg:35.80ms
step:764/2160 train_time:27371ms step_avg:35.83ms
step:765/2160 train_time:27431ms step_avg:35.86ms
step:766/2160 train_time:27492ms step_avg:35.89ms
step:767/2160 train_time:27557ms step_avg:35.93ms
step:768/2160 train_time:27618ms step_avg:35.96ms
step:769/2160 train_time:27681ms step_avg:36.00ms
step:770/2160 train_time:27742ms step_avg:36.03ms
step:771/2160 train_time:27804ms step_avg:36.06ms
step:772/2160 train_time:27863ms step_avg:36.09ms
step:773/2160 train_time:27924ms step_avg:36.12ms
step:774/2160 train_time:27984ms step_avg:36.15ms
step:775/2160 train_time:28043ms step_avg:36.18ms
step:776/2160 train_time:28102ms step_avg:36.21ms
step:777/2160 train_time:28162ms step_avg:36.25ms
step:778/2160 train_time:28222ms step_avg:36.28ms
step:779/2160 train_time:28283ms step_avg:36.31ms
step:780/2160 train_time:28343ms step_avg:36.34ms
step:781/2160 train_time:28403ms step_avg:36.37ms
step:782/2160 train_time:28463ms step_avg:36.40ms
step:783/2160 train_time:28524ms step_avg:36.43ms
step:784/2160 train_time:28584ms step_avg:36.46ms
step:785/2160 train_time:28645ms step_avg:36.49ms
step:786/2160 train_time:28706ms step_avg:36.52ms
step:787/2160 train_time:28767ms step_avg:36.55ms
step:788/2160 train_time:28827ms step_avg:36.58ms
step:789/2160 train_time:28888ms step_avg:36.61ms
step:790/2160 train_time:28947ms step_avg:36.64ms
step:791/2160 train_time:29008ms step_avg:36.67ms
step:792/2160 train_time:29067ms step_avg:36.70ms
step:793/2160 train_time:29129ms step_avg:36.73ms
step:794/2160 train_time:29188ms step_avg:36.76ms
step:795/2160 train_time:29250ms step_avg:36.79ms
step:796/2160 train_time:29309ms step_avg:36.82ms
step:797/2160 train_time:29370ms step_avg:36.85ms
step:798/2160 train_time:29430ms step_avg:36.88ms
step:799/2160 train_time:29491ms step_avg:36.91ms
step:800/2160 train_time:29551ms step_avg:36.94ms
step:801/2160 train_time:29612ms step_avg:36.97ms
step:802/2160 train_time:29673ms step_avg:37.00ms
step:803/2160 train_time:29735ms step_avg:37.03ms
step:804/2160 train_time:29794ms step_avg:37.06ms
step:805/2160 train_time:29856ms step_avg:37.09ms
step:806/2160 train_time:29915ms step_avg:37.12ms
step:807/2160 train_time:29976ms step_avg:37.15ms
step:808/2160 train_time:30036ms step_avg:37.17ms
step:809/2160 train_time:30098ms step_avg:37.20ms
step:810/2160 train_time:30157ms step_avg:37.23ms
step:811/2160 train_time:30218ms step_avg:37.26ms
step:812/2160 train_time:30277ms step_avg:37.29ms
step:813/2160 train_time:30338ms step_avg:37.32ms
step:814/2160 train_time:30398ms step_avg:37.34ms
step:815/2160 train_time:30459ms step_avg:37.37ms
step:816/2160 train_time:30518ms step_avg:37.40ms
step:817/2160 train_time:30579ms step_avg:37.43ms
step:818/2160 train_time:30639ms step_avg:37.46ms
step:819/2160 train_time:30700ms step_avg:37.48ms
step:820/2160 train_time:30759ms step_avg:37.51ms
step:821/2160 train_time:30820ms step_avg:37.54ms
step:822/2160 train_time:30879ms step_avg:37.57ms
step:823/2160 train_time:30940ms step_avg:37.59ms
step:824/2160 train_time:31000ms step_avg:37.62ms
step:825/2160 train_time:31061ms step_avg:37.65ms
step:826/2160 train_time:31120ms step_avg:37.68ms
step:827/2160 train_time:31181ms step_avg:37.70ms
step:828/2160 train_time:31240ms step_avg:37.73ms
step:829/2160 train_time:31301ms step_avg:37.76ms
step:830/2160 train_time:31360ms step_avg:37.78ms
step:831/2160 train_time:31420ms step_avg:37.81ms
step:832/2160 train_time:31480ms step_avg:37.84ms
step:833/2160 train_time:31541ms step_avg:37.86ms
step:834/2160 train_time:31601ms step_avg:37.89ms
step:835/2160 train_time:31661ms step_avg:37.92ms
step:836/2160 train_time:31721ms step_avg:37.94ms
step:837/2160 train_time:31781ms step_avg:37.97ms
step:838/2160 train_time:31841ms step_avg:38.00ms
step:839/2160 train_time:31902ms step_avg:38.02ms
step:840/2160 train_time:31961ms step_avg:38.05ms
step:841/2160 train_time:32021ms step_avg:38.08ms
step:842/2160 train_time:32080ms step_avg:38.10ms
step:843/2160 train_time:32141ms step_avg:38.13ms
step:844/2160 train_time:32200ms step_avg:38.15ms
step:845/2160 train_time:32261ms step_avg:38.18ms
step:846/2160 train_time:32320ms step_avg:38.20ms
step:847/2160 train_time:32381ms step_avg:38.23ms
step:848/2160 train_time:32440ms step_avg:38.26ms
step:849/2160 train_time:32501ms step_avg:38.28ms
step:850/2160 train_time:32560ms step_avg:38.31ms
step:851/2160 train_time:32621ms step_avg:38.33ms
step:852/2160 train_time:32681ms step_avg:38.36ms
step:853/2160 train_time:32741ms step_avg:38.38ms
step:854/2160 train_time:32801ms step_avg:38.41ms
step:855/2160 train_time:32861ms step_avg:38.43ms
step:856/2160 train_time:32920ms step_avg:38.46ms
step:857/2160 train_time:32981ms step_avg:38.48ms
step:858/2160 train_time:33041ms step_avg:38.51ms
step:859/2160 train_time:33101ms step_avg:38.53ms
step:860/2160 train_time:33161ms step_avg:38.56ms
step:861/2160 train_time:33222ms step_avg:38.59ms
step:862/2160 train_time:33281ms step_avg:38.61ms
step:863/2160 train_time:33342ms step_avg:38.64ms
step:864/2160 train_time:33402ms step_avg:38.66ms
step:865/2160 train_time:33462ms step_avg:38.68ms
step:866/2160 train_time:33522ms step_avg:38.71ms
step:867/2160 train_time:33582ms step_avg:38.73ms
step:868/2160 train_time:33642ms step_avg:38.76ms
step:869/2160 train_time:33702ms step_avg:38.78ms
step:870/2160 train_time:33762ms step_avg:38.81ms
step:871/2160 train_time:33822ms step_avg:38.83ms
step:872/2160 train_time:33881ms step_avg:38.85ms
step:873/2160 train_time:33942ms step_avg:38.88ms
step:874/2160 train_time:34001ms step_avg:38.90ms
step:875/2160 train_time:34061ms step_avg:38.93ms
step:876/2160 train_time:34121ms step_avg:38.95ms
step:877/2160 train_time:34181ms step_avg:38.97ms
step:878/2160 train_time:34240ms step_avg:39.00ms
step:879/2160 train_time:34301ms step_avg:39.02ms
step:880/2160 train_time:34360ms step_avg:39.05ms
step:881/2160 train_time:34421ms step_avg:39.07ms
step:882/2160 train_time:34481ms step_avg:39.09ms
step:883/2160 train_time:34542ms step_avg:39.12ms
step:884/2160 train_time:34601ms step_avg:39.14ms
step:885/2160 train_time:34661ms step_avg:39.17ms
step:886/2160 train_time:34721ms step_avg:39.19ms
step:887/2160 train_time:34781ms step_avg:39.21ms
step:888/2160 train_time:34841ms step_avg:39.24ms
step:889/2160 train_time:34901ms step_avg:39.26ms
step:890/2160 train_time:34960ms step_avg:39.28ms
step:891/2160 train_time:35020ms step_avg:39.30ms
step:892/2160 train_time:35079ms step_avg:39.33ms
step:893/2160 train_time:35140ms step_avg:39.35ms
step:894/2160 train_time:35199ms step_avg:39.37ms
step:895/2160 train_time:35260ms step_avg:39.40ms
step:896/2160 train_time:35320ms step_avg:39.42ms
step:897/2160 train_time:35380ms step_avg:39.44ms
step:898/2160 train_time:35440ms step_avg:39.47ms
step:899/2160 train_time:35501ms step_avg:39.49ms
step:900/2160 train_time:35560ms step_avg:39.51ms
step:901/2160 train_time:35621ms step_avg:39.53ms
step:902/2160 train_time:35680ms step_avg:39.56ms
step:903/2160 train_time:35741ms step_avg:39.58ms
step:904/2160 train_time:35801ms step_avg:39.60ms
step:905/2160 train_time:35861ms step_avg:39.63ms
step:906/2160 train_time:35920ms step_avg:39.65ms
step:907/2160 train_time:35980ms step_avg:39.67ms
step:908/2160 train_time:36040ms step_avg:39.69ms
step:909/2160 train_time:36100ms step_avg:39.71ms
step:910/2160 train_time:36159ms step_avg:39.74ms
step:911/2160 train_time:36220ms step_avg:39.76ms
step:912/2160 train_time:36279ms step_avg:39.78ms
step:913/2160 train_time:36340ms step_avg:39.80ms
step:914/2160 train_time:36400ms step_avg:39.82ms
step:915/2160 train_time:36460ms step_avg:39.85ms
step:916/2160 train_time:36520ms step_avg:39.87ms
step:917/2160 train_time:36580ms step_avg:39.89ms
step:918/2160 train_time:36640ms step_avg:39.91ms
step:919/2160 train_time:36700ms step_avg:39.94ms
step:920/2160 train_time:36760ms step_avg:39.96ms
step:921/2160 train_time:36820ms step_avg:39.98ms
step:922/2160 train_time:36880ms step_avg:40.00ms
step:923/2160 train_time:36940ms step_avg:40.02ms
step:924/2160 train_time:36999ms step_avg:40.04ms
step:925/2160 train_time:37060ms step_avg:40.06ms
step:926/2160 train_time:37119ms step_avg:40.09ms
step:927/2160 train_time:37180ms step_avg:40.11ms
step:928/2160 train_time:37239ms step_avg:40.13ms
step:929/2160 train_time:37300ms step_avg:40.15ms
step:930/2160 train_time:37359ms step_avg:40.17ms
step:931/2160 train_time:37420ms step_avg:40.19ms
step:932/2160 train_time:37480ms step_avg:40.21ms
step:933/2160 train_time:37540ms step_avg:40.24ms
step:934/2160 train_time:37600ms step_avg:40.26ms
step:935/2160 train_time:37660ms step_avg:40.28ms
step:936/2160 train_time:37720ms step_avg:40.30ms
step:937/2160 train_time:37781ms step_avg:40.32ms
step:938/2160 train_time:37841ms step_avg:40.34ms
step:939/2160 train_time:37901ms step_avg:40.36ms
step:940/2160 train_time:37961ms step_avg:40.38ms
step:941/2160 train_time:38021ms step_avg:40.40ms
step:942/2160 train_time:38080ms step_avg:40.43ms
step:943/2160 train_time:38141ms step_avg:40.45ms
step:944/2160 train_time:38200ms step_avg:40.47ms
step:945/2160 train_time:38260ms step_avg:40.49ms
step:946/2160 train_time:38320ms step_avg:40.51ms
step:947/2160 train_time:38381ms step_avg:40.53ms
step:948/2160 train_time:38441ms step_avg:40.55ms
step:949/2160 train_time:38501ms step_avg:40.57ms
step:950/2160 train_time:38561ms step_avg:40.59ms
step:951/2160 train_time:38621ms step_avg:40.61ms
step:952/2160 train_time:38681ms step_avg:40.63ms
step:953/2160 train_time:38742ms step_avg:40.65ms
step:954/2160 train_time:38801ms step_avg:40.67ms
step:955/2160 train_time:38862ms step_avg:40.69ms
step:956/2160 train_time:38921ms step_avg:40.71ms
step:957/2160 train_time:38982ms step_avg:40.73ms
step:958/2160 train_time:39042ms step_avg:40.75ms
step:959/2160 train_time:39102ms step_avg:40.77ms
step:960/2160 train_time:39161ms step_avg:40.79ms
step:961/2160 train_time:39221ms step_avg:40.81ms
step:962/2160 train_time:39281ms step_avg:40.83ms
step:963/2160 train_time:39341ms step_avg:40.85ms
step:964/2160 train_time:39401ms step_avg:40.87ms
step:965/2160 train_time:39461ms step_avg:40.89ms
step:966/2160 train_time:39520ms step_avg:40.91ms
step:967/2160 train_time:39581ms step_avg:40.93ms
step:968/2160 train_time:39641ms step_avg:40.95ms
step:969/2160 train_time:39702ms step_avg:40.97ms
step:970/2160 train_time:39761ms step_avg:40.99ms
step:971/2160 train_time:39821ms step_avg:41.01ms
step:972/2160 train_time:39881ms step_avg:41.03ms
step:973/2160 train_time:39942ms step_avg:41.05ms
step:974/2160 train_time:40001ms step_avg:41.07ms
step:975/2160 train_time:40062ms step_avg:41.09ms
step:976/2160 train_time:40121ms step_avg:41.11ms
step:977/2160 train_time:40181ms step_avg:41.13ms
step:978/2160 train_time:40241ms step_avg:41.15ms
step:979/2160 train_time:40301ms step_avg:41.17ms
step:980/2160 train_time:40360ms step_avg:41.18ms
step:981/2160 train_time:40421ms step_avg:41.20ms
step:982/2160 train_time:40480ms step_avg:41.22ms
step:983/2160 train_time:40541ms step_avg:41.24ms
step:984/2160 train_time:40600ms step_avg:41.26ms
step:985/2160 train_time:40661ms step_avg:41.28ms
step:986/2160 train_time:40720ms step_avg:41.30ms
step:987/2160 train_time:40780ms step_avg:41.32ms
step:988/2160 train_time:40840ms step_avg:41.34ms
step:989/2160 train_time:40901ms step_avg:41.36ms
step:990/2160 train_time:40961ms step_avg:41.37ms
step:991/2160 train_time:41021ms step_avg:41.39ms
step:992/2160 train_time:41081ms step_avg:41.41ms
step:993/2160 train_time:41141ms step_avg:41.43ms
step:994/2160 train_time:41200ms step_avg:41.45ms
step:995/2160 train_time:41261ms step_avg:41.47ms
step:996/2160 train_time:41320ms step_avg:41.49ms
step:997/2160 train_time:41381ms step_avg:41.51ms
step:998/2160 train_time:41441ms step_avg:41.52ms
step:999/2160 train_time:41501ms step_avg:41.54ms
step:1000/2160 train_time:41561ms step_avg:41.56ms
step:1000/2160 val_loss:3.6941 train_time:41622ms step_avg:41.62ms
step:1001/2160 train_time:41644ms step_avg:41.60ms
step:1002/2160 train_time:41686ms step_avg:41.60ms
step:1003/2160 train_time:41748ms step_avg:41.62ms
step:1004/2160 train_time:41814ms step_avg:41.65ms
step:1005/2160 train_time:41877ms step_avg:41.67ms
step:1006/2160 train_time:41937ms step_avg:41.69ms
step:1007/2160 train_time:41998ms step_avg:41.71ms
step:1008/2160 train_time:42057ms step_avg:41.72ms
step:1009/2160 train_time:42118ms step_avg:41.74ms
step:1010/2160 train_time:42177ms step_avg:41.76ms
step:1011/2160 train_time:42237ms step_avg:41.78ms
step:1012/2160 train_time:42296ms step_avg:41.79ms
step:1013/2160 train_time:42356ms step_avg:41.81ms
step:1014/2160 train_time:42415ms step_avg:41.83ms
step:1015/2160 train_time:42476ms step_avg:41.85ms
step:1016/2160 train_time:42537ms step_avg:41.87ms
step:1017/2160 train_time:42600ms step_avg:41.89ms
step:1018/2160 train_time:42661ms step_avg:41.91ms
step:1019/2160 train_time:42724ms step_avg:41.93ms
step:1020/2160 train_time:42784ms step_avg:41.95ms
step:1021/2160 train_time:42845ms step_avg:41.96ms
step:1022/2160 train_time:42906ms step_avg:41.98ms
step:1023/2160 train_time:42966ms step_avg:42.00ms
step:1024/2160 train_time:43025ms step_avg:42.02ms
step:1025/2160 train_time:43086ms step_avg:42.04ms
step:1026/2160 train_time:43145ms step_avg:42.05ms
step:1027/2160 train_time:43205ms step_avg:42.07ms
step:1028/2160 train_time:43264ms step_avg:42.09ms
step:1029/2160 train_time:43325ms step_avg:42.10ms
step:1030/2160 train_time:43384ms step_avg:42.12ms
step:1031/2160 train_time:43445ms step_avg:42.14ms
step:1032/2160 train_time:43504ms step_avg:42.16ms
step:1033/2160 train_time:43564ms step_avg:42.17ms
step:1034/2160 train_time:43624ms step_avg:42.19ms
step:1035/2160 train_time:43686ms step_avg:42.21ms
step:1036/2160 train_time:43745ms step_avg:42.23ms
step:1037/2160 train_time:43806ms step_avg:42.24ms
step:1038/2160 train_time:43865ms step_avg:42.26ms
step:1039/2160 train_time:43926ms step_avg:42.28ms
step:1040/2160 train_time:43985ms step_avg:42.29ms
step:1041/2160 train_time:44046ms step_avg:42.31ms
step:1042/2160 train_time:44105ms step_avg:42.33ms
step:1043/2160 train_time:44165ms step_avg:42.34ms
step:1044/2160 train_time:44224ms step_avg:42.36ms
step:1045/2160 train_time:44285ms step_avg:42.38ms
step:1046/2160 train_time:44345ms step_avg:42.39ms
step:1047/2160 train_time:44405ms step_avg:42.41ms
step:1048/2160 train_time:44464ms step_avg:42.43ms
step:1049/2160 train_time:44525ms step_avg:42.45ms
step:1050/2160 train_time:44585ms step_avg:42.46ms
step:1051/2160 train_time:44645ms step_avg:42.48ms
step:1052/2160 train_time:44705ms step_avg:42.50ms
step:1053/2160 train_time:44766ms step_avg:42.51ms
step:1054/2160 train_time:44825ms step_avg:42.53ms
step:1055/2160 train_time:44886ms step_avg:42.55ms
step:1056/2160 train_time:44945ms step_avg:42.56ms
step:1057/2160 train_time:45005ms step_avg:42.58ms
step:1058/2160 train_time:45065ms step_avg:42.59ms
step:1059/2160 train_time:45125ms step_avg:42.61ms
step:1060/2160 train_time:45184ms step_avg:42.63ms
step:1061/2160 train_time:45244ms step_avg:42.64ms
step:1062/2160 train_time:45304ms step_avg:42.66ms
step:1063/2160 train_time:45364ms step_avg:42.68ms
step:1064/2160 train_time:45424ms step_avg:42.69ms
step:1065/2160 train_time:45485ms step_avg:42.71ms
step:1066/2160 train_time:45544ms step_avg:42.72ms
step:1067/2160 train_time:45604ms step_avg:42.74ms
step:1068/2160 train_time:45663ms step_avg:42.76ms
step:1069/2160 train_time:45724ms step_avg:42.77ms
step:1070/2160 train_time:45783ms step_avg:42.79ms
step:1071/2160 train_time:45844ms step_avg:42.81ms
step:1072/2160 train_time:45904ms step_avg:42.82ms
step:1073/2160 train_time:45965ms step_avg:42.84ms
step:1074/2160 train_time:46025ms step_avg:42.85ms
step:1075/2160 train_time:46085ms step_avg:42.87ms
step:1076/2160 train_time:46144ms step_avg:42.88ms
step:1077/2160 train_time:46205ms step_avg:42.90ms
step:1078/2160 train_time:46264ms step_avg:42.92ms
step:1079/2160 train_time:46324ms step_avg:42.93ms
step:1080/2160 train_time:46384ms step_avg:42.95ms
step:1081/2160 train_time:46444ms step_avg:42.96ms
step:1082/2160 train_time:46503ms step_avg:42.98ms
step:1083/2160 train_time:46564ms step_avg:43.00ms
step:1084/2160 train_time:46624ms step_avg:43.01ms
step:1085/2160 train_time:46684ms step_avg:43.03ms
step:1086/2160 train_time:46744ms step_avg:43.04ms
step:1087/2160 train_time:46805ms step_avg:43.06ms
step:1088/2160 train_time:46864ms step_avg:43.07ms
step:1089/2160 train_time:46925ms step_avg:43.09ms
step:1090/2160 train_time:46985ms step_avg:43.11ms
step:1091/2160 train_time:47045ms step_avg:43.12ms
step:1092/2160 train_time:47104ms step_avg:43.14ms
step:1093/2160 train_time:47165ms step_avg:43.15ms
step:1094/2160 train_time:47224ms step_avg:43.17ms
step:1095/2160 train_time:47284ms step_avg:43.18ms
step:1096/2160 train_time:47344ms step_avg:43.20ms
step:1097/2160 train_time:47405ms step_avg:43.21ms
step:1098/2160 train_time:47464ms step_avg:43.23ms
step:1099/2160 train_time:47524ms step_avg:43.24ms
step:1100/2160 train_time:47584ms step_avg:43.26ms
step:1101/2160 train_time:47644ms step_avg:43.27ms
step:1102/2160 train_time:47703ms step_avg:43.29ms
step:1103/2160 train_time:47764ms step_avg:43.30ms
step:1104/2160 train_time:47824ms step_avg:43.32ms
step:1105/2160 train_time:47885ms step_avg:43.33ms
step:1106/2160 train_time:47944ms step_avg:43.35ms
step:1107/2160 train_time:48005ms step_avg:43.36ms
step:1108/2160 train_time:48064ms step_avg:43.38ms
step:1109/2160 train_time:48125ms step_avg:43.39ms
step:1110/2160 train_time:48185ms step_avg:43.41ms
step:1111/2160 train_time:48245ms step_avg:43.43ms
step:1112/2160 train_time:48305ms step_avg:43.44ms
step:1113/2160 train_time:48365ms step_avg:43.46ms
step:1114/2160 train_time:48425ms step_avg:43.47ms
step:1115/2160 train_time:48485ms step_avg:43.48ms
step:1116/2160 train_time:48545ms step_avg:43.50ms
step:1117/2160 train_time:48605ms step_avg:43.51ms
step:1118/2160 train_time:48664ms step_avg:43.53ms
step:1119/2160 train_time:48725ms step_avg:43.54ms
step:1120/2160 train_time:48785ms step_avg:43.56ms
step:1121/2160 train_time:48845ms step_avg:43.57ms
step:1122/2160 train_time:48905ms step_avg:43.59ms
step:1123/2160 train_time:48966ms step_avg:43.60ms
step:1124/2160 train_time:49025ms step_avg:43.62ms
step:1125/2160 train_time:49086ms step_avg:43.63ms
step:1126/2160 train_time:49145ms step_avg:43.65ms
step:1127/2160 train_time:49206ms step_avg:43.66ms
step:1128/2160 train_time:49265ms step_avg:43.67ms
step:1129/2160 train_time:49325ms step_avg:43.69ms
step:1130/2160 train_time:49385ms step_avg:43.70ms
step:1131/2160 train_time:49444ms step_avg:43.72ms
step:1132/2160 train_time:49503ms step_avg:43.73ms
step:1133/2160 train_time:49564ms step_avg:43.75ms
step:1134/2160 train_time:49624ms step_avg:43.76ms
step:1135/2160 train_time:49684ms step_avg:43.77ms
step:1136/2160 train_time:49743ms step_avg:43.79ms
step:1137/2160 train_time:49805ms step_avg:43.80ms
step:1138/2160 train_time:49864ms step_avg:43.82ms
step:1139/2160 train_time:49925ms step_avg:43.83ms
step:1140/2160 train_time:49985ms step_avg:43.85ms
step:1141/2160 train_time:50045ms step_avg:43.86ms
step:1142/2160 train_time:50105ms step_avg:43.87ms
step:1143/2160 train_time:50165ms step_avg:43.89ms
step:1144/2160 train_time:50225ms step_avg:43.90ms
step:1145/2160 train_time:50285ms step_avg:43.92ms
step:1146/2160 train_time:50344ms step_avg:43.93ms
step:1147/2160 train_time:50405ms step_avg:43.94ms
step:1148/2160 train_time:50464ms step_avg:43.96ms
step:1149/2160 train_time:50524ms step_avg:43.97ms
step:1150/2160 train_time:50584ms step_avg:43.99ms
step:1151/2160 train_time:50644ms step_avg:44.00ms
step:1152/2160 train_time:50703ms step_avg:44.01ms
step:1153/2160 train_time:50764ms step_avg:44.03ms
step:1154/2160 train_time:50823ms step_avg:44.04ms
step:1155/2160 train_time:50884ms step_avg:44.06ms
step:1156/2160 train_time:50944ms step_avg:44.07ms
step:1157/2160 train_time:51004ms step_avg:44.08ms
step:1158/2160 train_time:51063ms step_avg:44.10ms
step:1159/2160 train_time:51124ms step_avg:44.11ms
step:1160/2160 train_time:51184ms step_avg:44.12ms
step:1161/2160 train_time:51244ms step_avg:44.14ms
step:1162/2160 train_time:51303ms step_avg:44.15ms
step:1163/2160 train_time:51364ms step_avg:44.17ms
step:1164/2160 train_time:51423ms step_avg:44.18ms
step:1165/2160 train_time:51483ms step_avg:44.19ms
step:1166/2160 train_time:51543ms step_avg:44.21ms
step:1167/2160 train_time:51604ms step_avg:44.22ms
step:1168/2160 train_time:51664ms step_avg:44.23ms
step:1169/2160 train_time:51725ms step_avg:44.25ms
step:1170/2160 train_time:51784ms step_avg:44.26ms
step:1171/2160 train_time:51845ms step_avg:44.27ms
step:1172/2160 train_time:51905ms step_avg:44.29ms
step:1173/2160 train_time:51965ms step_avg:44.30ms
step:1174/2160 train_time:52025ms step_avg:44.31ms
step:1175/2160 train_time:52086ms step_avg:44.33ms
step:1176/2160 train_time:52145ms step_avg:44.34ms
step:1177/2160 train_time:52206ms step_avg:44.36ms
step:1178/2160 train_time:52265ms step_avg:44.37ms
step:1179/2160 train_time:52326ms step_avg:44.38ms
step:1180/2160 train_time:52385ms step_avg:44.39ms
step:1181/2160 train_time:52446ms step_avg:44.41ms
step:1182/2160 train_time:52505ms step_avg:44.42ms
step:1183/2160 train_time:52565ms step_avg:44.43ms
step:1184/2160 train_time:52625ms step_avg:44.45ms
step:1185/2160 train_time:52685ms step_avg:44.46ms
step:1186/2160 train_time:52745ms step_avg:44.47ms
step:1187/2160 train_time:52806ms step_avg:44.49ms
step:1188/2160 train_time:52865ms step_avg:44.50ms
step:1189/2160 train_time:52926ms step_avg:44.51ms
step:1190/2160 train_time:52986ms step_avg:44.53ms
step:1191/2160 train_time:53047ms step_avg:44.54ms
step:1192/2160 train_time:53106ms step_avg:44.55ms
step:1193/2160 train_time:53167ms step_avg:44.57ms
step:1194/2160 train_time:53226ms step_avg:44.58ms
step:1195/2160 train_time:53287ms step_avg:44.59ms
step:1196/2160 train_time:53346ms step_avg:44.60ms
step:1197/2160 train_time:53407ms step_avg:44.62ms
step:1198/2160 train_time:53466ms step_avg:44.63ms
step:1199/2160 train_time:53527ms step_avg:44.64ms
step:1200/2160 train_time:53587ms step_avg:44.66ms
step:1201/2160 train_time:53648ms step_avg:44.67ms
step:1202/2160 train_time:53707ms step_avg:44.68ms
step:1203/2160 train_time:53768ms step_avg:44.70ms
step:1204/2160 train_time:53828ms step_avg:44.71ms
step:1205/2160 train_time:53889ms step_avg:44.72ms
step:1206/2160 train_time:53948ms step_avg:44.73ms
step:1207/2160 train_time:54010ms step_avg:44.75ms
step:1208/2160 train_time:54069ms step_avg:44.76ms
step:1209/2160 train_time:54130ms step_avg:44.77ms
step:1210/2160 train_time:54190ms step_avg:44.79ms
step:1211/2160 train_time:54251ms step_avg:44.80ms
step:1212/2160 train_time:54311ms step_avg:44.81ms
step:1213/2160 train_time:54372ms step_avg:44.82ms
step:1214/2160 train_time:54432ms step_avg:44.84ms
step:1215/2160 train_time:54494ms step_avg:44.85ms
step:1216/2160 train_time:54554ms step_avg:44.86ms
step:1217/2160 train_time:54615ms step_avg:44.88ms
step:1218/2160 train_time:54676ms step_avg:44.89ms
step:1219/2160 train_time:54737ms step_avg:44.90ms
step:1220/2160 train_time:54797ms step_avg:44.92ms
step:1221/2160 train_time:54859ms step_avg:44.93ms
step:1222/2160 train_time:54918ms step_avg:44.94ms
step:1223/2160 train_time:54979ms step_avg:44.95ms
step:1224/2160 train_time:55039ms step_avg:44.97ms
step:1225/2160 train_time:55100ms step_avg:44.98ms
step:1226/2160 train_time:55160ms step_avg:44.99ms
step:1227/2160 train_time:55222ms step_avg:45.01ms
step:1228/2160 train_time:55281ms step_avg:45.02ms
step:1229/2160 train_time:55342ms step_avg:45.03ms
step:1230/2160 train_time:55402ms step_avg:45.04ms
step:1231/2160 train_time:55463ms step_avg:45.05ms
step:1232/2160 train_time:55523ms step_avg:45.07ms
step:1233/2160 train_time:55583ms step_avg:45.08ms
step:1234/2160 train_time:55643ms step_avg:45.09ms
step:1235/2160 train_time:55704ms step_avg:45.10ms
step:1236/2160 train_time:55763ms step_avg:45.12ms
step:1237/2160 train_time:55824ms step_avg:45.13ms
step:1238/2160 train_time:55884ms step_avg:45.14ms
step:1239/2160 train_time:55944ms step_avg:45.15ms
step:1240/2160 train_time:56003ms step_avg:45.16ms
step:1241/2160 train_time:56064ms step_avg:45.18ms
step:1242/2160 train_time:56124ms step_avg:45.19ms
step:1243/2160 train_time:56185ms step_avg:45.20ms
step:1244/2160 train_time:56244ms step_avg:45.21ms
step:1245/2160 train_time:56305ms step_avg:45.23ms
step:1246/2160 train_time:56365ms step_avg:45.24ms
step:1247/2160 train_time:56425ms step_avg:45.25ms
step:1248/2160 train_time:56486ms step_avg:45.26ms
step:1249/2160 train_time:56546ms step_avg:45.27ms
step:1250/2160 train_time:56606ms step_avg:45.28ms
step:1250/2160 val_loss:3.5691 train_time:56666ms step_avg:45.33ms
step:1251/2160 train_time:56689ms step_avg:45.31ms
step:1252/2160 train_time:56728ms step_avg:45.31ms
step:1253/2160 train_time:56791ms step_avg:45.32ms
step:1254/2160 train_time:56856ms step_avg:45.34ms
step:1255/2160 train_time:56917ms step_avg:45.35ms
step:1256/2160 train_time:56976ms step_avg:45.36ms
step:1257/2160 train_time:57037ms step_avg:45.38ms
step:1258/2160 train_time:57096ms step_avg:45.39ms
step:1259/2160 train_time:57157ms step_avg:45.40ms
step:1260/2160 train_time:57215ms step_avg:45.41ms
step:1261/2160 train_time:57276ms step_avg:45.42ms
step:1262/2160 train_time:57335ms step_avg:45.43ms
step:1263/2160 train_time:57395ms step_avg:45.44ms
step:1264/2160 train_time:57455ms step_avg:45.45ms
step:1265/2160 train_time:57517ms step_avg:45.47ms
step:1266/2160 train_time:57577ms step_avg:45.48ms
step:1267/2160 train_time:57642ms step_avg:45.49ms
step:1268/2160 train_time:57702ms step_avg:45.51ms
step:1269/2160 train_time:57765ms step_avg:45.52ms
step:1270/2160 train_time:57825ms step_avg:45.53ms
step:1271/2160 train_time:57886ms step_avg:45.54ms
step:1272/2160 train_time:57945ms step_avg:45.55ms
step:1273/2160 train_time:58006ms step_avg:45.57ms
step:1274/2160 train_time:58066ms step_avg:45.58ms
step:1275/2160 train_time:58126ms step_avg:45.59ms
step:1276/2160 train_time:58185ms step_avg:45.60ms
step:1277/2160 train_time:58246ms step_avg:45.61ms
step:1278/2160 train_time:58305ms step_avg:45.62ms
step:1279/2160 train_time:58366ms step_avg:45.63ms
step:1280/2160 train_time:58425ms step_avg:45.64ms
step:1281/2160 train_time:58486ms step_avg:45.66ms
step:1282/2160 train_time:58546ms step_avg:45.67ms
step:1283/2160 train_time:58606ms step_avg:45.68ms
step:1284/2160 train_time:58666ms step_avg:45.69ms
step:1285/2160 train_time:58727ms step_avg:45.70ms
step:1286/2160 train_time:58786ms step_avg:45.71ms
step:1287/2160 train_time:58847ms step_avg:45.72ms
step:1288/2160 train_time:58906ms step_avg:45.73ms
step:1289/2160 train_time:58967ms step_avg:45.75ms
step:1290/2160 train_time:59026ms step_avg:45.76ms
step:1291/2160 train_time:59087ms step_avg:45.77ms
step:1292/2160 train_time:59146ms step_avg:45.78ms
step:1293/2160 train_time:59206ms step_avg:45.79ms
step:1294/2160 train_time:59266ms step_avg:45.80ms
step:1295/2160 train_time:59326ms step_avg:45.81ms
step:1296/2160 train_time:59385ms step_avg:45.82ms
step:1297/2160 train_time:59446ms step_avg:45.83ms
step:1298/2160 train_time:59505ms step_avg:45.84ms
step:1299/2160 train_time:59565ms step_avg:45.85ms
step:1300/2160 train_time:59626ms step_avg:45.87ms
step:1301/2160 train_time:59686ms step_avg:45.88ms
step:1302/2160 train_time:59746ms step_avg:45.89ms
step:1303/2160 train_time:59807ms step_avg:45.90ms
step:1304/2160 train_time:59866ms step_avg:45.91ms
step:1305/2160 train_time:59927ms step_avg:45.92ms
step:1306/2160 train_time:59986ms step_avg:45.93ms
step:1307/2160 train_time:60047ms step_avg:45.94ms
step:1308/2160 train_time:60106ms step_avg:45.95ms
step:1309/2160 train_time:60167ms step_avg:45.96ms
step:1310/2160 train_time:60227ms step_avg:45.97ms
step:1311/2160 train_time:60287ms step_avg:45.99ms
step:1312/2160 train_time:60347ms step_avg:46.00ms
step:1313/2160 train_time:60408ms step_avg:46.01ms
step:1314/2160 train_time:60468ms step_avg:46.02ms
step:1315/2160 train_time:60529ms step_avg:46.03ms
step:1316/2160 train_time:60588ms step_avg:46.04ms
step:1317/2160 train_time:60649ms step_avg:46.05ms
step:1318/2160 train_time:60707ms step_avg:46.06ms
step:1319/2160 train_time:60768ms step_avg:46.07ms
step:1320/2160 train_time:60827ms step_avg:46.08ms
step:1321/2160 train_time:60888ms step_avg:46.09ms
step:1322/2160 train_time:60947ms step_avg:46.10ms
step:1323/2160 train_time:61008ms step_avg:46.11ms
step:1324/2160 train_time:61067ms step_avg:46.12ms
step:1325/2160 train_time:61128ms step_avg:46.13ms
step:1326/2160 train_time:61187ms step_avg:46.14ms
step:1327/2160 train_time:61248ms step_avg:46.16ms
step:1328/2160 train_time:61307ms step_avg:46.16ms
step:1329/2160 train_time:61368ms step_avg:46.18ms
step:1330/2160 train_time:61428ms step_avg:46.19ms
step:1331/2160 train_time:61488ms step_avg:46.20ms
step:1332/2160 train_time:61547ms step_avg:46.21ms
step:1333/2160 train_time:61608ms step_avg:46.22ms
step:1334/2160 train_time:61667ms step_avg:46.23ms
step:1335/2160 train_time:61728ms step_avg:46.24ms
step:1336/2160 train_time:61787ms step_avg:46.25ms
step:1337/2160 train_time:61848ms step_avg:46.26ms
step:1338/2160 train_time:61907ms step_avg:46.27ms
step:1339/2160 train_time:61968ms step_avg:46.28ms
step:1340/2160 train_time:62027ms step_avg:46.29ms
step:1341/2160 train_time:62088ms step_avg:46.30ms
step:1342/2160 train_time:62147ms step_avg:46.31ms
step:1343/2160 train_time:62207ms step_avg:46.32ms
step:1344/2160 train_time:62266ms step_avg:46.33ms
step:1345/2160 train_time:62327ms step_avg:46.34ms
step:1346/2160 train_time:62387ms step_avg:46.35ms
step:1347/2160 train_time:62447ms step_avg:46.36ms
step:1348/2160 train_time:62507ms step_avg:46.37ms
step:1349/2160 train_time:62568ms step_avg:46.38ms
step:1350/2160 train_time:62627ms step_avg:46.39ms
step:1351/2160 train_time:62687ms step_avg:46.40ms
step:1352/2160 train_time:62746ms step_avg:46.41ms
step:1353/2160 train_time:62807ms step_avg:46.42ms
step:1354/2160 train_time:62866ms step_avg:46.43ms
step:1355/2160 train_time:62927ms step_avg:46.44ms
step:1356/2160 train_time:62986ms step_avg:46.45ms
step:1357/2160 train_time:63047ms step_avg:46.46ms
step:1358/2160 train_time:63106ms step_avg:46.47ms
step:1359/2160 train_time:63166ms step_avg:46.48ms
step:1360/2160 train_time:63226ms step_avg:46.49ms
step:1361/2160 train_time:63286ms step_avg:46.50ms
step:1362/2160 train_time:63346ms step_avg:46.51ms
step:1363/2160 train_time:63407ms step_avg:46.52ms
step:1364/2160 train_time:63466ms step_avg:46.53ms
step:1365/2160 train_time:63527ms step_avg:46.54ms
step:1366/2160 train_time:63587ms step_avg:46.55ms
step:1367/2160 train_time:63647ms step_avg:46.56ms
step:1368/2160 train_time:63706ms step_avg:46.57ms
step:1369/2160 train_time:63767ms step_avg:46.58ms
step:1370/2160 train_time:63827ms step_avg:46.59ms
step:1371/2160 train_time:63887ms step_avg:46.60ms
step:1372/2160 train_time:63947ms step_avg:46.61ms
step:1373/2160 train_time:64007ms step_avg:46.62ms
step:1374/2160 train_time:64066ms step_avg:46.63ms
step:1375/2160 train_time:64127ms step_avg:46.64ms
step:1376/2160 train_time:64186ms step_avg:46.65ms
step:1377/2160 train_time:64247ms step_avg:46.66ms
step:1378/2160 train_time:64306ms step_avg:46.67ms
step:1379/2160 train_time:64367ms step_avg:46.68ms
step:1380/2160 train_time:64427ms step_avg:46.69ms
step:1381/2160 train_time:64488ms step_avg:46.70ms
step:1382/2160 train_time:64548ms step_avg:46.71ms
step:1383/2160 train_time:64609ms step_avg:46.72ms
step:1384/2160 train_time:64668ms step_avg:46.73ms
step:1385/2160 train_time:64729ms step_avg:46.74ms
step:1386/2160 train_time:64788ms step_avg:46.74ms
step:1387/2160 train_time:64848ms step_avg:46.75ms
step:1388/2160 train_time:64908ms step_avg:46.76ms
step:1389/2160 train_time:64969ms step_avg:46.77ms
step:1390/2160 train_time:65028ms step_avg:46.78ms
step:1391/2160 train_time:65089ms step_avg:46.79ms
step:1392/2160 train_time:65148ms step_avg:46.80ms
step:1393/2160 train_time:65209ms step_avg:46.81ms
step:1394/2160 train_time:65268ms step_avg:46.82ms
step:1395/2160 train_time:65330ms step_avg:46.83ms
step:1396/2160 train_time:65389ms step_avg:46.84ms
step:1397/2160 train_time:65450ms step_avg:46.85ms
step:1398/2160 train_time:65509ms step_avg:46.86ms
step:1399/2160 train_time:65570ms step_avg:46.87ms
step:1400/2160 train_time:65629ms step_avg:46.88ms
step:1401/2160 train_time:65691ms step_avg:46.89ms
step:1402/2160 train_time:65751ms step_avg:46.90ms
step:1403/2160 train_time:65812ms step_avg:46.91ms
step:1404/2160 train_time:65871ms step_avg:46.92ms
step:1405/2160 train_time:65932ms step_avg:46.93ms
step:1406/2160 train_time:65992ms step_avg:46.94ms
step:1407/2160 train_time:66054ms step_avg:46.95ms
step:1408/2160 train_time:66113ms step_avg:46.96ms
step:1409/2160 train_time:66175ms step_avg:46.97ms
step:1410/2160 train_time:66235ms step_avg:46.97ms
step:1411/2160 train_time:66296ms step_avg:46.99ms
step:1412/2160 train_time:66356ms step_avg:46.99ms
step:1413/2160 train_time:66417ms step_avg:47.00ms
step:1414/2160 train_time:66477ms step_avg:47.01ms
step:1415/2160 train_time:66539ms step_avg:47.02ms
step:1416/2160 train_time:66626ms step_avg:47.05ms
step:1417/2160 train_time:66715ms step_avg:47.08ms
step:1418/2160 train_time:66801ms step_avg:47.11ms
step:1419/2160 train_time:66891ms step_avg:47.14ms
step:1420/2160 train_time:66978ms step_avg:47.17ms
step:1421/2160 train_time:67067ms step_avg:47.20ms
step:1422/2160 train_time:67154ms step_avg:47.23ms
step:1423/2160 train_time:67243ms step_avg:47.25ms
step:1424/2160 train_time:67330ms step_avg:47.28ms
step:1425/2160 train_time:67419ms step_avg:47.31ms
step:1426/2160 train_time:67505ms step_avg:47.34ms
step:1427/2160 train_time:67595ms step_avg:47.37ms
step:1428/2160 train_time:67682ms step_avg:47.40ms
step:1429/2160 train_time:67771ms step_avg:47.43ms
step:1430/2160 train_time:67858ms step_avg:47.45ms
step:1431/2160 train_time:67947ms step_avg:47.48ms
step:1432/2160 train_time:68033ms step_avg:47.51ms
step:1433/2160 train_time:68122ms step_avg:47.54ms
step:1434/2160 train_time:68209ms step_avg:47.57ms
step:1435/2160 train_time:68298ms step_avg:47.59ms
step:1436/2160 train_time:68385ms step_avg:47.62ms
step:1437/2160 train_time:68474ms step_avg:47.65ms
step:1438/2160 train_time:68561ms step_avg:47.68ms
step:1439/2160 train_time:68649ms step_avg:47.71ms
step:1440/2160 train_time:68736ms step_avg:47.73ms
step:1441/2160 train_time:68824ms step_avg:47.76ms
step:1442/2160 train_time:68911ms step_avg:47.79ms
step:1443/2160 train_time:69000ms step_avg:47.82ms
step:1444/2160 train_time:69087ms step_avg:47.84ms
step:1445/2160 train_time:69176ms step_avg:47.87ms
step:1446/2160 train_time:69263ms step_avg:47.90ms
step:1447/2160 train_time:69353ms step_avg:47.93ms
step:1448/2160 train_time:69440ms step_avg:47.96ms
step:1449/2160 train_time:69529ms step_avg:47.98ms
step:1450/2160 train_time:69616ms step_avg:48.01ms
step:1451/2160 train_time:69705ms step_avg:48.04ms
step:1452/2160 train_time:69792ms step_avg:48.07ms
step:1453/2160 train_time:69881ms step_avg:48.09ms
step:1454/2160 train_time:69968ms step_avg:48.12ms
step:1455/2160 train_time:70056ms step_avg:48.15ms
step:1456/2160 train_time:70144ms step_avg:48.18ms
step:1457/2160 train_time:70232ms step_avg:48.20ms
step:1458/2160 train_time:70320ms step_avg:48.23ms
step:1459/2160 train_time:70410ms step_avg:48.26ms
step:1460/2160 train_time:70497ms step_avg:48.29ms
step:1461/2160 train_time:70586ms step_avg:48.31ms
step:1462/2160 train_time:70673ms step_avg:48.34ms
step:1463/2160 train_time:70762ms step_avg:48.37ms
step:1464/2160 train_time:70850ms step_avg:48.39ms
step:1465/2160 train_time:70938ms step_avg:48.42ms
step:1466/2160 train_time:71025ms step_avg:48.45ms
step:1467/2160 train_time:71114ms step_avg:48.48ms
step:1468/2160 train_time:71200ms step_avg:48.50ms
step:1469/2160 train_time:71290ms step_avg:48.53ms
step:1470/2160 train_time:71376ms step_avg:48.56ms
step:1471/2160 train_time:71466ms step_avg:48.58ms
step:1472/2160 train_time:71553ms step_avg:48.61ms
step:1473/2160 train_time:71641ms step_avg:48.64ms
step:1474/2160 train_time:71729ms step_avg:48.66ms
step:1475/2160 train_time:71817ms step_avg:48.69ms
step:1476/2160 train_time:71905ms step_avg:48.72ms
step:1477/2160 train_time:71994ms step_avg:48.74ms
step:1478/2160 train_time:72080ms step_avg:48.77ms
step:1479/2160 train_time:72169ms step_avg:48.80ms
step:1480/2160 train_time:72257ms step_avg:48.82ms
step:1481/2160 train_time:72345ms step_avg:48.85ms
step:1482/2160 train_time:72432ms step_avg:48.87ms
step:1483/2160 train_time:72521ms step_avg:48.90ms
step:1484/2160 train_time:72607ms step_avg:48.93ms
step:1485/2160 train_time:72696ms step_avg:48.95ms
step:1486/2160 train_time:72783ms step_avg:48.98ms
step:1487/2160 train_time:72873ms step_avg:49.01ms
step:1488/2160 train_time:72960ms step_avg:49.03ms
step:1489/2160 train_time:73050ms step_avg:49.06ms
step:1490/2160 train_time:73137ms step_avg:49.08ms
step:1491/2160 train_time:73225ms step_avg:49.11ms
step:1492/2160 train_time:73313ms step_avg:49.14ms
step:1493/2160 train_time:73401ms step_avg:49.16ms
step:1494/2160 train_time:73489ms step_avg:49.19ms
step:1495/2160 train_time:73577ms step_avg:49.22ms
step:1496/2160 train_time:73664ms step_avg:49.24ms
step:1497/2160 train_time:73754ms step_avg:49.27ms
step:1498/2160 train_time:73841ms step_avg:49.29ms
step:1499/2160 train_time:73930ms step_avg:49.32ms
step:1500/2160 train_time:74017ms step_avg:49.34ms
step:1500/2160 val_loss:3.4675 train_time:74106ms step_avg:49.40ms
step:1501/2160 train_time:74130ms step_avg:49.39ms
step:1502/2160 train_time:74196ms step_avg:49.40ms
step:1503/2160 train_time:74287ms step_avg:49.43ms
step:1504/2160 train_time:74378ms step_avg:49.45ms
step:1505/2160 train_time:74466ms step_avg:49.48ms
step:1506/2160 train_time:74554ms step_avg:49.50ms
step:1507/2160 train_time:74642ms step_avg:49.53ms
step:1508/2160 train_time:74727ms step_avg:49.55ms
step:1509/2160 train_time:74815ms step_avg:49.58ms
step:1510/2160 train_time:74901ms step_avg:49.60ms
step:1511/2160 train_time:74989ms step_avg:49.63ms
step:1512/2160 train_time:75080ms step_avg:49.66ms
step:1513/2160 train_time:75171ms step_avg:49.68ms
step:1514/2160 train_time:75258ms step_avg:49.71ms
step:1515/2160 train_time:75347ms step_avg:49.73ms
step:1516/2160 train_time:75434ms step_avg:49.76ms
step:1517/2160 train_time:75523ms step_avg:49.78ms
step:1518/2160 train_time:75610ms step_avg:49.81ms
step:1519/2160 train_time:75697ms step_avg:49.83ms
step:1520/2160 train_time:75784ms step_avg:49.86ms
step:1521/2160 train_time:75872ms step_avg:49.88ms
step:1522/2160 train_time:75958ms step_avg:49.91ms
step:1523/2160 train_time:76047ms step_avg:49.93ms
step:1524/2160 train_time:76136ms step_avg:49.96ms
step:1525/2160 train_time:76225ms step_avg:49.98ms
step:1526/2160 train_time:76313ms step_avg:50.01ms
step:1527/2160 train_time:76401ms step_avg:50.03ms
step:1528/2160 train_time:76488ms step_avg:50.06ms
step:1529/2160 train_time:76577ms step_avg:50.08ms
step:1530/2160 train_time:76663ms step_avg:50.11ms
step:1531/2160 train_time:76752ms step_avg:50.13ms
step:1532/2160 train_time:76838ms step_avg:50.16ms
step:1533/2160 train_time:76926ms step_avg:50.18ms
step:1534/2160 train_time:77013ms step_avg:50.20ms
step:1535/2160 train_time:77102ms step_avg:50.23ms
step:1536/2160 train_time:77190ms step_avg:50.25ms
step:1537/2160 train_time:77280ms step_avg:50.28ms
step:1538/2160 train_time:77367ms step_avg:50.30ms
step:1539/2160 train_time:77456ms step_avg:50.33ms
step:1540/2160 train_time:77543ms step_avg:50.35ms
step:1541/2160 train_time:77632ms step_avg:50.38ms
step:1542/2160 train_time:77719ms step_avg:50.40ms
step:1543/2160 train_time:77808ms step_avg:50.43ms
step:1544/2160 train_time:77895ms step_avg:50.45ms
step:1545/2160 train_time:77983ms step_avg:50.47ms
step:1546/2160 train_time:78071ms step_avg:50.50ms
step:1547/2160 train_time:78160ms step_avg:50.52ms
step:1548/2160 train_time:78248ms step_avg:50.55ms
step:1549/2160 train_time:78338ms step_avg:50.57ms
step:1550/2160 train_time:78425ms step_avg:50.60ms
step:1551/2160 train_time:78514ms step_avg:50.62ms
step:1552/2160 train_time:78600ms step_avg:50.64ms
step:1553/2160 train_time:78689ms step_avg:50.67ms
step:1554/2160 train_time:78776ms step_avg:50.69ms
step:1555/2160 train_time:78864ms step_avg:50.72ms
step:1556/2160 train_time:78951ms step_avg:50.74ms
step:1557/2160 train_time:79040ms step_avg:50.76ms
step:1558/2160 train_time:79127ms step_avg:50.79ms
step:1559/2160 train_time:79217ms step_avg:50.81ms
step:1560/2160 train_time:79304ms step_avg:50.84ms
step:1561/2160 train_time:79394ms step_avg:50.86ms
step:1562/2160 train_time:79481ms step_avg:50.88ms
step:1563/2160 train_time:79569ms step_avg:50.91ms
step:1564/2160 train_time:79656ms step_avg:50.93ms
step:1565/2160 train_time:79744ms step_avg:50.95ms
step:1566/2160 train_time:79832ms step_avg:50.98ms
step:1567/2160 train_time:79919ms step_avg:51.00ms
step:1568/2160 train_time:80006ms step_avg:51.02ms
step:1569/2160 train_time:80097ms step_avg:51.05ms
step:1570/2160 train_time:80184ms step_avg:51.07ms
step:1571/2160 train_time:80273ms step_avg:51.10ms
step:1572/2160 train_time:80360ms step_avg:51.12ms
step:1573/2160 train_time:80449ms step_avg:51.14ms
step:1574/2160 train_time:80536ms step_avg:51.17ms
step:1575/2160 train_time:80624ms step_avg:51.19ms
step:1576/2160 train_time:80712ms step_avg:51.21ms
step:1577/2160 train_time:80800ms step_avg:51.24ms
step:1578/2160 train_time:80887ms step_avg:51.26ms
step:1579/2160 train_time:80975ms step_avg:51.28ms
step:1580/2160 train_time:81063ms step_avg:51.31ms
step:1581/2160 train_time:81152ms step_avg:51.33ms
step:1582/2160 train_time:81238ms step_avg:51.35ms
step:1583/2160 train_time:81327ms step_avg:51.38ms
step:1584/2160 train_time:81414ms step_avg:51.40ms
step:1585/2160 train_time:81503ms step_avg:51.42ms
step:1586/2160 train_time:81590ms step_avg:51.44ms
step:1587/2160 train_time:81679ms step_avg:51.47ms
step:1588/2160 train_time:81767ms step_avg:51.49ms
step:1589/2160 train_time:81857ms step_avg:51.51ms
step:1590/2160 train_time:81944ms step_avg:51.54ms
step:1591/2160 train_time:82033ms step_avg:51.56ms
step:1592/2160 train_time:82119ms step_avg:51.58ms
step:1593/2160 train_time:82207ms step_avg:51.61ms
step:1594/2160 train_time:82295ms step_avg:51.63ms
step:1595/2160 train_time:82383ms step_avg:51.65ms
step:1596/2160 train_time:82470ms step_avg:51.67ms
step:1597/2160 train_time:82560ms step_avg:51.70ms
step:1598/2160 train_time:82646ms step_avg:51.72ms
step:1599/2160 train_time:82735ms step_avg:51.74ms
step:1600/2160 train_time:82822ms step_avg:51.76ms
step:1601/2160 train_time:82910ms step_avg:51.79ms
step:1602/2160 train_time:82998ms step_avg:51.81ms
step:1603/2160 train_time:83086ms step_avg:51.83ms
step:1604/2160 train_time:83174ms step_avg:51.85ms
step:1605/2160 train_time:83262ms step_avg:51.88ms
step:1606/2160 train_time:83349ms step_avg:51.90ms
step:1607/2160 train_time:83439ms step_avg:51.92ms
step:1608/2160 train_time:83526ms step_avg:51.94ms
step:1609/2160 train_time:83615ms step_avg:51.97ms
step:1610/2160 train_time:83703ms step_avg:51.99ms
step:1611/2160 train_time:83793ms step_avg:52.01ms
step:1612/2160 train_time:83879ms step_avg:52.03ms
step:1613/2160 train_time:83968ms step_avg:52.06ms
step:1614/2160 train_time:84055ms step_avg:52.08ms
step:1615/2160 train_time:84143ms step_avg:52.10ms
step:1616/2160 train_time:84231ms step_avg:52.12ms
step:1617/2160 train_time:84319ms step_avg:52.15ms
step:1618/2160 train_time:84406ms step_avg:52.17ms
step:1619/2160 train_time:84495ms step_avg:52.19ms
step:1620/2160 train_time:84583ms step_avg:52.21ms
step:1621/2160 train_time:84671ms step_avg:52.23ms
step:1622/2160 train_time:84758ms step_avg:52.26ms
step:1623/2160 train_time:84846ms step_avg:52.28ms
step:1624/2160 train_time:84935ms step_avg:52.30ms
step:1625/2160 train_time:85023ms step_avg:52.32ms
step:1626/2160 train_time:85111ms step_avg:52.34ms
step:1627/2160 train_time:85199ms step_avg:52.37ms
step:1628/2160 train_time:85286ms step_avg:52.39ms
step:1629/2160 train_time:85375ms step_avg:52.41ms
step:1630/2160 train_time:85463ms step_avg:52.43ms
step:1631/2160 train_time:85552ms step_avg:52.45ms
step:1632/2160 train_time:85639ms step_avg:52.47ms
step:1633/2160 train_time:85728ms step_avg:52.50ms
step:1634/2160 train_time:85815ms step_avg:52.52ms
step:1635/2160 train_time:85904ms step_avg:52.54ms
step:1636/2160 train_time:85991ms step_avg:52.56ms
step:1637/2160 train_time:86080ms step_avg:52.58ms
step:1638/2160 train_time:86167ms step_avg:52.60ms
step:1639/2160 train_time:86256ms step_avg:52.63ms
step:1640/2160 train_time:86343ms step_avg:52.65ms
step:1641/2160 train_time:86433ms step_avg:52.67ms
step:1642/2160 train_time:86519ms step_avg:52.69ms
step:1643/2160 train_time:86608ms step_avg:52.71ms
step:1644/2160 train_time:86695ms step_avg:52.73ms
step:1645/2160 train_time:86783ms step_avg:52.76ms
step:1646/2160 train_time:86870ms step_avg:52.78ms
step:1647/2160 train_time:86959ms step_avg:52.80ms
step:1648/2160 train_time:87046ms step_avg:52.82ms
step:1649/2160 train_time:87137ms step_avg:52.84ms
step:1650/2160 train_time:87223ms step_avg:52.86ms
step:1651/2160 train_time:87312ms step_avg:52.88ms
step:1652/2160 train_time:87398ms step_avg:52.90ms
step:1653/2160 train_time:87487ms step_avg:52.93ms
step:1654/2160 train_time:87574ms step_avg:52.95ms
step:1655/2160 train_time:87662ms step_avg:52.97ms
step:1656/2160 train_time:87749ms step_avg:52.99ms
step:1657/2160 train_time:87837ms step_avg:53.01ms
step:1658/2160 train_time:87924ms step_avg:53.03ms
step:1659/2160 train_time:88012ms step_avg:53.05ms
step:1660/2160 train_time:88100ms step_avg:53.07ms
step:1661/2160 train_time:88188ms step_avg:53.09ms
step:1662/2160 train_time:88277ms step_avg:53.11ms
step:1663/2160 train_time:88366ms step_avg:53.14ms
step:1664/2160 train_time:88454ms step_avg:53.16ms
step:1665/2160 train_time:88542ms step_avg:53.18ms
step:1666/2160 train_time:88629ms step_avg:53.20ms
step:1667/2160 train_time:88718ms step_avg:53.22ms
step:1668/2160 train_time:88805ms step_avg:53.24ms
step:1669/2160 train_time:88895ms step_avg:53.26ms
step:1670/2160 train_time:88981ms step_avg:53.28ms
step:1671/2160 train_time:89071ms step_avg:53.30ms
step:1672/2160 train_time:89157ms step_avg:53.32ms
step:1673/2160 train_time:89246ms step_avg:53.34ms
step:1674/2160 train_time:89333ms step_avg:53.37ms
step:1675/2160 train_time:89421ms step_avg:53.39ms
step:1676/2160 train_time:89509ms step_avg:53.41ms
step:1677/2160 train_time:89598ms step_avg:53.43ms
step:1678/2160 train_time:89684ms step_avg:53.45ms
step:1679/2160 train_time:89774ms step_avg:53.47ms
step:1680/2160 train_time:89860ms step_avg:53.49ms
step:1681/2160 train_time:89949ms step_avg:53.51ms
step:1682/2160 train_time:90036ms step_avg:53.53ms
step:1683/2160 train_time:90125ms step_avg:53.55ms
step:1684/2160 train_time:90213ms step_avg:53.57ms
step:1685/2160 train_time:90301ms step_avg:53.59ms
step:1686/2160 train_time:90388ms step_avg:53.61ms
step:1687/2160 train_time:90477ms step_avg:53.63ms
step:1688/2160 train_time:90564ms step_avg:53.65ms
step:1689/2160 train_time:90653ms step_avg:53.67ms
step:1690/2160 train_time:90739ms step_avg:53.69ms
step:1691/2160 train_time:90828ms step_avg:53.71ms
step:1692/2160 train_time:90916ms step_avg:53.73ms
step:1693/2160 train_time:91004ms step_avg:53.75ms
step:1694/2160 train_time:91092ms step_avg:53.77ms
step:1695/2160 train_time:91181ms step_avg:53.79ms
step:1696/2160 train_time:91267ms step_avg:53.81ms
step:1697/2160 train_time:91356ms step_avg:53.83ms
step:1698/2160 train_time:91443ms step_avg:53.85ms
step:1699/2160 train_time:91532ms step_avg:53.87ms
step:1700/2160 train_time:91619ms step_avg:53.89ms
step:1701/2160 train_time:91708ms step_avg:53.91ms
step:1702/2160 train_time:91795ms step_avg:53.93ms
step:1703/2160 train_time:91884ms step_avg:53.95ms
step:1704/2160 train_time:91971ms step_avg:53.97ms
step:1705/2160 train_time:92060ms step_avg:53.99ms
step:1706/2160 train_time:92148ms step_avg:54.01ms
step:1707/2160 train_time:92237ms step_avg:54.03ms
step:1708/2160 train_time:92324ms step_avg:54.05ms
step:1709/2160 train_time:92413ms step_avg:54.07ms
step:1710/2160 train_time:92500ms step_avg:54.09ms
step:1711/2160 train_time:92589ms step_avg:54.11ms
step:1712/2160 train_time:92677ms step_avg:54.13ms
step:1713/2160 train_time:92766ms step_avg:54.15ms
step:1714/2160 train_time:92853ms step_avg:54.17ms
step:1715/2160 train_time:92942ms step_avg:54.19ms
step:1716/2160 train_time:93030ms step_avg:54.21ms
step:1717/2160 train_time:93119ms step_avg:54.23ms
step:1718/2160 train_time:93207ms step_avg:54.25ms
step:1719/2160 train_time:93295ms step_avg:54.27ms
step:1720/2160 train_time:93382ms step_avg:54.29ms
step:1721/2160 train_time:93471ms step_avg:54.31ms
step:1722/2160 train_time:93558ms step_avg:54.33ms
step:1723/2160 train_time:93648ms step_avg:54.35ms
step:1724/2160 train_time:93735ms step_avg:54.37ms
step:1725/2160 train_time:93823ms step_avg:54.39ms
step:1726/2160 train_time:93911ms step_avg:54.41ms
step:1727/2160 train_time:93999ms step_avg:54.43ms
step:1728/2160 train_time:94086ms step_avg:54.45ms
step:1729/2160 train_time:94176ms step_avg:54.47ms
step:1730/2160 train_time:94262ms step_avg:54.49ms
step:1731/2160 train_time:94351ms step_avg:54.51ms
step:1732/2160 train_time:94438ms step_avg:54.53ms
step:1733/2160 train_time:94527ms step_avg:54.55ms
step:1734/2160 train_time:94614ms step_avg:54.56ms
step:1735/2160 train_time:94703ms step_avg:54.58ms
step:1736/2160 train_time:94790ms step_avg:54.60ms
step:1737/2160 train_time:94878ms step_avg:54.62ms
step:1738/2160 train_time:94966ms step_avg:54.64ms
step:1739/2160 train_time:95055ms step_avg:54.66ms
step:1740/2160 train_time:95142ms step_avg:54.68ms
step:1741/2160 train_time:95231ms step_avg:54.70ms
step:1742/2160 train_time:95318ms step_avg:54.72ms
step:1743/2160 train_time:95406ms step_avg:54.74ms
step:1744/2160 train_time:95493ms step_avg:54.76ms
step:1745/2160 train_time:95582ms step_avg:54.77ms
step:1746/2160 train_time:95669ms step_avg:54.79ms
step:1747/2160 train_time:95758ms step_avg:54.81ms
step:1748/2160 train_time:95845ms step_avg:54.83ms
step:1749/2160 train_time:95934ms step_avg:54.85ms
step:1750/2160 train_time:96020ms step_avg:54.87ms
step:1750/2160 val_loss:3.3779 train_time:96110ms step_avg:54.92ms
step:1751/2160 train_time:96134ms step_avg:54.90ms
step:1752/2160 train_time:96202ms step_avg:54.91ms
step:1753/2160 train_time:96297ms step_avg:54.93ms
step:1754/2160 train_time:96386ms step_avg:54.95ms
step:1755/2160 train_time:96474ms step_avg:54.97ms
step:1756/2160 train_time:96561ms step_avg:54.99ms
step:1757/2160 train_time:96649ms step_avg:55.01ms
step:1758/2160 train_time:96735ms step_avg:55.03ms
step:1759/2160 train_time:96822ms step_avg:55.04ms
step:1760/2160 train_time:96908ms step_avg:55.06ms
step:1761/2160 train_time:96997ms step_avg:55.08ms
step:1762/2160 train_time:97084ms step_avg:55.10ms
step:1763/2160 train_time:97174ms step_avg:55.12ms
step:1764/2160 train_time:97264ms step_avg:55.14ms
step:1765/2160 train_time:97354ms step_avg:55.16ms
step:1766/2160 train_time:97441ms step_avg:55.18ms
step:1767/2160 train_time:97530ms step_avg:55.20ms
step:1768/2160 train_time:97617ms step_avg:55.21ms
step:1769/2160 train_time:97704ms step_avg:55.23ms
step:1770/2160 train_time:97790ms step_avg:55.25ms
step:1771/2160 train_time:97878ms step_avg:55.27ms
step:1772/2160 train_time:97966ms step_avg:55.29ms
step:1773/2160 train_time:98054ms step_avg:55.30ms
step:1774/2160 train_time:98142ms step_avg:55.32ms
step:1775/2160 train_time:98233ms step_avg:55.34ms
step:1776/2160 train_time:98322ms step_avg:55.36ms
step:1777/2160 train_time:98411ms step_avg:55.38ms
step:1778/2160 train_time:98498ms step_avg:55.40ms
step:1779/2160 train_time:98587ms step_avg:55.42ms
step:1780/2160 train_time:98673ms step_avg:55.43ms
step:1781/2160 train_time:98761ms step_avg:55.45ms
step:1782/2160 train_time:98847ms step_avg:55.47ms
step:1783/2160 train_time:98936ms step_avg:55.49ms
step:1784/2160 train_time:99023ms step_avg:55.51ms
step:1785/2160 train_time:99113ms step_avg:55.53ms
step:1786/2160 train_time:99201ms step_avg:55.54ms
step:1787/2160 train_time:99291ms step_avg:55.56ms
step:1788/2160 train_time:99379ms step_avg:55.58ms
step:1789/2160 train_time:99468ms step_avg:55.60ms
step:1790/2160 train_time:99555ms step_avg:55.62ms
step:1791/2160 train_time:99644ms step_avg:55.64ms
step:1792/2160 train_time:99730ms step_avg:55.65ms
step:1793/2160 train_time:99818ms step_avg:55.67ms
step:1794/2160 train_time:99905ms step_avg:55.69ms
step:1795/2160 train_time:99993ms step_avg:55.71ms
step:1796/2160 train_time:100080ms step_avg:55.72ms
step:1797/2160 train_time:100170ms step_avg:55.74ms
step:1798/2160 train_time:100258ms step_avg:55.76ms
step:1799/2160 train_time:100348ms step_avg:55.78ms
step:1800/2160 train_time:100435ms step_avg:55.80ms
step:1801/2160 train_time:100524ms step_avg:55.82ms
step:1802/2160 train_time:100611ms step_avg:55.83ms
step:1803/2160 train_time:100699ms step_avg:55.85ms
step:1804/2160 train_time:100785ms step_avg:55.87ms
step:1805/2160 train_time:100874ms step_avg:55.89ms
step:1806/2160 train_time:100961ms step_avg:55.90ms
step:1807/2160 train_time:101050ms step_avg:55.92ms
step:1808/2160 train_time:101138ms step_avg:55.94ms
step:1809/2160 train_time:101228ms step_avg:55.96ms
step:1810/2160 train_time:101314ms step_avg:55.97ms
step:1811/2160 train_time:101404ms step_avg:55.99ms
step:1812/2160 train_time:101491ms step_avg:56.01ms
step:1813/2160 train_time:101580ms step_avg:56.03ms
step:1814/2160 train_time:101668ms step_avg:56.05ms
step:1815/2160 train_time:101757ms step_avg:56.06ms
step:1816/2160 train_time:101843ms step_avg:56.08ms
step:1817/2160 train_time:101932ms step_avg:56.10ms
step:1818/2160 train_time:102018ms step_avg:56.12ms
step:1819/2160 train_time:102108ms step_avg:56.13ms
step:1820/2160 train_time:102195ms step_avg:56.15ms
step:1821/2160 train_time:102284ms step_avg:56.17ms
step:1822/2160 train_time:102371ms step_avg:56.19ms
step:1823/2160 train_time:102460ms step_avg:56.20ms
step:1824/2160 train_time:102548ms step_avg:56.22ms
step:1825/2160 train_time:102637ms step_avg:56.24ms
step:1826/2160 train_time:102724ms step_avg:56.26ms
step:1827/2160 train_time:102812ms step_avg:56.27ms
step:1828/2160 train_time:102899ms step_avg:56.29ms
step:1829/2160 train_time:102988ms step_avg:56.31ms
step:1830/2160 train_time:103075ms step_avg:56.32ms
step:1831/2160 train_time:103163ms step_avg:56.34ms
step:1832/2160 train_time:103251ms step_avg:56.36ms
step:1833/2160 train_time:103340ms step_avg:56.38ms
step:1834/2160 train_time:103428ms step_avg:56.39ms
step:1835/2160 train_time:103517ms step_avg:56.41ms
step:1836/2160 train_time:103604ms step_avg:56.43ms
step:1837/2160 train_time:103693ms step_avg:56.45ms
step:1838/2160 train_time:103780ms step_avg:56.46ms
step:1839/2160 train_time:103870ms step_avg:56.48ms
step:1840/2160 train_time:103957ms step_avg:56.50ms
step:1841/2160 train_time:104047ms step_avg:56.52ms
step:1842/2160 train_time:104133ms step_avg:56.53ms
step:1843/2160 train_time:104222ms step_avg:56.55ms
step:1844/2160 train_time:104310ms step_avg:56.57ms
step:1845/2160 train_time:104399ms step_avg:56.58ms
step:1846/2160 train_time:104485ms step_avg:56.60ms
step:1847/2160 train_time:104574ms step_avg:56.62ms
step:1848/2160 train_time:104662ms step_avg:56.64ms
step:1849/2160 train_time:104751ms step_avg:56.65ms
step:1850/2160 train_time:104838ms step_avg:56.67ms
step:1851/2160 train_time:104928ms step_avg:56.69ms
step:1852/2160 train_time:105015ms step_avg:56.70ms
step:1853/2160 train_time:105104ms step_avg:56.72ms
step:1854/2160 train_time:105191ms step_avg:56.74ms
step:1855/2160 train_time:105280ms step_avg:56.75ms
step:1856/2160 train_time:105368ms step_avg:56.77ms
step:1857/2160 train_time:105457ms step_avg:56.79ms
step:1858/2160 train_time:105544ms step_avg:56.81ms
step:1859/2160 train_time:105633ms step_avg:56.82ms
step:1860/2160 train_time:105720ms step_avg:56.84ms
step:1861/2160 train_time:105809ms step_avg:56.86ms
step:1862/2160 train_time:105896ms step_avg:56.87ms
step:1863/2160 train_time:105986ms step_avg:56.89ms
step:1864/2160 train_time:106072ms step_avg:56.91ms
step:1865/2160 train_time:106162ms step_avg:56.92ms
step:1866/2160 train_time:106249ms step_avg:56.94ms
step:1867/2160 train_time:106338ms step_avg:56.96ms
step:1868/2160 train_time:106426ms step_avg:56.97ms
step:1869/2160 train_time:106514ms step_avg:56.99ms
step:1870/2160 train_time:106601ms step_avg:57.01ms
step:1871/2160 train_time:106690ms step_avg:57.02ms
step:1872/2160 train_time:106777ms step_avg:57.04ms
step:1873/2160 train_time:106866ms step_avg:57.06ms
step:1874/2160 train_time:106953ms step_avg:57.07ms
step:1875/2160 train_time:107042ms step_avg:57.09ms
step:1876/2160 train_time:107129ms step_avg:57.11ms
step:1877/2160 train_time:107219ms step_avg:57.12ms
step:1878/2160 train_time:107307ms step_avg:57.14ms
step:1879/2160 train_time:107395ms step_avg:57.16ms
step:1880/2160 train_time:107482ms step_avg:57.17ms
step:1881/2160 train_time:107571ms step_avg:57.19ms
step:1882/2160 train_time:107658ms step_avg:57.20ms
step:1883/2160 train_time:107746ms step_avg:57.22ms
step:1884/2160 train_time:107833ms step_avg:57.24ms
step:1885/2160 train_time:107922ms step_avg:57.25ms
step:1886/2160 train_time:108009ms step_avg:57.27ms
step:1887/2160 train_time:108098ms step_avg:57.29ms
step:1888/2160 train_time:108185ms step_avg:57.30ms
step:1889/2160 train_time:108273ms step_avg:57.32ms
step:1890/2160 train_time:108360ms step_avg:57.33ms
step:1891/2160 train_time:108450ms step_avg:57.35ms
step:1892/2160 train_time:108537ms step_avg:57.37ms
step:1893/2160 train_time:108625ms step_avg:57.38ms
step:1894/2160 train_time:108712ms step_avg:57.40ms
step:1895/2160 train_time:108801ms step_avg:57.41ms
step:1896/2160 train_time:108888ms step_avg:57.43ms
step:1897/2160 train_time:108978ms step_avg:57.45ms
step:1898/2160 train_time:109065ms step_avg:57.46ms
step:1899/2160 train_time:109154ms step_avg:57.48ms
step:1900/2160 train_time:109241ms step_avg:57.50ms
step:1901/2160 train_time:109330ms step_avg:57.51ms
step:1902/2160 train_time:109416ms step_avg:57.53ms
step:1903/2160 train_time:109506ms step_avg:57.54ms
step:1904/2160 train_time:109592ms step_avg:57.56ms
step:1905/2160 train_time:109682ms step_avg:57.58ms
step:1906/2160 train_time:109769ms step_avg:57.59ms
step:1907/2160 train_time:109857ms step_avg:57.61ms
step:1908/2160 train_time:109944ms step_avg:57.62ms
step:1909/2160 train_time:110033ms step_avg:57.64ms
step:1910/2160 train_time:110121ms step_avg:57.65ms
step:1911/2160 train_time:110211ms step_avg:57.67ms
step:1912/2160 train_time:110297ms step_avg:57.69ms
step:1913/2160 train_time:110386ms step_avg:57.70ms
step:1914/2160 train_time:110473ms step_avg:57.72ms
step:1915/2160 train_time:110562ms step_avg:57.73ms
step:1916/2160 train_time:110649ms step_avg:57.75ms
step:1917/2160 train_time:110738ms step_avg:57.77ms
step:1918/2160 train_time:110826ms step_avg:57.78ms
step:1919/2160 train_time:110914ms step_avg:57.80ms
step:1920/2160 train_time:111001ms step_avg:57.81ms
step:1921/2160 train_time:111090ms step_avg:57.83ms
step:1922/2160 train_time:111177ms step_avg:57.84ms
step:1923/2160 train_time:111266ms step_avg:57.86ms
step:1924/2160 train_time:111353ms step_avg:57.88ms
step:1925/2160 train_time:111441ms step_avg:57.89ms
step:1926/2160 train_time:111528ms step_avg:57.91ms
step:1927/2160 train_time:111617ms step_avg:57.92ms
step:1928/2160 train_time:111704ms step_avg:57.94ms
step:1929/2160 train_time:111793ms step_avg:57.95ms
step:1930/2160 train_time:111880ms step_avg:57.97ms
step:1931/2160 train_time:111969ms step_avg:57.98ms
step:1932/2160 train_time:112055ms step_avg:58.00ms
step:1933/2160 train_time:112145ms step_avg:58.02ms
step:1934/2160 train_time:112232ms step_avg:58.03ms
step:1935/2160 train_time:112321ms step_avg:58.05ms
step:1936/2160 train_time:112408ms step_avg:58.06ms
step:1937/2160 train_time:112496ms step_avg:58.08ms
step:1938/2160 train_time:112583ms step_avg:58.09ms
step:1939/2160 train_time:112672ms step_avg:58.11ms
step:1940/2160 train_time:112759ms step_avg:58.12ms
step:1941/2160 train_time:112848ms step_avg:58.14ms
step:1942/2160 train_time:112936ms step_avg:58.15ms
step:1943/2160 train_time:113024ms step_avg:58.17ms
step:1944/2160 train_time:113111ms step_avg:58.18ms
step:1945/2160 train_time:113200ms step_avg:58.20ms
step:1946/2160 train_time:113287ms step_avg:58.22ms
step:1947/2160 train_time:113376ms step_avg:58.23ms
step:1948/2160 train_time:113463ms step_avg:58.25ms
step:1949/2160 train_time:113552ms step_avg:58.26ms
step:1950/2160 train_time:113639ms step_avg:58.28ms
step:1951/2160 train_time:113729ms step_avg:58.29ms
step:1952/2160 train_time:113815ms step_avg:58.31ms
step:1953/2160 train_time:113905ms step_avg:58.32ms
step:1954/2160 train_time:113992ms step_avg:58.34ms
step:1955/2160 train_time:114081ms step_avg:58.35ms
step:1956/2160 train_time:114168ms step_avg:58.37ms
step:1957/2160 train_time:114257ms step_avg:58.38ms
step:1958/2160 train_time:114345ms step_avg:58.40ms
step:1959/2160 train_time:114433ms step_avg:58.41ms
step:1960/2160 train_time:114520ms step_avg:58.43ms
step:1961/2160 train_time:114610ms step_avg:58.44ms
step:1962/2160 train_time:114697ms step_avg:58.46ms
step:1963/2160 train_time:114786ms step_avg:58.47ms
step:1964/2160 train_time:114873ms step_avg:58.49ms
step:1965/2160 train_time:114962ms step_avg:58.50ms
step:1966/2160 train_time:115049ms step_avg:58.52ms
step:1967/2160 train_time:115138ms step_avg:58.53ms
step:1968/2160 train_time:115225ms step_avg:58.55ms
step:1969/2160 train_time:115314ms step_avg:58.56ms
step:1970/2160 train_time:115401ms step_avg:58.58ms
step:1971/2160 train_time:115490ms step_avg:58.59ms
step:1972/2160 train_time:115578ms step_avg:58.61ms
step:1973/2160 train_time:115668ms step_avg:58.63ms
step:1974/2160 train_time:115754ms step_avg:58.64ms
step:1975/2160 train_time:115843ms step_avg:58.65ms
step:1976/2160 train_time:115930ms step_avg:58.67ms
step:1977/2160 train_time:116019ms step_avg:58.68ms
step:1978/2160 train_time:116105ms step_avg:58.70ms
step:1979/2160 train_time:116195ms step_avg:58.71ms
step:1980/2160 train_time:116281ms step_avg:58.73ms
step:1981/2160 train_time:116371ms step_avg:58.74ms
step:1982/2160 train_time:116459ms step_avg:58.76ms
step:1983/2160 train_time:116547ms step_avg:58.77ms
step:1984/2160 train_time:116634ms step_avg:58.79ms
step:1985/2160 train_time:116723ms step_avg:58.80ms
step:1986/2160 train_time:116811ms step_avg:58.82ms
step:1987/2160 train_time:116900ms step_avg:58.83ms
step:1988/2160 train_time:116988ms step_avg:58.85ms
step:1989/2160 train_time:117077ms step_avg:58.86ms
step:1990/2160 train_time:117163ms step_avg:58.88ms
step:1991/2160 train_time:117252ms step_avg:58.89ms
step:1992/2160 train_time:117339ms step_avg:58.91ms
step:1993/2160 train_time:117429ms step_avg:58.92ms
step:1994/2160 train_time:117517ms step_avg:58.94ms
step:1995/2160 train_time:117606ms step_avg:58.95ms
step:1996/2160 train_time:117693ms step_avg:58.96ms
step:1997/2160 train_time:117781ms step_avg:58.98ms
step:1998/2160 train_time:117869ms step_avg:58.99ms
step:1999/2160 train_time:117958ms step_avg:59.01ms
step:2000/2160 train_time:118046ms step_avg:59.02ms
step:2000/2160 val_loss:3.3100 train_time:118135ms step_avg:59.07ms
step:2001/2160 train_time:118159ms step_avg:59.05ms
step:2002/2160 train_time:118226ms step_avg:59.05ms
step:2003/2160 train_time:118323ms step_avg:59.07ms
step:2004/2160 train_time:118412ms step_avg:59.09ms
step:2005/2160 train_time:118501ms step_avg:59.10ms
step:2006/2160 train_time:118587ms step_avg:59.12ms
step:2007/2160 train_time:118675ms step_avg:59.13ms
step:2008/2160 train_time:118761ms step_avg:59.14ms
step:2009/2160 train_time:118848ms step_avg:59.16ms
step:2010/2160 train_time:118935ms step_avg:59.17ms
step:2011/2160 train_time:119023ms step_avg:59.19ms
step:2012/2160 train_time:119111ms step_avg:59.20ms
step:2013/2160 train_time:119203ms step_avg:59.22ms
step:2014/2160 train_time:119292ms step_avg:59.23ms
step:2015/2160 train_time:119383ms step_avg:59.25ms
step:2016/2160 train_time:119471ms step_avg:59.26ms
step:2017/2160 train_time:119560ms step_avg:59.28ms
step:2018/2160 train_time:119646ms step_avg:59.29ms
step:2019/2160 train_time:119735ms step_avg:59.30ms
step:2020/2160 train_time:119821ms step_avg:59.32ms
step:2021/2160 train_time:119909ms step_avg:59.33ms
step:2022/2160 train_time:119996ms step_avg:59.35ms
step:2023/2160 train_time:120084ms step_avg:59.36ms
step:2024/2160 train_time:120172ms step_avg:59.37ms
step:2025/2160 train_time:120262ms step_avg:59.39ms
step:2026/2160 train_time:120350ms step_avg:59.40ms
step:2027/2160 train_time:120440ms step_avg:59.42ms
step:2028/2160 train_time:120528ms step_avg:59.43ms
step:2029/2160 train_time:120617ms step_avg:59.45ms
step:2030/2160 train_time:120704ms step_avg:59.46ms
step:2031/2160 train_time:120791ms step_avg:59.47ms
step:2032/2160 train_time:120877ms step_avg:59.49ms
step:2033/2160 train_time:120965ms step_avg:59.50ms
step:2034/2160 train_time:121053ms step_avg:59.51ms
step:2035/2160 train_time:121142ms step_avg:59.53ms
step:2036/2160 train_time:121230ms step_avg:59.54ms
step:2037/2160 train_time:121321ms step_avg:59.56ms
step:2038/2160 train_time:121408ms step_avg:59.57ms
step:2039/2160 train_time:121499ms step_avg:59.59ms
step:2040/2160 train_time:121586ms step_avg:59.60ms
step:2041/2160 train_time:121675ms step_avg:59.62ms
step:2042/2160 train_time:121762ms step_avg:59.63ms
step:2043/2160 train_time:121851ms step_avg:59.64ms
step:2044/2160 train_time:121937ms step_avg:59.66ms
step:2045/2160 train_time:122025ms step_avg:59.67ms
step:2046/2160 train_time:122112ms step_avg:59.68ms
step:2047/2160 train_time:122201ms step_avg:59.70ms
step:2048/2160 train_time:122290ms step_avg:59.71ms
step:2049/2160 train_time:122379ms step_avg:59.73ms
step:2050/2160 train_time:122468ms step_avg:59.74ms
step:2051/2160 train_time:122557ms step_avg:59.75ms
step:2052/2160 train_time:122644ms step_avg:59.77ms
step:2053/2160 train_time:122733ms step_avg:59.78ms
step:2054/2160 train_time:122820ms step_avg:59.80ms
step:2055/2160 train_time:122907ms step_avg:59.81ms
step:2056/2160 train_time:122994ms step_avg:59.82ms
step:2057/2160 train_time:123083ms step_avg:59.84ms
step:2058/2160 train_time:123170ms step_avg:59.85ms
step:2059/2160 train_time:123259ms step_avg:59.86ms
step:2060/2160 train_time:123347ms step_avg:59.88ms
step:2061/2160 train_time:123436ms step_avg:59.89ms
step:2062/2160 train_time:123523ms step_avg:59.90ms
step:2063/2160 train_time:123612ms step_avg:59.92ms
step:2064/2160 train_time:123699ms step_avg:59.93ms
step:2065/2160 train_time:123787ms step_avg:59.95ms
step:2066/2160 train_time:123874ms step_avg:59.96ms
step:2067/2160 train_time:123963ms step_avg:59.97ms
step:2068/2160 train_time:124050ms step_avg:59.99ms
step:2069/2160 train_time:124139ms step_avg:60.00ms
step:2070/2160 train_time:124227ms step_avg:60.01ms
step:2071/2160 train_time:124316ms step_avg:60.03ms
step:2072/2160 train_time:124404ms step_avg:60.04ms
step:2073/2160 train_time:124494ms step_avg:60.05ms
step:2074/2160 train_time:124581ms step_avg:60.07ms
step:2075/2160 train_time:124670ms step_avg:60.08ms
step:2076/2160 train_time:124757ms step_avg:60.09ms
step:2077/2160 train_time:124846ms step_avg:60.11ms
step:2078/2160 train_time:124933ms step_avg:60.12ms
step:2079/2160 train_time:125021ms step_avg:60.14ms
step:2080/2160 train_time:125108ms step_avg:60.15ms
step:2081/2160 train_time:125198ms step_avg:60.16ms
step:2082/2160 train_time:125285ms step_avg:60.18ms
step:2083/2160 train_time:125374ms step_avg:60.19ms
step:2084/2160 train_time:125461ms step_avg:60.20ms
step:2085/2160 train_time:125550ms step_avg:60.22ms
step:2086/2160 train_time:125638ms step_avg:60.23ms
step:2087/2160 train_time:125727ms step_avg:60.24ms
step:2088/2160 train_time:125814ms step_avg:60.26ms
step:2089/2160 train_time:125902ms step_avg:60.27ms
step:2090/2160 train_time:125989ms step_avg:60.28ms
step:2091/2160 train_time:126079ms step_avg:60.30ms
step:2092/2160 train_time:126165ms step_avg:60.31ms
step:2093/2160 train_time:126255ms step_avg:60.32ms
step:2094/2160 train_time:126342ms step_avg:60.34ms
step:2095/2160 train_time:126430ms step_avg:60.35ms
step:2096/2160 train_time:126517ms step_avg:60.36ms
step:2097/2160 train_time:126605ms step_avg:60.37ms
step:2098/2160 train_time:126693ms step_avg:60.39ms
step:2099/2160 train_time:126781ms step_avg:60.40ms
step:2100/2160 train_time:126869ms step_avg:60.41ms
step:2101/2160 train_time:126959ms step_avg:60.43ms
step:2102/2160 train_time:127046ms step_avg:60.44ms
step:2103/2160 train_time:127134ms step_avg:60.45ms
step:2104/2160 train_time:127222ms step_avg:60.47ms
step:2105/2160 train_time:127311ms step_avg:60.48ms
step:2106/2160 train_time:127398ms step_avg:60.49ms
step:2107/2160 train_time:127488ms step_avg:60.51ms
step:2108/2160 train_time:127575ms step_avg:60.52ms
step:2109/2160 train_time:127664ms step_avg:60.53ms
step:2110/2160 train_time:127751ms step_avg:60.55ms
step:2111/2160 train_time:127840ms step_avg:60.56ms
step:2112/2160 train_time:127927ms step_avg:60.57ms
step:2113/2160 train_time:128017ms step_avg:60.59ms
step:2114/2160 train_time:128105ms step_avg:60.60ms
step:2115/2160 train_time:128193ms step_avg:60.61ms
step:2116/2160 train_time:128280ms step_avg:60.62ms
step:2117/2160 train_time:128368ms step_avg:60.64ms
step:2118/2160 train_time:128456ms step_avg:60.65ms
step:2119/2160 train_time:128545ms step_avg:60.66ms
step:2120/2160 train_time:128631ms step_avg:60.68ms
step:2121/2160 train_time:128721ms step_avg:60.69ms
step:2122/2160 train_time:128808ms step_avg:60.70ms
step:2123/2160 train_time:128898ms step_avg:60.71ms
step:2124/2160 train_time:128985ms step_avg:60.73ms
step:2125/2160 train_time:129075ms step_avg:60.74ms
step:2126/2160 train_time:129162ms step_avg:60.75ms
step:2127/2160 train_time:129251ms step_avg:60.77ms
step:2128/2160 train_time:129339ms step_avg:60.78ms
step:2129/2160 train_time:129427ms step_avg:60.79ms
step:2130/2160 train_time:129515ms step_avg:60.80ms
step:2131/2160 train_time:129604ms step_avg:60.82ms
step:2132/2160 train_time:129691ms step_avg:60.83ms
step:2133/2160 train_time:129779ms step_avg:60.84ms
step:2134/2160 train_time:129868ms step_avg:60.86ms
step:2135/2160 train_time:129958ms step_avg:60.87ms
step:2136/2160 train_time:130046ms step_avg:60.88ms
step:2137/2160 train_time:130135ms step_avg:60.90ms
step:2138/2160 train_time:130223ms step_avg:60.91ms
step:2139/2160 train_time:130312ms step_avg:60.92ms
step:2140/2160 train_time:130399ms step_avg:60.93ms
step:2141/2160 train_time:130489ms step_avg:60.95ms
step:2142/2160 train_time:130576ms step_avg:60.96ms
step:2143/2160 train_time:130666ms step_avg:60.97ms
step:2144/2160 train_time:130753ms step_avg:60.99ms
step:2145/2160 train_time:130842ms step_avg:61.00ms
step:2146/2160 train_time:130930ms step_avg:61.01ms
step:2147/2160 train_time:131019ms step_avg:61.02ms
step:2148/2160 train_time:131107ms step_avg:61.04ms
step:2149/2160 train_time:131198ms step_avg:61.05ms
step:2150/2160 train_time:131286ms step_avg:61.06ms
step:2151/2160 train_time:131376ms step_avg:61.08ms
step:2152/2160 train_time:131462ms step_avg:61.09ms
step:2153/2160 train_time:131551ms step_avg:61.10ms
step:2154/2160 train_time:131640ms step_avg:61.11ms
step:2155/2160 train_time:131728ms step_avg:61.13ms
step:2156/2160 train_time:131815ms step_avg:61.14ms
step:2157/2160 train_time:131905ms step_avg:61.15ms
step:2158/2160 train_time:131992ms step_avg:61.16ms
step:2159/2160 train_time:132081ms step_avg:61.18ms
step:2160/2160 train_time:132169ms step_avg:61.19ms
step:2160/2160 val_loss:3.2777 train_time:132259ms step_avg:61.23ms
peak memory allocated: 29707 MiB reserved: 44696 MiB

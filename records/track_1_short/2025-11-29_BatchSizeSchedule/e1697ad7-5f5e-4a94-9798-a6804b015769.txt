import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = (16 / 8) * 0.8
    if x > 0.66:
        lr_max = (24 / 8) * 0.8
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sun Nov 30 03:08:25 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   30C    P0            117W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            110W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            107W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0            112W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   28C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2160 train_time:89ms step_avg:89.06ms
step:2/2160 train_time:127ms step_avg:63.59ms
step:3/2160 train_time:146ms step_avg:48.69ms
step:4/2160 train_time:166ms step_avg:41.49ms
step:5/2160 train_time:197ms step_avg:39.31ms
step:6/2160 train_time:274ms step_avg:45.63ms
step:7/2160 train_time:301ms step_avg:43.04ms
step:8/2160 train_time:334ms step_avg:41.76ms
step:9/2160 train_time:368ms step_avg:40.89ms
step:10/2160 train_time:401ms step_avg:40.08ms
step:11/2160 train_time:435ms step_avg:39.52ms
step:12/2160 train_time:468ms step_avg:38.97ms
step:13/2160 train_time:502ms step_avg:38.60ms
step:14/2160 train_time:535ms step_avg:38.20ms
step:15/2160 train_time:569ms step_avg:37.94ms
step:16/2160 train_time:602ms step_avg:37.63ms
step:17/2160 train_time:636ms step_avg:37.42ms
step:18/2160 train_time:669ms step_avg:37.16ms
step:19/2160 train_time:703ms step_avg:37.02ms
step:20/2160 train_time:736ms step_avg:36.82ms
step:21/2160 train_time:771ms step_avg:36.70ms
step:22/2160 train_time:804ms step_avg:36.53ms
step:23/2160 train_time:838ms step_avg:36.43ms
step:24/2160 train_time:871ms step_avg:36.30ms
step:25/2160 train_time:905ms step_avg:36.20ms
step:26/2160 train_time:938ms step_avg:36.08ms
step:27/2160 train_time:972ms step_avg:36.01ms
step:28/2160 train_time:1005ms step_avg:35.90ms
step:29/2160 train_time:1039ms step_avg:35.84ms
step:30/2160 train_time:1072ms step_avg:35.74ms
step:31/2160 train_time:1106ms step_avg:35.69ms
step:32/2160 train_time:1139ms step_avg:35.60ms
step:33/2160 train_time:1173ms step_avg:35.56ms
step:34/2160 train_time:1207ms step_avg:35.49ms
step:35/2160 train_time:1242ms step_avg:35.49ms
step:36/2160 train_time:1275ms step_avg:35.43ms
step:37/2160 train_time:1310ms step_avg:35.41ms
step:38/2160 train_time:1343ms step_avg:35.35ms
step:39/2160 train_time:1378ms step_avg:35.33ms
step:40/2160 train_time:1411ms step_avg:35.28ms
step:41/2160 train_time:1445ms step_avg:35.25ms
step:42/2160 train_time:1478ms step_avg:35.19ms
step:43/2160 train_time:1513ms step_avg:35.18ms
step:44/2160 train_time:1546ms step_avg:35.13ms
step:45/2160 train_time:1581ms step_avg:35.12ms
step:46/2160 train_time:1614ms step_avg:35.08ms
step:47/2160 train_time:1648ms step_avg:35.06ms
step:48/2160 train_time:1681ms step_avg:35.02ms
step:49/2160 train_time:1715ms step_avg:35.00ms
step:50/2160 train_time:1748ms step_avg:34.96ms
step:51/2160 train_time:1782ms step_avg:34.94ms
step:52/2160 train_time:1815ms step_avg:34.91ms
step:53/2160 train_time:1849ms step_avg:34.90ms
step:54/2160 train_time:1882ms step_avg:34.86ms
step:55/2160 train_time:1917ms step_avg:34.85ms
step:56/2160 train_time:1949ms step_avg:34.81ms
step:57/2160 train_time:1984ms step_avg:34.81ms
step:58/2160 train_time:2017ms step_avg:34.78ms
step:59/2160 train_time:2051ms step_avg:34.77ms
step:60/2160 train_time:2084ms step_avg:34.74ms
step:61/2160 train_time:2119ms step_avg:34.73ms
step:62/2160 train_time:2152ms step_avg:34.70ms
step:63/2160 train_time:2186ms step_avg:34.70ms
step:64/2160 train_time:2219ms step_avg:34.67ms
step:65/2160 train_time:2253ms step_avg:34.66ms
step:66/2160 train_time:2286ms step_avg:34.64ms
step:67/2160 train_time:2321ms step_avg:34.65ms
step:68/2160 train_time:2355ms step_avg:34.63ms
step:69/2160 train_time:2389ms step_avg:34.62ms
step:70/2160 train_time:2422ms step_avg:34.60ms
step:71/2160 train_time:2456ms step_avg:34.59ms
step:72/2160 train_time:2489ms step_avg:34.57ms
step:73/2160 train_time:2524ms step_avg:34.57ms
step:74/2160 train_time:2557ms step_avg:34.55ms
step:75/2160 train_time:2591ms step_avg:34.54ms
step:76/2160 train_time:2623ms step_avg:34.52ms
step:77/2160 train_time:2658ms step_avg:34.52ms
step:78/2160 train_time:2691ms step_avg:34.50ms
step:79/2160 train_time:2725ms step_avg:34.50ms
step:80/2160 train_time:2759ms step_avg:34.49ms
step:81/2160 train_time:2793ms step_avg:34.48ms
step:82/2160 train_time:2826ms step_avg:34.46ms
step:83/2160 train_time:2860ms step_avg:34.46ms
step:84/2160 train_time:2893ms step_avg:34.45ms
step:85/2160 train_time:2928ms step_avg:34.44ms
step:86/2160 train_time:2960ms step_avg:34.42ms
step:87/2160 train_time:2995ms step_avg:34.43ms
step:88/2160 train_time:3028ms step_avg:34.41ms
step:89/2160 train_time:3062ms step_avg:34.40ms
step:90/2160 train_time:3095ms step_avg:34.39ms
step:91/2160 train_time:3129ms step_avg:34.38ms
step:92/2160 train_time:3162ms step_avg:34.37ms
step:93/2160 train_time:3196ms step_avg:34.37ms
step:94/2160 train_time:3229ms step_avg:34.36ms
step:95/2160 train_time:3263ms step_avg:34.35ms
step:96/2160 train_time:3297ms step_avg:34.34ms
step:97/2160 train_time:3331ms step_avg:34.34ms
step:98/2160 train_time:3363ms step_avg:34.32ms
step:99/2160 train_time:3398ms step_avg:34.32ms
step:100/2160 train_time:3431ms step_avg:34.31ms
step:101/2160 train_time:3465ms step_avg:34.31ms
step:102/2160 train_time:3498ms step_avg:34.30ms
step:103/2160 train_time:3533ms step_avg:34.30ms
step:104/2160 train_time:3565ms step_avg:34.28ms
step:105/2160 train_time:3600ms step_avg:34.29ms
step:106/2160 train_time:3633ms step_avg:34.28ms
step:107/2160 train_time:3667ms step_avg:34.27ms
step:108/2160 train_time:3701ms step_avg:34.26ms
step:109/2160 train_time:3734ms step_avg:34.26ms
step:110/2160 train_time:3767ms step_avg:34.25ms
step:111/2160 train_time:3802ms step_avg:34.25ms
step:112/2160 train_time:3835ms step_avg:34.24ms
step:113/2160 train_time:3869ms step_avg:34.24ms
step:114/2160 train_time:3902ms step_avg:34.23ms
step:115/2160 train_time:3936ms step_avg:34.23ms
step:116/2160 train_time:3969ms step_avg:34.21ms
step:117/2160 train_time:4003ms step_avg:34.22ms
step:118/2160 train_time:4037ms step_avg:34.21ms
step:119/2160 train_time:4071ms step_avg:34.21ms
step:120/2160 train_time:4104ms step_avg:34.20ms
step:121/2160 train_time:4138ms step_avg:34.20ms
step:122/2160 train_time:4171ms step_avg:34.19ms
step:123/2160 train_time:4205ms step_avg:34.18ms
step:124/2160 train_time:4238ms step_avg:34.18ms
step:125/2160 train_time:4272ms step_avg:34.18ms
step:126/2160 train_time:4305ms step_avg:34.17ms
step:127/2160 train_time:4339ms step_avg:34.17ms
step:128/2160 train_time:4372ms step_avg:34.16ms
step:129/2160 train_time:4406ms step_avg:34.16ms
step:130/2160 train_time:4439ms step_avg:34.15ms
step:131/2160 train_time:4474ms step_avg:34.15ms
step:132/2160 train_time:4506ms step_avg:34.14ms
step:133/2160 train_time:4541ms step_avg:34.14ms
step:134/2160 train_time:4574ms step_avg:34.13ms
step:135/2160 train_time:4608ms step_avg:34.13ms
step:136/2160 train_time:4641ms step_avg:34.12ms
step:137/2160 train_time:4675ms step_avg:34.13ms
step:138/2160 train_time:4708ms step_avg:34.12ms
step:139/2160 train_time:4743ms step_avg:34.12ms
step:140/2160 train_time:4776ms step_avg:34.11ms
step:141/2160 train_time:4810ms step_avg:34.11ms
step:142/2160 train_time:4843ms step_avg:34.11ms
step:143/2160 train_time:4877ms step_avg:34.11ms
step:144/2160 train_time:4910ms step_avg:34.10ms
step:145/2160 train_time:4945ms step_avg:34.10ms
step:146/2160 train_time:4977ms step_avg:34.09ms
step:147/2160 train_time:5012ms step_avg:34.09ms
step:148/2160 train_time:5045ms step_avg:34.09ms
step:149/2160 train_time:5079ms step_avg:34.09ms
step:150/2160 train_time:5112ms step_avg:34.08ms
step:151/2160 train_time:5146ms step_avg:34.08ms
step:152/2160 train_time:5179ms step_avg:34.07ms
step:153/2160 train_time:5213ms step_avg:34.07ms
step:154/2160 train_time:5246ms step_avg:34.06ms
step:155/2160 train_time:5280ms step_avg:34.07ms
step:156/2160 train_time:5313ms step_avg:34.06ms
step:157/2160 train_time:5347ms step_avg:34.06ms
step:158/2160 train_time:5380ms step_avg:34.05ms
step:159/2160 train_time:5414ms step_avg:34.05ms
step:160/2160 train_time:5447ms step_avg:34.04ms
step:161/2160 train_time:5481ms step_avg:34.05ms
step:162/2160 train_time:5515ms step_avg:34.04ms
step:163/2160 train_time:5549ms step_avg:34.04ms
step:164/2160 train_time:5582ms step_avg:34.03ms
step:165/2160 train_time:5616ms step_avg:34.03ms
step:166/2160 train_time:5648ms step_avg:34.03ms
step:167/2160 train_time:5683ms step_avg:34.03ms
step:168/2160 train_time:5716ms step_avg:34.02ms
step:169/2160 train_time:5750ms step_avg:34.02ms
step:170/2160 train_time:5783ms step_avg:34.02ms
step:171/2160 train_time:5817ms step_avg:34.02ms
step:172/2160 train_time:5850ms step_avg:34.01ms
step:173/2160 train_time:5884ms step_avg:34.01ms
step:174/2160 train_time:5917ms step_avg:34.01ms
step:175/2160 train_time:5952ms step_avg:34.01ms
step:176/2160 train_time:5985ms step_avg:34.00ms
step:177/2160 train_time:6018ms step_avg:34.00ms
step:178/2160 train_time:6052ms step_avg:34.00ms
step:179/2160 train_time:6086ms step_avg:34.00ms
step:180/2160 train_time:6119ms step_avg:33.99ms
step:181/2160 train_time:6153ms step_avg:33.99ms
step:182/2160 train_time:6186ms step_avg:33.99ms
step:183/2160 train_time:6220ms step_avg:33.99ms
step:184/2160 train_time:6253ms step_avg:33.98ms
step:185/2160 train_time:6286ms step_avg:33.98ms
step:186/2160 train_time:6319ms step_avg:33.97ms
step:187/2160 train_time:6353ms step_avg:33.97ms
step:188/2160 train_time:6386ms step_avg:33.97ms
step:189/2160 train_time:6420ms step_avg:33.97ms
step:190/2160 train_time:6453ms step_avg:33.96ms
step:191/2160 train_time:6487ms step_avg:33.96ms
step:192/2160 train_time:6520ms step_avg:33.96ms
step:193/2160 train_time:6554ms step_avg:33.96ms
step:194/2160 train_time:6587ms step_avg:33.95ms
step:195/2160 train_time:6622ms step_avg:33.96ms
step:196/2160 train_time:6655ms step_avg:33.95ms
step:197/2160 train_time:6689ms step_avg:33.95ms
step:198/2160 train_time:6722ms step_avg:33.95ms
step:199/2160 train_time:6756ms step_avg:33.95ms
step:200/2160 train_time:6789ms step_avg:33.95ms
step:201/2160 train_time:6823ms step_avg:33.95ms
step:202/2160 train_time:6856ms step_avg:33.94ms
step:203/2160 train_time:6890ms step_avg:33.94ms
step:204/2160 train_time:6923ms step_avg:33.94ms
step:205/2160 train_time:6957ms step_avg:33.93ms
step:206/2160 train_time:6990ms step_avg:33.93ms
step:207/2160 train_time:7024ms step_avg:33.93ms
step:208/2160 train_time:7057ms step_avg:33.93ms
step:209/2160 train_time:7091ms step_avg:33.93ms
step:210/2160 train_time:7124ms step_avg:33.92ms
step:211/2160 train_time:7158ms step_avg:33.93ms
step:212/2160 train_time:7191ms step_avg:33.92ms
step:213/2160 train_time:7225ms step_avg:33.92ms
step:214/2160 train_time:7258ms step_avg:33.92ms
step:215/2160 train_time:7293ms step_avg:33.92ms
step:216/2160 train_time:7326ms step_avg:33.91ms
step:217/2160 train_time:7360ms step_avg:33.92ms
step:218/2160 train_time:7393ms step_avg:33.91ms
step:219/2160 train_time:7427ms step_avg:33.91ms
step:220/2160 train_time:7460ms step_avg:33.91ms
step:221/2160 train_time:7494ms step_avg:33.91ms
step:222/2160 train_time:7527ms step_avg:33.90ms
step:223/2160 train_time:7561ms step_avg:33.90ms
step:224/2160 train_time:7594ms step_avg:33.90ms
step:225/2160 train_time:7628ms step_avg:33.90ms
step:226/2160 train_time:7661ms step_avg:33.90ms
step:227/2160 train_time:7695ms step_avg:33.90ms
step:228/2160 train_time:7728ms step_avg:33.89ms
step:229/2160 train_time:7762ms step_avg:33.89ms
step:230/2160 train_time:7795ms step_avg:33.89ms
step:231/2160 train_time:7829ms step_avg:33.89ms
step:232/2160 train_time:7862ms step_avg:33.89ms
step:233/2160 train_time:7896ms step_avg:33.89ms
step:234/2160 train_time:7928ms step_avg:33.88ms
step:235/2160 train_time:7962ms step_avg:33.88ms
step:236/2160 train_time:7996ms step_avg:33.88ms
step:237/2160 train_time:8029ms step_avg:33.88ms
step:238/2160 train_time:8062ms step_avg:33.88ms
step:239/2160 train_time:8096ms step_avg:33.88ms
step:240/2160 train_time:8129ms step_avg:33.87ms
step:241/2160 train_time:8164ms step_avg:33.87ms
step:242/2160 train_time:8197ms step_avg:33.87ms
step:243/2160 train_time:8231ms step_avg:33.87ms
step:244/2160 train_time:8263ms step_avg:33.87ms
step:245/2160 train_time:8297ms step_avg:33.87ms
step:246/2160 train_time:8330ms step_avg:33.86ms
step:247/2160 train_time:8365ms step_avg:33.87ms
step:248/2160 train_time:8398ms step_avg:33.86ms
step:249/2160 train_time:8432ms step_avg:33.86ms
step:250/2160 train_time:8465ms step_avg:33.86ms
step:250/2160 val_loss:4.3027 train_time:8500ms step_avg:34.00ms
step:251/2160 train_time:8520ms step_avg:33.95ms
step:252/2160 train_time:8539ms step_avg:33.88ms
step:253/2160 train_time:8569ms step_avg:33.87ms
step:254/2160 train_time:8602ms step_avg:33.87ms
step:255/2160 train_time:8639ms step_avg:33.88ms
step:256/2160 train_time:8672ms step_avg:33.88ms
step:257/2160 train_time:8707ms step_avg:33.88ms
step:258/2160 train_time:8740ms step_avg:33.88ms
step:259/2160 train_time:8775ms step_avg:33.88ms
step:260/2160 train_time:8808ms step_avg:33.88ms
step:261/2160 train_time:8842ms step_avg:33.88ms
step:262/2160 train_time:8876ms step_avg:33.88ms
step:263/2160 train_time:8909ms step_avg:33.88ms
step:264/2160 train_time:8942ms step_avg:33.87ms
step:265/2160 train_time:8976ms step_avg:33.87ms
step:266/2160 train_time:9009ms step_avg:33.87ms
step:267/2160 train_time:9043ms step_avg:33.87ms
step:268/2160 train_time:9076ms step_avg:33.86ms
step:269/2160 train_time:9109ms step_avg:33.86ms
step:270/2160 train_time:9142ms step_avg:33.86ms
step:271/2160 train_time:9176ms step_avg:33.86ms
step:272/2160 train_time:9209ms step_avg:33.86ms
step:273/2160 train_time:9243ms step_avg:33.86ms
step:274/2160 train_time:9276ms step_avg:33.85ms
step:275/2160 train_time:9310ms step_avg:33.85ms
step:276/2160 train_time:9343ms step_avg:33.85ms
step:277/2160 train_time:9376ms step_avg:33.85ms
step:278/2160 train_time:9409ms step_avg:33.85ms
step:279/2160 train_time:9443ms step_avg:33.85ms
step:280/2160 train_time:9476ms step_avg:33.84ms
step:281/2160 train_time:9510ms step_avg:33.84ms
step:282/2160 train_time:9544ms step_avg:33.84ms
step:283/2160 train_time:9578ms step_avg:33.84ms
step:284/2160 train_time:9611ms step_avg:33.84ms
step:285/2160 train_time:9646ms step_avg:33.85ms
step:286/2160 train_time:9679ms step_avg:33.84ms
step:287/2160 train_time:9714ms step_avg:33.85ms
step:288/2160 train_time:9747ms step_avg:33.84ms
step:289/2160 train_time:9781ms step_avg:33.84ms
step:290/2160 train_time:9814ms step_avg:33.84ms
step:291/2160 train_time:9848ms step_avg:33.84ms
step:292/2160 train_time:9882ms step_avg:33.84ms
step:293/2160 train_time:9916ms step_avg:33.84ms
step:294/2160 train_time:9948ms step_avg:33.84ms
step:295/2160 train_time:9983ms step_avg:33.84ms
step:296/2160 train_time:10015ms step_avg:33.84ms
step:297/2160 train_time:10050ms step_avg:33.84ms
step:298/2160 train_time:10083ms step_avg:33.84ms
step:299/2160 train_time:10117ms step_avg:33.84ms
step:300/2160 train_time:10150ms step_avg:33.83ms
step:301/2160 train_time:10184ms step_avg:33.83ms
step:302/2160 train_time:10217ms step_avg:33.83ms
step:303/2160 train_time:10251ms step_avg:33.83ms
step:304/2160 train_time:10284ms step_avg:33.83ms
step:305/2160 train_time:10318ms step_avg:33.83ms
step:306/2160 train_time:10351ms step_avg:33.83ms
step:307/2160 train_time:10385ms step_avg:33.83ms
step:308/2160 train_time:10417ms step_avg:33.82ms
step:309/2160 train_time:10452ms step_avg:33.82ms
step:310/2160 train_time:10485ms step_avg:33.82ms
step:311/2160 train_time:10518ms step_avg:33.82ms
step:312/2160 train_time:10551ms step_avg:33.82ms
step:313/2160 train_time:10585ms step_avg:33.82ms
step:314/2160 train_time:10618ms step_avg:33.82ms
step:315/2160 train_time:10653ms step_avg:33.82ms
step:316/2160 train_time:10686ms step_avg:33.82ms
step:317/2160 train_time:10720ms step_avg:33.82ms
step:318/2160 train_time:10753ms step_avg:33.82ms
step:319/2160 train_time:10788ms step_avg:33.82ms
step:320/2160 train_time:10821ms step_avg:33.82ms
step:321/2160 train_time:10855ms step_avg:33.82ms
step:322/2160 train_time:10888ms step_avg:33.81ms
step:323/2160 train_time:10923ms step_avg:33.82ms
step:324/2160 train_time:10956ms step_avg:33.82ms
step:325/2160 train_time:10990ms step_avg:33.82ms
step:326/2160 train_time:11023ms step_avg:33.81ms
step:327/2160 train_time:11058ms step_avg:33.82ms
step:328/2160 train_time:11090ms step_avg:33.81ms
step:329/2160 train_time:11125ms step_avg:33.81ms
step:330/2160 train_time:11158ms step_avg:33.81ms
step:331/2160 train_time:11192ms step_avg:33.81ms
step:332/2160 train_time:11225ms step_avg:33.81ms
step:333/2160 train_time:11258ms step_avg:33.81ms
step:334/2160 train_time:11291ms step_avg:33.81ms
step:335/2160 train_time:11326ms step_avg:33.81ms
step:336/2160 train_time:11359ms step_avg:33.81ms
step:337/2160 train_time:11393ms step_avg:33.81ms
step:338/2160 train_time:11426ms step_avg:33.80ms
step:339/2160 train_time:11460ms step_avg:33.80ms
step:340/2160 train_time:11493ms step_avg:33.80ms
step:341/2160 train_time:11527ms step_avg:33.80ms
step:342/2160 train_time:11560ms step_avg:33.80ms
step:343/2160 train_time:11594ms step_avg:33.80ms
step:344/2160 train_time:11627ms step_avg:33.80ms
step:345/2160 train_time:11661ms step_avg:33.80ms
step:346/2160 train_time:11694ms step_avg:33.80ms
step:347/2160 train_time:11728ms step_avg:33.80ms
step:348/2160 train_time:11762ms step_avg:33.80ms
step:349/2160 train_time:11796ms step_avg:33.80ms
step:350/2160 train_time:11829ms step_avg:33.80ms
step:351/2160 train_time:11863ms step_avg:33.80ms
step:352/2160 train_time:11896ms step_avg:33.80ms
step:353/2160 train_time:11930ms step_avg:33.80ms
step:354/2160 train_time:11963ms step_avg:33.79ms
step:355/2160 train_time:11997ms step_avg:33.80ms
step:356/2160 train_time:12030ms step_avg:33.79ms
step:357/2160 train_time:12065ms step_avg:33.79ms
step:358/2160 train_time:12098ms step_avg:33.79ms
step:359/2160 train_time:12132ms step_avg:33.79ms
step:360/2160 train_time:12165ms step_avg:33.79ms
step:361/2160 train_time:12199ms step_avg:33.79ms
step:362/2160 train_time:12232ms step_avg:33.79ms
step:363/2160 train_time:12266ms step_avg:33.79ms
step:364/2160 train_time:12299ms step_avg:33.79ms
step:365/2160 train_time:12333ms step_avg:33.79ms
step:366/2160 train_time:12366ms step_avg:33.79ms
step:367/2160 train_time:12400ms step_avg:33.79ms
step:368/2160 train_time:12433ms step_avg:33.79ms
step:369/2160 train_time:12467ms step_avg:33.79ms
step:370/2160 train_time:12500ms step_avg:33.78ms
step:371/2160 train_time:12534ms step_avg:33.78ms
step:372/2160 train_time:12567ms step_avg:33.78ms
step:373/2160 train_time:12601ms step_avg:33.78ms
step:374/2160 train_time:12634ms step_avg:33.78ms
step:375/2160 train_time:12668ms step_avg:33.78ms
step:376/2160 train_time:12701ms step_avg:33.78ms
step:377/2160 train_time:12735ms step_avg:33.78ms
step:378/2160 train_time:12768ms step_avg:33.78ms
step:379/2160 train_time:12802ms step_avg:33.78ms
step:380/2160 train_time:12835ms step_avg:33.78ms
step:381/2160 train_time:12869ms step_avg:33.78ms
step:382/2160 train_time:12902ms step_avg:33.78ms
step:383/2160 train_time:12936ms step_avg:33.78ms
step:384/2160 train_time:12969ms step_avg:33.77ms
step:385/2160 train_time:13003ms step_avg:33.78ms
step:386/2160 train_time:13036ms step_avg:33.77ms
step:387/2160 train_time:13071ms step_avg:33.77ms
step:388/2160 train_time:13103ms step_avg:33.77ms
step:389/2160 train_time:13138ms step_avg:33.77ms
step:390/2160 train_time:13171ms step_avg:33.77ms
step:391/2160 train_time:13205ms step_avg:33.77ms
step:392/2160 train_time:13238ms step_avg:33.77ms
step:393/2160 train_time:13272ms step_avg:33.77ms
step:394/2160 train_time:13305ms step_avg:33.77ms
step:395/2160 train_time:13339ms step_avg:33.77ms
step:396/2160 train_time:13372ms step_avg:33.77ms
step:397/2160 train_time:13406ms step_avg:33.77ms
step:398/2160 train_time:13439ms step_avg:33.77ms
step:399/2160 train_time:13473ms step_avg:33.77ms
step:400/2160 train_time:13506ms step_avg:33.77ms
step:401/2160 train_time:13540ms step_avg:33.77ms
step:402/2160 train_time:13573ms step_avg:33.76ms
step:403/2160 train_time:13607ms step_avg:33.77ms
step:404/2160 train_time:13640ms step_avg:33.76ms
step:405/2160 train_time:13675ms step_avg:33.76ms
step:406/2160 train_time:13708ms step_avg:33.76ms
step:407/2160 train_time:13742ms step_avg:33.76ms
step:408/2160 train_time:13776ms step_avg:33.76ms
step:409/2160 train_time:13809ms step_avg:33.76ms
step:410/2160 train_time:13842ms step_avg:33.76ms
step:411/2160 train_time:13876ms step_avg:33.76ms
step:412/2160 train_time:13909ms step_avg:33.76ms
step:413/2160 train_time:13944ms step_avg:33.76ms
step:414/2160 train_time:13977ms step_avg:33.76ms
step:415/2160 train_time:14011ms step_avg:33.76ms
step:416/2160 train_time:14044ms step_avg:33.76ms
step:417/2160 train_time:14078ms step_avg:33.76ms
step:418/2160 train_time:14111ms step_avg:33.76ms
step:419/2160 train_time:14145ms step_avg:33.76ms
step:420/2160 train_time:14179ms step_avg:33.76ms
step:421/2160 train_time:14213ms step_avg:33.76ms
step:422/2160 train_time:14246ms step_avg:33.76ms
step:423/2160 train_time:14279ms step_avg:33.76ms
step:424/2160 train_time:14312ms step_avg:33.76ms
step:425/2160 train_time:14346ms step_avg:33.76ms
step:426/2160 train_time:14380ms step_avg:33.75ms
step:427/2160 train_time:14413ms step_avg:33.76ms
step:428/2160 train_time:14446ms step_avg:33.75ms
step:429/2160 train_time:14481ms step_avg:33.75ms
step:430/2160 train_time:14513ms step_avg:33.75ms
step:431/2160 train_time:14548ms step_avg:33.75ms
step:432/2160 train_time:14581ms step_avg:33.75ms
step:433/2160 train_time:14615ms step_avg:33.75ms
step:434/2160 train_time:14648ms step_avg:33.75ms
step:435/2160 train_time:14682ms step_avg:33.75ms
step:436/2160 train_time:14715ms step_avg:33.75ms
step:437/2160 train_time:14749ms step_avg:33.75ms
step:438/2160 train_time:14782ms step_avg:33.75ms
step:439/2160 train_time:14816ms step_avg:33.75ms
step:440/2160 train_time:14849ms step_avg:33.75ms
step:441/2160 train_time:14883ms step_avg:33.75ms
step:442/2160 train_time:14916ms step_avg:33.75ms
step:443/2160 train_time:14951ms step_avg:33.75ms
step:444/2160 train_time:14984ms step_avg:33.75ms
step:445/2160 train_time:15018ms step_avg:33.75ms
step:446/2160 train_time:15051ms step_avg:33.75ms
step:447/2160 train_time:15085ms step_avg:33.75ms
step:448/2160 train_time:15118ms step_avg:33.75ms
step:449/2160 train_time:15152ms step_avg:33.75ms
step:450/2160 train_time:15185ms step_avg:33.74ms
step:451/2160 train_time:15219ms step_avg:33.75ms
step:452/2160 train_time:15252ms step_avg:33.74ms
step:453/2160 train_time:15286ms step_avg:33.74ms
step:454/2160 train_time:15320ms step_avg:33.74ms
step:455/2160 train_time:15354ms step_avg:33.74ms
step:456/2160 train_time:15387ms step_avg:33.74ms
step:457/2160 train_time:15421ms step_avg:33.74ms
step:458/2160 train_time:15454ms step_avg:33.74ms
step:459/2160 train_time:15488ms step_avg:33.74ms
step:460/2160 train_time:15521ms step_avg:33.74ms
step:461/2160 train_time:15555ms step_avg:33.74ms
step:462/2160 train_time:15588ms step_avg:33.74ms
step:463/2160 train_time:15623ms step_avg:33.74ms
step:464/2160 train_time:15656ms step_avg:33.74ms
step:465/2160 train_time:15690ms step_avg:33.74ms
step:466/2160 train_time:15723ms step_avg:33.74ms
step:467/2160 train_time:15757ms step_avg:33.74ms
step:468/2160 train_time:15790ms step_avg:33.74ms
step:469/2160 train_time:15825ms step_avg:33.74ms
step:470/2160 train_time:15858ms step_avg:33.74ms
step:471/2160 train_time:15892ms step_avg:33.74ms
step:472/2160 train_time:15925ms step_avg:33.74ms
step:473/2160 train_time:15958ms step_avg:33.74ms
step:474/2160 train_time:15992ms step_avg:33.74ms
step:475/2160 train_time:16025ms step_avg:33.74ms
step:476/2160 train_time:16059ms step_avg:33.74ms
step:477/2160 train_time:16093ms step_avg:33.74ms
step:478/2160 train_time:16126ms step_avg:33.74ms
step:479/2160 train_time:16160ms step_avg:33.74ms
step:480/2160 train_time:16193ms step_avg:33.74ms
step:481/2160 train_time:16227ms step_avg:33.74ms
step:482/2160 train_time:16260ms step_avg:33.74ms
step:483/2160 train_time:16295ms step_avg:33.74ms
step:484/2160 train_time:16327ms step_avg:33.73ms
step:485/2160 train_time:16362ms step_avg:33.74ms
step:486/2160 train_time:16395ms step_avg:33.73ms
step:487/2160 train_time:16429ms step_avg:33.74ms
step:488/2160 train_time:16463ms step_avg:33.73ms
step:489/2160 train_time:16497ms step_avg:33.74ms
step:490/2160 train_time:16530ms step_avg:33.73ms
step:491/2160 train_time:16563ms step_avg:33.73ms
step:492/2160 train_time:16596ms step_avg:33.73ms
step:493/2160 train_time:16630ms step_avg:33.73ms
step:494/2160 train_time:16663ms step_avg:33.73ms
step:495/2160 train_time:16697ms step_avg:33.73ms
step:496/2160 train_time:16730ms step_avg:33.73ms
step:497/2160 train_time:16764ms step_avg:33.73ms
step:498/2160 train_time:16798ms step_avg:33.73ms
step:499/2160 train_time:16832ms step_avg:33.73ms
step:500/2160 train_time:16865ms step_avg:33.73ms
step:500/2160 val_loss:4.0093 train_time:16900ms step_avg:33.80ms
step:501/2160 train_time:16921ms step_avg:33.77ms
step:502/2160 train_time:16940ms step_avg:33.74ms
step:503/2160 train_time:16968ms step_avg:33.73ms
step:504/2160 train_time:17002ms step_avg:33.73ms
step:505/2160 train_time:17038ms step_avg:33.74ms
step:506/2160 train_time:17071ms step_avg:33.74ms
step:507/2160 train_time:17105ms step_avg:33.74ms
step:508/2160 train_time:17139ms step_avg:33.74ms
step:509/2160 train_time:17173ms step_avg:33.74ms
step:510/2160 train_time:17206ms step_avg:33.74ms
step:511/2160 train_time:17240ms step_avg:33.74ms
step:512/2160 train_time:17273ms step_avg:33.74ms
step:513/2160 train_time:17306ms step_avg:33.74ms
step:514/2160 train_time:17339ms step_avg:33.73ms
step:515/2160 train_time:17373ms step_avg:33.73ms
step:516/2160 train_time:17406ms step_avg:33.73ms
step:517/2160 train_time:17440ms step_avg:33.73ms
step:518/2160 train_time:17473ms step_avg:33.73ms
step:519/2160 train_time:17506ms step_avg:33.73ms
step:520/2160 train_time:17539ms step_avg:33.73ms
step:521/2160 train_time:17573ms step_avg:33.73ms
step:522/2160 train_time:17606ms step_avg:33.73ms
step:523/2160 train_time:17640ms step_avg:33.73ms
step:524/2160 train_time:17673ms step_avg:33.73ms
step:525/2160 train_time:17707ms step_avg:33.73ms
step:526/2160 train_time:17740ms step_avg:33.73ms
step:527/2160 train_time:17774ms step_avg:33.73ms
step:528/2160 train_time:17807ms step_avg:33.72ms
step:529/2160 train_time:17841ms step_avg:33.73ms
step:530/2160 train_time:17875ms step_avg:33.73ms
step:531/2160 train_time:17909ms step_avg:33.73ms
step:532/2160 train_time:17943ms step_avg:33.73ms
step:533/2160 train_time:17977ms step_avg:33.73ms
step:534/2160 train_time:18010ms step_avg:33.73ms
step:535/2160 train_time:18045ms step_avg:33.73ms
step:536/2160 train_time:18078ms step_avg:33.73ms
step:537/2160 train_time:18112ms step_avg:33.73ms
step:538/2160 train_time:18146ms step_avg:33.73ms
step:539/2160 train_time:18180ms step_avg:33.73ms
step:540/2160 train_time:18212ms step_avg:33.73ms
step:541/2160 train_time:18247ms step_avg:33.73ms
step:542/2160 train_time:18280ms step_avg:33.73ms
step:543/2160 train_time:18314ms step_avg:33.73ms
step:544/2160 train_time:18347ms step_avg:33.73ms
step:545/2160 train_time:18382ms step_avg:33.73ms
step:546/2160 train_time:18414ms step_avg:33.73ms
step:547/2160 train_time:18449ms step_avg:33.73ms
step:548/2160 train_time:18482ms step_avg:33.73ms
step:549/2160 train_time:18516ms step_avg:33.73ms
step:550/2160 train_time:18549ms step_avg:33.72ms
step:551/2160 train_time:18583ms step_avg:33.73ms
step:552/2160 train_time:18616ms step_avg:33.72ms
step:553/2160 train_time:18649ms step_avg:33.72ms
step:554/2160 train_time:18682ms step_avg:33.72ms
step:555/2160 train_time:18716ms step_avg:33.72ms
step:556/2160 train_time:18750ms step_avg:33.72ms
step:557/2160 train_time:18784ms step_avg:33.72ms
step:558/2160 train_time:18817ms step_avg:33.72ms
step:559/2160 train_time:18851ms step_avg:33.72ms
step:560/2160 train_time:18884ms step_avg:33.72ms
step:561/2160 train_time:18918ms step_avg:33.72ms
step:562/2160 train_time:18951ms step_avg:33.72ms
step:563/2160 train_time:18986ms step_avg:33.72ms
step:564/2160 train_time:19020ms step_avg:33.72ms
step:565/2160 train_time:19054ms step_avg:33.72ms
step:566/2160 train_time:19087ms step_avg:33.72ms
step:567/2160 train_time:19121ms step_avg:33.72ms
step:568/2160 train_time:19155ms step_avg:33.72ms
step:569/2160 train_time:19189ms step_avg:33.72ms
step:570/2160 train_time:19222ms step_avg:33.72ms
step:571/2160 train_time:19256ms step_avg:33.72ms
step:572/2160 train_time:19289ms step_avg:33.72ms
step:573/2160 train_time:19324ms step_avg:33.72ms
step:574/2160 train_time:19357ms step_avg:33.72ms
step:575/2160 train_time:19391ms step_avg:33.72ms
step:576/2160 train_time:19424ms step_avg:33.72ms
step:577/2160 train_time:19458ms step_avg:33.72ms
step:578/2160 train_time:19491ms step_avg:33.72ms
step:579/2160 train_time:19525ms step_avg:33.72ms
step:580/2160 train_time:19558ms step_avg:33.72ms
step:581/2160 train_time:19592ms step_avg:33.72ms
step:582/2160 train_time:19625ms step_avg:33.72ms
step:583/2160 train_time:19659ms step_avg:33.72ms
step:584/2160 train_time:19692ms step_avg:33.72ms
step:585/2160 train_time:19726ms step_avg:33.72ms
step:586/2160 train_time:19759ms step_avg:33.72ms
step:587/2160 train_time:19793ms step_avg:33.72ms
step:588/2160 train_time:19826ms step_avg:33.72ms
step:589/2160 train_time:19860ms step_avg:33.72ms
step:590/2160 train_time:19894ms step_avg:33.72ms
step:591/2160 train_time:19928ms step_avg:33.72ms
step:592/2160 train_time:19961ms step_avg:33.72ms
step:593/2160 train_time:19995ms step_avg:33.72ms
step:594/2160 train_time:20028ms step_avg:33.72ms
step:595/2160 train_time:20063ms step_avg:33.72ms
step:596/2160 train_time:20096ms step_avg:33.72ms
step:597/2160 train_time:20130ms step_avg:33.72ms
step:598/2160 train_time:20163ms step_avg:33.72ms
step:599/2160 train_time:20197ms step_avg:33.72ms
step:600/2160 train_time:20230ms step_avg:33.72ms
step:601/2160 train_time:20264ms step_avg:33.72ms
step:602/2160 train_time:20298ms step_avg:33.72ms
step:603/2160 train_time:20332ms step_avg:33.72ms
step:604/2160 train_time:20365ms step_avg:33.72ms
step:605/2160 train_time:20399ms step_avg:33.72ms
step:606/2160 train_time:20432ms step_avg:33.72ms
step:607/2160 train_time:20466ms step_avg:33.72ms
step:608/2160 train_time:20500ms step_avg:33.72ms
step:609/2160 train_time:20534ms step_avg:33.72ms
step:610/2160 train_time:20567ms step_avg:33.72ms
step:611/2160 train_time:20601ms step_avg:33.72ms
step:612/2160 train_time:20634ms step_avg:33.72ms
step:613/2160 train_time:20668ms step_avg:33.72ms
step:614/2160 train_time:20701ms step_avg:33.72ms
step:615/2160 train_time:20735ms step_avg:33.72ms
step:616/2160 train_time:20768ms step_avg:33.71ms
step:617/2160 train_time:20802ms step_avg:33.72ms
step:618/2160 train_time:20836ms step_avg:33.71ms
step:619/2160 train_time:20870ms step_avg:33.72ms
step:620/2160 train_time:20903ms step_avg:33.71ms
step:621/2160 train_time:20937ms step_avg:33.72ms
step:622/2160 train_time:20970ms step_avg:33.71ms
step:623/2160 train_time:21005ms step_avg:33.72ms
step:624/2160 train_time:21038ms step_avg:33.71ms
step:625/2160 train_time:21072ms step_avg:33.71ms
step:626/2160 train_time:21105ms step_avg:33.71ms
step:627/2160 train_time:21139ms step_avg:33.71ms
step:628/2160 train_time:21172ms step_avg:33.71ms
step:629/2160 train_time:21207ms step_avg:33.71ms
step:630/2160 train_time:21240ms step_avg:33.71ms
step:631/2160 train_time:21274ms step_avg:33.71ms
step:632/2160 train_time:21307ms step_avg:33.71ms
step:633/2160 train_time:21341ms step_avg:33.71ms
step:634/2160 train_time:21374ms step_avg:33.71ms
step:635/2160 train_time:21408ms step_avg:33.71ms
step:636/2160 train_time:21441ms step_avg:33.71ms
step:637/2160 train_time:21475ms step_avg:33.71ms
step:638/2160 train_time:21508ms step_avg:33.71ms
step:639/2160 train_time:21542ms step_avg:33.71ms
step:640/2160 train_time:21575ms step_avg:33.71ms
step:641/2160 train_time:21609ms step_avg:33.71ms
step:642/2160 train_time:21642ms step_avg:33.71ms
step:643/2160 train_time:21676ms step_avg:33.71ms
step:644/2160 train_time:21708ms step_avg:33.71ms
step:645/2160 train_time:21743ms step_avg:33.71ms
step:646/2160 train_time:21776ms step_avg:33.71ms
step:647/2160 train_time:21811ms step_avg:33.71ms
step:648/2160 train_time:21844ms step_avg:33.71ms
step:649/2160 train_time:21878ms step_avg:33.71ms
step:650/2160 train_time:21911ms step_avg:33.71ms
step:651/2160 train_time:21945ms step_avg:33.71ms
step:652/2160 train_time:21979ms step_avg:33.71ms
step:653/2160 train_time:22013ms step_avg:33.71ms
step:654/2160 train_time:22046ms step_avg:33.71ms
step:655/2160 train_time:22080ms step_avg:33.71ms
step:656/2160 train_time:22113ms step_avg:33.71ms
step:657/2160 train_time:22148ms step_avg:33.71ms
step:658/2160 train_time:22181ms step_avg:33.71ms
step:659/2160 train_time:22215ms step_avg:33.71ms
step:660/2160 train_time:22248ms step_avg:33.71ms
step:661/2160 train_time:22282ms step_avg:33.71ms
step:662/2160 train_time:22315ms step_avg:33.71ms
step:663/2160 train_time:22349ms step_avg:33.71ms
step:664/2160 train_time:22382ms step_avg:33.71ms
step:665/2160 train_time:22416ms step_avg:33.71ms
step:666/2160 train_time:22450ms step_avg:33.71ms
step:667/2160 train_time:22484ms step_avg:33.71ms
step:668/2160 train_time:22517ms step_avg:33.71ms
step:669/2160 train_time:22551ms step_avg:33.71ms
step:670/2160 train_time:22584ms step_avg:33.71ms
step:671/2160 train_time:22617ms step_avg:33.71ms
step:672/2160 train_time:22650ms step_avg:33.71ms
step:673/2160 train_time:22685ms step_avg:33.71ms
step:674/2160 train_time:22718ms step_avg:33.71ms
step:675/2160 train_time:22752ms step_avg:33.71ms
step:676/2160 train_time:22785ms step_avg:33.71ms
step:677/2160 train_time:22819ms step_avg:33.71ms
step:678/2160 train_time:22852ms step_avg:33.71ms
step:679/2160 train_time:22887ms step_avg:33.71ms
step:680/2160 train_time:22920ms step_avg:33.71ms
step:681/2160 train_time:22954ms step_avg:33.71ms
step:682/2160 train_time:22987ms step_avg:33.71ms
step:683/2160 train_time:23021ms step_avg:33.71ms
step:684/2160 train_time:23054ms step_avg:33.70ms
step:685/2160 train_time:23088ms step_avg:33.71ms
step:686/2160 train_time:23121ms step_avg:33.70ms
step:687/2160 train_time:23156ms step_avg:33.71ms
step:688/2160 train_time:23189ms step_avg:33.70ms
step:689/2160 train_time:23223ms step_avg:33.71ms
step:690/2160 train_time:23256ms step_avg:33.70ms
step:691/2160 train_time:23291ms step_avg:33.71ms
step:692/2160 train_time:23324ms step_avg:33.70ms
step:693/2160 train_time:23358ms step_avg:33.71ms
step:694/2160 train_time:23391ms step_avg:33.70ms
step:695/2160 train_time:23425ms step_avg:33.71ms
step:696/2160 train_time:23459ms step_avg:33.70ms
step:697/2160 train_time:23493ms step_avg:33.71ms
step:698/2160 train_time:23526ms step_avg:33.70ms
step:699/2160 train_time:23560ms step_avg:33.71ms
step:700/2160 train_time:23593ms step_avg:33.70ms
step:701/2160 train_time:23627ms step_avg:33.70ms
step:702/2160 train_time:23660ms step_avg:33.70ms
step:703/2160 train_time:23694ms step_avg:33.70ms
step:704/2160 train_time:23727ms step_avg:33.70ms
step:705/2160 train_time:23761ms step_avg:33.70ms
step:706/2160 train_time:23794ms step_avg:33.70ms
step:707/2160 train_time:23829ms step_avg:33.70ms
step:708/2160 train_time:23863ms step_avg:33.70ms
step:709/2160 train_time:23923ms step_avg:33.74ms
step:710/2160 train_time:23982ms step_avg:33.78ms
step:711/2160 train_time:24044ms step_avg:33.82ms
step:712/2160 train_time:24104ms step_avg:33.85ms
step:713/2160 train_time:24165ms step_avg:33.89ms
step:714/2160 train_time:24225ms step_avg:33.93ms
step:715/2160 train_time:24286ms step_avg:33.97ms
step:716/2160 train_time:24345ms step_avg:34.00ms
step:717/2160 train_time:24406ms step_avg:34.04ms
step:718/2160 train_time:24466ms step_avg:34.08ms
step:719/2160 train_time:24528ms step_avg:34.11ms
step:720/2160 train_time:24587ms step_avg:34.15ms
step:721/2160 train_time:24648ms step_avg:34.19ms
step:722/2160 train_time:24708ms step_avg:34.22ms
step:723/2160 train_time:24770ms step_avg:34.26ms
step:724/2160 train_time:24829ms step_avg:34.29ms
step:725/2160 train_time:24890ms step_avg:34.33ms
step:726/2160 train_time:24950ms step_avg:34.37ms
step:727/2160 train_time:25011ms step_avg:34.40ms
step:728/2160 train_time:25071ms step_avg:34.44ms
step:729/2160 train_time:25133ms step_avg:34.48ms
step:730/2160 train_time:25193ms step_avg:34.51ms
step:731/2160 train_time:25255ms step_avg:34.55ms
step:732/2160 train_time:25314ms step_avg:34.58ms
step:733/2160 train_time:25375ms step_avg:34.62ms
step:734/2160 train_time:25435ms step_avg:34.65ms
step:735/2160 train_time:25496ms step_avg:34.69ms
step:736/2160 train_time:25555ms step_avg:34.72ms
step:737/2160 train_time:25617ms step_avg:34.76ms
step:738/2160 train_time:25676ms step_avg:34.79ms
step:739/2160 train_time:25738ms step_avg:34.83ms
step:740/2160 train_time:25797ms step_avg:34.86ms
step:741/2160 train_time:25859ms step_avg:34.90ms
step:742/2160 train_time:25918ms step_avg:34.93ms
step:743/2160 train_time:25980ms step_avg:34.97ms
step:744/2160 train_time:26040ms step_avg:35.00ms
step:745/2160 train_time:26101ms step_avg:35.04ms
step:746/2160 train_time:26161ms step_avg:35.07ms
step:747/2160 train_time:26222ms step_avg:35.10ms
step:748/2160 train_time:26282ms step_avg:35.14ms
step:749/2160 train_time:26343ms step_avg:35.17ms
step:750/2160 train_time:26403ms step_avg:35.20ms
step:750/2160 val_loss:3.8597 train_time:26465ms step_avg:35.29ms
step:751/2160 train_time:26485ms step_avg:35.27ms
step:752/2160 train_time:26525ms step_avg:35.27ms
step:753/2160 train_time:26585ms step_avg:35.31ms
step:754/2160 train_time:26645ms step_avg:35.34ms
step:755/2160 train_time:26707ms step_avg:35.37ms
step:756/2160 train_time:26767ms step_avg:35.41ms
step:757/2160 train_time:26828ms step_avg:35.44ms
step:758/2160 train_time:26886ms step_avg:35.47ms
step:759/2160 train_time:26947ms step_avg:35.50ms
step:760/2160 train_time:27006ms step_avg:35.53ms
step:761/2160 train_time:27066ms step_avg:35.57ms
step:762/2160 train_time:27125ms step_avg:35.60ms
step:763/2160 train_time:27186ms step_avg:35.63ms
step:764/2160 train_time:27246ms step_avg:35.66ms
step:765/2160 train_time:27307ms step_avg:35.70ms
step:766/2160 train_time:27370ms step_avg:35.73ms
step:767/2160 train_time:27439ms step_avg:35.77ms
step:768/2160 train_time:27499ms step_avg:35.81ms
step:769/2160 train_time:27561ms step_avg:35.84ms
step:770/2160 train_time:27620ms step_avg:35.87ms
step:771/2160 train_time:27681ms step_avg:35.90ms
step:772/2160 train_time:27741ms step_avg:35.93ms
step:773/2160 train_time:27802ms step_avg:35.97ms
step:774/2160 train_time:27861ms step_avg:36.00ms
step:775/2160 train_time:27922ms step_avg:36.03ms
step:776/2160 train_time:27981ms step_avg:36.06ms
step:777/2160 train_time:28042ms step_avg:36.09ms
step:778/2160 train_time:28101ms step_avg:36.12ms
step:779/2160 train_time:28162ms step_avg:36.15ms
step:780/2160 train_time:28221ms step_avg:36.18ms
step:781/2160 train_time:28283ms step_avg:36.21ms
step:782/2160 train_time:28343ms step_avg:36.24ms
step:783/2160 train_time:28406ms step_avg:36.28ms
step:784/2160 train_time:28468ms step_avg:36.31ms
step:785/2160 train_time:28530ms step_avg:36.34ms
step:786/2160 train_time:28590ms step_avg:36.37ms
step:787/2160 train_time:28651ms step_avg:36.41ms
step:788/2160 train_time:28711ms step_avg:36.43ms
step:789/2160 train_time:28771ms step_avg:36.47ms
step:790/2160 train_time:28830ms step_avg:36.49ms
step:791/2160 train_time:28891ms step_avg:36.52ms
step:792/2160 train_time:28950ms step_avg:36.55ms
step:793/2160 train_time:29011ms step_avg:36.58ms
step:794/2160 train_time:29069ms step_avg:36.61ms
step:795/2160 train_time:29130ms step_avg:36.64ms
step:796/2160 train_time:29190ms step_avg:36.67ms
step:797/2160 train_time:29251ms step_avg:36.70ms
step:798/2160 train_time:29311ms step_avg:36.73ms
step:799/2160 train_time:29372ms step_avg:36.76ms
step:800/2160 train_time:29432ms step_avg:36.79ms
step:801/2160 train_time:29494ms step_avg:36.82ms
step:802/2160 train_time:29554ms step_avg:36.85ms
step:803/2160 train_time:29615ms step_avg:36.88ms
step:804/2160 train_time:29674ms step_avg:36.91ms
step:805/2160 train_time:29735ms step_avg:36.94ms
step:806/2160 train_time:29794ms step_avg:36.97ms
step:807/2160 train_time:29855ms step_avg:37.00ms
step:808/2160 train_time:29916ms step_avg:37.02ms
step:809/2160 train_time:29977ms step_avg:37.05ms
step:810/2160 train_time:30036ms step_avg:37.08ms
step:811/2160 train_time:30097ms step_avg:37.11ms
step:812/2160 train_time:30157ms step_avg:37.14ms
step:813/2160 train_time:30219ms step_avg:37.17ms
step:814/2160 train_time:30278ms step_avg:37.20ms
step:815/2160 train_time:30340ms step_avg:37.23ms
step:816/2160 train_time:30400ms step_avg:37.25ms
step:817/2160 train_time:30461ms step_avg:37.28ms
step:818/2160 train_time:30521ms step_avg:37.31ms
step:819/2160 train_time:30582ms step_avg:37.34ms
step:820/2160 train_time:30642ms step_avg:37.37ms
step:821/2160 train_time:30703ms step_avg:37.40ms
step:822/2160 train_time:30762ms step_avg:37.42ms
step:823/2160 train_time:30823ms step_avg:37.45ms
step:824/2160 train_time:30883ms step_avg:37.48ms
step:825/2160 train_time:30944ms step_avg:37.51ms
step:826/2160 train_time:31004ms step_avg:37.54ms
step:827/2160 train_time:31065ms step_avg:37.56ms
step:828/2160 train_time:31124ms step_avg:37.59ms
step:829/2160 train_time:31186ms step_avg:37.62ms
step:830/2160 train_time:31245ms step_avg:37.64ms
step:831/2160 train_time:31307ms step_avg:37.67ms
step:832/2160 train_time:31366ms step_avg:37.70ms
step:833/2160 train_time:31428ms step_avg:37.73ms
step:834/2160 train_time:31488ms step_avg:37.75ms
step:835/2160 train_time:31549ms step_avg:37.78ms
step:836/2160 train_time:31609ms step_avg:37.81ms
step:837/2160 train_time:31670ms step_avg:37.84ms
step:838/2160 train_time:31729ms step_avg:37.86ms
step:839/2160 train_time:31791ms step_avg:37.89ms
step:840/2160 train_time:31851ms step_avg:37.92ms
step:841/2160 train_time:31912ms step_avg:37.95ms
step:842/2160 train_time:31972ms step_avg:37.97ms
step:843/2160 train_time:32033ms step_avg:38.00ms
step:844/2160 train_time:32092ms step_avg:38.02ms
step:845/2160 train_time:32153ms step_avg:38.05ms
step:846/2160 train_time:32212ms step_avg:38.08ms
step:847/2160 train_time:32274ms step_avg:38.10ms
step:848/2160 train_time:32333ms step_avg:38.13ms
step:849/2160 train_time:32394ms step_avg:38.16ms
step:850/2160 train_time:32454ms step_avg:38.18ms
step:851/2160 train_time:32516ms step_avg:38.21ms
step:852/2160 train_time:32575ms step_avg:38.23ms
step:853/2160 train_time:32637ms step_avg:38.26ms
step:854/2160 train_time:32697ms step_avg:38.29ms
step:855/2160 train_time:32758ms step_avg:38.31ms
step:856/2160 train_time:32818ms step_avg:38.34ms
step:857/2160 train_time:32880ms step_avg:38.37ms
step:858/2160 train_time:32939ms step_avg:38.39ms
step:859/2160 train_time:33000ms step_avg:38.42ms
step:860/2160 train_time:33060ms step_avg:38.44ms
step:861/2160 train_time:33121ms step_avg:38.47ms
step:862/2160 train_time:33180ms step_avg:38.49ms
step:863/2160 train_time:33241ms step_avg:38.52ms
step:864/2160 train_time:33301ms step_avg:38.54ms
step:865/2160 train_time:33362ms step_avg:38.57ms
step:866/2160 train_time:33421ms step_avg:38.59ms
step:867/2160 train_time:33483ms step_avg:38.62ms
step:868/2160 train_time:33543ms step_avg:38.64ms
step:869/2160 train_time:33604ms step_avg:38.67ms
step:870/2160 train_time:33664ms step_avg:38.69ms
step:871/2160 train_time:33726ms step_avg:38.72ms
step:872/2160 train_time:33785ms step_avg:38.74ms
step:873/2160 train_time:33847ms step_avg:38.77ms
step:874/2160 train_time:33906ms step_avg:38.79ms
step:875/2160 train_time:33968ms step_avg:38.82ms
step:876/2160 train_time:34027ms step_avg:38.84ms
step:877/2160 train_time:34088ms step_avg:38.87ms
step:878/2160 train_time:34148ms step_avg:38.89ms
step:879/2160 train_time:34209ms step_avg:38.92ms
step:880/2160 train_time:34268ms step_avg:38.94ms
step:881/2160 train_time:34329ms step_avg:38.97ms
step:882/2160 train_time:34388ms step_avg:38.99ms
step:883/2160 train_time:34449ms step_avg:39.01ms
step:884/2160 train_time:34508ms step_avg:39.04ms
step:885/2160 train_time:34570ms step_avg:39.06ms
step:886/2160 train_time:34629ms step_avg:39.08ms
step:887/2160 train_time:34690ms step_avg:39.11ms
step:888/2160 train_time:34751ms step_avg:39.13ms
step:889/2160 train_time:34812ms step_avg:39.16ms
step:890/2160 train_time:34872ms step_avg:39.18ms
step:891/2160 train_time:34932ms step_avg:39.21ms
step:892/2160 train_time:34992ms step_avg:39.23ms
step:893/2160 train_time:35052ms step_avg:39.25ms
step:894/2160 train_time:35112ms step_avg:39.27ms
step:895/2160 train_time:35173ms step_avg:39.30ms
step:896/2160 train_time:35232ms step_avg:39.32ms
step:897/2160 train_time:35293ms step_avg:39.35ms
step:898/2160 train_time:35354ms step_avg:39.37ms
step:899/2160 train_time:35415ms step_avg:39.39ms
step:900/2160 train_time:35474ms step_avg:39.42ms
step:901/2160 train_time:35536ms step_avg:39.44ms
step:902/2160 train_time:35596ms step_avg:39.46ms
step:903/2160 train_time:35658ms step_avg:39.49ms
step:904/2160 train_time:35718ms step_avg:39.51ms
step:905/2160 train_time:35779ms step_avg:39.53ms
step:906/2160 train_time:35838ms step_avg:39.56ms
step:907/2160 train_time:35899ms step_avg:39.58ms
step:908/2160 train_time:35959ms step_avg:39.60ms
step:909/2160 train_time:36020ms step_avg:39.63ms
step:910/2160 train_time:36079ms step_avg:39.65ms
step:911/2160 train_time:36141ms step_avg:39.67ms
step:912/2160 train_time:36201ms step_avg:39.69ms
step:913/2160 train_time:36262ms step_avg:39.72ms
step:914/2160 train_time:36322ms step_avg:39.74ms
step:915/2160 train_time:36383ms step_avg:39.76ms
step:916/2160 train_time:36443ms step_avg:39.78ms
step:917/2160 train_time:36504ms step_avg:39.81ms
step:918/2160 train_time:36564ms step_avg:39.83ms
step:919/2160 train_time:36626ms step_avg:39.85ms
step:920/2160 train_time:36686ms step_avg:39.88ms
step:921/2160 train_time:36747ms step_avg:39.90ms
step:922/2160 train_time:36807ms step_avg:39.92ms
step:923/2160 train_time:36868ms step_avg:39.94ms
step:924/2160 train_time:36927ms step_avg:39.96ms
step:925/2160 train_time:36988ms step_avg:39.99ms
step:926/2160 train_time:37048ms step_avg:40.01ms
step:927/2160 train_time:37109ms step_avg:40.03ms
step:928/2160 train_time:37168ms step_avg:40.05ms
step:929/2160 train_time:37230ms step_avg:40.07ms
step:930/2160 train_time:37289ms step_avg:40.10ms
step:931/2160 train_time:37351ms step_avg:40.12ms
step:932/2160 train_time:37410ms step_avg:40.14ms
step:933/2160 train_time:37471ms step_avg:40.16ms
step:934/2160 train_time:37530ms step_avg:40.18ms
step:935/2160 train_time:37591ms step_avg:40.20ms
step:936/2160 train_time:37651ms step_avg:40.23ms
step:937/2160 train_time:37712ms step_avg:40.25ms
step:938/2160 train_time:37772ms step_avg:40.27ms
step:939/2160 train_time:37832ms step_avg:40.29ms
step:940/2160 train_time:37891ms step_avg:40.31ms
step:941/2160 train_time:37953ms step_avg:40.33ms
step:942/2160 train_time:38012ms step_avg:40.35ms
step:943/2160 train_time:38073ms step_avg:40.37ms
step:944/2160 train_time:38132ms step_avg:40.39ms
step:945/2160 train_time:38194ms step_avg:40.42ms
step:946/2160 train_time:38253ms step_avg:40.44ms
step:947/2160 train_time:38315ms step_avg:40.46ms
step:948/2160 train_time:38374ms step_avg:40.48ms
step:949/2160 train_time:38435ms step_avg:40.50ms
step:950/2160 train_time:38494ms step_avg:40.52ms
step:951/2160 train_time:38556ms step_avg:40.54ms
step:952/2160 train_time:38616ms step_avg:40.56ms
step:953/2160 train_time:38677ms step_avg:40.58ms
step:954/2160 train_time:38737ms step_avg:40.60ms
step:955/2160 train_time:38798ms step_avg:40.63ms
step:956/2160 train_time:38857ms step_avg:40.65ms
step:957/2160 train_time:38918ms step_avg:40.67ms
step:958/2160 train_time:38977ms step_avg:40.69ms
step:959/2160 train_time:39039ms step_avg:40.71ms
step:960/2160 train_time:39099ms step_avg:40.73ms
step:961/2160 train_time:39160ms step_avg:40.75ms
step:962/2160 train_time:39220ms step_avg:40.77ms
step:963/2160 train_time:39281ms step_avg:40.79ms
step:964/2160 train_time:39341ms step_avg:40.81ms
step:965/2160 train_time:39403ms step_avg:40.83ms
step:966/2160 train_time:39462ms step_avg:40.85ms
step:967/2160 train_time:39524ms step_avg:40.87ms
step:968/2160 train_time:39584ms step_avg:40.89ms
step:969/2160 train_time:39646ms step_avg:40.91ms
step:970/2160 train_time:39705ms step_avg:40.93ms
step:971/2160 train_time:39766ms step_avg:40.95ms
step:972/2160 train_time:39825ms step_avg:40.97ms
step:973/2160 train_time:39887ms step_avg:40.99ms
step:974/2160 train_time:39947ms step_avg:41.01ms
step:975/2160 train_time:40007ms step_avg:41.03ms
step:976/2160 train_time:40067ms step_avg:41.05ms
step:977/2160 train_time:40129ms step_avg:41.07ms
step:978/2160 train_time:40188ms step_avg:41.09ms
step:979/2160 train_time:40249ms step_avg:41.11ms
step:980/2160 train_time:40308ms step_avg:41.13ms
step:981/2160 train_time:40369ms step_avg:41.15ms
step:982/2160 train_time:40429ms step_avg:41.17ms
step:983/2160 train_time:40491ms step_avg:41.19ms
step:984/2160 train_time:40551ms step_avg:41.21ms
step:985/2160 train_time:40611ms step_avg:41.23ms
step:986/2160 train_time:40671ms step_avg:41.25ms
step:987/2160 train_time:40732ms step_avg:41.27ms
step:988/2160 train_time:40791ms step_avg:41.29ms
step:989/2160 train_time:40852ms step_avg:41.31ms
step:990/2160 train_time:40911ms step_avg:41.32ms
step:991/2160 train_time:40972ms step_avg:41.34ms
step:992/2160 train_time:41031ms step_avg:41.36ms
step:993/2160 train_time:41093ms step_avg:41.38ms
step:994/2160 train_time:41152ms step_avg:41.40ms
step:995/2160 train_time:41213ms step_avg:41.42ms
step:996/2160 train_time:41273ms step_avg:41.44ms
step:997/2160 train_time:41335ms step_avg:41.46ms
step:998/2160 train_time:41394ms step_avg:41.48ms
step:999/2160 train_time:41456ms step_avg:41.50ms
step:1000/2160 train_time:41515ms step_avg:41.52ms
step:1000/2160 val_loss:3.6971 train_time:41578ms step_avg:41.58ms
step:1001/2160 train_time:41597ms step_avg:41.56ms
step:1002/2160 train_time:41639ms step_avg:41.56ms
step:1003/2160 train_time:41704ms step_avg:41.58ms
step:1004/2160 train_time:41765ms step_avg:41.60ms
step:1005/2160 train_time:41826ms step_avg:41.62ms
step:1006/2160 train_time:41886ms step_avg:41.64ms
step:1007/2160 train_time:41946ms step_avg:41.65ms
step:1008/2160 train_time:42005ms step_avg:41.67ms
step:1009/2160 train_time:42066ms step_avg:41.69ms
step:1010/2160 train_time:42124ms step_avg:41.71ms
step:1011/2160 train_time:42185ms step_avg:41.73ms
step:1012/2160 train_time:42244ms step_avg:41.74ms
step:1013/2160 train_time:42305ms step_avg:41.76ms
step:1014/2160 train_time:42364ms step_avg:41.78ms
step:1015/2160 train_time:42425ms step_avg:41.80ms
step:1016/2160 train_time:42484ms step_avg:41.81ms
step:1017/2160 train_time:42547ms step_avg:41.84ms
step:1018/2160 train_time:42607ms step_avg:41.85ms
step:1019/2160 train_time:42670ms step_avg:41.87ms
step:1020/2160 train_time:42730ms step_avg:41.89ms
step:1021/2160 train_time:42792ms step_avg:41.91ms
step:1022/2160 train_time:42852ms step_avg:41.93ms
step:1023/2160 train_time:42914ms step_avg:41.95ms
step:1024/2160 train_time:42973ms step_avg:41.97ms
step:1025/2160 train_time:43034ms step_avg:41.98ms
step:1026/2160 train_time:43094ms step_avg:42.00ms
step:1027/2160 train_time:43155ms step_avg:42.02ms
step:1028/2160 train_time:43215ms step_avg:42.04ms
step:1029/2160 train_time:43276ms step_avg:42.06ms
step:1030/2160 train_time:43335ms step_avg:42.07ms
step:1031/2160 train_time:43396ms step_avg:42.09ms
step:1032/2160 train_time:43455ms step_avg:42.11ms
step:1033/2160 train_time:43517ms step_avg:42.13ms
step:1034/2160 train_time:43578ms step_avg:42.14ms
step:1035/2160 train_time:43640ms step_avg:42.16ms
step:1036/2160 train_time:43701ms step_avg:42.18ms
step:1037/2160 train_time:43763ms step_avg:42.20ms
step:1038/2160 train_time:43823ms step_avg:42.22ms
step:1039/2160 train_time:43884ms step_avg:42.24ms
step:1040/2160 train_time:43942ms step_avg:42.25ms
step:1041/2160 train_time:44004ms step_avg:42.27ms
step:1042/2160 train_time:44063ms step_avg:42.29ms
step:1043/2160 train_time:44124ms step_avg:42.30ms
step:1044/2160 train_time:44183ms step_avg:42.32ms
step:1045/2160 train_time:44244ms step_avg:42.34ms
step:1046/2160 train_time:44304ms step_avg:42.36ms
step:1047/2160 train_time:44365ms step_avg:42.37ms
step:1048/2160 train_time:44424ms step_avg:42.39ms
step:1049/2160 train_time:44486ms step_avg:42.41ms
step:1050/2160 train_time:44545ms step_avg:42.42ms
step:1051/2160 train_time:44606ms step_avg:42.44ms
step:1052/2160 train_time:44666ms step_avg:42.46ms
step:1053/2160 train_time:44728ms step_avg:42.48ms
step:1054/2160 train_time:44787ms step_avg:42.49ms
step:1055/2160 train_time:44849ms step_avg:42.51ms
step:1056/2160 train_time:44908ms step_avg:42.53ms
step:1057/2160 train_time:44970ms step_avg:42.54ms
step:1058/2160 train_time:45029ms step_avg:42.56ms
step:1059/2160 train_time:45090ms step_avg:42.58ms
step:1060/2160 train_time:45150ms step_avg:42.59ms
step:1061/2160 train_time:45211ms step_avg:42.61ms
step:1062/2160 train_time:45270ms step_avg:42.63ms
step:1063/2160 train_time:45332ms step_avg:42.65ms
step:1064/2160 train_time:45391ms step_avg:42.66ms
step:1065/2160 train_time:45453ms step_avg:42.68ms
step:1066/2160 train_time:45512ms step_avg:42.69ms
step:1067/2160 train_time:45573ms step_avg:42.71ms
step:1068/2160 train_time:45633ms step_avg:42.73ms
step:1069/2160 train_time:45694ms step_avg:42.74ms
step:1070/2160 train_time:45754ms step_avg:42.76ms
step:1071/2160 train_time:45816ms step_avg:42.78ms
step:1072/2160 train_time:45876ms step_avg:42.79ms
step:1073/2160 train_time:45937ms step_avg:42.81ms
step:1074/2160 train_time:45997ms step_avg:42.83ms
step:1075/2160 train_time:46059ms step_avg:42.85ms
step:1076/2160 train_time:46119ms step_avg:42.86ms
step:1077/2160 train_time:46180ms step_avg:42.88ms
step:1078/2160 train_time:46240ms step_avg:42.89ms
step:1079/2160 train_time:46301ms step_avg:42.91ms
step:1080/2160 train_time:46361ms step_avg:42.93ms
step:1081/2160 train_time:46422ms step_avg:42.94ms
step:1082/2160 train_time:46482ms step_avg:42.96ms
step:1083/2160 train_time:46543ms step_avg:42.98ms
step:1084/2160 train_time:46603ms step_avg:42.99ms
step:1085/2160 train_time:46665ms step_avg:43.01ms
step:1086/2160 train_time:46724ms step_avg:43.02ms
step:1087/2160 train_time:46785ms step_avg:43.04ms
step:1088/2160 train_time:46845ms step_avg:43.06ms
step:1089/2160 train_time:46906ms step_avg:43.07ms
step:1090/2160 train_time:46966ms step_avg:43.09ms
step:1091/2160 train_time:47027ms step_avg:43.10ms
step:1092/2160 train_time:47087ms step_avg:43.12ms
step:1093/2160 train_time:47148ms step_avg:43.14ms
step:1094/2160 train_time:47208ms step_avg:43.15ms
step:1095/2160 train_time:47270ms step_avg:43.17ms
step:1096/2160 train_time:47330ms step_avg:43.18ms
step:1097/2160 train_time:47391ms step_avg:43.20ms
step:1098/2160 train_time:47451ms step_avg:43.22ms
step:1099/2160 train_time:47513ms step_avg:43.23ms
step:1100/2160 train_time:47573ms step_avg:43.25ms
step:1101/2160 train_time:47634ms step_avg:43.26ms
step:1102/2160 train_time:47693ms step_avg:43.28ms
step:1103/2160 train_time:47754ms step_avg:43.29ms
step:1104/2160 train_time:47814ms step_avg:43.31ms
step:1105/2160 train_time:47876ms step_avg:43.33ms
step:1106/2160 train_time:47936ms step_avg:43.34ms
step:1107/2160 train_time:47998ms step_avg:43.36ms
step:1108/2160 train_time:48058ms step_avg:43.37ms
step:1109/2160 train_time:48120ms step_avg:43.39ms
step:1110/2160 train_time:48180ms step_avg:43.41ms
step:1111/2160 train_time:48242ms step_avg:43.42ms
step:1112/2160 train_time:48302ms step_avg:43.44ms
step:1113/2160 train_time:48363ms step_avg:43.45ms
step:1114/2160 train_time:48423ms step_avg:43.47ms
step:1115/2160 train_time:48484ms step_avg:43.48ms
step:1116/2160 train_time:48543ms step_avg:43.50ms
step:1117/2160 train_time:48605ms step_avg:43.51ms
step:1118/2160 train_time:48664ms step_avg:43.53ms
step:1119/2160 train_time:48725ms step_avg:43.54ms
step:1120/2160 train_time:48784ms step_avg:43.56ms
step:1121/2160 train_time:48846ms step_avg:43.57ms
step:1122/2160 train_time:48906ms step_avg:43.59ms
step:1123/2160 train_time:48967ms step_avg:43.60ms
step:1124/2160 train_time:49027ms step_avg:43.62ms
step:1125/2160 train_time:49088ms step_avg:43.63ms
step:1126/2160 train_time:49148ms step_avg:43.65ms
step:1127/2160 train_time:49209ms step_avg:43.66ms
step:1128/2160 train_time:49269ms step_avg:43.68ms
step:1129/2160 train_time:49332ms step_avg:43.69ms
step:1130/2160 train_time:49391ms step_avg:43.71ms
step:1131/2160 train_time:49453ms step_avg:43.73ms
step:1132/2160 train_time:49512ms step_avg:43.74ms
step:1133/2160 train_time:49574ms step_avg:43.75ms
step:1134/2160 train_time:49633ms step_avg:43.77ms
step:1135/2160 train_time:49694ms step_avg:43.78ms
step:1136/2160 train_time:49754ms step_avg:43.80ms
step:1137/2160 train_time:49815ms step_avg:43.81ms
step:1138/2160 train_time:49875ms step_avg:43.83ms
step:1139/2160 train_time:49936ms step_avg:43.84ms
step:1140/2160 train_time:49996ms step_avg:43.86ms
step:1141/2160 train_time:50058ms step_avg:43.87ms
step:1142/2160 train_time:50118ms step_avg:43.89ms
step:1143/2160 train_time:50180ms step_avg:43.90ms
step:1144/2160 train_time:50240ms step_avg:43.92ms
step:1145/2160 train_time:50301ms step_avg:43.93ms
step:1146/2160 train_time:50361ms step_avg:43.94ms
step:1147/2160 train_time:50422ms step_avg:43.96ms
step:1148/2160 train_time:50482ms step_avg:43.97ms
step:1149/2160 train_time:50543ms step_avg:43.99ms
step:1150/2160 train_time:50602ms step_avg:44.00ms
step:1151/2160 train_time:50664ms step_avg:44.02ms
step:1152/2160 train_time:50724ms step_avg:44.03ms
step:1153/2160 train_time:50785ms step_avg:44.05ms
step:1154/2160 train_time:50845ms step_avg:44.06ms
step:1155/2160 train_time:50906ms step_avg:44.07ms
step:1156/2160 train_time:50966ms step_avg:44.09ms
step:1157/2160 train_time:51028ms step_avg:44.10ms
step:1158/2160 train_time:51087ms step_avg:44.12ms
step:1159/2160 train_time:51149ms step_avg:44.13ms
step:1160/2160 train_time:51209ms step_avg:44.15ms
step:1161/2160 train_time:51270ms step_avg:44.16ms
step:1162/2160 train_time:51330ms step_avg:44.17ms
step:1163/2160 train_time:51392ms step_avg:44.19ms
step:1164/2160 train_time:51452ms step_avg:44.20ms
step:1165/2160 train_time:51514ms step_avg:44.22ms
step:1166/2160 train_time:51574ms step_avg:44.23ms
step:1167/2160 train_time:51635ms step_avg:44.25ms
step:1168/2160 train_time:51694ms step_avg:44.26ms
step:1169/2160 train_time:51756ms step_avg:44.27ms
step:1170/2160 train_time:51816ms step_avg:44.29ms
step:1171/2160 train_time:51878ms step_avg:44.30ms
step:1172/2160 train_time:51938ms step_avg:44.32ms
step:1173/2160 train_time:51999ms step_avg:44.33ms
step:1174/2160 train_time:52059ms step_avg:44.34ms
step:1175/2160 train_time:52121ms step_avg:44.36ms
step:1176/2160 train_time:52182ms step_avg:44.37ms
step:1177/2160 train_time:52244ms step_avg:44.39ms
step:1178/2160 train_time:52304ms step_avg:44.40ms
step:1179/2160 train_time:52365ms step_avg:44.41ms
step:1180/2160 train_time:52424ms step_avg:44.43ms
step:1181/2160 train_time:52485ms step_avg:44.44ms
step:1182/2160 train_time:52544ms step_avg:44.45ms
step:1183/2160 train_time:52605ms step_avg:44.47ms
step:1184/2160 train_time:52664ms step_avg:44.48ms
step:1185/2160 train_time:52726ms step_avg:44.49ms
step:1186/2160 train_time:52785ms step_avg:44.51ms
step:1187/2160 train_time:52846ms step_avg:44.52ms
step:1188/2160 train_time:52906ms step_avg:44.53ms
step:1189/2160 train_time:52968ms step_avg:44.55ms
step:1190/2160 train_time:53027ms step_avg:44.56ms
step:1191/2160 train_time:53089ms step_avg:44.57ms
step:1192/2160 train_time:53149ms step_avg:44.59ms
step:1193/2160 train_time:53211ms step_avg:44.60ms
step:1194/2160 train_time:53270ms step_avg:44.62ms
step:1195/2160 train_time:53332ms step_avg:44.63ms
step:1196/2160 train_time:53392ms step_avg:44.64ms
step:1197/2160 train_time:53453ms step_avg:44.66ms
step:1198/2160 train_time:53512ms step_avg:44.67ms
step:1199/2160 train_time:53574ms step_avg:44.68ms
step:1200/2160 train_time:53633ms step_avg:44.69ms
step:1201/2160 train_time:53695ms step_avg:44.71ms
step:1202/2160 train_time:53755ms step_avg:44.72ms
step:1203/2160 train_time:53816ms step_avg:44.73ms
step:1204/2160 train_time:53876ms step_avg:44.75ms
step:1205/2160 train_time:53937ms step_avg:44.76ms
step:1206/2160 train_time:53997ms step_avg:44.77ms
step:1207/2160 train_time:54059ms step_avg:44.79ms
step:1208/2160 train_time:54119ms step_avg:44.80ms
step:1209/2160 train_time:54181ms step_avg:44.81ms
step:1210/2160 train_time:54241ms step_avg:44.83ms
step:1211/2160 train_time:54303ms step_avg:44.84ms
step:1212/2160 train_time:54363ms step_avg:44.85ms
step:1213/2160 train_time:54425ms step_avg:44.87ms
step:1214/2160 train_time:54484ms step_avg:44.88ms
step:1215/2160 train_time:54546ms step_avg:44.89ms
step:1216/2160 train_time:54605ms step_avg:44.91ms
step:1217/2160 train_time:54667ms step_avg:44.92ms
step:1218/2160 train_time:54726ms step_avg:44.93ms
step:1219/2160 train_time:54787ms step_avg:44.94ms
step:1220/2160 train_time:54847ms step_avg:44.96ms
step:1221/2160 train_time:54910ms step_avg:44.97ms
step:1222/2160 train_time:54969ms step_avg:44.98ms
step:1223/2160 train_time:55031ms step_avg:45.00ms
step:1224/2160 train_time:55090ms step_avg:45.01ms
step:1225/2160 train_time:55153ms step_avg:45.02ms
step:1226/2160 train_time:55213ms step_avg:45.04ms
step:1227/2160 train_time:55275ms step_avg:45.05ms
step:1228/2160 train_time:55334ms step_avg:45.06ms
step:1229/2160 train_time:55395ms step_avg:45.07ms
step:1230/2160 train_time:55454ms step_avg:45.08ms
step:1231/2160 train_time:55516ms step_avg:45.10ms
step:1232/2160 train_time:55576ms step_avg:45.11ms
step:1233/2160 train_time:55638ms step_avg:45.12ms
step:1234/2160 train_time:55697ms step_avg:45.14ms
step:1235/2160 train_time:55759ms step_avg:45.15ms
step:1236/2160 train_time:55818ms step_avg:45.16ms
step:1237/2160 train_time:55880ms step_avg:45.17ms
step:1238/2160 train_time:55941ms step_avg:45.19ms
step:1239/2160 train_time:56002ms step_avg:45.20ms
step:1240/2160 train_time:56061ms step_avg:45.21ms
step:1241/2160 train_time:56123ms step_avg:45.22ms
step:1242/2160 train_time:56183ms step_avg:45.24ms
step:1243/2160 train_time:56244ms step_avg:45.25ms
step:1244/2160 train_time:56304ms step_avg:45.26ms
step:1245/2160 train_time:56365ms step_avg:45.27ms
step:1246/2160 train_time:56424ms step_avg:45.28ms
step:1247/2160 train_time:56485ms step_avg:45.30ms
step:1248/2160 train_time:56545ms step_avg:45.31ms
step:1249/2160 train_time:56606ms step_avg:45.32ms
step:1250/2160 train_time:56666ms step_avg:45.33ms
step:1250/2160 val_loss:3.5786 train_time:56728ms step_avg:45.38ms
step:1251/2160 train_time:56748ms step_avg:45.36ms
step:1252/2160 train_time:56788ms step_avg:45.36ms
step:1253/2160 train_time:56852ms step_avg:45.37ms
step:1254/2160 train_time:56913ms step_avg:45.39ms
step:1255/2160 train_time:56974ms step_avg:45.40ms
step:1256/2160 train_time:57034ms step_avg:45.41ms
step:1257/2160 train_time:57094ms step_avg:45.42ms
step:1258/2160 train_time:57153ms step_avg:45.43ms
step:1259/2160 train_time:57213ms step_avg:45.44ms
step:1260/2160 train_time:57272ms step_avg:45.45ms
step:1261/2160 train_time:57333ms step_avg:45.47ms
step:1262/2160 train_time:57393ms step_avg:45.48ms
step:1263/2160 train_time:57453ms step_avg:45.49ms
step:1264/2160 train_time:57513ms step_avg:45.50ms
step:1265/2160 train_time:57573ms step_avg:45.51ms
step:1266/2160 train_time:57633ms step_avg:45.52ms
step:1267/2160 train_time:57695ms step_avg:45.54ms
step:1268/2160 train_time:57757ms step_avg:45.55ms
step:1269/2160 train_time:57819ms step_avg:45.56ms
step:1270/2160 train_time:57879ms step_avg:45.57ms
step:1271/2160 train_time:57941ms step_avg:45.59ms
step:1272/2160 train_time:58000ms step_avg:45.60ms
step:1273/2160 train_time:58062ms step_avg:45.61ms
step:1274/2160 train_time:58122ms step_avg:45.62ms
step:1275/2160 train_time:58183ms step_avg:45.63ms
step:1276/2160 train_time:58242ms step_avg:45.64ms
step:1277/2160 train_time:58303ms step_avg:45.66ms
step:1278/2160 train_time:58362ms step_avg:45.67ms
step:1279/2160 train_time:58423ms step_avg:45.68ms
step:1280/2160 train_time:58482ms step_avg:45.69ms
step:1281/2160 train_time:58544ms step_avg:45.70ms
step:1282/2160 train_time:58603ms step_avg:45.71ms
step:1283/2160 train_time:58666ms step_avg:45.73ms
step:1284/2160 train_time:58727ms step_avg:45.74ms
step:1285/2160 train_time:58790ms step_avg:45.75ms
step:1286/2160 train_time:58850ms step_avg:45.76ms
step:1287/2160 train_time:58912ms step_avg:45.77ms
step:1288/2160 train_time:58972ms step_avg:45.79ms
step:1289/2160 train_time:59034ms step_avg:45.80ms
step:1290/2160 train_time:59093ms step_avg:45.81ms
step:1291/2160 train_time:59155ms step_avg:45.82ms
step:1292/2160 train_time:59214ms step_avg:45.83ms
step:1293/2160 train_time:59275ms step_avg:45.84ms
step:1294/2160 train_time:59334ms step_avg:45.85ms
step:1295/2160 train_time:59394ms step_avg:45.86ms
step:1296/2160 train_time:59454ms step_avg:45.87ms
step:1297/2160 train_time:59515ms step_avg:45.89ms
step:1298/2160 train_time:59574ms step_avg:45.90ms
step:1299/2160 train_time:59636ms step_avg:45.91ms
step:1300/2160 train_time:59697ms step_avg:45.92ms
step:1301/2160 train_time:59759ms step_avg:45.93ms
step:1302/2160 train_time:59819ms step_avg:45.94ms
step:1303/2160 train_time:59881ms step_avg:45.96ms
step:1304/2160 train_time:59940ms step_avg:45.97ms
step:1305/2160 train_time:60002ms step_avg:45.98ms
step:1306/2160 train_time:60061ms step_avg:45.99ms
step:1307/2160 train_time:60123ms step_avg:46.00ms
step:1308/2160 train_time:60182ms step_avg:46.01ms
step:1309/2160 train_time:60243ms step_avg:46.02ms
step:1310/2160 train_time:60303ms step_avg:46.03ms
step:1311/2160 train_time:60364ms step_avg:46.04ms
step:1312/2160 train_time:60423ms step_avg:46.05ms
step:1313/2160 train_time:60484ms step_avg:46.07ms
step:1314/2160 train_time:60544ms step_avg:46.08ms
step:1315/2160 train_time:60606ms step_avg:46.09ms
step:1316/2160 train_time:60665ms step_avg:46.10ms
step:1317/2160 train_time:60728ms step_avg:46.11ms
step:1318/2160 train_time:60789ms step_avg:46.12ms
step:1319/2160 train_time:60851ms step_avg:46.13ms
step:1320/2160 train_time:60911ms step_avg:46.14ms
step:1321/2160 train_time:60972ms step_avg:46.16ms
step:1322/2160 train_time:61032ms step_avg:46.17ms
step:1323/2160 train_time:61094ms step_avg:46.18ms
step:1324/2160 train_time:61154ms step_avg:46.19ms
step:1325/2160 train_time:61215ms step_avg:46.20ms
step:1326/2160 train_time:61274ms step_avg:46.21ms
step:1327/2160 train_time:61335ms step_avg:46.22ms
step:1328/2160 train_time:61395ms step_avg:46.23ms
step:1329/2160 train_time:61456ms step_avg:46.24ms
step:1330/2160 train_time:61516ms step_avg:46.25ms
step:1331/2160 train_time:61578ms step_avg:46.26ms
step:1332/2160 train_time:61638ms step_avg:46.27ms
step:1333/2160 train_time:61700ms step_avg:46.29ms
step:1334/2160 train_time:61759ms step_avg:46.30ms
step:1335/2160 train_time:61822ms step_avg:46.31ms
step:1336/2160 train_time:61882ms step_avg:46.32ms
step:1337/2160 train_time:61944ms step_avg:46.33ms
step:1338/2160 train_time:62004ms step_avg:46.34ms
step:1339/2160 train_time:62065ms step_avg:46.35ms
step:1340/2160 train_time:62126ms step_avg:46.36ms
step:1341/2160 train_time:62187ms step_avg:46.37ms
step:1342/2160 train_time:62246ms step_avg:46.38ms
step:1343/2160 train_time:62308ms step_avg:46.39ms
step:1344/2160 train_time:62367ms step_avg:46.40ms
step:1345/2160 train_time:62429ms step_avg:46.42ms
step:1346/2160 train_time:62489ms step_avg:46.43ms
step:1347/2160 train_time:62551ms step_avg:46.44ms
step:1348/2160 train_time:62611ms step_avg:46.45ms
step:1349/2160 train_time:62673ms step_avg:46.46ms
step:1350/2160 train_time:62732ms step_avg:46.47ms
step:1351/2160 train_time:62793ms step_avg:46.48ms
step:1352/2160 train_time:62853ms step_avg:46.49ms
step:1353/2160 train_time:62915ms step_avg:46.50ms
step:1354/2160 train_time:62974ms step_avg:46.51ms
step:1355/2160 train_time:63036ms step_avg:46.52ms
step:1356/2160 train_time:63095ms step_avg:46.53ms
step:1357/2160 train_time:63157ms step_avg:46.54ms
step:1358/2160 train_time:63216ms step_avg:46.55ms
step:1359/2160 train_time:63278ms step_avg:46.56ms
step:1360/2160 train_time:63337ms step_avg:46.57ms
step:1361/2160 train_time:63400ms step_avg:46.58ms
step:1362/2160 train_time:63459ms step_avg:46.59ms
step:1363/2160 train_time:63521ms step_avg:46.60ms
step:1364/2160 train_time:63581ms step_avg:46.61ms
step:1365/2160 train_time:63643ms step_avg:46.62ms
step:1366/2160 train_time:63702ms step_avg:46.63ms
step:1367/2160 train_time:63764ms step_avg:46.64ms
step:1368/2160 train_time:63823ms step_avg:46.65ms
step:1369/2160 train_time:63885ms step_avg:46.67ms
step:1370/2160 train_time:63944ms step_avg:46.67ms
step:1371/2160 train_time:64005ms step_avg:46.69ms
step:1372/2160 train_time:64065ms step_avg:46.69ms
step:1373/2160 train_time:64127ms step_avg:46.71ms
step:1374/2160 train_time:64188ms step_avg:46.72ms
step:1375/2160 train_time:64250ms step_avg:46.73ms
step:1376/2160 train_time:64310ms step_avg:46.74ms
step:1377/2160 train_time:64371ms step_avg:46.75ms
step:1378/2160 train_time:64431ms step_avg:46.76ms
step:1379/2160 train_time:64492ms step_avg:46.77ms
step:1380/2160 train_time:64553ms step_avg:46.78ms
step:1381/2160 train_time:64613ms step_avg:46.79ms
step:1382/2160 train_time:64673ms step_avg:46.80ms
step:1383/2160 train_time:64734ms step_avg:46.81ms
step:1384/2160 train_time:64794ms step_avg:46.82ms
step:1385/2160 train_time:64855ms step_avg:46.83ms
step:1386/2160 train_time:64914ms step_avg:46.84ms
step:1387/2160 train_time:64975ms step_avg:46.85ms
step:1388/2160 train_time:65035ms step_avg:46.86ms
step:1389/2160 train_time:65096ms step_avg:46.87ms
step:1390/2160 train_time:65155ms step_avg:46.87ms
step:1391/2160 train_time:65217ms step_avg:46.88ms
step:1392/2160 train_time:65276ms step_avg:46.89ms
step:1393/2160 train_time:65338ms step_avg:46.90ms
step:1394/2160 train_time:65399ms step_avg:46.91ms
step:1395/2160 train_time:65460ms step_avg:46.92ms
step:1396/2160 train_time:65520ms step_avg:46.93ms
step:1397/2160 train_time:65581ms step_avg:46.94ms
step:1398/2160 train_time:65641ms step_avg:46.95ms
step:1399/2160 train_time:65703ms step_avg:46.96ms
step:1400/2160 train_time:65763ms step_avg:46.97ms
step:1401/2160 train_time:65824ms step_avg:46.98ms
step:1402/2160 train_time:65884ms step_avg:46.99ms
step:1403/2160 train_time:65945ms step_avg:47.00ms
step:1404/2160 train_time:66004ms step_avg:47.01ms
step:1405/2160 train_time:66066ms step_avg:47.02ms
step:1406/2160 train_time:66125ms step_avg:47.03ms
step:1407/2160 train_time:66187ms step_avg:47.04ms
step:1408/2160 train_time:66247ms step_avg:47.05ms
step:1409/2160 train_time:66308ms step_avg:47.06ms
step:1410/2160 train_time:66368ms step_avg:47.07ms
step:1411/2160 train_time:66429ms step_avg:47.08ms
step:1412/2160 train_time:66490ms step_avg:47.09ms
step:1413/2160 train_time:66552ms step_avg:47.10ms
step:1414/2160 train_time:66612ms step_avg:47.11ms
step:1415/2160 train_time:66674ms step_avg:47.12ms
step:1416/2160 train_time:66761ms step_avg:47.15ms
step:1417/2160 train_time:66850ms step_avg:47.18ms
step:1418/2160 train_time:66937ms step_avg:47.21ms
step:1419/2160 train_time:67027ms step_avg:47.24ms
step:1420/2160 train_time:67114ms step_avg:47.26ms
step:1421/2160 train_time:67204ms step_avg:47.29ms
step:1422/2160 train_time:67292ms step_avg:47.32ms
step:1423/2160 train_time:67383ms step_avg:47.35ms
step:1424/2160 train_time:67470ms step_avg:47.38ms
step:1425/2160 train_time:67561ms step_avg:47.41ms
step:1426/2160 train_time:67648ms step_avg:47.44ms
step:1427/2160 train_time:67737ms step_avg:47.47ms
step:1428/2160 train_time:67824ms step_avg:47.50ms
step:1429/2160 train_time:67912ms step_avg:47.52ms
step:1430/2160 train_time:68000ms step_avg:47.55ms
step:1431/2160 train_time:68089ms step_avg:47.58ms
step:1432/2160 train_time:68176ms step_avg:47.61ms
step:1433/2160 train_time:68266ms step_avg:47.64ms
step:1434/2160 train_time:68354ms step_avg:47.67ms
step:1435/2160 train_time:68444ms step_avg:47.70ms
step:1436/2160 train_time:68531ms step_avg:47.72ms
step:1437/2160 train_time:68621ms step_avg:47.75ms
step:1438/2160 train_time:68707ms step_avg:47.78ms
step:1439/2160 train_time:68796ms step_avg:47.81ms
step:1440/2160 train_time:68883ms step_avg:47.84ms
step:1441/2160 train_time:68972ms step_avg:47.86ms
step:1442/2160 train_time:69060ms step_avg:47.89ms
step:1443/2160 train_time:69149ms step_avg:47.92ms
step:1444/2160 train_time:69236ms step_avg:47.95ms
step:1445/2160 train_time:69326ms step_avg:47.98ms
step:1446/2160 train_time:69413ms step_avg:48.00ms
step:1447/2160 train_time:69502ms step_avg:48.03ms
step:1448/2160 train_time:69589ms step_avg:48.06ms
step:1449/2160 train_time:69679ms step_avg:48.09ms
step:1450/2160 train_time:69767ms step_avg:48.12ms
step:1451/2160 train_time:69856ms step_avg:48.14ms
step:1452/2160 train_time:69943ms step_avg:48.17ms
step:1453/2160 train_time:70032ms step_avg:48.20ms
step:1454/2160 train_time:70119ms step_avg:48.23ms
step:1455/2160 train_time:70209ms step_avg:48.25ms
step:1456/2160 train_time:70297ms step_avg:48.28ms
step:1457/2160 train_time:70385ms step_avg:48.31ms
step:1458/2160 train_time:70472ms step_avg:48.34ms
step:1459/2160 train_time:70563ms step_avg:48.36ms
step:1460/2160 train_time:70650ms step_avg:48.39ms
step:1461/2160 train_time:70741ms step_avg:48.42ms
step:1462/2160 train_time:70828ms step_avg:48.45ms
step:1463/2160 train_time:70917ms step_avg:48.47ms
step:1464/2160 train_time:71005ms step_avg:48.50ms
step:1465/2160 train_time:71094ms step_avg:48.53ms
step:1466/2160 train_time:71182ms step_avg:48.56ms
step:1467/2160 train_time:71271ms step_avg:48.58ms
step:1468/2160 train_time:71359ms step_avg:48.61ms
step:1469/2160 train_time:71448ms step_avg:48.64ms
step:1470/2160 train_time:71535ms step_avg:48.66ms
step:1471/2160 train_time:71625ms step_avg:48.69ms
step:1472/2160 train_time:71713ms step_avg:48.72ms
step:1473/2160 train_time:71802ms step_avg:48.75ms
step:1474/2160 train_time:71889ms step_avg:48.77ms
step:1475/2160 train_time:71979ms step_avg:48.80ms
step:1476/2160 train_time:72066ms step_avg:48.83ms
step:1477/2160 train_time:72156ms step_avg:48.85ms
step:1478/2160 train_time:72243ms step_avg:48.88ms
step:1479/2160 train_time:72332ms step_avg:48.91ms
step:1480/2160 train_time:72419ms step_avg:48.93ms
step:1481/2160 train_time:72509ms step_avg:48.96ms
step:1482/2160 train_time:72596ms step_avg:48.99ms
step:1483/2160 train_time:72686ms step_avg:49.01ms
step:1484/2160 train_time:72773ms step_avg:49.04ms
step:1485/2160 train_time:72863ms step_avg:49.07ms
step:1486/2160 train_time:72950ms step_avg:49.09ms
step:1487/2160 train_time:73040ms step_avg:49.12ms
step:1488/2160 train_time:73128ms step_avg:49.14ms
step:1489/2160 train_time:73216ms step_avg:49.17ms
step:1490/2160 train_time:73304ms step_avg:49.20ms
step:1491/2160 train_time:73393ms step_avg:49.22ms
step:1492/2160 train_time:73482ms step_avg:49.25ms
step:1493/2160 train_time:73570ms step_avg:49.28ms
step:1494/2160 train_time:73658ms step_avg:49.30ms
step:1495/2160 train_time:73747ms step_avg:49.33ms
step:1496/2160 train_time:73834ms step_avg:49.35ms
step:1497/2160 train_time:73923ms step_avg:49.38ms
step:1498/2160 train_time:74011ms step_avg:49.41ms
step:1499/2160 train_time:74100ms step_avg:49.43ms
step:1500/2160 train_time:74188ms step_avg:49.46ms
step:1500/2160 val_loss:3.4962 train_time:74277ms step_avg:49.52ms
step:1501/2160 train_time:74298ms step_avg:49.50ms
step:1502/2160 train_time:74366ms step_avg:49.51ms
step:1503/2160 train_time:74456ms step_avg:49.54ms
step:1504/2160 train_time:74546ms step_avg:49.57ms
step:1505/2160 train_time:74635ms step_avg:49.59ms
step:1506/2160 train_time:74722ms step_avg:49.62ms
step:1507/2160 train_time:74810ms step_avg:49.64ms
step:1508/2160 train_time:74897ms step_avg:49.67ms
step:1509/2160 train_time:74985ms step_avg:49.69ms
step:1510/2160 train_time:75073ms step_avg:49.72ms
step:1511/2160 train_time:75163ms step_avg:49.74ms
step:1512/2160 train_time:75255ms step_avg:49.77ms
step:1513/2160 train_time:75345ms step_avg:49.80ms
step:1514/2160 train_time:75433ms step_avg:49.82ms
step:1515/2160 train_time:75521ms step_avg:49.85ms
step:1516/2160 train_time:75608ms step_avg:49.87ms
step:1517/2160 train_time:75697ms step_avg:49.90ms
step:1518/2160 train_time:75784ms step_avg:49.92ms
step:1519/2160 train_time:75872ms step_avg:49.95ms
step:1520/2160 train_time:75958ms step_avg:49.97ms
step:1521/2160 train_time:76047ms step_avg:50.00ms
step:1522/2160 train_time:76134ms step_avg:50.02ms
step:1523/2160 train_time:76224ms step_avg:50.05ms
step:1524/2160 train_time:76313ms step_avg:50.07ms
step:1525/2160 train_time:76404ms step_avg:50.10ms
step:1526/2160 train_time:76491ms step_avg:50.13ms
step:1527/2160 train_time:76581ms step_avg:50.15ms
step:1528/2160 train_time:76668ms step_avg:50.18ms
step:1529/2160 train_time:76756ms step_avg:50.20ms
step:1530/2160 train_time:76843ms step_avg:50.22ms
step:1531/2160 train_time:76932ms step_avg:50.25ms
step:1532/2160 train_time:77019ms step_avg:50.27ms
step:1533/2160 train_time:77108ms step_avg:50.30ms
step:1534/2160 train_time:77195ms step_avg:50.32ms
step:1535/2160 train_time:77285ms step_avg:50.35ms
step:1536/2160 train_time:77373ms step_avg:50.37ms
step:1537/2160 train_time:77462ms step_avg:50.40ms
step:1538/2160 train_time:77550ms step_avg:50.42ms
step:1539/2160 train_time:77639ms step_avg:50.45ms
step:1540/2160 train_time:77725ms step_avg:50.47ms
step:1541/2160 train_time:77814ms step_avg:50.50ms
step:1542/2160 train_time:77901ms step_avg:50.52ms
step:1543/2160 train_time:77990ms step_avg:50.54ms
step:1544/2160 train_time:78077ms step_avg:50.57ms
step:1545/2160 train_time:78166ms step_avg:50.59ms
step:1546/2160 train_time:78254ms step_avg:50.62ms
step:1547/2160 train_time:78343ms step_avg:50.64ms
step:1548/2160 train_time:78431ms step_avg:50.67ms
step:1549/2160 train_time:78520ms step_avg:50.69ms
step:1550/2160 train_time:78608ms step_avg:50.71ms
step:1551/2160 train_time:78697ms step_avg:50.74ms
step:1552/2160 train_time:78784ms step_avg:50.76ms
step:1553/2160 train_time:78873ms step_avg:50.79ms
step:1554/2160 train_time:78960ms step_avg:50.81ms
step:1555/2160 train_time:79049ms step_avg:50.84ms
step:1556/2160 train_time:79137ms step_avg:50.86ms
step:1557/2160 train_time:79226ms step_avg:50.88ms
step:1558/2160 train_time:79314ms step_avg:50.91ms
step:1559/2160 train_time:79404ms step_avg:50.93ms
step:1560/2160 train_time:79491ms step_avg:50.96ms
step:1561/2160 train_time:79580ms step_avg:50.98ms
step:1562/2160 train_time:79667ms step_avg:51.00ms
step:1563/2160 train_time:79756ms step_avg:51.03ms
step:1564/2160 train_time:79844ms step_avg:51.05ms
step:1565/2160 train_time:79933ms step_avg:51.08ms
step:1566/2160 train_time:80020ms step_avg:51.10ms
step:1567/2160 train_time:80109ms step_avg:51.12ms
step:1568/2160 train_time:80195ms step_avg:51.15ms
step:1569/2160 train_time:80285ms step_avg:51.17ms
step:1570/2160 train_time:80373ms step_avg:51.19ms
step:1571/2160 train_time:80462ms step_avg:51.22ms
step:1572/2160 train_time:80550ms step_avg:51.24ms
step:1573/2160 train_time:80639ms step_avg:51.26ms
step:1574/2160 train_time:80727ms step_avg:51.29ms
step:1575/2160 train_time:80816ms step_avg:51.31ms
step:1576/2160 train_time:80904ms step_avg:51.33ms
step:1577/2160 train_time:80993ms step_avg:51.36ms
step:1578/2160 train_time:81080ms step_avg:51.38ms
step:1579/2160 train_time:81169ms step_avg:51.41ms
step:1580/2160 train_time:81257ms step_avg:51.43ms
step:1581/2160 train_time:81346ms step_avg:51.45ms
step:1582/2160 train_time:81434ms step_avg:51.48ms
step:1583/2160 train_time:81523ms step_avg:51.50ms
step:1584/2160 train_time:81611ms step_avg:51.52ms
step:1585/2160 train_time:81700ms step_avg:51.55ms
step:1586/2160 train_time:81787ms step_avg:51.57ms
step:1587/2160 train_time:81877ms step_avg:51.59ms
step:1588/2160 train_time:81965ms step_avg:51.61ms
step:1589/2160 train_time:82053ms step_avg:51.64ms
step:1590/2160 train_time:82140ms step_avg:51.66ms
step:1591/2160 train_time:82229ms step_avg:51.68ms
step:1592/2160 train_time:82317ms step_avg:51.71ms
step:1593/2160 train_time:82406ms step_avg:51.73ms
step:1594/2160 train_time:82494ms step_avg:51.75ms
step:1595/2160 train_time:82583ms step_avg:51.78ms
step:1596/2160 train_time:82671ms step_avg:51.80ms
step:1597/2160 train_time:82759ms step_avg:51.82ms
step:1598/2160 train_time:82847ms step_avg:51.84ms
step:1599/2160 train_time:82936ms step_avg:51.87ms
step:1600/2160 train_time:83023ms step_avg:51.89ms
step:1601/2160 train_time:83113ms step_avg:51.91ms
step:1602/2160 train_time:83200ms step_avg:51.93ms
step:1603/2160 train_time:83289ms step_avg:51.96ms
step:1604/2160 train_time:83377ms step_avg:51.98ms
step:1605/2160 train_time:83466ms step_avg:52.00ms
step:1606/2160 train_time:83554ms step_avg:52.03ms
step:1607/2160 train_time:83643ms step_avg:52.05ms
step:1608/2160 train_time:83730ms step_avg:52.07ms
step:1609/2160 train_time:83819ms step_avg:52.09ms
step:1610/2160 train_time:83906ms step_avg:52.12ms
step:1611/2160 train_time:83995ms step_avg:52.14ms
step:1612/2160 train_time:84082ms step_avg:52.16ms
step:1613/2160 train_time:84172ms step_avg:52.18ms
step:1614/2160 train_time:84259ms step_avg:52.21ms
step:1615/2160 train_time:84349ms step_avg:52.23ms
step:1616/2160 train_time:84436ms step_avg:52.25ms
step:1617/2160 train_time:84526ms step_avg:52.27ms
step:1618/2160 train_time:84613ms step_avg:52.29ms
step:1619/2160 train_time:84703ms step_avg:52.32ms
step:1620/2160 train_time:84791ms step_avg:52.34ms
step:1621/2160 train_time:84880ms step_avg:52.36ms
step:1622/2160 train_time:84967ms step_avg:52.38ms
step:1623/2160 train_time:85056ms step_avg:52.41ms
step:1624/2160 train_time:85144ms step_avg:52.43ms
step:1625/2160 train_time:85233ms step_avg:52.45ms
step:1626/2160 train_time:85320ms step_avg:52.47ms
step:1627/2160 train_time:85409ms step_avg:52.49ms
step:1628/2160 train_time:85496ms step_avg:52.52ms
step:1629/2160 train_time:85586ms step_avg:52.54ms
step:1630/2160 train_time:85674ms step_avg:52.56ms
step:1631/2160 train_time:85763ms step_avg:52.58ms
step:1632/2160 train_time:85851ms step_avg:52.60ms
step:1633/2160 train_time:85940ms step_avg:52.63ms
step:1634/2160 train_time:86027ms step_avg:52.65ms
step:1635/2160 train_time:86117ms step_avg:52.67ms
step:1636/2160 train_time:86204ms step_avg:52.69ms
step:1637/2160 train_time:86294ms step_avg:52.71ms
step:1638/2160 train_time:86381ms step_avg:52.74ms
step:1639/2160 train_time:86471ms step_avg:52.76ms
step:1640/2160 train_time:86558ms step_avg:52.78ms
step:1641/2160 train_time:86648ms step_avg:52.80ms
step:1642/2160 train_time:86736ms step_avg:52.82ms
step:1643/2160 train_time:86824ms step_avg:52.85ms
step:1644/2160 train_time:86912ms step_avg:52.87ms
step:1645/2160 train_time:87001ms step_avg:52.89ms
step:1646/2160 train_time:87088ms step_avg:52.91ms
step:1647/2160 train_time:87177ms step_avg:52.93ms
step:1648/2160 train_time:87265ms step_avg:52.95ms
step:1649/2160 train_time:87354ms step_avg:52.97ms
step:1650/2160 train_time:87441ms step_avg:52.99ms
step:1651/2160 train_time:87531ms step_avg:53.02ms
step:1652/2160 train_time:87618ms step_avg:53.04ms
step:1653/2160 train_time:87707ms step_avg:53.06ms
step:1654/2160 train_time:87794ms step_avg:53.08ms
step:1655/2160 train_time:87882ms step_avg:53.10ms
step:1656/2160 train_time:87970ms step_avg:53.12ms
step:1657/2160 train_time:88059ms step_avg:53.14ms
step:1658/2160 train_time:88147ms step_avg:53.16ms
step:1659/2160 train_time:88236ms step_avg:53.19ms
step:1660/2160 train_time:88324ms step_avg:53.21ms
step:1661/2160 train_time:88413ms step_avg:53.23ms
step:1662/2160 train_time:88500ms step_avg:53.25ms
step:1663/2160 train_time:88590ms step_avg:53.27ms
step:1664/2160 train_time:88677ms step_avg:53.29ms
step:1665/2160 train_time:88766ms step_avg:53.31ms
step:1666/2160 train_time:88852ms step_avg:53.33ms
step:1667/2160 train_time:88941ms step_avg:53.35ms
step:1668/2160 train_time:89029ms step_avg:53.37ms
step:1669/2160 train_time:89119ms step_avg:53.40ms
step:1670/2160 train_time:89206ms step_avg:53.42ms
step:1671/2160 train_time:89295ms step_avg:53.44ms
step:1672/2160 train_time:89382ms step_avg:53.46ms
step:1673/2160 train_time:89472ms step_avg:53.48ms
step:1674/2160 train_time:89559ms step_avg:53.50ms
step:1675/2160 train_time:89649ms step_avg:53.52ms
step:1676/2160 train_time:89737ms step_avg:53.54ms
step:1677/2160 train_time:89826ms step_avg:53.56ms
step:1678/2160 train_time:89914ms step_avg:53.58ms
step:1679/2160 train_time:90004ms step_avg:53.61ms
step:1680/2160 train_time:90091ms step_avg:53.63ms
step:1681/2160 train_time:90180ms step_avg:53.65ms
step:1682/2160 train_time:90267ms step_avg:53.67ms
step:1683/2160 train_time:90357ms step_avg:53.69ms
step:1684/2160 train_time:90444ms step_avg:53.71ms
step:1685/2160 train_time:90534ms step_avg:53.73ms
step:1686/2160 train_time:90622ms step_avg:53.75ms
step:1687/2160 train_time:90711ms step_avg:53.77ms
step:1688/2160 train_time:90798ms step_avg:53.79ms
step:1689/2160 train_time:90888ms step_avg:53.81ms
step:1690/2160 train_time:90975ms step_avg:53.83ms
step:1691/2160 train_time:91064ms step_avg:53.85ms
step:1692/2160 train_time:91152ms step_avg:53.87ms
step:1693/2160 train_time:91241ms step_avg:53.89ms
step:1694/2160 train_time:91329ms step_avg:53.91ms
step:1695/2160 train_time:91419ms step_avg:53.93ms
step:1696/2160 train_time:91507ms step_avg:53.95ms
step:1697/2160 train_time:91596ms step_avg:53.98ms
step:1698/2160 train_time:91684ms step_avg:54.00ms
step:1699/2160 train_time:91774ms step_avg:54.02ms
step:1700/2160 train_time:91861ms step_avg:54.04ms
step:1701/2160 train_time:91951ms step_avg:54.06ms
step:1702/2160 train_time:92038ms step_avg:54.08ms
step:1703/2160 train_time:92127ms step_avg:54.10ms
step:1704/2160 train_time:92214ms step_avg:54.12ms
step:1705/2160 train_time:92303ms step_avg:54.14ms
step:1706/2160 train_time:92391ms step_avg:54.16ms
step:1707/2160 train_time:92479ms step_avg:54.18ms
step:1708/2160 train_time:92567ms step_avg:54.20ms
step:1709/2160 train_time:92657ms step_avg:54.22ms
step:1710/2160 train_time:92746ms step_avg:54.24ms
step:1711/2160 train_time:92834ms step_avg:54.26ms
step:1712/2160 train_time:92921ms step_avg:54.28ms
step:1713/2160 train_time:93011ms step_avg:54.30ms
step:1714/2160 train_time:93099ms step_avg:54.32ms
step:1715/2160 train_time:93187ms step_avg:54.34ms
step:1716/2160 train_time:93274ms step_avg:54.36ms
step:1717/2160 train_time:93363ms step_avg:54.38ms
step:1718/2160 train_time:93452ms step_avg:54.40ms
step:1719/2160 train_time:93541ms step_avg:54.42ms
step:1720/2160 train_time:93628ms step_avg:54.44ms
step:1721/2160 train_time:93718ms step_avg:54.46ms
step:1722/2160 train_time:93805ms step_avg:54.47ms
step:1723/2160 train_time:93895ms step_avg:54.50ms
step:1724/2160 train_time:93983ms step_avg:54.51ms
step:1725/2160 train_time:94072ms step_avg:54.53ms
step:1726/2160 train_time:94160ms step_avg:54.55ms
step:1727/2160 train_time:94250ms step_avg:54.57ms
step:1728/2160 train_time:94337ms step_avg:54.59ms
step:1729/2160 train_time:94426ms step_avg:54.61ms
step:1730/2160 train_time:94513ms step_avg:54.63ms
step:1731/2160 train_time:94603ms step_avg:54.65ms
step:1732/2160 train_time:94690ms step_avg:54.67ms
step:1733/2160 train_time:94779ms step_avg:54.69ms
step:1734/2160 train_time:94867ms step_avg:54.71ms
step:1735/2160 train_time:94956ms step_avg:54.73ms
step:1736/2160 train_time:95044ms step_avg:54.75ms
step:1737/2160 train_time:95133ms step_avg:54.77ms
step:1738/2160 train_time:95220ms step_avg:54.79ms
step:1739/2160 train_time:95309ms step_avg:54.81ms
step:1740/2160 train_time:95397ms step_avg:54.83ms
step:1741/2160 train_time:95487ms step_avg:54.85ms
step:1742/2160 train_time:95575ms step_avg:54.86ms
step:1743/2160 train_time:95664ms step_avg:54.88ms
step:1744/2160 train_time:95751ms step_avg:54.90ms
step:1745/2160 train_time:95840ms step_avg:54.92ms
step:1746/2160 train_time:95928ms step_avg:54.94ms
step:1747/2160 train_time:96017ms step_avg:54.96ms
step:1748/2160 train_time:96104ms step_avg:54.98ms
step:1749/2160 train_time:96194ms step_avg:55.00ms
step:1750/2160 train_time:96281ms step_avg:55.02ms
step:1750/2160 val_loss:3.3951 train_time:96371ms step_avg:55.07ms
step:1751/2160 train_time:96391ms step_avg:55.05ms
step:1752/2160 train_time:96463ms step_avg:55.06ms
step:1753/2160 train_time:96554ms step_avg:55.08ms
step:1754/2160 train_time:96641ms step_avg:55.10ms
step:1755/2160 train_time:96729ms step_avg:55.12ms
step:1756/2160 train_time:96816ms step_avg:55.13ms
step:1757/2160 train_time:96904ms step_avg:55.15ms
step:1758/2160 train_time:96993ms step_avg:55.17ms
step:1759/2160 train_time:97080ms step_avg:55.19ms
step:1760/2160 train_time:97167ms step_avg:55.21ms
step:1761/2160 train_time:97256ms step_avg:55.23ms
step:1762/2160 train_time:97344ms step_avg:55.25ms
step:1763/2160 train_time:97436ms step_avg:55.27ms
step:1764/2160 train_time:97524ms step_avg:55.29ms
step:1765/2160 train_time:97614ms step_avg:55.31ms
step:1766/2160 train_time:97701ms step_avg:55.32ms
step:1767/2160 train_time:97789ms step_avg:55.34ms
step:1768/2160 train_time:97876ms step_avg:55.36ms
step:1769/2160 train_time:97964ms step_avg:55.38ms
step:1770/2160 train_time:98050ms step_avg:55.40ms
step:1771/2160 train_time:98138ms step_avg:55.41ms
step:1772/2160 train_time:98225ms step_avg:55.43ms
step:1773/2160 train_time:98315ms step_avg:55.45ms
step:1774/2160 train_time:98403ms step_avg:55.47ms
step:1775/2160 train_time:98494ms step_avg:55.49ms
step:1776/2160 train_time:98582ms step_avg:55.51ms
step:1777/2160 train_time:98672ms step_avg:55.53ms
step:1778/2160 train_time:98759ms step_avg:55.54ms
step:1779/2160 train_time:98848ms step_avg:55.56ms
step:1780/2160 train_time:98935ms step_avg:55.58ms
step:1781/2160 train_time:99024ms step_avg:55.60ms
step:1782/2160 train_time:99111ms step_avg:55.62ms
step:1783/2160 train_time:99200ms step_avg:55.64ms
step:1784/2160 train_time:99287ms step_avg:55.65ms
step:1785/2160 train_time:99377ms step_avg:55.67ms
step:1786/2160 train_time:99465ms step_avg:55.69ms
step:1787/2160 train_time:99556ms step_avg:55.71ms
step:1788/2160 train_time:99643ms step_avg:55.73ms
step:1789/2160 train_time:99733ms step_avg:55.75ms
step:1790/2160 train_time:99820ms step_avg:55.77ms
step:1791/2160 train_time:99910ms step_avg:55.78ms
step:1792/2160 train_time:99997ms step_avg:55.80ms
step:1793/2160 train_time:100085ms step_avg:55.82ms
step:1794/2160 train_time:100173ms step_avg:55.84ms
step:1795/2160 train_time:100262ms step_avg:55.86ms
step:1796/2160 train_time:100349ms step_avg:55.87ms
step:1797/2160 train_time:100439ms step_avg:55.89ms
step:1798/2160 train_time:100527ms step_avg:55.91ms
step:1799/2160 train_time:100616ms step_avg:55.93ms
step:1800/2160 train_time:100703ms step_avg:55.95ms
step:1801/2160 train_time:100794ms step_avg:55.97ms
step:1802/2160 train_time:100881ms step_avg:55.98ms
step:1803/2160 train_time:100970ms step_avg:56.00ms
step:1804/2160 train_time:101057ms step_avg:56.02ms
step:1805/2160 train_time:101147ms step_avg:56.04ms
step:1806/2160 train_time:101234ms step_avg:56.05ms
step:1807/2160 train_time:101323ms step_avg:56.07ms
step:1808/2160 train_time:101410ms step_avg:56.09ms
step:1809/2160 train_time:101500ms step_avg:56.11ms
step:1810/2160 train_time:101588ms step_avg:56.13ms
step:1811/2160 train_time:101677ms step_avg:56.14ms
step:1812/2160 train_time:101765ms step_avg:56.16ms
step:1813/2160 train_time:101854ms step_avg:56.18ms
step:1814/2160 train_time:101942ms step_avg:56.20ms
step:1815/2160 train_time:102031ms step_avg:56.22ms
step:1816/2160 train_time:102119ms step_avg:56.23ms
step:1817/2160 train_time:102207ms step_avg:56.25ms
step:1818/2160 train_time:102295ms step_avg:56.27ms
step:1819/2160 train_time:102384ms step_avg:56.29ms
step:1820/2160 train_time:102471ms step_avg:56.30ms
step:1821/2160 train_time:102560ms step_avg:56.32ms
step:1822/2160 train_time:102648ms step_avg:56.34ms
step:1823/2160 train_time:102739ms step_avg:56.36ms
step:1824/2160 train_time:102826ms step_avg:56.37ms
step:1825/2160 train_time:102915ms step_avg:56.39ms
step:1826/2160 train_time:103002ms step_avg:56.41ms
step:1827/2160 train_time:103091ms step_avg:56.43ms
step:1828/2160 train_time:103180ms step_avg:56.44ms
step:1829/2160 train_time:103269ms step_avg:56.46ms
step:1830/2160 train_time:103356ms step_avg:56.48ms
step:1831/2160 train_time:103445ms step_avg:56.50ms
step:1832/2160 train_time:103533ms step_avg:56.51ms
step:1833/2160 train_time:103623ms step_avg:56.53ms
step:1834/2160 train_time:103711ms step_avg:56.55ms
step:1835/2160 train_time:103801ms step_avg:56.57ms
step:1836/2160 train_time:103888ms step_avg:56.58ms
step:1837/2160 train_time:103978ms step_avg:56.60ms
step:1838/2160 train_time:104065ms step_avg:56.62ms
step:1839/2160 train_time:104155ms step_avg:56.64ms
step:1840/2160 train_time:104242ms step_avg:56.65ms
step:1841/2160 train_time:104331ms step_avg:56.67ms
step:1842/2160 train_time:104419ms step_avg:56.69ms
step:1843/2160 train_time:104507ms step_avg:56.70ms
step:1844/2160 train_time:104596ms step_avg:56.72ms
step:1845/2160 train_time:104685ms step_avg:56.74ms
step:1846/2160 train_time:104773ms step_avg:56.76ms
step:1847/2160 train_time:104862ms step_avg:56.77ms
step:1848/2160 train_time:104949ms step_avg:56.79ms
step:1849/2160 train_time:105039ms step_avg:56.81ms
step:1850/2160 train_time:105126ms step_avg:56.82ms
step:1851/2160 train_time:105215ms step_avg:56.84ms
step:1852/2160 train_time:105302ms step_avg:56.86ms
step:1853/2160 train_time:105391ms step_avg:56.88ms
step:1854/2160 train_time:105479ms step_avg:56.89ms
step:1855/2160 train_time:105568ms step_avg:56.91ms
step:1856/2160 train_time:105656ms step_avg:56.93ms
step:1857/2160 train_time:105745ms step_avg:56.94ms
step:1858/2160 train_time:105833ms step_avg:56.96ms
step:1859/2160 train_time:105923ms step_avg:56.98ms
step:1860/2160 train_time:106010ms step_avg:56.99ms
step:1861/2160 train_time:106099ms step_avg:57.01ms
step:1862/2160 train_time:106186ms step_avg:57.03ms
step:1863/2160 train_time:106276ms step_avg:57.05ms
step:1864/2160 train_time:106363ms step_avg:57.06ms
step:1865/2160 train_time:106452ms step_avg:57.08ms
step:1866/2160 train_time:106540ms step_avg:57.10ms
step:1867/2160 train_time:106629ms step_avg:57.11ms
step:1868/2160 train_time:106716ms step_avg:57.13ms
step:1869/2160 train_time:106805ms step_avg:57.15ms
step:1870/2160 train_time:106893ms step_avg:57.16ms
step:1871/2160 train_time:106983ms step_avg:57.18ms
step:1872/2160 train_time:107070ms step_avg:57.20ms
step:1873/2160 train_time:107160ms step_avg:57.21ms
step:1874/2160 train_time:107248ms step_avg:57.23ms
step:1875/2160 train_time:107337ms step_avg:57.25ms
step:1876/2160 train_time:107425ms step_avg:57.26ms
step:1877/2160 train_time:107515ms step_avg:57.28ms
step:1878/2160 train_time:107603ms step_avg:57.30ms
step:1879/2160 train_time:107693ms step_avg:57.31ms
step:1880/2160 train_time:107781ms step_avg:57.33ms
step:1881/2160 train_time:107870ms step_avg:57.35ms
step:1882/2160 train_time:107958ms step_avg:57.36ms
step:1883/2160 train_time:108048ms step_avg:57.38ms
step:1884/2160 train_time:108135ms step_avg:57.40ms
step:1885/2160 train_time:108223ms step_avg:57.41ms
step:1886/2160 train_time:108311ms step_avg:57.43ms
step:1887/2160 train_time:108400ms step_avg:57.45ms
step:1888/2160 train_time:108487ms step_avg:57.46ms
step:1889/2160 train_time:108577ms step_avg:57.48ms
step:1890/2160 train_time:108665ms step_avg:57.49ms
step:1891/2160 train_time:108755ms step_avg:57.51ms
step:1892/2160 train_time:108842ms step_avg:57.53ms
step:1893/2160 train_time:108931ms step_avg:57.54ms
step:1894/2160 train_time:109018ms step_avg:57.56ms
step:1895/2160 train_time:109108ms step_avg:57.58ms
step:1896/2160 train_time:109195ms step_avg:57.59ms
step:1897/2160 train_time:109283ms step_avg:57.61ms
step:1898/2160 train_time:109372ms step_avg:57.62ms
step:1899/2160 train_time:109462ms step_avg:57.64ms
step:1900/2160 train_time:109550ms step_avg:57.66ms
step:1901/2160 train_time:109639ms step_avg:57.67ms
step:1902/2160 train_time:109726ms step_avg:57.69ms
step:1903/2160 train_time:109815ms step_avg:57.71ms
step:1904/2160 train_time:109903ms step_avg:57.72ms
step:1905/2160 train_time:109993ms step_avg:57.74ms
step:1906/2160 train_time:110080ms step_avg:57.75ms
step:1907/2160 train_time:110169ms step_avg:57.77ms
step:1908/2160 train_time:110257ms step_avg:57.79ms
step:1909/2160 train_time:110345ms step_avg:57.80ms
step:1910/2160 train_time:110432ms step_avg:57.82ms
step:1911/2160 train_time:110523ms step_avg:57.84ms
step:1912/2160 train_time:110610ms step_avg:57.85ms
step:1913/2160 train_time:110700ms step_avg:57.87ms
step:1914/2160 train_time:110788ms step_avg:57.88ms
step:1915/2160 train_time:110877ms step_avg:57.90ms
step:1916/2160 train_time:110963ms step_avg:57.91ms
step:1917/2160 train_time:111053ms step_avg:57.93ms
step:1918/2160 train_time:111141ms step_avg:57.95ms
step:1919/2160 train_time:111230ms step_avg:57.96ms
step:1920/2160 train_time:111318ms step_avg:57.98ms
step:1921/2160 train_time:111406ms step_avg:57.99ms
step:1922/2160 train_time:111494ms step_avg:58.01ms
step:1923/2160 train_time:111583ms step_avg:58.03ms
step:1924/2160 train_time:111670ms step_avg:58.04ms
step:1925/2160 train_time:111760ms step_avg:58.06ms
step:1926/2160 train_time:111848ms step_avg:58.07ms
step:1927/2160 train_time:111937ms step_avg:58.09ms
step:1928/2160 train_time:112023ms step_avg:58.10ms
step:1929/2160 train_time:112113ms step_avg:58.12ms
step:1930/2160 train_time:112201ms step_avg:58.14ms
step:1931/2160 train_time:112290ms step_avg:58.15ms
step:1932/2160 train_time:112378ms step_avg:58.17ms
step:1933/2160 train_time:112467ms step_avg:58.18ms
step:1934/2160 train_time:112554ms step_avg:58.20ms
step:1935/2160 train_time:112644ms step_avg:58.21ms
step:1936/2160 train_time:112733ms step_avg:58.23ms
step:1937/2160 train_time:112822ms step_avg:58.25ms
step:1938/2160 train_time:112909ms step_avg:58.26ms
step:1939/2160 train_time:112999ms step_avg:58.28ms
step:1940/2160 train_time:113086ms step_avg:58.29ms
step:1941/2160 train_time:113176ms step_avg:58.31ms
step:1942/2160 train_time:113262ms step_avg:58.32ms
step:1943/2160 train_time:113352ms step_avg:58.34ms
step:1944/2160 train_time:113439ms step_avg:58.35ms
step:1945/2160 train_time:113528ms step_avg:58.37ms
step:1946/2160 train_time:113615ms step_avg:58.38ms
step:1947/2160 train_time:113704ms step_avg:58.40ms
step:1948/2160 train_time:113792ms step_avg:58.41ms
step:1949/2160 train_time:113882ms step_avg:58.43ms
step:1950/2160 train_time:113970ms step_avg:58.45ms
step:1951/2160 train_time:114059ms step_avg:58.46ms
step:1952/2160 train_time:114147ms step_avg:58.48ms
step:1953/2160 train_time:114237ms step_avg:58.49ms
step:1954/2160 train_time:114324ms step_avg:58.51ms
step:1955/2160 train_time:114412ms step_avg:58.52ms
step:1956/2160 train_time:114500ms step_avg:58.54ms
step:1957/2160 train_time:114589ms step_avg:58.55ms
step:1958/2160 train_time:114677ms step_avg:58.57ms
step:1959/2160 train_time:114766ms step_avg:58.58ms
step:1960/2160 train_time:114853ms step_avg:58.60ms
step:1961/2160 train_time:114942ms step_avg:58.61ms
step:1962/2160 train_time:115029ms step_avg:58.63ms
step:1963/2160 train_time:115119ms step_avg:58.64ms
step:1964/2160 train_time:115207ms step_avg:58.66ms
step:1965/2160 train_time:115296ms step_avg:58.67ms
step:1966/2160 train_time:115383ms step_avg:58.69ms
step:1967/2160 train_time:115472ms step_avg:58.70ms
step:1968/2160 train_time:115560ms step_avg:58.72ms
step:1969/2160 train_time:115649ms step_avg:58.74ms
step:1970/2160 train_time:115736ms step_avg:58.75ms
step:1971/2160 train_time:115826ms step_avg:58.76ms
step:1972/2160 train_time:115913ms step_avg:58.78ms
step:1973/2160 train_time:116003ms step_avg:58.80ms
step:1974/2160 train_time:116090ms step_avg:58.81ms
step:1975/2160 train_time:116179ms step_avg:58.82ms
step:1976/2160 train_time:116265ms step_avg:58.84ms
step:1977/2160 train_time:116356ms step_avg:58.85ms
step:1978/2160 train_time:116442ms step_avg:58.87ms
step:1979/2160 train_time:116532ms step_avg:58.88ms
step:1980/2160 train_time:116620ms step_avg:58.90ms
step:1981/2160 train_time:116709ms step_avg:58.91ms
step:1982/2160 train_time:116797ms step_avg:58.93ms
step:1983/2160 train_time:116886ms step_avg:58.94ms
step:1984/2160 train_time:116973ms step_avg:58.96ms
step:1985/2160 train_time:117062ms step_avg:58.97ms
step:1986/2160 train_time:117149ms step_avg:58.99ms
step:1987/2160 train_time:117239ms step_avg:59.00ms
step:1988/2160 train_time:117326ms step_avg:59.02ms
step:1989/2160 train_time:117416ms step_avg:59.03ms
step:1990/2160 train_time:117503ms step_avg:59.05ms
step:1991/2160 train_time:117593ms step_avg:59.06ms
step:1992/2160 train_time:117681ms step_avg:59.08ms
step:1993/2160 train_time:117770ms step_avg:59.09ms
step:1994/2160 train_time:117858ms step_avg:59.11ms
step:1995/2160 train_time:117947ms step_avg:59.12ms
step:1996/2160 train_time:118034ms step_avg:59.14ms
step:1997/2160 train_time:118123ms step_avg:59.15ms
step:1998/2160 train_time:118210ms step_avg:59.16ms
step:1999/2160 train_time:118300ms step_avg:59.18ms
step:2000/2160 train_time:118387ms step_avg:59.19ms
step:2000/2160 val_loss:3.3159 train_time:118478ms step_avg:59.24ms
step:2001/2160 train_time:118497ms step_avg:59.22ms
step:2002/2160 train_time:118572ms step_avg:59.23ms
step:2003/2160 train_time:118664ms step_avg:59.24ms
step:2004/2160 train_time:118752ms step_avg:59.26ms
step:2005/2160 train_time:118841ms step_avg:59.27ms
step:2006/2160 train_time:118927ms step_avg:59.29ms
step:2007/2160 train_time:119016ms step_avg:59.30ms
step:2008/2160 train_time:119102ms step_avg:59.31ms
step:2009/2160 train_time:119191ms step_avg:59.33ms
step:2010/2160 train_time:119278ms step_avg:59.34ms
step:2011/2160 train_time:119367ms step_avg:59.36ms
step:2012/2160 train_time:119456ms step_avg:59.37ms
step:2013/2160 train_time:119548ms step_avg:59.39ms
step:2014/2160 train_time:119637ms step_avg:59.40ms
step:2015/2160 train_time:119726ms step_avg:59.42ms
step:2016/2160 train_time:119814ms step_avg:59.43ms
step:2017/2160 train_time:119903ms step_avg:59.45ms
step:2018/2160 train_time:119990ms step_avg:59.46ms
step:2019/2160 train_time:120078ms step_avg:59.47ms
step:2020/2160 train_time:120166ms step_avg:59.49ms
step:2021/2160 train_time:120254ms step_avg:59.50ms
step:2022/2160 train_time:120342ms step_avg:59.52ms
step:2023/2160 train_time:120433ms step_avg:59.53ms
step:2024/2160 train_time:120521ms step_avg:59.55ms
step:2025/2160 train_time:120612ms step_avg:59.56ms
step:2026/2160 train_time:120700ms step_avg:59.58ms
step:2027/2160 train_time:120789ms step_avg:59.59ms
step:2028/2160 train_time:120877ms step_avg:59.60ms
step:2029/2160 train_time:120966ms step_avg:59.62ms
step:2030/2160 train_time:121052ms step_avg:59.63ms
step:2031/2160 train_time:121140ms step_avg:59.65ms
step:2032/2160 train_time:121226ms step_avg:59.66ms
step:2033/2160 train_time:121316ms step_avg:59.67ms
step:2034/2160 train_time:121404ms step_avg:59.69ms
step:2035/2160 train_time:121493ms step_avg:59.70ms
step:2036/2160 train_time:121581ms step_avg:59.72ms
step:2037/2160 train_time:121672ms step_avg:59.73ms
step:2038/2160 train_time:121759ms step_avg:59.74ms
step:2039/2160 train_time:121848ms step_avg:59.76ms
step:2040/2160 train_time:121936ms step_avg:59.77ms
step:2041/2160 train_time:122024ms step_avg:59.79ms
step:2042/2160 train_time:122112ms step_avg:59.80ms
step:2043/2160 train_time:122201ms step_avg:59.81ms
step:2044/2160 train_time:122287ms step_avg:59.83ms
step:2045/2160 train_time:122376ms step_avg:59.84ms
step:2046/2160 train_time:122465ms step_avg:59.86ms
step:2047/2160 train_time:122554ms step_avg:59.87ms
step:2048/2160 train_time:122642ms step_avg:59.88ms
step:2049/2160 train_time:122732ms step_avg:59.90ms
step:2050/2160 train_time:122821ms step_avg:59.91ms
step:2051/2160 train_time:122909ms step_avg:59.93ms
step:2052/2160 train_time:122996ms step_avg:59.94ms
step:2053/2160 train_time:123085ms step_avg:59.95ms
step:2054/2160 train_time:123173ms step_avg:59.97ms
step:2055/2160 train_time:123262ms step_avg:59.98ms
step:2056/2160 train_time:123350ms step_avg:59.99ms
step:2057/2160 train_time:123439ms step_avg:60.01ms
step:2058/2160 train_time:123528ms step_avg:60.02ms
step:2059/2160 train_time:123618ms step_avg:60.04ms
step:2060/2160 train_time:123705ms step_avg:60.05ms
step:2061/2160 train_time:123794ms step_avg:60.07ms
step:2062/2160 train_time:123882ms step_avg:60.08ms
step:2063/2160 train_time:123971ms step_avg:60.09ms
step:2064/2160 train_time:124058ms step_avg:60.11ms
step:2065/2160 train_time:124147ms step_avg:60.12ms
step:2066/2160 train_time:124234ms step_avg:60.13ms
step:2067/2160 train_time:124324ms step_avg:60.15ms
step:2068/2160 train_time:124412ms step_avg:60.16ms
step:2069/2160 train_time:124502ms step_avg:60.17ms
step:2070/2160 train_time:124590ms step_avg:60.19ms
step:2071/2160 train_time:124680ms step_avg:60.20ms
step:2072/2160 train_time:124767ms step_avg:60.22ms
step:2073/2160 train_time:124858ms step_avg:60.23ms
step:2074/2160 train_time:124945ms step_avg:60.24ms
step:2075/2160 train_time:125033ms step_avg:60.26ms
step:2076/2160 train_time:125120ms step_avg:60.27ms
step:2077/2160 train_time:125210ms step_avg:60.28ms
step:2078/2160 train_time:125298ms step_avg:60.30ms
step:2079/2160 train_time:125386ms step_avg:60.31ms
step:2080/2160 train_time:125474ms step_avg:60.32ms
step:2081/2160 train_time:125563ms step_avg:60.34ms
step:2082/2160 train_time:125650ms step_avg:60.35ms
step:2083/2160 train_time:125740ms step_avg:60.36ms
step:2084/2160 train_time:125827ms step_avg:60.38ms
step:2085/2160 train_time:125917ms step_avg:60.39ms
step:2086/2160 train_time:126004ms step_avg:60.40ms
step:2087/2160 train_time:126093ms step_avg:60.42ms
step:2088/2160 train_time:126181ms step_avg:60.43ms
step:2089/2160 train_time:126270ms step_avg:60.45ms
step:2090/2160 train_time:126357ms step_avg:60.46ms
step:2091/2160 train_time:126446ms step_avg:60.47ms
step:2092/2160 train_time:126534ms step_avg:60.48ms
step:2093/2160 train_time:126624ms step_avg:60.50ms
step:2094/2160 train_time:126712ms step_avg:60.51ms
step:2095/2160 train_time:126802ms step_avg:60.53ms
step:2096/2160 train_time:126889ms step_avg:60.54ms
step:2097/2160 train_time:126979ms step_avg:60.55ms
step:2098/2160 train_time:127066ms step_avg:60.57ms
step:2099/2160 train_time:127155ms step_avg:60.58ms
step:2100/2160 train_time:127242ms step_avg:60.59ms
step:2101/2160 train_time:127331ms step_avg:60.61ms
step:2102/2160 train_time:127420ms step_avg:60.62ms
step:2103/2160 train_time:127509ms step_avg:60.63ms
step:2104/2160 train_time:127597ms step_avg:60.65ms
step:2105/2160 train_time:127686ms step_avg:60.66ms
step:2106/2160 train_time:127773ms step_avg:60.67ms
step:2107/2160 train_time:127864ms step_avg:60.69ms
step:2108/2160 train_time:127952ms step_avg:60.70ms
step:2109/2160 train_time:128042ms step_avg:60.71ms
step:2110/2160 train_time:128129ms step_avg:60.72ms
step:2111/2160 train_time:128220ms step_avg:60.74ms
step:2112/2160 train_time:128307ms step_avg:60.75ms
step:2113/2160 train_time:128396ms step_avg:60.76ms
step:2114/2160 train_time:128484ms step_avg:60.78ms
step:2115/2160 train_time:128573ms step_avg:60.79ms
step:2116/2160 train_time:128661ms step_avg:60.80ms
step:2117/2160 train_time:128749ms step_avg:60.82ms
step:2118/2160 train_time:128837ms step_avg:60.83ms
step:2119/2160 train_time:128926ms step_avg:60.84ms
step:2120/2160 train_time:129013ms step_avg:60.86ms
step:2121/2160 train_time:129103ms step_avg:60.87ms
step:2122/2160 train_time:129192ms step_avg:60.88ms
step:2123/2160 train_time:129281ms step_avg:60.90ms
step:2124/2160 train_time:129368ms step_avg:60.91ms
step:2125/2160 train_time:129458ms step_avg:60.92ms
step:2126/2160 train_time:129545ms step_avg:60.93ms
step:2127/2160 train_time:129635ms step_avg:60.95ms
step:2128/2160 train_time:129723ms step_avg:60.96ms
step:2129/2160 train_time:129813ms step_avg:60.97ms
step:2130/2160 train_time:129901ms step_avg:60.99ms
step:2131/2160 train_time:129991ms step_avg:61.00ms
step:2132/2160 train_time:130078ms step_avg:61.01ms
step:2133/2160 train_time:130167ms step_avg:61.03ms
step:2134/2160 train_time:130255ms step_avg:61.04ms
step:2135/2160 train_time:130344ms step_avg:61.05ms
step:2136/2160 train_time:130432ms step_avg:61.06ms
step:2137/2160 train_time:130523ms step_avg:61.08ms
step:2138/2160 train_time:130611ms step_avg:61.09ms
step:2139/2160 train_time:130701ms step_avg:61.10ms
step:2140/2160 train_time:130790ms step_avg:61.12ms
step:2141/2160 train_time:130879ms step_avg:61.13ms
step:2142/2160 train_time:130967ms step_avg:61.14ms
step:2143/2160 train_time:131056ms step_avg:61.16ms
step:2144/2160 train_time:131144ms step_avg:61.17ms
step:2145/2160 train_time:131234ms step_avg:61.18ms
step:2146/2160 train_time:131321ms step_avg:61.19ms
step:2147/2160 train_time:131411ms step_avg:61.21ms
step:2148/2160 train_time:131498ms step_avg:61.22ms
step:2149/2160 train_time:131587ms step_avg:61.23ms
step:2150/2160 train_time:131675ms step_avg:61.24ms
step:2151/2160 train_time:131765ms step_avg:61.26ms
step:2152/2160 train_time:131853ms step_avg:61.27ms
step:2153/2160 train_time:131942ms step_avg:61.28ms
step:2154/2160 train_time:132030ms step_avg:61.30ms
step:2155/2160 train_time:132121ms step_avg:61.31ms
step:2156/2160 train_time:132209ms step_avg:61.32ms
step:2157/2160 train_time:132298ms step_avg:61.33ms
step:2158/2160 train_time:132386ms step_avg:61.35ms
step:2159/2160 train_time:132476ms step_avg:61.36ms
step:2160/2160 train_time:132563ms step_avg:61.37ms
step:2160/2160 val_loss:3.2795 train_time:132654ms step_avg:61.41ms
peak memory allocated: 29896 MiB reserved: 61208 MiB

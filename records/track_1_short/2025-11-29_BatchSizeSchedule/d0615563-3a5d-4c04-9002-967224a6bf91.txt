import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device="cuda", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng


@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A
        aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X
        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

# -----------------------------------------------------------------------------
# Muon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor.
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter smear_gate (1 param 7 padding params)
        2. reduce scatter attn_gate (10 params 6 padding params)
        3. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        4. reduce scatter attn/mlp round 2 (16 mlp params)
        5. wait on step 1, then compute update of 1 and schedule all gather
        6. wait on step 2, then compute update of 2 and schedule all gather
        7. wait on step 3, then compute update of 3 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        8. wait on 4, then compute update of 4 and schedule all gather
        9. wait for each all gather to complete and update params
    Empirically, leading with small params provides an additional 0.2s improvement.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            group["momentum_buffer"].zero_()
            group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        module_group_order = ['smear_gate', 'attn_gate', 'attn', 'mlp']
        params_list = list(params)
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        group_sizes = [1, 10, 16, 16]
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    @torch.no_grad()
    def step(self):
        # Efficient systems-wise implementation of step developed by @YouJiacheng,
        # @KonstantinWilleke, @alexrgilbert, @adricarda, @tuttyfrutyee, @vdlad,
        # @ryanyang0, @vagrawal, and @varunneal.
        rank = dist.get_rank()
        group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))

        all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"]
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params])
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                # Reshape attn params from [hdim, dim*4] to [4,hdim,dim]
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1], grad_shape[2] // 4)
            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1])
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr" not in group:
                group["param_lr"] = (
                    max(1., param_shape[-2] / param_shape[-1]) ** 0.5
                    * ref_param.new_tensor(
                        [getattr(param, "lr_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                    ).view(-1, 1, 1)
                )

                group["param_wd"] = ref_param.new_tensor(
                    [getattr(param, "wd_mul", 1.0) for param in params[module_idx:module_idx + num_params]]
                ).view(-1, 1, 1)

            # Determine LR and WR
            eff_lr = group["lr"] * group["param_lr"]
            eff_wd = group["lr"] * group["weight_decay"] * group["param_wd"]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads)

            # NorMuon: second_momentum_buffer tracks squared magnitude of gradients along one dim (https://arxiv.org/pdf/2510.05491)
            v_norm = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_mean = v_chunk.square().mean(dim=-1 if param_shape[-2] >= param_shape[-1] else -2, keepdim=True)
            second_momentum_buffer.lerp_(v_mean.to(dtype=ref_param.dtype), 1 - group["beta2"])
            step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
            v_chunk.mul_(step_size)
            v_norm_new = v_chunk.norm(dim=(-2, -1), keepdim=True)
            v_chunk.mul_(v_norm / v_norm_new.clamp_min_(1e-10))

            v_chunk = v_chunk.view(grad_shape)

            updated_params = torch.empty_like(grad_chunk)
            param_chunk = torch.stack(params[module_idx:module_idx + num_params]) if num_params > 0 else torch.zeros_like(v_chunk)

            # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            mask = (v_chunk * param_chunk) >= 0
            v_chunk.addcmul_(param_chunk, (eff_wd * mask).to(ref_param.dtype))

            param_chunk.addcmul_(v_chunk, -eff_lr)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete and copy results back into original parameter tensors.
        for info in all_gather_infos:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, lr: float = 1e-3, betas: tuple[float, float] = (0.9, 0.999), eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)
        params = list(params)
        sizes = {p.shape for p in params}
        # create one buffer per unique parameter-size
        param_groups = []
        for size in sizes:
            group_params = [p for p in params if p.shape == size]
            param_groups.append(dict(params=group_params))
        super().__init__(param_groups, defaults)
        # init state
        for p in params:
            chunk_size = p.size(0) // self.world_size
            exp_avg = torch.zeros_like(p[:chunk_size], dtype=torch.bfloat16, device=p[0].device)
            exp_avg_sq = torch.zeros_like(exp_avg)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=exp_avg_sq)
        # DistributedAdam implementation by @vagrawal, @akash5474

        self.should_sync = False

        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            for param in params:
                hook = param.register_post_accumulate_grad_hook(self._sync_gradient)
                self._reduce_scatter_hooks.append(hook)

    @torch.compile
    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        rank_size = grad.shape[0] // self.world_size
        grad_slice = torch.empty_like(grad[:rank_size])
        self._reduce_scatter_futures[param] = (
            dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
            grad_slice
        )

    @torch.compile
    @torch.no_grad()
    def step(self):
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        for group in reversed(self.param_groups):
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in reversed(group['params']):
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                rank_size = param.shape[0] // self.world_size
                p_slice = param[rank * rank_size:(rank + 1) * rank_size]
                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]

                exp_avg = state["exp_avg"]
                exp_avg_sq = state["exp_avg_sq"]
                state["step"] += 1
                t = state["step"]
                # weight decay
                if wd != 0:
                    eff_weight_decay = lr * wd * getattr(param, "wd_mul", 1.0)
                    p_slice.mul_(1 - eff_weight_decay)
                # update running averages
                exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)
                exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)
                # bias corrections
                bias1 = 1 - beta1 ** t
                bias2 = 1 - beta2 ** t
                # compute step
                denom = exp_avg_sq.sqrt().add_(eps)
                step_size = lr * (bias2 ** 0.5 / bias1)
                update = exp_avg.div(denom).mul_(step_size)
                p_slice.add_(other=update, alpha=-1.0)

                all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())

        self._reduce_scatter_futures.clear()
        torch.futures.collect_all(all_gather_futures).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

# yarn implementation @classiclarryd
class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//4)])
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.cos = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.sin = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.cos.copy_(theta.cos())
        self.sin.copy_(theta.sin())
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

def rotary(x_BTHD: Tensor, cos: Tensor, sin: Tensor):
    assert cos.size(0) >= x_BTHD.size(-3)
    cos, sin = (
        cos[None, : x_BTHD.size(-3), None, :],
        sin[None, : x_BTHD.size(-3), None, :],
    )
    x1, x2 = x_BTHD.chunk(2, dim=-1)
    y1 = x1 * cos + x2 * sin
    y2 = x1 * (-sin) + x2 * cos
    return torch.cat((y1, y2), 3)

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    cos: torch.Tensor
    sin: torch.Tensor
    attn_scale: float

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = 0.5 * (self.dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # make matrices the same shape as MLP to enable batched call in optimizer
        self.qkvo_w = nn.Parameter(torch.empty(self.hdim, self.dim*4))
        # label module to enable custom optimizer sizing
        self.qkvo_w.label='attn'

        with torch.no_grad():
            self.qkvo_w.view(4,self.hdim, self.dim)[:3].uniform_(-bound, bound) # init QKV weights
            self.qkvo_w.view(4,self.hdim, self.dim)[3].zero_() # init output weights to zero

        # sparse gated attention to enable context based no-op by @classiclarryd
        self.attn_gate = CastedLinear(12, num_heads)
        # label module to enable custom optimizer sizing
        self.attn_gate.weight.label = 'attn_gate'

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        cos, sin = attn_args.cos, attn_args.sin
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, attn_scale, bm_size = attn_args.seqlens, attn_args.attn_scale, attn_args.bm_size

        q, k, v = F.linear(x, self.qkvo_w.view(4, self.hdim, self.dim)[:3].flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = rotary(q, cos, sin), rotary(k, cos, sin)
        if ve is not None:
            v = sa_lambdas[0] * v + sa_lambdas[1] * ve.view_as(v) # @ KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = sa_lambdas[0] * v

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(self.attn_gate(x[..., :self.attn_gate.weight.size(-1)])).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, self.qkvo_w.view(4, self.hdim, self.dim)[3].type_as(y))
        return y


class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # make matrices the same shape to enable batched call in optimizer
        self.c_fc = nn.Parameter(torch.empty(dim, hdim))
        self.c_proj = nn.Parameter(torch.empty(dim, hdim))
        # label modules to enable custom optimizer sizing
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        # corrective factor to account for transpose
        self.c_fc.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.T.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, head_dim, num_heads) if layer_idx not in [0, 7] else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim) if layer_idx != 0 else None

    def forward(self, x: Tensor, x0: Tensor, lambdas: Tensor, attn_args: AttnArgs):
        x = lambdas[0] * x + lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int):
        super().__init__()
        vocab_size = next_multiple_of_n(vocab_size, n=128)
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.smear_gate = CastedLinear(12, 1)
        # label modules to enable custom optimizer sizing
        self.smear_gate.weight.label = 'smear_gate'
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)
        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=(model_dim**0.5)/448, w_s=2**-9, grad_s=1/448)
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        pad = (-num_layers * 5 - 2) % dist.get_world_size()
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    -1.5
                    * torch.ones(num_layers),  # skip_weights -> σ(-1.5) ≈ 0.18
                    *[
                        torch.tensor([1.1, 0.0]) for _ in range(num_layers)
                    ],  # block lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                        # ~3x higher weight to layer 1 compared to 12 at init.
                    *[
                        torch.tensor([0.5, 0.5]) for _ in range(num_layers)
                    ],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    torch.ones(pad),
                ]
            )
        )
        # set learning rates
        for param in self.embed.parameters():
            param.lr_mul = 75.
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
        self.lm_head.weight.lr_mul = 1.0
        self.scalars.lr_mul = 5.0

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, ws_short: int, ws_long: int):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [None, ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [None, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == len(self.blocks)

        x = self.embed(input_seq)

        skip_weights = self.scalars[:(len(self.blocks) // 2)]
        lambdas = self.scalars[1 * len(self.blocks): 3 * len(self.blocks)].view(-1, 2)
        sa_lambdas = self.scalars[3 * len(self.blocks): 5 * len(self.blocks)].view(-1, 2)
        smear_lambda = self.scalars[5 * len(self.blocks)]
        backout_lambda = self.scalars[5 * len(self.blocks)+1]

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # create skip connection from layer 4 (long attn window) to layer 7 (no attn op)
        skip_connections = []
        n = len(self.blocks) // 2
        skip_in = [4]
        skip_out = [7]

        x_backout = None
        backout_layer = 8
        # skip layer zero
        for i in range(1,len(self.blocks)):
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                cos=self.yarn.cos,
                sin=self.yarn.sin,
                attn_scale=self.yarn.attn_scale
            )
            if i in skip_out:
                gate = torch.sigmoid(skip_weights[i - n])  # in (0, 1)
                x = x + gate * skip_connections.pop()
            x = self.blocks[i](x, x0, lambdas[i], attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 8 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        logits_for_loss = logits.float() if not self.training else logits
        loss = F.cross_entropy(
            logits_for_loss.view(-1, logits_for_loss.size(-1)),
            target_seq,
            reduction="sum" if self.training else "mean",
        )
        return loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead of position {cur}; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        new_params = yield (
            _inputs.to(device="cuda", dtype=torch.int32, non_blocking=True),
            _targets.to(device="cuda", dtype=torch.int64, non_blocking=True),
            _cum_lengths.to(device="cuda", dtype=torch.int32, non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len


# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 2120  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.55  # fraction of num_scheduled_iterations spent cooling down the learning rate
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=12,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size)
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n and "gate" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]
gate_params = [p for n, p in model.named_parameters() if "gate" in n]

# init the optimizer(s)
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = DistAdam(
    scalar_params + head_params + embed_params,
    lr=0.008,
    betas=(0.65, 0.95),
    eps=1e-8,
    weight_decay=0.0,
)
optimizer2 = NorMuon(hidden_matrix_params + gate_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: tied to batch size schedule
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 0.33:
       lr_max = (16 / 8) * 0.8
    if x > 0.66:
        lr_max = (24 / 8) * 0.8
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]


def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]


def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

def step_optimizers(step: int, optimizers, model):
    # update lr
    for optimizer in optimizers:
        for group in optimizer.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)

    # set muon momentum based on step
    momentum = get_muon_momentum(step)
    for group in optimizers[1].param_groups:
        group["momentum"] = momentum

    # on even steps, only step Muon params
    # on odd steps, step all params
    if step%2==0:
        optimizers[1].step()
        optimizers[1].zero_grad(set_to_none=True)
    else:
        for optimizer in optimizers:
            optimizer.step()
        model.zero_grad(set_to_none=True)
        # disable sync in the next training step for the adam optimizer
        optimizers[0].should_sync = False

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
ws_schedule = list(args.ws_schedule) + [args.ws_final]
bs_schedule = list(args.train_bs_schedule) + [args.train_bs_extension]
ws_long = ws_schedule[0]
model.yarn.reset()
assert len(ws_schedule) == len(bs_schedule), "This warmup assumes len(ws_schedule) == len(bs_schedule)"

for idx in range(len(ws_schedule)):
    send_args = None
    if idx != 0:
        new_ws_long = ws_schedule[idx]
        model.yarn.apply(ws_long, new_ws_long)
        ws_long = new_ws_long
        send_args = (bs_schedule[idx], args.train_max_seq_len, grad_accum_steps)
    for step in range(warmup_steps):
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        model(inputs, targets, cum_seqlens, ws_long//2, ws_long).backward()
        for opt in optimizers:
            opt.step()
        model.zero_grad(set_to_none=True)

model.yarn.reset() # rotary buffer is not stored in state_dict
optimizer2.reset() # muon momentum buffers not in state dict
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del train_loader, initial_state

########################################
#        Training and validation       #
########################################

step_batch_size = args.train_bs_schedule[0]
train_loader = distributed_data_generator(args.train_files, step_batch_size, args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
ws_short, ws_long = get_ws(0)
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    ws_short, new_ws_long = get_ws(step)
    if new_ws_long != ws_long:
        model.yarn.apply(ws_long, new_ws_long)
        ws_long=new_ws_long

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            ws_long = args.ws_validate_post_yarn_ext
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                val_loss += model(inputs, targets, cum_seqlens, ws_short, ws_long)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    new_step_batch_size = get_bs(step)
    send_args = (new_step_batch_size, args.train_max_seq_len, grad_accum_steps) if new_step_batch_size != step_batch_size else None
    step_batch_size = new_step_batch_size
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizer on the last iteration before we step it
        if idx == grad_accum_steps - 1 and step % 2 == 1:
            optimizers[0].should_sync = True

        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, ws_short, ws_long) / grad_accum_steps).backward()
    step_optimizers(step, optimizers, model)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Aug 15 2025, 14:32:43) [GCC 11.4.0]
Running PyTorch 2.10.0.dev20250926+cu126 compiled for CUDA 12.6
Running Triton version 3.5.0
Sun Nov 30 03:21:15 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   36C    P0            123W /  700W |    5858MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0            109W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   36C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0            113W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   34C    P0            115W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   31C    P0            116W /  700W |    1520MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/2160 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/2160 train_time:94ms step_avg:93.95ms
step:2/2160 train_time:132ms step_avg:65.86ms
step:3/2160 train_time:150ms step_avg:50.14ms
step:4/2160 train_time:170ms step_avg:42.48ms
step:5/2160 train_time:200ms step_avg:40.10ms
step:6/2160 train_time:294ms step_avg:49.04ms
step:7/2160 train_time:323ms step_avg:46.09ms
step:8/2160 train_time:356ms step_avg:44.44ms
step:9/2160 train_time:389ms step_avg:43.26ms
step:10/2160 train_time:422ms step_avg:42.21ms
step:11/2160 train_time:456ms step_avg:41.50ms
step:12/2160 train_time:489ms step_avg:40.78ms
step:13/2160 train_time:524ms step_avg:40.30ms
step:14/2160 train_time:557ms step_avg:39.78ms
step:15/2160 train_time:591ms step_avg:39.40ms
step:16/2160 train_time:624ms step_avg:39.00ms
step:17/2160 train_time:658ms step_avg:38.71ms
step:18/2160 train_time:691ms step_avg:38.40ms
step:19/2160 train_time:725ms step_avg:38.18ms
step:20/2160 train_time:759ms step_avg:37.93ms
step:21/2160 train_time:793ms step_avg:37.76ms
step:22/2160 train_time:826ms step_avg:37.54ms
step:23/2160 train_time:860ms step_avg:37.39ms
step:24/2160 train_time:893ms step_avg:37.22ms
step:25/2160 train_time:927ms step_avg:37.09ms
step:26/2160 train_time:960ms step_avg:36.93ms
step:27/2160 train_time:995ms step_avg:36.84ms
step:28/2160 train_time:1027ms step_avg:36.70ms
step:29/2160 train_time:1062ms step_avg:36.62ms
step:30/2160 train_time:1095ms step_avg:36.51ms
step:31/2160 train_time:1129ms step_avg:36.43ms
step:32/2160 train_time:1162ms step_avg:36.32ms
step:33/2160 train_time:1197ms step_avg:36.27ms
step:34/2160 train_time:1230ms step_avg:36.18ms
step:35/2160 train_time:1266ms step_avg:36.16ms
step:36/2160 train_time:1299ms step_avg:36.08ms
step:37/2160 train_time:1334ms step_avg:36.05ms
step:38/2160 train_time:1367ms step_avg:35.97ms
step:39/2160 train_time:1402ms step_avg:35.94ms
step:40/2160 train_time:1435ms step_avg:35.87ms
step:41/2160 train_time:1469ms step_avg:35.84ms
step:42/2160 train_time:1503ms step_avg:35.78ms
step:43/2160 train_time:1537ms step_avg:35.74ms
step:44/2160 train_time:1570ms step_avg:35.67ms
step:45/2160 train_time:1604ms step_avg:35.65ms
step:46/2160 train_time:1638ms step_avg:35.60ms
step:47/2160 train_time:1672ms step_avg:35.57ms
step:48/2160 train_time:1705ms step_avg:35.52ms
step:49/2160 train_time:1739ms step_avg:35.49ms
step:50/2160 train_time:1772ms step_avg:35.44ms
step:51/2160 train_time:1806ms step_avg:35.41ms
step:52/2160 train_time:1839ms step_avg:35.37ms
step:53/2160 train_time:1873ms step_avg:35.35ms
step:54/2160 train_time:1907ms step_avg:35.31ms
step:55/2160 train_time:1940ms step_avg:35.28ms
step:56/2160 train_time:1973ms step_avg:35.24ms
step:57/2160 train_time:2008ms step_avg:35.22ms
step:58/2160 train_time:2041ms step_avg:35.18ms
step:59/2160 train_time:2075ms step_avg:35.17ms
step:60/2160 train_time:2108ms step_avg:35.13ms
step:61/2160 train_time:2142ms step_avg:35.11ms
step:62/2160 train_time:2175ms step_avg:35.08ms
step:63/2160 train_time:2209ms step_avg:35.07ms
step:64/2160 train_time:2243ms step_avg:35.04ms
step:65/2160 train_time:2277ms step_avg:35.03ms
step:66/2160 train_time:2310ms step_avg:35.00ms
step:67/2160 train_time:2344ms step_avg:34.99ms
step:68/2160 train_time:2378ms step_avg:34.97ms
step:69/2160 train_time:2412ms step_avg:34.96ms
step:70/2160 train_time:2445ms step_avg:34.93ms
step:71/2160 train_time:2479ms step_avg:34.92ms
step:72/2160 train_time:2512ms step_avg:34.89ms
step:73/2160 train_time:2547ms step_avg:34.89ms
step:74/2160 train_time:2580ms step_avg:34.86ms
step:75/2160 train_time:2614ms step_avg:34.86ms
step:76/2160 train_time:2647ms step_avg:34.83ms
step:77/2160 train_time:2682ms step_avg:34.83ms
step:78/2160 train_time:2715ms step_avg:34.81ms
step:79/2160 train_time:2749ms step_avg:34.80ms
step:80/2160 train_time:2782ms step_avg:34.78ms
step:81/2160 train_time:2817ms step_avg:34.77ms
step:82/2160 train_time:2849ms step_avg:34.75ms
step:83/2160 train_time:2884ms step_avg:34.75ms
step:84/2160 train_time:2917ms step_avg:34.73ms
step:85/2160 train_time:2951ms step_avg:34.72ms
step:86/2160 train_time:2984ms step_avg:34.70ms
step:87/2160 train_time:3019ms step_avg:34.70ms
step:88/2160 train_time:3052ms step_avg:34.68ms
step:89/2160 train_time:3086ms step_avg:34.67ms
step:90/2160 train_time:3119ms step_avg:34.66ms
step:91/2160 train_time:3153ms step_avg:34.65ms
step:92/2160 train_time:3186ms step_avg:34.63ms
step:93/2160 train_time:3221ms step_avg:34.63ms
step:94/2160 train_time:3254ms step_avg:34.61ms
step:95/2160 train_time:3288ms step_avg:34.61ms
step:96/2160 train_time:3321ms step_avg:34.59ms
step:97/2160 train_time:3356ms step_avg:34.60ms
step:98/2160 train_time:3389ms step_avg:34.58ms
step:99/2160 train_time:3423ms step_avg:34.58ms
step:100/2160 train_time:3456ms step_avg:34.56ms
step:101/2160 train_time:3490ms step_avg:34.56ms
step:102/2160 train_time:3523ms step_avg:34.54ms
step:103/2160 train_time:3558ms step_avg:34.54ms
step:104/2160 train_time:3591ms step_avg:34.53ms
step:105/2160 train_time:3626ms step_avg:34.53ms
step:106/2160 train_time:3659ms step_avg:34.52ms
step:107/2160 train_time:3693ms step_avg:34.52ms
step:108/2160 train_time:3727ms step_avg:34.51ms
step:109/2160 train_time:3761ms step_avg:34.50ms
step:110/2160 train_time:3793ms step_avg:34.49ms
step:111/2160 train_time:3827ms step_avg:34.48ms
step:112/2160 train_time:3862ms step_avg:34.48ms
step:113/2160 train_time:3895ms step_avg:34.47ms
step:114/2160 train_time:3928ms step_avg:34.45ms
step:115/2160 train_time:3962ms step_avg:34.45ms
step:116/2160 train_time:3995ms step_avg:34.44ms
step:117/2160 train_time:4029ms step_avg:34.44ms
step:118/2160 train_time:4062ms step_avg:34.43ms
step:119/2160 train_time:4096ms step_avg:34.42ms
step:120/2160 train_time:4129ms step_avg:34.41ms
step:121/2160 train_time:4164ms step_avg:34.41ms
step:122/2160 train_time:4197ms step_avg:34.40ms
step:123/2160 train_time:4231ms step_avg:34.40ms
step:124/2160 train_time:4264ms step_avg:34.39ms
step:125/2160 train_time:4298ms step_avg:34.38ms
step:126/2160 train_time:4331ms step_avg:34.37ms
step:127/2160 train_time:4365ms step_avg:34.37ms
step:128/2160 train_time:4398ms step_avg:34.36ms
step:129/2160 train_time:4432ms step_avg:34.36ms
step:130/2160 train_time:4465ms step_avg:34.35ms
step:131/2160 train_time:4499ms step_avg:34.35ms
step:132/2160 train_time:4532ms step_avg:34.34ms
step:133/2160 train_time:4567ms step_avg:34.34ms
step:134/2160 train_time:4600ms step_avg:34.33ms
step:135/2160 train_time:4634ms step_avg:34.33ms
step:136/2160 train_time:4667ms step_avg:34.32ms
step:137/2160 train_time:4701ms step_avg:34.32ms
step:138/2160 train_time:4734ms step_avg:34.31ms
step:139/2160 train_time:4769ms step_avg:34.31ms
step:140/2160 train_time:4802ms step_avg:34.30ms
step:141/2160 train_time:4836ms step_avg:34.30ms
step:142/2160 train_time:4869ms step_avg:34.29ms
step:143/2160 train_time:4903ms step_avg:34.29ms
step:144/2160 train_time:4936ms step_avg:34.28ms
step:145/2160 train_time:4970ms step_avg:34.28ms
step:146/2160 train_time:5003ms step_avg:34.27ms
step:147/2160 train_time:5037ms step_avg:34.27ms
step:148/2160 train_time:5071ms step_avg:34.26ms
step:149/2160 train_time:5105ms step_avg:34.26ms
step:150/2160 train_time:5139ms step_avg:34.26ms
step:151/2160 train_time:5173ms step_avg:34.26ms
step:152/2160 train_time:5206ms step_avg:34.25ms
step:153/2160 train_time:5240ms step_avg:34.25ms
step:154/2160 train_time:5273ms step_avg:34.24ms
step:155/2160 train_time:5307ms step_avg:34.24ms
step:156/2160 train_time:5340ms step_avg:34.23ms
step:157/2160 train_time:5374ms step_avg:34.23ms
step:158/2160 train_time:5407ms step_avg:34.22ms
step:159/2160 train_time:5441ms step_avg:34.22ms
step:160/2160 train_time:5474ms step_avg:34.21ms
step:161/2160 train_time:5508ms step_avg:34.21ms
step:162/2160 train_time:5541ms step_avg:34.20ms
step:163/2160 train_time:5575ms step_avg:34.20ms
step:164/2160 train_time:5608ms step_avg:34.20ms
step:165/2160 train_time:5642ms step_avg:34.19ms
step:166/2160 train_time:5675ms step_avg:34.19ms
step:167/2160 train_time:5709ms step_avg:34.19ms
step:168/2160 train_time:5742ms step_avg:34.18ms
step:169/2160 train_time:5776ms step_avg:34.18ms
step:170/2160 train_time:5809ms step_avg:34.17ms
step:171/2160 train_time:5844ms step_avg:34.17ms
step:172/2160 train_time:5877ms step_avg:34.17ms
step:173/2160 train_time:5911ms step_avg:34.17ms
step:174/2160 train_time:5943ms step_avg:34.16ms
step:175/2160 train_time:5978ms step_avg:34.16ms
step:176/2160 train_time:6011ms step_avg:34.15ms
step:177/2160 train_time:6046ms step_avg:34.16ms
step:178/2160 train_time:6079ms step_avg:34.15ms
step:179/2160 train_time:6113ms step_avg:34.15ms
step:180/2160 train_time:6146ms step_avg:34.15ms
step:181/2160 train_time:6180ms step_avg:34.14ms
step:182/2160 train_time:6214ms step_avg:34.14ms
step:183/2160 train_time:6248ms step_avg:34.14ms
step:184/2160 train_time:6281ms step_avg:34.13ms
step:185/2160 train_time:6315ms step_avg:34.13ms
step:186/2160 train_time:6347ms step_avg:34.13ms
step:187/2160 train_time:6382ms step_avg:34.13ms
step:188/2160 train_time:6415ms step_avg:34.12ms
step:189/2160 train_time:6449ms step_avg:34.12ms
step:190/2160 train_time:6481ms step_avg:34.11ms
step:191/2160 train_time:6516ms step_avg:34.11ms
step:192/2160 train_time:6549ms step_avg:34.11ms
step:193/2160 train_time:6583ms step_avg:34.11ms
step:194/2160 train_time:6617ms step_avg:34.11ms
step:195/2160 train_time:6651ms step_avg:34.11ms
step:196/2160 train_time:6684ms step_avg:34.10ms
step:197/2160 train_time:6717ms step_avg:34.10ms
step:198/2160 train_time:6750ms step_avg:34.09ms
step:199/2160 train_time:6784ms step_avg:34.09ms
step:200/2160 train_time:6818ms step_avg:34.09ms
step:201/2160 train_time:6852ms step_avg:34.09ms
step:202/2160 train_time:6884ms step_avg:34.08ms
step:203/2160 train_time:6919ms step_avg:34.08ms
step:204/2160 train_time:6951ms step_avg:34.07ms
step:205/2160 train_time:6986ms step_avg:34.08ms
step:206/2160 train_time:7019ms step_avg:34.07ms
step:207/2160 train_time:7054ms step_avg:34.08ms
step:208/2160 train_time:7087ms step_avg:34.07ms
step:209/2160 train_time:7121ms step_avg:34.07ms
step:210/2160 train_time:7154ms step_avg:34.07ms
step:211/2160 train_time:7188ms step_avg:34.07ms
step:212/2160 train_time:7221ms step_avg:34.06ms
step:213/2160 train_time:7256ms step_avg:34.06ms
step:214/2160 train_time:7289ms step_avg:34.06ms
step:215/2160 train_time:7323ms step_avg:34.06ms
step:216/2160 train_time:7356ms step_avg:34.05ms
step:217/2160 train_time:7390ms step_avg:34.06ms
step:218/2160 train_time:7423ms step_avg:34.05ms
step:219/2160 train_time:7457ms step_avg:34.05ms
step:220/2160 train_time:7490ms step_avg:34.05ms
step:221/2160 train_time:7524ms step_avg:34.05ms
step:222/2160 train_time:7557ms step_avg:34.04ms
step:223/2160 train_time:7591ms step_avg:34.04ms
step:224/2160 train_time:7624ms step_avg:34.04ms
step:225/2160 train_time:7658ms step_avg:34.04ms
step:226/2160 train_time:7691ms step_avg:34.03ms
step:227/2160 train_time:7725ms step_avg:34.03ms
step:228/2160 train_time:7759ms step_avg:34.03ms
step:229/2160 train_time:7792ms step_avg:34.03ms
step:230/2160 train_time:7825ms step_avg:34.02ms
step:231/2160 train_time:7859ms step_avg:34.02ms
step:232/2160 train_time:7892ms step_avg:34.02ms
step:233/2160 train_time:7927ms step_avg:34.02ms
step:234/2160 train_time:7960ms step_avg:34.02ms
step:235/2160 train_time:7994ms step_avg:34.02ms
step:236/2160 train_time:8027ms step_avg:34.01ms
step:237/2160 train_time:8061ms step_avg:34.01ms
step:238/2160 train_time:8094ms step_avg:34.01ms
step:239/2160 train_time:8128ms step_avg:34.01ms
step:240/2160 train_time:8161ms step_avg:34.00ms
step:241/2160 train_time:8195ms step_avg:34.01ms
step:242/2160 train_time:8228ms step_avg:34.00ms
step:243/2160 train_time:8263ms step_avg:34.00ms
step:244/2160 train_time:8296ms step_avg:34.00ms
step:245/2160 train_time:8330ms step_avg:34.00ms
step:246/2160 train_time:8363ms step_avg:33.99ms
step:247/2160 train_time:8397ms step_avg:33.99ms
step:248/2160 train_time:8430ms step_avg:33.99ms
step:249/2160 train_time:8464ms step_avg:33.99ms
step:250/2160 train_time:8497ms step_avg:33.99ms
step:250/2160 val_loss:4.2968 train_time:8532ms step_avg:34.13ms
step:251/2160 train_time:8551ms step_avg:34.07ms
step:252/2160 train_time:8569ms step_avg:34.00ms
step:253/2160 train_time:8603ms step_avg:34.01ms
step:254/2160 train_time:8637ms step_avg:34.00ms
step:255/2160 train_time:8674ms step_avg:34.02ms
step:256/2160 train_time:8708ms step_avg:34.02ms
step:257/2160 train_time:8743ms step_avg:34.02ms
step:258/2160 train_time:8776ms step_avg:34.02ms
step:259/2160 train_time:8811ms step_avg:34.02ms
step:260/2160 train_time:8844ms step_avg:34.02ms
step:261/2160 train_time:8879ms step_avg:34.02ms
step:262/2160 train_time:8911ms step_avg:34.01ms
step:263/2160 train_time:8945ms step_avg:34.01ms
step:264/2160 train_time:8978ms step_avg:34.01ms
step:265/2160 train_time:9012ms step_avg:34.01ms
step:266/2160 train_time:9045ms step_avg:34.00ms
step:267/2160 train_time:9079ms step_avg:34.00ms
step:268/2160 train_time:9112ms step_avg:34.00ms
step:269/2160 train_time:9146ms step_avg:34.00ms
step:270/2160 train_time:9179ms step_avg:34.00ms
step:271/2160 train_time:9213ms step_avg:34.00ms
step:272/2160 train_time:9246ms step_avg:33.99ms
step:273/2160 train_time:9280ms step_avg:33.99ms
step:274/2160 train_time:9313ms step_avg:33.99ms
step:275/2160 train_time:9347ms step_avg:33.99ms
step:276/2160 train_time:9380ms step_avg:33.98ms
step:277/2160 train_time:9413ms step_avg:33.98ms
step:278/2160 train_time:9446ms step_avg:33.98ms
step:279/2160 train_time:9480ms step_avg:33.98ms
step:280/2160 train_time:9513ms step_avg:33.98ms
step:281/2160 train_time:9547ms step_avg:33.98ms
step:282/2160 train_time:9580ms step_avg:33.97ms
step:283/2160 train_time:9615ms step_avg:33.97ms
step:284/2160 train_time:9648ms step_avg:33.97ms
step:285/2160 train_time:9683ms step_avg:33.98ms
step:286/2160 train_time:9716ms step_avg:33.97ms
step:287/2160 train_time:9751ms step_avg:33.97ms
step:288/2160 train_time:9784ms step_avg:33.97ms
step:289/2160 train_time:9818ms step_avg:33.97ms
step:290/2160 train_time:9851ms step_avg:33.97ms
step:291/2160 train_time:9885ms step_avg:33.97ms
step:292/2160 train_time:9918ms step_avg:33.97ms
step:293/2160 train_time:9952ms step_avg:33.97ms
step:294/2160 train_time:9985ms step_avg:33.96ms
step:295/2160 train_time:10020ms step_avg:33.96ms
step:296/2160 train_time:10053ms step_avg:33.96ms
step:297/2160 train_time:10087ms step_avg:33.96ms
step:298/2160 train_time:10120ms step_avg:33.96ms
step:299/2160 train_time:10155ms step_avg:33.96ms
step:300/2160 train_time:10187ms step_avg:33.96ms
step:301/2160 train_time:10221ms step_avg:33.96ms
step:302/2160 train_time:10254ms step_avg:33.95ms
step:303/2160 train_time:10288ms step_avg:33.95ms
step:304/2160 train_time:10321ms step_avg:33.95ms
step:305/2160 train_time:10355ms step_avg:33.95ms
step:306/2160 train_time:10388ms step_avg:33.95ms
step:307/2160 train_time:10422ms step_avg:33.95ms
step:308/2160 train_time:10454ms step_avg:33.94ms
step:309/2160 train_time:10488ms step_avg:33.94ms
step:310/2160 train_time:10521ms step_avg:33.94ms
step:311/2160 train_time:10555ms step_avg:33.94ms
step:312/2160 train_time:10588ms step_avg:33.94ms
step:313/2160 train_time:10622ms step_avg:33.94ms
step:314/2160 train_time:10656ms step_avg:33.94ms
step:315/2160 train_time:10690ms step_avg:33.94ms
step:316/2160 train_time:10723ms step_avg:33.93ms
step:317/2160 train_time:10757ms step_avg:33.93ms
step:318/2160 train_time:10790ms step_avg:33.93ms
step:319/2160 train_time:10825ms step_avg:33.93ms
step:320/2160 train_time:10858ms step_avg:33.93ms
step:321/2160 train_time:10892ms step_avg:33.93ms
step:322/2160 train_time:10925ms step_avg:33.93ms
step:323/2160 train_time:10959ms step_avg:33.93ms
step:324/2160 train_time:10992ms step_avg:33.93ms
step:325/2160 train_time:11027ms step_avg:33.93ms
step:326/2160 train_time:11059ms step_avg:33.92ms
step:327/2160 train_time:11094ms step_avg:33.93ms
step:328/2160 train_time:11127ms step_avg:33.92ms
step:329/2160 train_time:11161ms step_avg:33.92ms
step:330/2160 train_time:11194ms step_avg:33.92ms
step:331/2160 train_time:11228ms step_avg:33.92ms
step:332/2160 train_time:11261ms step_avg:33.92ms
step:333/2160 train_time:11295ms step_avg:33.92ms
step:334/2160 train_time:11328ms step_avg:33.91ms
step:335/2160 train_time:11362ms step_avg:33.92ms
step:336/2160 train_time:11395ms step_avg:33.91ms
step:337/2160 train_time:11429ms step_avg:33.91ms
step:338/2160 train_time:11462ms step_avg:33.91ms
step:339/2160 train_time:11496ms step_avg:33.91ms
step:340/2160 train_time:11529ms step_avg:33.91ms
step:341/2160 train_time:11563ms step_avg:33.91ms
step:342/2160 train_time:11596ms step_avg:33.91ms
step:343/2160 train_time:11630ms step_avg:33.91ms
step:344/2160 train_time:11663ms step_avg:33.90ms
step:345/2160 train_time:11697ms step_avg:33.90ms
step:346/2160 train_time:11730ms step_avg:33.90ms
step:347/2160 train_time:11764ms step_avg:33.90ms
step:348/2160 train_time:11797ms step_avg:33.90ms
step:349/2160 train_time:11831ms step_avg:33.90ms
step:350/2160 train_time:11864ms step_avg:33.90ms
step:351/2160 train_time:11898ms step_avg:33.90ms
step:352/2160 train_time:11931ms step_avg:33.90ms
step:353/2160 train_time:11966ms step_avg:33.90ms
step:354/2160 train_time:11999ms step_avg:33.90ms
step:355/2160 train_time:12033ms step_avg:33.90ms
step:356/2160 train_time:12066ms step_avg:33.89ms
step:357/2160 train_time:12100ms step_avg:33.89ms
step:358/2160 train_time:12133ms step_avg:33.89ms
step:359/2160 train_time:12167ms step_avg:33.89ms
step:360/2160 train_time:12200ms step_avg:33.89ms
step:361/2160 train_time:12235ms step_avg:33.89ms
step:362/2160 train_time:12268ms step_avg:33.89ms
step:363/2160 train_time:12302ms step_avg:33.89ms
step:364/2160 train_time:12334ms step_avg:33.89ms
step:365/2160 train_time:12368ms step_avg:33.89ms
step:366/2160 train_time:12402ms step_avg:33.88ms
step:367/2160 train_time:12436ms step_avg:33.89ms
step:368/2160 train_time:12469ms step_avg:33.88ms
step:369/2160 train_time:12503ms step_avg:33.88ms
step:370/2160 train_time:12536ms step_avg:33.88ms
step:371/2160 train_time:12570ms step_avg:33.88ms
step:372/2160 train_time:12603ms step_avg:33.88ms
step:373/2160 train_time:12637ms step_avg:33.88ms
step:374/2160 train_time:12670ms step_avg:33.88ms
step:375/2160 train_time:12704ms step_avg:33.88ms
step:376/2160 train_time:12737ms step_avg:33.88ms
step:377/2160 train_time:12771ms step_avg:33.88ms
step:378/2160 train_time:12805ms step_avg:33.87ms
step:379/2160 train_time:12839ms step_avg:33.88ms
step:380/2160 train_time:12872ms step_avg:33.87ms
step:381/2160 train_time:12906ms step_avg:33.87ms
step:382/2160 train_time:12939ms step_avg:33.87ms
step:383/2160 train_time:12973ms step_avg:33.87ms
step:384/2160 train_time:13006ms step_avg:33.87ms
step:385/2160 train_time:13040ms step_avg:33.87ms
step:386/2160 train_time:13073ms step_avg:33.87ms
step:387/2160 train_time:13108ms step_avg:33.87ms
step:388/2160 train_time:13141ms step_avg:33.87ms
step:389/2160 train_time:13175ms step_avg:33.87ms
step:390/2160 train_time:13208ms step_avg:33.87ms
step:391/2160 train_time:13242ms step_avg:33.87ms
step:392/2160 train_time:13275ms step_avg:33.87ms
step:393/2160 train_time:13309ms step_avg:33.87ms
step:394/2160 train_time:13342ms step_avg:33.86ms
step:395/2160 train_time:13376ms step_avg:33.86ms
step:396/2160 train_time:13409ms step_avg:33.86ms
step:397/2160 train_time:13443ms step_avg:33.86ms
step:398/2160 train_time:13477ms step_avg:33.86ms
step:399/2160 train_time:13511ms step_avg:33.86ms
step:400/2160 train_time:13544ms step_avg:33.86ms
step:401/2160 train_time:13578ms step_avg:33.86ms
step:402/2160 train_time:13611ms step_avg:33.86ms
step:403/2160 train_time:13645ms step_avg:33.86ms
step:404/2160 train_time:13678ms step_avg:33.86ms
step:405/2160 train_time:13712ms step_avg:33.86ms
step:406/2160 train_time:13745ms step_avg:33.85ms
step:407/2160 train_time:13779ms step_avg:33.86ms
step:408/2160 train_time:13812ms step_avg:33.85ms
step:409/2160 train_time:13846ms step_avg:33.85ms
step:410/2160 train_time:13880ms step_avg:33.85ms
step:411/2160 train_time:13914ms step_avg:33.85ms
step:412/2160 train_time:13947ms step_avg:33.85ms
step:413/2160 train_time:13981ms step_avg:33.85ms
step:414/2160 train_time:14014ms step_avg:33.85ms
step:415/2160 train_time:14048ms step_avg:33.85ms
step:416/2160 train_time:14081ms step_avg:33.85ms
step:417/2160 train_time:14116ms step_avg:33.85ms
step:418/2160 train_time:14148ms step_avg:33.85ms
step:419/2160 train_time:14183ms step_avg:33.85ms
step:420/2160 train_time:14216ms step_avg:33.85ms
step:421/2160 train_time:14250ms step_avg:33.85ms
step:422/2160 train_time:14283ms step_avg:33.85ms
step:423/2160 train_time:14317ms step_avg:33.85ms
step:424/2160 train_time:14350ms step_avg:33.84ms
step:425/2160 train_time:14384ms step_avg:33.84ms
step:426/2160 train_time:14417ms step_avg:33.84ms
step:427/2160 train_time:14451ms step_avg:33.84ms
step:428/2160 train_time:14484ms step_avg:33.84ms
step:429/2160 train_time:14518ms step_avg:33.84ms
step:430/2160 train_time:14551ms step_avg:33.84ms
step:431/2160 train_time:14585ms step_avg:33.84ms
step:432/2160 train_time:14618ms step_avg:33.84ms
step:433/2160 train_time:14653ms step_avg:33.84ms
step:434/2160 train_time:14686ms step_avg:33.84ms
step:435/2160 train_time:14720ms step_avg:33.84ms
step:436/2160 train_time:14753ms step_avg:33.84ms
step:437/2160 train_time:14788ms step_avg:33.84ms
step:438/2160 train_time:14821ms step_avg:33.84ms
step:439/2160 train_time:14855ms step_avg:33.84ms
step:440/2160 train_time:14887ms step_avg:33.84ms
step:441/2160 train_time:14922ms step_avg:33.84ms
step:442/2160 train_time:14955ms step_avg:33.83ms
step:443/2160 train_time:14989ms step_avg:33.84ms
step:444/2160 train_time:15022ms step_avg:33.83ms
step:445/2160 train_time:15057ms step_avg:33.84ms
step:446/2160 train_time:15090ms step_avg:33.84ms
step:447/2160 train_time:15124ms step_avg:33.83ms
step:448/2160 train_time:15158ms step_avg:33.83ms
step:449/2160 train_time:15192ms step_avg:33.83ms
step:450/2160 train_time:15225ms step_avg:33.83ms
step:451/2160 train_time:15259ms step_avg:33.83ms
step:452/2160 train_time:15292ms step_avg:33.83ms
step:453/2160 train_time:15326ms step_avg:33.83ms
step:454/2160 train_time:15359ms step_avg:33.83ms
step:455/2160 train_time:15393ms step_avg:33.83ms
step:456/2160 train_time:15426ms step_avg:33.83ms
step:457/2160 train_time:15460ms step_avg:33.83ms
step:458/2160 train_time:15493ms step_avg:33.83ms
step:459/2160 train_time:15527ms step_avg:33.83ms
step:460/2160 train_time:15560ms step_avg:33.83ms
step:461/2160 train_time:15594ms step_avg:33.83ms
step:462/2160 train_time:15627ms step_avg:33.82ms
step:463/2160 train_time:15661ms step_avg:33.83ms
step:464/2160 train_time:15694ms step_avg:33.82ms
step:465/2160 train_time:15728ms step_avg:33.82ms
step:466/2160 train_time:15761ms step_avg:33.82ms
step:467/2160 train_time:15796ms step_avg:33.82ms
step:468/2160 train_time:15828ms step_avg:33.82ms
step:469/2160 train_time:15863ms step_avg:33.82ms
step:470/2160 train_time:15896ms step_avg:33.82ms
step:471/2160 train_time:15930ms step_avg:33.82ms
step:472/2160 train_time:15963ms step_avg:33.82ms
step:473/2160 train_time:15997ms step_avg:33.82ms
step:474/2160 train_time:16030ms step_avg:33.82ms
step:475/2160 train_time:16065ms step_avg:33.82ms
step:476/2160 train_time:16098ms step_avg:33.82ms
step:477/2160 train_time:16132ms step_avg:33.82ms
step:478/2160 train_time:16165ms step_avg:33.82ms
step:479/2160 train_time:16199ms step_avg:33.82ms
step:480/2160 train_time:16233ms step_avg:33.82ms
step:481/2160 train_time:16267ms step_avg:33.82ms
step:482/2160 train_time:16300ms step_avg:33.82ms
step:483/2160 train_time:16334ms step_avg:33.82ms
step:484/2160 train_time:16367ms step_avg:33.82ms
step:485/2160 train_time:16401ms step_avg:33.82ms
step:486/2160 train_time:16435ms step_avg:33.82ms
step:487/2160 train_time:16469ms step_avg:33.82ms
step:488/2160 train_time:16502ms step_avg:33.82ms
step:489/2160 train_time:16536ms step_avg:33.82ms
step:490/2160 train_time:16569ms step_avg:33.81ms
step:491/2160 train_time:16604ms step_avg:33.82ms
step:492/2160 train_time:16637ms step_avg:33.81ms
step:493/2160 train_time:16671ms step_avg:33.81ms
step:494/2160 train_time:16704ms step_avg:33.81ms
step:495/2160 train_time:16738ms step_avg:33.81ms
step:496/2160 train_time:16771ms step_avg:33.81ms
step:497/2160 train_time:16805ms step_avg:33.81ms
step:498/2160 train_time:16839ms step_avg:33.81ms
step:499/2160 train_time:16873ms step_avg:33.81ms
step:500/2160 train_time:16906ms step_avg:33.81ms
step:500/2160 val_loss:4.0054 train_time:16941ms step_avg:33.88ms
step:501/2160 train_time:16960ms step_avg:33.85ms
step:502/2160 train_time:16978ms step_avg:33.82ms
step:503/2160 train_time:17012ms step_avg:33.82ms
step:504/2160 train_time:17046ms step_avg:33.82ms
step:505/2160 train_time:17080ms step_avg:33.82ms
step:506/2160 train_time:17114ms step_avg:33.82ms
step:507/2160 train_time:17149ms step_avg:33.82ms
step:508/2160 train_time:17182ms step_avg:33.82ms
step:509/2160 train_time:17216ms step_avg:33.82ms
step:510/2160 train_time:17249ms step_avg:33.82ms
step:511/2160 train_time:17283ms step_avg:33.82ms
step:512/2160 train_time:17316ms step_avg:33.82ms
step:513/2160 train_time:17350ms step_avg:33.82ms
step:514/2160 train_time:17383ms step_avg:33.82ms
step:515/2160 train_time:17416ms step_avg:33.82ms
step:516/2160 train_time:17449ms step_avg:33.82ms
step:517/2160 train_time:17483ms step_avg:33.82ms
step:518/2160 train_time:17516ms step_avg:33.81ms
step:519/2160 train_time:17550ms step_avg:33.82ms
step:520/2160 train_time:17583ms step_avg:33.81ms
step:521/2160 train_time:17617ms step_avg:33.81ms
step:522/2160 train_time:17650ms step_avg:33.81ms
step:523/2160 train_time:17684ms step_avg:33.81ms
step:524/2160 train_time:17716ms step_avg:33.81ms
step:525/2160 train_time:17751ms step_avg:33.81ms
step:526/2160 train_time:17784ms step_avg:33.81ms
step:527/2160 train_time:17818ms step_avg:33.81ms
step:528/2160 train_time:17851ms step_avg:33.81ms
step:529/2160 train_time:17885ms step_avg:33.81ms
step:530/2160 train_time:17918ms step_avg:33.81ms
step:531/2160 train_time:17953ms step_avg:33.81ms
step:532/2160 train_time:17986ms step_avg:33.81ms
step:533/2160 train_time:18020ms step_avg:33.81ms
step:534/2160 train_time:18053ms step_avg:33.81ms
step:535/2160 train_time:18088ms step_avg:33.81ms
step:536/2160 train_time:18122ms step_avg:33.81ms
step:537/2160 train_time:18156ms step_avg:33.81ms
step:538/2160 train_time:18189ms step_avg:33.81ms
step:539/2160 train_time:18224ms step_avg:33.81ms
step:540/2160 train_time:18257ms step_avg:33.81ms
step:541/2160 train_time:18291ms step_avg:33.81ms
step:542/2160 train_time:18324ms step_avg:33.81ms
step:543/2160 train_time:18358ms step_avg:33.81ms
step:544/2160 train_time:18391ms step_avg:33.81ms
step:545/2160 train_time:18425ms step_avg:33.81ms
step:546/2160 train_time:18458ms step_avg:33.81ms
step:547/2160 train_time:18492ms step_avg:33.81ms
step:548/2160 train_time:18525ms step_avg:33.81ms
step:549/2160 train_time:18559ms step_avg:33.81ms
step:550/2160 train_time:18592ms step_avg:33.80ms
step:551/2160 train_time:18626ms step_avg:33.80ms
step:552/2160 train_time:18659ms step_avg:33.80ms
step:553/2160 train_time:18693ms step_avg:33.80ms
step:554/2160 train_time:18726ms step_avg:33.80ms
step:555/2160 train_time:18760ms step_avg:33.80ms
step:556/2160 train_time:18793ms step_avg:33.80ms
step:557/2160 train_time:18827ms step_avg:33.80ms
step:558/2160 train_time:18860ms step_avg:33.80ms
step:559/2160 train_time:18895ms step_avg:33.80ms
step:560/2160 train_time:18927ms step_avg:33.80ms
step:561/2160 train_time:18961ms step_avg:33.80ms
step:562/2160 train_time:18995ms step_avg:33.80ms
step:563/2160 train_time:19029ms step_avg:33.80ms
step:564/2160 train_time:19063ms step_avg:33.80ms
step:565/2160 train_time:19097ms step_avg:33.80ms
step:566/2160 train_time:19130ms step_avg:33.80ms
step:567/2160 train_time:19164ms step_avg:33.80ms
step:568/2160 train_time:19197ms step_avg:33.80ms
step:569/2160 train_time:19232ms step_avg:33.80ms
step:570/2160 train_time:19265ms step_avg:33.80ms
step:571/2160 train_time:19299ms step_avg:33.80ms
step:572/2160 train_time:19332ms step_avg:33.80ms
step:573/2160 train_time:19367ms step_avg:33.80ms
step:574/2160 train_time:19400ms step_avg:33.80ms
step:575/2160 train_time:19435ms step_avg:33.80ms
step:576/2160 train_time:19468ms step_avg:33.80ms
step:577/2160 train_time:19502ms step_avg:33.80ms
step:578/2160 train_time:19535ms step_avg:33.80ms
step:579/2160 train_time:19569ms step_avg:33.80ms
step:580/2160 train_time:19602ms step_avg:33.80ms
step:581/2160 train_time:19636ms step_avg:33.80ms
step:582/2160 train_time:19669ms step_avg:33.80ms
step:583/2160 train_time:19703ms step_avg:33.80ms
step:584/2160 train_time:19736ms step_avg:33.79ms
step:585/2160 train_time:19770ms step_avg:33.80ms
step:586/2160 train_time:19803ms step_avg:33.79ms
step:587/2160 train_time:19838ms step_avg:33.79ms
step:588/2160 train_time:19870ms step_avg:33.79ms
step:589/2160 train_time:19904ms step_avg:33.79ms
step:590/2160 train_time:19938ms step_avg:33.79ms
step:591/2160 train_time:19972ms step_avg:33.79ms
step:592/2160 train_time:20005ms step_avg:33.79ms
step:593/2160 train_time:20039ms step_avg:33.79ms
step:594/2160 train_time:20073ms step_avg:33.79ms
step:595/2160 train_time:20107ms step_avg:33.79ms
step:596/2160 train_time:20140ms step_avg:33.79ms
step:597/2160 train_time:20174ms step_avg:33.79ms
step:598/2160 train_time:20207ms step_avg:33.79ms
step:599/2160 train_time:20241ms step_avg:33.79ms
step:600/2160 train_time:20274ms step_avg:33.79ms
step:601/2160 train_time:20309ms step_avg:33.79ms
step:602/2160 train_time:20342ms step_avg:33.79ms
step:603/2160 train_time:20376ms step_avg:33.79ms
step:604/2160 train_time:20409ms step_avg:33.79ms
step:605/2160 train_time:20444ms step_avg:33.79ms
step:606/2160 train_time:20477ms step_avg:33.79ms
step:607/2160 train_time:20511ms step_avg:33.79ms
step:608/2160 train_time:20544ms step_avg:33.79ms
step:609/2160 train_time:20579ms step_avg:33.79ms
step:610/2160 train_time:20612ms step_avg:33.79ms
step:611/2160 train_time:20647ms step_avg:33.79ms
step:612/2160 train_time:20680ms step_avg:33.79ms
step:613/2160 train_time:20714ms step_avg:33.79ms
step:614/2160 train_time:20747ms step_avg:33.79ms
step:615/2160 train_time:20782ms step_avg:33.79ms
step:616/2160 train_time:20814ms step_avg:33.79ms
step:617/2160 train_time:20849ms step_avg:33.79ms
step:618/2160 train_time:20882ms step_avg:33.79ms
step:619/2160 train_time:20916ms step_avg:33.79ms
step:620/2160 train_time:20949ms step_avg:33.79ms
step:621/2160 train_time:20983ms step_avg:33.79ms
step:622/2160 train_time:21016ms step_avg:33.79ms
step:623/2160 train_time:21050ms step_avg:33.79ms
step:624/2160 train_time:21083ms step_avg:33.79ms
step:625/2160 train_time:21117ms step_avg:33.79ms
step:626/2160 train_time:21150ms step_avg:33.79ms
step:627/2160 train_time:21184ms step_avg:33.79ms
step:628/2160 train_time:21218ms step_avg:33.79ms
step:629/2160 train_time:21252ms step_avg:33.79ms
step:630/2160 train_time:21285ms step_avg:33.79ms
step:631/2160 train_time:21319ms step_avg:33.79ms
step:632/2160 train_time:21352ms step_avg:33.78ms
step:633/2160 train_time:21386ms step_avg:33.79ms
step:634/2160 train_time:21420ms step_avg:33.78ms
step:635/2160 train_time:21454ms step_avg:33.79ms
step:636/2160 train_time:21487ms step_avg:33.78ms
step:637/2160 train_time:21521ms step_avg:33.78ms
step:638/2160 train_time:21554ms step_avg:33.78ms
step:639/2160 train_time:21588ms step_avg:33.78ms
step:640/2160 train_time:21621ms step_avg:33.78ms
step:641/2160 train_time:21656ms step_avg:33.78ms
step:642/2160 train_time:21689ms step_avg:33.78ms
step:643/2160 train_time:21723ms step_avg:33.78ms
step:644/2160 train_time:21756ms step_avg:33.78ms
step:645/2160 train_time:21790ms step_avg:33.78ms
step:646/2160 train_time:21823ms step_avg:33.78ms
step:647/2160 train_time:21857ms step_avg:33.78ms
step:648/2160 train_time:21890ms step_avg:33.78ms
step:649/2160 train_time:21924ms step_avg:33.78ms
step:650/2160 train_time:21957ms step_avg:33.78ms
step:651/2160 train_time:21992ms step_avg:33.78ms
step:652/2160 train_time:22025ms step_avg:33.78ms
step:653/2160 train_time:22059ms step_avg:33.78ms
step:654/2160 train_time:22093ms step_avg:33.78ms
step:655/2160 train_time:22127ms step_avg:33.78ms
step:656/2160 train_time:22160ms step_avg:33.78ms
step:657/2160 train_time:22194ms step_avg:33.78ms
step:658/2160 train_time:22227ms step_avg:33.78ms
step:659/2160 train_time:22261ms step_avg:33.78ms
step:660/2160 train_time:22294ms step_avg:33.78ms
step:661/2160 train_time:22329ms step_avg:33.78ms
step:662/2160 train_time:22362ms step_avg:33.78ms
step:663/2160 train_time:22396ms step_avg:33.78ms
step:664/2160 train_time:22429ms step_avg:33.78ms
step:665/2160 train_time:22463ms step_avg:33.78ms
step:666/2160 train_time:22496ms step_avg:33.78ms
step:667/2160 train_time:22530ms step_avg:33.78ms
step:668/2160 train_time:22563ms step_avg:33.78ms
step:669/2160 train_time:22598ms step_avg:33.78ms
step:670/2160 train_time:22631ms step_avg:33.78ms
step:671/2160 train_time:22665ms step_avg:33.78ms
step:672/2160 train_time:22698ms step_avg:33.78ms
step:673/2160 train_time:22732ms step_avg:33.78ms
step:674/2160 train_time:22765ms step_avg:33.78ms
step:675/2160 train_time:22800ms step_avg:33.78ms
step:676/2160 train_time:22833ms step_avg:33.78ms
step:677/2160 train_time:22867ms step_avg:33.78ms
step:678/2160 train_time:22900ms step_avg:33.78ms
step:679/2160 train_time:22934ms step_avg:33.78ms
step:680/2160 train_time:22967ms step_avg:33.78ms
step:681/2160 train_time:23001ms step_avg:33.78ms
step:682/2160 train_time:23035ms step_avg:33.77ms
step:683/2160 train_time:23069ms step_avg:33.78ms
step:684/2160 train_time:23102ms step_avg:33.77ms
step:685/2160 train_time:23136ms step_avg:33.77ms
step:686/2160 train_time:23169ms step_avg:33.77ms
step:687/2160 train_time:23203ms step_avg:33.77ms
step:688/2160 train_time:23236ms step_avg:33.77ms
step:689/2160 train_time:23270ms step_avg:33.77ms
step:690/2160 train_time:23303ms step_avg:33.77ms
step:691/2160 train_time:23338ms step_avg:33.77ms
step:692/2160 train_time:23371ms step_avg:33.77ms
step:693/2160 train_time:23406ms step_avg:33.77ms
step:694/2160 train_time:23440ms step_avg:33.77ms
step:695/2160 train_time:23474ms step_avg:33.78ms
step:696/2160 train_time:23507ms step_avg:33.77ms
step:697/2160 train_time:23541ms step_avg:33.78ms
step:698/2160 train_time:23575ms step_avg:33.77ms
step:699/2160 train_time:23609ms step_avg:33.77ms
step:700/2160 train_time:23642ms step_avg:33.77ms
step:701/2160 train_time:23676ms step_avg:33.77ms
step:702/2160 train_time:23709ms step_avg:33.77ms
step:703/2160 train_time:23743ms step_avg:33.77ms
step:704/2160 train_time:23776ms step_avg:33.77ms
step:705/2160 train_time:23811ms step_avg:33.77ms
step:706/2160 train_time:23844ms step_avg:33.77ms
step:707/2160 train_time:23878ms step_avg:33.77ms
step:708/2160 train_time:23912ms step_avg:33.77ms
step:709/2160 train_time:23972ms step_avg:33.81ms
step:710/2160 train_time:24032ms step_avg:33.85ms
step:711/2160 train_time:24093ms step_avg:33.89ms
step:712/2160 train_time:24153ms step_avg:33.92ms
step:713/2160 train_time:24215ms step_avg:33.96ms
step:714/2160 train_time:24274ms step_avg:34.00ms
step:715/2160 train_time:24337ms step_avg:34.04ms
step:716/2160 train_time:24396ms step_avg:34.07ms
step:717/2160 train_time:24458ms step_avg:34.11ms
step:718/2160 train_time:24518ms step_avg:34.15ms
step:719/2160 train_time:24580ms step_avg:34.19ms
step:720/2160 train_time:24640ms step_avg:34.22ms
step:721/2160 train_time:24701ms step_avg:34.26ms
step:722/2160 train_time:24761ms step_avg:34.29ms
step:723/2160 train_time:24822ms step_avg:34.33ms
step:724/2160 train_time:24882ms step_avg:34.37ms
step:725/2160 train_time:24943ms step_avg:34.40ms
step:726/2160 train_time:25003ms step_avg:34.44ms
step:727/2160 train_time:25065ms step_avg:34.48ms
step:728/2160 train_time:25125ms step_avg:34.51ms
step:729/2160 train_time:25186ms step_avg:34.55ms
step:730/2160 train_time:25247ms step_avg:34.58ms
step:731/2160 train_time:25309ms step_avg:34.62ms
step:732/2160 train_time:25369ms step_avg:34.66ms
step:733/2160 train_time:25430ms step_avg:34.69ms
step:734/2160 train_time:25490ms step_avg:34.73ms
step:735/2160 train_time:25552ms step_avg:34.76ms
step:736/2160 train_time:25611ms step_avg:34.80ms
step:737/2160 train_time:25672ms step_avg:34.83ms
step:738/2160 train_time:25731ms step_avg:34.87ms
step:739/2160 train_time:25793ms step_avg:34.90ms
step:740/2160 train_time:25852ms step_avg:34.94ms
step:741/2160 train_time:25914ms step_avg:34.97ms
step:742/2160 train_time:25974ms step_avg:35.00ms
step:743/2160 train_time:26035ms step_avg:35.04ms
step:744/2160 train_time:26096ms step_avg:35.07ms
step:745/2160 train_time:26158ms step_avg:35.11ms
step:746/2160 train_time:26218ms step_avg:35.14ms
step:747/2160 train_time:26280ms step_avg:35.18ms
step:748/2160 train_time:26340ms step_avg:35.21ms
step:749/2160 train_time:26401ms step_avg:35.25ms
step:750/2160 train_time:26461ms step_avg:35.28ms
step:750/2160 val_loss:3.8613 train_time:26523ms step_avg:35.36ms
step:751/2160 train_time:26543ms step_avg:35.34ms
step:752/2160 train_time:26585ms step_avg:35.35ms
step:753/2160 train_time:26647ms step_avg:35.39ms
step:754/2160 train_time:26707ms step_avg:35.42ms
step:755/2160 train_time:26768ms step_avg:35.45ms
step:756/2160 train_time:26827ms step_avg:35.49ms
step:757/2160 train_time:26888ms step_avg:35.52ms
step:758/2160 train_time:26946ms step_avg:35.55ms
step:759/2160 train_time:27007ms step_avg:35.58ms
step:760/2160 train_time:27065ms step_avg:35.61ms
step:761/2160 train_time:27126ms step_avg:35.65ms
step:762/2160 train_time:27185ms step_avg:35.68ms
step:763/2160 train_time:27246ms step_avg:35.71ms
step:764/2160 train_time:27305ms step_avg:35.74ms
step:765/2160 train_time:27366ms step_avg:35.77ms
step:766/2160 train_time:27429ms step_avg:35.81ms
step:767/2160 train_time:27498ms step_avg:35.85ms
step:768/2160 train_time:27562ms step_avg:35.89ms
step:769/2160 train_time:27623ms step_avg:35.92ms
step:770/2160 train_time:27683ms step_avg:35.95ms
step:771/2160 train_time:27744ms step_avg:35.98ms
step:772/2160 train_time:27803ms step_avg:36.01ms
step:773/2160 train_time:27865ms step_avg:36.05ms
step:774/2160 train_time:27924ms step_avg:36.08ms
step:775/2160 train_time:27984ms step_avg:36.11ms
step:776/2160 train_time:28043ms step_avg:36.14ms
step:777/2160 train_time:28104ms step_avg:36.17ms
step:778/2160 train_time:28163ms step_avg:36.20ms
step:779/2160 train_time:28224ms step_avg:36.23ms
step:780/2160 train_time:28283ms step_avg:36.26ms
step:781/2160 train_time:28344ms step_avg:36.29ms
step:782/2160 train_time:28405ms step_avg:36.32ms
step:783/2160 train_time:28469ms step_avg:36.36ms
step:784/2160 train_time:28531ms step_avg:36.39ms
step:785/2160 train_time:28594ms step_avg:36.43ms
step:786/2160 train_time:28654ms step_avg:36.46ms
step:787/2160 train_time:28717ms step_avg:36.49ms
step:788/2160 train_time:28776ms step_avg:36.52ms
step:789/2160 train_time:28837ms step_avg:36.55ms
step:790/2160 train_time:28895ms step_avg:36.58ms
step:791/2160 train_time:28956ms step_avg:36.61ms
step:792/2160 train_time:29015ms step_avg:36.64ms
step:793/2160 train_time:29077ms step_avg:36.67ms
step:794/2160 train_time:29136ms step_avg:36.69ms
step:795/2160 train_time:29196ms step_avg:36.72ms
step:796/2160 train_time:29255ms step_avg:36.75ms
step:797/2160 train_time:29317ms step_avg:36.78ms
step:798/2160 train_time:29377ms step_avg:36.81ms
step:799/2160 train_time:29439ms step_avg:36.84ms
step:800/2160 train_time:29500ms step_avg:36.87ms
step:801/2160 train_time:29562ms step_avg:36.91ms
step:802/2160 train_time:29622ms step_avg:36.94ms
step:803/2160 train_time:29684ms step_avg:36.97ms
step:804/2160 train_time:29744ms step_avg:36.99ms
step:805/2160 train_time:29805ms step_avg:37.02ms
step:806/2160 train_time:29864ms step_avg:37.05ms
step:807/2160 train_time:29925ms step_avg:37.08ms
step:808/2160 train_time:29984ms step_avg:37.11ms
step:809/2160 train_time:30045ms step_avg:37.14ms
step:810/2160 train_time:30104ms step_avg:37.17ms
step:811/2160 train_time:30165ms step_avg:37.19ms
step:812/2160 train_time:30224ms step_avg:37.22ms
step:813/2160 train_time:30285ms step_avg:37.25ms
step:814/2160 train_time:30345ms step_avg:37.28ms
step:815/2160 train_time:30406ms step_avg:37.31ms
step:816/2160 train_time:30466ms step_avg:37.34ms
step:817/2160 train_time:30529ms step_avg:37.37ms
step:818/2160 train_time:30590ms step_avg:37.40ms
step:819/2160 train_time:30652ms step_avg:37.43ms
step:820/2160 train_time:30712ms step_avg:37.45ms
step:821/2160 train_time:30773ms step_avg:37.48ms
step:822/2160 train_time:30833ms step_avg:37.51ms
step:823/2160 train_time:30894ms step_avg:37.54ms
step:824/2160 train_time:30954ms step_avg:37.57ms
step:825/2160 train_time:31015ms step_avg:37.59ms
step:826/2160 train_time:31074ms step_avg:37.62ms
step:827/2160 train_time:31136ms step_avg:37.65ms
step:828/2160 train_time:31194ms step_avg:37.67ms
step:829/2160 train_time:31256ms step_avg:37.70ms
step:830/2160 train_time:31315ms step_avg:37.73ms
step:831/2160 train_time:31376ms step_avg:37.76ms
step:832/2160 train_time:31435ms step_avg:37.78ms
step:833/2160 train_time:31497ms step_avg:37.81ms
step:834/2160 train_time:31557ms step_avg:37.84ms
step:835/2160 train_time:31619ms step_avg:37.87ms
step:836/2160 train_time:31679ms step_avg:37.89ms
step:837/2160 train_time:31740ms step_avg:37.92ms
step:838/2160 train_time:31800ms step_avg:37.95ms
step:839/2160 train_time:31862ms step_avg:37.98ms
step:840/2160 train_time:31922ms step_avg:38.00ms
step:841/2160 train_time:31983ms step_avg:38.03ms
step:842/2160 train_time:32043ms step_avg:38.06ms
step:843/2160 train_time:32104ms step_avg:38.08ms
step:844/2160 train_time:32164ms step_avg:38.11ms
step:845/2160 train_time:32225ms step_avg:38.14ms
step:846/2160 train_time:32284ms step_avg:38.16ms
step:847/2160 train_time:32345ms step_avg:38.19ms
step:848/2160 train_time:32404ms step_avg:38.21ms
step:849/2160 train_time:32465ms step_avg:38.24ms
step:850/2160 train_time:32525ms step_avg:38.26ms
step:851/2160 train_time:32587ms step_avg:38.29ms
step:852/2160 train_time:32647ms step_avg:38.32ms
step:853/2160 train_time:32708ms step_avg:38.35ms
step:854/2160 train_time:32768ms step_avg:38.37ms
step:855/2160 train_time:32830ms step_avg:38.40ms
step:856/2160 train_time:32889ms step_avg:38.42ms
step:857/2160 train_time:32951ms step_avg:38.45ms
step:858/2160 train_time:33011ms step_avg:38.47ms
step:859/2160 train_time:33072ms step_avg:38.50ms
step:860/2160 train_time:33131ms step_avg:38.52ms
step:861/2160 train_time:33192ms step_avg:38.55ms
step:862/2160 train_time:33252ms step_avg:38.57ms
step:863/2160 train_time:33313ms step_avg:38.60ms
step:864/2160 train_time:33372ms step_avg:38.63ms
step:865/2160 train_time:33433ms step_avg:38.65ms
step:866/2160 train_time:33493ms step_avg:38.68ms
step:867/2160 train_time:33555ms step_avg:38.70ms
step:868/2160 train_time:33614ms step_avg:38.73ms
step:869/2160 train_time:33676ms step_avg:38.75ms
step:870/2160 train_time:33735ms step_avg:38.78ms
step:871/2160 train_time:33796ms step_avg:38.80ms
step:872/2160 train_time:33855ms step_avg:38.83ms
step:873/2160 train_time:33917ms step_avg:38.85ms
step:874/2160 train_time:33977ms step_avg:38.87ms
step:875/2160 train_time:34038ms step_avg:38.90ms
step:876/2160 train_time:34097ms step_avg:38.92ms
step:877/2160 train_time:34158ms step_avg:38.95ms
step:878/2160 train_time:34217ms step_avg:38.97ms
step:879/2160 train_time:34279ms step_avg:39.00ms
step:880/2160 train_time:34338ms step_avg:39.02ms
step:881/2160 train_time:34400ms step_avg:39.05ms
step:882/2160 train_time:34460ms step_avg:39.07ms
step:883/2160 train_time:34522ms step_avg:39.10ms
step:884/2160 train_time:34581ms step_avg:39.12ms
step:885/2160 train_time:34643ms step_avg:39.14ms
step:886/2160 train_time:34702ms step_avg:39.17ms
step:887/2160 train_time:34763ms step_avg:39.19ms
step:888/2160 train_time:34823ms step_avg:39.21ms
step:889/2160 train_time:34884ms step_avg:39.24ms
step:890/2160 train_time:34944ms step_avg:39.26ms
step:891/2160 train_time:35005ms step_avg:39.29ms
step:892/2160 train_time:35065ms step_avg:39.31ms
step:893/2160 train_time:35126ms step_avg:39.33ms
step:894/2160 train_time:35185ms step_avg:39.36ms
step:895/2160 train_time:35247ms step_avg:39.38ms
step:896/2160 train_time:35306ms step_avg:39.40ms
step:897/2160 train_time:35367ms step_avg:39.43ms
step:898/2160 train_time:35427ms step_avg:39.45ms
step:899/2160 train_time:35489ms step_avg:39.48ms
step:900/2160 train_time:35548ms step_avg:39.50ms
step:901/2160 train_time:35610ms step_avg:39.52ms
step:902/2160 train_time:35669ms step_avg:39.54ms
step:903/2160 train_time:35731ms step_avg:39.57ms
step:904/2160 train_time:35790ms step_avg:39.59ms
step:905/2160 train_time:35852ms step_avg:39.62ms
step:906/2160 train_time:35912ms step_avg:39.64ms
step:907/2160 train_time:35973ms step_avg:39.66ms
step:908/2160 train_time:36033ms step_avg:39.68ms
step:909/2160 train_time:36094ms step_avg:39.71ms
step:910/2160 train_time:36153ms step_avg:39.73ms
step:911/2160 train_time:36214ms step_avg:39.75ms
step:912/2160 train_time:36273ms step_avg:39.77ms
step:913/2160 train_time:36335ms step_avg:39.80ms
step:914/2160 train_time:36394ms step_avg:39.82ms
step:915/2160 train_time:36455ms step_avg:39.84ms
step:916/2160 train_time:36515ms step_avg:39.86ms
step:917/2160 train_time:36576ms step_avg:39.89ms
step:918/2160 train_time:36635ms step_avg:39.91ms
step:919/2160 train_time:36697ms step_avg:39.93ms
step:920/2160 train_time:36756ms step_avg:39.95ms
step:921/2160 train_time:36818ms step_avg:39.98ms
step:922/2160 train_time:36877ms step_avg:40.00ms
step:923/2160 train_time:36939ms step_avg:40.02ms
step:924/2160 train_time:36998ms step_avg:40.04ms
step:925/2160 train_time:37060ms step_avg:40.06ms
step:926/2160 train_time:37119ms step_avg:40.09ms
step:927/2160 train_time:37181ms step_avg:40.11ms
step:928/2160 train_time:37240ms step_avg:40.13ms
step:929/2160 train_time:37302ms step_avg:40.15ms
step:930/2160 train_time:37362ms step_avg:40.17ms
step:931/2160 train_time:37423ms step_avg:40.20ms
step:932/2160 train_time:37482ms step_avg:40.22ms
step:933/2160 train_time:37543ms step_avg:40.24ms
step:934/2160 train_time:37603ms step_avg:40.26ms
step:935/2160 train_time:37665ms step_avg:40.28ms
step:936/2160 train_time:37724ms step_avg:40.30ms
step:937/2160 train_time:37785ms step_avg:40.33ms
step:938/2160 train_time:37844ms step_avg:40.35ms
step:939/2160 train_time:37905ms step_avg:40.37ms
step:940/2160 train_time:37965ms step_avg:40.39ms
step:941/2160 train_time:38027ms step_avg:40.41ms
step:942/2160 train_time:38087ms step_avg:40.43ms
step:943/2160 train_time:38148ms step_avg:40.45ms
step:944/2160 train_time:38208ms step_avg:40.47ms
step:945/2160 train_time:38269ms step_avg:40.50ms
step:946/2160 train_time:38329ms step_avg:40.52ms
step:947/2160 train_time:38390ms step_avg:40.54ms
step:948/2160 train_time:38450ms step_avg:40.56ms
step:949/2160 train_time:38511ms step_avg:40.58ms
step:950/2160 train_time:38571ms step_avg:40.60ms
step:951/2160 train_time:38632ms step_avg:40.62ms
step:952/2160 train_time:38691ms step_avg:40.64ms
step:953/2160 train_time:38753ms step_avg:40.66ms
step:954/2160 train_time:38813ms step_avg:40.68ms
step:955/2160 train_time:38875ms step_avg:40.71ms
step:956/2160 train_time:38934ms step_avg:40.73ms
step:957/2160 train_time:38994ms step_avg:40.75ms
step:958/2160 train_time:39054ms step_avg:40.77ms
step:959/2160 train_time:39115ms step_avg:40.79ms
step:960/2160 train_time:39175ms step_avg:40.81ms
step:961/2160 train_time:39236ms step_avg:40.83ms
step:962/2160 train_time:39295ms step_avg:40.85ms
step:963/2160 train_time:39357ms step_avg:40.87ms
step:964/2160 train_time:39416ms step_avg:40.89ms
step:965/2160 train_time:39478ms step_avg:40.91ms
step:966/2160 train_time:39537ms step_avg:40.93ms
step:967/2160 train_time:39599ms step_avg:40.95ms
step:968/2160 train_time:39659ms step_avg:40.97ms
step:969/2160 train_time:39721ms step_avg:40.99ms
step:970/2160 train_time:39780ms step_avg:41.01ms
step:971/2160 train_time:39841ms step_avg:41.03ms
step:972/2160 train_time:39901ms step_avg:41.05ms
step:973/2160 train_time:39962ms step_avg:41.07ms
step:974/2160 train_time:40021ms step_avg:41.09ms
step:975/2160 train_time:40083ms step_avg:41.11ms
step:976/2160 train_time:40142ms step_avg:41.13ms
step:977/2160 train_time:40204ms step_avg:41.15ms
step:978/2160 train_time:40263ms step_avg:41.17ms
step:979/2160 train_time:40325ms step_avg:41.19ms
step:980/2160 train_time:40384ms step_avg:41.21ms
step:981/2160 train_time:40445ms step_avg:41.23ms
step:982/2160 train_time:40505ms step_avg:41.25ms
step:983/2160 train_time:40567ms step_avg:41.27ms
step:984/2160 train_time:40626ms step_avg:41.29ms
step:985/2160 train_time:40687ms step_avg:41.31ms
step:986/2160 train_time:40747ms step_avg:41.33ms
step:987/2160 train_time:40808ms step_avg:41.35ms
step:988/2160 train_time:40867ms step_avg:41.36ms
step:989/2160 train_time:40928ms step_avg:41.38ms
step:990/2160 train_time:40988ms step_avg:41.40ms
step:991/2160 train_time:41049ms step_avg:41.42ms
step:992/2160 train_time:41110ms step_avg:41.44ms
step:993/2160 train_time:41171ms step_avg:41.46ms
step:994/2160 train_time:41231ms step_avg:41.48ms
step:995/2160 train_time:41292ms step_avg:41.50ms
step:996/2160 train_time:41351ms step_avg:41.52ms
step:997/2160 train_time:41413ms step_avg:41.54ms
step:998/2160 train_time:41472ms step_avg:41.56ms
step:999/2160 train_time:41533ms step_avg:41.58ms
step:1000/2160 train_time:41593ms step_avg:41.59ms
step:1000/2160 val_loss:3.6939 train_time:41654ms step_avg:41.65ms
step:1001/2160 train_time:41675ms step_avg:41.63ms
step:1002/2160 train_time:41714ms step_avg:41.63ms
step:1003/2160 train_time:41778ms step_avg:41.65ms
step:1004/2160 train_time:41839ms step_avg:41.67ms
step:1005/2160 train_time:41901ms step_avg:41.69ms
step:1006/2160 train_time:41960ms step_avg:41.71ms
step:1007/2160 train_time:42021ms step_avg:41.73ms
step:1008/2160 train_time:42081ms step_avg:41.75ms
step:1009/2160 train_time:42142ms step_avg:41.77ms
step:1010/2160 train_time:42200ms step_avg:41.78ms
step:1011/2160 train_time:42261ms step_avg:41.80ms
step:1012/2160 train_time:42320ms step_avg:41.82ms
step:1013/2160 train_time:42381ms step_avg:41.84ms
step:1014/2160 train_time:42440ms step_avg:41.85ms
step:1015/2160 train_time:42501ms step_avg:41.87ms
step:1016/2160 train_time:42560ms step_avg:41.89ms
step:1017/2160 train_time:42623ms step_avg:41.91ms
step:1018/2160 train_time:42683ms step_avg:41.93ms
step:1019/2160 train_time:42746ms step_avg:41.95ms
step:1020/2160 train_time:42807ms step_avg:41.97ms
step:1021/2160 train_time:42868ms step_avg:41.99ms
step:1022/2160 train_time:42928ms step_avg:42.00ms
step:1023/2160 train_time:42989ms step_avg:42.02ms
step:1024/2160 train_time:43049ms step_avg:42.04ms
step:1025/2160 train_time:43111ms step_avg:42.06ms
step:1026/2160 train_time:43170ms step_avg:42.08ms
step:1027/2160 train_time:43231ms step_avg:42.09ms
step:1028/2160 train_time:43291ms step_avg:42.11ms
step:1029/2160 train_time:43352ms step_avg:42.13ms
step:1030/2160 train_time:43412ms step_avg:42.15ms
step:1031/2160 train_time:43473ms step_avg:42.17ms
step:1032/2160 train_time:43533ms step_avg:42.18ms
step:1033/2160 train_time:43594ms step_avg:42.20ms
step:1034/2160 train_time:43653ms step_avg:42.22ms
step:1035/2160 train_time:43715ms step_avg:42.24ms
step:1036/2160 train_time:43775ms step_avg:42.25ms
step:1037/2160 train_time:43837ms step_avg:42.27ms
step:1038/2160 train_time:43897ms step_avg:42.29ms
step:1039/2160 train_time:43959ms step_avg:42.31ms
step:1040/2160 train_time:44018ms step_avg:42.33ms
step:1041/2160 train_time:44080ms step_avg:42.34ms
step:1042/2160 train_time:44140ms step_avg:42.36ms
step:1043/2160 train_time:44202ms step_avg:42.38ms
step:1044/2160 train_time:44262ms step_avg:42.40ms
step:1045/2160 train_time:44323ms step_avg:42.41ms
step:1046/2160 train_time:44383ms step_avg:42.43ms
step:1047/2160 train_time:44444ms step_avg:42.45ms
step:1048/2160 train_time:44503ms step_avg:42.46ms
step:1049/2160 train_time:44564ms step_avg:42.48ms
step:1050/2160 train_time:44623ms step_avg:42.50ms
step:1051/2160 train_time:44685ms step_avg:42.52ms
step:1052/2160 train_time:44745ms step_avg:42.53ms
step:1053/2160 train_time:44806ms step_avg:42.55ms
step:1054/2160 train_time:44866ms step_avg:42.57ms
step:1055/2160 train_time:44928ms step_avg:42.59ms
step:1056/2160 train_time:44987ms step_avg:42.60ms
step:1057/2160 train_time:45049ms step_avg:42.62ms
step:1058/2160 train_time:45109ms step_avg:42.64ms
step:1059/2160 train_time:45171ms step_avg:42.65ms
step:1060/2160 train_time:45231ms step_avg:42.67ms
step:1061/2160 train_time:45292ms step_avg:42.69ms
step:1062/2160 train_time:45352ms step_avg:42.70ms
step:1063/2160 train_time:45412ms step_avg:42.72ms
step:1064/2160 train_time:45472ms step_avg:42.74ms
step:1065/2160 train_time:45533ms step_avg:42.75ms
step:1066/2160 train_time:45593ms step_avg:42.77ms
step:1067/2160 train_time:45654ms step_avg:42.79ms
step:1068/2160 train_time:45714ms step_avg:42.80ms
step:1069/2160 train_time:45774ms step_avg:42.82ms
step:1070/2160 train_time:45834ms step_avg:42.84ms
step:1071/2160 train_time:45895ms step_avg:42.85ms
step:1072/2160 train_time:45954ms step_avg:42.87ms
step:1073/2160 train_time:46016ms step_avg:42.89ms
step:1074/2160 train_time:46076ms step_avg:42.90ms
step:1075/2160 train_time:46137ms step_avg:42.92ms
step:1076/2160 train_time:46198ms step_avg:42.93ms
step:1077/2160 train_time:46260ms step_avg:42.95ms
step:1078/2160 train_time:46319ms step_avg:42.97ms
step:1079/2160 train_time:46380ms step_avg:42.98ms
step:1080/2160 train_time:46440ms step_avg:43.00ms
step:1081/2160 train_time:46501ms step_avg:43.02ms
step:1082/2160 train_time:46561ms step_avg:43.03ms
step:1083/2160 train_time:46622ms step_avg:43.05ms
step:1084/2160 train_time:46682ms step_avg:43.06ms
step:1085/2160 train_time:46743ms step_avg:43.08ms
step:1086/2160 train_time:46802ms step_avg:43.10ms
step:1087/2160 train_time:46863ms step_avg:43.11ms
step:1088/2160 train_time:46923ms step_avg:43.13ms
step:1089/2160 train_time:46984ms step_avg:43.14ms
step:1090/2160 train_time:47044ms step_avg:43.16ms
step:1091/2160 train_time:47105ms step_avg:43.18ms
step:1092/2160 train_time:47165ms step_avg:43.19ms
step:1093/2160 train_time:47227ms step_avg:43.21ms
step:1094/2160 train_time:47287ms step_avg:43.22ms
step:1095/2160 train_time:47348ms step_avg:43.24ms
step:1096/2160 train_time:47408ms step_avg:43.26ms
step:1097/2160 train_time:47469ms step_avg:43.27ms
step:1098/2160 train_time:47529ms step_avg:43.29ms
step:1099/2160 train_time:47590ms step_avg:43.30ms
step:1100/2160 train_time:47650ms step_avg:43.32ms
step:1101/2160 train_time:47711ms step_avg:43.33ms
step:1102/2160 train_time:47771ms step_avg:43.35ms
step:1103/2160 train_time:47832ms step_avg:43.37ms
step:1104/2160 train_time:47892ms step_avg:43.38ms
step:1105/2160 train_time:47954ms step_avg:43.40ms
step:1106/2160 train_time:48014ms step_avg:43.41ms
step:1107/2160 train_time:48074ms step_avg:43.43ms
step:1108/2160 train_time:48134ms step_avg:43.44ms
step:1109/2160 train_time:48195ms step_avg:43.46ms
step:1110/2160 train_time:48254ms step_avg:43.47ms
step:1111/2160 train_time:48316ms step_avg:43.49ms
step:1112/2160 train_time:48375ms step_avg:43.50ms
step:1113/2160 train_time:48437ms step_avg:43.52ms
step:1114/2160 train_time:48497ms step_avg:43.53ms
step:1115/2160 train_time:48558ms step_avg:43.55ms
step:1116/2160 train_time:48619ms step_avg:43.57ms
step:1117/2160 train_time:48680ms step_avg:43.58ms
step:1118/2160 train_time:48741ms step_avg:43.60ms
step:1119/2160 train_time:48803ms step_avg:43.61ms
step:1120/2160 train_time:48863ms step_avg:43.63ms
step:1121/2160 train_time:48924ms step_avg:43.64ms
step:1122/2160 train_time:48983ms step_avg:43.66ms
step:1123/2160 train_time:49044ms step_avg:43.67ms
step:1124/2160 train_time:49104ms step_avg:43.69ms
step:1125/2160 train_time:49165ms step_avg:43.70ms
step:1126/2160 train_time:49224ms step_avg:43.72ms
step:1127/2160 train_time:49286ms step_avg:43.73ms
step:1128/2160 train_time:49345ms step_avg:43.75ms
step:1129/2160 train_time:49407ms step_avg:43.76ms
step:1130/2160 train_time:49467ms step_avg:43.78ms
step:1131/2160 train_time:49529ms step_avg:43.79ms
step:1132/2160 train_time:49589ms step_avg:43.81ms
step:1133/2160 train_time:49650ms step_avg:43.82ms
step:1134/2160 train_time:49711ms step_avg:43.84ms
step:1135/2160 train_time:49772ms step_avg:43.85ms
step:1136/2160 train_time:49832ms step_avg:43.87ms
step:1137/2160 train_time:49893ms step_avg:43.88ms
step:1138/2160 train_time:49953ms step_avg:43.90ms
step:1139/2160 train_time:50014ms step_avg:43.91ms
step:1140/2160 train_time:50073ms step_avg:43.92ms
step:1141/2160 train_time:50134ms step_avg:43.94ms
step:1142/2160 train_time:50193ms step_avg:43.95ms
step:1143/2160 train_time:50254ms step_avg:43.97ms
step:1144/2160 train_time:50314ms step_avg:43.98ms
step:1145/2160 train_time:50375ms step_avg:44.00ms
step:1146/2160 train_time:50435ms step_avg:44.01ms
step:1147/2160 train_time:50497ms step_avg:44.03ms
step:1148/2160 train_time:50557ms step_avg:44.04ms
step:1149/2160 train_time:50619ms step_avg:44.06ms
step:1150/2160 train_time:50680ms step_avg:44.07ms
step:1151/2160 train_time:50742ms step_avg:44.09ms
step:1152/2160 train_time:50802ms step_avg:44.10ms
step:1153/2160 train_time:50864ms step_avg:44.11ms
step:1154/2160 train_time:50923ms step_avg:44.13ms
step:1155/2160 train_time:50984ms step_avg:44.14ms
step:1156/2160 train_time:51044ms step_avg:44.16ms
step:1157/2160 train_time:51105ms step_avg:44.17ms
step:1158/2160 train_time:51164ms step_avg:44.18ms
step:1159/2160 train_time:51225ms step_avg:44.20ms
step:1160/2160 train_time:51284ms step_avg:44.21ms
step:1161/2160 train_time:51345ms step_avg:44.23ms
step:1162/2160 train_time:51405ms step_avg:44.24ms
step:1163/2160 train_time:51467ms step_avg:44.25ms
step:1164/2160 train_time:51527ms step_avg:44.27ms
step:1165/2160 train_time:51588ms step_avg:44.28ms
step:1166/2160 train_time:51649ms step_avg:44.30ms
step:1167/2160 train_time:51711ms step_avg:44.31ms
step:1168/2160 train_time:51771ms step_avg:44.32ms
step:1169/2160 train_time:51832ms step_avg:44.34ms
step:1170/2160 train_time:51892ms step_avg:44.35ms
step:1171/2160 train_time:51953ms step_avg:44.37ms
step:1172/2160 train_time:52013ms step_avg:44.38ms
step:1173/2160 train_time:52073ms step_avg:44.39ms
step:1174/2160 train_time:52133ms step_avg:44.41ms
step:1175/2160 train_time:52194ms step_avg:44.42ms
step:1176/2160 train_time:52253ms step_avg:44.43ms
step:1177/2160 train_time:52314ms step_avg:44.45ms
step:1178/2160 train_time:52373ms step_avg:44.46ms
step:1179/2160 train_time:52435ms step_avg:44.47ms
step:1180/2160 train_time:52495ms step_avg:44.49ms
step:1181/2160 train_time:52556ms step_avg:44.50ms
step:1182/2160 train_time:52616ms step_avg:44.51ms
step:1183/2160 train_time:52678ms step_avg:44.53ms
step:1184/2160 train_time:52738ms step_avg:44.54ms
step:1185/2160 train_time:52800ms step_avg:44.56ms
step:1186/2160 train_time:52860ms step_avg:44.57ms
step:1187/2160 train_time:52921ms step_avg:44.58ms
step:1188/2160 train_time:52981ms step_avg:44.60ms
step:1189/2160 train_time:53042ms step_avg:44.61ms
step:1190/2160 train_time:53102ms step_avg:44.62ms
step:1191/2160 train_time:53163ms step_avg:44.64ms
step:1192/2160 train_time:53222ms step_avg:44.65ms
step:1193/2160 train_time:53284ms step_avg:44.66ms
step:1194/2160 train_time:53343ms step_avg:44.68ms
step:1195/2160 train_time:53404ms step_avg:44.69ms
step:1196/2160 train_time:53464ms step_avg:44.70ms
step:1197/2160 train_time:53525ms step_avg:44.72ms
step:1198/2160 train_time:53585ms step_avg:44.73ms
step:1199/2160 train_time:53647ms step_avg:44.74ms
step:1200/2160 train_time:53707ms step_avg:44.76ms
step:1201/2160 train_time:53768ms step_avg:44.77ms
step:1202/2160 train_time:53828ms step_avg:44.78ms
step:1203/2160 train_time:53889ms step_avg:44.80ms
step:1204/2160 train_time:53949ms step_avg:44.81ms
step:1205/2160 train_time:54011ms step_avg:44.82ms
step:1206/2160 train_time:54071ms step_avg:44.83ms
step:1207/2160 train_time:54132ms step_avg:44.85ms
step:1208/2160 train_time:54192ms step_avg:44.86ms
step:1209/2160 train_time:54253ms step_avg:44.87ms
step:1210/2160 train_time:54312ms step_avg:44.89ms
step:1211/2160 train_time:54373ms step_avg:44.90ms
step:1212/2160 train_time:54432ms step_avg:44.91ms
step:1213/2160 train_time:54493ms step_avg:44.92ms
step:1214/2160 train_time:54553ms step_avg:44.94ms
step:1215/2160 train_time:54614ms step_avg:44.95ms
step:1216/2160 train_time:54674ms step_avg:44.96ms
step:1217/2160 train_time:54735ms step_avg:44.98ms
step:1218/2160 train_time:54795ms step_avg:44.99ms
step:1219/2160 train_time:54856ms step_avg:45.00ms
step:1220/2160 train_time:54916ms step_avg:45.01ms
step:1221/2160 train_time:54978ms step_avg:45.03ms
step:1222/2160 train_time:55038ms step_avg:45.04ms
step:1223/2160 train_time:55099ms step_avg:45.05ms
step:1224/2160 train_time:55159ms step_avg:45.06ms
step:1225/2160 train_time:55220ms step_avg:45.08ms
step:1226/2160 train_time:55280ms step_avg:45.09ms
step:1227/2160 train_time:55341ms step_avg:45.10ms
step:1228/2160 train_time:55401ms step_avg:45.11ms
step:1229/2160 train_time:55462ms step_avg:45.13ms
step:1230/2160 train_time:55521ms step_avg:45.14ms
step:1231/2160 train_time:55582ms step_avg:45.15ms
step:1232/2160 train_time:55642ms step_avg:45.16ms
step:1233/2160 train_time:55703ms step_avg:45.18ms
step:1234/2160 train_time:55763ms step_avg:45.19ms
step:1235/2160 train_time:55824ms step_avg:45.20ms
step:1236/2160 train_time:55884ms step_avg:45.21ms
step:1237/2160 train_time:55945ms step_avg:45.23ms
step:1238/2160 train_time:56005ms step_avg:45.24ms
step:1239/2160 train_time:56066ms step_avg:45.25ms
step:1240/2160 train_time:56126ms step_avg:45.26ms
step:1241/2160 train_time:56187ms step_avg:45.28ms
step:1242/2160 train_time:56248ms step_avg:45.29ms
step:1243/2160 train_time:56309ms step_avg:45.30ms
step:1244/2160 train_time:56369ms step_avg:45.31ms
step:1245/2160 train_time:56430ms step_avg:45.33ms
step:1246/2160 train_time:56491ms step_avg:45.34ms
step:1247/2160 train_time:56552ms step_avg:45.35ms
step:1248/2160 train_time:56612ms step_avg:45.36ms
step:1249/2160 train_time:56673ms step_avg:45.37ms
step:1250/2160 train_time:56732ms step_avg:45.39ms
step:1250/2160 val_loss:3.5785 train_time:56794ms step_avg:45.44ms
step:1251/2160 train_time:56813ms step_avg:45.41ms
step:1252/2160 train_time:56858ms step_avg:45.41ms
step:1253/2160 train_time:56922ms step_avg:45.43ms
step:1254/2160 train_time:56984ms step_avg:45.44ms
step:1255/2160 train_time:57046ms step_avg:45.46ms
step:1256/2160 train_time:57106ms step_avg:45.47ms
step:1257/2160 train_time:57166ms step_avg:45.48ms
step:1258/2160 train_time:57225ms step_avg:45.49ms
step:1259/2160 train_time:57286ms step_avg:45.50ms
step:1260/2160 train_time:57344ms step_avg:45.51ms
step:1261/2160 train_time:57405ms step_avg:45.52ms
step:1262/2160 train_time:57464ms step_avg:45.53ms
step:1263/2160 train_time:57524ms step_avg:45.55ms
step:1264/2160 train_time:57583ms step_avg:45.56ms
step:1265/2160 train_time:57644ms step_avg:45.57ms
step:1266/2160 train_time:57704ms step_avg:45.58ms
step:1267/2160 train_time:57768ms step_avg:45.59ms
step:1268/2160 train_time:57831ms step_avg:45.61ms
step:1269/2160 train_time:57895ms step_avg:45.62ms
step:1270/2160 train_time:57955ms step_avg:45.63ms
step:1271/2160 train_time:58017ms step_avg:45.65ms
step:1272/2160 train_time:58077ms step_avg:45.66ms
step:1273/2160 train_time:58139ms step_avg:45.67ms
step:1274/2160 train_time:58198ms step_avg:45.68ms
step:1275/2160 train_time:58259ms step_avg:45.69ms
step:1276/2160 train_time:58318ms step_avg:45.70ms
step:1277/2160 train_time:58379ms step_avg:45.72ms
step:1278/2160 train_time:58439ms step_avg:45.73ms
step:1279/2160 train_time:58499ms step_avg:45.74ms
step:1280/2160 train_time:58558ms step_avg:45.75ms
step:1281/2160 train_time:58619ms step_avg:45.76ms
step:1282/2160 train_time:58680ms step_avg:45.77ms
step:1283/2160 train_time:58743ms step_avg:45.79ms
step:1284/2160 train_time:58804ms step_avg:45.80ms
step:1285/2160 train_time:58866ms step_avg:45.81ms
step:1286/2160 train_time:58927ms step_avg:45.82ms
step:1287/2160 train_time:58988ms step_avg:45.83ms
step:1288/2160 train_time:59048ms step_avg:45.84ms
step:1289/2160 train_time:59109ms step_avg:45.86ms
step:1290/2160 train_time:59169ms step_avg:45.87ms
step:1291/2160 train_time:59230ms step_avg:45.88ms
step:1292/2160 train_time:59291ms step_avg:45.89ms
step:1293/2160 train_time:59352ms step_avg:45.90ms
step:1294/2160 train_time:59412ms step_avg:45.91ms
step:1295/2160 train_time:59473ms step_avg:45.93ms
step:1296/2160 train_time:59532ms step_avg:45.94ms
step:1297/2160 train_time:59594ms step_avg:45.95ms
step:1298/2160 train_time:59653ms step_avg:45.96ms
step:1299/2160 train_time:59715ms step_avg:45.97ms
step:1300/2160 train_time:59775ms step_avg:45.98ms
step:1301/2160 train_time:59836ms step_avg:45.99ms
step:1302/2160 train_time:59896ms step_avg:46.00ms
step:1303/2160 train_time:59957ms step_avg:46.01ms
step:1304/2160 train_time:60017ms step_avg:46.03ms
step:1305/2160 train_time:60079ms step_avg:46.04ms
step:1306/2160 train_time:60139ms step_avg:46.05ms
step:1307/2160 train_time:60201ms step_avg:46.06ms
step:1308/2160 train_time:60261ms step_avg:46.07ms
step:1309/2160 train_time:60323ms step_avg:46.08ms
step:1310/2160 train_time:60383ms step_avg:46.09ms
step:1311/2160 train_time:60444ms step_avg:46.11ms
step:1312/2160 train_time:60504ms step_avg:46.12ms
step:1313/2160 train_time:60565ms step_avg:46.13ms
step:1314/2160 train_time:60624ms step_avg:46.14ms
step:1315/2160 train_time:60685ms step_avg:46.15ms
step:1316/2160 train_time:60744ms step_avg:46.16ms
step:1317/2160 train_time:60806ms step_avg:46.17ms
step:1318/2160 train_time:60865ms step_avg:46.18ms
step:1319/2160 train_time:60927ms step_avg:46.19ms
step:1320/2160 train_time:60986ms step_avg:46.20ms
step:1321/2160 train_time:61048ms step_avg:46.21ms
step:1322/2160 train_time:61108ms step_avg:46.22ms
step:1323/2160 train_time:61170ms step_avg:46.24ms
step:1324/2160 train_time:61230ms step_avg:46.25ms
step:1325/2160 train_time:61291ms step_avg:46.26ms
step:1326/2160 train_time:61351ms step_avg:46.27ms
step:1327/2160 train_time:61413ms step_avg:46.28ms
step:1328/2160 train_time:61472ms step_avg:46.29ms
step:1329/2160 train_time:61533ms step_avg:46.30ms
step:1330/2160 train_time:61592ms step_avg:46.31ms
step:1331/2160 train_time:61654ms step_avg:46.32ms
step:1332/2160 train_time:61713ms step_avg:46.33ms
step:1333/2160 train_time:61774ms step_avg:46.34ms
step:1334/2160 train_time:61833ms step_avg:46.35ms
step:1335/2160 train_time:61895ms step_avg:46.36ms
step:1336/2160 train_time:61955ms step_avg:46.37ms
step:1337/2160 train_time:62016ms step_avg:46.38ms
step:1338/2160 train_time:62075ms step_avg:46.39ms
step:1339/2160 train_time:62137ms step_avg:46.41ms
step:1340/2160 train_time:62196ms step_avg:46.42ms
step:1341/2160 train_time:62258ms step_avg:46.43ms
step:1342/2160 train_time:62318ms step_avg:46.44ms
step:1343/2160 train_time:62380ms step_avg:46.45ms
step:1344/2160 train_time:62440ms step_avg:46.46ms
step:1345/2160 train_time:62501ms step_avg:46.47ms
step:1346/2160 train_time:62561ms step_avg:46.48ms
step:1347/2160 train_time:62623ms step_avg:46.49ms
step:1348/2160 train_time:62683ms step_avg:46.50ms
step:1349/2160 train_time:62744ms step_avg:46.51ms
step:1350/2160 train_time:62804ms step_avg:46.52ms
step:1351/2160 train_time:62865ms step_avg:46.53ms
step:1352/2160 train_time:62925ms step_avg:46.54ms
step:1353/2160 train_time:62986ms step_avg:46.55ms
step:1354/2160 train_time:63046ms step_avg:46.56ms
step:1355/2160 train_time:63107ms step_avg:46.57ms
step:1356/2160 train_time:63167ms step_avg:46.58ms
step:1357/2160 train_time:63229ms step_avg:46.59ms
step:1358/2160 train_time:63289ms step_avg:46.60ms
step:1359/2160 train_time:63350ms step_avg:46.62ms
step:1360/2160 train_time:63411ms step_avg:46.63ms
step:1361/2160 train_time:63472ms step_avg:46.64ms
step:1362/2160 train_time:63532ms step_avg:46.65ms
step:1363/2160 train_time:63593ms step_avg:46.66ms
step:1364/2160 train_time:63653ms step_avg:46.67ms
step:1365/2160 train_time:63714ms step_avg:46.68ms
step:1366/2160 train_time:63773ms step_avg:46.69ms
step:1367/2160 train_time:63834ms step_avg:46.70ms
step:1368/2160 train_time:63893ms step_avg:46.71ms
step:1369/2160 train_time:63954ms step_avg:46.72ms
step:1370/2160 train_time:64014ms step_avg:46.73ms
step:1371/2160 train_time:64075ms step_avg:46.74ms
step:1372/2160 train_time:64135ms step_avg:46.75ms
step:1373/2160 train_time:64196ms step_avg:46.76ms
step:1374/2160 train_time:64255ms step_avg:46.76ms
step:1375/2160 train_time:64317ms step_avg:46.78ms
step:1376/2160 train_time:64378ms step_avg:46.79ms
step:1377/2160 train_time:64439ms step_avg:46.80ms
step:1378/2160 train_time:64499ms step_avg:46.81ms
step:1379/2160 train_time:64560ms step_avg:46.82ms
step:1380/2160 train_time:64620ms step_avg:46.83ms
step:1381/2160 train_time:64682ms step_avg:46.84ms
step:1382/2160 train_time:64741ms step_avg:46.85ms
step:1383/2160 train_time:64803ms step_avg:46.86ms
step:1384/2160 train_time:64862ms step_avg:46.87ms
step:1385/2160 train_time:64924ms step_avg:46.88ms
step:1386/2160 train_time:64983ms step_avg:46.89ms
step:1387/2160 train_time:65045ms step_avg:46.90ms
step:1388/2160 train_time:65104ms step_avg:46.90ms
step:1389/2160 train_time:65165ms step_avg:46.92ms
step:1390/2160 train_time:65225ms step_avg:46.92ms
step:1391/2160 train_time:65287ms step_avg:46.94ms
step:1392/2160 train_time:65347ms step_avg:46.94ms
step:1393/2160 train_time:65409ms step_avg:46.96ms
step:1394/2160 train_time:65469ms step_avg:46.97ms
step:1395/2160 train_time:65531ms step_avg:46.98ms
step:1396/2160 train_time:65591ms step_avg:46.99ms
step:1397/2160 train_time:65654ms step_avg:47.00ms
step:1398/2160 train_time:65714ms step_avg:47.01ms
step:1399/2160 train_time:65774ms step_avg:47.02ms
step:1400/2160 train_time:65833ms step_avg:47.02ms
step:1401/2160 train_time:65894ms step_avg:47.03ms
step:1402/2160 train_time:65953ms step_avg:47.04ms
step:1403/2160 train_time:66014ms step_avg:47.05ms
step:1404/2160 train_time:66074ms step_avg:47.06ms
step:1405/2160 train_time:66135ms step_avg:47.07ms
step:1406/2160 train_time:66194ms step_avg:47.08ms
step:1407/2160 train_time:66255ms step_avg:47.09ms
step:1408/2160 train_time:66314ms step_avg:47.10ms
step:1409/2160 train_time:66376ms step_avg:47.11ms
step:1410/2160 train_time:66435ms step_avg:47.12ms
step:1411/2160 train_time:66497ms step_avg:47.13ms
step:1412/2160 train_time:66557ms step_avg:47.14ms
step:1413/2160 train_time:66619ms step_avg:47.15ms
step:1414/2160 train_time:66678ms step_avg:47.16ms
step:1415/2160 train_time:66740ms step_avg:47.17ms
step:1416/2160 train_time:66827ms step_avg:47.19ms
step:1417/2160 train_time:66916ms step_avg:47.22ms
step:1418/2160 train_time:67003ms step_avg:47.25ms
step:1419/2160 train_time:67093ms step_avg:47.28ms
step:1420/2160 train_time:67179ms step_avg:47.31ms
step:1421/2160 train_time:67268ms step_avg:47.34ms
step:1422/2160 train_time:67357ms step_avg:47.37ms
step:1423/2160 train_time:67446ms step_avg:47.40ms
step:1424/2160 train_time:67534ms step_avg:47.43ms
step:1425/2160 train_time:67625ms step_avg:47.46ms
step:1426/2160 train_time:67712ms step_avg:47.48ms
step:1427/2160 train_time:67801ms step_avg:47.51ms
step:1428/2160 train_time:67888ms step_avg:47.54ms
step:1429/2160 train_time:67977ms step_avg:47.57ms
step:1430/2160 train_time:68064ms step_avg:47.60ms
step:1431/2160 train_time:68155ms step_avg:47.63ms
step:1432/2160 train_time:68241ms step_avg:47.65ms
step:1433/2160 train_time:68331ms step_avg:47.68ms
step:1434/2160 train_time:68419ms step_avg:47.71ms
step:1435/2160 train_time:68508ms step_avg:47.74ms
step:1436/2160 train_time:68595ms step_avg:47.77ms
step:1437/2160 train_time:68686ms step_avg:47.80ms
step:1438/2160 train_time:68774ms step_avg:47.83ms
step:1439/2160 train_time:68863ms step_avg:47.85ms
step:1440/2160 train_time:68950ms step_avg:47.88ms
step:1441/2160 train_time:69040ms step_avg:47.91ms
step:1442/2160 train_time:69128ms step_avg:47.94ms
step:1443/2160 train_time:69217ms step_avg:47.97ms
step:1444/2160 train_time:69305ms step_avg:48.00ms
step:1445/2160 train_time:69395ms step_avg:48.02ms
step:1446/2160 train_time:69483ms step_avg:48.05ms
step:1447/2160 train_time:69572ms step_avg:48.08ms
step:1448/2160 train_time:69660ms step_avg:48.11ms
step:1449/2160 train_time:69749ms step_avg:48.14ms
step:1450/2160 train_time:69837ms step_avg:48.16ms
step:1451/2160 train_time:69926ms step_avg:48.19ms
step:1452/2160 train_time:70013ms step_avg:48.22ms
step:1453/2160 train_time:70102ms step_avg:48.25ms
step:1454/2160 train_time:70189ms step_avg:48.27ms
step:1455/2160 train_time:70278ms step_avg:48.30ms
step:1456/2160 train_time:70366ms step_avg:48.33ms
step:1457/2160 train_time:70455ms step_avg:48.36ms
step:1458/2160 train_time:70543ms step_avg:48.38ms
step:1459/2160 train_time:70631ms step_avg:48.41ms
step:1460/2160 train_time:70719ms step_avg:48.44ms
step:1461/2160 train_time:70808ms step_avg:48.47ms
step:1462/2160 train_time:70895ms step_avg:48.49ms
step:1463/2160 train_time:70984ms step_avg:48.52ms
step:1464/2160 train_time:71072ms step_avg:48.55ms
step:1465/2160 train_time:71161ms step_avg:48.57ms
step:1466/2160 train_time:71248ms step_avg:48.60ms
step:1467/2160 train_time:71337ms step_avg:48.63ms
step:1468/2160 train_time:71424ms step_avg:48.65ms
step:1469/2160 train_time:71513ms step_avg:48.68ms
step:1470/2160 train_time:71600ms step_avg:48.71ms
step:1471/2160 train_time:71690ms step_avg:48.74ms
step:1472/2160 train_time:71778ms step_avg:48.76ms
step:1473/2160 train_time:71867ms step_avg:48.79ms
step:1474/2160 train_time:71954ms step_avg:48.82ms
step:1475/2160 train_time:72044ms step_avg:48.84ms
step:1476/2160 train_time:72130ms step_avg:48.87ms
step:1477/2160 train_time:72220ms step_avg:48.90ms
step:1478/2160 train_time:72307ms step_avg:48.92ms
step:1479/2160 train_time:72397ms step_avg:48.95ms
step:1480/2160 train_time:72485ms step_avg:48.98ms
step:1481/2160 train_time:72575ms step_avg:49.00ms
step:1482/2160 train_time:72661ms step_avg:49.03ms
step:1483/2160 train_time:72751ms step_avg:49.06ms
step:1484/2160 train_time:72838ms step_avg:49.08ms
step:1485/2160 train_time:72928ms step_avg:49.11ms
step:1486/2160 train_time:73015ms step_avg:49.14ms
step:1487/2160 train_time:73104ms step_avg:49.16ms
step:1488/2160 train_time:73192ms step_avg:49.19ms
step:1489/2160 train_time:73280ms step_avg:49.21ms
step:1490/2160 train_time:73368ms step_avg:49.24ms
step:1491/2160 train_time:73458ms step_avg:49.27ms
step:1492/2160 train_time:73546ms step_avg:49.29ms
step:1493/2160 train_time:73635ms step_avg:49.32ms
step:1494/2160 train_time:73723ms step_avg:49.35ms
step:1495/2160 train_time:73811ms step_avg:49.37ms
step:1496/2160 train_time:73899ms step_avg:49.40ms
step:1497/2160 train_time:73988ms step_avg:49.42ms
step:1498/2160 train_time:74075ms step_avg:49.45ms
step:1499/2160 train_time:74164ms step_avg:49.48ms
step:1500/2160 train_time:74252ms step_avg:49.50ms
step:1500/2160 val_loss:3.4976 train_time:74342ms step_avg:49.56ms
step:1501/2160 train_time:74361ms step_avg:49.54ms
step:1502/2160 train_time:74431ms step_avg:49.55ms
step:1503/2160 train_time:74522ms step_avg:49.58ms
step:1504/2160 train_time:74612ms step_avg:49.61ms
step:1505/2160 train_time:74701ms step_avg:49.64ms
step:1506/2160 train_time:74787ms step_avg:49.66ms
step:1507/2160 train_time:74875ms step_avg:49.68ms
step:1508/2160 train_time:74960ms step_avg:49.71ms
step:1509/2160 train_time:75048ms step_avg:49.73ms
step:1510/2160 train_time:75136ms step_avg:49.76ms
step:1511/2160 train_time:75226ms step_avg:49.79ms
step:1512/2160 train_time:75318ms step_avg:49.81ms
step:1513/2160 train_time:75408ms step_avg:49.84ms
step:1514/2160 train_time:75497ms step_avg:49.87ms
step:1515/2160 train_time:75588ms step_avg:49.89ms
step:1516/2160 train_time:75675ms step_avg:49.92ms
step:1517/2160 train_time:75764ms step_avg:49.94ms
step:1518/2160 train_time:75851ms step_avg:49.97ms
step:1519/2160 train_time:75938ms step_avg:49.99ms
step:1520/2160 train_time:76025ms step_avg:50.02ms
step:1521/2160 train_time:76114ms step_avg:50.04ms
step:1522/2160 train_time:76201ms step_avg:50.07ms
step:1523/2160 train_time:76291ms step_avg:50.09ms
step:1524/2160 train_time:76380ms step_avg:50.12ms
step:1525/2160 train_time:76471ms step_avg:50.14ms
step:1526/2160 train_time:76559ms step_avg:50.17ms
step:1527/2160 train_time:76648ms step_avg:50.20ms
step:1528/2160 train_time:76736ms step_avg:50.22ms
step:1529/2160 train_time:76824ms step_avg:50.24ms
step:1530/2160 train_time:76911ms step_avg:50.27ms
step:1531/2160 train_time:76999ms step_avg:50.29ms
step:1532/2160 train_time:77086ms step_avg:50.32ms
step:1533/2160 train_time:77175ms step_avg:50.34ms
step:1534/2160 train_time:77263ms step_avg:50.37ms
step:1535/2160 train_time:77352ms step_avg:50.39ms
step:1536/2160 train_time:77441ms step_avg:50.42ms
step:1537/2160 train_time:77531ms step_avg:50.44ms
step:1538/2160 train_time:77618ms step_avg:50.47ms
step:1539/2160 train_time:77707ms step_avg:50.49ms
step:1540/2160 train_time:77794ms step_avg:50.52ms
step:1541/2160 train_time:77883ms step_avg:50.54ms
step:1542/2160 train_time:77971ms step_avg:50.56ms
step:1543/2160 train_time:78060ms step_avg:50.59ms
step:1544/2160 train_time:78148ms step_avg:50.61ms
step:1545/2160 train_time:78237ms step_avg:50.64ms
step:1546/2160 train_time:78325ms step_avg:50.66ms
step:1547/2160 train_time:78414ms step_avg:50.69ms
step:1548/2160 train_time:78502ms step_avg:50.71ms
step:1549/2160 train_time:78592ms step_avg:50.74ms
step:1550/2160 train_time:78680ms step_avg:50.76ms
step:1551/2160 train_time:78770ms step_avg:50.79ms
step:1552/2160 train_time:78858ms step_avg:50.81ms
step:1553/2160 train_time:78946ms step_avg:50.83ms
step:1554/2160 train_time:79033ms step_avg:50.86ms
step:1555/2160 train_time:79122ms step_avg:50.88ms
step:1556/2160 train_time:79211ms step_avg:50.91ms
step:1557/2160 train_time:79301ms step_avg:50.93ms
step:1558/2160 train_time:79389ms step_avg:50.96ms
step:1559/2160 train_time:79479ms step_avg:50.98ms
step:1560/2160 train_time:79566ms step_avg:51.00ms
step:1561/2160 train_time:79655ms step_avg:51.03ms
step:1562/2160 train_time:79743ms step_avg:51.05ms
step:1563/2160 train_time:79831ms step_avg:51.08ms
step:1564/2160 train_time:79919ms step_avg:51.10ms
step:1565/2160 train_time:80007ms step_avg:51.12ms
step:1566/2160 train_time:80094ms step_avg:51.15ms
step:1567/2160 train_time:80183ms step_avg:51.17ms
step:1568/2160 train_time:80270ms step_avg:51.19ms
step:1569/2160 train_time:80360ms step_avg:51.22ms
step:1570/2160 train_time:80447ms step_avg:51.24ms
step:1571/2160 train_time:80537ms step_avg:51.26ms
step:1572/2160 train_time:80624ms step_avg:51.29ms
step:1573/2160 train_time:80714ms step_avg:51.31ms
step:1574/2160 train_time:80802ms step_avg:51.34ms
step:1575/2160 train_time:80891ms step_avg:51.36ms
step:1576/2160 train_time:80978ms step_avg:51.38ms
step:1577/2160 train_time:81067ms step_avg:51.41ms
step:1578/2160 train_time:81154ms step_avg:51.43ms
step:1579/2160 train_time:81243ms step_avg:51.45ms
step:1580/2160 train_time:81331ms step_avg:51.48ms
step:1581/2160 train_time:81420ms step_avg:51.50ms
step:1582/2160 train_time:81508ms step_avg:51.52ms
step:1583/2160 train_time:81597ms step_avg:51.55ms
step:1584/2160 train_time:81684ms step_avg:51.57ms
step:1585/2160 train_time:81773ms step_avg:51.59ms
step:1586/2160 train_time:81860ms step_avg:51.61ms
step:1587/2160 train_time:81950ms step_avg:51.64ms
step:1588/2160 train_time:82038ms step_avg:51.66ms
step:1589/2160 train_time:82127ms step_avg:51.68ms
step:1590/2160 train_time:82214ms step_avg:51.71ms
step:1591/2160 train_time:82304ms step_avg:51.73ms
step:1592/2160 train_time:82391ms step_avg:51.75ms
step:1593/2160 train_time:82480ms step_avg:51.78ms
step:1594/2160 train_time:82568ms step_avg:51.80ms
step:1595/2160 train_time:82658ms step_avg:51.82ms
step:1596/2160 train_time:82745ms step_avg:51.84ms
step:1597/2160 train_time:82834ms step_avg:51.87ms
step:1598/2160 train_time:82922ms step_avg:51.89ms
step:1599/2160 train_time:83011ms step_avg:51.91ms
step:1600/2160 train_time:83098ms step_avg:51.94ms
step:1601/2160 train_time:83187ms step_avg:51.96ms
step:1602/2160 train_time:83274ms step_avg:51.98ms
step:1603/2160 train_time:83362ms step_avg:52.00ms
step:1604/2160 train_time:83450ms step_avg:52.03ms
step:1605/2160 train_time:83540ms step_avg:52.05ms
step:1606/2160 train_time:83628ms step_avg:52.07ms
step:1607/2160 train_time:83717ms step_avg:52.10ms
step:1608/2160 train_time:83805ms step_avg:52.12ms
step:1609/2160 train_time:83894ms step_avg:52.14ms
step:1610/2160 train_time:83981ms step_avg:52.16ms
step:1611/2160 train_time:84071ms step_avg:52.19ms
step:1612/2160 train_time:84158ms step_avg:52.21ms
step:1613/2160 train_time:84247ms step_avg:52.23ms
step:1614/2160 train_time:84334ms step_avg:52.25ms
step:1615/2160 train_time:84423ms step_avg:52.27ms
step:1616/2160 train_time:84511ms step_avg:52.30ms
step:1617/2160 train_time:84600ms step_avg:52.32ms
step:1618/2160 train_time:84688ms step_avg:52.34ms
step:1619/2160 train_time:84777ms step_avg:52.36ms
step:1620/2160 train_time:84864ms step_avg:52.39ms
step:1621/2160 train_time:84953ms step_avg:52.41ms
step:1622/2160 train_time:85041ms step_avg:52.43ms
step:1623/2160 train_time:85129ms step_avg:52.45ms
step:1624/2160 train_time:85217ms step_avg:52.47ms
step:1625/2160 train_time:85307ms step_avg:52.50ms
step:1626/2160 train_time:85394ms step_avg:52.52ms
step:1627/2160 train_time:85483ms step_avg:52.54ms
step:1628/2160 train_time:85570ms step_avg:52.56ms
step:1629/2160 train_time:85660ms step_avg:52.58ms
step:1630/2160 train_time:85747ms step_avg:52.61ms
step:1631/2160 train_time:85837ms step_avg:52.63ms
step:1632/2160 train_time:85925ms step_avg:52.65ms
step:1633/2160 train_time:86014ms step_avg:52.67ms
step:1634/2160 train_time:86102ms step_avg:52.69ms
step:1635/2160 train_time:86190ms step_avg:52.72ms
step:1636/2160 train_time:86278ms step_avg:52.74ms
step:1637/2160 train_time:86368ms step_avg:52.76ms
step:1638/2160 train_time:86455ms step_avg:52.78ms
step:1639/2160 train_time:86544ms step_avg:52.80ms
step:1640/2160 train_time:86631ms step_avg:52.82ms
step:1641/2160 train_time:86720ms step_avg:52.85ms
step:1642/2160 train_time:86808ms step_avg:52.87ms
step:1643/2160 train_time:86897ms step_avg:52.89ms
step:1644/2160 train_time:86984ms step_avg:52.91ms
step:1645/2160 train_time:87073ms step_avg:52.93ms
step:1646/2160 train_time:87162ms step_avg:52.95ms
step:1647/2160 train_time:87251ms step_avg:52.98ms
step:1648/2160 train_time:87339ms step_avg:53.00ms
step:1649/2160 train_time:87428ms step_avg:53.02ms
step:1650/2160 train_time:87516ms step_avg:53.04ms
step:1651/2160 train_time:87605ms step_avg:53.06ms
step:1652/2160 train_time:87692ms step_avg:53.08ms
step:1653/2160 train_time:87781ms step_avg:53.10ms
step:1654/2160 train_time:87868ms step_avg:53.12ms
step:1655/2160 train_time:87956ms step_avg:53.15ms
step:1656/2160 train_time:88044ms step_avg:53.17ms
step:1657/2160 train_time:88133ms step_avg:53.19ms
step:1658/2160 train_time:88221ms step_avg:53.21ms
step:1659/2160 train_time:88310ms step_avg:53.23ms
step:1660/2160 train_time:88399ms step_avg:53.25ms
step:1661/2160 train_time:88489ms step_avg:53.27ms
step:1662/2160 train_time:88576ms step_avg:53.30ms
step:1663/2160 train_time:88665ms step_avg:53.32ms
step:1664/2160 train_time:88753ms step_avg:53.34ms
step:1665/2160 train_time:88841ms step_avg:53.36ms
step:1666/2160 train_time:88928ms step_avg:53.38ms
step:1667/2160 train_time:89017ms step_avg:53.40ms
step:1668/2160 train_time:89105ms step_avg:53.42ms
step:1669/2160 train_time:89193ms step_avg:53.44ms
step:1670/2160 train_time:89281ms step_avg:53.46ms
step:1671/2160 train_time:89371ms step_avg:53.48ms
step:1672/2160 train_time:89458ms step_avg:53.50ms
step:1673/2160 train_time:89548ms step_avg:53.53ms
step:1674/2160 train_time:89635ms step_avg:53.55ms
step:1675/2160 train_time:89724ms step_avg:53.57ms
step:1676/2160 train_time:89812ms step_avg:53.59ms
step:1677/2160 train_time:89901ms step_avg:53.61ms
step:1678/2160 train_time:89988ms step_avg:53.63ms
step:1679/2160 train_time:90077ms step_avg:53.65ms
step:1680/2160 train_time:90165ms step_avg:53.67ms
step:1681/2160 train_time:90254ms step_avg:53.69ms
step:1682/2160 train_time:90342ms step_avg:53.71ms
step:1683/2160 train_time:90432ms step_avg:53.73ms
step:1684/2160 train_time:90519ms step_avg:53.75ms
step:1685/2160 train_time:90608ms step_avg:53.77ms
step:1686/2160 train_time:90696ms step_avg:53.79ms
step:1687/2160 train_time:90785ms step_avg:53.81ms
step:1688/2160 train_time:90872ms step_avg:53.83ms
step:1689/2160 train_time:90961ms step_avg:53.85ms
step:1690/2160 train_time:91048ms step_avg:53.87ms
step:1691/2160 train_time:91137ms step_avg:53.90ms
step:1692/2160 train_time:91224ms step_avg:53.91ms
step:1693/2160 train_time:91314ms step_avg:53.94ms
step:1694/2160 train_time:91401ms step_avg:53.96ms
step:1695/2160 train_time:91492ms step_avg:53.98ms
step:1696/2160 train_time:91579ms step_avg:54.00ms
step:1697/2160 train_time:91668ms step_avg:54.02ms
step:1698/2160 train_time:91756ms step_avg:54.04ms
step:1699/2160 train_time:91844ms step_avg:54.06ms
step:1700/2160 train_time:91932ms step_avg:54.08ms
step:1701/2160 train_time:92021ms step_avg:54.10ms
step:1702/2160 train_time:92108ms step_avg:54.12ms
step:1703/2160 train_time:92197ms step_avg:54.14ms
step:1704/2160 train_time:92285ms step_avg:54.16ms
step:1705/2160 train_time:92374ms step_avg:54.18ms
step:1706/2160 train_time:92463ms step_avg:54.20ms
step:1707/2160 train_time:92552ms step_avg:54.22ms
step:1708/2160 train_time:92639ms step_avg:54.24ms
step:1709/2160 train_time:92729ms step_avg:54.26ms
step:1710/2160 train_time:92816ms step_avg:54.28ms
step:1711/2160 train_time:92905ms step_avg:54.30ms
step:1712/2160 train_time:92992ms step_avg:54.32ms
step:1713/2160 train_time:93081ms step_avg:54.34ms
step:1714/2160 train_time:93168ms step_avg:54.36ms
step:1715/2160 train_time:93257ms step_avg:54.38ms
step:1716/2160 train_time:93345ms step_avg:54.40ms
step:1717/2160 train_time:93435ms step_avg:54.42ms
step:1718/2160 train_time:93524ms step_avg:54.44ms
step:1719/2160 train_time:93612ms step_avg:54.46ms
step:1720/2160 train_time:93700ms step_avg:54.48ms
step:1721/2160 train_time:93789ms step_avg:54.50ms
step:1722/2160 train_time:93877ms step_avg:54.52ms
step:1723/2160 train_time:93967ms step_avg:54.54ms
step:1724/2160 train_time:94054ms step_avg:54.56ms
step:1725/2160 train_time:94142ms step_avg:54.58ms
step:1726/2160 train_time:94230ms step_avg:54.59ms
step:1727/2160 train_time:94319ms step_avg:54.61ms
step:1728/2160 train_time:94407ms step_avg:54.63ms
step:1729/2160 train_time:94496ms step_avg:54.65ms
step:1730/2160 train_time:94583ms step_avg:54.67ms
step:1731/2160 train_time:94672ms step_avg:54.69ms
step:1732/2160 train_time:94760ms step_avg:54.71ms
step:1733/2160 train_time:94848ms step_avg:54.73ms
step:1734/2160 train_time:94936ms step_avg:54.75ms
step:1735/2160 train_time:95026ms step_avg:54.77ms
step:1736/2160 train_time:95113ms step_avg:54.79ms
step:1737/2160 train_time:95203ms step_avg:54.81ms
step:1738/2160 train_time:95290ms step_avg:54.83ms
step:1739/2160 train_time:95379ms step_avg:54.85ms
step:1740/2160 train_time:95466ms step_avg:54.87ms
step:1741/2160 train_time:95556ms step_avg:54.89ms
step:1742/2160 train_time:95644ms step_avg:54.90ms
step:1743/2160 train_time:95733ms step_avg:54.92ms
step:1744/2160 train_time:95820ms step_avg:54.94ms
step:1745/2160 train_time:95909ms step_avg:54.96ms
step:1746/2160 train_time:95997ms step_avg:54.98ms
step:1747/2160 train_time:96086ms step_avg:55.00ms
step:1748/2160 train_time:96173ms step_avg:55.02ms
step:1749/2160 train_time:96262ms step_avg:55.04ms
step:1750/2160 train_time:96349ms step_avg:55.06ms
step:1750/2160 val_loss:3.3942 train_time:96441ms step_avg:55.11ms
step:1751/2160 train_time:96460ms step_avg:55.09ms
step:1752/2160 train_time:96533ms step_avg:55.10ms
step:1753/2160 train_time:96626ms step_avg:55.12ms
step:1754/2160 train_time:96713ms step_avg:55.14ms
step:1755/2160 train_time:96801ms step_avg:55.16ms
step:1756/2160 train_time:96888ms step_avg:55.18ms
step:1757/2160 train_time:96975ms step_avg:55.19ms
step:1758/2160 train_time:97063ms step_avg:55.21ms
step:1759/2160 train_time:97150ms step_avg:55.23ms
step:1760/2160 train_time:97237ms step_avg:55.25ms
step:1761/2160 train_time:97327ms step_avg:55.27ms
step:1762/2160 train_time:97416ms step_avg:55.29ms
step:1763/2160 train_time:97507ms step_avg:55.31ms
step:1764/2160 train_time:97596ms step_avg:55.33ms
step:1765/2160 train_time:97686ms step_avg:55.35ms
step:1766/2160 train_time:97774ms step_avg:55.36ms
step:1767/2160 train_time:97863ms step_avg:55.38ms
step:1768/2160 train_time:97949ms step_avg:55.40ms
step:1769/2160 train_time:98038ms step_avg:55.42ms
step:1770/2160 train_time:98125ms step_avg:55.44ms
step:1771/2160 train_time:98214ms step_avg:55.46ms
step:1772/2160 train_time:98301ms step_avg:55.47ms
step:1773/2160 train_time:98391ms step_avg:55.49ms
step:1774/2160 train_time:98480ms step_avg:55.51ms
step:1775/2160 train_time:98570ms step_avg:55.53ms
step:1776/2160 train_time:98660ms step_avg:55.55ms
step:1777/2160 train_time:98748ms step_avg:55.57ms
step:1778/2160 train_time:98835ms step_avg:55.59ms
step:1779/2160 train_time:98924ms step_avg:55.61ms
step:1780/2160 train_time:99010ms step_avg:55.62ms
step:1781/2160 train_time:99098ms step_avg:55.64ms
step:1782/2160 train_time:99185ms step_avg:55.66ms
step:1783/2160 train_time:99275ms step_avg:55.68ms
step:1784/2160 train_time:99362ms step_avg:55.70ms
step:1785/2160 train_time:99452ms step_avg:55.72ms
step:1786/2160 train_time:99539ms step_avg:55.73ms
step:1787/2160 train_time:99628ms step_avg:55.75ms
step:1788/2160 train_time:99716ms step_avg:55.77ms
step:1789/2160 train_time:99806ms step_avg:55.79ms
step:1790/2160 train_time:99894ms step_avg:55.81ms
step:1791/2160 train_time:99983ms step_avg:55.82ms
step:1792/2160 train_time:100069ms step_avg:55.84ms
step:1793/2160 train_time:100158ms step_avg:55.86ms
step:1794/2160 train_time:100246ms step_avg:55.88ms
step:1795/2160 train_time:100336ms step_avg:55.90ms
step:1796/2160 train_time:100423ms step_avg:55.92ms
step:1797/2160 train_time:100513ms step_avg:55.93ms
step:1798/2160 train_time:100601ms step_avg:55.95ms
step:1799/2160 train_time:100690ms step_avg:55.97ms
step:1800/2160 train_time:100778ms step_avg:55.99ms
step:1801/2160 train_time:100868ms step_avg:56.01ms
step:1802/2160 train_time:100956ms step_avg:56.02ms
step:1803/2160 train_time:101044ms step_avg:56.04ms
step:1804/2160 train_time:101131ms step_avg:56.06ms
step:1805/2160 train_time:101220ms step_avg:56.08ms
step:1806/2160 train_time:101307ms step_avg:56.09ms
step:1807/2160 train_time:101396ms step_avg:56.11ms
step:1808/2160 train_time:101485ms step_avg:56.13ms
step:1809/2160 train_time:101574ms step_avg:56.15ms
step:1810/2160 train_time:101662ms step_avg:56.17ms
step:1811/2160 train_time:101751ms step_avg:56.18ms
step:1812/2160 train_time:101838ms step_avg:56.20ms
step:1813/2160 train_time:101927ms step_avg:56.22ms
step:1814/2160 train_time:102014ms step_avg:56.24ms
step:1815/2160 train_time:102103ms step_avg:56.25ms
step:1816/2160 train_time:102190ms step_avg:56.27ms
step:1817/2160 train_time:102279ms step_avg:56.29ms
step:1818/2160 train_time:102367ms step_avg:56.31ms
step:1819/2160 train_time:102457ms step_avg:56.33ms
step:1820/2160 train_time:102545ms step_avg:56.34ms
step:1821/2160 train_time:102635ms step_avg:56.36ms
step:1822/2160 train_time:102722ms step_avg:56.38ms
step:1823/2160 train_time:102812ms step_avg:56.40ms
step:1824/2160 train_time:102900ms step_avg:56.41ms
step:1825/2160 train_time:102989ms step_avg:56.43ms
step:1826/2160 train_time:103076ms step_avg:56.45ms
step:1827/2160 train_time:103165ms step_avg:56.47ms
step:1828/2160 train_time:103251ms step_avg:56.48ms
step:1829/2160 train_time:103341ms step_avg:56.50ms
step:1830/2160 train_time:103428ms step_avg:56.52ms
step:1831/2160 train_time:103517ms step_avg:56.54ms
step:1832/2160 train_time:103606ms step_avg:56.55ms
step:1833/2160 train_time:103696ms step_avg:56.57ms
step:1834/2160 train_time:103784ms step_avg:56.59ms
step:1835/2160 train_time:103875ms step_avg:56.61ms
step:1836/2160 train_time:103962ms step_avg:56.62ms
step:1837/2160 train_time:104051ms step_avg:56.64ms
step:1838/2160 train_time:104138ms step_avg:56.66ms
step:1839/2160 train_time:104227ms step_avg:56.68ms
step:1840/2160 train_time:104313ms step_avg:56.69ms
step:1841/2160 train_time:104403ms step_avg:56.71ms
step:1842/2160 train_time:104490ms step_avg:56.73ms
step:1843/2160 train_time:104580ms step_avg:56.74ms
step:1844/2160 train_time:104668ms step_avg:56.76ms
step:1845/2160 train_time:104757ms step_avg:56.78ms
step:1846/2160 train_time:104844ms step_avg:56.80ms
step:1847/2160 train_time:104935ms step_avg:56.81ms
step:1848/2160 train_time:105021ms step_avg:56.83ms
step:1849/2160 train_time:105111ms step_avg:56.85ms
step:1850/2160 train_time:105197ms step_avg:56.86ms
step:1851/2160 train_time:105286ms step_avg:56.88ms
step:1852/2160 train_time:105373ms step_avg:56.90ms
step:1853/2160 train_time:105463ms step_avg:56.91ms
step:1854/2160 train_time:105551ms step_avg:56.93ms
step:1855/2160 train_time:105640ms step_avg:56.95ms
step:1856/2160 train_time:105728ms step_avg:56.97ms
step:1857/2160 train_time:105817ms step_avg:56.98ms
step:1858/2160 train_time:105905ms step_avg:57.00ms
step:1859/2160 train_time:105994ms step_avg:57.02ms
step:1860/2160 train_time:106081ms step_avg:57.03ms
step:1861/2160 train_time:106171ms step_avg:57.05ms
step:1862/2160 train_time:106258ms step_avg:57.07ms
step:1863/2160 train_time:106346ms step_avg:57.08ms
step:1864/2160 train_time:106434ms step_avg:57.10ms
step:1865/2160 train_time:106523ms step_avg:57.12ms
step:1866/2160 train_time:106611ms step_avg:57.13ms
step:1867/2160 train_time:106699ms step_avg:57.15ms
step:1868/2160 train_time:106787ms step_avg:57.17ms
step:1869/2160 train_time:106876ms step_avg:57.18ms
step:1870/2160 train_time:106964ms step_avg:57.20ms
step:1871/2160 train_time:107053ms step_avg:57.22ms
step:1872/2160 train_time:107140ms step_avg:57.23ms
step:1873/2160 train_time:107229ms step_avg:57.25ms
step:1874/2160 train_time:107316ms step_avg:57.27ms
step:1875/2160 train_time:107406ms step_avg:57.28ms
step:1876/2160 train_time:107494ms step_avg:57.30ms
step:1877/2160 train_time:107583ms step_avg:57.32ms
step:1878/2160 train_time:107670ms step_avg:57.33ms
step:1879/2160 train_time:107760ms step_avg:57.35ms
step:1880/2160 train_time:107847ms step_avg:57.37ms
step:1881/2160 train_time:107937ms step_avg:57.38ms
step:1882/2160 train_time:108024ms step_avg:57.40ms
step:1883/2160 train_time:108113ms step_avg:57.42ms
step:1884/2160 train_time:108200ms step_avg:57.43ms
step:1885/2160 train_time:108290ms step_avg:57.45ms
step:1886/2160 train_time:108378ms step_avg:57.46ms
step:1887/2160 train_time:108467ms step_avg:57.48ms
step:1888/2160 train_time:108555ms step_avg:57.50ms
step:1889/2160 train_time:108645ms step_avg:57.51ms
step:1890/2160 train_time:108732ms step_avg:57.53ms
step:1891/2160 train_time:108821ms step_avg:57.55ms
step:1892/2160 train_time:108909ms step_avg:57.56ms
step:1893/2160 train_time:108998ms step_avg:57.58ms
step:1894/2160 train_time:109085ms step_avg:57.60ms
step:1895/2160 train_time:109175ms step_avg:57.61ms
step:1896/2160 train_time:109262ms step_avg:57.63ms
step:1897/2160 train_time:109351ms step_avg:57.64ms
step:1898/2160 train_time:109438ms step_avg:57.66ms
step:1899/2160 train_time:109527ms step_avg:57.68ms
step:1900/2160 train_time:109614ms step_avg:57.69ms
step:1901/2160 train_time:109703ms step_avg:57.71ms
step:1902/2160 train_time:109791ms step_avg:57.72ms
step:1903/2160 train_time:109881ms step_avg:57.74ms
step:1904/2160 train_time:109968ms step_avg:57.76ms
step:1905/2160 train_time:110057ms step_avg:57.77ms
step:1906/2160 train_time:110145ms step_avg:57.79ms
step:1907/2160 train_time:110235ms step_avg:57.81ms
step:1908/2160 train_time:110322ms step_avg:57.82ms
step:1909/2160 train_time:110411ms step_avg:57.84ms
step:1910/2160 train_time:110498ms step_avg:57.85ms
step:1911/2160 train_time:110587ms step_avg:57.87ms
step:1912/2160 train_time:110675ms step_avg:57.88ms
step:1913/2160 train_time:110764ms step_avg:57.90ms
step:1914/2160 train_time:110852ms step_avg:57.92ms
step:1915/2160 train_time:110941ms step_avg:57.93ms
step:1916/2160 train_time:111028ms step_avg:57.95ms
step:1917/2160 train_time:111117ms step_avg:57.96ms
step:1918/2160 train_time:111205ms step_avg:57.98ms
step:1919/2160 train_time:111295ms step_avg:58.00ms
step:1920/2160 train_time:111382ms step_avg:58.01ms
step:1921/2160 train_time:111471ms step_avg:58.03ms
step:1922/2160 train_time:111558ms step_avg:58.04ms
step:1923/2160 train_time:111647ms step_avg:58.06ms
step:1924/2160 train_time:111734ms step_avg:58.07ms
step:1925/2160 train_time:111823ms step_avg:58.09ms
step:1926/2160 train_time:111910ms step_avg:58.10ms
step:1927/2160 train_time:111999ms step_avg:58.12ms
step:1928/2160 train_time:112087ms step_avg:58.14ms
step:1929/2160 train_time:112176ms step_avg:58.15ms
step:1930/2160 train_time:112264ms step_avg:58.17ms
step:1931/2160 train_time:112352ms step_avg:58.18ms
step:1932/2160 train_time:112440ms step_avg:58.20ms
step:1933/2160 train_time:112528ms step_avg:58.21ms
step:1934/2160 train_time:112615ms step_avg:58.23ms
step:1935/2160 train_time:112705ms step_avg:58.25ms
step:1936/2160 train_time:112792ms step_avg:58.26ms
step:1937/2160 train_time:112882ms step_avg:58.28ms
step:1938/2160 train_time:112968ms step_avg:58.29ms
step:1939/2160 train_time:113057ms step_avg:58.31ms
step:1940/2160 train_time:113145ms step_avg:58.32ms
step:1941/2160 train_time:113235ms step_avg:58.34ms
step:1942/2160 train_time:113323ms step_avg:58.35ms
step:1943/2160 train_time:113412ms step_avg:58.37ms
step:1944/2160 train_time:113499ms step_avg:58.38ms
step:1945/2160 train_time:113588ms step_avg:58.40ms
step:1946/2160 train_time:113676ms step_avg:58.42ms
step:1947/2160 train_time:113765ms step_avg:58.43ms
step:1948/2160 train_time:113852ms step_avg:58.45ms
step:1949/2160 train_time:113941ms step_avg:58.46ms
step:1950/2160 train_time:114029ms step_avg:58.48ms
step:1951/2160 train_time:114118ms step_avg:58.49ms
step:1952/2160 train_time:114206ms step_avg:58.51ms
step:1953/2160 train_time:114295ms step_avg:58.52ms
step:1954/2160 train_time:114383ms step_avg:58.54ms
step:1955/2160 train_time:114471ms step_avg:58.55ms
step:1956/2160 train_time:114559ms step_avg:58.57ms
step:1957/2160 train_time:114647ms step_avg:58.58ms
step:1958/2160 train_time:114735ms step_avg:58.60ms
step:1959/2160 train_time:114824ms step_avg:58.61ms
step:1960/2160 train_time:114912ms step_avg:58.63ms
step:1961/2160 train_time:115001ms step_avg:58.64ms
step:1962/2160 train_time:115089ms step_avg:58.66ms
step:1963/2160 train_time:115178ms step_avg:58.67ms
step:1964/2160 train_time:115266ms step_avg:58.69ms
step:1965/2160 train_time:115354ms step_avg:58.70ms
step:1966/2160 train_time:115441ms step_avg:58.72ms
step:1967/2160 train_time:115532ms step_avg:58.73ms
step:1968/2160 train_time:115618ms step_avg:58.75ms
step:1969/2160 train_time:115707ms step_avg:58.76ms
step:1970/2160 train_time:115795ms step_avg:58.78ms
step:1971/2160 train_time:115884ms step_avg:58.79ms
step:1972/2160 train_time:115971ms step_avg:58.81ms
step:1973/2160 train_time:116061ms step_avg:58.82ms
step:1974/2160 train_time:116148ms step_avg:58.84ms
step:1975/2160 train_time:116237ms step_avg:58.85ms
step:1976/2160 train_time:116324ms step_avg:58.87ms
step:1977/2160 train_time:116414ms step_avg:58.88ms
step:1978/2160 train_time:116501ms step_avg:58.90ms
step:1979/2160 train_time:116590ms step_avg:58.91ms
step:1980/2160 train_time:116678ms step_avg:58.93ms
step:1981/2160 train_time:116767ms step_avg:58.94ms
step:1982/2160 train_time:116856ms step_avg:58.96ms
step:1983/2160 train_time:116944ms step_avg:58.97ms
step:1984/2160 train_time:117032ms step_avg:58.99ms
step:1985/2160 train_time:117120ms step_avg:59.00ms
step:1986/2160 train_time:117208ms step_avg:59.02ms
step:1987/2160 train_time:117298ms step_avg:59.03ms
step:1988/2160 train_time:117387ms step_avg:59.05ms
step:1989/2160 train_time:117477ms step_avg:59.06ms
step:1990/2160 train_time:117564ms step_avg:59.08ms
step:1991/2160 train_time:117653ms step_avg:59.09ms
step:1992/2160 train_time:117740ms step_avg:59.11ms
step:1993/2160 train_time:117829ms step_avg:59.12ms
step:1994/2160 train_time:117916ms step_avg:59.14ms
step:1995/2160 train_time:118005ms step_avg:59.15ms
step:1996/2160 train_time:118093ms step_avg:59.16ms
step:1997/2160 train_time:118183ms step_avg:59.18ms
step:1998/2160 train_time:118270ms step_avg:59.19ms
step:1999/2160 train_time:118360ms step_avg:59.21ms
step:2000/2160 train_time:118447ms step_avg:59.22ms
step:2000/2160 val_loss:3.3152 train_time:118538ms step_avg:59.27ms
step:2001/2160 train_time:118557ms step_avg:59.25ms
step:2002/2160 train_time:118629ms step_avg:59.26ms
step:2003/2160 train_time:118720ms step_avg:59.27ms
step:2004/2160 train_time:118808ms step_avg:59.29ms
step:2005/2160 train_time:118896ms step_avg:59.30ms
step:2006/2160 train_time:118983ms step_avg:59.31ms
step:2007/2160 train_time:119071ms step_avg:59.33ms
step:2008/2160 train_time:119159ms step_avg:59.34ms
step:2009/2160 train_time:119247ms step_avg:59.36ms
step:2010/2160 train_time:119335ms step_avg:59.37ms
step:2011/2160 train_time:119423ms step_avg:59.39ms
step:2012/2160 train_time:119512ms step_avg:59.40ms
step:2013/2160 train_time:119603ms step_avg:59.42ms
step:2014/2160 train_time:119691ms step_avg:59.43ms
step:2015/2160 train_time:119782ms step_avg:59.44ms
step:2016/2160 train_time:119869ms step_avg:59.46ms
step:2017/2160 train_time:119958ms step_avg:59.47ms
step:2018/2160 train_time:120044ms step_avg:59.49ms
step:2019/2160 train_time:120132ms step_avg:59.50ms
step:2020/2160 train_time:120220ms step_avg:59.52ms
step:2021/2160 train_time:120310ms step_avg:59.53ms
step:2022/2160 train_time:120398ms step_avg:59.54ms
step:2023/2160 train_time:120487ms step_avg:59.56ms
step:2024/2160 train_time:120575ms step_avg:59.57ms
step:2025/2160 train_time:120666ms step_avg:59.59ms
step:2026/2160 train_time:120754ms step_avg:59.60ms
step:2027/2160 train_time:120844ms step_avg:59.62ms
step:2028/2160 train_time:120931ms step_avg:59.63ms
step:2029/2160 train_time:121019ms step_avg:59.64ms
step:2030/2160 train_time:121106ms step_avg:59.66ms
step:2031/2160 train_time:121194ms step_avg:59.67ms
step:2032/2160 train_time:121282ms step_avg:59.69ms
step:2033/2160 train_time:121371ms step_avg:59.70ms
step:2034/2160 train_time:121458ms step_avg:59.71ms
step:2035/2160 train_time:121547ms step_avg:59.73ms
step:2036/2160 train_time:121636ms step_avg:59.74ms
step:2037/2160 train_time:121725ms step_avg:59.76ms
step:2038/2160 train_time:121814ms step_avg:59.77ms
step:2039/2160 train_time:121903ms step_avg:59.79ms
step:2040/2160 train_time:121990ms step_avg:59.80ms
step:2041/2160 train_time:122079ms step_avg:59.81ms
step:2042/2160 train_time:122166ms step_avg:59.83ms
step:2043/2160 train_time:122255ms step_avg:59.84ms
step:2044/2160 train_time:122342ms step_avg:59.85ms
step:2045/2160 train_time:122431ms step_avg:59.87ms
step:2046/2160 train_time:122518ms step_avg:59.88ms
step:2047/2160 train_time:122608ms step_avg:59.90ms
step:2048/2160 train_time:122696ms step_avg:59.91ms
step:2049/2160 train_time:122786ms step_avg:59.92ms
step:2050/2160 train_time:122874ms step_avg:59.94ms
step:2051/2160 train_time:122963ms step_avg:59.95ms
step:2052/2160 train_time:123050ms step_avg:59.97ms
step:2053/2160 train_time:123139ms step_avg:59.98ms
step:2054/2160 train_time:123226ms step_avg:59.99ms
step:2055/2160 train_time:123315ms step_avg:60.01ms
step:2056/2160 train_time:123403ms step_avg:60.02ms
step:2057/2160 train_time:123491ms step_avg:60.03ms
step:2058/2160 train_time:123578ms step_avg:60.05ms
step:2059/2160 train_time:123668ms step_avg:60.06ms
step:2060/2160 train_time:123755ms step_avg:60.08ms
step:2061/2160 train_time:123845ms step_avg:60.09ms
step:2062/2160 train_time:123933ms step_avg:60.10ms
step:2063/2160 train_time:124022ms step_avg:60.12ms
step:2064/2160 train_time:124109ms step_avg:60.13ms
step:2065/2160 train_time:124199ms step_avg:60.14ms
step:2066/2160 train_time:124286ms step_avg:60.16ms
step:2067/2160 train_time:124376ms step_avg:60.17ms
step:2068/2160 train_time:124462ms step_avg:60.18ms
step:2069/2160 train_time:124551ms step_avg:60.20ms
step:2070/2160 train_time:124639ms step_avg:60.21ms
step:2071/2160 train_time:124729ms step_avg:60.23ms
step:2072/2160 train_time:124816ms step_avg:60.24ms
step:2073/2160 train_time:124906ms step_avg:60.25ms
step:2074/2160 train_time:124995ms step_avg:60.27ms
step:2075/2160 train_time:125084ms step_avg:60.28ms
step:2076/2160 train_time:125171ms step_avg:60.29ms
step:2077/2160 train_time:125261ms step_avg:60.31ms
step:2078/2160 train_time:125348ms step_avg:60.32ms
step:2079/2160 train_time:125436ms step_avg:60.33ms
step:2080/2160 train_time:125524ms step_avg:60.35ms
step:2081/2160 train_time:125614ms step_avg:60.36ms
step:2082/2160 train_time:125702ms step_avg:60.38ms
step:2083/2160 train_time:125792ms step_avg:60.39ms
step:2084/2160 train_time:125880ms step_avg:60.40ms
step:2085/2160 train_time:125971ms step_avg:60.42ms
step:2086/2160 train_time:126058ms step_avg:60.43ms
step:2087/2160 train_time:126147ms step_avg:60.44ms
step:2088/2160 train_time:126235ms step_avg:60.46ms
step:2089/2160 train_time:126324ms step_avg:60.47ms
step:2090/2160 train_time:126411ms step_avg:60.48ms
step:2091/2160 train_time:126500ms step_avg:60.50ms
step:2092/2160 train_time:126588ms step_avg:60.51ms
step:2093/2160 train_time:126678ms step_avg:60.52ms
step:2094/2160 train_time:126766ms step_avg:60.54ms
step:2095/2160 train_time:126855ms step_avg:60.55ms
step:2096/2160 train_time:126943ms step_avg:60.56ms
step:2097/2160 train_time:127033ms step_avg:60.58ms
step:2098/2160 train_time:127120ms step_avg:60.59ms
step:2099/2160 train_time:127210ms step_avg:60.60ms
step:2100/2160 train_time:127297ms step_avg:60.62ms
step:2101/2160 train_time:127387ms step_avg:60.63ms
step:2102/2160 train_time:127475ms step_avg:60.64ms
step:2103/2160 train_time:127563ms step_avg:60.66ms
step:2104/2160 train_time:127650ms step_avg:60.67ms
step:2105/2160 train_time:127740ms step_avg:60.68ms
step:2106/2160 train_time:127828ms step_avg:60.70ms
step:2107/2160 train_time:127917ms step_avg:60.71ms
step:2108/2160 train_time:128005ms step_avg:60.72ms
step:2109/2160 train_time:128094ms step_avg:60.74ms
step:2110/2160 train_time:128182ms step_avg:60.75ms
step:2111/2160 train_time:128271ms step_avg:60.76ms
step:2112/2160 train_time:128359ms step_avg:60.78ms
step:2113/2160 train_time:128448ms step_avg:60.79ms
step:2114/2160 train_time:128536ms step_avg:60.80ms
step:2115/2160 train_time:128624ms step_avg:60.82ms
step:2116/2160 train_time:128712ms step_avg:60.83ms
step:2117/2160 train_time:128801ms step_avg:60.84ms
step:2118/2160 train_time:128889ms step_avg:60.85ms
step:2119/2160 train_time:128977ms step_avg:60.87ms
step:2120/2160 train_time:129065ms step_avg:60.88ms
step:2121/2160 train_time:129154ms step_avg:60.89ms
step:2122/2160 train_time:129242ms step_avg:60.91ms
step:2123/2160 train_time:129331ms step_avg:60.92ms
step:2124/2160 train_time:129419ms step_avg:60.93ms
step:2125/2160 train_time:129508ms step_avg:60.94ms
step:2126/2160 train_time:129595ms step_avg:60.96ms
step:2127/2160 train_time:129686ms step_avg:60.97ms
step:2128/2160 train_time:129774ms step_avg:60.98ms
step:2129/2160 train_time:129863ms step_avg:61.00ms
step:2130/2160 train_time:129951ms step_avg:61.01ms
step:2131/2160 train_time:130041ms step_avg:61.02ms
step:2132/2160 train_time:130130ms step_avg:61.04ms
step:2133/2160 train_time:130220ms step_avg:61.05ms
step:2134/2160 train_time:130308ms step_avg:61.06ms
step:2135/2160 train_time:130398ms step_avg:61.08ms
step:2136/2160 train_time:130485ms step_avg:61.09ms
step:2137/2160 train_time:130574ms step_avg:61.10ms
step:2138/2160 train_time:130662ms step_avg:61.11ms
step:2139/2160 train_time:130752ms step_avg:61.13ms
step:2140/2160 train_time:130840ms step_avg:61.14ms
step:2141/2160 train_time:130930ms step_avg:61.15ms
step:2142/2160 train_time:131018ms step_avg:61.17ms
step:2143/2160 train_time:131107ms step_avg:61.18ms
step:2144/2160 train_time:131195ms step_avg:61.19ms
step:2145/2160 train_time:131284ms step_avg:61.20ms
step:2146/2160 train_time:131372ms step_avg:61.22ms
step:2147/2160 train_time:131461ms step_avg:61.23ms
step:2148/2160 train_time:131548ms step_avg:61.24ms
step:2149/2160 train_time:131637ms step_avg:61.26ms
step:2150/2160 train_time:131725ms step_avg:61.27ms
step:2151/2160 train_time:131815ms step_avg:61.28ms
step:2152/2160 train_time:131903ms step_avg:61.29ms
step:2153/2160 train_time:131994ms step_avg:61.31ms
step:2154/2160 train_time:132081ms step_avg:61.32ms
step:2155/2160 train_time:132172ms step_avg:61.33ms
step:2156/2160 train_time:132260ms step_avg:61.35ms
step:2157/2160 train_time:132349ms step_avg:61.36ms
step:2158/2160 train_time:132438ms step_avg:61.37ms
step:2159/2160 train_time:132527ms step_avg:61.38ms
step:2160/2160 train_time:132615ms step_avg:61.40ms
step:2160/2160 val_loss:3.2794 train_time:132706ms step_avg:61.44ms
peak memory allocated: 29896 MiB reserved: 61112 MiB
